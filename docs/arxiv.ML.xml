<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>


<item>
<title>SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models</title>
<link>https://arxiv.org/abs/2503.13503</link>
<guid>https://arxiv.org/abs/2503.13503</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, AI4Science, Data Quality, Model Capability

Summary: 
SciHorizon is a proposed comprehensive assessment framework for evaluating AI4Science readiness from both scientific data and LLM perspectives. It includes a generalizable framework for assessing AI-ready scientific data, comprising dimensions such as Quality, FAIRness, Explainability, and Compliance. The study recommends AI-ready datasets for Earth, Life, and Materials Sciences based on data resource papers. Additionally, the framework evaluates the capabilities of LLMs across various scientific disciplines using 16 assessment dimensions and five core indicators. The assessment covers Mathematics, Physics, Chemistry, Life Sciences, Earth, and Space Sciences, with over 50 LLMs evaluated using benchmark datasets. All results are publicly available online at www.scihorizon.cn/en. 

<br /><br />Summary: SciHorizon proposes a comprehensive framework for assessing AI4Science readiness, evaluating AI-ready scientific data and LLM capabilities. Recommendations for datasets in Earth, Life, and Materials Sciences are provided, along with an evaluation of over 50 LLMs across multiple scientific disciplines. <div>
arXiv:2503.13503v3 Announce Type: replace 
Abstract: In recent years, the rapid advancement of Artificial Intelligence (AI) technologies, particularly Large Language Models (LLMs), has revolutionized the paradigm of scientific discovery, establishing AI-for-Science (AI4Science) as a dynamic and evolving field. However, there is still a lack of an effective framework for the overall assessment of AI4Science, particularly from a holistic perspective on data quality and model capability. Therefore, in this study, we propose SciHorizon, a comprehensive assessment framework designed to benchmark the readiness of AI4Science from both scientific data and LLM perspectives. First, we introduce a generalizable framework for assessing AI-ready scientific data, encompassing four key dimensions: Quality, FAIRness, Explainability, and Compliance-which are subdivided into 15 sub-dimensions. Drawing on data resource papers published between 2018 and 2023 in peer-reviewed journals, we present recommendation lists of AI-ready datasets for Earth, Life, and Materials Sciences, making a novel and original contribution to the field. Concurrently, to assess the capabilities of LLMs across multiple scientific disciplines, we establish 16 assessment dimensions based on five core indicators Knowledge, Understanding, Reasoning, Multimodality, and Values spanning Mathematics, Physics, Chemistry, Life Sciences, and Earth and Space Sciences. Using the developed benchmark datasets, we have conducted a comprehensive evaluation of over 50 representative open-source and closed source LLMs. All the results are publicly available and can be accessed online at www.scihorizon.cn/en.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18447</link>
<guid>https://arxiv.org/abs/2505.18447</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, transfer learning, policy, performance guarantees, negative transfer

Summary:
The paper introduces a novel framework for transfer reinforcement learning that addresses challenges such as the lack of performance guarantees and the risk of negative transfer. The framework is based on the pessimism principle and constructs a conservative estimation of the target domain's performance. This approach ensures safe and reliable decision-making while also avoiding negative transfer when multiple source domains are involved. Two types of conservative estimations are developed, and their effectiveness is rigorously characterized. Efficient distributed algorithms with convergence guarantees are proposed. The framework provides a theoretically sound and practically robust solution for transfer learning in reinforcement learning. <div>
arXiv:2505.18447v2 Announce Type: replace 
Abstract: Transfer reinforcement learning aims to derive a near-optimal policy for a target environment with limited data by leveraging abundant data from related source domains. However, it faces two key challenges: the lack of performance guarantees for the transferred policy, which can lead to undesired actions, and the risk of negative transfer when multiple source domains are involved. We propose a novel framework based on the pessimism principle, which constructs and optimizes a conservative estimation of the target domain's performance. Our framework effectively addresses the two challenges by providing an optimized lower bound on target performance, ensuring safe and reliable decisions, and by exhibiting monotonic improvement with respect to the quality of the source domains, thereby avoiding negative transfer. We construct two types of conservative estimations, rigorously characterize their effectiveness, and develop efficient distributed algorithms with convergence guarantees. Our framework provides a theoretically sound and practically robust solution for transfer learning in reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark</title>
<link>https://arxiv.org/abs/2505.16968</link>
<guid>https://arxiv.org/abs/2505.16968</guid>
<content:encoded><![CDATA[
<div> dataset, model suite, GPU code transpilation, assembly-level translation, domain-specific language models
<br />
Summary:<br />
The paper introduces CASS, a dataset and model suite for GPU code transpilation across architectures. The dataset contains 70k verified code pairs for source-level and assembly-level translation. The CASS family of domain-specific language models achieves high accuracy in source and assembly translation, surpassing commercial baselines. The generated code maintains native performance in the majority of test cases. A benchmark, CASS-Bench, with 16 GPU domains and ground-truth execution is introduced for evaluation. All data, models, and tools are open-sourced to encourage advancements in GPU compiler tooling, binary compatibility, and hardware translation guided by large language models.
<br /> <div>
arXiv:2505.16968v3 Announce Type: replace-cross 
Abstract: We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA <--> HIP) and assembly-level (Nvidia SASS <--> AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.20569</link>
<guid>https://arxiv.org/abs/2505.20569</guid>
<content:encoded><![CDATA[
<div> Keywords: Object Hallucination, Vision-Language Models, Contrastive Decoding, Retrieval Visual Contrastive Decoding, Logit Level<br />
<br />
Summary: 
The article introduces RVCD (Retrieval Visual Contrastive Decoding) as a method to address the challenge of Object Hallucination in Large Vision-Language Models. RVCD builds upon previous studies on contrastive decoding and leverages both negative and positive images at the logit level to suppress OH. By referencing AI-generated images representing a single concept, RVCD demonstrates significant improvements over existing decoding-based methods. This advanced approach offers a promising solution to the persistence of OH in vision-language models, enhancing the model's ability to accurately interpret and generate visual content. <div>
arXiv:2505.20569v2 Announce Type: replace-cross 
Abstract: Despite significant advancements in Large Vision-Language Models, Object Hallucination (OH) remains a persistent challenge. Building upon prior studies on contrastive decoding that address this issue without requiring additional model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an advanced method to suppress OH. RVCD leverages both negative and positive images at the logit level, explicitly referencing AI-generated images designed to represent a single concept. Our approach demonstrates substantial improvements over existing decoding-based methods.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data</title>
<link>https://arxiv.org/abs/2505.20688</link>
<guid>https://arxiv.org/abs/2505.20688</guid>
<content:encoded><![CDATA[
<div> FDR control, neuroimaging, spatial dependencies, statistical methods, Alzheimer's Disease Neuroimaging Initiative<br />
<br />
Summary: 
The article proposes a novel spatial FDR control method, fcHMRF-LIS, for voxel-wise multiple testing in neuroimaging data analysis. By integrating the LIS-based testing procedure with a fully connected hidden Markov random field (fcHMRF), the method effectively captures complex spatial dependencies. It addresses three key challenges in neuroimaging applications: maintaining low variability in false discovery proportion and false non-discovery proportion, achieving computational scalability for high-resolution data, and accurately controlling FDR while reducing false non-discovery rates. The proposed method outperforms existing approaches in terms of accuracy and computational efficiency, as demonstrated through extensive simulations and application to an Alzheimer's disease dataset from the ADNI. The efficient expectation-maximization algorithm and innovative techniques such as CRF-RNN and permutohedral lattice filtering contribute to the method's success in accurately identifying neurobiologically relevant brain regions. <br /><br /> <div>
arXiv:2505.20688v2 Announce Type: replace-cross 
Abstract: False discovery rate (FDR) control methods are essential for voxel-wise multiple testing in neuroimaging data analysis, where hundreds of thousands or even millions of tests are conducted to detect brain regions associated with disease-related changes. Classical FDR control methods (e.g., BH, q-value, and LocalFDR) assume independence among tests and often lead to high false non-discovery rates (FNR). Although various spatial FDR control methods have been developed to improve power, they still fall short of jointly addressing three major challenges in neuroimaging applications: capturing complex spatial dependencies, maintaining low variability in both false discovery proportion (FDP) and false non-discovery proportion (FNP) across replications, and achieving computational scalability for high-resolution data. To address these challenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR control method for voxel-wise multiple testing. It integrates the local index of significance (LIS)-based testing procedure with a novel fully connected hidden Markov random field (fcHMRF) designed to model complex spatial structures using a parsimonious parameterization. We develop an efficient expectation-maximization algorithm incorporating mean-field approximation, the Conditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and permutohedral lattice filtering, reducing the time complexity from quadratic to linear in the number of tests. Extensive simulations demonstrate that fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability in FDP and FNP, and a higher number of true positives compared to existing methods. Applied to an FDG-PET dataset from the Alzheimer's Disease Neuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain regions and offers notable advantages in computational efficiency.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation</title>
<link>https://arxiv.org/abs/2505.20745</link>
<guid>https://arxiv.org/abs/2505.20745</guid>
<content:encoded><![CDATA[
<div> acoustics, representation models, auscultation, heart rate estimation, pre-training

Summary:
- The study investigates the encoding of auscultation in self-supervised acoustic representation foundation models (FMs) for vital sign analysis.
- Six FMs, including HuBERT, wav2vec2, wavLM, Whisper, CLAP, and an in-house CLAP model, are evaluated using a phonocardiogram dataset and heart rate estimation model.
- Results show that FMs offer comparable performance to a baseline method relying on acoustic features for heart rate estimation.
- The in-house CLAP model's audio encoder outperforms the baseline, achieving a lower mean absolute error despite domain mismatch.
- This research demonstrates the potential of pre-trained FMs in enhancing heart sound analysis and vital sign monitoring. 

<br /><br />Summary: <div>
arXiv:2505.20745v2 Announce Type: replace-cross 
Abstract: Auscultation, particularly heart sound, is a non-invasive technique that provides essential vital sign information. Recently, self-supervised acoustic representation foundation models (FMs) have been proposed to offer insights into acoustics-based vital signs. However, there has been little exploration of the extent to which auscultation is encoded in these pre-trained FM representations. In this work, using a publicly available phonocardiogram (PCG) dataset and a heart rate (HR) estimation model, we conduct a layer-wise investigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM, Whisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP model. Additionally, we implement the baseline method from Nie et al., 2024 (which relies on acoustic features) and show that overall, representation vectors from pre-trained foundation models (FMs) offer comparable performance to the baseline. Notably, HR estimation using the representations from the audio encoder of the in-house CLAP model outperforms the results obtained from the baseline, achieving a lower mean absolute error (MAE) across various train/validation/test splits despite the domain mismatch.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection</title>
<link>https://arxiv.org/abs/2505.20956</link>
<guid>https://arxiv.org/abs/2505.20956</guid>
<content:encoded><![CDATA[
<div> Active learning, Bioacoustic sound event detection, Mismatch-first farthest-traversal, Committee voting disagreement, Diversity analysis<br />
Summary:<br />
Bioacoustic sound event detection (BioSED) is essential for biodiversity conservation but faces challenges like limited annotated data, sparse events, and species diversity. This study introduces the mismatch-first farthest-traversal (MFFT) method for efficient model development with limited labeling budget. The method combines committee voting disagreement and diversity analysis and achieves a mean average precision (mAP) of 68% in cold-start scenarios and 71% in warm-start scenarios with only 2.3% of annotations. This performance is close to fully-supervised results. MFFT excels in detecting rare species, crucial for monitoring endangered species. The study also refines a BioSED dataset for active learning algorithm evaluation, highlighting MFFT's practical value in BioSED tasks. <div>
arXiv:2505.20956v2 Announce Type: replace-cross 
Abstract: Bioacoustic sound event detection (BioSED) is crucial for biodiversity conservation but faces practical challenges during model development and training: limited amounts of annotated data, sparse events, species diversity, and class imbalance. To address these challenges efficiently with a limited labeling budget, we apply the mismatch-first farthest-traversal (MFFT), an active learning method integrating committee voting disagreement and diversity analysis. We also refine an existing BioSED dataset specifically for evaluating active learning algorithms. Experimental results demonstrate that MFFT achieves a mAP of 68% when cold-starting and 71% when warm-starting (which is close to the fully-supervised mAP of 75%) while using only 2.3% of the annotations. Notably, MFFT excels in cold-start scenarios and with rare species, which are critical for monitoring endangered species, demonstrating its practical value.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeSocial: Blockchain-based Decentralized Social Networks</title>
<link>https://arxiv.org/abs/2505.21388</link>
<guid>https://arxiv.org/abs/2505.21388</guid>
<content:encoded><![CDATA[
<div> Decentralized, social network, learning framework, blockchain, user-driven <br />
<br />
Summary: The research introduces DeSocial, a decentralized social network learning framework deployed on an Ethereum local development chain. It gives users the ability to choose algorithms for personalized social predictions in a blockchain environment. Users can evaluate multiple backbone models on their local subgraph, select the most suitable backbone for personalized prediction, and aggregate prediction results through majority voting from validation nodes. The framework shows significant improvement compared to centralized models, emphasizing user empowerment in blockchain-based decentralized social networks. Multi-node validation and personalized algorithm selection based on blockchain play a key role in enhancing social predictions in a decentralized setting. The implementation of the framework is available on GitHub for further exploration and development. <br /> <div>
arXiv:2505.21388v2 Announce Type: replace-cross 
Abstract: Web 2.0 social platforms are inherently centralized, with user data and algorithmic decisions controlled by the platform. However, users can only passively receive social predictions without being able to choose the underlying algorithm, which limits personalization. Fortunately, with the emergence of blockchain, users are allowed to choose algorithms that are tailored to their local situation, improving prediction results in a personalized way. In a blockchain environment, each user possesses its own model to perform the social prediction, capturing different perspectives on social interactions. In our work, we propose DeSocial, a decentralized social network learning framework deployed on an Ethereum (ETH) local development chain that integrates distributed data storage, node-level consensus, and user-driven model selection through Ganache. In the first stage, each user leverages DeSocial to evaluate multiple backbone models on their local subgraph. DeSocial coordinates the execution and returns model-wise prediction results, enabling the user to select the most suitable backbone for personalized social prediction. Then, DeSocial uniformly selects several validation nodes that possess the algorithm specified by each user, and aggregates the prediction results by majority voting, to prevent errors caused by any single model's misjudgment. Extensive experiments show that DeSocial has an evident improvement compared to the five classical centralized social network learning models, promoting user empowerment in blockchain-based decentralized social networks, showing the importance of multi-node validation and personalized algorithm selection based on blockchain. Our implementation is available at: https://github.com/agiresearch/DeSocial.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs</title>
<link>https://arxiv.org/abs/2505.22686</link>
<guid>https://arxiv.org/abs/2505.22686</guid>
<content:encoded><![CDATA[
<div> LSTM, GRU, BiLSTM, BiGRU, KAN, TKAN, SiLU, GeLU, MiSH, tropical Africa, weather forecasting, deep recurrent neural networks, temperature, precipitation, pressure<br />
<br />
Summary:<br />
- Study benchmarks deep neural networks for weather forecasting in tropical Africa.
- Models evaluated on temperature, precipitation, and pressure data from Abidjan and Kigali.
- KAN shows high accuracy in temperature prediction.
- TKAN variants improve precipitation forecasting in low-rainfall regions.
- RNNs outperform KAN models for atmospheric pressure prediction.
- Spline-based neural architectures offer efficient and data-efficient forecasting capabilities. <div>
arXiv:2505.22686v1 Announce Type: new 
Abstract: Weather forecasting is crucial for managing risks and economic planning, particularly in tropical Africa, where extreme events severely impact livelihoods. Yet, existing forecasting methods often struggle with the region's complex, non-linear weather patterns. This study benchmarks deep recurrent neural networks such as $\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and Kolmogorov-Arnold-based models $(\texttt{KAN} and \texttt{TKAN})$ for daily forecasting of temperature, precipitation, and pressure in two tropical cities: Abidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce two customized variants of $ \texttt{TKAN}$ that replace its original $\texttt{SiLU}$ activation function with $ \texttt{GeLU}$ and \texttt{MiSH}, respectively. Using station-level meteorological data spanning from 2010 to 2024, we evaluate all the models on standard regression metrics. $\texttt{KAN}$ achieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali, $\texttt{MSE} < 0.0014~^\circ C ^2$), while $\texttt{TKAN}$ variants minimize absolute errors for precipitation forecasting in low-rainfall regimes. The customized $\texttt{TKAN}$ models demonstrate improvements over the standard $\texttt{TKAN}$ across both datasets. Classical \texttt{RNNs} remain highly competitive for atmospheric pressure ($R^2 \approx 0.83{-}0.86$), outperforming $\texttt{KAN}$-based models in this task. These results highlight the potential of spline-based neural architectures for efficient and data-efficient forecasting.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SlimLLM: Accurate Structured Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2505.22689</link>
<guid>https://arxiv.org/abs/2505.22689</guid>
<content:encoded><![CDATA[
<div> structured pruning, large language models, SlimLLM, channel pruning, attention head pruning 

Summary: 
- Large language models (LLMs) are powerful but computationally expensive, limiting their deployment.
- Structured pruning is an effective way to compress LLM parameters, but determining importance and minimizing performance loss are key challenges.
- SlimLLM is a new structured pruning method for LLMs that evaluates importance based on entire channels or attention heads.
- The approach considers interdependence among elements within sub-modules, improving pruning effectiveness.
- A linear regression strategy for the output matrix enables quick performance recovery and a layer-based importance ratio determines the pruning ratio for each layer.
- SlimLLM outperforms other methods and achieves state-of-the-art performance on the LLaMA benchmark. 

<br /><br />Summary: <div>
arXiv:2505.22689v1 Announce Type: new 
Abstract: Large language models(LLMs) have garnered significant attention and demonstrated impressive capabilities in a wide range of applications. However, due to their enormous computational costs, the deployment and application of LLMs are often severely limited. To address this issue, structured pruning is an effective solution to compress the parameters of LLMs. Determining the importance of each sub-module in LLMs and minimizing performance loss are critical issues that need to be carefully addressed in structured pruning. In this paper, we propose an effective and fast structured pruning method named SlimLLM for large language models. For channel and attention head pruning, we evaluate the importance based on the entire channel or head, rather than merely aggregating the importance of individual elements within a sub-module. This approach enables a more holistic consideration of the interdependence among elements within the sub-module. In addition, we design a simple linear regression strategy for the output matrix to quickly recover performance. We also propose layer-based importance ratio to determine the pruning ratio for each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other methods and achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning</title>
<link>https://arxiv.org/abs/2505.22694</link>
<guid>https://arxiv.org/abs/2505.22694</guid>
<content:encoded><![CDATA[
<div> Parameter-Efficient Fine-Tuning, Large Language Models, Low-Rank Adaptation, Mixture of Low-Rank Experts, Multi-task scenarios
Summary:
In this study, the authors propose a novel approach, Mixture of Low-Rank Experts (MoRE), for multi-task Parameter-Efficient Fine-Tuning of Large Language Models (LLMs). MoRE aligns different ranks of Low-Rank Adaptation (LoRA) modules with different tasks, creating low-rank experts, and utilizes an adaptive rank selector to choose the appropriate expert for each task. By jointly training these experts, MoRE enhances the adaptability and efficiency of LoRA in multi-task scenarios. Extensive experiments on various multi-task benchmarks with different LLMs show that MoRE outperforms traditional LoRA and its variants in multi-task settings without increasing inference cost. The released model and code aim to benefit the research community by facilitating further exploration of efficient fine-tuning methods for LLMs.<br /><br />Summary: <div>
arXiv:2505.22694v1 Announce Type: new 
Abstract: With the rapid development of Large Language Models (LLMs), Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant attention, which aims to achieve efficient fine-tuning of LLMs with fewer parameters. As a representative PEFT method, Low-Rank Adaptation (LoRA) introduces low-rank matrices to approximate the incremental tuning parameters and achieves impressive performance over multiple scenarios. After that, plenty of improvements have been proposed for further improvement. However, these methods either focus on single-task scenarios or separately train multiple LoRA modules for multi-task scenarios, limiting the efficiency and effectiveness of LoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in this paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for multi-task PEFT. Specifically, instead of using an individual LoRA for each task, we align different ranks of LoRA module with different tasks, which we named low-rank experts. Moreover, we design a novel adaptive rank selector to select the appropriate expert for each task. By jointly training low-rank experts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task scenarios. Finally, we conduct extensive experiments over multiple multi-task benchmarks along with different LLMs to verify model performance. Experimental results demonstrate that compared to traditional LoRA and its variants, MoRE significantly improves the performance of LLMs in multi-task scenarios and incurs no additional inference cost. We also release the model and code to facilitate the community.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning</title>
<link>https://arxiv.org/abs/2505.22695</link>
<guid>https://arxiv.org/abs/2505.22695</guid>
<content:encoded><![CDATA[
<div> Keywords: ride-hailing, optimization, fairness, language models, intelligent transportation systems

Summary: 
The article introduces the LLM-ODDR framework, leveraging Large Language Models for optimizing order dispatching and driver repositioning in ride-hailing platforms. It addresses challenges in driver income fairness, interpretability, and adaptability to real-world dynamics. The framework includes components for Multi-objective-guided Order Value Refinement, Fairness-aware Order Dispatching, and Spatiotemporal Demand-Aware Driver Repositioning. The JointDR-GPT model is developed for ODDR tasks with domain knowledge. Experiments with real-world data from Manhattan taxi operations show significant performance improvements compared to traditional methods. This is the first exploration of using LLMs as decision-making agents in ride-hailing ODDR tasks, providing foundational insights for integrating advanced language models into intelligent transportation systems.<br /><br />Summary: <div>
arXiv:2505.22695v1 Announce Type: new 
Abstract: Ride-hailing platforms face significant challenges in optimizing order dispatching and driver repositioning operations in dynamic urban environments. Traditional approaches based on combinatorial optimization, rule-based heuristics, and reinforcement learning often overlook driver income fairness, interpretability, and adaptability to real-world dynamics. To address these gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models (LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in ride-hailing services. LLM-ODDR framework comprises three key components: (1) Multi-objective-guided Order Value Refinement, which evaluates orders by considering multiple objectives to determine their overall value; (2) Fairness-aware Order Dispatching, which balances platform revenue with driver income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning, which optimizes idle vehicle placement based on historical patterns and projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for ODDR tasks with domain knowledge. Extensive experiments on real-world datasets from Manhattan taxi operations demonstrate that our framework significantly outperforms traditional methods in terms of effectiveness, adaptability to anomalous conditions, and decision interpretability. To our knowledge, this is the first exploration of LLMs as decision-making agents in ride-hailing ODDR tasks, establishing foundational insights for integrating advanced language models within intelligent transportation systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Does Neuroevolution Outcompete Reinforcement Learning in Transfer Learning Tasks?</title>
<link>https://arxiv.org/abs/2505.22696</link>
<guid>https://arxiv.org/abs/2505.22696</guid>
<content:encoded><![CDATA[
<div> transfer learning, reinforcement learning, neuroevolution, curriculum, robotics

Summary:
Neuroevolution (NE) is investigated for its transfer learning capabilities, with two benchmarks introduced: stepping gates and ecorobot. Stepping gates task neural networks with emulating logic circuits, while ecorobot extends the Brax physics engine with obstacles and different robotic morphologies. A curriculum is present in both benchmarks to evaluate skill transfer across tasks of increasing complexity. The empirical analysis shows that NE methods vary in their transfer abilities and often outperform RL baselines. These findings support the potential of NE as a foundation for building adaptable agents and highlight future challenges for scaling NE to complex, real-world problems.
<br /><br />Summary: <div>
arXiv:2505.22696v1 Announce Type: new 
Abstract: The ability to continuously and efficiently transfer skills across tasks is a hallmark of biological intelligence and a long-standing goal in artificial systems. Reinforcement learning (RL), a dominant paradigm for learning in high-dimensional control tasks, is known to suffer from brittleness to task variations and catastrophic forgetting. Neuroevolution (NE) has recently gained attention for its robustness, scalability, and capacity to escape local optima. In this paper, we investigate an understudied dimension of NE: its transfer learning capabilities. To this end, we introduce two benchmarks: a) in stepping gates, neural networks are tasked with emulating logic circuits, with designs that emphasize modular repetition and variation b) ecorobot extends the Brax physics engine with objects such as walls and obstacles and the ability to easily switch between different robotic morphologies. Crucial in both benchmarks is the presence of a curriculum that enables evaluating skill transfer across tasks of increasing complexity. Our empirical analysis shows that NE methods vary in their transfer abilities and frequently outperform RL baselines. Our findings support the potential of NE as a foundation for building more adaptable agents and highlight future challenges for scaling NE to complex, real-world problems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Update Your Transformer to the Latest Release: Re-Basin of Task Vectors</title>
<link>https://arxiv.org/abs/2505.22697</link>
<guid>https://arxiv.org/abs/2505.22697</guid>
<content:encoded><![CDATA[
<div> transfer learning, fine-tuning, foundation models, data-free transfer, Transformer models 
Summary: 
- The study explores the possibility of transferring fine-tuning to a new version of a pretrained model without retraining.
- The authors propose a methodology based on weight permutations and model re-basin principles to re-base modifications made to the original base model.
- The approach is tailored for Transformer models, addressing challenges such as residual connections and multi-head attention layers.
- A two-level method rooted in spectral theory is introduced, involving attention heads permutation and parameter adjustment within selected pairs of heads.
- Extensive experiments on visual and textual tasks demonstrate successful transfer of fine-tuned knowledge to new pre-trained backbones without the need for additional training data. 
<br /><br />Summary: <div>
arXiv:2505.22697v1 Announce Type: new 
Abstract: Foundation models serve as the backbone for numerous specialized models developed through fine-tuning. However, when the underlying pretrained model is updated or retrained (e.g., on larger and more curated datasets), the fine-tuned model becomes obsolete, losing its utility and requiring retraining. This raises the question: is it possible to transfer fine-tuning to a new release of the model? In this work, we investigate how to transfer fine-tuning to a new checkpoint without having to re-train, in a data-free manner. To do so, we draw principles from model re-basin and provide a recipe based on weight permutations to re-base the modifications made to the original base model, often called task vector. In particular, our approach tailors model re-basin for Transformer models, taking into account the challenges of residual connections and multi-head attention layers. Specifically, we propose a two-level method rooted in spectral theory, initially permuting the attention heads and subsequently adjusting parameters within select pairs of heads. Through extensive experiments on visual and textual tasks, we achieve the seamless transfer of fine-tuned knowledge to new pre-trained backbones without relying on a single training step or datapoint. Code is available at https://github.com/aimagelab/TransFusion.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Private Rate-Constrained Optimization with Applications to Fair Learning</title>
<link>https://arxiv.org/abs/2505.22703</link>
<guid>https://arxiv.org/abs/2505.22703</guid>
<content:encoded><![CDATA[
<div> Optimization, Trustworthy ML, Fairness Constraints, Differential Privacy, Algorithm <br />
Summary: Many trustworthy ML problems involve minimizing model error while meeting prediction rate constraints on selected marginals, such as group fairness constraints. This study explores constrained minimization under differential privacy (DP). The RaCO-DP algorithm, a DP adaptation of Stochastic Gradient Descent-Ascent (SGDA), tackles rate constraint problems by solving their Lagrangian formulation. The algorithm's privacy cost is equivalent to privately estimating a histogram for the mini-batch at each optimization step. Convergence is proven through an innovative analysis leveraging the dual parameter's linear structure. Empirical tests on learning under group fairness constraints reveal that RaCO-DP outperforms existing private learning methods in fairness-utility trade-offs. <br /><br /> <div>
arXiv:2505.22703v1 Announce Type: new 
Abstract: Many problems in trustworthy ML can be formulated as minimization of the model error under constraints on the prediction rates of the model for suitably-chosen marginals, including most group fairness constraints (demographic parity, equality of odds, etc.). In this work, we study such constrained minimization problems under differential privacy (DP). Standard DP optimization techniques like DP-SGD rely on the loss function's decomposability into per-sample contributions. However, rate constraints introduce inter-sample dependencies, violating the decomposability requirement. To address this, we develop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA) algorithm which solves the Lagrangian formulation of rate constraint problems. We demonstrate that the additional privacy cost of incorporating these constraints reduces to privately estimating a histogram over the mini-batch at each optimization step. We prove the convergence of our algorithm through a novel analysis of SGDA that leverages the linear structure of the dual parameter. Finally, empirical results on learning under group fairness constraints demonstrate that our method Pareto-dominates existing private learning approaches in fairness-utility trade-offs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference</title>
<link>https://arxiv.org/abs/2505.22758</link>
<guid>https://arxiv.org/abs/2505.22758</guid>
<content:encoded><![CDATA[
<div> kernel, large language models, single-batch inference, FlashFormer, transformer-based 

Summary:
FlashFormer is a new kernel designed to accelerate single-batch inference for transformer-based large language models. The kernel aims to optimize performance for low-batch inference scenarios, where memory bandwidth and kernel launch overheads are key factors. Compared to existing state-of-the-art inference kernels, FlashFormer demonstrates significant speedups across various model sizes and quantization settings. This specialized kernel is particularly useful for applications such as edge deployment and latency-sensitive tasks, where low-batch inference is crucial. The size and compute characteristics of modern language models have driven the need for tailored kernels, and FlashFormer addresses the specific challenges of single-batch inference, providing efficient solutions for improving performance and efficiency in this context. <div>
arXiv:2505.22758v1 Announce Type: new 
Abstract: The size and compute characteristics of modern large language models have led to an increased interest in developing specialized kernels tailored for training and inference. Existing kernels primarily optimize for compute utilization, targeting the large-batch training and inference settings. However, low-batch inference, where memory bandwidth and kernel launch overheads contribute are significant factors, remains important for many applications of interest such as in edge deployment and latency-sensitive applications. This paper describes FlashFormer, a proof-of-concept kernel for accelerating single-batch inference for transformer-based large language models. Across various model sizes and quantizations settings, we observe nontrivial speedups compared to existing state-of-the-art inference kernels.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-time augmentation improves efficiency in conformal prediction</title>
<link>https://arxiv.org/abs/2505.22764</link>
<guid>https://arxiv.org/abs/2505.22764</guid>
<content:encoded><![CDATA[
<div> test-time augmentation, conformal classifier, predicted classes, probabilistic guarantee, inductive biases

Summary: 
Test-time augmentation (TTA) is shown to reduce the size of predicted classes produced by conformal classifiers while providing probabilistic guarantees. The approach is flexible, computationally efficient, and requires no model retraining. It can be combined with any conformal score and reduces prediction set sizes by 10%-14% on average. An evaluation across three datasets, three models, two conformal scoring methods, different guarantee strengths, and distribution shifts demonstrates the effectiveness of TTA in enhancing the conformal pipeline. <div>
arXiv:2505.22764v1 Announce Type: new 
Abstract: A conformal classifier produces a set of predicted classes and provides a probabilistic guarantee that the set includes the true class. Unfortunately, it is often the case that conformal classifiers produce uninformatively large sets. In this work, we show that test-time augmentation (TTA)--a technique that introduces inductive biases during inference--reduces the size of the sets produced by conformal classifiers. Our approach is flexible, computationally efficient, and effective. It can be combined with any conformal score, requires no model retraining, and reduces prediction set sizes by 10%-14% on average. We conduct an evaluation of the approach spanning three datasets, three models, two established conformal scoring methods, different guarantee strengths, and several distribution shifts to show when and why test-time augmentation is a useful addition to the conformal pipeline.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multivariate de Bruijn Graphs: A Symbolic Graph Framework for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.22768</link>
<guid>https://arxiv.org/abs/2505.22768</guid>
<content:encoded><![CDATA[
<div> Keywords: Time series forecasting, DRAGON encoder, Multivariate de Bruijn Graphs, symbolic representation, neural modeling

Summary: 
Time series forecasting is a challenging task due to temporal heterogeneity and high dimensionality. In this work, the DRAGON encoder is introduced, utilizing Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between symbolic representations and neural modeling. DRAGON discretizes continuous input sequences and maps them onto a fixed graph structure, allowing dynamic context recovery through graph-based attention. Integrated into a dual-branch architecture, DRAGON enhances conventional CNN-based encoders with symbolic, structure-aware representations. The code developed for this study is available on GitHub <br /><br />Summary: <div>
arXiv:2505.22768v1 Announce Type: new 
Abstract: Time series forecasting remains a challenging task for foundation models due to temporal heterogeneity, high dimensionality, and the lack of inherent symbolic structure. In this work, we propose DRAGON (Discrete Representation and Augmented Graph encoding Over deBruijN Graphs), a novel encoder that introduces Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between symbolic representations and neural modeling. DRAGON discretizes continuous input sequences and maps them onto a fixed graph structure, enabling dynamic context recovery via graph-based attention. Integrated as an auxiliary module within a dual-branch architecture, DRAGON augments conventional CNN-based encoders with symbolic, structure-aware representations. All code developed for this study is available at: https://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrated Value-Aware Model Learning with Stochastic Environment Models</title>
<link>https://arxiv.org/abs/2505.22772</link>
<guid>https://arxiv.org/abs/2505.22772</guid>
<content:encoded><![CDATA[
<div> value-aware, model learning, MuZero loss, surrogate losses, calibrated

Summary:
- The paper discusses value-aware model learning and the popular MuZero loss used in model-based reinforcement learning.
- It highlights that the MuZero loss, along with similar losses, are uncalibrated surrogate losses, leading to incorrect model and value function recovery.
- The authors propose corrections to address this issue and improve model and value function predictions.
- The study explores the relationship between loss calibration, latent model architectures, and auxiliary losses in training MuZero-style agents.
- The findings suggest that while deterministic models can predict accurate values, learning calibrated stochastic models remains beneficial. 

<br /><br />Summary: <div>
arXiv:2505.22772v1 Announce Type: new 
Abstract: The idea of value-aware model learning, that models should produce accurate value estimates, has gained prominence in model-based reinforcement learning. The MuZero loss, which penalizes a model's value function prediction compared to the ground-truth value function, has been utilized in several prominent empirical works in the literature. However, theoretical investigation into its strengths and weaknesses is limited. In this paper, we analyze the family of value-aware model learning losses, which includes the popular MuZero loss. We show that these losses, as normally used, are uncalibrated surrogate losses, which means that they do not always recover the correct model and value function. Building on this insight, we propose corrections to solve this issue. Furthermore, we investigate the interplay between the loss calibration, latent model architectures, and auxiliary losses that are commonly employed when training MuZero-style agents. We show that while deterministic models can be sufficient to predict accurate values, learning calibrated stochastic models is still advantageous.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Models Have a Supply Chain Problem</title>
<link>https://arxiv.org/abs/2505.22778</link>
<guid>https://arxiv.org/abs/2505.22778</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, open ecosystem, supply-chain risks, Sigstore, transparency

Summary: 
Machine Learning models are widely available online, offering exciting opportunities for non-experts. However, the open ecosystem also poses risks, such as the potential for malicious attacks or vulnerabilities in the models. Sigstore, a solution for open-source software supply chains, could be used to enhance transparency in open ML models. By enabling model publishers to sign their models and verify properties of the datasets used, Sigstore could mitigate the risks associated with the current ecosystem. This approach would help prevent attackers from substituting models with harmful content, ensure models are trained on secure frameworks, and validate the integrity of the data used. Through transparency and verification mechanisms, Sigstore could establish trust and enhance the security of open ML models. 

<br /><br />Summary: <div>
arXiv:2505.22778v1 Announce Type: new 
Abstract: Powerful machine learning (ML) models are now readily available online, which creates exciting possibilities for users who lack the deep technical expertise or substantial computing resources needed to develop them. On the other hand, this type of open ecosystem comes with many risks. In this paper, we argue that the current ecosystem for open ML models contains significant supply-chain risks, some of which have been exploited already in real attacks. These include an attacker replacing a model with something malicious (e.g., malware), or a model being trained using a vulnerable version of a framework or on restricted or poisoned data. We then explore how Sigstore, a solution designed to bring transparency to open-source software supply chains, can be used to bring transparency to open ML models, in terms of enabling model publishers to sign their models and prove properties about the datasets they use.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the Latent Space Dynamics of Neural Models</title>
<link>https://arxiv.org/abs/2505.22785</link>
<guid>https://arxiv.org/abs/2505.22785</guid>
<content:encoded><![CDATA[
<div> latent space, autoencoder models, neural networks, dynamical systems, vector field

Summary:
Neural networks can be viewed as dynamical systems that operate on a latent manifold, creating a vector field through the encoding-decoding map of autoencoder models. Training neural models introduces biases leading to attractor points in this vector field. This representation allows for analyzing generalization and memorization during training, extracting prior knowledge from network parameters encoded in attractors, and identifying out-of-distribution samples based on their trajectories. The approach is validated on vision foundation models, demonstrating its practicality. <div>
arXiv:2505.22785v1 Announce Type: new 
Abstract: Neural networks transform high-dimensional data into compact, structured representations, often modeled as elements of a lower dimensional latent space. In this paper, we present an alternative interpretation of neural models as dynamical systems acting on the latent manifold. Specifically, we show that autoencoder models implicitly define a latent vector field on the manifold, derived by iteratively applying the encoding-decoding map, without any additional training. We observe that standard training procedures introduce inductive biases that lead to the emergence of attractor points within this vector field. Drawing on this insight, we propose to leverage the vector field as a representation for the network, providing a novel tool to analyze the properties of the model and the data. This representation enables to: (i) analyze the generalization and memorization regimes of neural models, even throughout training; (ii) extract prior knowledge encoded in the network's parameters from the attractors, without requiring any input data; (iii) identify out-of-distribution samples from their trajectories in the vector field. We further validate our approach on vision foundation models, showcasing the applicability and effectiveness of our method in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Preimage Approximation for Neural Network Certification</title>
<link>https://arxiv.org/abs/2505.22798</link>
<guid>https://arxiv.org/abs/2505.22798</guid>
<content:encoded><![CDATA[
<div> certification, neural networks, preimage approximation, scalability, algorithmic improvements <br />
Summary: <br />
The article focuses on the need for effective neural network certification in safety- and security-critical applications, specifically against "patch attacks" that obscure parts of images. It introduces improvements to the PREMAP algorithm for preimage approximation, including tighter bounds, adaptive Monte Carlo sampling, and enhanced branching heuristics. These enhancements result in significant efficiency gains, making the method applicable to convolutional neural networks that were previously impractical. Through experiments on reinforcement learning control benchmarks, the study showcases the scalability and potential of preimage approximation for reliability and robustness certification in neural networks. <div>
arXiv:2505.22798v1 Announce Type: new 
Abstract: The growing reliance on artificial intelligence in safety- and security-critical applications demands effective neural network certification. A challenging real-world use case is certification against ``patch attacks'', where adversarial patches or lighting conditions obscure parts of images, for example traffic signs. One approach to certification, which also gives quantitative coverage estimates, utilizes preimages of neural networks, i.e., the set of inputs that lead to a specified output. However, these preimage approximation methods, including the state-of-the-art PREMAP algorithm, struggle with scalability. This paper presents novel algorithmic improvements to PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved branching heuristics. We demonstrate efficiency improvements of at least an order of magnitude on reinforcement learning control benchmarks, and show that our method scales to convolutional neural networks that were previously infeasible. Our results demonstrate the potential of preimage approximation methodology for reliability and robustness certification.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLUE: Neural Networks Calibration via Learning Uncertainty-Error alignment</title>
<link>https://arxiv.org/abs/2505.22803</link>
<guid>https://arxiv.org/abs/2505.22803</guid>
<content:encoded><![CDATA[
<div> Alignment, Uncertainty, Neural Networks, Calibration, CLUE

Summary:
CLUE (Calibration via Learning Uncertainty-Error Alignment) introduces a novel approach for reliable uncertainty estimation in neural networks. It focuses on aligning predicted uncertainty with observed error during training, leading to well-calibrated models with accurate uncertainty estimates. The method optimizes predictive performance and calibration simultaneously using summary statistics of uncertainty and loss. CLUE is fully differentiable, domain-agnostic, and seamlessly integrates into standard training pipelines. Extensive experiments across various tasks show that CLUE achieves superior calibration quality and competitive predictive performance compared to existing techniques, even in out-of-distribution and domain-shift scenarios. The approach does not incur significant computational overhead, making it practical for real-world applications. <div>
arXiv:2505.22803v1 Announce Type: new 
Abstract: Reliable uncertainty estimation is critical for deploying neural networks (NNs) in real-world applications. While existing calibration techniques often rely on post-hoc adjustments or coarse-grained binning methods, they remain limited in scalability, differentiability, and generalization across domains. In this work, we introduce CLUE (Calibration via Learning Uncertainty-Error Alignment), a novel approach that explicitly aligns predicted uncertainty with observed error during training, grounded in the principle that well-calibrated models should produce uncertainty estimates that match their empirical loss. CLUE adopts a novel loss function that jointly optimizes predictive performance and calibration, using summary statistics of uncertainty and loss as proxies. The proposed method is fully differentiable, domain-agnostic, and compatible with standard training pipelines. Through extensive experiments on vision, regression, and language modeling tasks, including out-of-distribution and domain-shift scenarios, we demonstrate that CLUE achieves superior calibration quality and competitive predictive performance with respect to state-of-the-art approaches without imposing significant computational overhead.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Factor: Quality Is a Dataset-Intrinsic Property</title>
<link>https://arxiv.org/abs/2505.22813</link>
<guid>https://arxiv.org/abs/2505.22813</guid>
<content:encoded><![CDATA[
<div> dataset quality, machine learning classifiers, model architecture, dataset size, class balance

Summary:
Quality of datasets is an intrinsic factor that significantly influences the performance of machine learning classifiers, independent of model architecture, dataset size, and class balance. Through experimentation with thousands of controlled datasets and various classifier architectures, a strong correlation was found between classifier performance and dataset quality. This correlation suggests that dataset quality is a fundamental property linked to the quality of the constituent classes within the datasets. The study highlights dataset quality as a separate target for optimizing machine-learning-based classification, alongside size, class balance, and model architecture. This research provides valuable insights for enhancing the performance of machine learning classifiers by considering and improving dataset quality as a key factor. <div>
arXiv:2505.22813v1 Announce Type: new 
Abstract: In the universal quest to optimize machine-learning classifiers, three factors -- model architecture, dataset size, and class balance -- have been shown to influence test-time performance but do not fully account for it. Previously, evidence was presented for an additional factor that can be referred to as dataset quality, but it was unclear whether this was actually a joint property of the dataset and the model architecture, or an intrinsic property of the dataset itself. If quality is truly dataset-intrinsic and independent of model architecture, dataset size, and class balance, then the same datasets should perform better (or worse) regardless of these other factors. To test this hypothesis, here we create thousands of datasets, each controlled for size and class balance, and use them to train classifiers with a wide range of architectures, from random forests and support-vector machines to deep networks. We find that classifier performance correlates strongly by subset across architectures ($R^2=0.79$), supporting quality as an intrinsic property of datasets independent of dataset size and class balance and of model architecture. Digging deeper, we find that dataset quality appears to be an emergent property of something more fundamental: the quality of datasets' constituent classes. Thus, quality joins size, class balance, and model architecture as an independent correlate of performance and a separate target for optimizing machine-learning-based classification.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference Learning with Response Time</title>
<link>https://arxiv.org/abs/2505.22820</link>
<guid>https://arxiv.org/abs/2505.22820</guid>
<content:encoded><![CDATA[
<div> response time data, human preference learning, reward model elicitation, Evidence Accumulation Drift Diffusion model, Neyman-orthogonal loss functions

Summary:
This paper introduces a method to incorporate response time data into human preference learning frameworks, enhancing reward model elicitation. By integrating the Evidence Accumulation Drift Diffusion model, the study utilizes response time information to capture preference strength. The proposed Neyman-orthogonal loss functions ensure optimal convergence rates for reward model learning compared to traditional preference learning approaches. The theoretical analysis reveals exponential error scaling in conventional methods for linear reward functions, which is significantly reduced to polynomial scaling through the response time-augmented approach. The study extends its findings to non-parametric reward function spaces, demonstrating convergence properties for more complex reward models. Extensive experiments in image preference learning validate the theoretical conclusions. <div>
arXiv:2505.22820v1 Announce Type: new 
Abstract: This paper investigates the integration of response time data into human preference learning frameworks for more effective reward model elicitation. While binary preference data has become fundamental in fine-tuning foundation models, generative AI systems, and other large-scale models, the valuable temporal information inherent in user decision-making remains largely unexploited. We propose novel methodologies to incorporate response time information alongside binary choice data, leveraging the Evidence Accumulation Drift Diffusion (EZ) model, under which response time is informative of the preference strength. We develop Neyman-orthogonal loss functions that achieve oracle convergence rates for reward model learning, matching the theoretical optimal rates that would be attained if the expected response times for each query were known a priori. Our theoretical analysis demonstrates that for linear reward functions, conventional preference learning suffers from error rates that scale exponentially with reward magnitude. In contrast, our response time-augmented approach reduces this to polynomial scaling, representing a significant improvement in sample efficiency. We extend these guarantees to non-parametric reward function spaces, establishing convergence properties for more complex, realistic reward models. Our extensive experiments validate our theoretical findings in the context of preference learning over images.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PGLearn -- An Open-Source Learning Toolkit for Optimal Power Flow</title>
<link>https://arxiv.org/abs/2505.22825</link>
<guid>https://arxiv.org/abs/2505.22825</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Optimal Power Flow, Datasets, Evaluation Metrics, PGLearn <br />
Summary: <br />
Machine Learning techniques are increasingly being used to address Optimal Power Flow (OPF) problems in energy grids. The lack of standardized datasets and evaluation metrics has hindered progress in this field. To tackle this challenge, PGLearn introduces a comprehensive suite of standardized datasets and evaluation tools for ML and OPF. These datasets capture real-life operating conditions and include time series data for large-scale systems. PGLearn supports multiple OPF formulations and provides a toolkit for training and benchmarking ML models. By making datasets publicly available and promoting standardized evaluation metrics, PGLearn aims to democratize research in machine learning applications for OPF, reducing the burden of data generation and enabling fair comparisons between methodologies. Demanding open, standardized datasets and evaluation metrics, PGLearn accelerates research and innovation in ML for OPF. <div>
arXiv:2505.22825v1 Announce Type: new 
Abstract: Machine Learning (ML) techniques for Optimal Power Flow (OPF) problems have recently garnered significant attention, reflecting a broader trend of leveraging ML to approximate and/or accelerate the resolution of complex optimization problems. These developments are necessitated by the increased volatility and scale in energy production for modern and future grids. However, progress in ML for OPF is hindered by the lack of standardized datasets and evaluation metrics, from generating and solving OPF instances, to training and benchmarking machine learning models. To address this challenge, this paper introduces PGLearn, a comprehensive suite of standardized datasets and evaluation tools for ML and OPF. PGLearn provides datasets that are representative of real-life operating conditions, by explicitly capturing both global and local variability in the data generation, and by, for the first time, including time series data for several large-scale systems. In addition, it supports multiple OPF formulations, including AC, DC, and second-order cone formulations. Standardized datasets are made publicly available to democratize access to this field, reduce the burden of data generation, and enable the fair comparison of various methodologies. PGLearn also includes a robust toolkit for training, evaluating, and benchmarking machine learning models for OPF, with the goal of standardizing performance evaluation across the field. By promoting open, standardized datasets and evaluation metrics, PGLearn aims at democratizing and accelerating research and innovation in machine learning applications for optimal power flow problems. Datasets are available for download at https://www.huggingface.co/PGLearn.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies</title>
<link>https://arxiv.org/abs/2505.22829</link>
<guid>https://arxiv.org/abs/2505.22829</guid>
<content:encoded><![CDATA[
<div> Keywords: distribution shift, AI safety, conceptual synergies, methodological synergies, integration <br />
Summary: This paper explores the intersection of distribution shift and AI safety by analyzing their conceptual and methodological connections. The authors identify two types of relationships between specific causes of distribution shift and fine-grained AI safety issues: methods targeting a particular shift type can also address corresponding safety goals, and some shifts and safety issues can be formally converted into each other, allowing for shared methodology. By highlighting these synergies, the paper advocates for a more integrated approach to distribution shift and AI safety research. This unified perspective could lead to a deeper understanding of how addressing distribution shift challenges can contribute to achieving AI safety objectives, and vice versa. The findings emphasize the importance of fundamental integration between these two areas of research for advancing the development of safe and robust AI systems. <br /><br />Summary: <div>
arXiv:2505.22829v1 Announce Type: new 
Abstract: This paper bridges distribution shift and AI safety through a comprehensive analysis of their conceptual and methodological synergies. While prior discussions often focus on narrow cases or informal analogies, we establish two types connections between specific causes of distribution shift and fine-grained AI safety issues: (1) methods addressing a specific shift type can help achieve corresponding safety goals, or (2) certain shifts and safety issues can be formally reduced to each other, enabling mutual adaptation of their methods. Our findings provide a unified perspective that encourages fundamental integration between distribution shift and AI safety research.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Diffusion Models Improve Adversarial Robustness?</title>
<link>https://arxiv.org/abs/2505.22839</link>
<guid>https://arxiv.org/abs/2505.22839</guid>
<content:encoded><![CDATA[
<div> randomness, compression, diffusion models, adversarial robustness, purification

Summary:
Diffusion models have been found to enhance adversarial robustness empirically, but the precise mechanisms behind this improvement have been unclear. Contrary to intuition, the models actually increase the distance to clean samples rather than decreasing it. The purified images are heavily influenced by the internal randomness of the diffusion models, leading to a compression effect within each randomness configuration. When evaluating robustness under fixed randomness, the improvement drops to 24% on CIFAR-10, much lower than previous reports. The remaining robustness gain is strongly correlated with the model's ability to compress the input space, making the compression rate a reliable indicator of robustness without requiring gradient-based analysis. These findings shed light on how diffusion-based purification works and provide insights for developing more effective adversarial purification systems.<br /><br />Summary: <div>
arXiv:2505.22839v1 Announce Type: new 
Abstract: Recent findings suggest that diffusion models significantly enhance empirical adversarial robustness. While some intuitive explanations have been proposed, the precise mechanisms underlying these improvements remain unclear. In this work, we systematically investigate how and how well diffusion models improve adversarial robustness. First, we observe that diffusion models intriguingly increase, rather than decrease, the $\ell_p$ distance to clean samples--challenging the intuition that purification denoises inputs closer to the original data. Second, we find that the purified images are heavily influenced by the internal randomness of diffusion models, where a compression effect arises within each randomness configuration. Motivated by this observation, we evaluate robustness under fixed randomness and find that the improvement drops to approximately 24% on CIFAR-10--substantially lower than prior reports approaching 70%. Importantly, we show that this remaining robustness gain strongly correlates with the model's ability to compress the input space, revealing the compression rate as a reliable robustness indicator without requiring gradient-based analysis. Our findings provide novel insights into the mechanisms underlying diffusion-based purification, and offer guidance for developing more effective and principled adversarial purification systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development and Validation of SXI++ LNM Algorithm for Sepsis Prediction</title>
<link>https://arxiv.org/abs/2505.22840</link>
<guid>https://arxiv.org/abs/2505.22840</guid>
<content:encoded><![CDATA[
<div> Keywords: sepsis prediction, machine learning, deep neural network, accuracy, precision 

Summary: 
The study introduces the SXI++ LNM, a machine learning scoring system designed to enhance sepsis prediction by incorporating multiple algorithms and deep neural networks. The model was trained and evaluated using different dataset distributions to ensure robustness in clinical applications. Results show that the SXI++ LNM outperformed existing methods in three use cases, achieving an impressive AUC of 0.99 with high precision and accuracy. The model demonstrated a precision of 99.9% and an accuracy of 99.99%, showcasing its reliability in predicting sepsis. The utilization of a deep neural network has proved to be effective in enhancing predictive performance, with the model showcasing exceptional accuracy and precision. The findings suggest that the SXI++ LNM could be a valuable tool in improving sepsis prediction and diagnosis, showcasing its potential in clinical settings. 

<br /><br />Summary: <div>
arXiv:2505.22840v1 Announce Type: new 
Abstract: Sepsis is a life-threatening condition affecting over 48.9 million people globally and causing 11 million deaths annually. Despite medical advancements, predicting sepsis remains a challenge due to non-specific symptoms and complex pathophysiology. The SXI++ LNM is a machine learning scoring system that refines sepsis prediction by leveraging multiple algorithms and deep neural networks. This study aims to improve robustness in clinical applications and evaluates the predictive performance of the SXI++ LNM for sepsis prediction. The model, utilizing a deep neural network, was trained and tested using multiple scenarios with different dataset distributions. The model's performance was assessed against unseen test data, and accuracy, precision, and area under the curve (AUC) were calculated. THE SXI++ LNM outperformed the state of the art in three use cases, achieving an AUC of 0.99 (95% CI: 0.98-1.00). The model demonstrated a precision of 99.9% (95% CI: 99.8-100.0) and an accuracy of 99.99% (95% CI: 99.98-100.0), maintaining high reliability.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel-Smoothed Scores for Denoising Diffusion: A Bias-Variance Study</title>
<link>https://arxiv.org/abs/2505.22841</link>
<guid>https://arxiv.org/abs/2505.22841</guid>
<content:encoded><![CDATA[
<div> diffusion models, generative sampling, regularization, memorization, variance control  
Summary:  
Diffusion models are now the standard in high-fidelity generative sampling but can be prone to memorization. The empirical score can overfit the dataset, leading to reverse-time SDE samples being mostly training points. Introducing a kernel-smoothed empirical score can help reduce noise variance and improve generalization by balancing bias and variance. Regularization on the score or increasing the size of the training dataset can prevent memorization. The spectral decomposition of the forward diffusion suggests better variance control under certain conditions. Reverse diffusion with a kernel-smoothed empirical score can be seen as a gradient descent towards a Log-Exponential Double-Kernel Density Estimator, highlighting the regularization mechanisms in denoising diffusions. Even simple regularization without learning can mitigate memorization and enhance generalization. Experimental results on synthetic and MNIST datasets support these findings. <div>
arXiv:2505.22841v1 Announce Type: new 
Abstract: Diffusion models now set the benchmark in high-fidelity generative sampling, yet they can, in principle, be prone to memorization. In this case, their learned score overfits the finite dataset so that the reverse-time SDE samples are mostly training points. In this paper, we interpret the empirical score as a noisy version of the true score and show that its covariance matrix is asymptotically a re-weighted data PCA. In large dimension, the small time limit makes the noise variance blow up while simultaneously reducing spatial correlation. To reduce this variance, we introduce a kernel-smoothed empirical score and analyze its bias-variance trade-off. We derive asymptotic bounds on the Kullback-Leibler divergence between the true distribution and the one generated by the modified reverse SDE. Regularization on the score has the same effect as increasing the size of the training dataset, and thus helps prevent memorization. A spectral decomposition of the forward diffusion suggests better variance control under some regularity conditions of the true data distribution. Reverse diffusion with kernel-smoothed empirical score can be reformulated as a gradient descent drifted toward a Log-Exponential Double-Kernel Density Estimator (LED-KDE). This perspective highlights two regularization mechanisms taking place in denoising diffusions: an initial Gaussian kernel first diffuses mass isotropically in the ambient space, while a second kernel applied in score space concentrates and spreads that mass along the data manifold. Hence, even a straightforward regularization-without any learning-already mitigates memorization and enhances generalization. Numerically, we illustrate our results with several experiments on synthetic and MNIST datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation</title>
<link>https://arxiv.org/abs/2505.22846</link>
<guid>https://arxiv.org/abs/2505.22846</guid>
<content:encoded><![CDATA[
<div> interactive theorem proving, generative artificial intelligence, Rocq generation, premise selection, self-attentive embedder model 

Summary: 
The paper evaluates different approaches to Rocq proof generation and suggests improvements, emphasizing the importance of premise selection. A novel approach utilizing a self-attentive embedder model shows promising results with a 28% performance increase. A multi-stage agentic system designed for formal verification effectively addresses the challenge of writing Rocq proofs. An ablation study is conducted, highlighting the use of multi-agent debate in proof synthesis planning. The research showcases the potential of combining interactive theorem proving with generative artificial intelligence for Rocq proof generation, offering insights for enhancing performance and efficiency.<br /><br />Summary: <div>
arXiv:2505.22846v1 Announce Type: new 
Abstract: Interactive Theorem Proving was repeatedly shown to be fruitful combined with Generative Artificial Intelligence. This paper assesses multiple approaches to Rocq generation and illuminates potential avenues for improvement. We highlight the importance of thorough premise selection for generating Rocq proofs and propose a novel approach, leveraging retrieval via a self-attentive embedder model. The evaluation of the designed approach shows up to 28% relative increase of the generator's performance. We tackle the problem of writing Rocq proofs using a multi-stage agentic system, tailored for formal verification, and demonstrate its high effectiveness. We conduct an ablation study and show the use of multi-agent debate on the planning stage of proof synthesis.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel</title>
<link>https://arxiv.org/abs/2505.22861</link>
<guid>https://arxiv.org/abs/2505.22861</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, causal reasoning, physics-informed kernel, exploration, virtual tools
Summary:
Causal-PIK is proposed for tasks involving complex interactions with unknown dynamics, leveraging Bayesian optimization and a Physics-Informed Kernel for efficient search and improved actions. Experimental results on Virtual Tools and PHYRE benchmarks show Causal-PIK outperforms state-of-the-art methods, requiring fewer actions. Comparison to human studies, including a new user study on PHYRE, reveals Causal-PIK's competitiveness on challenging tasks even for human problem solvers. This method enhances causal reasoning in planning processes, offering a promising approach for tasks with intricate causal interactions. <div>
arXiv:2505.22861v1 Announce Type: new 
Abstract: Tasks that involve complex interactions between objects with unknown dynamics make planning before execution difficult. These tasks require agents to iteratively improve their actions after actively exploring causes and effects in the environment. For these type of tasks, we propose Causal-PIK, a method that leverages Bayesian optimization to reason about causal interactions via a Physics-Informed Kernel to help guide efficient search for the best next action. Experimental results on Virtual Tools and PHYRE physical reasoning benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring fewer actions to reach the goal. We also compare Causal-PIK to human studies, including results from a new user study we conducted on the PHYRE benchmark. We find that Causal-PIK remains competitive on tasks that are very challenging, even for human problem-solvers.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Offline RL via Efficient and Expressive Shortcut Models</title>
<link>https://arxiv.org/abs/2505.22866</link>
<guid>https://arxiv.org/abs/2505.22866</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, flow models, offline reinforcement learning, shortcut models, scalable

Summary: 
Offline reinforcement learning faces challenges in applying diffusion and flow models due to their iterative noise sampling processes. The paper introduces Scalable Offline Reinforcement Learning (SORL), which utilizes shortcut models to scale training and inference efficiently in a one-stage training procedure. SORL's policy can handle complex data distributions and achieves strong performance in various offline RL tasks. At test time, SORL utilizes the learned Q-function for sequential and parallel inference scaling. The algorithm shows positive scaling behavior with increased test-time compute. By leveraging shortcut models, SORL simplifies policy optimization and demonstrates promising results in offline reinforcement learning applications.<br /><br />Summary: <div>
arXiv:2505.22866v1 Announce Type: new 
Abstract: Diffusion and flow models have emerged as powerful generative approaches capable of modeling diverse and multimodal behavior. However, applying these models to offline reinforcement learning (RL) remains challenging due to the iterative nature of their noise sampling processes, making policy optimization difficult. In this paper, we introduce Scalable Offline Reinforcement Learning (SORL), a new offline RL algorithm that leverages shortcut models - a novel class of generative models - to scale both training and inference. SORL's policy can capture complex data distributions and can be trained simply and efficiently in a one-stage training procedure. At test time, SORL introduces both sequential and parallel inference scaling by using the learned Q-function as a verifier. We demonstrate that SORL achieves strong performance across a range of offline RL tasks and exhibits positive scaling behavior with increased test-time compute. We release the code at nico-espinosadice.github.io/projects/sorl.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints</title>
<link>https://arxiv.org/abs/2505.22881</link>
<guid>https://arxiv.org/abs/2505.22881</guid>
<content:encoded><![CDATA[
<div> uncertainty, constraints, machine learning, optimization, stochastic <br />
<br />
Summary: <br />
The study introduces an extension of contextual stochastic linear optimization (CSLO) with inequality constraints influenced by uncertain parameters predicted by a machine learning model. The contextual uncertainty sets are constructed using methods like conformal prediction to handle the uncertainty in constraints. A new loss function called "Smart Predict-then-Optimize with Robust Constraints" (SPO-RC) is introduced, which is feasibility-sensitive and measures the decision error of predicted objective parameters. A convex surrogate called SPO-RC+ is also introduced, and its Fisher consistency with SPO-RC is proven. Training on truncated datasets and correcting sample selection bias using importance reweighting techniques are proposed to enhance performance. Experimental results on fractional knapsack and alloy production problems demonstrate the effectiveness of SPO-RC+ in handling uncertain constraints and the improvement in performance by combining truncation with importance reweighting. <div>
arXiv:2505.22881v1 Announce Type: new 
Abstract: We study an extension of contextual stochastic linear optimization (CSLO) that, in contrast to most of the existing literature, involves inequality constraints that depend on uncertain parameters predicted by a machine learning model. To handle the constraint uncertainty, we use contextual uncertainty sets constructed via methods like conformal prediction. Given a contextual uncertainty set method, we introduce the "Smart Predict-then-Optimize with Robust Constraints" (SPO-RC) loss, a feasibility-sensitive adaptation of the SPO loss that measures decision error of predicted objective parameters. We also introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with SPO-RC. To enhance performance, we train on truncated datasets where true constraint parameters lie within the uncertainty sets, and we correct the induced sample selection bias using importance reweighting techniques. Through experiments on fractional knapsack and alloy production problem instances, we demonstrate that SPO-RC+ effectively handles uncertainty in constraints and that combining truncation with importance reweighting can further improve performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Dynamic Regret of Following the Regularized Leader: Optimism with History Pruning</title>
<link>https://arxiv.org/abs/2505.22899</link>
<guid>https://arxiv.org/abs/2505.22899</guid>
<content:encoded><![CDATA[
<div> keywords: Follow the Regularized Leader, Online Convex Optimization, dynamic regret guarantees, agile iterates, pruning <br />
Summary: <br />
This article revisits the Follow the Regularized Leader (FTRL) framework for Online Convex Optimization (OCO) over compact sets, focusing on achieving dynamic regret guarantees. It highlights that FTRL can produce "agile" iterates and recover known dynamic regret bounds through optimistic composition of future costs and linearization of past costs. By comparing FTRL against dynamic comparators, it provides a way to interpolate between greedy and agile updates, offering refined control over regret terms and optimism without cyclic dependence. The analysis shows that it is not the lazy projection style of FTRL hindering dynamic regret, but the decoupling of the algorithm's state from its iterates. Pruning is introduced to synchronize the state and iterates when necessary, allowing for minimal recursive regularization akin to AdaFTRL. <div>
arXiv:2505.22899v1 Announce Type: new 
Abstract: We revisit the Follow the Regularized Leader (FTRL) framework for Online Convex Optimization (OCO) over compact sets, focusing on achieving dynamic regret guarantees. Prior work has highlighted the framework's limitations in dynamic environments due to its tendency to produce "lazy" iterates. However, building on insights showing FTRL's ability to produce "agile" iterates, we show that it can indeed recover known dynamic regret bounds through optimistic composition of future costs and careful linearization of past costs, which can lead to pruning some of them. This new analysis of FTRL against dynamic comparators yields a principled way to interpolate between greedy and agile updates and offers several benefits, including refined control over regret terms, optimism without cyclic dependence, and the application of minimal recursive regularization akin to AdaFTRL. More broadly, we show that it is not the lazy projection style of FTRL that hinders (optimistic) dynamic regret, but the decoupling of the algorithm's state (linearized history) from its iterates, allowing the state to grow arbitrarily. Instead, pruning synchronizes these two when necessary.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defining Foundation Models for Computational Science: A Call for Clarity and Rigor</title>
<link>https://arxiv.org/abs/2505.22904</link>
<guid>https://arxiv.org/abs/2505.22904</guid>
<content:encoded><![CDATA[
<div> foundation models, computational science, generality, reusability, scalability

Summary:
In this paper, the authors address the lack of a universally accepted definition of foundation models in computational science. They propose a formal definition grounded in generality, reusability, and scalability, essential for these models to exhibit. Drawing parallels with traditional methods like finite element and finite volume, they introduce the Data-Driven Finite Element Method (DD-FEM). This framework combines the modular structure of classical FEM with the representation power of data-driven learning. DD-FEM effectively addresses key challenges such as scalability, adaptability, and physics consistency in foundation models for computational science. By integrating traditional numerical methods with modern AI paradigms, this work establishes a strong foundation for evaluating and developing innovative approaches towards future foundation models in computational science.<br /><br />Summary: <div>
arXiv:2505.22904v1 Announce Type: new 
Abstract: The widespread success of foundation models in natural language processing and computer vision has inspired researchers to extend the concept to scientific machine learning and computational science. However, this position paper argues that as the term "foundation model" is an evolving concept, its application in computational science is increasingly used without a universally accepted definition, potentially creating confusion and diluting its precise scientific meaning. In this paper, we address this gap by proposing a formal definition of foundation models in computational science, grounded in the core values of generality, reusability, and scalability. We articulate a set of essential and desirable characteristics that such models must exhibit, drawing parallels with traditional foundational methods, like the finite element and finite volume methods. Furthermore, we introduce the Data-Driven Finite Element Method (DD-FEM), a framework that fuses the modular structure of classical FEM with the representational power of data-driven learning. We demonstrate how DD-FEM addresses many of the key challenges in realizing foundation models for computational science, including scalability, adaptability, and physics consistency. By bridging traditional numerical methods with modern AI paradigms, this work provides a rigorous foundation for evaluating and developing novel approaches toward future foundation models in computational science.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference</title>
<link>https://arxiv.org/abs/2505.22913</link>
<guid>https://arxiv.org/abs/2505.22913</guid>
<content:encoded><![CDATA[
<div> Keywords: unstructured sparsity, KV cache compression, pruning strategies, custom attention kernel, decode performance

Summary:
The study demonstrates that unstructured sparsity significantly enhances KV cache compression for Large Language Models (LLMs), achieving up to 70% sparsity without sacrificing accuracy or requiring fine-tuning. Per-token magnitude-based pruning proves highly effective for both Key and Value caches, surpassing previous structured pruning methods. The Key cache benefits from outlier elements, while the Value cache benefits from simple magnitude-based pruning. KV cache size is a bottleneck in decode performance due to high memory overhead, addressed by a bitmap-based sparse format and a custom attention kernel. This allows for compressing and computing over compressed caches pruned to arbitrary sparsity patterns, accelerating memory-bound operations in decode computations. The custom attention kernel and bitmap-based format enable significant compression of KV cache, leading to longer context lengths and increased tokens/sec throughput compared to dense inference. The pruning mechanism and sparse attention kernel are accessible at https://github.com/dhjoo98/mustafar<br /><br />Summary: The study highlights the efficacy of unstructured sparsity in improving KV cache compression for LLMs, utilizing per-token magnitude-based pruning for Key and Value caches. A custom attention kernel and bitmap-based sparse format address the issue of KV cache size as a bottleneck in decode performance, enabling significant acceleration of memory-bound operations and increased throughput compared to dense inference. <div>
arXiv:2505.22913v1 Announce Type: new 
Abstract: We demonstrate that unstructured sparsity significantly improves KV cache compression for LLMs, enabling sparsity levels up to 70% without compromising accuracy or requiring fine-tuning. We conduct a systematic exploration of pruning strategies and find per-token magnitude-based pruning as highly effective for both Key and Value caches under unstructured sparsity, surpassing prior structured pruning schemes. The Key cache benefits from prominent outlier elements, while the Value cache surprisingly benefits from a simple magnitude-based pruning despite its uniform distribution. KV cache size is the major bottleneck in decode performance due to high memory overhead for large context lengths. To address this, we use a bitmap-based sparse format and a custom attention kernel capable of compressing and directly computing over compressed caches pruned to arbitrary sparsity patterns, significantly accelerating memory-bound operations in decode computations and thereby compensating for the overhead of runtime pruning and compression. Our custom attention kernel coupled with the bitmap-based format delivers substantial compression of KV cache upto 45% of dense inference and thereby enables longer context length and increased tokens/sec throughput of upto 2.23x compared to dense inference. Our pruning mechanism and sparse attention kernel is available at https://github.com/dhjoo98/mustafar.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking</title>
<link>https://arxiv.org/abs/2505.22922</link>
<guid>https://arxiv.org/abs/2505.22922</guid>
<content:encoded><![CDATA[
<div> Efficient Pre-training, Large Language Models, Memory Efficiency, Benchmark Evaluation, Performance Improvement
<br />
Summary: 
This article explores methods for making large language models more memory and parameter efficient during pre-training. It discusses the challenges posed by the increasing size of LLMs and the computational resources required for training and fine-tuning. The study surveys state-of-the-art approaches for efficient pre-training and evaluates several memory efficient methods to determine their performance across various model sizes. The researchers find that full-rank training performs best with the right optimizer and hyperparameters, and that incorporating high-rank updates in low-rank approaches can enhance their performance. Additionally, they propose two practical techniques, weight refactorization and momentum reset, to improve efficient pre-training methods. Applying these techniques to a low-rank model results in lower perplexity than popular memory efficient algorithms while using less memory. <div>
arXiv:2505.22922v1 Announce Type: new 
Abstract: Fueled by their remarkable ability to tackle diverse tasks across multiple domains, large language models (LLMs) have grown at an unprecedented rate, with some recent models containing trillions of parameters. This growth is accompanied by substantial computational challenges, particularly regarding the memory and compute resources required for training and fine-tuning. Numerous approaches have been explored to address these issues, such as LoRA. While these methods are effective for fine-tuning, their application to pre-training is significantly more challenging due to the need to learn vast datasets. Motivated by this issue, we aim to address the following questions: Can parameter- or memory-efficient methods enhance pre-training efficiency while achieving performance comparable to full-model training? How can the performance gap be narrowed? To this end, the contributions of this work are the following. (1) We begin by conducting a comprehensive survey that summarizes state-of-the-art methods for efficient pre-training. (2) We perform a benchmark evaluation of several representative memory efficient pre-training approaches to comprehensively evaluate their performance across model sizes. We observe that with a proper choice of optimizer and hyperparameters, full-rank training delivers the best performance, as expected. We also notice that incorporating high-rank updates in low-rank approaches is the key to improving their performance. (3) Finally, we propose two practical techniques, namely weight refactorization and momentum reset, to enhance the performance of efficient pre-training methods. We observe that applying these techniques to the low-rank method (on a 1B model) can achieve a lower perplexity than popular memory efficient algorithms such as GaLore and Fira, while simultaneously using about 25% less memory.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models</title>
<link>https://arxiv.org/abs/2505.22935</link>
<guid>https://arxiv.org/abs/2505.22935</guid>
<content:encoded><![CDATA[
<div> Bernoulli edge-flip corruptions, denoisers, Graph Diffusion Models, noise conditioning, high-dimensional graph data <br />
Summary: <br />
This study questions the necessity of explicit noise-level conditioning in Graph Diffusion Models (GDMs) by investigating whether denoisers can infer noise levels implicitly from corrupted graph structures. The research focuses on Bernoulli edge-flip corruptions and extends to more complex scenarios involving structure-attribute noise. Empirical evaluations on synthetic and real-world graph datasets, using models like GDSS and DiGress, show that unconditional GDMs can achieve comparable or better performance than conditioned models. Additionally, unconditional models require fewer parameters (4-6% reduction) and less computation time (8-10% decrease). The results suggest that the inherent high-dimensional nature of graph data may contain enough information for the denoising process, paving the way for simpler and more efficient GDM architectures. <div>
arXiv:2505.22935v1 Announce Type: new 
Abstract: Explicit noise-level conditioning is widely regarded as essential for the effective operation of Graph Diffusion Models (GDMs). In this work, we challenge this assumption by investigating whether denoisers can implicitly infer noise levels directly from corrupted graph structures, potentially eliminating the need for explicit noise conditioning. To this end, we develop a theoretical framework centered on Bernoulli edge-flip corruptions and extend it to encompass more complex scenarios involving coupled structure-attribute noise. Extensive empirical evaluations on both synthetic and real-world graph datasets, using models such as GDSS and DiGress, provide strong support for our theoretical findings. Notably, unconditional GDMs achieve performance comparable or superior to their conditioned counterparts, while also offering reductions in parameters (4-6%) and computation time (8-10%). Our results suggest that the high-dimensional nature of graph data itself often encodes sufficient information for the denoising process, opening avenues for simpler, more efficient GDM architectures.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed Graph Grammars for Sequence-based Learning</title>
<link>https://arxiv.org/abs/2505.22949</link>
<guid>https://arxiv.org/abs/2505.22949</guid>
<content:encoded><![CDATA[
<div> Keywords: Directed acyclic graphs, DAGs, encoder, decoder, grammar-based approach

Summary:
Directed acyclic graphs (DAGs) are commonly used in various applications such as electronic circuits and neural architectures. While there are effective encoders for DAGs, decoding them poses a challenge due to the multiple topological orders of nodes. This work introduces a grammar-based approach to create a sequential representation of a DAG in a principled and compact manner. The graph is seen as derivations over an unambiguous grammar, where the DAG corresponds to a unique sequence of production rules, enabling lossless compression of the data. This representation has diverse applications, including constructing generative models for graph generation, learning latent spaces for property prediction, and using sequence representational continuity for Bayesian Optimization over structured data. The code for this approach is available on GitHub for further exploration. 

<br /><br />Summary: Directed acyclic graphs are widely used but decoding them is challenging. A grammar-based approach is proposed to create a sequential representation of DAGs for various applications such as generative models and property prediction, enabling efficient data compression and Bayesian optimization. <div>
arXiv:2505.22949v1 Announce Type: new 
Abstract: Directed acyclic graphs (DAGs) are a class of graphs commonly used in practice, with examples that include electronic circuits, Bayesian networks, and neural architectures. While many effective encoders exist for DAGs, it remains challenging to decode them in a principled manner, because the nodes of a DAG can have many different topological orders. In this work, we propose a grammar-based approach to constructing a principled, compact and equivalent sequential representation of a DAG. Specifically, we view a graph as derivations over an unambiguous grammar, where the DAG corresponds to a unique sequence of production rules. Equivalently, the procedure to construct such a description can be viewed as a lossless compression of the data. Such a representation has many uses, including building a generative model for graph generation, learning a latent space for property prediction, and leveraging the sequence representational continuity for Bayesian Optimization over structured data. Code is available at https://github.com/shiningsunnyday/induction.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming</title>
<link>https://arxiv.org/abs/2505.22967</link>
<guid>https://arxiv.org/abs/2505.22967</guid>
<content:encoded><![CDATA[
<div> framework, safety-constrained, graph evolution, workflow generation, agentic reasoning <br />
Summary: <br />
MermaidFlow is a framework that aims to improve the robustness and executability of workflow generation in autonomous agents by redefining the search space through safety-constrained graph evolution. Workflows are represented using Mermaid, a structured and human-interpretable graph language, to ensure verifiability. Domain-aware evolutionary operators such as crossover, mutation, insertion, and deletion are formulated to maintain semantic correctness while enhancing structural diversity. The framework consistently achieves better success rates and faster convergence to executable plans on agent reasoning benchmarks without changing task settings or evaluation protocols. The experimental results highlight the scalability and interpretability of safety-constrained graph evolution in developing reliable agentic reasoning systems. <div>
arXiv:2505.22967v1 Announce Type: new 
Abstract: Despite the promise of autonomous agentic reasoning, existing workflow generation methods frequently produce fragile, unexecutable plans due to unconstrained LLM-driven construction. We introduce MermaidFlow, a framework that redefines the agentic search space through safety-constrained graph evolution. At its core, MermaidFlow represent workflows as a verifiable intermediate representation using Mermaid, a structured and human-interpretable graph language. We formulate domain-aware evolutionary operators, i.e., crossover, mutation, insertion, and deletion, to preserve semantic correctness while promoting structural diversity, enabling efficient exploration of a high-quality, statically verifiable workflow space. Without modifying task settings or evaluation protocols, MermaidFlow achieves consistent improvements in success rates and faster convergence to executable plans on the agent reasoning benchmark. The experimental results demonstrate that safety-constrained graph evolution offers a scalable, modular foundation for robust and interpretable agentic reasoning systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EquiReg: Equivariance Regularized Diffusion for Inverse Problems</title>
<link>https://arxiv.org/abs/2505.22973</link>
<guid>https://arxiv.org/abs/2505.22973</guid>
<content:encoded><![CDATA[
<div> diffusion models, image restoration, inverse problems, Equivariance Regularized, Bayesian framework

Summary: 
The article introduces Equivariance Regularized (EquiReg) diffusion as a novel approach to enhance reconstructions in diffusion-based inverse problem solvers. These solvers utilize a likelihood term to generate data consistent with the posterior distribution. EquiReg improves reconstructions by penalizing diffusion trajectories that deviate from the data manifold, leading to more stable and consistent results compared to current methods relying on isotropic Gaussian approximations. The framework defines a distribution-dependent equivariance error, identifies functions with low error for on-manifold samples, and utilizes these functions to regularize the diffusion sampling process. When applied to various solvers, EquiReg demonstrates superior performance in both linear and nonlinear image restoration tasks, as well as in reconstructing partial differential equations. <div>
arXiv:2505.22973v1 Announce Type: new 
Abstract: Diffusion models represent the state-of-the-art for solving inverse problems such as image restoration tasks. In the Bayesian framework, diffusion-based inverse solvers incorporate a likelihood term to guide the prior sampling process, generating data consistent with the posterior distribution. However, due to the intractability of the likelihood term, many current methods rely on isotropic Gaussian approximations, which lead to deviations from the data manifold and result in inconsistent, unstable reconstructions. We propose Equivariance Regularized (EquiReg) diffusion, a general framework for regularizing posterior sampling in diffusion-based inverse problem solvers. EquiReg enhances reconstructions by reweighting diffusion trajectories and penalizing those that deviate from the data manifold. We define a new distribution-dependent equivariance error, empirically identify functions that exhibit low error for on-manifold samples and higher error for off-manifold samples, and leverage these functions to regularize the diffusion sampling process. When applied to a variety of solvers, EquiReg outperforms state-of-the-art diffusion models in both linear and nonlinear image restoration tasks, as well as in reconstructing partial differential equations.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Approach to Improving Fairness in K-means Clustering</title>
<link>https://arxiv.org/abs/2505.22984</link>
<guid>https://arxiv.org/abs/2505.22984</guid>
<content:encoded><![CDATA[
<div> Algorithm, Fairness, K-means, Clustering, Optimization  
Summary:  
- The popular K-means clustering algorithm may have biases in terms of sensitive variables, leading to unfair outcomes.
- A two-stage optimization approach is proposed to improve the fairness of K-means clustering.
- Two efficient algorithms are developed to identify data points that impact fairness, focusing on nearest outside data points and highly mixed data points.
- Experiments on benchmark datasets demonstrate significant improvement in fairness without compromising clustering quality.
- The proposed algorithms can be easily applied to various clustering algorithms and fairness metrics.  
<br /><br />Summary: <div>
arXiv:2505.22984v1 Announce Type: new 
Abstract: The popular K-means clustering algorithm potentially suffers from a major weakness for further analysis or interpretation. Some cluster may have disproportionately more (or fewer) points from one of the subpopulations in terms of some sensitive variable, e.g., gender or race. Such a fairness issue may cause bias and unexpected social consequences. This work attempts to improve the fairness of K-means clustering with a two-stage optimization formulation--clustering first and then adjust cluster membership of a small subset of selected data points. Two computationally efficient algorithms are proposed in identifying those data points that are expensive for fairness, with one focusing on nearest data points outside of a cluster and the other on highly 'mixed' data points. Experiments on benchmark datasets show substantial improvement on fairness with a minimal impact to clustering quality. The proposed algorithms can be easily extended to a broad class of clustering algorithms or fairness metrics.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Distillation for Reservoir-based Classifier: Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.22985</link>
<guid>https://arxiv.org/abs/2505.22985</guid>
<content:encoded><![CDATA[
<div> Echo State Network, PatchEchoClassifier, human activity recognition, knowledge distillation, energy-efficient

Summary: 
PatchEchoClassifier is introduced as an energy-efficient classifier for time-series data, specifically for human activity recognition using sensor signals. The model utilizes an Echo State Network mechanism and a tokenizer to extract patch-level representations. It incorporates a knowledge distillation framework to transfer knowledge from a high-capacity MLP-Mixer teacher to the lightweight reservoir-based student model. Experimental results show that PatchEchoClassifier achieves over 80% accuracy while reducing computational cost significantly, requiring only a fraction of the FLOPS compared to DeepConvLSTM. These findings highlight PatchEchoClassifier as a promising solution for real-time and energy-efficient human activity recognition in edge computing environments. <div>
arXiv:2505.22985v1 Announce Type: new 
Abstract: This paper aims to develop an energy-efficient classifier for time-series data by introducing PatchEchoClassifier, a novel model that leverages a reservoir-based mechanism known as the Echo State Network (ESN). The model is designed for human activity recognition (HAR) using one-dimensional sensor signals and incorporates a tokenizer to extract patch-level representations. To train the model efficiently, we propose a knowledge distillation framework that transfers knowledge from a high-capacity MLP-Mixer teacher to the lightweight reservoir-based student model. Experimental evaluations on multiple HAR datasets demonstrate that our model achieves over 80 percent accuracy while significantly reducing computational cost. Notably, PatchEchoClassifier requires only about one-sixth of the floating point operations (FLOPS) compared to DeepConvLSTM, a widely used convolutional baseline. These results suggest that PatchEchoClassifier is a promising solution for real-time and energy-efficient human activity recognition in edge computing environments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Preserving Adaptive Rounding</title>
<link>https://arxiv.org/abs/2505.22988</link>
<guid>https://arxiv.org/abs/2505.22988</guid>
<content:encoded><![CDATA[
<div> quantization, post-training, linear layers, Hessian, algorithm  
Summary:  
The article introduces a new post-training quantization algorithm, named YAQA, that aims to produce a compressed model with an output distribution close to the original model. Unlike existing methods that focus on minimizing activation error in linear layers independently, YAQA considers the impact of subsequent layers by using Kronecker-factored approximations of each layer's Hessian with respect to the full model KL divergence. YAQA consists of two components: computation of tractable sketches of the full layerwise Hessian and a quantizer-independent rounding algorithm with theoretical guarantees. Empirical results show that YAQA reduces the KL divergence to the original model by approximately 30% across various models and quantizers, while also achieving state-of-the-art performance on downstream tasks. <div>
arXiv:2505.22988v1 Announce Type: new 
Abstract: The main goal of post-training quantization (PTQ) is to produced a compressed model whose output distribution is as close to the original model's as possible. To do this tractably, almost all LLM PTQ algorithms quantize linear layers by independently minimizing the immediate activation error. However, this localized objective ignores the effect of subsequent layers, so reducing it does not necessarily give a closer model. In this work, we introduce Yet Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses Kronecker-factored approximations of each linear layer's Hessian with respect to the \textit{full model} KL divergence. YAQA consists of two components: Kronecker-factored sketches of the full layerwise Hessian that can be tractably computed for hundred-billion parameter LLMs, and a quantizer-independent rounding algorithm that uses these sketches and comes with theoretical guarantees. Across a wide range of models and quantizers, YAQA empirically reduces the KL divergence to the original model by $\approx 30\%$ while achieving state of the art performance on downstream tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Number of Clusters in a Dataset: A Regularized K-means Approach</title>
<link>https://arxiv.org/abs/2505.22991</link>
<guid>https://arxiv.org/abs/2505.22991</guid>
<content:encoded><![CDATA[
<div> Regularized k-means algorithm; ideal clusters; additive regularization; multiplicative regularization; hyperparameter selection <br />
Summary:<br />
The study focuses on determining the correct number of clusters in unlabeled datasets using regularized k-means algorithms. The research derives rigorous bounds for the hyperparameter lambda, assuming clusters to be ideal spheres to mimic k-means clusters. It explores both additive and multiplicative regularization functions to reduce ambiguity in finding multiple solutions. Consensus among solutions helps alleviate the issue in certain scenarios. Experiments showcase the algorithm's performance as clusters deviate from the ideal assumption, highlighting the effectiveness of the approach in practical situations where ideal conditions may not hold. The analysis provides valuable insights into hyperparameter selection and the impact of regularizations on clustering outcomes.  <br /> <div>
arXiv:2505.22991v1 Announce Type: new 
Abstract: Finding the number of meaningful clusters in an unlabeled dataset is important in many applications. Regularized k-means algorithm is a possible approach frequently used to find the correct number of distinct clusters in datasets. The most common formulation of the regularization function is the additive linear term $\lambda k$, where $k$ is the number of clusters and $\lambda$ a positive coefficient. Currently, there are no principled guidelines for setting a value for the critical hyperparameter $\lambda$. In this paper, we derive rigorous bounds for $\lambda$ assuming clusters are {\em ideal}. Ideal clusters (defined as $d$-dimensional spheres with identical radii) are close proxies for k-means clusters ($d$-dimensional spherically symmetric distributions with identical standard deviations). Experiments show that the k-means algorithm with additive regularizer often yields multiple solutions. Thus, we also analyze k-means algorithm with multiplicative regularizer. The consensus among k-means solutions with additive and multiplicative regularizations reduces the ambiguity of multiple solutions in certain cases. We also present selected experiments that demonstrate performance of the regularized k-means algorithms as clusters deviate from the ideal assumption.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation</title>
<link>https://arxiv.org/abs/2505.22994</link>
<guid>https://arxiv.org/abs/2505.22994</guid>
<content:encoded><![CDATA[
<div> neuromodulation, artificial neural networks, conditioning, manifold optimization, generalization<br />
Summary:<br />
The article explores a novel approach inspired by neuromodulation to efficiently learn a range of similar tasks in artificial neural networks. Instead of conditioning a network upon task context by injecting context as input, the proposed method involves learning weights that are smoothly parameterized functions of task context variables. This approach optimizes a smooth manifold in weight space with a predefined topology, allowing for improved generalization across tasks. By training manifolds with various topologies and using conditioning to select points on the manifold during inference, the method outperforms traditional conditioning techniques and better generalizes to out-of-distribution samples. This strategy offers a principled and effective alternative to traditional conditioning in artificial neural networks. <div>
arXiv:2505.22994v1 Announce Type: new 
Abstract: One frequently wishes to learn a range of similar tasks as efficiently as possible, re-using knowledge across tasks. In artificial neural networks, this is typically accomplished by conditioning a network upon task context by injecting context as input. Brains have a different strategy: the parameters themselves are modulated as a function of various neuromodulators such as serotonin. Here, we take inspiration from neuromodulation and propose to learn weights which are smoothly parameterized functions of task context variables. Rather than optimize a weight vector, i.e. a single point in weight space, we optimize a smooth manifold in weight space with a predefined topology. To accomplish this, we derive a formal treatment of optimization of manifolds as the minimization of a loss functional subject to a constraint on volumetric movement, analogous to gradient descent. During inference, conditioning selects a single point on this manifold which serves as the effective weight matrix for a particular sub-task. This strategy for conditioning has two main advantages. First, the topology of the manifold (whether a line, circle, or torus) is a convenient lever for inductive biases about the relationship between tasks. Second, learning in one state smoothly affects the entire manifold, encouraging generalization across states. To verify this, we train manifolds with several topologies, including straight lines in weight space (for conditioning on e.g. noise level in input data) and ellipses (for rotated images). Despite their simplicity, these parameterizations outperform conditioning identical networks by input concatenation and better generalize to out-of-distribution samples. These results suggest that modulating weights over low-dimensional manifolds offers a principled and effective alternative to traditional conditioning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents for Bargaining with Utility-based Feedback</title>
<link>https://arxiv.org/abs/2505.22998</link>
<guid>https://arxiv.org/abs/2505.22998</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, negotiation strategies, opponent-aware reasoning, utility theory, structured feedback

Summary:
The article introduces a new framework for enhancing large language models (LLMs) in realistic bargaining scenarios. It presents a benchmark dataset called BargainArena with six complex scenarios to improve strategy modeling. The evaluation metrics are human-aligned and economically grounded, incorporating agent utility and negotiation power to promote opponent-aware reasoning. A structured feedback mechanism allows LLMs to refine their bargaining strategies iteratively and collaborate with in-context learning prompts. Experimental results show that LLMs often exhibit misaligned negotiation strategies with human preferences, but the structured feedback mechanism improves their performance significantly, fostering deeper strategic reasoning and opponent-awareness. This framework aims to address the challenges LLMs face in understanding and adapting to complex human factors in bargaining interactions. 

<br /><br />Summary: <div>
arXiv:2505.22998v1 Announce Type: new 
Abstract: Bargaining, a critical aspect of real-world interactions, presents challenges for large language models (LLMs) due to limitations in strategic depth and adaptation to complex human factors. Existing benchmarks often fail to capture this real-world complexity. To address this and enhance LLM capabilities in realistic bargaining, we introduce a comprehensive framework centered on utility-based feedback. Our contributions are threefold: (1) BargainArena, a novel benchmark dataset with six intricate scenarios (e.g., deceptive practices, monopolies) to facilitate diverse strategy modeling; (2) human-aligned, economically-grounded evaluation metrics inspired by utility theory, incorporating agent utility and negotiation power, which implicitly reflect and promote opponent-aware reasoning (OAR); and (3) a structured feedback mechanism enabling LLMs to iteratively refine their bargaining strategies. This mechanism can positively collaborate with in-context learning (ICL) prompts, including those explicitly designed to foster OAR. Experimental results show that LLMs often exhibit negotiation strategies misaligned with human preferences, and that our structured feedback mechanism significantly improves their performance, yielding deeper strategic and opponent-aware reasoning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Cross-domain Robust Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.23003</link>
<guid>https://arxiv.org/abs/2505.23003</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, robust RL, offline algorithms, simulator data, sample efficiency  
Summary:<br />
The paper introduces HYDRO, a novel Hybrid Cross-Domain Robust RL framework that addresses challenges in robust reinforcement learning. It aims to learn robust policies under uncertainties by maximizing value within an uncertainty set. HYDRO utilizes an online simulator to complement limited offline datasets, tackling the dynamics mismatch issue. Using uncertainty filtering and prioritized sampling, HYDRO selects relevant and reliable simulator samples, minimizing performance gaps between the simulator and worst-case models. Experimental results show HYDRO outperforms existing methods in various tasks, highlighting its potential to enhance sample efficiency in offline robust RL. <br /> <div>
arXiv:2505.23003v1 Announce Type: new 
Abstract: Robust reinforcement learning (RL) aims to learn policies that remain effective despite uncertainties in its environment, which frequently arise in real-world applications due to variations in environment dynamics. The robust RL methods learn a robust policy by maximizing value under the worst-case models within a predefined uncertainty set. Offline robust RL algorithms are particularly promising in scenarios where only a fixed dataset is available and new data cannot be collected. However, these approaches often require extensive offline data, and gathering such datasets for specific tasks in specific environments can be both costly and time-consuming. Using an imperfect simulator offers a faster, cheaper, and safer way to collect data for training, but it can suffer from dynamics mismatch. In this paper, we introduce HYDRO, the first Hybrid Cross-Domain Robust RL framework designed to address these challenges. HYDRO utilizes an online simulator to complement the limited amount of offline datasets in the non-trivial context of robust RL. By measuring and minimizing performance gaps between the simulator and the worst-case models in the uncertainty set, HYDRO employs novel uncertainty filtering and prioritized sampling to select the most relevant and reliable simulator samples. Our extensive experiments demonstrate HYDRO's superior performance over existing methods across various tasks, underscoring its potential to improve sample efficiency in offline robust RL.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining</title>
<link>https://arxiv.org/abs/2505.23004</link>
<guid>https://arxiv.org/abs/2505.23004</guid>
<content:encoded><![CDATA[
<div> vision language model, CLIP, MLLM, QLIP, visual understanding

Summary:
QLIP is introduced as a drop-in replacement for the CLIP vision encoder in Multimodal Large Language Models (MLLMs), addressing limitations such as fixed input resolutions and inability to produce separated embeddings for dissimilar images. Two key factors, mesoscopic bias and interpolation bias, are identified as underlying these limitations. QLIP integrates seamlessly with existing MLLMs, enhancing both coarse-grained and fine-grained visual understanding without requiring retraining. It introduces an image quadtree with content-aware patchification to improve visual question answering accuracy in LLaVA v1.5 models, achieving up to a 13.6 percent increase in detailed understanding performance on challenging benchmarks like $V^{\ast}.Summary: <div>
arXiv:2505.23004v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) encode images into visual tokens, aligning visual and textual signals within a shared latent space to facilitate crossmodal representation learning. The CLIP model is a widely adopted foundational vision language model whose vision encoder has played a critical role in the development of MLLMs such as LLaVA. However, the CLIP vision encoder suffers from notable limitations including being constrained to only handling fixed input resolutions and a failure to produce separated embeddings for dissimilar images. Replacing the vision encoder of an existing model typically incurs substantial computational costs because such a change often necessitates retraining the entire model pipeline.
  In this work, we identify two factors which underlie the limitations of the CLIP vision encoder: mesoscopic bias and interpolation bias. To address these issues, we propose QLIP, a drop-in replacement for CLIP that can be seamlessly integrated with existing MLLMs with only a few lines of code and can enhance both coarse-grained and fine-grained visual understanding, without re-training. QLIP is designed around an image quadtree which replaces the standard uniform grid patches with a novel content aware patchification. Our experimental results demonstrate that QLIP improves the general visual question answering accuracy of the LLaVA v1.5 model series across various model sizes--without requiring retraining or fine-tuning of the full MLLM. Notably, QLIP boosts detailed understanding performance on the challenging $V^{\ast}$ benchmark by up to 13.6 percent.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge</title>
<link>https://arxiv.org/abs/2505.23009</link>
<guid>https://arxiv.org/abs/2505.23009</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Speech, EmergentTTS, benchmark, evaluation, dataset

Summary: 
EmergentTTS-Eval is a new benchmark for Text-to-Speech (TTS) models, focusing on nuanced and semantically complex text. It covers six challenging scenarios including emotions, foreign words, and syntactic complexity. The benchmark automates test-case generation and evaluation, making it easily extensible. Using a model-as-a-judge approach with a Large Audio Language Model (LALM), it assesses speech across various dimensions such as emotion, prosody, and pronunciation accuracy. State-of-the-art TTS systems like 11Labs, Deepgram, and OpenAI's 4o-mini-TTS were evaluated on EmergentTTS-Eval, revealing performance differences. The model-as-a-judge approach shows robust TTS assessment and correlates well with human preferences. The evaluation code and dataset are open-sourced for further exploration. <br /><br />Summary: <div>
arXiv:2505.23009v1 Announce Type: new 
Abstract: Text-to-Speech (TTS) benchmarks often fail to capture how well models handle nuanced and semantically complex text. Building on $\textit{EmergentTTS}$, we introduce $\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic complexity, complex pronunciation (e.g. URLs, formulas), and questions. Crucially, our framework automates both test-case generation and evaluation, making the benchmark easily extensible. Starting from a small set of human-written seed prompts, we iteratively extend them using LLMs to target specific structural, phonetic and prosodic challenges, resulting in 1,645 diverse test cases. Moreover, we employ a model-as-a-judge approach, using a Large Audio Language Model (LALM) to assess the speech across multiple dimensions such as expressed emotion, prosodic, intonational, and pronunciation accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems, such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval, demonstrating its ability to reveal fine-grained performance differences. Results show that the model-as-a-judge approach offers robust TTS assessment and a high correlation with human preferences. We open source the evaluation $\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the $\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Complexity Control Facilitates Reasoning Ability of LLMs</title>
<link>https://arxiv.org/abs/2505.23013</link>
<guid>https://arxiv.org/abs/2505.23013</guid>
<content:encoded><![CDATA[
<div> control, complexity, language models, generalizability, scaling law

Summary:<br />
This study focuses on enhancing the generalizability of large language models (LLMs) by controlling model complexity through adjustments in initialization rate and weight decay coefficient. The researchers demonstrate that by effectively managing model complexity, LLMs show improved scaling laws across various model sizes and data sizes. Comparing benchmark performances of LLMs pretrained on 1T tokens with different complexity hyperparameters, the study highlights the benefits of constant initialization rates for faster scaling law descent. This approach suggests that complexity control is a promising avenue for advancing LLM capabilities in a more reliable manner. <div>
arXiv:2505.23013v1 Announce Type: new 
Abstract: The reasoning ability of large language models (LLMs) has been rapidly advancing in recent years, attracting interest in more fundamental approaches that can reliably enhance their generalizability. This work demonstrates that model complexity control, conveniently implementable by adjusting the initialization rate and weight decay coefficient, improves the scaling law of LLMs consistently over varying model sizes and data sizes. This gain is further illustrated by comparing the benchmark performance of 2.4B models pretrained on 1T tokens with different complexity hyperparameters. Instead of fixing the initialization std, we found that a constant initialization rate (the exponent of std) enables the scaling law to descend faster in both model and data sizes. These results indicate that complexity control is a promising direction for the continual advancement of LLMs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations</title>
<link>https://arxiv.org/abs/2505.23014</link>
<guid>https://arxiv.org/abs/2505.23014</guid>
<content:encoded><![CDATA[
<div> message passing, graph neural networks, hyperbolic partial differential equations, spectral graph neural networks, topological structure<br />
Summary:<br />
This paper presents a novel approach to graph neural networks by formulating message passing as a system of hyperbolic partial differential equations (hyperbolic PDEs). By mapping node representations into a solution space spanned by eigenvectors describing the topological structure of graphs, the system allows for the decomposition of node features into a superposition of these eigenvectors, enhancing interpretability and extracting fundamental characteristics of the topology. The use of hyperbolic PDEs also establishes a connection with spectral graph neural networks, improving their performance across various graph tasks. Additionally, the introduction of polynomials to approximate filter functions enhances flexibility and further boosts the effectiveness of spectral GNNs. Experimental results demonstrate the strong performance and flexibility of this hyperbolic PDE paradigm in improving spectral GNNs. <br /> <div>
arXiv:2505.23014v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) leverage message passing mechanisms to learn the topological features of graph data. Traditional GNNs learns node features in a spatial domain unrelated to the topology, which can hardly ensure topological features. In this paper, we formulates message passing as a system of hyperbolic partial differential equations (hyperbolic PDEs), constituting a dynamical system that explicitly maps node representations into a particular solution space. This solution space is spanned by a set of eigenvectors describing the topological structure of graphs. Within this system, for any moment in time, a node features can be decomposed into a superposition of the basis of eigenvectors. This not only enhances the interpretability of message passing but also enables the explicit extraction of fundamental characteristics about the topological structure. Furthermore, by solving this system of hyperbolic partial differential equations, we establish a connection with spectral graph neural networks (spectral GNNs), serving as a message passing enhancement paradigm for spectral GNNs.We further introduce polynomials to approximate arbitrary filter functions. Extensive experiments demonstrate that the paradigm of hyperbolic PDEs not only exhibits strong flexibility but also significantly enhances the performance of various spectral GNNs across diverse graph tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.23017</link>
<guid>https://arxiv.org/abs/2505.23017</guid>
<content:encoded><![CDATA[
<div> generative model, probabilistic time series forecasting, KoopmanNet, KalmanNet, long-term forecasting
Summary:
- The article introduces $K^2$VAE, a generative model for Probabilistic Time Series Forecasting (PTSF) that addresses the challenges of Long-term Probabilistic Time Series Forecasting (LPTSF).
- $K^2$VAE leverages a KoopmanNet to transform nonlinear time series into a linear dynamical system.
- It then uses a KalmanNet to refine predictions and model uncertainty in the linear system, reducing error accumulation in long-term forecasting.
- Extensive experiments show that $K^2$VAE outperforms existing methods in both short- and long-term PTSF, providing a more efficient and accurate solution. 
- The model shows promise in improving forecasting accuracy and efficiency across various fields such as economics, energy, and transportation.<br /><br />Summary: <div>
arXiv:2505.23017v1 Announce Type: new 
Abstract: Probabilistic Time Series Forecasting (PTSF) plays a crucial role in decision-making across various fields, including economics, energy, and transportation. Most existing methods excell at short-term forecasting, while overlooking the hurdles of Long-term Probabilistic Time Series Forecasting (LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have a significant adverse effect on prediction accuracy, and make generative models inefficient by increasing the cost of each iteration. To overcome these limitations, we introduce $K^2$VAE, an efficient VAE-based generative model that leverages a KoopmanNet to transform nonlinear time series into a linear dynamical system, and devises a KalmanNet to refine predictions and model uncertainty in such linear system, which reduces error accumulation in long-term forecasting. Extensive experiments demonstrate that $K^2$VAE outperforms state-of-the-art methods in both short- and long-term PTSF, providing a more efficient and accurate solution.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference</title>
<link>https://arxiv.org/abs/2505.23022</link>
<guid>https://arxiv.org/abs/2505.23022</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, SLOs, SCORPIO, throughput, heterogeneous workloads 

Summary: 
SCORPIO is a new Large Language Model (LLM) serving system that focuses on optimizing system goodput and meeting Service Level Objectives (SLOs) such as Time to First Token (TTFT) and Time Per Output Token (TPOT). The system is designed to handle workloads with varying SLO requirements by employing adaptive scheduling strategies across admission control, queue management, and batch selection. SCORPIO includes a TTFT Guard that prioritizes requests based on deadlines and rejects unattainable ones, as well as a TPOT Guard that uses a credit-based batching mechanism and VBS-based admission control. These modules are supported by a predictive module. Evaluation results show that SCORPIO significantly improves system goodput and SLO adherence compared to existing methods, with improvements of up to 14.4X in goodput and 46.5% in SLO adherence. <br /><br />Summary: <div>
arXiv:2505.23022v1 Announce Type: new 
Abstract: Existing Large Language Model (LLM) serving systems prioritize maximum throughput. They often neglect Service Level Objectives (SLOs) such as Time to First Token (TTFT) and Time Per Output Token (TPOT), which leads to suboptimal SLO attainment. This paper introduces SCORPIO, an SLO-oriented LLM serving system designed to maximize system goodput and SLO attainment for workloads with heterogeneous SLOs. Our core insight is to exploit SLO heterogeneity for adaptive scheduling across admission control, queue management, and batch selection. SCORPIO features a TTFT Guard, which employs least-deadline-first reordering and rejects unattainable requests, and a TPOT Guard, which utilizes a VBS-based admission control and a novel credit-based batching mechanism. Both guards are supported by a predictive module. Evaluations demonstrate that SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5% compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of Federated Prompt Learning for Vision Language Model</title>
<link>https://arxiv.org/abs/2505.23024</link>
<guid>https://arxiv.org/abs/2505.23024</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Model, prompt learning, federated learning, label skew, domain shift

Summary: 
The study explores the application of prompt learning with Vision Language Models (VLM) in federated learning scenarios. It compares the behavioral differences between language prompt learning (LPT) and vision prompt learning (VPT) under challenges like label skew and domain shift. Various configurations such as client scale, aggregation strategies, and prompt length are evaluated to understand the robustness of Federated Prompt Learning (FPL). Strategies for enhancing prompt learning in complex scenarios, where label skew and domain shift coexist, are also discussed, including leveraging both prompt types when computational resources allow. The findings provide practical insights for optimizing prompt learning in federated settings and contribute to the broader deployment of VLMs in privacy-preserving environments. 

<br /><br />Summary: <div>
arXiv:2505.23024v1 Announce Type: new 
Abstract: The Vision Language Model (VLM) excels in aligning vision and language representations, and prompt learning has emerged as a key technique for adapting such models to downstream tasks. However, the application of prompt learning with VLM in federated learning (\fl{}) scenarios remains underexplored. This paper systematically investigates the behavioral differences between language prompt learning (LPT) and vision prompt learning (VPT) under data heterogeneity challenges, including label skew and domain shift. We conduct extensive experiments to evaluate the impact of various \fl{} and prompt configurations, such as client scale, aggregation strategies, and prompt length, to assess the robustness of Federated Prompt Learning (FPL). Furthermore, we explore strategies for enhancing prompt learning in complex scenarios where label skew and domain shift coexist, including leveraging both prompt types when computational resources allow. Our findings offer practical insights into optimizing prompt learning in federated settings, contributing to the broader deployment of VLMs in privacy-preserving environments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift</title>
<link>https://arxiv.org/abs/2505.23027</link>
<guid>https://arxiv.org/abs/2505.23027</guid>
<content:encoded><![CDATA[
<div> subpopulation shift, machine learning models, ensemble of diverse classifiers, feature extractor network, prototypical classifiers <br />
Summary: <br />
The article discusses the challenge of subpopulation shift in machine learning models and proposes a novel solution using Diverse Prototypical Ensembles (DPEs). Traditional methods rely on re-weighting strategies based on assumptions about subpopulations, which may not be feasible for many real-world datasets. DPEs involve using an ensemble of diverse classifiers that focus on different features and samples, thus adapting to the risk associated with subpopulations. By replacing the linear classification layer with a mixture of prototypical classifiers in a feature extractor network, DPEs outperform prior approaches in worst-group accuracy on nine real-world datasets. This method shows promising results in addressing subpopulation shift without the need for explicit annotations of group membership. <div>
arXiv:2505.23027v1 Announce Type: new 
Abstract: The subpopulationtion shift, characterized by a disparity in subpopulation distributibetween theween the training and target datasets, can significantly degrade the performance of machine learning models. Current solutions to subpopulation shift involve modifying empirical risk minimization with re-weighting strategies to improve generalization. This strategy relies on assumptions about the number and nature of subpopulations and annotations on group membership, which are unavailable for many real-world datasets. Instead, we propose using an ensemble of diverse classifiers to adaptively capture risk associated with subpopulations. Given a feature extractor network, we replace its standard linear classification layer with a mixture of prototypical classifiers, where each member is trained to classify the data while focusing on different features and samples from other members. In empirical evaluation on nine real-world datasets, covering diverse domains and kinds of subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often outperforms the prior state-of-the-art in worst-group accuracy. The code is available at https://github.com/minhto2802/dpe4subpop
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks</title>
<link>https://arxiv.org/abs/2505.23032</link>
<guid>https://arxiv.org/abs/2505.23032</guid>
<content:encoded><![CDATA[
<div> Bayesian framework, Prior-data Fitted Networks, neural scaling law, uncertainty quantification, meta-learning <br />
Summary:
This research introduces a Bayesian approach using Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation. The method incorporates a prior distribution to generate synthetic functions that mimic real-world scaling laws, enabling the model to learn the extrapolation. The study validates the effectiveness of the proposed framework by comparing it against existing point estimation and Bayesian methods on real neural scaling laws. Results demonstrate superior performance, particularly in scenarios with limited data such as Bayesian active learning, highlighting the method's potential for reliable and uncertainty-aware extrapolation in practical applications. <div>
arXiv:2505.23032v1 Announce Type: new 
Abstract: Scaling has been a major driver of recent advancements in deep learning. Numerous empirical studies have found that scaling laws often follow the power-law and proposed several variants of power-law functions to predict the scaling behavior at larger scales. However, existing methods mostly rely on point estimation and do not quantify uncertainty, which is crucial for real-world applications involving decision-making problems such as determining the expected performance improvements achievable by investing additional computational resources. In this work, we explore a Bayesian framework based on Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation. Specifically, we design a prior distribution that enables the sampling of infinitely many synthetic functions resembling real-world neural scaling laws, allowing our PFN to meta-learn the extrapolation. We validate the effectiveness of our approach on real-world neural scaling laws, comparing it against both the existing point estimation methods and Bayesian approaches. Our method demonstrates superior performance, particularly in data-limited scenarios such as Bayesian active learning, underscoring its potential for reliable, uncertainty-aware extrapolation in practical applications.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data</title>
<link>https://arxiv.org/abs/2505.23042</link>
<guid>https://arxiv.org/abs/2505.23042</guid>
<content:encoded><![CDATA[
arXiv:2505.23042v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models have inspired the development of foundation models across various domains. In this study, we evaluate the efficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art foundation EEG model, on a real-world stress classification dataset collected in a graduate classroom. Unlike previous studies that primarily evaluate LEMs using data from controlled clinical settings, our work assesses their applicability to real-world environments. We train a binary classifier that distinguishes between normal and elevated stress states using resting-state EEG data recorded from 18 graduate students during a class session. The best-performing fine-tuned model achieves a balanced accuracy of 90.47% with a 5-second window, significantly outperforming traditional stress classifiers in both accuracy and inference efficiency. We further evaluate the robustness of the fine-tuned LEM under random data shuffling and reduced channel counts. These results demonstrate the capability of LEMs to effectively process real-world EEG data and highlight their potential to revolutionize brain-computer interface applications by shifting the focus from model-centric to data-centric design.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation</title>
<link>https://arxiv.org/abs/2505.23048</link>
<guid>https://arxiv.org/abs/2505.23048</guid>
<content:encoded><![CDATA[
arXiv:2505.23048v1 Announce Type: new 
Abstract: Trajectory data is crucial for various applications but often suffers from incompleteness due to device limitations and diverse collection scenarios. Existing imputation methods rely on sparse trajectory or travel information, such as velocity, to infer missing points. However, these approaches assume that sparse trajectories retain essential behavioral patterns, which place significant demands on data acquisition and overlook the potential of large-scale human trajectory embeddings. To address this, we propose ProDiff, a trajectory imputation framework that uses only two endpoints as minimal information. It integrates prototype learning to embed human movement patterns and a denoising diffusion probabilistic model for robust spatiotemporal reconstruction. Joint training with a tailored loss function ensures effective imputation. ProDiff outperforms state-of-the-art methods, improving accuracy by 6.28\% on FourSquare and 2.52\% on WuXi. Further analysis shows a 0.927 correlation between generated and real trajectories, demonstrating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration</title>
<link>https://arxiv.org/abs/2505.23049</link>
<guid>https://arxiv.org/abs/2505.23049</guid>
<content:encoded><![CDATA[
arXiv:2505.23049v1 Announce Type: new 
Abstract: Pruning is a widely used technique to compress large language models (LLMs) by removing unimportant weights, but it often suffers from significant performance degradation - especially under semi-structured sparsity constraints. Existing pruning methods primarily focus on estimating the importance of individual weights, which limits their ability to preserve critical capabilities of the model. In this work, we propose a new perspective: rather than merely selecting which weights to prune, we first redistribute parameter importance to make the model inherently more amenable to pruning. By minimizing the information entropy of normalized importance scores, our approach concentrates importance onto a smaller subset of weights, thereby enhancing pruning robustness. We instantiate this idea through DenoiseRotator, which applies learnable orthogonal transformations to the model's weight matrices. Our method is model-agnostic and can be seamlessly integrated with existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4 semi-structured sparsity, DenoiseRotator consistently improves perplexity and zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4 semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are available at https://github.com/Axel-gu/DenoiseRotator.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents</title>
<link>https://arxiv.org/abs/2505.23055</link>
<guid>https://arxiv.org/abs/2505.23055</guid>
<content:encoded><![CDATA[
arXiv:2505.23055v1 Announce Type: new 
Abstract: Clinical decision-making is inherently complex and fast-paced, particularly in emergency departments (EDs) where critical, rapid and high-stakes decisions are made. Clinical Decision Rules (CDRs) are standardized evidence-based tools that combine signs, symptoms, and clinical variables into decision trees to make consistent and accurate diagnoses. CDR usage is often hindered by the clinician's cognitive load, limiting their ability to quickly recall and apply the appropriate rules. We introduce CDR-Agent, a novel LLM-based system designed to enhance ED decision-making by autonomously identifying and applying the most appropriate CDRs based on unstructured clinical notes. To validate CDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although CDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\% (synthetic) and 8.7\% (CDR-Bench) accuracy gain relative to the standalone LLM baseline in CDR selection. Moreover, CDR-Agent significantly reduces computational overhead. Using these datasets, we demonstrated that CDR-Agent not only selects relevant CDRs efficiently, but makes cautious yet effective imaging decisions by minimizing unnecessary interventions while successfully identifying most positively diagnosed cases, outperforming traditional LLM prompting approaches. Code for our work can be found at: https://github.com/zhenxianglance/medagent-cdr-agent
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DINGO: Constrained Inference for Diffusion LLMs</title>
<link>https://arxiv.org/abs/2505.23061</link>
<guid>https://arxiv.org/abs/2505.23061</guid>
<content:encoded><![CDATA[
arXiv:2505.23061v1 Announce Type: new 
Abstract: Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering significant potential for improved runtime efficiency. However, existing diffusion models lack the ability to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models that generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, which are designed for sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model's predicted distribution, while strictly satisfying any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point improvement over unconstrained inference
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data</title>
<link>https://arxiv.org/abs/2505.23062</link>
<guid>https://arxiv.org/abs/2505.23062</guid>
<content:encoded><![CDATA[
arXiv:2505.23062v1 Announce Type: new 
Abstract: Incorporating pre-collected offline data from a source environment can significantly improve the sample efficiency of reinforcement learning (RL), but this benefit is often challenged by discrepancies between the transition dynamics of the source and target environments. Existing methods typically address this issue by penalizing or filtering out source transitions in high dynamics-gap regions. However, their estimation of the dynamics gap often relies on KL divergence or mutual information, which can be ill-defined when the source and target dynamics have disjoint support. To overcome these limitations, we propose CompFlow, a method grounded in the theoretical connection between flow matching and optimal transport. Specifically, we model the target dynamics as a conditional flow built upon the output distribution of the source-domain flow, rather than learning it directly from a Gaussian prior. This composite structure offers two key advantages: (1) improved generalization for learning target dynamics, and (2) a principled estimation of the dynamics gap via the Wasserstein distance between source and target transitions. Leveraging our principled estimation of the dynamics gap, we further introduce an optimistic active data collection strategy that prioritizes exploration in regions of high dynamics gap, and theoretically prove that it reduces the performance disparity with the optimal policy. Empirically, CompFlow outperforms strong baselines across several RL benchmarks with shifted dynamics.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification</title>
<link>https://arxiv.org/abs/2505.23063</link>
<guid>https://arxiv.org/abs/2505.23063</guid>
<content:encoded><![CDATA[
arXiv:2505.23063v1 Announce Type: new 
Abstract: Crop disease detection and classification is a critical challenge in agriculture, with major implications for productivity, food security, and environmental sustainability. While deep learning models such as CNN and ViT have shown excellent performance in classifying plant diseases from images, their large-scale deployment is often limited by data privacy concerns. Federated Learning (FL) addresses this issue, but centralized FL remains vulnerable to single-point failures and scalability limits. In this paper, we introduce a novel Decentralized Federated Learning (DFL) framework that uses validation loss (Loss_val) both to guide model sharing between peers and to correct local training via an adaptive loss function controlled by weighting parameter. We conduct extensive experiments using PlantVillage datasets with three deep learning architectures (ResNet50, VGG16, and ViT_B16), analyzing the impact of weighting parameter, the number of shared models, the number of clients, and the use of Loss_val versus Loss_train of other clients. Results demonstrate that our DFL approach not only improves accuracy and convergence speed, but also ensures better generalization and robustness across heterogeneous data environments making it particularly well-suited for privacy-preserving agricultural applications.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Learning with Bayesian-Oriented Gradient Calibration</title>
<link>https://arxiv.org/abs/2505.23071</link>
<guid>https://arxiv.org/abs/2505.23071</guid>
<content:encoded><![CDATA[
arXiv:2505.23071v1 Announce Type: new 
Abstract: Multi-Modal Learning (MML) integrates information from diverse modalities to improve predictive accuracy. However, existing methods mainly aggregate gradients with fixed weights and treat all dimensions equally, overlooking the intrinsic gradient uncertainty of each modality. This may lead to (i) excessive updates in sensitive dimensions, degrading performance, and (ii) insufficient updates in less sensitive dimensions, hindering learning. To address this issue, we propose BOGC-MML, a Bayesian-Oriented Gradient Calibration method for MML to explicitly model the gradient uncertainty and guide the model optimization towards the optimal direction. Specifically, we first model each modality's gradient as a random variable and derive its probability distribution, capturing the full uncertainty in the gradient space. Then, we propose an effective method that converts the precision (inverse variance) of each gradient distribution into a scalar evidence. This evidence quantifies the confidence of each modality in every gradient dimension. Using these evidences, we explicitly quantify per-dimension uncertainties and fuse them via a reduced Dempster-Shafer rule. The resulting uncertainty-weighted aggregation produces a calibrated update direction that balances sensitivity and conservatism across dimensions. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness and advantages of the proposed method.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Boosting Decision Tree with LSTM for Investment Prediction</title>
<link>https://arxiv.org/abs/2505.23084</link>
<guid>https://arxiv.org/abs/2505.23084</guid>
<content:encoded><![CDATA[
arXiv:2505.23084v1 Announce Type: new 
Abstract: This paper proposes a hybrid framework combining LSTM (Long Short-Term Memory) networks with LightGBM and CatBoost for stock price prediction. The framework processes time-series financial data and evaluates performance using seven models: Artificial Neural Networks (ANNs), Convolutional Neural Networks (CNNs), Bidirectional LSTM (BiLSTM), vanilla LSTM, XGBoost, LightGBM, and standard Neural Networks (NNs). Key metrics, including MAE, R-squared, MSE, and RMSE, are used to establish benchmarks across different time scales.
  Building on these benchmarks, we develop an ensemble model that combines the strengths of sequential and tree-based approaches. Experimental results show that the proposed framework improves accuracy by 10 to 15 percent compared to individual models and reduces error during market changes. This study highlights the potential of ensemble methods for financial forecasting and provides a flexible design for integrating new machine learning techniques.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Spherical Transformer for Efficient Molecular Modeling</title>
<link>https://arxiv.org/abs/2505.23086</link>
<guid>https://arxiv.org/abs/2505.23086</guid>
<content:encoded><![CDATA[
arXiv:2505.23086v1 Announce Type: new 
Abstract: SE(3)-equivariant Graph Neural Networks (GNNs) have significantly advanced molecular system modeling by employing group representations. However, their message passing processes, which rely on tensor product-based convolutions, are limited by insufficient non-linearity and incomplete group representations, thereby restricting expressiveness. To overcome these limitations, we introduce the Equivariant Spherical Transformer (EST), a novel framework that leverages a Transformer structure within the spatial domain of group representations after Fourier transform. We theoretically and empirically demonstrate that EST can encompass the function space of tensor products while achieving superior expressiveness. Furthermore, EST's equivariant inductive bias is guaranteed through a uniform sampling strategy for the Fourier transform. Our experiments demonstrate state-of-the-art performance by EST on various molecular benchmarks, including OC20 and QM9.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAP: Revisiting Weight Decomposition for Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2505.23094</link>
<guid>https://arxiv.org/abs/2505.23094</guid>
<content:encoded><![CDATA[
arXiv:2505.23094v1 Announce Type: new 
Abstract: The rapid development of large language models has revolutionized natural language processing, but their fine-tuning remains computationally expensive, hindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, have emerged as solutions. Recent work like DoRA attempts to further decompose weight adaptation into direction and magnitude components. However, existing formulations often define direction heuristically at the column level, lacking a principled geometric foundation. In this paper, we propose MAP, a novel framework that reformulates weight matrices as high-dimensional vectors and decouples their adaptation into direction and magnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns a directional update, and introduces two scalar coefficients to independently scale the magnitude of the base and update vectors. This design enables more interpretable and flexible adaptation, and can be seamlessly integrated into existing PEFT methods. Extensive experiments show that MAP significantly improves performance when coupling with existing methods, offering a simple yet powerful enhancement to existing PEFT methods. Given the universality and simplicity of MAP, we hope it can serve as a default setting for designing future PEFT methods.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Search for Vehicle Routing with Multiple Time Windows</title>
<link>https://arxiv.org/abs/2505.23098</link>
<guid>https://arxiv.org/abs/2505.23098</guid>
<content:encoded><![CDATA[
arXiv:2505.23098v1 Announce Type: new 
Abstract: In this study, we propose a reinforcement learning-based adaptive variable neighborhood search (RL-AVNS) method designed for effectively solving the Vehicle Routing Problem with Multiple Time Windows (VRPMTW). Unlike traditional adaptive approaches that rely solely on historical operator performance, our method integrates a reinforcement learning framework to dynamically select neighborhood operators based on real-time solution states and learned experience. We introduce a fitness metric that quantifies customers' temporal flexibility to improve the shaking phase, and employ a transformer-based neural policy network to intelligently guide operator selection during the local search. Extensive computational experiments are conducted on realistic scenarios derived from the replenishment of unmanned vending machines, characterized by multiple clustered replenishment windows. Results demonstrate that RL-AVNS significantly outperforms traditional variable neighborhood search (VNS), adaptive VNS (AVNS), and state-of-the-art learning-based heuristics, achieving substantial improvements in solution quality and computational efficiency across various instance scales and time window complexities. Particularly notable is the algorithm's capability to generalize effectively to problem instances not encountered during training, underscoring its practical utility for complex logistics scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weight Spectra Induced Efficient Model Adaptation</title>
<link>https://arxiv.org/abs/2505.23099</link>
<guid>https://arxiv.org/abs/2505.23099</guid>
<content:encoded><![CDATA[
arXiv:2505.23099v1 Announce Type: new 
Abstract: Large-scale foundation models have demonstrated remarkable versatility across a wide range of downstream tasks. However, fully fine-tuning these models incurs prohibitive computational costs, motivating the development of Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, which introduces low-rank updates to pre-trained weights. Despite their empirical success, the underlying mechanisms by which PEFT modifies model parameters remain underexplored. In this work, we present a systematic investigation into the structural changes of weight matrices during fully fine-tuning. Through singular value decomposition (SVD), we reveal that fine-tuning predominantly amplifies the top singular values while leaving the remainder largely intact, suggesting that task-specific knowledge is injected into a low-dimensional subspace. Furthermore, we find that the dominant singular vectors are reoriented in task-specific directions, whereas the non-dominant subspace remains stable. Building on these insights, we propose a novel method that leverages learnable rescaling of top singular directions, enabling precise modulation of the most influential components without disrupting the global structure. Our approach achieves consistent improvements over strong baselines across multiple tasks, highlighting the efficacy of structurally informed fine-tuning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LUMION: Fast Fault Recovery for ML Jobs Using Programmable Optical Fabrics</title>
<link>https://arxiv.org/abs/2505.23105</link>
<guid>https://arxiv.org/abs/2505.23105</guid>
<content:encoded><![CDATA[
arXiv:2505.23105v1 Announce Type: new 
Abstract: When accelerators fail in modern ML datacenters, operators migrate the affected ML training or inference jobs to entirely new racks. This approach, while preserving network performance, is highly inefficient, requiring datacenters to reserve full racks of idle accelerators for fault tolerance. In this paper, we address this resource inefficiency by introducing LUMION, a novel reconfigurable optical fabric for connecting accelerators within a datacenter rack. Instead of migrating entire ML jobs, LUMION dynamically integrates spare accelerators into ongoing workloads as failures occur, thereby maintaining consistent performance without costly migrations. We show the benefits of LUMION by building an end-to-end hardware prototype. Our experiments fine-tune Llama 3.2 and show that LUMION swaps a failed GPU with a healthy one and restarts the ML job within ~ 1 second of the failure. LUMION achieves higher inter-GPU bandwidth compared to traditional electrical racks after replacing failed accelerators with spare ones, leading to nearly 2X improvement in fine-tuning throughput.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery</title>
<link>https://arxiv.org/abs/2505.23106</link>
<guid>https://arxiv.org/abs/2505.23106</guid>
<content:encoded><![CDATA[
arXiv:2505.23106v1 Announce Type: new 
Abstract: Attention mechanisms have emerged as transformative tools in core AI domains such as natural language processing and computer vision. Yet, their largely untapped potential for modeling intricate physical systems presents a compelling frontier. Learning such systems often entails discovering operators that map between functional spaces using limited instances of function pairs -- a task commonly framed as a severely ill-posed inverse PDE problem. In this work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator architecture that builds upon and enhances Nonlocal Attention Operators (NAO) in both predictive accuracy and computational efficiency. NIPS employs a linear attention mechanism to enable scalable learning and integrates a learnable kernel network that acts as a channel-independent convolution in Fourier space. As a consequence, NIPS eliminates the need to explicitly compute and store large pairwise interactions, effectively amortizing the cost of handling spatial interactions into the Fourier transform. Empirical evaluations demonstrate that NIPS consistently surpasses NAO and other baselines across diverse benchmarks, heralding a substantial leap in scalable, interpretable, and efficient physics learning. Our code and data accompanying this paper are available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables</title>
<link>https://arxiv.org/abs/2505.23116</link>
<guid>https://arxiv.org/abs/2505.23116</guid>
<content:encoded><![CDATA[
arXiv:2505.23116v1 Announce Type: new 
Abstract: Time series forecasting with exogenous variables is a critical emerging paradigm that presents unique challenges in modeling dependencies between variables. Traditional models often struggle to differentiate between endogenous and exogenous variables, leading to inefficiencies and overfitting. In this paper, we introduce CrossLinear, a novel Linear-based forecasting model that addresses these challenges by incorporating a plug-and-play cross-correlation embedding module. This lightweight module captures the dependencies between variables with minimal computational cost and seamlessly integrates into existing neural networks. Specifically, it captures time-invariant and direct variable dependencies while disregarding time-varying or indirect dependencies, thereby mitigating the risk of overfitting in dependency modeling and contributing to consistent performance improvements. Furthermore, CrossLinear employs patch-wise processing and a global linear head to effectively capture both short-term and long-term temporal dependencies, further improving its forecasting precision. Extensive experiments on 12 real-world datasets demonstrate that CrossLinear achieves superior performance in both short-term and long-term forecasting tasks. The ablation study underscores the effectiveness of the cross-correlation embedding module. Additionally, the generalizability of this module makes it a valuable plug-in for various forecasting tasks across different domains. Codes are available at https://github.com/mumiao2000/CrossLinear.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking</title>
<link>https://arxiv.org/abs/2505.23117</link>
<guid>https://arxiv.org/abs/2505.23117</guid>
<content:encoded><![CDATA[
arXiv:2505.23117v1 Announce Type: new 
Abstract: In the era of large-scale training, model merging has evolved into a tool for creating multitasking models efficiently. It enables the knowledge of models to be fused, without the need for heavy computation as required in traditional multitask learning. Existing merging methods often assume that entries at identical positions in weight matrices serve the same function, enabling straightforward entry-wise comparison and merging. However, this assumption overlooks the complexity of finetuned neural networks, where neurons may develop distinct feature compositions, making direct entry-wise merging problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective approach that leverages Singular Value Decomposition to decompose and coordinate weight matrices into an aligned joint space, where entry-wise merging becomes possible. We showcase the effectiveness of DRM across various settings ranging from smaller encoder-based such as ViT and DeBERTa, encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B. Our experimental results show that DRM outperforms several state-of-the-art merging techniques across full finetuning and low-rank adaptation settings. Moreover, our analysis reveals renormalization as the crucial component for creating a robust and even joint space for merging, significantly contributing to the method's performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs</title>
<link>https://arxiv.org/abs/2505.23131</link>
<guid>https://arxiv.org/abs/2505.23131</guid>
<content:encoded><![CDATA[
arXiv:2505.23131v1 Announce Type: new 
Abstract: We study the problem of assigning operations in a dataflow graph to devices to minimize execution time in a work-conserving system, with emphasis on complex machine learning workloads. Prior learning-based methods often struggle due to three key limitations: (1) reliance on bulk-synchronous systems like TensorFlow, which under-utilize devices due to barrier synchronization; (2) lack of awareness of the scheduling mechanism of underlying systems when designing learning-based methods; and (3) exclusive dependence on reinforcement learning, ignoring the structure of effective heuristics designed by experts. In this paper, we propose \textsc{Doppler}, a three-stage framework for training dual-policy networks consisting of 1) a $\mathsf{SEL}$ policy for selecting operations and 2) a $\mathsf{PLC}$ policy for placing chosen operations on devices. Our experiments show that \textsc{Doppler} outperforms all baseline methods across tasks by reducing system execution time and additionally demonstrates sampling efficiency by reducing per-episode training time.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VERINA: Benchmarking Verifiable Code Generation</title>
<link>https://arxiv.org/abs/2505.23135</link>
<guid>https://arxiv.org/abs/2505.23135</guid>
<content:encoded><![CDATA[
arXiv:2505.23135v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly integrated in software development, but ensuring correctness in LLM-generated code remains challenging and often requires costly manual review. Verifiable code generation -- jointly generating code, specifications, and proofs of code-specification alignment -- offers a promising path to address this limitation and further unleash LLMs' benefits in coding. Yet, there exists a significant gap in evaluation: current benchmarks often lack support for end-to-end verifiable code generation. In this paper, we introduce Verina (Verifiable Code Generation Arena), a high-quality benchmark enabling a comprehensive and modular evaluation of code, specification, and proof generation as well as their compositions. Verina consists of 189 manually curated coding tasks in Lean, with detailed problem descriptions, reference implementations, formal specifications, and extensive test suites. Our extensive evaluation of state-of-the-art LLMs reveals significant challenges in verifiable code generation, especially in proof generation, underscoring the need for improving LLM-based theorem provers in verification domains. The best model, OpenAI o4-mini, generates only 61.4% correct code, 51.0% sound and complete specifications, and 3.6% successful proofs, with one trial per task. We hope Verina will catalyze progress in verifiable code generation by providing a rigorous and comprehensive benchmark. We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina and our evaluation code on https://github.com/sunblaze-ucb/verina.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners</title>
<link>https://arxiv.org/abs/2505.23150</link>
<guid>https://arxiv.org/abs/2505.23150</guid>
<content:encoded><![CDATA[
arXiv:2505.23150v1 Announce Type: new 
Abstract: Recent advances in language modeling and vision stem from training large models on diverse, multi-task data. This paradigm has had limited impact in value-based reinforcement learning (RL), where improvements are often driven by small models trained in a single-task context. This is because in multi-task RL sparse rewards and gradient conflicts make optimization of temporal difference brittle. Practical workflows for generalist policies therefore avoid online training, instead cloning expert trajectories or distilling collections of single-task policies into one agent. In this work, we show that the use of high-capacity value models trained via cross-entropy and conditioned on learnable task embeddings addresses the problem of task interference in online RL, allowing for robust and scalable multi-task training. We test our approach on 7 multi-task benchmarks with over 280 unique tasks, spanning high degree-of-freedom humanoid control and discrete vision-based RL. We find that, despite its simplicity, the proposed approach leads to state-of-the-art single and multi-task performance, as well as sample-efficient transfer to new tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Best Arm Identification with Possibly Biased Offline Data</title>
<link>https://arxiv.org/abs/2505.23165</link>
<guid>https://arxiv.org/abs/2505.23165</guid>
<content:encoded><![CDATA[
arXiv:2505.23165v1 Announce Type: new 
Abstract: We study the best arm identification (BAI) problem with potentially biased offline data in the fixed confidence setting, which commonly arises in real-world scenarios such as clinical trials. We prove an impossibility result for adaptive algorithms without prior knowledge of the bias bound between online and offline distributions. To address this, we propose the LUCB-H algorithm, which introduces adaptive confidence bounds by incorporating an auxiliary bias correction to balance offline and online data within the LUCB framework. Theoretical analysis shows that LUCB-H matches the sample complexity of standard LUCB when offline data is misleading and significantly outperforms it when offline data is helpful. We also derive an instance-dependent lower bound that matches the upper bound of LUCB-H in certain scenarios. Numerical experiments further demonstrate the robustness and adaptability of LUCB-H in effectively incorporating offline data.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization</title>
<link>https://arxiv.org/abs/2505.23173</link>
<guid>https://arxiv.org/abs/2505.23173</guid>
<content:encoded><![CDATA[
arXiv:2505.23173v1 Announce Type: new 
Abstract: Deep learning models often struggle to maintain performance when deployed on data distributions different from their training data, particularly in real-world applications where environmental conditions frequently change. While Multi-source Domain Generalization (MDG) has shown promise in addressing this challenge by leveraging multiple source domains during training, its practical application is limited by the significant costs and difficulties associated with creating multi-domain datasets. To address this limitation, we propose Pseudo Multi-source Domain Generalization (PMDG), a novel framework that enables the application of sophisticated MDG algorithms in more practical Single-source Domain Generalization (SDG) settings. PMDG generates multiple pseudo-domains from a single source domain through style transfer and data augmentation techniques, creating a synthetic multi-domain dataset that can be used with existing MDG algorithms. Through extensive experiments with PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze the effectiveness of PMDG across multiple datasets and architectures. Our analysis reveals several key findings, including a positive correlation between MDG and PMDG performance and the potential of pseudo-domains to match or exceed actual multi-domain performance with sufficient data. These comprehensive empirical results provide valuable insights for future research in domain generalization. Our code is available at https://github.com/s-enmt/PseudoDomainBed.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning</title>
<link>https://arxiv.org/abs/2505.23176</link>
<guid>https://arxiv.org/abs/2505.23176</guid>
<content:encoded><![CDATA[
arXiv:2505.23176v1 Announce Type: new 
Abstract: To improve the training efficiency of federated learning (FL), previous research has employed low-rank decomposition techniques to reduce communication overhead. In this paper, we seek to enhance the performance of these low-rank decomposition methods. Specifically, we focus on three key issues related to decomposition in FL: what to decompose, how to decompose, and how to aggregate. Subsequently, we introduce three novel techniques: Model Update Decomposition (MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware Decomposition (AAD), each targeting a specific issue. These techniques are complementary and can be applied simultaneously to achieve optimal performance. Additionally, we provide a rigorous theoretical analysis to ensure the convergence of the proposed MUD. Extensive experimental results show that our approach achieves faster convergence and superior accuracy compared to relevant baseline methods. The code is available at https://github.com/Leopold1423/fedmud-icml25.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification</title>
<link>https://arxiv.org/abs/2505.23181</link>
<guid>https://arxiv.org/abs/2505.23181</guid>
<content:encoded><![CDATA[
arXiv:2505.23181v1 Announce Type: new 
Abstract: Contrastive learning has emerged as a competent approach for unsupervised representation learning. However, the design of an optimal augmentation strategy, although crucial for contrastive learning, is less explored for time series classification tasks. Existing predefined time-domain augmentation methods are primarily adopted from vision and are not specific to time series data. Consequently, this cross-modality incompatibility may distort the semantically relevant information of time series by introducing mismatched patterns into the data. To address this limitation, we present a novel perspective from the frequency domain and identify three advantages for downstream classification: global, independent, and compact. To fully utilize the three properties, we propose the lightweight yet effective Frequency Refined Augmentation (FreRA) tailored for time series contrastive learning on classification tasks, which can be seamlessly integrated with contrastive learning frameworks in a plug-and-play manner. Specifically, FreRA automatically separates critical and unimportant frequency components. Accordingly, we propose semantic-aware Identity Modification and semantic-agnostic Self-adaptive Modification to protect semantically relevant information in the critical frequency components and infuse variance into the unimportant ones respectively. Theoretically, we prove that FreRA generates semantic-preserving views. Empirically, we conduct extensive experiments on two benchmark datasets, including UCR and UEA archives, as well as five large-scale datasets on diverse applications. FreRA consistently outperforms ten leading baselines on time series classification, anomaly detection, and transfer learning tasks, demonstrating superior capabilities in contrastive representation learning and generalization in transfer learning scenarios across diverse datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation</title>
<link>https://arxiv.org/abs/2505.23182</link>
<guid>https://arxiv.org/abs/2505.23182</guid>
<content:encoded><![CDATA[
arXiv:2505.23182v1 Announce Type: new 
Abstract: Collaborative training methods like Federated Learning (FL) and Split Learning (SL) enable distributed machine learning without sharing raw data. However, FL assumes clients can train entire models, which is infeasible for large-scale models. In contrast, while SL alleviates the client memory constraint in FL by offloading most training to the server, it increases network latency due to its sequential nature. Other methods address the conundrum by using local loss functions for parallel client-side training to improve efficiency, but they lack server feedback and potentially suffer poor accuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation Gradient Estimation), a new federated split learning algorithm that estimates server-side gradient feedback via auxiliary models. These auxiliary models periodically adapt to emulate server behavior on local datasets. We show that FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is the number of communication rounds. This result matches FedAvg, while significantly reducing communication costs and client memory requirements. Our empirical results also verify that it outperforms existing state-of-the-art FSL methods, offering both communication efficiency and accuracy.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Is Better Than One: Rotations Scale LoRAs</title>
<link>https://arxiv.org/abs/2505.23184</link>
<guid>https://arxiv.org/abs/2505.23184</guid>
<content:encoded><![CDATA[
arXiv:2505.23184v1 Announce Type: new 
Abstract: Scaling Low-Rank Adaptation (LoRA)-based Mixture-of-Experts (MoE) facilitates large language models (LLMs) to efficiently adapt to diverse tasks. However, traditional gating mechanisms that route inputs to the best experts may fundamentally hinder LLMs' scalability, leading to poor generalization and underfitting issues. We identify that the root cause lies in the restricted expressiveness of existing weighted-sum mechanisms, both within and outside the convex cone of LoRA representations. This motivates us to propose RadarGate, a novel geometrically inspired gating method that introduces rotational operations of LoRAs representations to boost the expressiveness and facilitate richer feature interactions among multiple LoRAs for scalable LLMs. Specifically, we first fuse each LoRA representation to other LoRAs using a learnable component and then feed the output to a rotation matrix. This matrix involves learnable parameters that define the relative angular relationship between LoRA representations. Such a simple yet effective mechanism provides an extra degree of freedom, facilitating the learning of cross-LoRA synergies and properly tracking the challenging poor generalization and underfitting issues as the number of LoRA grows. Extensive experiments on 6 public benchmarks across 21 tasks show the effectiveness of our RadarGate for scaling LoRAs. We also provide valuable insights, revealing that the rotations to each pair of representations are contrastive, encouraging closer alignment of semantically similar representations during geometrical transformation while pushing distance ones further apart. We will release our code to the community.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Effective Receptive Field of Message-Passing Neural Networks</title>
<link>https://arxiv.org/abs/2505.23185</link>
<guid>https://arxiv.org/abs/2505.23185</guid>
<content:encoded><![CDATA[
arXiv:2505.23185v1 Announce Type: new 
Abstract: Message-Passing Neural Networks (MPNNs) have become a cornerstone for processing and analyzing graph-structured data. However, their effectiveness is often hindered by phenomena such as over-squashing, where long-range dependencies or interactions are inadequately captured and expressed in the MPNN output. This limitation mirrors the challenges of the Effective Receptive Field (ERF) in Convolutional Neural Networks (CNNs), where the theoretical receptive field is underutilized in practice. In this work, we show and theoretically explain the limited ERF problem in MPNNs. Furthermore, inspired by recent advances in ERF augmentation for CNNs, we propose an Interleaved Multiscale Message-Passing Neural Networks (IM-MPNN) architecture to address these problems in MPNNs. Our method incorporates a hierarchical coarsening of the graph, enabling message-passing across multiscale representations and facilitating long-range interactions without excessive depth or parameterization. Through extensive evaluations on benchmarks such as the Long-Range Graph Benchmark (LRGB), we demonstrate substantial improvements over baseline MPNNs in capturing long-range dependencies while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer</title>
<link>https://arxiv.org/abs/2505.23190</link>
<guid>https://arxiv.org/abs/2505.23190</guid>
<content:encoded><![CDATA[
arXiv:2505.23190v1 Announce Type: new 
Abstract: In this study, we propose a novel neural network approach, termed DeepRTE, to address the steady-state Radiative Transfer Equation (RTE). The RTE is a differential-integral equation that governs the propagation of radiation through a participating medium, with applications spanning diverse domains such as neutron transport, atmospheric radiative transfer, heat transfer, and optical imaging. Our proposed DeepRTE framework leverages pre-trained attention-based neural networks to solve the RTE with high accuracy and computational efficiency. The efficacy of the proposed approach is substantiated through comprehensive numerical experiments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics</title>
<link>https://arxiv.org/abs/2505.23194</link>
<guid>https://arxiv.org/abs/2505.23194</guid>
<content:encoded><![CDATA[
arXiv:2505.23194v1 Announce Type: new 
Abstract: Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning method. In standard LoRA layers, one of the matrices, $A$ or $B$, is initialized to zero, ensuring that fine-tuning starts from the pretrained model. However, there is no theoretical support for this practice. In this paper, we investigate the impact of non-zero initialization on LoRA's fine-tuning dynamics from an infinite-width perspective. Our analysis reveals that, compared to zero initialization, simultaneously initializing $A$ and $B$ to non-zero values improves LoRA's robustness to suboptimal learning rates, particularly smaller ones. Further analysis indicates that although the non-zero initialization of $AB$ introduces random noise into the pretrained weight, it generally does not affect fine-tuning performance. In other words, fine-tuning does not need to strictly start from the pretrained model. The validity of our findings is confirmed through extensive experiments across various models and datasets. The code is available at https://github.com/Leopold1423/non_zero_lora-icml25.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning</title>
<link>https://arxiv.org/abs/2505.23195</link>
<guid>https://arxiv.org/abs/2505.23195</guid>
<content:encoded><![CDATA[
arXiv:2505.23195v1 Announce Type: new 
Abstract: Scaling laws motivate the development of Time Series Foundation Models (TSFMs) that pre-train vast parameters and achieve remarkable zero-shot forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot consistently outperform smaller, specialized models trained on full-shot downstream data. A key question is how to realize effective adaptation of TSFMs for a target forecasting task. Through empirical studies on various TSFMs, the pre-trained models often exhibit inherent sparsity and redundancy in computation, suggesting that TSFMs have learned to activate task-relevant network substructures to accommodate diverse forecasting tasks. To preserve this valuable prior knowledge, we propose a structured pruning method to regularize the subsequent fine-tuning process by focusing it on a more relevant and compact parameter space. Extensive experiments on seven TSFMs and six benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly improves forecasting performance compared to fine-tuning original models. This "prune-then-finetune" paradigm often enables TSFMs to achieve state-of-the-art performance and surpass strong specialized baselines.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Daunce: Data Attribution through Uncertainty Estimation</title>
<link>https://arxiv.org/abs/2505.23223</link>
<guid>https://arxiv.org/abs/2505.23223</guid>
<content:encoded><![CDATA[
arXiv:2505.23223v1 Announce Type: new 
Abstract: Training data attribution (TDA) methods aim to identify which training examples influence a model's predictions on specific test data most. By quantifying these influences, TDA supports critical applications such as data debugging, curation, and valuation. Gradient-based TDA methods rely on gradients and second-order information, limiting their applicability at scale. While recent random projection-based methods improve scalability, they often suffer from degraded attribution accuracy. Motivated by connections between uncertainty and influence functions, we introduce Daunce - a simple yet effective data attribution approach through uncertainty estimation. Our method operates by fine-tuning a collection of perturbed models and computing the covariance of per-example losses across these models as the attribution score. Daunce is scalable to large language models (LLMs) and achieves more accurate attribution compared to existing TDA methods. We validate Daunce on tasks ranging from vision tasks to LLM fine-tuning, and further demonstrate its compatibility with black-box model access. Applied to OpenAI's GPT models, our method achieves, to our knowledge, the first instance of data attribution on proprietary LLMs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizability vs. Counterfactual Explainability Trade-Off</title>
<link>https://arxiv.org/abs/2505.23225</link>
<guid>https://arxiv.org/abs/2505.23225</guid>
<content:encoded><![CDATA[
arXiv:2505.23225v1 Announce Type: new 
Abstract: In this work, we investigate the relationship between model generalization and counterfactual explainability in supervised learning. We introduce the notion of $\varepsilon$-valid counterfactual probability ($\varepsilon$-VCP) -- the probability of finding perturbations of a data point within its $\varepsilon$-neighborhood that result in a label change. We provide a theoretical analysis of $\varepsilon$-VCP in relation to the geometry of the model's decision boundary, showing that $\varepsilon$-VCP tends to increase with model overfitting. Our findings establish a rigorous connection between poor generalization and the ease of counterfactual generation, revealing an inherent trade-off between generalization and counterfactual explainability. Empirical results validate our theory, suggesting $\varepsilon$-VCP as a practical proxy for quantitatively characterizing overfitting.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method</title>
<link>https://arxiv.org/abs/2505.23228</link>
<guid>https://arxiv.org/abs/2505.23228</guid>
<content:encoded><![CDATA[
arXiv:2505.23228v1 Announce Type: new 
Abstract: The rapid growth in feature dimension may introduce implicit associations between features and labels in multi-label datasets, making the relationships between features and labels increasingly complex. Moreover, existing methods often adopt low-dimensional linear decomposition to explore the associations between features and labels. However, linear decomposition struggles to capture complex nonlinear associations and may lead to misalignment between the feature space and the label space. To address these two critical challenges, we propose innovative solutions. First, we design a random walk graph that integrates feature-feature, label-label, and feature-label relationships to accurately capture nonlinear and implicit indirect associations, while optimizing the latent representations of associations between features and labels after low-rank decomposition. Second, we align the variable spaces by leveraging low-dimensional representation coefficients, while preserving the manifold structure between the original high-dimensional multi-label data and the low-dimensional representation space. Extensive experiments and ablation studies conducted on seven benchmark datasets and three representative datasets using various evaluation metrics demonstrate the superiority of the proposed method\footnote{Code: https://github.com/Heilong623/-GRW-}.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivalence of stochastic and deterministic policy gradients</title>
<link>https://arxiv.org/abs/2505.23244</link>
<guid>https://arxiv.org/abs/2505.23244</guid>
<content:encoded><![CDATA[
arXiv:2505.23244v1 Announce Type: new 
Abstract: Policy gradients in continuous control have been derived for both stochastic and deterministic policies. Here we study the relationship between the two. In a widely-used family of MDPs involving Gaussian control noise and quadratic control costs, we show that the stochastic and deterministic policy gradients, natural gradients, and state value functions are identical; while the state-control value functions are different. We then develop a general procedure for constructing an MDP with deterministic policy that is equivalent to a given MDP with stochastic policy. The controls of this new MDP are the sufficient statistics of the stochastic policy in the original MDP. Our results suggest that policy gradient methods can be unified by approximating state value functions rather than state-control value functions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Participant Contributions in Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2505.23246</link>
<guid>https://arxiv.org/abs/2505.23246</guid>
<content:encoded><![CDATA[
arXiv:2505.23246v1 Announce Type: new 
Abstract: Federated learning (FL) enables multiple clients to collaboratively train models without sharing their data. Measuring participant contributions in FL is crucial for incentivizing clients and ensuring transparency. While various methods have been proposed for contribution measurement, they are designed exclusively for centralized federated learning (CFL), where a central server collects and aggregates client models, along with evaluating their contributions. Meanwhile, decentralized federated learning (DFL), in which clients exchange models directly without a central server, has gained significant attention for mitigating communication bottlenecks and eliminating a single point of failure. However, applying existing contribution measurement methods to DFL is challenging due to the presence of multiple global models and the absence of a central server. In this study, we present novel methodologies for measuring participant contributions in DFL. We first propose DFL-Shapley, an extension of the Shapley value tailored for DFL, adapting this widely used CFL metric to decentralized settings. Given the impracticality of computing the ideal DFL-Shapley in real-world systems, we introduce DFL-MR, a computable approximation that estimates overall contributions by accumulating round-wise Shapley values. We evaluate DFL-Shapley and DFL-MR across various FL scenarios and compare them with existing CFL metrics. The experimental results confirm DFL-Shapley as a valid ground-truth metric and demonstrate DFL-MR's proximity to DFL-Shapley across various settings, highlighting their effectiveness as contribution metrics in DFL.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating RLHF Training with Reward Variance Increase</title>
<link>https://arxiv.org/abs/2505.23247</link>
<guid>https://arxiv.org/abs/2505.23247</guid>
<content:encoded><![CDATA[
arXiv:2505.23247v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique for ensuring that large language models (LLMs) are aligned with human values and preferences during the post-training phase. As an effective RLHF approach, group relative policy optimization (GRPO) has demonstrated success in many LLM-based applications. However, efficient GRPO-based RLHF training remains a challenge. Recent studies reveal that a higher reward variance of the initial policy model leads to faster RLHF training. Inspired by this finding, we propose a practical reward adjustment model to accelerate RLHF training by provably increasing the reward variance and preserving the relative preferences and reward expectation. Our reward adjustment method inherently poses a nonconvex optimization problem, which is NP-hard to solve in general. To overcome the computational challenges, we design a novel $O(n \log n)$ algorithm to find a global solution of the nonconvex reward adjustment model by explicitly characterizing the extreme points of the feasible set. As an important application, we naturally integrate this reward adjustment model into the GRPO algorithm, leading to a more efficient GRPO with reward variance increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we provide an indirect explanation for the empirical effectiveness of GRPO with rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment results demonstrate that the GRPOVI algorithm can significantly improve the RLHF training efficiency compared to the original GRPO algorithm.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiently Access Diffusion Fisher: Within the Outer Product Span Space</title>
<link>https://arxiv.org/abs/2505.23264</link>
<guid>https://arxiv.org/abs/2505.23264</guid>
<content:encoded><![CDATA[
arXiv:2505.23264v1 Announce Type: new 
Abstract: Recent Diffusion models (DMs) advancements have explored incorporating the second-order diffusion Fisher information (DF), defined as the negative Hessian of log density, into various downstream tasks and theoretical analysis. However, current practices typically approximate the diffusion Fisher by applying auto-differentiation to the learned score network. This black-box method, though straightforward, lacks any accuracy guarantee and is time-consuming. In this paper, we show that the diffusion Fisher actually resides within a space spanned by the outer products of score and initial data. Based on the outer-product structure, we develop two efficient approximation algorithms to access the trace and matrix-vector multiplication of DF, respectively. These algorithms bypass the auto-differentiation operations with time-efficient vector-product calculations. Furthermore, we establish the approximation error bounds for the proposed algorithms. Experiments in likelihood evaluation and adjoint optimization demonstrate the superior accuracy and reduced computational cost of our proposed algorithms. Additionally, based on the novel outer-product formulation of DF, we design the first numerical verification experiment for the optimal transport property of the general PF-ODE deduced map.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs</title>
<link>https://arxiv.org/abs/2505.23270</link>
<guid>https://arxiv.org/abs/2505.23270</guid>
<content:encoded><![CDATA[
arXiv:2505.23270v1 Announce Type: new 
Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable advancements, drawing significant attention from the research community. Their capabilities are largely attributed to large-scale architectures, which require extensive training on massive datasets. However, such datasets often contain sensitive or copyrighted content sourced from the public internet, raising concerns about data privacy and ownership. Regulatory frameworks, such as the General Data Protection Regulation (GDPR), grant individuals the right to request the removal of such sensitive information. This has motivated the development of machine unlearning algorithms that aim to remove specific knowledge from models without the need for costly retraining. Despite these advancements, evaluating the efficacy of unlearning algorithms remains a challenge due to the inherent complexity and generative nature of LLMs. In this work, we introduce a comprehensive auditing framework for unlearning evaluation, comprising three benchmark datasets, six unlearning algorithms, and five prompt-based auditing methods. By using various auditing algorithms, we evaluate the effectiveness and robustness of different unlearning strategies. To explore alternatives beyond prompt-based auditing, we propose a novel technique that leverages intermediate activation perturbations, addressing the limitations of auditing methods that rely solely on model inputs and outputs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of the Land Use and Land Cover Changes in Different Governorates of Oman using Spatiotemporal Multi-spectral Satellite Data</title>
<link>https://arxiv.org/abs/2505.23285</link>
<guid>https://arxiv.org/abs/2505.23285</guid>
<content:encoded><![CDATA[
arXiv:2505.23285v1 Announce Type: new 
Abstract: Land cover and land use (LULC) changes are key applications of satellite imagery, and they have critical roles in resource management, urbanization, protection of soils and the environment, and enhancing sustainable development. The literature has heavily utilized multispectral spatiotemporal satellite data alongside advanced machine learning algorithms to monitor and predict LULC changes. This study analyzes and compares LULC changes across various governorates (provinces) of the Sultanate of Oman from 2016 to 2021 using annual time steps. For the chosen region, multispectral spatiotemporal data were acquired from the open-source Sentinel-2 satellite dataset. Supervised machine learning algorithms were used to train and classify different land covers, such as water bodies, crops, urban, etc. The constructed model was subsequently applied within the study region, allowing for an effective comparative evaluation of LULC changes within the given timeframe.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score-based Generative Modeling for Conditional Independence Testing</title>
<link>https://arxiv.org/abs/2505.23309</link>
<guid>https://arxiv.org/abs/2505.23309</guid>
<content:encoded><![CDATA[
arXiv:2505.23309v1 Announce Type: new 
Abstract: Determining conditional independence (CI) relationships between random variables is a fundamental yet challenging task in machine learning and statistics, especially in high-dimensional settings. Existing generative model-based CI testing methods, such as those utilizing generative adversarial networks (GANs), often struggle with undesirable modeling of conditional distributions and training instability, resulting in subpar performance. To address these issues, we propose a novel CI testing method via score-based generative modeling, which achieves precise Type I error control and strong testing power. Concretely, we first employ a sliced conditional score matching scheme to accurately estimate conditional score and use Langevin dynamics conditional sampling to generate null hypothesis samples, ensuring precise Type I error control. Then, we incorporate a goodness-of-fit stage into the method to verify generated samples and enhance interpretability in practice. We theoretically establish the error bound of conditional distributions modeled by score-based generative models and prove the validity of our CI tests. Extensive experiments on both synthetic and real-world datasets show that our method significantly outperforms existing state-of-the-art methods, providing a promising way to revitalize generative model-based CI testing.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Parameter Estimation for Bayesian Network Classifiers using Hierarchical Linear Smoothing</title>
<link>https://arxiv.org/abs/2505.23320</link>
<guid>https://arxiv.org/abs/2505.23320</guid>
<content:encoded><![CDATA[
arXiv:2505.23320v1 Announce Type: new 
Abstract: Bayesian network classifiers (BNCs) possess a number of properties desirable for a modern classifier: They are easily interpretable, highly scalable, and offer adaptable complexity. However, traditional methods for learning BNCs have historically underperformed when compared to leading classification methods such as random forests. Recent parameter smoothing techniques using hierarchical Dirichlet processes (HDPs) have enabled BNCs to achieve performance competitive with random forests on categorical data, but these techniques are relatively inflexible, and require a complicated, specialized sampling process. In this paper, we introduce a novel method for parameter estimation that uses a log-linear regression to approximate the behaviour of HDPs. As a linear model, our method is remarkably flexible and simple to interpret, and can leverage the vast literature on learning linear models. Our experiments show that our method can outperform HDP smoothing while being orders of magnitude faster, remaining competitive with random forests on categorical data.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X2Graph for Cancer Subtyping Prediction on Biological Tabular Data</title>
<link>https://arxiv.org/abs/2505.23334</link>
<guid>https://arxiv.org/abs/2505.23334</guid>
<content:encoded><![CDATA[
arXiv:2505.23334v1 Announce Type: new 
Abstract: Despite the transformative impact of deep learning on text, audio, and image datasets, its dominance in tabular data, especially in the medical domain where data are often scarce, remains less clear. In this paper, we propose X2Graph, a novel deep learning method that achieves strong performance on small biological tabular datasets. X2Graph leverages external knowledge about the relationships between table columns, such as gene interactions, to convert each sample into a graph structure. This transformation enables the application of standard message passing algorithms for graph modeling. Our X2Graph method demonstrates superior performance compared to existing tree-based and deep learning methods across three cancer subtyping datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matryoshka Model Learning for Improved Elastic Student Models</title>
<link>https://arxiv.org/abs/2505.23337</link>
<guid>https://arxiv.org/abs/2505.23337</guid>
<content:encoded><![CDATA[
arXiv:2505.23337v1 Announce Type: new 
Abstract: Industry-grade ML models are carefully designed to meet rapidly evolving serving constraints, which requires significant resources for model development. In this paper, we propose MatTA, a framework for training multiple accurate Student models using a novel Teacher-TA-Student recipe. TA models are larger versions of the Student models with higher capacity, and thus allow Student models to better relate to the Teacher model and also bring in more domain-specific expertise. Furthermore, multiple accurate Student models can be extracted from the TA model. Therefore, despite only one training run, our methodology provides multiple servable options to trade off accuracy for lower serving cost. We demonstrate the proposed method, MatTA, on proprietary datasets and models. Its practical efficacy is underscored by live A/B tests within a production ML system, demonstrating 20% improvement on a key metric. We also demonstrate our method on GPT-2 Medium, a public model, and achieve relative improvements of over 24% on SAT Math and over 10% on the LAMBADA benchmark.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Positional Autoencoders as Self-supervised Learners</title>
<link>https://arxiv.org/abs/2505.23345</link>
<guid>https://arxiv.org/abs/2505.23345</guid>
<content:encoded><![CDATA[
arXiv:2505.23345v1 Announce Type: new 
Abstract: Graph self-supervised learning seeks to learn effective graph representations without relying on labeled data. Among various approaches, graph autoencoders (GAEs) have gained significant attention for their efficiency and scalability. Typically, GAEs take incomplete graphs as input and predict missing elements, such as masked nodes or edges. While effective, our experimental investigation reveals that traditional node or edge masking paradigms primarily capture low-frequency signals in the graph and fail to learn the expressive structural information. To address these issues, we propose Graph Positional Autoencoders (GraphPAE), which employs a dual-path architecture to reconstruct both node features and positions. Specifically, the feature path uses positional encoding to enhance the message-passing processing, improving GAE's ability to predict the corrupted information. The position path, on the other hand, leverages node representations to refine positions and approximate eigenvectors, thereby enabling the encoder to learn diverse frequency information. We conduct extensive experiments to verify the effectiveness of GraphPAE, including heterophilic node classification, graph property prediction, and transfer learning. The results demonstrate that GraphPAE achieves state-of-the-art performance and consistently outperforms baselines by a large margin.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentinel: Scheduling Live Streams with Proactive Anomaly Detection in Crowdsourced Cloud-Edge Platforms</title>
<link>https://arxiv.org/abs/2505.23347</link>
<guid>https://arxiv.org/abs/2505.23347</guid>
<content:encoded><![CDATA[
arXiv:2505.23347v1 Announce Type: new 
Abstract: With the rapid growth of live streaming services, Crowdsourced Cloud-edge service Platforms (CCPs) are playing an increasingly important role in meeting the increasing demand. Although stream scheduling plays a critical role in optimizing CCPs' revenue, most optimization strategies struggle to achieve practical results due to various anomalies in unstable CCPs. Additionally, the substantial scale of CCPs magnifies the difficulties of anomaly detection in time-sensitive scheduling. To tackle these challenges, this paper proposes Sentinel, a proactive anomaly detection-based scheduling framework. Sentinel models the scheduling process as a two-stage Pre-Post-Scheduling paradigm: in the pre-scheduling stage, Sentinel conducts anomaly detection and constructs a strategy pool; in the post-scheduling stage, upon request arrival, it triggers an appropriate scheduling based on a pre-generated strategy to implement the scheduling process. Extensive experiments on realistic datasets show that Sentinel significantly reduces anomaly frequency by 70%, improves revenue by 74%, and doubles the scheduling speed.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reward Fairness in RLHF: From a Resource Allocation Perspective</title>
<link>https://arxiv.org/abs/2505.23349</link>
<guid>https://arxiv.org/abs/2505.23349</guid>
<content:encoded><![CDATA[
arXiv:2505.23349v1 Announce Type: new 
Abstract: Rewards serve as proxies for human preferences and play a crucial role in Reinforcement Learning from Human Feedback (RLHF). However, if these rewards are inherently imperfect, exhibiting various biases, they can adversely affect the alignment of large language models (LLMs). In this paper, we collectively define the various biases present in rewards as the problem of reward unfairness. We propose a bias-agnostic method to address the issue of reward fairness from a resource allocation perspective, without specifically designing for each type of bias, yet effectively mitigating them. Specifically, we model preference learning as a resource allocation problem, treating rewards as resources to be allocated while considering the trade-off between utility and fairness in their distribution. We propose two methods, Fairness Regularization and Fairness Coefficient, to achieve fairness in rewards. We apply our methods in both verification and reinforcement learning scenarios to obtain a fairness reward model and a policy model, respectively. Experiments conducted in these scenarios demonstrate that our approach aligns LLMs with human preferences in a more fair manner.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control</title>
<link>https://arxiv.org/abs/2505.23355</link>
<guid>https://arxiv.org/abs/2505.23355</guid>
<content:encoded><![CDATA[
arXiv:2505.23355v1 Announce Type: new 
Abstract: Climate control is crucial for greenhouse production as it directly affects crop growth and resource use. Reinforcement learning (RL) has received increasing attention in this field, but still faces challenges, including limited training efficiency and high reliance on initial learning conditions. Interactive RL, which combines human (grower) input with the RL agent's learning, offers a potential solution to overcome these challenges. However, interactive RL has not yet been applied to greenhouse climate control and may face challenges related to imperfect inputs. Therefore, this paper aims to explore the possibility and performance of applying interactive RL with imperfect inputs into greenhouse climate control, by: (1) developing three representative interactive RL algorithms tailored for greenhouse climate control (reward shaping, policy shaping and control sharing); (2) analyzing how input characteristics are often contradicting, and how the trade-offs between them make grower's inputs difficult to perfect; (3) proposing a neural network-based approach to enhance the robustness of interactive RL agents under limited input availability; (4) conducting a comprehensive evaluation of the three interactive RL algorithms with imperfect inputs in a simulated greenhouse environment. The demonstration shows that interactive RL incorporating imperfect grower inputs has the potential to improve the performance of the RL agent. RL algorithms that influence action selection, such as policy shaping and control sharing, perform better when dealing with imperfect inputs, achieving 8.4% and 6.8% improvement in profit, respectively. In contrast, reward shaping, an algorithm that manipulates the reward function, is sensitive to imperfect inputs and leads to a 9.4% decrease in profit. This highlights the importance of selecting an appropriate mechanism when incorporating imperfect inputs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Spectral Backpropagation for Efficient Neural Network Training</title>
<link>https://arxiv.org/abs/2505.23369</link>
<guid>https://arxiv.org/abs/2505.23369</guid>
<content:encoded><![CDATA[
arXiv:2505.23369v1 Announce Type: new 
Abstract: Dynamic Spectral Backpropagation (DSBP) enhances neural network training under resource constraints by projecting gradients onto principal eigenvectors, reducing complexity and promoting flat minima. Five extensions are proposed, dynamic spectral inference, spectral architecture optimization, spectral meta learning, spectral transfer regularization, and Lie algebra inspired dynamics, to address challenges in robustness, fewshot learning, and hardware efficiency. Supported by a third order stochastic differential equation (SDE) and a PAC Bayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank Adaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion MNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive experiments and visualizations. Future work focuses on scalability, bias mitigation, and ethical considerations.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models</title>
<link>https://arxiv.org/abs/2505.23378</link>
<guid>https://arxiv.org/abs/2505.23378</guid>
<content:encoded><![CDATA[
arXiv:2505.23378v1 Announce Type: new 
Abstract: Speaker-dependent modelling can substantially improve performance in speech-based health monitoring applications. While mixed-effect models are commonly used for such speaker adaptation, they require computationally expensive retraining for each new observation, making them impractical in a production environment. We reformulate this task as a meta-learning problem and explore three approaches of increasing complexity: ensemble-based distance models, prototypical networks, and transformer-based sequence models. Using pre-trained speech embeddings, we evaluate these methods on a large longitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting time since sleep from speech as a function of fatigue, a symptom commonly associated with ill-health. Our results demonstrate that all meta-learning approaches tested outperformed both cross-sectional and conventional mixed-effects models, with a transformer-based method achieving the strongest performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Modeling Method for Pathloss Model Discovery</title>
<link>https://arxiv.org/abs/2505.23383</link>
<guid>https://arxiv.org/abs/2505.23383</guid>
<content:encoded><![CDATA[
arXiv:2505.23383v1 Announce Type: new 
Abstract: Modeling propagation is the cornerstone for designing and optimizing next-generation wireless systems, with a particular emphasis on 5G and beyond era. Traditional modeling methods have long relied on statistic-based techniques to characterize propagation behavior across different environments. With the expansion of wireless communication systems, there is a growing demand for methods that guarantee the accuracy and interoperability of modeling. Artificial intelligence (AI)-based techniques, in particular, are increasingly being adopted to overcome this challenge, although the interpretability is not assured with most of these methods. Inspired by recent advancements in AI, this paper proposes a novel approach that accelerates the discovery of path loss models while maintaining interpretability. The proposed method automates the model formulation, evaluation, and refinement, facilitating model discovery. We evaluate two techniques: one based on Deep Symbolic Regression, offering full interpretability, and the second based on Kolmogorov-Arnold Networks, providing two levels of interpretability. Both approaches are evaluated on two synthetic and two real-world datasets. Our results show that Kolmogorov-Arnold Networks achieve R^2 values close to 1 with minimal prediction error, while Deep Symbolic Regression generates compact models with moderate accuracy. Moreover, on the selected examples, we demonstrate that automated methods outperform traditional methods, achieving up to 75% reduction in prediction errors, offering accurate and explainable solutions with potential to increase the efficiency of discovering next-generation path loss models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Buffer-free Class-Incremental Learning with Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2505.23412</link>
<guid>https://arxiv.org/abs/2505.23412</guid>
<content:encoded><![CDATA[
arXiv:2505.23412v1 Announce Type: new 
Abstract: Class-incremental learning (CIL) poses significant challenges in open-world scenarios, where models must not only learn new classes over time without forgetting previous ones but also handle inputs from unknown classes that a closed-set model would misclassify. Recent works address both issues by (i)~training multi-head models using the task-incremental learning framework, and (ii) predicting the task identity employing out-of-distribution (OOD) detectors. While effective, the latter mainly relies on joint training with a memory buffer of past data, raising concerns around privacy, scalability, and increased training time. In this paper, we present an in-depth analysis of post-hoc OOD detection methods and investigate their potential to eliminate the need for a memory buffer. We uncover that these methods, when applied appropriately at inference time, can serve as a strong substitute for buffer-based OOD detection. We show that this buffer-free approach achieves comparable or superior performance to buffer-based methods both in terms of class-incremental learning and the rejection of unknown samples. Experimental results on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings, offering new insights into the design of efficient and privacy-preserving CIL systems for open-world settings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional predictive coding</title>
<link>https://arxiv.org/abs/2505.23415</link>
<guid>https://arxiv.org/abs/2505.23415</guid>
<content:encoded><![CDATA[
arXiv:2505.23415v1 Announce Type: new 
Abstract: Predictive coding (PC) is an influential computational model of visual learning and inference in the brain. Classical PC was proposed as a top-down generative model, where the brain actively predicts upcoming visual inputs, and inference minimises the prediction errors. Recent studies have also shown that PC can be formulated as a discriminative model, where sensory inputs predict neural activities in a feedforward manner. However, experimental evidence suggests that the brain employs both generative and discriminative inference, while unidirectional PC models show degraded performance in tasks requiring bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC model that incorporates both generative and discriminative inference while maintaining a biologically plausible circuit implementation. We show that bPC matches or outperforms unidirectional models in their specialised generative or discriminative tasks, by developing an energy landscape that simultaneously suits both tasks. We also demonstrate bPC's superior performance in two biologically relevant tasks including multimodal learning and inference with missing information, suggesting that bPC resembles biological visual inference more closely.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses</title>
<link>https://arxiv.org/abs/2505.23421</link>
<guid>https://arxiv.org/abs/2505.23421</guid>
<content:encoded><![CDATA[
arXiv:2505.23421v1 Announce Type: new 
Abstract: In China's competitive fresh e-commerce market, optimizing operational strategies, especially inventory management in front-end warehouses, is key to enhance customer satisfaction and to gain a competitive edge. Front-end warehouses are placed in residential areas to ensure the timely delivery of fresh goods and are usually in small size. This brings the challenge of deciding which goods to stock and in what quantities, taking into account capacity constraints. To address this issue, traditional predict-then-optimize (PTO) methods that predict sales and then decide on inventory often don't align prediction with inventory goals, as well as fail to prioritize consumer satisfaction. This paper proposes a multi-task Optimize-then-Predict-then-Optimize (OTPTO) approach that jointly optimizes product selection and inventory management, aiming to increase consumer satisfaction by maximizing the full order fulfillment rate. Our method employs a 0-1 mixed integer programming model OM1 to determine historically optimal inventory levels, and then uses a product selection model PM1 and the stocking model PM2 for prediction. The combined results are further refined through a post-processing algorithm OM2. Experimental results from JD.com's 7Fresh platform demonstrate the robustness and significant advantages of our OTPTO method. Compared to the PTO approach, our OTPTO method substantially enhances the full order fulfillment rate by 4.34% (a relative increase of 7.05%) and narrows the gap to the optimal full order fulfillment rate by 5.27%. These findings substantiate the efficacy of the OTPTO method in managing inventory at front-end warehouses of fresh e-commerce platforms and provide valuable insights for future research in this domain.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced DACER Algorithm with High Diffusion Efficiency</title>
<link>https://arxiv.org/abs/2505.23426</link>
<guid>https://arxiv.org/abs/2505.23426</guid>
<content:encoded><![CDATA[
arXiv:2505.23426v1 Announce Type: new 
Abstract: Due to their expressive capacity, diffusion models have shown great promise in offline RL and imitation learning. Diffusion Actor-Critic with Entropy Regulator (DACER) extended this capability to online RL by using the reverse diffusion process as a policy approximator, trained end-to-end with policy gradient methods, achieving strong performance. However, this comes at the cost of requiring many diffusion steps, which significantly hampers training efficiency, while directly reducing the steps leads to noticeable performance degradation. Critically, the lack of inference efficiency becomes a significant bottleneck for applying diffusion policies in real-time online RL settings. To improve training and inference efficiency while maintaining or even enhancing performance, we propose a Q-gradient field objective as an auxiliary optimization target to guide the denoising process at each diffusion step. Nonetheless, we observe that the independence of the Q-gradient field from the diffusion time step negatively impacts the performance of the diffusion policy. To address this, we introduce a temporal weighting mechanism that enables the model to efficiently eliminate large-scale noise in the early stages and refine actions in the later stages. Experimental results on MuJoCo benchmarks and several multimodal tasks demonstrate that the DACER2 algorithm achieves state-of-the-art performance in most MuJoCo control tasks with only five diffusion steps, while also exhibiting stronger multimodality compared to DACER.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Validity of Head Motion Patterns as Generalisable Depression Biomarkers</title>
<link>https://arxiv.org/abs/2505.23427</link>
<guid>https://arxiv.org/abs/2505.23427</guid>
<content:encoded><![CDATA[
arXiv:2505.23427v1 Announce Type: new 
Abstract: Depression is a debilitating mood disorder negatively impacting millions worldwide. While researchers have explored multiple verbal and non-verbal behavioural cues for automated depression assessment, head motion has received little attention thus far. Further, the common practice of validating machine learning models via a single dataset can limit model generalisability. This work examines the effectiveness and generalisability of models utilising elementary head motion units, termed kinemes, for depression severity estimation. Specifically, we consider three depression datasets from different western cultures (German: AVEC2013, Australian: Blackdog and American: Pitt datasets) with varied contextual and recording settings to investigate the generalisability of the derived kineme patterns via two methods: (i) k-fold cross-validation over individual/multiple datasets, and (ii) model reuse on other datasets. Evaluating classification and regression performance with classical machine learning methods, our results show that: (1) head motion patterns are efficient biomarkers for estimating depression severity, achieving highly competitive performance for both classification and regression tasks on a variety of datasets, including achieving the second best Mean Absolute Error (MAE) on the AVEC2013 dataset, and (2) kineme-based features are more generalisable than (a) raw head motion descriptors for binary severity classification, and (b) other visual behavioural cues for severity estimation (regression).
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity-Aware Policy Optimization for Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.23433</link>
<guid>https://arxiv.org/abs/2505.23433</guid>
<content:encoded><![CDATA[
arXiv:2505.23433v1 Announce Type: new 
Abstract: The reasoning capabilities of large language models (LLMs) have advanced rapidly, particularly following the release of DeepSeek R1, which has inspired a surge of research into data quality and reinforcement learning (RL) algorithms. Despite the pivotal role diversity plays in RL, its influence on LLM reasoning remains largely underexplored. To bridge this gap, this work presents a systematic investigation into the impact of diversity in RL-based training for LLM reasoning, and proposes a novel diversity-aware policy optimization method. Across evaluations on 12 LLMs, we observe a strong positive correlation between the solution diversity and Potential at k (a novel metric quantifying an LLM's reasoning potential) in high-performing models. This finding motivates our method to explicitly promote diversity during RL training. Specifically, we design a token-level diversity and reformulate it into a practical objective, then we selectively apply it to positive samples. Integrated into the R1-zero training framework, our method achieves a 3.5 percent average improvement across four mathematical reasoning benchmarks, while generating more diverse and robust solutions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounded-Abstention Pairwise Learning to Rank</title>
<link>https://arxiv.org/abs/2505.23437</link>
<guid>https://arxiv.org/abs/2505.23437</guid>
<content:encoded><![CDATA[
arXiv:2505.23437v1 Announce Type: new 
Abstract: Ranking systems influence decision-making in high-stakes domains like health, education, and employment, where they can have substantial economic and social impacts. This makes the integration of safety mechanisms essential. One such mechanism is $\textit{abstention}$, which enables algorithmic decision-making system to defer uncertain or low-confidence decisions to human experts. While abstention have been predominantly explored in the context of classification tasks, its application to other machine learning paradigms remains underexplored. In this paper, we introduce a novel method for abstention in pairwise learning-to-rank tasks. Our approach is based on thresholding the ranker's conditional risk: the system abstains from making a decision when the estimated risk exceeds a predefined threshold. Our contributions are threefold: a theoretical characterization of the optimal abstention strategy, a model-agnostic, plug-in algorithm for constructing abstaining ranking models, and a comprehensive empirical evaluations across multiple datasets, demonstrating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Regularization Methods for Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2505.23442</link>
<guid>https://arxiv.org/abs/2505.23442</guid>
<content:encoded><![CDATA[
arXiv:2505.23442v1 Announce Type: new 
Abstract: Knowledge graph completion (KGC) has attracted considerable attention in recent years because it is critical to improving the quality of knowledge graphs. Researchers have continuously explored various models. However, most previous efforts have neglected to take advantage of regularization from a deeper perspective and therefore have not been used to their full potential. This paper rethinks the application of regularization methods in KGC. Through extensive empirical studies on various KGC models, we find that carefully designed regularization not only alleviates overfitting and reduces variance but also enables these models to break through the upper bounds of their original performance. Furthermore, we introduce a novel sparse-regularization method that embeds the concept of rank-based selective sparsity into the KGC regularizer. The core idea is to selectively penalize those components with significant features in the embedding vector, thus effectively ignoring many components that contribute little and may only represent noise. Various comparative experiments on multiple datasets and multiple models show that the SPR regularization method is better than other regularization methods and can enable the KGC model to further break through the performance margin.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Classification with Non-Linear Classifiers</title>
<link>https://arxiv.org/abs/2505.23443</link>
<guid>https://arxiv.org/abs/2505.23443</guid>
<content:encoded><![CDATA[
arXiv:2505.23443v1 Announce Type: new 
Abstract: In strategic classification, the standard supervised learning setting is extended to support the notion of strategic user behavior in the form of costly feature manipulations made in response to a classifier. While standard learning supports a broad range of model classes, the study of strategic classification has, so far, been dedicated mostly to linear classifiers. This work aims to expand the horizon by exploring how strategic behavior manifests under non-linear classifiers and what this implies for learning. We take a bottom-up approach showing how non-linearity affects decision boundary points, classifier expressivity, and model classes complexity. A key finding is that universal approximators (e.g., neural nets) are no longer universal once the environment is strategic. We demonstrate empirically how this can create performance gaps even on an unrestricted model class.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Inversion for Uncertainty-Aware Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2505.23448</link>
<guid>https://arxiv.org/abs/2505.23448</guid>
<content:encoded><![CDATA[
arXiv:2505.23448v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection and uncertainty estimation (UE) are critical components for building safe machine learning systems, especially in real-world scenarios where unexpected inputs are inevitable. In this work, we propose a novel framework that combines network inversion with classifier training to simultaneously address both OOD detection and uncertainty estimation. For a standard n-class classification task, we extend the classifier to an (n+1)-class model by introducing a "garbage" class, initially populated with random gaussian noise to represent outlier inputs. After each training epoch, we use network inversion to reconstruct input images corresponding to all output classes that initially appear as noisy and incoherent and are therefore excluded to the garbage class for retraining the classifier. This cycle of training, inversion, and exclusion continues iteratively till the inverted samples begin to resemble the in-distribution data more closely, suggesting that the classifier has learned to carve out meaningful decision boundaries while sanitising the class manifolds by pushing OOD content into the garbage class. During inference, this training scheme enables the model to effectively detect and reject OOD samples by classifying them into the garbage class. Furthermore, the confidence scores associated with each prediction can be used to estimate uncertainty for both in-distribution and OOD inputs. Our approach is scalable, interpretable, and does not require access to external OOD datasets or post-hoc calibration techniques while providing a unified solution to the dual challenges of OOD detection and uncertainty estimation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Guidance Is a Controllable Policy Improvement Operator</title>
<link>https://arxiv.org/abs/2505.23458</link>
<guid>https://arxiv.org/abs/2505.23458</guid>
<content:encoded><![CDATA[
arXiv:2505.23458v1 Announce Type: new 
Abstract: At the core of reinforcement learning is the idea of learning beyond the performance in the data. However, scaling such systems has proven notoriously tricky. In contrast, techniques from generative modeling have proven remarkably scalable and are simple to train. In this work, we combine these strengths, by deriving a direct relation between policy improvement and guidance of diffusion models. The resulting framework, CFGRL, is trained with the simplicity of supervised learning, yet can further improve on the policies in the data. On offline RL tasks, we observe a reliable trend -- increased guidance weighting leads to increased performance. Of particular importance, CFGRL can operate without explicitly learning a value function, allowing us to generalize simple supervised methods (e.g., goal-conditioned behavioral cloning) to further prioritize optimality, gaining performance for "free" across the board.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment</title>
<link>https://arxiv.org/abs/2505.23459</link>
<guid>https://arxiv.org/abs/2505.23459</guid>
<content:encoded><![CDATA[
arXiv:2505.23459v1 Announce Type: new 
Abstract: Ensuring convergence of policy gradient methods in federated reinforcement learning (FRL) under environment heterogeneity remains a major challenge. In this work, we first establish that heterogeneity, perhaps counter-intuitively, can necessitate optimal policies to be non-deterministic or even time-varying, even in tabular environments. Subsequently, we prove global convergence results for federated policy gradient (FedPG) algorithms employing local updates, under a {\L}ojasiewicz condition that holds only for each individual agent, in both entropy-regularized and non-regularized scenarios. Crucially, our theoretical analysis shows that FedPG attains linear speed-up with respect to the number of agents, a property central to efficient federated learning. Leveraging insights from our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient method that employs a carefully constructed softmax-inspired parameterization coupled with an appropriate regularization scheme. We further demonstrate explicit convergence rates for b-RS-FedPG toward near-optimal stationary policies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG consistently outperform federated Q-learning on heterogeneous settings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refining Labeling Functions with Limited Labeled Data</title>
<link>https://arxiv.org/abs/2505.23470</link>
<guid>https://arxiv.org/abs/2505.23470</guid>
<content:encoded><![CDATA[
arXiv:2505.23470v1 Announce Type: new 
Abstract: Programmatic weak supervision (PWS) significantly reduces human effort for labeling data by combining the outputs of user-provided labeling functions (LFs) on unlabeled datapoints. However, the quality of the generated labels depends directly on the accuracy of the LFs. In this work, we study the problem of fixing LFs based on a small set of labeled examples. Towards this goal, we develop novel techniques for repairing a set of LFs by minimally changing their results on the labeled examples such that the fixed LFs ensure that (i) there is sufficient evidence for the correct label of each labeled datapoint and (ii) the accuracy of each repaired LF is sufficiently high. We model LFs as conditional rules which enables us to refine them, i.e., to selectively change their output for some inputs. We demonstrate experimentally that our system improves the quality of LFs based on surprisingly small sets of labeled datapoints.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training</title>
<link>https://arxiv.org/abs/2505.23489</link>
<guid>https://arxiv.org/abs/2505.23489</guid>
<content:encoded><![CDATA[
arXiv:2505.23489v1 Announce Type: new 
Abstract: We present a thermodynamic interpretation of the stationary behavior of stochastic gradient descent (SGD) under fixed learning rates (LRs) in neural network training. We show that SGD implicitly minimizes a free energy function $F=U-TS$, balancing training loss $U$ and the entropy of the weights distribution $S$, with temperature $T$ determined by the LR. This perspective offers a new lens on why high LRs prevent training from converging to the loss minima and how different LRs lead to stabilization at different loss levels. We empirically validate the free energy framework on both underparameterized (UP) and overparameterized (OP) models. UP models consistently follow free energy minimization, with temperature increasing monotonically with LR, while for OP models, the temperature effectively drops to zero at low LRs, causing SGD to minimize the loss directly and converge to an optimum. We attribute this mismatch to differences in the signal-to-noise ratio of stochastic gradients near optima, supported by both a toy example and neural network experiments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Errors of Imperfect Multitask Learners When Distributions Shift</title>
<link>https://arxiv.org/abs/2505.23496</link>
<guid>https://arxiv.org/abs/2505.23496</guid>
<content:encoded><![CDATA[
arXiv:2505.23496v1 Announce Type: new 
Abstract: When data are noisy, a statistical learner's goal is to resolve epistemic uncertainty about the data it will encounter at test-time, i.e., to identify the distribution of test (target) data. Many real-world learning settings introduce sources of epistemic uncertainty that can not be resolved on the basis of training (source) data alone: The source data may arise from multiple tasks (multitask learning), the target data may differ systematically from the source data tasks (distribution shift), and/or the learner may not arrive at an accurate characterization of the source data (imperfect learning). We introduce a principled definition of epistemic error, and provide a generic, decompositional epistemic error bound. Our error bound is the first to (i) consider epistemic error specifically, (ii) accommodate all the sources of epistemic uncertainty above, and (iii) separately attribute the error to each of multiple aspects of the learning procedure and environment. As corollaries of the generic result, we provide (i) epistemic error bounds specialized to the settings of Bayesian transfer learning and distribution shift within $\epsilon$-neighborhoods, and (ii) a set of corresponding generalization bounds. Finally, we provide a novel definition of negative transfer, and validate its insights in a synthetic experimental setting.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty</title>
<link>https://arxiv.org/abs/2505.23506</link>
<guid>https://arxiv.org/abs/2505.23506</guid>
<content:encoded><![CDATA[
arXiv:2505.23506v1 Announce Type: new 
Abstract: In recent years various supervised learning methods that disentangle aleatoric and epistemic uncertainty based on second-order distributions have been proposed. We argue that these methods fail to capture critical components of epistemic uncertainty, particularly due to the often-neglected component of model bias. To show this, we make use of a more fine-grained taxonomy of epistemic uncertainty sources in machine learning models, and analyse how the classical bias-variance decomposition of the expected prediction error can be decomposed into different parts reflecting these uncertainties. By using a simulation-based evaluation protocol which encompasses epistemic uncertainty due to both procedural- and data-driven uncertainty components, we illustrate that current methods rarely capture the full spectrum of epistemic uncertainty. Through theoretical insights and synthetic experiments, we show that high model bias can lead to misleadingly low estimates of epistemic uncertainty, and common second-order uncertainty quantification methods systematically blur bias-induced errors into aleatoric estimates, thereby underrepresenting epistemic uncertainty. Our findings underscore that meaningful aleatoric estimates are feasible only if all relevant sources of epistemic uncertainty are properly represented.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity</title>
<link>https://arxiv.org/abs/2505.23520</link>
<guid>https://arxiv.org/abs/2505.23520</guid>
<content:encoded><![CDATA[
arXiv:2505.23520v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with extended context lengths face significant computational challenges during the pre-filling phase, primarily due to the quadratic complexity of self-attention. Existing methods typically employ dynamic pattern matching and block-sparse low-level implementations. However, their reliance on local information for pattern identification fails to capture global contexts, and the coarse granularity of blocks leads to persistent internal sparsity, resulting in suboptimal accuracy and efficiency. To address these limitations, we propose \textbf{AnchorAttention}, a difference-aware, dynamic sparse attention mechanism that efficiently identifies critical attention regions at a finer stripe granularity while adapting to global contextual information, achieving superior speed and accuracy. AnchorAttention comprises three key components: (1) \textbf{Pattern-based Anchor Computation}, leveraging the commonalities present across all inputs to rapidly compute a set of near-maximum scores as the anchor; (2) \textbf{Difference-aware Stripe Sparsity Identification}, performing difference-aware comparisons with the anchor to quickly obtain discrete coordinates of significant regions in a stripe-like sparsity pattern; (3) \textbf{Fine-grained Sparse Computation}, replacing the traditional contiguous KV block loading approach with simultaneous discrete KV position loading to maximize sparsity rates while preserving full hardware computational potential. With its finer-grained sparsity strategy, \textbf{AnchorAttention} achieves higher sparsity rates at the same recall level, significantly reducing computation time. Compared to previous state-of-the-art methods, at a text length of 128k, it achieves a speedup of 1.44$\times$ while maintaining higher recall rates.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating AllReduce with a Persistent Straggler</title>
<link>https://arxiv.org/abs/2505.23523</link>
<guid>https://arxiv.org/abs/2505.23523</guid>
<content:encoded><![CDATA[
arXiv:2505.23523v1 Announce Type: new 
Abstract: Distributed machine learning workloads use data and tensor parallelism for training and inference, both of which rely on the AllReduce collective to synchronize gradients or activations. However, bulk-synchronous AllReduce algorithms can be delayed by a persistent straggler that is slower to reach the synchronization barrier required to begin the collective. To address this challenge, we propose StragglAR: an AllReduce algorithm that accelerates distributed training and inference in the presence of persistent stragglers. StragglAR implements a ReduceScatter among the remaining GPUs during the straggler-induced delay, and then executes a novel collective algorithm to complete the AllReduce once the straggler reaches the synchronization barrier. StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient AllReduce algorithms (e.g., Ring) for large GPU clusters with persistent stragglers. On an 8-GPU server, our implementation of StragglAR yields a 22% speedup over state-of-the-art AllReduce algorithms.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normalizing Flows are Capable Models for RL</title>
<link>https://arxiv.org/abs/2505.23527</link>
<guid>https://arxiv.org/abs/2505.23527</guid>
<content:encoded><![CDATA[
arXiv:2505.23527v1 Announce Type: new 
Abstract: Modern reinforcement learning (RL) algorithms have found success by using powerful probabilistic models, such as transformers, energy-based models, and diffusion/flow-based models. To this end, RL researchers often choose to pay the price of accommodating these models into their algorithms -- diffusion models are expressive, but are computationally intensive due to their reliance on solving differential equations, while autoregressive transformer models are scalable but typically require learning discrete representations. Normalizing flows (NFs), by contrast, seem to provide an appealing alternative, as they enable likelihoods and sampling without solving differential equations or autoregressive architectures. However, their potential in RL has received limited attention, partly due to the prevailing belief that normalizing flows lack sufficient expressivity. We show that this is not the case. Building on recent work in NFs, we propose a single NF architecture which integrates seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy measure. Our approach leads to much simpler algorithms, and achieves higher performance in imitation learning, offline, goal conditioned RL and unsupervised RL.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images</title>
<link>https://arxiv.org/abs/2505.23528</link>
<guid>https://arxiv.org/abs/2505.23528</guid>
<content:encoded><![CDATA[
arXiv:2505.23528v1 Announce Type: new 
Abstract: The present study performs a comprehensive fairness analysis of machine learning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and Alzheimer's disease (AD) from MRI-derived neuroimaging features. Biases associated with age, race, and gender in a multi-cohort dataset, as well as the influence of proxy features encoding these sensitive attributes, are investigated. The reliability of various fairness definitions and metrics in the identification of such biases is also assessed. Based on the most appropriate fairness measures, a comparative analysis of widely used pre-processing, in-processing, and post-processing bias mitigation strategies is performed. Moreover, a novel composite measure is introduced to quantify the trade-off between fairness and performance by considering the F1-score and the equalized odds ratio, making it appropriate for medical diagnostic applications. The obtained results reveal the existence of biases related to age and race, while no significant gender bias is observed. The deployed mitigation strategies yield varying improvements in terms of fairness across the different sensitive attributes and studied subproblems. For race and gender, Reject Option Classification improves equalized odds by 46% and 57%, respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI versus AD subproblem, whereas for age, in the same subproblem, adversarial debiasing yields the highest equalized odds improvement of 40% with a harmonic mean score of 0.69. Insights are provided into how variations in AD neuropathology and risk factors, associated with demographic characteristics, influence model fairness.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning</title>
<link>https://arxiv.org/abs/2505.23529</link>
<guid>https://arxiv.org/abs/2505.23529</guid>
<content:encoded><![CDATA[
arXiv:2505.23529v1 Announce Type: new 
Abstract: Graph Representation Learning (GRL) is a fundamental task in machine learning, aiming to encode high-dimensional graph-structured data into low-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used in GRL because they can avoid expensive human annotation. In this work, we propose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our approach introduces a subgraph Gaussian embedding module, which adaptively maps subgraphs to a structured Gaussian space, ensuring the preservation of input subgraph characteristics while generating subgraphs with a controlled distribution. We then employ optimal transport distances, more precisely the Wasserstein and Gromov-Wasserstein distances, to effectively measure the similarity between subgraphs, enhancing the robustness of the contrastive learning process. Extensive experiments across multiple benchmarks demonstrate that \method~outperforms or presents competitive performance against state-of-the-art approaches. Our findings provide insights into the design of SSL methods for GRL, emphasizing the importance of the distribution of the generated contrastive pairs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Aware Tensor Network Structure Search</title>
<link>https://arxiv.org/abs/2505.23537</link>
<guid>https://arxiv.org/abs/2505.23537</guid>
<content:encoded><![CDATA[
arXiv:2505.23537v1 Announce Type: new 
Abstract: Tensor networks (TNs) provide efficient representations of high-dimensional data, yet identification of the optimal TN structures, the so called tensor network structure search (TN-SS) problem, remains a challenge. Current state-of-the-art (SOTA) algorithms are computationally expensive as they require extensive function evaluations, which is prohibitive for real-world applications. In addition, existing methods ignore valuable domain information inherent in real-world tensor data and lack transparency in their identified TN structures. To this end, we propose a novel TN-SS framework, termed the tnLLM, which incorporates domain information about the data and harnesses the reasoning capabilities of large language models (LLMs) to directly predict suitable TN structures. The proposed framework involves a domain-aware prompting pipeline which instructs the LLM to infer suitable TN structures based on the real-world relationships between tensor modes. In this way, our approach is capable of not only iteratively optimizing the objective function, but also generating domain-aware explanations for the identified structures. Experimental results demonstrate that tnLLM achieves comparable TN-SS objective function values with much fewer function evaluations compared to SOTA algorithms. Furthermore, we demonstrate that the LLM-enabled domain information can be used to find good initializations in the search space for sampling-based SOTA methods to accelerate their convergence while preserving theoretical performance guarantees.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing the Moore-Penrose Pseudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis</title>
<link>https://arxiv.org/abs/2505.23552</link>
<guid>https://arxiv.org/abs/2505.23552</guid>
<content:encoded><![CDATA[
arXiv:2505.23552v1 Announce Type: new 
Abstract: This paper investigates the comparative performance of two fundamental approaches to solving linear regression problems: the closed-form Moore-Penrose pseudoinverse and the iterative gradient descent method. Linear regression is a cornerstone of predictive modeling, and the choice of solver can significantly impact efficiency and accuracy. I review and discuss the theoretical underpinnings of both methods, analyze their computational complexity, and evaluate their empirical behavior on synthetic datasets with controlled characteristics, as well as on established real-world datasets. My results delineate the conditions under which each method excels in terms of computational time, numerical stability, and predictive accuracy. This work aims to provide practical guidance for researchers and practitioners in machine learning when selecting between direct, exact solutions and iterative, approximate solutions for linear regression tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling</title>
<link>https://arxiv.org/abs/2505.23555</link>
<guid>https://arxiv.org/abs/2505.23555</guid>
<content:encoded><![CDATA[
arXiv:2505.23555v1 Announce Type: new 
Abstract: Federated LoRA has emerged as a promising technique for efficiently fine-tuning large language models (LLMs) on distributed devices by reducing the number of trainable parameters. However, existing approaches often inadequately overlook the theoretical and practical implications of system and data heterogeneity, thereby failing to optimize the overall training efficiency, particularly in terms of wall-clock time. In this paper, we propose an adaptive federated LoRA strategy with independent client sampling to minimize the convergence wall-clock time of federated fine-tuning under both computation and communication heterogeneity. We first derive a new convergence bound for federated LoRA with arbitrary and independent client sampling, notably without requiring the stringent bounded gradient assumption. Then, we introduce an adaptive bandwidth allocation scheme that accounts for heterogeneous client resources and system bandwidth constraints. Based on the derived theory, we formulate and solve a non-convex optimization problem to jointly determine the LoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock convergence time. An efficient and low-complexity algorithm is developed to approximate the solution. Finally, extensive experiments demonstrate that our approach significantly reduces wall-clock training time compared to state-of-the-art methods across various models and datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models</title>
<link>https://arxiv.org/abs/2505.23564</link>
<guid>https://arxiv.org/abs/2505.23564</guid>
<content:encoded><![CDATA[
arXiv:2505.23564v1 Announce Type: new 
Abstract: Enhancing the reasoning capabilities of large language models effectively using reinforcement learning (RL) remains a crucial challenge. Existing approaches primarily adopt two contrasting advantage estimation granularities: Token-level methods (e.g., PPO) aim to provide the fine-grained advantage signals but suffer from inaccurate estimation due to difficulties in training an accurate critic model. On the other extreme, trajectory-level methods (e.g., GRPO) solely rely on a coarse-grained advantage signal from the final reward, leading to imprecise credit assignment. To address these limitations, we propose Segment Policy Optimization (SPO), a novel RL framework that leverages segment-level advantage estimation at an intermediate granularity, achieving a better balance by offering more precise credit assignment than trajectory-level methods and requiring fewer estimation points than token-level methods, enabling accurate advantage estimation based on Monte Carlo (MC) without a critic model. SPO features three components with novel strategies: (1) flexible segment partition; (2) accurate segment advantage estimation; and (3) policy optimization using segment advantages, including a novel probability-mask strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain for short chain-of-thought (CoT), featuring novel cutpoint-based partition and chain-based advantage estimation, achieving $6$-$12$ percentage point improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT, featuring novel tree-based advantage estimation, which significantly reduces the cost of MC estimation, achieving $7$-$11$ percentage point improvements over GRPO on MATH500 under 2K and 4K context evaluation. We make our code publicly available at https://github.com/AIFrameResearch/SPO.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRO: A Python Library for Distributionally Robust Optimization in Machine Learning</title>
<link>https://arxiv.org/abs/2505.23565</link>
<guid>https://arxiv.org/abs/2505.23565</guid>
<content:encoded><![CDATA[
arXiv:2505.23565v1 Announce Type: new 
Abstract: We introduce dro, an open-source Python library for distributionally robust optimization (DRO) for regression and classification problems. The library implements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO methods. Furthermore, dro is compatible with both scikit-learn and PyTorch. Through vectorization and optimization approximation techniques, dro reduces runtime by 10x to over 1000x compared to baseline implementations on large-scale datasets. Comprehensive documentation is available at https://python-dro.org.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum Likelihood Learning of Latent Dynamics Without Reconstruction</title>
<link>https://arxiv.org/abs/2505.23569</link>
<guid>https://arxiv.org/abs/2505.23569</guid>
<content:encoded><![CDATA[
arXiv:2505.23569v1 Announce Type: new 
Abstract: We introduce a novel unsupervised learning method for time series data with latent dynamical structure: the recognition-parametrized Gaussian state space model (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian Gaussian latents explaining statistical dependence between observations at different time steps, combining the intuition of contrastive methods with the flexible tools of probabilistic generative models. Unlike contrastive approaches, the RP-GSSM is a valid probabilistic model learned via maximum likelihood. Unlike generative approaches, the RP-GSSM has no need for an explicit network mapping from latents to observations, allowing it to focus model capacity on inference of latents. The model is both tractable and expressive: it admits exact inference thanks to its jointly Gaussian latent prior, while maintaining expressivity with an arbitrarily nonlinear neural network link between observations and latents. These qualities allow the RP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary losses, or optimizer scheduling. We show how this approach outperforms alternatives on problems that include learning nonlinear stochastic dynamics from video, with or without background distractors. Our results position the RP-GSSM as a useful foundation model for a variety of downstream applications.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model</title>
<link>https://arxiv.org/abs/2505.23579</link>
<guid>https://arxiv.org/abs/2505.23579</guid>
<content:encoded><![CDATA[
arXiv:2505.23579v1 Announce Type: new 
Abstract: Unlocking deep, interpretable biological reasoning from complex genomic data is a major AI challenge hindering scientific discovery. Current DNA foundation models, despite strong sequence representation, struggle with multi-step reasoning and lack inherent transparent, biologically intuitive explanations. We introduce BioReason, a pioneering architecture that, for the first time, deeply integrates a DNA foundation model with a Large Language Model (LLM). This novel connection enables the LLM to directly process and reason with genomic information as a fundamental input, fostering a new form of multimodal biological understanding. BioReason's sophisticated multi-step reasoning is developed through supervised fine-tuning and targeted reinforcement learning, guiding the system to generate logical, biologically coherent deductions. On biological reasoning benchmarks including KEGG-based disease pathway prediction - where accuracy improves from 88% to 97% - and variant effect prediction, BioReason demonstrates an average 15% performance gain over strong single-modality baselines. BioReason reasons over unseen biological entities and articulates decision-making through interpretable, step-by-step biological traces, offering a transformative approach for AI in biology that enables deeper mechanistic insights and accelerates testable hypothesis generation from genomic data. Data, code, and checkpoints are publicly available at https://github.com/bowang-lab/BioReason
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Time Series Forecasting via Instance-aware Post-hoc Revision</title>
<link>https://arxiv.org/abs/2505.23583</link>
<guid>https://arxiv.org/abs/2505.23583</guid>
<content:encoded><![CDATA[
arXiv:2505.23583v1 Announce Type: new 
Abstract: Time series forecasting plays a vital role in various real-world applications and has attracted significant attention in recent decades. While recent methods have achieved remarkable accuracy by incorporating advanced inductive biases and training strategies, we observe that instance-level variations remain a significant challenge. These variations--stemming from distribution shifts, missing data, and long-tail patterns--often lead to suboptimal forecasts for specific instances, even when overall performance appears strong. To address this issue, we propose a model-agnostic framework, PIR, designed to enhance forecasting performance through Post-forecasting Identification and Revision. Specifically, PIR first identifies biased forecasting instances by estimating their accuracy. Based on this, the framework revises the forecasts using contextual information, including covariates and historical time series, from both local and global perspectives in a post-processing fashion. Extensive experiments on real-world datasets with mainstream forecasting models demonstrate that PIR effectively mitigates instance-level errors and significantly improves forecasting reliability.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-Policy RL with Optimal Reward Baseline</title>
<link>https://arxiv.org/abs/2505.23585</link>
<guid>https://arxiv.org/abs/2505.23585</guid>
<content:encoded><![CDATA[
arXiv:2505.23585v1 Announce Type: new 
Abstract: Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO introduces the optimal reward baseline that theoretically minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The implementation is provided at https://github.com/microsoft/LMOps/tree/main/opo.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerated Training of Federated Learning via Second-Order Methods</title>
<link>https://arxiv.org/abs/2505.23588</link>
<guid>https://arxiv.org/abs/2505.23588</guid>
<content:encoded><![CDATA[
arXiv:2505.23588v1 Announce Type: new 
Abstract: This paper explores second-order optimization methods in Federated Learning (FL), addressing the critical challenges of slow convergence and the excessive communication rounds required to achieve optimal performance from the global model. While existing surveys in FL primarily focus on challenges related to statistical and device label heterogeneity, as well as privacy and security concerns in first-order FL methods, less attention has been given to the issue of slow model training. This slow training often leads to the need for excessive communication rounds or increased communication costs, particularly when data across clients are highly heterogeneous. In this paper, we examine various FL methods that leverage second-order optimization to accelerate the training process. We provide a comprehensive categorization of state-of-the-art second-order FL methods and compare their performance based on convergence speed, computational cost, memory usage, transmission overhead, and generalization of the global model. Our findings show the potential of incorporating Hessian curvature through second-order optimization into FL and highlight key challenges, such as the efficient utilization of Hessian and its inverse in FL. This work lays the groundwork for future research aimed at developing scalable and efficient federated optimization methods for improving the training of the global model in FL.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models</title>
<link>https://arxiv.org/abs/2505.23593</link>
<guid>https://arxiv.org/abs/2505.23593</guid>
<content:encoded><![CDATA[
arXiv:2505.23593v1 Announce Type: new 
Abstract: Post-training of foundation language models has emerged as a promising research domain in federated learning (FL) with the goal to enable privacy-preserving model improvements and adaptations to user's downstream tasks. Recent advances in this area adopt centralized post-training approaches that build upon black-box foundation language models where there is no access to model weights and architecture details. Although the use of black-box models has been successful in centralized post-training, their blind replication in FL raises several concerns. Our position is that using black-box models in FL contradicts the core principles of federation such as data privacy and autonomy. In this position paper, we critically analyze the usage of black-box models in federated post-training, and provide a detailed account of various aspects of openness and their implications for FL.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Performance for Code Generation on Noisy Tasks</title>
<link>https://arxiv.org/abs/2505.23598</link>
<guid>https://arxiv.org/abs/2505.23598</guid>
<content:encoded><![CDATA[
arXiv:2505.23598v1 Announce Type: new 
Abstract: This paper investigates the ability of large language models (LLMs) to recognise and solve tasks which have been obfuscated beyond recognition. Focusing on competitive programming and benchmark tasks (LeetCode and MATH), we compare performance across multiple models and obfuscation methods, such as noise and redaction. We demonstrate that all evaluated LLMs can solve tasks obfuscated to a level where the text would be unintelligible to human readers, and does not contain key pieces of instruction or context. We introduce the concept of eager pattern matching to describe this behaviour, which is not observed in tasks published after the models' knowledge cutoff date, indicating strong memorisation or overfitting to training data, rather than legitimate reasoning about the presented problem. We report empirical evidence of distinct performance decay patterns between contaminated and unseen datasets. We discuss the implications for benchmarking and evaluations of model behaviour, arguing for caution when designing experiments using standard datasets. We also propose measuring the decay of performance under obfuscation as a possible strategy for detecting dataset contamination and highlighting potential safety risks and interpretability issues for automated software systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Transferring Transferability: Towards a Theory for Size Generalization</title>
<link>https://arxiv.org/abs/2505.23599</link>
<guid>https://arxiv.org/abs/2505.23599</guid>
<content:encoded><![CDATA[
arXiv:2505.23599v1 Announce Type: new 
Abstract: Many modern learning tasks require models that can take inputs of varying sizes. Consequently, dimension-independent architectures have been proposed for domains where the inputs are graphs, sets, and point clouds. Recent work on graph neural networks has explored whether a model trained on low-dimensional data can transfer its performance to higher-dimensional inputs. We extend this body of work by introducing a general framework for transferability across dimensions. We show that transferability corresponds precisely to continuity in a limit space formed by identifying small problem instances with equivalent large ones. This identification is driven by the data and the learning task. We instantiate our framework on existing architectures, and implement the necessary changes to ensure their transferability. Finally, we provide design principles for designing new transferable models. Numerical experiments support our findings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model</title>
<link>https://arxiv.org/abs/2505.23606</link>
<guid>https://arxiv.org/abs/2505.23606</guid>
<content:encoded><![CDATA[
arXiv:2505.23606v1 Announce Type: new 
Abstract: Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, a unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as a scalable and effective backbone for unified generation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Model Design for Explainable Machine Learning-based Electricity Applications</title>
<link>https://arxiv.org/abs/2505.23607</link>
<guid>https://arxiv.org/abs/2505.23607</guid>
<content:encoded><![CDATA[
arXiv:2505.23607v1 Announce Type: new 
Abstract: The transition from traditional power grids to smart grids, significant increase in the use of renewable energy sources, and soaring electricity prices has triggered a digital transformation of the energy infrastructure that enables new, data driven, applications often supported by machine learning models. However, the majority of the developed machine learning models rely on univariate data. To date, a structured study considering the role meta-data and additional measurements resulting in multivariate data is missing. In this paper we propose a taxonomy that identifies and structures various types of data related to energy applications. The taxonomy can be used to guide application specific data model development for training machine learning models. Focusing on a household electricity forecasting application, we validate the effectiveness of the proposed taxonomy in guiding the selection of the features for various types of models. As such, we study of the effect of domain, contextual and behavioral features on the forecasting accuracy of four interpretable machine learning techniques and three openly available datasets. Finally, using a feature importance techniques, we explain individual feature contributions to the forecasting accuracy.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Generalized Skew Spectrum of Graphs</title>
<link>https://arxiv.org/abs/2505.23609</link>
<guid>https://arxiv.org/abs/2505.23609</guid>
<content:encoded><![CDATA[
arXiv:2505.23609v1 Announce Type: new 
Abstract: This paper proposes a family of permutation-invariant graph embeddings, generalizing the Skew Spectrum of graphs of Kondor & Borgwardt (2008). Grounded in group theory and harmonic analysis, our method introduces a new class of graph invariants that are isomorphism-invariant and capable of embedding richer graph structures - including attributed graphs, multilayer graphs, and hypergraphs - which the Skew Spectrum could not handle. Our generalization further defines a family of functions that enables a trade-off between computational complexity and expressivity. By applying generalization-preserving heuristics to this family, we improve the Skew Spectrum's expressivity at the same computational cost. We formally prove the invariance of our generalization, demonstrate its improved expressiveness through experiments, and discuss its efficient computation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-time Scaling of Diffusion Models through Classical Search</title>
<link>https://arxiv.org/abs/2505.23614</link>
<guid>https://arxiv.org/abs/2505.23614</guid>
<content:encoded><![CDATA[
arXiv:2505.23614v1 Announce Type: new 
Abstract: Classical search algorithms have long underpinned modern artificial intelligence. In this work, we tackle the challenge of inference-time control in diffusion models -- adapting generated outputs to meet diverse test-time objectives -- using principles from classical search. We propose a general framework that orchestrates local and global search to efficiently navigate the generative space. It employs a theoretically grounded local search via annealed Langevin MCMC and performs compute-efficient global exploration using breadth-first and depth-first tree search. We evaluate our approach on a range of challenging domains, including planning, offline reinforcement learning, and image generation. Across all tasks, we observe significant gains in both performance and efficiency. These results show that classical search provides a principled and practical foundation for inference-time scaling in diffusion models. Project page at diffusion-inference-scaling.github.io.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Interpretable Differentiable Logic Networks for Tabular Regression</title>
<link>https://arxiv.org/abs/2505.23615</link>
<guid>https://arxiv.org/abs/2505.23615</guid>
<content:encoded><![CDATA[
arXiv:2505.23615v1 Announce Type: new 
Abstract: Neural networks (NNs) achieve outstanding performance in many domains; however, their decision processes are often opaque and their inference can be computationally expensive in resource-constrained environments. We recently proposed Differentiable Logic Networks (DLNs) to address these issues for tabular classification based on relaxing discrete logic into a differentiable form, thereby enabling gradient-based learning of networks built from binary logic operations. DLNs offer interpretable reasoning and substantially lower inference cost.
  We extend the DLN framework to supervised tabular regression. Specifically, we redesign the final output layer to support continuous targets and unify the original two-phase training procedure into a single differentiable stage. We evaluate the resulting model on 15 public regression benchmarks, comparing it with modern neural networks and classical regression baselines. Regression DLNs match or exceed baseline accuracy while preserving interpretability and fast inference. Our results show that DLNs are a viable, cost-effective alternative for regression tasks, especially where model transparency and computational efficiency are important.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection</title>
<link>https://arxiv.org/abs/2505.23627</link>
<guid>https://arxiv.org/abs/2505.23627</guid>
<content:encoded><![CDATA[
arXiv:2505.23627v1 Announce Type: new 
Abstract: Identifying mistakes (i.e., miscues) made while reading aloud is commonly approached post-hoc by comparing automatic speech recognition (ASR) transcriptions to the target reading text. However, post-hoc methods perform poorly when ASR inaccurately transcribes verbatim speech. To improve on current methods for reading error annotation, we propose a novel end-to-end architecture that incorporates the target reading text via prompting and is trained for both improved verbatim transcription and direct miscue detection. Our contributions include: first, demonstrating that incorporating reading text through prompting benefits verbatim transcription performance over fine-tuning, and second, showing that it is feasible to augment speech recognition tasks for end-to-end miscue detection. We conducted two case studies -- children's read-aloud and adult atypical speech -- and found that our proposed strategies improve verbatim transcription and miscue detection compared to current state-of-the-art.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment</title>
<link>https://arxiv.org/abs/2505.23634</link>
<guid>https://arxiv.org/abs/2505.23634</guid>
<content:encoded><![CDATA[
arXiv:2505.23634v1 Announce Type: new 
Abstract: The model context protocol (MCP) has been widely adapted as an open standard enabling the seamless integration of generative AI agents. However, recent work has shown the MCP is susceptible to retrieval-based "falsely benign" attacks (FBAs), allowing malicious system access and credential theft, but requiring that users download compromised files directly to their systems. Herein, we show that the threat model of MCP-based attacks is significantly broader than previously thought, i.e., attackers need only post malicious content online to deceive MCP agents into carrying out their attacks on unsuspecting victims' systems.
  To improve alignment guardrails against such attacks, we introduce a new MCP dataset of FBAs and (truly) benign samples to explore the effectiveness of direct preference optimization (DPO) for the refusal training of large language models (LLMs). While DPO improves model guardrails against such attacks, we show that the efficacy of refusal learning varies drastically depending on the model's original post-training alignment scheme--e.g., GRPO-based LLMs learn to refuse extremely poorly. Thus, to further improve FBA refusals, we introduce Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel preference alignment strategy based on RAG. We show that RAG-Pref significantly improves the ability of LLMs to refuse FBAs, particularly when combined with DPO alignment, thus drastically improving guardrails against MCP-based attacks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global optimization of graph acquisition functions for neural architecture search</title>
<link>https://arxiv.org/abs/2505.23640</link>
<guid>https://arxiv.org/abs/2505.23640</guid>
<content:encoded><![CDATA[
arXiv:2505.23640v1 Announce Type: new 
Abstract: Graph Bayesian optimization (BO) has shown potential as a powerful and data-efficient tool for neural architecture search (NAS). Most existing graph BO works focus on developing graph surrogates models, i.e., metrics of networks and/or different kernels to quantify the similarity between networks. However, the acquisition optimization, as a discrete optimization task over graph structures, is not well studied due to the complexity of formulating the graph search space and acquisition functions. This paper presents explicit optimization formulations for graph input space including properties such as reachability and shortest paths, which are used later to formulate graph kernels and the acquisition function. We theoretically prove that the proposed encoding is an equivalent representation of the graph space and provide restrictions for the NAS domain with either node or edge labels. Numerical results over several NAS benchmarks show that our method efficiently finds the optimal architecture for most cases, highlighting its efficacy.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Chain of Thought Enables Parallel Exploration and Reasoning</title>
<link>https://arxiv.org/abs/2505.23648</link>
<guid>https://arxiv.org/abs/2505.23648</guid>
<content:encoded><![CDATA[
arXiv:2505.23648v1 Announce Type: new 
Abstract: Current language models generate chain-of-thought traces by autoregressively sampling tokens from a finite vocabulary. While this discrete sampling has achieved remarkable success, conducting chain-of-thought with continuously-valued tokens (CoT2) offers a richer and more expressive alternative. Our work examines the benefits of CoT2 through logical reasoning tasks that inherently require search capabilities and provide optimization and exploration methods for CoT2. Theoretically, we show that CoT2 allows the model to track multiple traces in parallel and quantify its benefits for inference efficiency. Notably, one layer transformer equipped with CoT2 can provably solve the combinatorial "subset sum problem" given sufficient embedding dimension. These insights lead to a novel and effective supervision strategy where we match the softmax outputs to the empirical token distributions of a set of target traces. Complementing this, we introduce sampling strategies that unlock policy optimization and self-improvement for CoT2. Our first strategy samples and composes $K$ discrete tokens at each decoding step to control the level of parallelism, and reduces to standard CoT when $K=1$. Our second strategy relies on continuous exploration over the probability simplex. Experiments confirm that policy optimization with CoT2 indeed improves the performance of the model beyond its initial discrete or continuous supervision.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.23651</link>
<guid>https://arxiv.org/abs/2505.23651</guid>
<content:encoded><![CDATA[
arXiv:2505.23651v1 Announce Type: new 
Abstract: Model merging has emerged as a powerful technique for combining task-specific weights, achieving superior performance in multi-target domain adaptation. However, when applied to practical scenarios, such as quantized models, new challenges arise. In practical scenarios, quantization is often applied to target-specific data, but this process restricts the domain of interest and introduces discretization effects, making model merging highly non-trivial. In this study, we analyze the impact of quantization on model merging through the lens of error barriers. Leveraging these insights, we propose a novel post-training quantization, HDRQ - Hessian and distant regularizing quantization - that is designed to consider model merging for multi-target domain adaptation. Our approach ensures that the quantization process incurs minimal deviation from the source pre-trained model while flattening the loss surface to facilitate smooth model merging. To our knowledge, this is the first study on this challenge, and extensive experiments confirm its effectiveness.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How does Transformer Learn Implicit Reasoning?</title>
<link>https://arxiv.org/abs/2505.23653</link>
<guid>https://arxiv.org/abs/2505.23653</guid>
<content:encoded><![CDATA[
arXiv:2505.23653v1 Announce Type: new 
Abstract: Recent work suggests that large language models (LLMs) can perform multi-hop reasoning implicitly -- producing correct answers without explicitly verbalizing intermediate steps -- but the underlying mechanisms remain poorly understood. In this paper, we study how such implicit reasoning emerges by training transformers from scratch in a controlled symbolic environment. Our analysis reveals a three-stage developmental trajectory: early memorization, followed by in-distribution generalization, and eventually cross-distribution generalization. We find that training with atomic triples is not necessary but accelerates learning, and that second-hop generalization relies on query-level exposure to specific compositional structures. To interpret these behaviors, we introduce two diagnostic tools: cross-query semantic patching, which identifies semantically reusable intermediate representations, and a cosine-based representational lens, which reveals that successful reasoning correlates with the cosine-base clustering in hidden space. This clustering phenomenon in turn provides a coherent explanation for the behavioral dynamics observed across training, linking representational structure to reasoning capability. These findings provide new insights into the interpretability of implicit multi-hop reasoning in LLMs, helping to clarify how complex reasoning processes unfold internally and offering pathways to enhance the transparency of such models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction</title>
<link>https://arxiv.org/abs/2505.23663</link>
<guid>https://arxiv.org/abs/2505.23663</guid>
<content:encoded><![CDATA[
arXiv:2505.23663v1 Announce Type: new 
Abstract: The cost and accuracy of simulating complex physical systems using the Finite Element Method (FEM) scales with the resolution of the underlying mesh. Adaptive meshes improve computational efficiency by refining resolution in critical regions, but typically require task-specific heuristics or cumbersome manual design by a human expert. We propose Adaptive Meshing By Expert Reconstruction (AMBER), a supervised learning approach to mesh adaptation. Starting from a coarse mesh, AMBER iteratively predicts the sizing field, i.e., a function mapping from the geometry to the local element size of the target mesh, and uses this prediction to produce a new intermediate mesh using an out-of-the-box mesh generator. This process is enabled through a hierarchical graph neural network, and relies on data augmentation by automatically projecting expert labels onto AMBER-generated data during training. We evaluate AMBER on 2D and 3D datasets, including classical physics problems, mechanical components, and real-world industrial designs with human expert meshes. AMBER generalizes to unseen geometries and consistently outperforms multiple recent baselines, including ones using Graph and Convolutional Neural Networks, and Reinforcement Learning-based approaches.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds</title>
<link>https://arxiv.org/abs/2505.23673</link>
<guid>https://arxiv.org/abs/2505.23673</guid>
<content:encoded><![CDATA[
arXiv:2505.23673v1 Announce Type: new 
Abstract: Bayesian optimization (BO) with preference-based feedback has recently garnered significant attention due to its emerging applications. We refer to this problem as Bayesian Optimization from Human Feedback (BOHF), which differs from conventional BO by learning the best actions from a reduced feedback model, where only the preference between two actions is revealed to the learner at each time step. The objective is to identify the best action using a limited number of preference queries, typically obtained through costly human feedback. Existing work, which adopts the Bradley-Terry-Luce (BTL) feedback model, provides regret bounds for the performance of several algorithms. In this work, within the same framework we develop tighter performance guarantees. Specifically, we derive regret bounds of $\tilde{\mathcal{O}}(\sqrt{\Gamma(T)T})$, where $\Gamma(T)$ represents the maximum information gain$\unicode{x2014}$a kernel-specific complexity term$\unicode{x2014}$and $T$ is the number of queries. Our results significantly improve upon existing bounds. Notably, for common kernels, we show that the order-optimal sample complexities of conventional BO$\unicode{x2014}$achieved with richer feedback models$\unicode{x2014}$are recovered. In other words, the same number of preferential samples as scalar-valued samples is sufficient to find a nearly optimal solution.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Mode Connectivity via Parameter Space Symmetry</title>
<link>https://arxiv.org/abs/2505.23681</link>
<guid>https://arxiv.org/abs/2505.23681</guid>
<content:encoded><![CDATA[
arXiv:2505.23681v1 Announce Type: new 
Abstract: Neural network minima are often connected by curves along which train and test loss remain nearly constant, a phenomenon known as mode connectivity. While this property has enabled applications such as model merging and fine-tuning, its theoretical explanation remains unclear. We propose a new approach to exploring the connectedness of minima using parameter space symmetry. By linking the topology of symmetry groups to that of the minima, we derive the number of connected components of the minima of linear networks and show that skip connections reduce this number. We then examine when mode connectivity and linear mode connectivity hold or fail, using parameter symmetries which account for a significant part of the minimum. Finally, we provide explicit expressions for connecting curves in the minima induced by symmetry. Using the curvature of these curves, we derive conditions under which linear mode connectivity approximately holds. Our findings highlight the role of continuous symmetries in understanding the neural network loss landscape.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Compositional Functions with Transformers from Easy-to-Hard Data</title>
<link>https://arxiv.org/abs/2505.23683</link>
<guid>https://arxiv.org/abs/2505.23683</guid>
<content:encoded><![CDATA[
arXiv:2505.23683v1 Announce Type: new 
Abstract: Transformer-based language models have demonstrated impressive capabilities across a range of complex reasoning tasks. Prior theoretical work exploring the expressive power of transformers has shown that they can efficiently perform multi-step reasoning tasks involving parallelizable computations. However, the learnability of such constructions, particularly the conditions on the data distribution that enable efficient learning via gradient-based optimization, remains an open question. Towards answering this question, in this work we study the learnability of the $k$-fold composition task, which requires computing an interleaved composition of $k$ input permutations and $k$ hidden permutations, and can be expressed by a transformer with $O(\log k)$ layers. On the negative front, we prove a Statistical Query (SQ) lower bound showing that any SQ learner that makes only polynomially-many queries to an SQ oracle for the $k$-fold composition task distribution must have sample size exponential in $k$, thus establishing a statistical-computational gap. On the other hand, we show that this function class can be efficiently learned, with runtime and sample complexity polynomial in $k$, by gradient descent on an $O(\log k)$-depth transformer via two different curriculum learning strategies: one in which data consists of $k'$-fold composition functions with $k' \le k$ presented in increasing difficulty, and another in which all such data is presented simultaneously. Our work sheds light on the necessity and sufficiency of having both easy and hard examples in the data distribution for transformers to learn complex compositional tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms</title>
<link>https://arxiv.org/abs/2505.23696</link>
<guid>https://arxiv.org/abs/2505.23696</guid>
<content:encoded><![CDATA[
arXiv:2505.23696v1 Announce Type: new 
Abstract: Solving systems of polynomial equations, particularly those with finitely many solutions, is a crucial challenge across many scientific fields. Traditional methods like Gr\"obner and Border bases are fundamental but suffer from high computational costs, which have motivated recent Deep Learning approaches to improve efficiency, albeit at the expense of output correctness. In this work, we introduce the Oracle Border Basis Algorithm, the first Deep Learning approach that accelerates Border basis computation while maintaining output guarantees. To this end, we design and train a Transformer-based oracle that identifies and eliminates computationally expensive reduction steps, which we find to dominate the algorithm's runtime. By selectively invoking this oracle during critical phases of computation, we achieve substantial speedup factors of up to 3.5x compared to the base algorithm, without compromising the correctness of results. To generate the training data, we develop a sampling method and provide the first sampling theorem for border bases. We construct a tokenization and embedding scheme tailored to monomial-centered algebraic computations, resulting in a compact and expressive input representation, which reduces the number of tokens to encode an $n$-variate polynomial by a factor of $O(n)$. Our learning approach is data efficient, stable, and a practical enhancement to traditional computer algebra algorithms and symbolic computation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiCoFlex: Model-agnostic diverse counterfactuals with flexible control</title>
<link>https://arxiv.org/abs/2505.23700</link>
<guid>https://arxiv.org/abs/2505.23700</guid>
<content:encoded><![CDATA[
arXiv:2505.23700v1 Announce Type: new 
Abstract: Counterfactual explanations play a pivotal role in explainable artificial intelligence (XAI) by offering intuitive, human-understandable alternatives that elucidate machine learning model decisions. Despite their significance, existing methods for generating counterfactuals often require constant access to the predictive model, involve computationally intensive optimization for each instance and lack the flexibility to adapt to new user-defined constraints without retraining. In this paper, we propose DiCoFlex, a novel model-agnostic, conditional generative framework that produces multiple diverse counterfactuals in a single forward pass. Leveraging conditional normalizing flows trained solely on labeled data, DiCoFlex addresses key limitations by enabling real-time user-driven customization of constraints such as sparsity and actionability at inference time. Extensive experiments on standard benchmark datasets show that DiCoFlex outperforms existing methods in terms of validity, diversity, proximity, and constraint adherence, making it a practical and scalable solution for counterfactual generation in sensitive decision-making domains.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs</title>
<link>https://arxiv.org/abs/2505.23702</link>
<guid>https://arxiv.org/abs/2505.23702</guid>
<content:encoded><![CDATA[
arXiv:2505.23702v1 Announce Type: new 
Abstract: We introduce (U)NFV, a modular neural network architecture that generalizes classical finite volume (FV) methods for solving hyperbolic conservation laws. Hyperbolic partial differential equations (PDEs) are challenging to solve, particularly conservation laws whose physically relevant solutions contain shocks and discontinuities. FV methods are widely used for their mathematical properties: convergence to entropy solutions, flow conservation, or total variation diminishing, but often lack accuracy and flexibility in complex settings. Neural Finite Volume addresses these limitations by learning update rules over extended spatial and temporal stencils while preserving conservation structure. It supports both supervised training on solution data (NFV) and unsupervised training via weak-form residual loss (UNFV). Applied to first-order conservation laws, (U)NFV achieves up to 10x lower error than Godunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin solvers with far less complexity. On traffic modeling problems, both from PDEs and from experimental highway data, (U)NFV captures nonlinear wave dynamics with significantly higher fidelity and scalability than traditional FV approaches.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better</title>
<link>https://arxiv.org/abs/2505.23705</link>
<guid>https://arxiv.org/abs/2505.23705</guid>
<content:encoded><![CDATA[
arXiv:2505.23705v1 Announce Type: new 
Abstract: Vision-language-action (VLA) models provide a powerful approach to training control policies for physical systems, such as robots, by combining end-to-end learning with transfer of semantic knowledge from web-scale vision-language model (VLM) training. However, the constraints of real-time control are often at odds with the design of VLMs: the most powerful VLMs have tens or hundreds of billions of parameters, presenting an obstacle to real-time inference, and operate on discrete tokens rather than the continuous-valued outputs that are required for controlling robots. To address this challenge, recent VLA models have used specialized modules for efficient continuous control, such as action experts or continuous output heads, which typically require adding new untrained parameters to the pretrained VLM backbone. While these modules improve real-time and control capabilities, it remains an open question whether they preserve or degrade the semantic knowledge contained in the pretrained VLM, and what effect they have on the VLA training dynamics. In this paper, we study this question in the context of VLAs that include a continuous diffusion or flow matching action expert, showing that naively including such experts significantly harms both training speed and knowledge transfer. We provide an extensive analysis of various design choices, their impact on performance and knowledge transfer, and propose a technique for insulating the VLM backbone during VLA training that mitigates this issue. Videos are available at https://pi.website/research/knowledge_insulation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning</title>
<link>https://arxiv.org/abs/2505.23719</link>
<guid>https://arxiv.org/abs/2505.23719</guid>
<content:encoded><![CDATA[
arXiv:2505.23719v1 Announce Type: new 
Abstract: In-context learning, the ability of large language models to perform tasks using only examples provided in the prompt, has recently been adapted for time series forecasting. This paradigm enables zero-shot prediction, where past values serve as context for forecasting future values, making powerful forecasting tools accessible to non-experts and increasing the performance when training data are scarce. Most existing zero-shot forecasting approaches rely on transformer architectures, which, despite their success in language, often fall short of expectations in time series forecasting, where recurrent models like LSTMs frequently have the edge. Conversely, while LSTMs are well-suited for time series modeling due to their state-tracking capabilities, they lack strong in-context learning abilities. We introduce TiRex that closes this gap by leveraging xLSTM, an enhanced LSTM with competitive in-context learning skills. Unlike transformers, state-space models, or parallelizable RNNs such as RWKV, TiRex retains state-tracking, a critical property for long-horizon forecasting. To further facilitate its state-tracking ability, we propose a training-time masking strategy called CPM. TiRex sets a new state of the art in zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and Chronos-ZS, outperforming significantly larger models including TabPFN-TS (Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce) across both short- and long-term forecasts.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents</title>
<link>https://arxiv.org/abs/2505.23720</link>
<guid>https://arxiv.org/abs/2505.23720</guid>
<content:encoded><![CDATA[
arXiv:2505.23720v1 Announce Type: new 
Abstract: This paper considers a contextual bandit problem involving multiple agents, where a learner sequentially observes the contexts and the agent's reported arms, and then selects the arm that maximizes the system's overall reward. Existing work in contextual bandits assumes that agents truthfully report their arms, which is unrealistic in many real-life applications. For instance, consider an online platform with multiple sellers; some sellers may misrepresent product quality to gain an advantage, such as having the platform preferentially recommend their products to online users. To address this challenge, we propose an algorithm, COBRA, for contextual bandit problems involving strategic agents that disincentivize their strategic behavior without using any monetary incentives, while having incentive compatibility and a sub-linear regret guarantee. Our experimental results also validate the different performance aspects of our proposed algorithm.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffER: Categorical Diffusion for Chemical Retrosynthesis</title>
<link>https://arxiv.org/abs/2505.23721</link>
<guid>https://arxiv.org/abs/2505.23721</guid>
<content:encoded><![CDATA[
arXiv:2505.23721v1 Announce Type: new 
Abstract: Methods for automatic chemical retrosynthesis have found recent success through the application of models traditionally built for natural language processing, primarily through transformer neural networks. These models have demonstrated significant ability to translate between the SMILES encodings of chemical products and reactants, but are constrained as a result of their autoregressive nature. We propose DiffER, an alternative template-free method for retrosynthesis prediction in the form of categorical diffusion, which allows the entire output SMILES sequence to be predicted in unison. We construct an ensemble of diffusion models which achieves state-of-the-art performance for top-1 accuracy and competitive performance for top-3, top-5, and top-10 accuracy among template-free methods. We prove that DiffER is a strong baseline for a new class of template-free model, capable of learning a variety of synthetic techniques used in laboratory settings and outperforming a variety of other template-free methods on top-k accuracy metrics. By constructing an ensemble of categorical diffusion models with a novel length prediction component with variance, our method is able to approximately sample from the posterior distribution of reactants, producing results with strong metrics of confidence and likelihood. Furthermore, our analyses demonstrate that accurate prediction of the SMILES sequence length is key to further boosting the performance of categorical diffusion models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA</title>
<link>https://arxiv.org/abs/2505.23724</link>
<guid>https://arxiv.org/abs/2505.23724</guid>
<content:encoded><![CDATA[
arXiv:2505.23724v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), are indispensable for efficiently customizing Large Language Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and knowledge forgetting problems. Recent studies have leveraged the power of designed LoRA initialization, to enhance the fine-tuning efficiency, or to preserve knowledge in the pre-trained LLM. However, none of these works can address the two cases at the same time. To this end, we introduce Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework engineered to navigate the trade-off between efficient fine-tuning and knowledge preservation. We achieve this by constraining the output of trainable LoRA adapters in a low-rank subspace, where the context information of fine-tuning data is most preserved while the context information of preserved knowledge is least retained, in a balanced way. Such constraint enables the trainable weights to primarily focus on the main features of fine-tuning data while avoiding damaging the preserved knowledge features. We provide theoretical analysis on our method, and conduct extensive experiments including safety preservation and world knowledge preservation, on various downstream tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning performance while markedly diminishing knowledge forgetting, surpassing contemporary LoRA initialization methods.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuLoCo: Muon is a practical inner optimizer for DiLoCo</title>
<link>https://arxiv.org/abs/2505.23725</link>
<guid>https://arxiv.org/abs/2505.23725</guid>
<content:encoded><![CDATA[
arXiv:2505.23725v1 Announce Type: new 
Abstract: DiLoCo is a powerful framework for training large language models (LLMs) under networking constraints with advantages for increasing parallelism and accelerator utilization in data center settings. Despite significantly reducing communication frequency, however, DiLoCo's communication steps still involve all-reducing a complete copy of the model's parameters. While existing works have explored ways to reduce communication in DiLoCo, the role of error feedback accumulators and the effect of the inner-optimizer on compressibility remain under-explored. In this work, we investigate the effectiveness of standard compression methods including Top-k sparsification and quantization for reducing the communication overhead of DiLoCo when paired with two local optimizers (AdamW and Muon). Our experiments pre-training decoder-only transformer language models (LMs) reveal that leveraging Muon as the inner optimizer for DiLoCo along with an error-feedback accumulator allows to aggressively compress the communicated delta to 2-bits with next to no performance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo) significantly outperforms DiLoCo while communicating 8X less and having identical memory complexity.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast</title>
<link>https://arxiv.org/abs/2505.23732</link>
<guid>https://arxiv.org/abs/2505.23732</guid>
<content:encoded><![CDATA[
arXiv:2505.23732v1 Announce Type: new 
Abstract: Current emotion-based contrastive language-audio pretraining (CLAP) methods typically learn by na\"ively aligning audio samples with corresponding text prompts. Consequently, this approach fails to capture the ordinal nature of emotions, hindering inter-emotion understanding and often resulting in a wide modality gap between the audio and text embeddings due to insufficient alignment. To handle these drawbacks, we introduce EmotionRankCLAP, a supervised contrastive learning approach that uses dimensional attributes of emotional speech and natural language prompts to jointly capture fine-grained emotion variations and improve cross-modal alignment. Our approach utilizes a Rank-N-Contrast objective to learn ordered relationships by contrasting samples based on their rankings in the valence-arousal space. EmotionRankCLAP outperforms existing emotion-CLAP methods in modeling emotion ordinality across modalities, measured via a cross-modal retrieval task.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?</title>
<link>https://arxiv.org/abs/2505.23749</link>
<guid>https://arxiv.org/abs/2505.23749</guid>
<content:encoded><![CDATA[
arXiv:2505.23749v1 Announce Type: new 
Abstract: After pre-training, large language models are aligned with human preferences based on pairwise comparisons. State-of-the-art alignment methods (such as PPO-based RLHF and DPO) are built on the assumption of aligning with a single preference model, despite being deployed in settings where users have diverse preferences. As a result, it is not even clear that these alignment methods produce models that satisfy users on average -- a minimal requirement for pluralistic alignment. Drawing on social choice theory and modeling users' comparisons through individual Bradley-Terry (BT) models, we introduce an alignment method's distortion: the worst-case ratio between the optimal achievable average utility, and the average utility of the learned policy.
  The notion of distortion helps draw sharp distinctions between alignment methods: Nash Learning from Human Feedback achieves the minimax optimal distortion of $(\frac{1}{2} + o(1)) \cdot \beta$ (for the BT temperature $\beta$), robustly across utility distributions, distributions of comparison pairs, and permissible KL divergences from the reference policy. RLHF and DPO, by contrast, suffer $\geq (1 - o(1)) \cdot \beta$ distortion already without a KL constraint, and $e^{\Omega(\beta)}$ or even unbounded distortion in the full setting, depending on how comparison pairs are sampled.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REOrdering Patches Improves Vision Models</title>
<link>https://arxiv.org/abs/2505.23751</link>
<guid>https://arxiv.org/abs/2505.23751</guid>
<content:encoded><![CDATA[
arXiv:2505.23751v1 Announce Type: new 
Abstract: Sequence models such as transformers require inputs to be represented as one-dimensional sequences. In vision, this typically involves flattening images using a fixed row-major (raster-scan) order. While full self-attention is permutation-equivariant, modern long-sequence transformers increasingly rely on architectural approximations that break this invariance and introduce sensitivity to patch ordering. We show that patch order significantly affects model performance in such settings, with simple alternatives like column-major or Hilbert curves yielding notable accuracy shifts. Motivated by this, we propose REOrder, a two-stage framework for discovering task-optimal patch orderings. First, we derive an information-theoretic prior by evaluating the compressibility of various patch sequences. Then, we learn a policy over permutations by optimizing a Plackett-Luce policy using REINFORCE. This approach enables efficient learning in a combinatorial permutation space. REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Immunization from a Condition Number Perspective</title>
<link>https://arxiv.org/abs/2505.23760</link>
<guid>https://arxiv.org/abs/2505.23760</guid>
<content:encoded><![CDATA[
arXiv:2505.23760v1 Announce Type: new 
Abstract: Model immunization aims to pre-train models that are difficult to fine-tune on harmful tasks while retaining their utility on other non-harmful tasks. Though prior work has shown empirical evidence for immunizing text-to-image models, the key understanding of when immunization is possible and a precise definition of an immunized model remain unclear. In this work, we propose a framework, based on the condition number of a Hessian matrix, to analyze model immunization for linear models. Building on this framework, we design an algorithm with regularization terms to control the resulting condition numbers after pre-training. Empirical results on linear models and non-linear deep-nets demonstrate the effectiveness of the proposed algorithm on model immunization. The code is available at https://github.com/amberyzheng/model-immunization-cond-num.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Information: An Information-Theoretic Perspective on Preference Optimization</title>
<link>https://arxiv.org/abs/2505.23761</link>
<guid>https://arxiv.org/abs/2505.23761</guid>
<content:encoded><![CDATA[
arXiv:2505.23761v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) has become a standard technique for aligning language models with human preferences in a supervised manner. Despite its empirical success, the theoretical justification behind its log-ratio reward parameterization remains incomplete. In this work, we address this gap by utilizing the Differential Information Distribution (DID): a distribution over token sequences that captures the information gained during policy updates. First, we show that when preference labels encode the differential information required to transform a reference policy into a target policy, the log-ratio reward in DPO emerges as the uniquely optimal form for learning the target policy via preference optimization. This result naturally yields a closed-form expression for the optimal sampling distribution over rejected responses. Second, we find that the condition for preferences to encode differential information is fundamentally linked to an implicit assumption regarding log-margin ordered policies-an inductive bias widely used in preference optimization yet previously unrecognized. Finally, by analyzing the entropy of the DID, we characterize how learning low-entropy differential information reinforces the policy distribution, while high-entropy differential information induces a smoothing effect, which explains the log-likelihood displacement phenomenon. We validate our theoretical findings in synthetic experiments and extend them to real-world instruction-following datasets. Our results suggest that learning high-entropy differential information is crucial for general instruction-following, while learning low-entropy differential information benefits knowledge-intensive question answering. Overall, our work presents a unifying perspective on the DPO objective, the structure of preference data, and resulting policy behaviors through the lens of differential information.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User-centric Vehicle-to-Grid Optimization with an Input Convex Neural Network-based Battery Degradation Model</title>
<link>https://arxiv.org/abs/2505.11047</link>
<guid>https://arxiv.org/abs/2505.11047</guid>
<content:encoded><![CDATA[
arXiv:2505.11047v1 Announce Type: cross 
Abstract: We propose a data-driven, user-centric vehicle-to-grid (V2G) methodology based on multi-objective optimization to balance battery degradation and V2G revenue according to EV user preference. Given the lack of accurate and generalizable battery degradation models, we leverage input convex neural networks (ICNNs) to develop a data-driven degradation model trained on extensive experimental datasets. This approach enables our model to capture nonconvex dependencies on battery temperature and time while maintaining convexity with respect to the charging rate. Such a partial convexity property ensures that the second stage of our methodology remains computationally efficient. In the second stage, we integrate our data-driven degradation model into a multi-objective optimization framework to generate an optimal smart charging profile for each EV. This profile effectively balances the trade-off between financial benefits from V2G participation and battery degradation, controlled by a hyperparameter reflecting the user prioritization of battery health. Numerical simulations show the high accuracy of the ICNN model in predicting battery degradation for unseen data. Finally, we present a trade-off curve illustrating financial benefits from V2G versus losses from battery health degradation based on user preferences and showcase smart charging strategies under realistic scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Network-Based Representation of BIM Models for Embedding Semantic, Spatial, and Topological Data</title>
<link>https://arxiv.org/abs/2505.22670</link>
<guid>https://arxiv.org/abs/2505.22670</guid>
<content:encoded><![CDATA[
arXiv:2505.22670v1 Announce Type: cross 
Abstract: Building Information Modeling (BIM) has revolutionized the construction industry by providing a comprehensive digital representation of building structures throughout their lifecycle. However, existing research lacks effective methods for capturing the complex spatial and topological relationships between components in BIM models, which are essential for understanding design patterns and enhancing decision-making. This study proposes a unified network-based representation method that integrates the "semantic-spatial-topological" multi-dimensional design features of BIM models. By extending the IFC (Industry Foundation Classes) standard, we introduce local spatial relationships and topological connections between components to enrich the network structure. This representation method enables a more detailed understanding of component interactions, dependencies, and implicit design patterns, effectively capturing the semantic, topological, and spatial relationships in BIM, and holds significant potential for the representation and learning of design patterns.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PSBench: a large-scale benchmark for estimating the accuracy of protein complex structural models</title>
<link>https://arxiv.org/abs/2505.22674</link>
<guid>https://arxiv.org/abs/2505.22674</guid>
<content:encoded><![CDATA[
arXiv:2505.22674v1 Announce Type: cross 
Abstract: Predicting protein complex structures is essential for protein function analysis, protein design, and drug discovery. While AI methods like AlphaFold can predict accurate structural models for many protein complexes, reliably estimating the quality of these predicted models (estimation of model accuracy, or EMA) for model ranking and selection remains a major challenge. A key barrier to developing effective machine learning-based EMA methods is the lack of large, diverse, and well-annotated datasets for training and evaluation. To address this gap, we introduce PSBench, a benchmark suite comprising four large-scale, labeled datasets generated during the 15th and 16th community-wide Critical Assessment of Protein Structure Prediction (CASP15 and CASP16). PSBench includes over one million structural models covering a wide range of protein sequence lengths, complex stoichiometries, functional classes, and modeling difficulties. Each model is annotated with multiple complementary quality scores at the global, local, and interface levels. PSBench also provides multiple evaluation metrics and baseline EMA methods to facilitate rigorous comparisons. To demonstrate PSBench's utility, we trained and evaluated GATE, a graph transformer-based EMA method, on the CASP15 data. GATE was blindly tested in CASP16 (2024), where it ranked among the top-performing EMA methods. These results highlight PSBench as a valuable resource for advancing EMA research in protein complex modeling. PSBench is publicly available at: https://github.com/BioinfoMachineLearning/PSBench.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient deep learning model to Predict Stock Price Movement Based on Limit Order Book</title>
<link>https://arxiv.org/abs/2505.22678</link>
<guid>https://arxiv.org/abs/2505.22678</guid>
<content:encoded><![CDATA[
arXiv:2505.22678v1 Announce Type: cross 
Abstract: In high-frequency trading (HFT), leveraging limit order books (LOB) to model stock price movements is crucial for achieving profitable outcomes. However, this task is challenging due to the high-dimensional and volatile nature of the original data. Even recent deep learning models often struggle to capture price movement patterns effectively, particularly without well-designed features. We observed that raw LOB data exhibits inherent symmetry between the ask and bid sides, and the bid-ask differences demonstrate greater stability and lower complexity compared to the original data. Building on this insight, we propose a novel approach in which leverages the Siamese architecture to enhance the performance of existing deep learning models. The core idea involves processing the ask and bid sides separately using the same module with shared parameters. We applied our Siamese-based methods to several widely used strong baselines and validated their effectiveness using data from 14 military industry stocks in the Chinese A-share market. Furthermore, we integrated multi-head attention (MHA) mechanisms with the Long Short-Term Memory (LSTM) module to investigate its role in modeling stock price movements. Our experiments used raw data and widely used Order Flow Imbalance (OFI) features as input with some strong baseline models. The results show that our method improves the performance of strong baselines in over 75$% of cases, excluding the Multi-Layer Perception (MLP) baseline, which performed poorly and is not considered practical. Furthermore, we found that Multi-Head Attention can enhance model performance, particularly over shorter forecasting horizons.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Fairness Directly from Modularity: a New Way for Fair Community Partitioning</title>
<link>https://arxiv.org/abs/2505.22684</link>
<guid>https://arxiv.org/abs/2505.22684</guid>
<content:encoded><![CDATA[
arXiv:2505.22684v1 Announce Type: cross 
Abstract: Community partitioning is crucial in network analysis, with modularity optimization being the prevailing technique. However, traditional modularity-based methods often overlook fairness, a critical aspect in real-world applications. To address this, we introduce protected group networks and propose a novel fairness-modularity metric. This metric extends traditional modularity by explicitly incorporating fairness, and we prove that minimizing it yields naturally fair partitions for protected groups while maintaining theoretical soundness. We develop a general optimization framework for fairness partitioning and design the efficient Fair Fast Newman (FairFN) algorithm, enhancing the Fast Newman (FN) method to optimize both modularity and fairness. Experiments show FairFN achieves significantly improved fairness and high-quality partitions compared to state-of-the-art methods, especially on unbalanced datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the effectiveness of multimodal data in forecasting SARS-COV-2 case surges</title>
<link>https://arxiv.org/abs/2505.22688</link>
<guid>https://arxiv.org/abs/2505.22688</guid>
<content:encoded><![CDATA[
arXiv:2505.22688v1 Announce Type: cross 
Abstract: The COVID-19 pandemic response relied heavily on statistical and machine learning models to predict key outcomes such as case prevalence and fatality rates. These predictions were instrumental in enabling timely public health interventions that helped break transmission cycles. While most existing models are grounded in traditional epidemiological data, the potential of alternative datasets, such as those derived from genomic information and human behavior, remains underexplored. In the current study, we investigated the usefulness of diverse modalities of feature sets in predicting case surges. Our results highlight the relative effectiveness of biological (e.g., mutations), public health (e.g., case counts, policy interventions) and human behavioral features (e.g., mobility and social media conversations) in predicting country-level case surges. Importantly, we uncover considerable heterogeneity in predictive performance across countries and feature modalities, suggesting that surge prediction models may need to be tailored to specific national contexts and pandemic phases. Overall, our work highlights the value of integrating alternative data sources into existing disease surveillance frameworks to enhance the prediction of pandemic dynamics.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-Computation Gaps in Quantum Learning via Low-Degree Likelihood</title>
<link>https://arxiv.org/abs/2505.22743</link>
<guid>https://arxiv.org/abs/2505.22743</guid>
<content:encoded><![CDATA[
arXiv:2505.22743v1 Announce Type: cross 
Abstract: In a variety of physically relevant settings for learning from quantum data, designing protocols that can computationally efficiently extract information remains largely an art, and there are important cases where we believe this to be impossible, that is, where there is an information-computation gap. While there is a large array of tools in the classical literature for giving evidence for average-case hardness of statistical inference problems, the corresponding tools in the quantum literature are far more limited. One such framework in the classical literature, the low-degree method, makes predictions about hardness of inference problems based on the failure of estimators given by low-degree polynomials. In this work, we extend this framework to the quantum setting.
  We establish a general connection between state designs and low-degree hardness. We use this to obtain the first information-computation gaps for learning Gibbs states of random, sparse, non-local Hamiltonians. We also use it to prove hardness for learning random shallow quantum circuit states in a challenging model where states can be measured in adaptively chosen bases. To our knowledge, the ability to model adaptivity within the low-degree framework was open even in classical settings. In addition, we also obtain a low-degree hardness result for quantum error mitigation against strategies with single-qubit measurements.
  We define a new quantum generalization of the planted biclique problem and identify the threshold at which this problem becomes computationally hard for protocols that perform local measurements. Interestingly, the complexity landscape for this problem shifts when going from local measurements to more entangled single-copy measurements.
  We show average-case hardness for the "standard" variant of Learning Stabilizers with Noise and for agnostically learning product states.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StarBASE-GP: Biologically-Guided Automated Machine Learning for Genotype-to-Phenotype Association Analysis</title>
<link>https://arxiv.org/abs/2505.22746</link>
<guid>https://arxiv.org/abs/2505.22746</guid>
<content:encoded><![CDATA[
arXiv:2505.22746v1 Announce Type: cross 
Abstract: We present the Star-Based Automated Single-locus and Epistasis analysis tool - Genetic Programming (StarBASE-GP), an automated framework for discovering meaningful genetic variants associated with phenotypic variation in large-scale genomic datasets. StarBASE-GP uses a genetic programming-based multi-objective optimization strategy to evolve machine learning pipelines that simultaneously maximize explanatory power (r2) and minimize pipeline complexity. Biological domain knowledge is integrated at multiple stages, including the use of nine inheritance encoding strategies to model deviations from additivity, a custom linkage disequilibrium pruning node that minimizes redundancy among features, and a dynamic variant recommendation system that prioritizes informative candidates for pipeline inclusion. We evaluate StarBASE-GP on a cohort of Rattus norvegicus (brown rat) to identify variants associated with body mass index, benchmarking its performance against a random baseline and a biologically naive version of the tool. StarBASE-GP consistently evolves Pareto fronts with superior performance, yielding higher accuracy in identifying both ground truth and novel quantitative trait loci, highlighting relevant targets for future validation. By incorporating evolutionary search and relevant biological theory into a flexible automated machine learning framework, StarBASE-GP demonstrates robust potential for advancing variant discovery in complex traits.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-orthogonalizing attractor neural networks emerging from the free energy principle</title>
<link>https://arxiv.org/abs/2505.22749</link>
<guid>https://arxiv.org/abs/2505.22749</guid>
<content:encoded><![CDATA[
arXiv:2505.22749v1 Announce Type: cross 
Abstract: Attractor dynamics are a hallmark of many complex systems, including the brain. Understanding how such self-organizing dynamics emerge from first principles is crucial for advancing our understanding of neuronal computations and the design of artificial intelligence systems. Here we formalize how attractor networks emerge from the free energy principle applied to a universal partitioning of random dynamical systems. Our approach obviates the need for explicitly imposed learning and inference rules and identifies emergent, but efficient and biologically plausible inference and learning dynamics for such self-organizing systems. These result in a collective, multi-level Bayesian active inference process. Attractors on the free energy landscape encode prior beliefs; inference integrates sensory data into posterior beliefs; and learning fine-tunes couplings to minimize long-term surprise. Analytically and via simulations, we establish that the proposed networks favor approximately orthogonalized attractor representations, a consequence of simultaneously optimizing predictive accuracy and model complexity. These attractors efficiently span the input subspace, enhancing generalization and the mutual information between hidden causes and observable effects. Furthermore, while random data presentation leads to symmetric and sparse couplings, sequential data fosters asymmetric couplings and non-equilibrium steady-state dynamics, offering a natural extension to conventional Boltzmann Machines. Our findings offer a unifying theory of self-organizing attractor networks, providing novel insights for AI and neuroscience.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposing Elements of Problem Solving: What "Math" Does RL Teach?</title>
<link>https://arxiv.org/abs/2505.22756</link>
<guid>https://arxiv.org/abs/2505.22756</guid>
<content:encoded><![CDATA[
arXiv:2505.22756v1 Announce Type: cross 
Abstract: Mathematical reasoning tasks have become prominent benchmarks for assessing the reasoning capabilities of LLMs, especially with reinforcement learning (RL) methods such as GRPO showing significant performance gains. However, accuracy metrics alone do not support fine-grained assessment of capabilities and fail to reveal which problem-solving skills have been internalized. To better understand these capabilities, we propose to decompose problem solving into fundamental capabilities: Plan (mapping questions to sequences of steps), Execute (correctly performing solution steps), and Verify (identifying the correctness of a solution). Empirically, we find that GRPO mainly enhances the execution skill-improving execution robustness on problems the model already knows how to solve-a phenomenon we call temperature distillation. More importantly, we show that RL-trained models struggle with fundamentally new problems, hitting a 'coverage wall' due to insufficient planning skills. To explore RL's impact more deeply, we construct a minimal, synthetic solution-tree navigation task as an analogy for mathematical problem-solving. This controlled setup replicates our empirical findings, confirming RL primarily boosts execution robustness. Importantly, in this setting, we identify conditions under which RL can potentially overcome the coverage wall through improved exploration and generalization to new solution paths. Our findings provide insights into the role of RL in enhancing LLM reasoning, expose key limitations, and suggest a path toward overcoming these barriers. Code is available at https://github.com/cfpark00/RL-Wall.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-convex entropic mean-field optimization via Best Response flow</title>
<link>https://arxiv.org/abs/2505.22760</link>
<guid>https://arxiv.org/abs/2505.22760</guid>
<content:encoded><![CDATA[
arXiv:2505.22760v1 Announce Type: cross 
Abstract: We study the problem of minimizing non-convex functionals on the space of probability measures, regularized by the relative entropy (KL divergence) with respect to a fixed reference measure, as well as the corresponding problem of solving entropy-regularized non-convex-non-concave min-max problems. We utilize the Best Response flow (also known in the literature as the fictitious play flow) and study how its convergence is influenced by the relation between the degree of non-convexity of the functional under consideration, the regularization parameter and the tail behaviour of the reference measure. In particular, we demonstrate how to choose the regularizer, given the non-convex functional, so that the Best Response operator becomes a contraction with respect to the $L^1$-Wasserstein distance, which then ensures the existence of its unique fixed point, which is then shown to be the unique global minimizer for our optimization problem. This extends recent results where the Best Response flow was applied to solve convex optimization problems regularized by the relative entropy with respect to arbitrary reference measures, and with arbitrary values of the regularization parameter. Our results explain precisely how the assumption of convexity can be relaxed, at the expense of making a specific choice of the regularizer. Additionally, we demonstrate how these results can be applied in reinforcement learning in the context of policy optimization for Markov Decision Processes and Markov games with softmax parametrized policies in the mean-field regime.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIAS-SAM: Medical Image Anomaly Segmentation without thresholding</title>
<link>https://arxiv.org/abs/2505.22762</link>
<guid>https://arxiv.org/abs/2505.22762</guid>
<content:encoded><![CDATA[
arXiv:2505.22762v1 Announce Type: cross 
Abstract: This paper presents MIAS-SAM, a novel approach for the segmentation of anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to store relevant image features, which are extracted from normal data using the SAM encoder. At inference time, the embedding patches extracted from the SAM encoder are compared with those in the memory bank to obtain the anomaly map. Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt the SAM decoder, obtaining an accurate segmentation from the previously extracted features. Differently from prior works, MIAS-SAM does not require to define a threshold value to obtain the segmentation from the anomaly map. Experimental results conducted on three publicly available datasets, each with a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show accurate anomaly segmentation capabilities measured using DICE score. The code is available at: https://github.com/warpcut/MIAS-SAM
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-Sample Convergence Bounds for Trust Region Policy Optimization in Mean-Field Games</title>
<link>https://arxiv.org/abs/2505.22781</link>
<guid>https://arxiv.org/abs/2505.22781</guid>
<content:encoded><![CDATA[
arXiv:2505.22781v1 Announce Type: cross 
Abstract: We introduce Mean-Field Trust Region Policy Optimization (MF-TRPO), a novel algorithm designed to compute approximate Nash equilibria for ergodic Mean-Field Games (MFG) in finite state-action spaces. Building on the well-established performance of TRPO in the reinforcement learning (RL) setting, we extend its methodology to the MFG framework, leveraging its stability and robustness in policy optimization. Under standard assumptions in the MFG literature, we provide a rigorous analysis of MF-TRPO, establishing theoretical guarantees on its convergence. Our results cover both the exact formulation of the algorithm and its sample-based counterpart, where we derive high-probability guarantees and finite sample complexity. This work advances MFG optimization by bridging RL techniques with mean-field decision-making, offering a theoretically grounded approach to solving complex multi-agent problems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Convolutional Autoencoder for Interference Mitigation in FMCW Radar Altimeters</title>
<link>https://arxiv.org/abs/2505.22783</link>
<guid>https://arxiv.org/abs/2505.22783</guid>
<content:encoded><![CDATA[
arXiv:2505.22783v1 Announce Type: cross 
Abstract: We investigate the end-to-end altitude estimation performance of a convolutional autoencoder-based interference mitigation approach for frequency-modulated continuous-wave (FMCW) radar altimeters. Specifically, we show that a Temporal Convolutional Network (TCN) autoencoder effectively exploits temporal correlations in the received signal, providing superior interference suppression compared to a Least Mean Squares (LMS) adaptive filter. Unlike existing approaches, the present method operates directly on the received FMCW signal. Additionally, we identify key challenges in applying deep learning to wideband FMCW interference mitigation and outline directions for future research to enhance real-time feasibility and generalization to arbitrary interference conditions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation</title>
<link>https://arxiv.org/abs/2505.22805</link>
<guid>https://arxiv.org/abs/2505.22805</guid>
<content:encoded><![CDATA[
arXiv:2505.22805v1 Announce Type: cross 
Abstract: In order to navigate safely and reliably in off-road and unstructured environments, robots must detect anomalies that are out-of-distribution (OOD) with respect to the training data. We present an analysis-by-synthesis approach for pixel-wise anomaly detection without making any assumptions about the nature of OOD data. Given an input image, we use a generative diffusion model to synthesize an edited image that removes anomalies while keeping the remaining image unchanged. Then, we formulate anomaly detection as analyzing which image segments were modified by the diffusion model. We propose a novel inference approach for guided diffusion by analyzing the ideal guidance gradient and deriving a principled approximation that bootstraps the diffusion model to predict guidance gradients. Our editing technique is purely test-time that can be integrated into existing workflows without the need for retraining or fine-tuning. Finally, we use a combination of vision-language foundation models to compare pixels in a learned feature space and detect semantically meaningful edits, enabling accurate anomaly detection for off-road navigation. Project website: https://siddancha.github.io/anomalies-by-diffusion-synthesis/
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution free M-estimation</title>
<link>https://arxiv.org/abs/2505.22807</link>
<guid>https://arxiv.org/abs/2505.22807</guid>
<content:encoded><![CDATA[
arXiv:2505.22807v1 Announce Type: cross 
Abstract: The basic question of delineating those statistical problems that are solvable without making any assumptions on the underlying data distribution has long animated statistics and learning theory. This paper characterizes when a (univariate) convex M-estimation or stochastic optimization problem is solvable in such an assumption-free setting, providing a precise dividing line between solvable and unsolvable problems. The conditions we identify show, perhaps surprisingly, that Lipschitz continuity of the loss being minimized is not necessary for distribution free minimization, and they are also distinct from classical characterizations of learnability in machine learning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highly Efficient and Effective LLMs with Multi-Boolean Architectures</title>
<link>https://arxiv.org/abs/2505.22811</link>
<guid>https://arxiv.org/abs/2505.22811</guid>
<content:encoded><![CDATA[
arXiv:2505.22811v1 Announce Type: cross 
Abstract: Weight binarization has emerged as a promising strategy to drastically reduce the complexity of large language models (LLMs). It is mainly classified into two approaches: post-training binarization and finetuning with training-aware binarization methods. The first approach, while having low complexity, leads to significant loss of information from the original LLMs, resulting in poor performance. The second approach, on the other hand, relies heavily on full-precision latent weights for gradient approximation of binary weights, which not only remains suboptimal but also introduces substantial complexity. In this paper, we introduce a novel framework that effectively transforms LLMs into multi-kernel Boolean parameters, for the first time, finetunes them directly in the Boolean domain, eliminating the need for expensive latent weights. This significantly reduces complexity during both finetuning and inference. Through extensive and insightful experiments across a wide range of LLMs, we demonstrate that our method outperforms recent ultra low-bit quantization and binarization methods.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction</title>
<link>https://arxiv.org/abs/2505.22815</link>
<guid>https://arxiv.org/abs/2505.22815</guid>
<content:encoded><![CDATA[
arXiv:2505.22815v1 Announce Type: cross 
Abstract: Irregular Multivariate Time Series (IMTS) forecasting is challenging due to the unaligned nature of multi-channel signals and the prevalence of extensive missing data. Existing methods struggle to capture reliable temporal patterns from such data due to significant missing values. While pre-trained foundation models show potential for addressing these challenges, they are typically designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel information and its success in RTS forecasting, we propose VIMTS, a framework adapting Visual MAE for IMTS forecasting. To mitigate the effect of missing values, VIMTS first processes IMTS along the timeline into feature patches at equal intervals. These patches are then complemented using learned cross-channel dependencies. Then it leverages visual MAE's capability in handling sparse multichannel data for patch reconstruction, followed by a coarse-to-fine technique to generate precise predictions from focused contexts. In addition, we integrate self-supervised learning for improved IMTS modeling by adapting the visual MAE to IMTS data. Extensive experiments demonstrate VIMTS's superior performance and few-shot capability, advancing the application of visual foundation models in more general time series tasks. Our code is available at https://github.com/WHU-HZY/VIMTS.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation</title>
<link>https://arxiv.org/abs/2505.22842</link>
<guid>https://arxiv.org/abs/2505.22842</guid>
<content:encoded><![CDATA[
arXiv:2505.22842v1 Announce Type: cross 
Abstract: Transformer-based language models rely on positional encoding (PE) to handle token order and support context length extrapolation. However, existing PE methods lack theoretical clarity and rely on limited evaluation metrics to substantiate their extrapolation claims. We propose the Bayesian Attention Mechanism (BAM), a theoretical framework that formulates positional encoding as a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE and ALiBi) and motivates a new Generalized Gaussian positional prior that substantially improves long-context generalization. Empirically, BAM enables accurate information retrieval at $500\times$ the training context length, outperforming previous state-of-the-art context length generalization in long context retrieval accuracy while maintaining comparable perplexity and introducing minimal additional parameters.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding</title>
<link>https://arxiv.org/abs/2505.22857</link>
<guid>https://arxiv.org/abs/2505.22857</guid>
<content:encoded><![CDATA[
arXiv:2505.22857v1 Announce Type: cross 
Abstract: Statistical n-gram language models are widely used for context-biasing tasks in Automatic Speech Recognition (ASR). However, existing implementations lack computational efficiency due to poor parallelization, making context-biasing less appealing for industrial use. This work rethinks data structures for statistical n-gram language models to enable fast and parallel operations for GPU-optimized inference. Our approach, named NGPU-LM, introduces customizable greedy decoding for all major ASR model types - including transducers, attention encoder-decoder models, and CTC - with less than 7% computational overhead. The proposed approach can eliminate more than 50% of the accuracy gap between greedy and beam search for out-of-domain scenarios while avoiding significant slowdown caused by beam search. The implementation of the proposed NGPU-LM is open-sourced.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permissioned LLMs: Enforcing Access Control in Large Language Models</title>
<link>https://arxiv.org/abs/2505.22860</link>
<guid>https://arxiv.org/abs/2505.22860</guid>
<content:encoded><![CDATA[
arXiv:2505.22860v1 Announce Type: cross 
Abstract: In enterprise settings, organizational data is segregated, siloed and carefully protected by elaborate access control frameworks. These access control structures can completely break down if an LLM fine-tuned on the siloed data serves requests, for downstream tasks, from individuals with disparate access privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs that superimpose the organizational data access control structures on query responses they generate. We formalize abstractions underpinning the means to determine whether access control enforcement happens correctly over LLM query responses. Our formalism introduces the notion of a relevant response that can be used to prove whether a PermLLM mechanism has been implemented correctly. We also introduce a novel metric, called access advantage, to empirically evaluate the efficacy of a PermLLM mechanism. We introduce three novel PermLLM mechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired access control. We furthermore present two instantiations of access advantage--(i) Domain Distinguishability Index (DDI) based on Membership Inference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility evaluation. We demonstrate the efficacy of our PermLLM mechanisms through extensive experiments on four public datasets (GPQA, RCV1, SimpleQA, and WMDP), in addition to evaluating the validity of DDI and UGI metrics themselves for quantifying access control in LLMs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrossNAS: A Cross-Layer Neural Architecture Search Framework for PIM Systems</title>
<link>https://arxiv.org/abs/2505.22868</link>
<guid>https://arxiv.org/abs/2505.22868</guid>
<content:encoded><![CDATA[
arXiv:2505.22868v1 Announce Type: cross 
Abstract: In this paper, we propose the CrossNAS framework, an automated approach for exploring a vast, multidimensional search space that spans various design abstraction layers-circuits, architecture, and systems-to optimize the deployment of machine learning workloads on analog processing-in-memory (PIM) systems. CrossNAS leverages the single-path one-shot weight-sharing strategy combined with the evolutionary search for the first time in the context of PIM system mapping and optimization. CrossNAS sets a new benchmark for PIM neural architecture search (NAS), outperforming previous methods in both accuracy and energy efficiency while maintaining comparable or shorter search times.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models</title>
<link>https://arxiv.org/abs/2505.22869</link>
<guid>https://arxiv.org/abs/2505.22869</guid>
<content:encoded><![CDATA[
arXiv:2505.22869v1 Announce Type: cross 
Abstract: Existing PLMs generate protein sequences based on a single-condition constraint from a specific modality, struggling to simultaneously satisfy multiple constraints across different modalities. In this work, we introduce CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein GENeration. CFP-Gen facilitates the de novo protein design by integrating multimodal conditions with functional, sequence, and structural constraints. Specifically, an Annotation-Guided Feature Modulation (AGFM) module is introduced to dynamically adjust the protein feature distribution based on composable functional annotations, e.g., GO terms, IPR domains and EC numbers. Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures residue-wise interaction to ensure more precise control. Additionally, off-the-shelf 3D structure encoders can be seamlessly integrated to impose geometric constraints. We demonstrate that CFP-Gen enables high-throughput generation of novel proteins with functionality comparable to natural proteins, while achieving a high success rate in designing multifunctional proteins. Code and data available at https://github.com/yinjunbo/cfpgen.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.22914</link>
<guid>https://arxiv.org/abs/2505.22914</guid>
<content:encoded><![CDATA[
arXiv:2505.22914v1 Announce Type: cross 
Abstract: Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging</title>
<link>https://arxiv.org/abs/2505.22934</link>
<guid>https://arxiv.org/abs/2505.22934</guid>
<content:encoded><![CDATA[
arXiv:2505.22934v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LMs) for individual tasks yields strong performance but is expensive for deployment and storage. Recent works explore model merging to combine multiple task-specific models into a single multi-task model without additional training. However, existing merging methods often fail for models fine-tuned with low-rank adaptation (LoRA), due to significant performance degradation. In this paper, we show that this issue arises from a previously overlooked interplay between model parameters and data distributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM) to constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates relevant to one task do not adversely shift outputs for others. Our approach can seamlessly integrate with most existing merging algorithms, reducing the unintended interference among tasks. Extensive experiments on eight datasets, tested with three widely used LMs and two large LMs, demonstrate that our method not only boosts merging performance but also preserves single-task accuracy. Furthermore, our approach exhibits greater robustness to the hyperparameters of merging. These results highlight the importance of data-parameter interaction in model merging and offer a plug-and-play solution for merging LoRA models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Social Choice: The Next Generation</title>
<link>https://arxiv.org/abs/2505.22939</link>
<guid>https://arxiv.org/abs/2505.22939</guid>
<content:encoded><![CDATA[
arXiv:2505.22939v1 Announce Type: cross 
Abstract: A key task in certain democratic processes is to produce a concise slate of statements that proportionally represents the full spectrum of user opinions. This task is similar to committee elections, but unlike traditional settings, the candidate set comprises all possible statements of varying lengths, and so it can only be accessed through specific queries. Combining social choice and large language models, prior work has approached this challenge through a framework of generative social choice. We extend the framework in two fundamental ways, providing theoretical guarantees even in the face of approximately optimal queries and a budget limit on the overall length of the slate. Using GPT-4o to implement queries, we showcase our approach on datasets related to city improvement measures and drug reviews, demonstrating its effectiveness in generating representative slates from unstructured user opinions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates</title>
<link>https://arxiv.org/abs/2505.22943</link>
<guid>https://arxiv.org/abs/2505.22943</guid>
<content:encoded><![CDATA[
arXiv:2505.22943v1 Announce Type: cross 
Abstract: While pre-trained multimodal representations (e.g., CLIP) have shown impressive capabilities, they exhibit significant compositional vulnerabilities leading to counterintuitive judgments. We introduce Multimodal Adversarial Compositionality (MAC), a benchmark that leverages large language models (LLMs) to generate deceptive text samples to exploit these vulnerabilities across different modalities and evaluates them through both sample-wise attack success rate and group-wise entropy-based diversity. To improve zero-shot methods, we propose a self-training approach that leverages rejection-sampling fine-tuning with diversity-promoting filtering, which enhances both attack success rate and sample diversity. Using smaller language models like Llama-3.1-8B, our approach demonstrates superior performance in revealing compositional vulnerabilities across various multimodal representations, including images, videos, and audios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NegVQA: Can Vision Language Models Understand Negation?</title>
<link>https://arxiv.org/abs/2505.22946</link>
<guid>https://arxiv.org/abs/2505.22946</guid>
<content:encoded><![CDATA[
arXiv:2505.22946v1 Announce Type: cross 
Abstract: Negation is a fundamental linguistic phenomenon that can entirely reverse the meaning of a sentence. As vision language models (VLMs) continue to advance and are deployed in high-stakes applications, assessing their ability to comprehend negation becomes essential. To address this, we introduce NegVQA, a visual question answering (VQA) benchmark consisting of 7,379 two-choice questions covering diverse negation scenarios and image-question distributions. We construct NegVQA by leveraging large language models to generate negated versions of questions from existing VQA datasets. Evaluating 20 state-of-the-art VLMs across seven model families, we find that these models struggle significantly with negation, exhibiting a substantial performance drop compared to their responses to the original questions. Furthermore, we uncover a U-shaped scaling trend, where increasing model size initially degrades performance on NegVQA before leading to improvements. Our benchmark reveals critical gaps in VLMs' negation understanding and offers insights into future VLM development. Project page available at https://yuhui-zh15.github.io/NegVQA/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness</title>
<link>https://arxiv.org/abs/2505.22960</link>
<guid>https://arxiv.org/abs/2505.22960</guid>
<content:encoded><![CDATA[
arXiv:2505.22960v1 Announce Type: cross 
Abstract: The remarkable growth in large language model (LLM) capabilities has spurred exploration into multi-agent systems, with debate frameworks emerging as a promising avenue for enhanced problem-solving. These multi-agent debate (MAD) approaches, where agents collaboratively present, critique, and refine arguments, potentially offer improved reasoning, robustness, and diverse perspectives over monolithic models. Despite prior studies leveraging MAD, a systematic understanding of its effectiveness compared to self-agent methods, particularly under varying conditions, remains elusive. This paper seeks to fill this gap by conceptualizing MAD as a test-time computational scaling technique, distinguished by collaborative refinement and diverse exploration capabilities. We conduct a comprehensive empirical investigation comparing MAD with strong self-agent test-time scaling baselines on mathematical reasoning and safety-related tasks. Our study systematically examines the influence of task difficulty, model scale, and agent diversity on MAD's performance. Key findings reveal that, for mathematical reasoning, MAD offers limited advantages over self-agent scaling but becomes more effective with increased problem difficulty and decreased model capability, while agent diversity shows little benefit. Conversely, for safety tasks, MAD's collaborative refinement can increase vulnerability, but incorporating diverse agent configurations facilitates a gradual reduction in attack success through the collaborative refinement process. We believe our findings provide critical guidance for the future development of more effective and strategically deployed MAD systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind</title>
<link>https://arxiv.org/abs/2505.22961</link>
<guid>https://arxiv.org/abs/2505.22961</guid>
<content:encoded><![CDATA[
arXiv:2505.22961v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponent's thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, we introduce Theory of Mind Augmented Persuader (ToMAP), a novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuader's awareness and analysis of the opponent's mental state. Specifically, we begin by prompting the persuader to consider possible objections to the target central claim, and then use a text encoder paired with a trained MLP classifier to predict the opponent's current stance on these counterclaims. Our carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with a relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore our method's effectiveness and highlight its potential for developing more persuasive language agents. Code is available at: https://github.com/ulab-uiuc/ToMAP.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Scaling Laws for EHR Foundation Models</title>
<link>https://arxiv.org/abs/2505.22964</link>
<guid>https://arxiv.org/abs/2505.22964</guid>
<content:encoded><![CDATA[
arXiv:2505.22964v1 Announce Type: cross 
Abstract: The emergence of scaling laws has profoundly shaped the development of large language models (LLMs), enabling predictable performance gains through systematic increases in model size, dataset volume, and compute. Yet, these principles remain largely unexplored in the context of electronic health records (EHRs) -- a rich, sequential, and globally abundant data source that differs structurally from natural language. In this work, we present the first empirical investigation of scaling laws for EHR foundation models. By training transformer architectures on patient timeline data from the MIMIC-IV database across varying model sizes and compute budgets, we identify consistent scaling patterns, including parabolic IsoFLOPs curves and power-law relationships between compute, model parameters, data size, and clinical utility. These findings demonstrate that EHR models exhibit scaling behavior analogous to LLMs, offering predictive insights into resource-efficient training strategies. Our results lay the groundwork for developing powerful EHR foundation models capable of transforming clinical prediction tasks and advancing personalized healthcare.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning coordinated badminton skills for legged manipulators</title>
<link>https://arxiv.org/abs/2505.22974</link>
<guid>https://arxiv.org/abs/2505.22974</guid>
<content:encoded><![CDATA[
arXiv:2505.22974v1 Announce Type: cross 
Abstract: Coordinating the motion between lower and upper limbs and aligning limb control with perception are substantial challenges in robotics, particularly in dynamic environments. To this end, we introduce an approach for enabling legged mobile manipulators to play badminton, a task that requires precise coordination of perception, locomotion, and arm swinging. We propose a unified reinforcement learning-based control policy for whole-body visuomotor skills involving all degrees of freedom to achieve effective shuttlecock tracking and striking. This policy is informed by a perception noise model that utilizes real-world camera data, allowing for consistent perception error levels between simulation and deployment and encouraging learned active perception behaviors. Our method includes a shuttlecock prediction model, constrained reinforcement learning for robust motion control, and integrated system identification techniques to enhance deployment readiness. Extensive experimental results in a variety of environments validate the robot's capability to predict shuttlecock trajectories, navigate the service area effectively, and execute precise strikes against human players, demonstrating the feasibility of using legged mobile manipulators in complex and dynamic sports scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design</title>
<link>https://arxiv.org/abs/2505.22990</link>
<guid>https://arxiv.org/abs/2505.22990</guid>
<content:encoded><![CDATA[
arXiv:2505.22990v1 Announce Type: cross 
Abstract: RF/Analog design is essential for bridging digital technologies with real-world signals, ensuring the functionality and reliability of a wide range of electronic systems. However, analog design procedures are often intricate, time-consuming and reliant on expert intuition, and hinder the time and cost efficiency of circuit development. To overcome the limitations of the manual circuit design, we introduce MenTeR - a multiagent workflow integrated into an end-to-end analog design framework. By employing multiple specialized AI agents that collaboratively address different aspects of the design process, such as specification understanding, circuit optimization, and test bench validation, MenTeR reduces the dependency on frequent trial-and-error-style intervention. MenTeR not only accelerates the design cycle time but also facilitates a broader exploration of the design space, demonstrating robust capabilities in handling real-world analog systems. We believe that MenTeR lays the groundwork for future "RF/Analog Copilots" that can collaborate seamlessly with human designers.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Foundations of the Deep Copula Classifier: A Generative Approach to Modeling Dependent Features</title>
<link>https://arxiv.org/abs/2505.22997</link>
<guid>https://arxiv.org/abs/2505.22997</guid>
<content:encoded><![CDATA[
arXiv:2505.22997v1 Announce Type: cross 
Abstract: Traditional classifiers often assume feature independence or rely on overly simplistic relationships, leading to poor performance in settings where real-world dependencies matter. We introduce the Deep Copula Classifier (DCC), a generative model that separates the learning of each feature's marginal distribution from the modeling of their joint dependence structure via neural network-parameterized copulas. For each class, lightweight neural networks are used to flexibly and adaptively capture feature interactions, making DCC particularly effective when classification is driven by complex dependencies. We establish that DCC converges to the Bayes-optimal classifier under standard conditions and provide explicit convergence rates of O(n^{-r/(2r + d)}) for r-smooth copula densities. Beyond theoretical guarantees, we outline several practical extensions, including high-dimensional scalability through vine and factor copula architectures, semi-supervised learning via entropy regularization, and online adaptation using streaming gradient methods. By unifying statistical rigor with the representational power of neural networks, DCC offers a mathematically grounded and interpretable framework for dependency-aware classification.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction</title>
<link>https://arxiv.org/abs/2505.23034</link>
<guid>https://arxiv.org/abs/2505.23034</guid>
<content:encoded><![CDATA[
arXiv:2505.23034v1 Announce Type: cross 
Abstract: Drug-drug interaction (DDI) prediction is critical for treatment safety. While large language models (LLMs) show promise in pharmaceutical tasks, their effectiveness in DDI prediction remains challenging. Inspired by the well-established clinical practice where physicians routinely reference similar historical cases to guide their decisions through case-based reasoning (CBR), we propose CBR-DDI, a novel framework that distills pharmacological principles from historical cases to improve LLM reasoning for DDI tasks. CBR-DDI constructs a knowledge repository by leveraging LLMs to extract pharmacological insights and graph neural networks (GNNs) to model drug associations. A hybrid retrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to effectively retrieve and reuse relevant cases. We further introduce a representative sampling strategy for dynamic case refinement. Extensive experiments demonstrate that CBR-DDI achieves state-of-the-art performance, with a significant 28.7% accuracy improvement over both popular LLMs and CBR baseline, while maintaining high interpretability and flexibility.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Last-Iterate Convergence of Shuffling Gradient Methods for Nonsmooth Convex Optimization</title>
<link>https://arxiv.org/abs/2505.23056</link>
<guid>https://arxiv.org/abs/2505.23056</guid>
<content:encoded><![CDATA[
arXiv:2505.23056v1 Announce Type: cross 
Abstract: We study the convergence of the shuffling gradient method, a popular algorithm employed to minimize the finite-sum function with regularization, in which functions are passed to apply (Proximal) Gradient Descent (GD) one by one whose order is determined by a permutation on the indices of functions. In contrast to its easy implementation and effective performance in practice, the theoretical understanding remains limited. A recent advance by (Liu & Zhou, 2024b) establishes the first last-iterate convergence results under various settings, especially proving the optimal rates for smooth (strongly) convex optimization. However, their bounds for nonsmooth (strongly) convex functions are only as fast as Proximal GD. In this work, we provide the first improved last-iterate analysis for the nonsmooth case demonstrating that the widely used Random Reshuffle ($\textsf{RR}$) and Single Shuffle ($\textsf{SS}$) strategies are both provably faster than Proximal GD, reflecting the benefit of randomness. As an important implication, we give the first (nearly) optimal convergence result for the suffix average under the $\textsf{RR}$ sampling scheme in the general convex case, matching the lower bound shown by (Koren et al., 2022).
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Quantum Approximate $k$NN Algorithm via Granular-Ball Computing</title>
<link>https://arxiv.org/abs/2505.23066</link>
<guid>https://arxiv.org/abs/2505.23066</guid>
<content:encoded><![CDATA[
arXiv:2505.23066v1 Announce Type: cross 
Abstract: High time complexity is one of the biggest challenges faced by $k$-Nearest Neighbors ($k$NN). Although current classical and quantum $k$NN algorithms have made some improvements, they still have a speed bottleneck when facing large amounts of data. To address this issue, we propose an innovative algorithm called Granular-Ball based Quantum $k$NN(GB-Q$k$NN). This approach achieves higher efficiency by first employing granular-balls, which reduces the data size needed to processed. The search process is then accelerated by adopting a Hierarchical Navigable Small World (HNSW) method. Moreover, we optimize the time-consuming steps, such as distance calculation, of the HNSW via quantization, further reducing the time complexity of the construct and search process. By combining the use of granular-balls and quantization of the HNSW method, our approach manages to take advantage of these treatments and significantly reduces the time complexity of the $k$NN-like algorithms, as revealed by a comprehensive complexity analysis.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble</title>
<link>https://arxiv.org/abs/2505.23075</link>
<guid>https://arxiv.org/abs/2505.23075</guid>
<content:encoded><![CDATA[
arXiv:2505.23075v1 Announce Type: cross 
Abstract: Despite the growing clinical adoption of large language models (LLMs), current approaches heavily rely on single model architectures. To overcome risks of obsolescence and rigid dependence on single model systems, we present a novel framework, termed the Consensus Mechanism. Mimicking clinical triage and multidisciplinary clinical decision-making, the Consensus Mechanism implements an ensemble of specialized medical expert agents enabling improved clinical decision making while maintaining robust adaptability. This architecture enables the Consensus Mechanism to be optimized for cost, latency, or performance, purely based on its interior model configuration.
  To rigorously evaluate the Consensus Mechanism, we employed three medical evaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the differential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism achieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and Google's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an increase in accuracy on MedQA ($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 3.4\%$) and MedMCQA ($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 9.1\%$). These accuracy gains extended to differential diagnosis generation, where our system demonstrated improved recall and precision (F1$_\mathrm{consensus}$ = 0.326 vs. F1$_{\mathrm{O3\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX (Top1$_\mathrm{consensus}$ = 52.0% vs. Top1$_{\mathrm{O3\text{-}high}}$ = 45.2%).
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Methods with Online Scaling Part I. Theoretical Foundations</title>
<link>https://arxiv.org/abs/2505.23081</link>
<guid>https://arxiv.org/abs/2505.23081</guid>
<content:encoded><![CDATA[
arXiv:2505.23081v1 Announce Type: cross 
Abstract: This paper establishes the theoretical foundations of the online scaled gradient methods (OSGM), a framework that utilizes online learning to adapt stepsizes and provably accelerate first-order methods. OSGM quantifies the effectiveness of a stepsize by a feedback function motivated from a convergence measure and uses the feedback to adjust the stepsize through an online learning algorithm. Consequently, instantiations of OSGM achieve convergence rates that are asymptotically no worse than the optimal stepsize. OSGM yields desirable convergence guarantees on smooth convex problems, including 1) trajectory-dependent global convergence on smooth convex objectives; 2) an improved complexity result on smooth strongly convex problems, and 3) local superlinear convergence. Notably, OSGM constitutes a new family of first-order methods with non-asymptotic superlinear convergence, joining the celebrated quasi-Newton methods. Finally, OSGM explains the empirical success of the popular hypergradient-descent heuristic in optimization for machine learning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals</title>
<link>https://arxiv.org/abs/2505.23124</link>
<guid>https://arxiv.org/abs/2505.23124</guid>
<content:encoded><![CDATA[
arXiv:2505.23124v1 Announce Type: cross 
Abstract: We initiate the study of a repeated principal-agent problem over a finite horizon $T$, where a principal sequentially interacts with $K\geq 2$ types of agents arriving in an adversarial order. At each round, the principal strategically chooses one of the $N$ arms to incentivize for an arriving agent of unknown type. The agent then chooses an arm based on its own utility and the provided incentive, and the principal receives a corresponding reward. The objective is to minimize regret against the best incentive in hindsight. Without prior knowledge of agent behavior, we show that the problem becomes intractable, leading to linear regret. We analyze two key settings where sublinear regret is achievable. In the first setting, the principal knows the arm each agent type would select greedily for any given incentive. Under this setting, we propose an algorithm that achieves a regret bound of $O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$ and provide a matching lower bound up to a $\log K$ factor. In the second setting, an agent's response varies smoothly with the incentive and is governed by a Lipschitz constant $L\geq 1$. Under this setting, we show that there is an algorithm with a regret bound of $\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to logarithmic factors. Finally, we extend our algorithmic results for both settings by allowing the principal to incentivize multiple arms simultaneously in each round.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing</title>
<link>https://arxiv.org/abs/2505.23145</link>
<guid>https://arxiv.org/abs/2505.23145</guid>
<content:encoded><![CDATA[
arXiv:2505.23145v1 Announce Type: cross 
Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3, enabling text-driven manipulation by solving an ordinary differential equation (ODE). While the lack of exact latent inversion is a core advantage of these methods, it often results in unstable editing trajectories and poor source consistency. To address this limitation, we propose FlowAlign, a novel inversion-free flow-based framework for consistent image editing with principled trajectory control. FlowAlign introduces a flow-matching loss as a regularization mechanism to promote smoother and more stable trajectories during the editing process. Notably, the flow-matching loss is shown to explicitly balance semantic alignment with the edit prompt and structural consistency with the source image along the trajectory. Furthermore, FlowAlign naturally supports reverse editing by simply reversing the ODE trajectory, highlighting the reversible and consistent nature of the transformation. Extensive experiments demonstrate that FlowAlign outperforms existing methods in both source preservation and editing controllability.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Adaptive Least Mean Squares Algorithms over Simplicial Complexes</title>
<link>https://arxiv.org/abs/2505.23160</link>
<guid>https://arxiv.org/abs/2505.23160</guid>
<content:encoded><![CDATA[
arXiv:2505.23160v1 Announce Type: cross 
Abstract: This paper introduces a novel adaptive framework for processing dynamic flow signals over simplicial complexes, extending classical least-mean-squares (LMS) methods to high-order topological domains. Building on discrete Hodge theory, we present a topological LMS algorithm that efficiently processes streaming signals observed over time-varying edge subsets. We provide a detailed stochastic analysis of the algorithm, deriving its stability conditions, steady-state mean-square-error, and convergence speed, while exploring the impact of edge sampling on performance. We also propose strategies to design optimal edge sampling probabilities, minimizing rate while ensuring desired estimation accuracy. Assuming partial knowledge of the complex structure (e.g., the underlying graph), we introduce an adaptive topology inference method that integrates with the proposed LMS framework. Additionally, we propose a distributed version of the algorithm and analyze its stability and mean-square-error properties. Empirical results on synthetic and real-world traffic data demonstrate that our approach, in both centralized and distributed settings, outperforms graph-based LMS methods by leveraging higher-order topological features.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Inversion turns CLIP into a Decoder</title>
<link>https://arxiv.org/abs/2505.23161</link>
<guid>https://arxiv.org/abs/2505.23161</guid>
<content:encoded><![CDATA[
arXiv:2505.23161v1 Announce Type: cross 
Abstract: CLIP is a discriminative model trained to align images and text in a shared embedding space. Due to its multimodal structure, it serves as the backbone of many generative pipelines, where a decoder is trained to map from the shared space back to images. In this work, we show that image synthesis is nevertheless possible using CLIP alone -- without any decoder, training, or fine-tuning. Our approach optimizes a frequency-aware implicit neural representation that encourages coarse-to-fine generation by stratifying frequencies across network layers. To stabilize this inverse mapping, we introduce adversarially robust initialization, a lightweight Orthogonal Procrustes projection to align local text and image embeddings, and a blending loss that anchors outputs to natural image statistics. Without altering CLIP's weights, this framework unlocks capabilities such as text-to-image generation, style transfer, and image reconstruction. These findings suggest that discriminative models may hold untapped generative potential, hidden in plain sight.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JAPAN: Joint Adaptive Prediction Areas with Normalising-Flows</title>
<link>https://arxiv.org/abs/2505.23196</link>
<guid>https://arxiv.org/abs/2505.23196</guid>
<content:encoded><![CDATA[
arXiv:2505.23196v1 Announce Type: cross 
Abstract: Conformal prediction provides a model-agnostic framework for uncertainty quantification with finite-sample validity guarantees, making it an attractive tool for constructing reliable prediction sets. However, existing approaches commonly rely on residual-based conformity scores, which impose geometric constraints and struggle when the underlying distribution is multimodal. In particular, they tend to produce overly conservative prediction areas centred around the mean, often failing to capture the true shape of complex predictive distributions. In this work, we introduce JAPAN (Joint Adaptive Prediction Areas with Normalising-Flows), a conformal prediction framework that uses density-based conformity scores. By leveraging flow-based models, JAPAN estimates the (predictive) density and constructs prediction areas by thresholding on the estimated density scores, enabling compact, potentially disjoint, and context-adaptive regions that retain finite-sample coverage guarantees. We theoretically motivate the efficiency of JAPAN and empirically validate it across multivariate regression and forecasting tasks, demonstrating good calibration and tighter prediction areas compared to existing baselines. We also provide several \emph{extensions} adding flexibility to our proposed framework.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM</title>
<link>https://arxiv.org/abs/2505.23207</link>
<guid>https://arxiv.org/abs/2505.23207</guid>
<content:encoded><![CDATA[
arXiv:2505.23207v1 Announce Type: cross 
Abstract: Overlapping Speech Detection (OSD) aims to identify regions where multiple speakers overlap in a conversation, a critical challenge in multi-party speech processing. This work proposes a speaker-aware progressive OSD model that leverages a progressive training strategy to enhance the correlation between subtasks such as voice activity detection (VAD) and overlap detection. To improve acoustic representation, we explore the effectiveness of state-of-the-art self-supervised learning (SSL) models, including WavLM and wav2vec 2.0, while incorporating a speaker attention module to enrich features with frame-level speaker information. Experimental results show that the proposed method achieves state-of-the-art performance, with an F1 score of 82.76\% on the AMI test set, demonstrating its robustness and effectiveness in OSD.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory Generator Matching for Time Series</title>
<link>https://arxiv.org/abs/2505.23215</link>
<guid>https://arxiv.org/abs/2505.23215</guid>
<content:encoded><![CDATA[
arXiv:2505.23215v1 Announce Type: cross 
Abstract: Accurately modeling time-continuous stochastic processes from irregular observations remains a significant challenge. In this paper, we leverage ideas from generative modeling of image data to push the boundary of time series generation. For this, we find new generators of SDEs and jump processes, inspired by trajectory flow matching, that have the marginal distributions of the time series of interest. Specifically, we can handle discontinuities of the underlying processes by parameterizing the jump kernel densities by scaled Gaussians that allow for closed form formulas of the corresponding Kullback-Leibler divergence in the loss. Unlike most other approaches, we are able to handle irregularly sampled time series.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint estimation of smooth graph signals from partial linear measurements</title>
<link>https://arxiv.org/abs/2505.23240</link>
<guid>https://arxiv.org/abs/2505.23240</guid>
<content:encoded><![CDATA[
arXiv:2505.23240v1 Announce Type: cross 
Abstract: Given an undirected and connected graph $G$ on $T$ vertices, suppose each vertex $t$ has a latent signal $x_t \in \mathbb{R}^n$ associated to it. Given partial linear measurements of the signals, for a potentially small subset of the vertices, our goal is to estimate $x_t$'s. Assuming that the signals are smooth w.r.t $G$, in the sense that the quadratic variation of the signals over the graph is small, we obtain non-asymptotic bounds on the mean squared error for jointly recovering $x_t$'s, for the smoothness penalized least squares estimator. In particular, this implies for certain choices of $G$ that this estimator is weakly consistent (as $T \rightarrow \infty$) under potentially very stringent sampling, where only one coordinate is measured per vertex for a vanishingly small fraction of the vertices. The results are extended to a ``multi-layer'' ranking problem where $x_t$ corresponds to the latent strengths of a collection of $n$ items, and noisy pairwise difference measurements are obtained at each ``layer'' $t$ via a measurement graph $G_t$. Weak consistency is established for certain choices of $G$ even when the individual $G_t$'s are very sparse and disconnected.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable Thompson Sampling: Valid Inference via Variance Inflation</title>
<link>https://arxiv.org/abs/2505.23260</link>
<guid>https://arxiv.org/abs/2505.23260</guid>
<content:encoded><![CDATA[
arXiv:2505.23260v1 Announce Type: cross 
Abstract: We consider the problem of statistical inference when the data is collected via a Thompson Sampling-type algorithm. While Thompson Sampling (TS) is known to be both asymptotically optimal and empirically effective, its adaptive sampling scheme poses challenges for constructing confidence intervals for model parameters. We propose and analyze a variant of TS, called Stable Thompson Sampling, in which the posterior variance is inflated by a logarithmic factor. We show that this modification leads to asymptotically normal estimates of the arm means, despite the non-i.i.d. nature of the data. Importantly, this statistical benefit comes at a modest cost: the variance inflation increases regret by only a logarithmic factor compared to standard TS. Our results reveal a principled trade-off: by paying a small price in regret, one can enable valid statistical inference for adaptive decision-making algorithms.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LADA: Scalable Label-Specific CLIP Adapter for Continual Learning</title>
<link>https://arxiv.org/abs/2505.23271</link>
<guid>https://arxiv.org/abs/2505.23271</guid>
<content:encoded><![CDATA[
arXiv:2505.23271v1 Announce Type: cross 
Abstract: Continual learning with vision-language models like CLIP offers a pathway toward scalable machine learning systems by leveraging its transferable representations. Existing CLIP-based methods adapt the pre-trained image encoder by adding multiple sets of learnable parameters, with each task using a partial set of parameters. This requires selecting the expected parameters for input images during inference, which is prone to error that degrades performance. To address this problem, we introduce LADA (Label-specific ADApter). Instead of partitioning parameters across tasks, LADA appends lightweight, label-specific memory units to the frozen CLIP image encoder, enabling discriminative feature generation by aggregating task-agnostic knowledge. To prevent catastrophic forgetting, LADA employs feature distillation for seen classes, preventing their features from being interfered with by new classes. Positioned after the image encoder, LADA prevents gradient flow to the frozen CLIP parameters, ensuring efficient training. Extensive results show that LADA achieves state-of-the-art performance in continual learning settings. The implementation code is available at https://github.com/MaolinLuo/LADA.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Does Response Length Affect Long-Form Factuality</title>
<link>https://arxiv.org/abs/2505.23295</link>
<guid>https://arxiv.org/abs/2505.23295</guid>
<content:encoded><![CDATA[
arXiv:2505.23295v1 Announce Type: cross 
Abstract: Large language models (LLMs) are widely used for long-form text generation. However, factual errors in the responses would undermine their reliability. Despite growing attention to LLM factuality, the effect of response length on factuality remains underexplored. In this work, we systematically investigate this relationship by first introducing an automatic and bi-level long-form factuality evaluation framework, which achieves high agreement with human annotations while being cost-effective. Using this framework, we conduct controlled experiments and find that longer responses exhibit lower factual precision, confirming the presence of length bias. To explain this phenomenon, we empirically examine three hypotheses: error propagation, long context, and facts exhaustion. Our results reveal that facts exhaustion, where the model gradually exhausts more reliable knowledge, is the primary cause of factual degradation, rather than the other two hypotheses.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction</title>
<link>https://arxiv.org/abs/2505.23305</link>
<guid>https://arxiv.org/abs/2505.23305</guid>
<content:encoded><![CDATA[
arXiv:2505.23305v1 Announce Type: cross 
Abstract: We present MGE-LDM, a unified latent diffusion framework for simultaneous music generation, source imputation, and query-driven source separation. Unlike prior approaches constrained to fixed instrument classes, MGE-LDM learns a joint distribution over full mixtures, submixtures, and individual stems within a single compact latent diffusion model. At inference, MGE-LDM enables (1) complete mixture generation, (2) partial generation (i.e., source imputation), and (3) text-conditioned extraction of arbitrary sources. By formulating both separation and imputation as conditional inpainting tasks in the latent space, our approach supports flexible, class-agnostic manipulation of arbitrary instrument sources. Notably, MGE-LDM can be trained jointly across heterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without relying on predefined instrument categories. Audio samples are available at our project page: https://yoongi43.github.io/MGELDM_Samples/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition</title>
<link>https://arxiv.org/abs/2505.23313</link>
<guid>https://arxiv.org/abs/2505.23313</guid>
<content:encoded><![CDATA[
arXiv:2505.23313v1 Announce Type: cross 
Abstract: Pedestrian Attribute Recognition (PAR) is an indispensable task in human-centered research and has made great progress in recent years with the development of deep neural networks. However, the potential vulnerability and anti-interference ability have still not been fully explored. To bridge this gap, this paper proposes the first adversarial attack and defense framework for pedestrian attribute recognition. Specifically, we exploit both global- and patch-level attacks on the pedestrian images, based on the pre-trained CLIP-based PAR framework. It first divides the input pedestrian image into non-overlapping patches and embeds them into feature embeddings using a projection layer. Meanwhile, the attribute set is expanded into sentences using prompts and embedded into attribute features using a pre-trained CLIP text encoder. A multi-modal Transformer is adopted to fuse the obtained vision and text tokens, and a feed-forward network is utilized for attribute recognition. Based on the aforementioned PAR framework, we adopt the adversarial semantic and label-perturbation to generate the adversarial noise, termed ASL-PAR. We also design a semantic offset defense strategy to suppress the influence of adversarial attacks. Extensive experiments conducted on both digital domains (i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the effectiveness of our proposed adversarial attack and defense strategies for the pedestrian attribute recognition. The source code of this paper will be released on https://github.com/Event-AHU/OpenPAR.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments</title>
<link>https://arxiv.org/abs/2505.23315</link>
<guid>https://arxiv.org/abs/2505.23315</guid>
<content:encoded><![CDATA[
arXiv:2505.23315v1 Announce Type: cross 
Abstract: A key ethical challenge in Automated Essay Scoring (AES) is ensuring that scores are only released when they meet high reliability standards. Confidence modelling addresses this by assigning a reliability estimate measure, in the form of a confidence score, to each automated score. In this study, we frame confidence estimation as a classification task: predicting whether an AES-generated score correctly places a candidate in the appropriate CEFR level. While this is a binary decision, we leverage the inherent granularity of the scoring domain in two ways. First, we reformulate the task as an n-ary classification problem using score binning. Second, we introduce a set of novel Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that incorporate the ordinal structure of CEFR labels. Our best-performing model achieves an F1 score of 0.97, and enables the system to release 47% of scores with 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to approximately 92% (approx.) CEFR agreement from the standalone AES model where we release all AM predicted scores.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2505.23331</link>
<guid>https://arxiv.org/abs/2505.23331</guid>
<content:encoded><![CDATA[
arXiv:2505.23331v1 Announce Type: cross 
Abstract: Fine-tuning pre-trained generative models with Reinforcement Learning (RL) has emerged as an effective approach for aligning outputs more closely with nuanced human preferences. In this paper, we investigate the application of Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual autoregressive (VAR) models. Our empirical results demonstrate that this approach enables alignment to intricate reward signals derived from aesthetic predictors and CLIP embeddings, significantly enhancing image quality and enabling precise control over the generation style. Interestingly, by leveraging CLIP, our method can help VAR models generalize beyond their initial ImageNet distribution: through RL-driven exploration, these models can generate images aligned with prompts referencing image styles that were absent during pre-training. In summary, we show that RL-based fine-tuning is both efficient and effective for VAR models, benefiting particularly from their fast inference speeds, which are advantageous for online sampling, an aspect that poses significant challenges for diffusion-based alternatives.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Descriptor Is All You Need: Accurate Machine Learning of Nonadiabatic Coupling Vectors</title>
<link>https://arxiv.org/abs/2505.23344</link>
<guid>https://arxiv.org/abs/2505.23344</guid>
<content:encoded><![CDATA[
arXiv:2505.23344v1 Announce Type: cross 
Abstract: Nonadiabatic couplings (NACs) play a crucial role in modeling photochemical and photophysical processes with methods such as the widely used fewest-switches surface hopping (FSSH). There is therefore a strong incentive to machine learn NACs for accelerating simulations. However, this is challenging due to NACs' vectorial, double-valued character and the singularity near a conical intersection seam. For the first time, we design NAC-specific descriptors based on our domain expertise and show that they allow learning NACs with never-before-reported accuracy of $R^2$ exceeding 0.99. The key to success is also our new ML phase-correction procedure. We demonstrate the efficiency and robustness of our approach on a prototypical example of fully ML-driven FSSH simulations of fulvene targeting the SA-2-CASSCF(6,6) electronic structure level. This ML-FSSH dynamics leads to an accurate description of $S_1$ decay while reducing error bars by allowing the execution of a large ensemble of trajectories. Our implementations are available in open-source MLatom.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Estimation Loss Control in Variational Quantum Sensing via Online Conformal Inference</title>
<link>https://arxiv.org/abs/2505.23389</link>
<guid>https://arxiv.org/abs/2505.23389</guid>
<content:encoded><![CDATA[
arXiv:2505.23389v1 Announce Type: cross 
Abstract: Quantum sensing exploits non-classical effects to overcome limitations of classical sensors, with applications ranging from gravitational-wave detection to nanoscale imaging. However, practical quantum sensors built on noisy intermediate-scale quantum (NISQ) devices face significant noise and sampling constraints, and current variational quantum sensing (VQS) methods lack rigorous performance guarantees. This paper proposes an online control framework for VQS that dynamically updates the variational parameters while providing deterministic error bars on the estimates. By leveraging online conformal inference techniques, the approach produces sequential estimation sets with a guaranteed long-term risk level. Experiments on a quantum magnetometry task confirm that the proposed dynamic VQS approach maintains the required reliability over time, while still yielding precise estimates. The results demonstrate the practical benefits of combining variational quantum algorithms with online conformal inference to achieve reliable quantum sensing on NISQ devices.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Editing for Audio-Visual Dubbing</title>
<link>https://arxiv.org/abs/2505.23406</link>
<guid>https://arxiv.org/abs/2505.23406</guid>
<content:encoded><![CDATA[
arXiv:2505.23406v1 Announce Type: cross 
Abstract: Visual dubbing, the synchronization of facial movements with new speech, is crucial for making content accessible across different languages, enabling broader global reach. However, current methods face significant limitations. Existing approaches often generate talking faces, hindering seamless integration into original scenes, or employ inpainting techniques that discard vital visual information like partial occlusions and lighting variations. This work introduces EdiDub, a novel framework that reformulates visual dubbing as a content-aware editing task. EdiDub preserves the original video context by utilizing a specialized conditioning scheme to ensure faithful and accurate modifications rather than mere copying. On multiple benchmarks, including a challenging occluded-lip dataset, EdiDub significantly improves identity preservation and synchronization. Human evaluations further confirm its superiority, achieving higher synchronization and visual naturalness scores compared to the leading methods. These results demonstrate that our content-aware editing approach outperforms traditional generation or inpainting, particularly in maintaining complex visual elements while ensuring accurate lip synchronization.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction</title>
<link>https://arxiv.org/abs/2505.23416</link>
<guid>https://arxiv.org/abs/2505.23416</guid>
<content:encoded><![CDATA[
arXiv:2505.23416v1 Announce Type: cross 
Abstract: Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3-4$\times$ and FlashAttention decoding latency by approximately 2$\times$, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90% cache budget ratio under multi-query scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Learning via k-DTW: A Novel Dissimilarity Measure for Curves</title>
<link>https://arxiv.org/abs/2505.23431</link>
<guid>https://arxiv.org/abs/2505.23431</guid>
<content:encoded><![CDATA[
arXiv:2505.23431v1 Announce Type: cross 
Abstract: This paper introduces $k$-Dynamic Time Warping ($k$-DTW), a novel dissimilarity measure for polygonal curves. $k$-DTW has stronger metric properties than Dynamic Time Warping (DTW) and is more robust to outliers than the Fr\'{e}chet distance, which are the two gold standards of dissimilarity measures for polygonal curves. We show interesting properties of $k$-DTW and give an exact algorithm as well as a $(1+\varepsilon)$-approximation algorithm for $k$-DTW by a parametric search for the $k$-th largest matched distance. We prove the first dimension-free learning bounds for curves and further learning theoretic results. $k$-DTW not only admits smaller sample size than DTW for the problem of learning the median of curves, where some factors depending on the curves' complexity $m$ are replaced by $k$, but we also show a surprising separation on the associated Rademacher and Gaussian complexities: $k$-DTW admits strictly smaller bounds than DTW, by a factor $\tilde\Omega(\sqrt{m})$ when $k\ll m$. We complement our theoretical findings with an experimental illustration of the benefits of using $k$-DTW for clustering and nearest neighbor classification.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Risk Awareness in Rational Agents under Resource Constraints</title>
<link>https://arxiv.org/abs/2505.23436</link>
<guid>https://arxiv.org/abs/2505.23436</guid>
<content:encoded><![CDATA[
arXiv:2505.23436v1 Announce Type: cross 
Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed to interact with humans and to solve sequential decision-making problems under (approximate) utility functions and internal models. When such problems have resource or failure constraints where action sequences may be forcibly terminated once resources are exhausted, agents face implicit trade-offs that reshape their utility-driven (rational) behaviour. Additionally, since these agents are typically commissioned by a human principal to act on their behalf, asymmetries in constraint exposure can give rise to previously unanticipated misalignment between human objectives and agent incentives. We formalise this setting through a survival bandit framework, provide theoretical and empirical results that quantify the impact of survival-driven preference shifts, identify conditions under which misalignment emerges and propose mechanisms to mitigate the emergence of risk-seeking or risk-averse behaviours. As a result, this work aims to increase understanding and interpretability of emergent behaviours of AI agents operating under such survival pressure, and offer guidelines for safely deploying such AI systems in critical resource-limited environments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Strong, Weak and Benign Goodhart's law. An independence-free and paradigm-agnostic formalisation</title>
<link>https://arxiv.org/abs/2505.23445</link>
<guid>https://arxiv.org/abs/2505.23445</guid>
<content:encoded><![CDATA[
arXiv:2505.23445v1 Announce Type: cross 
Abstract: Goodhart's law is a famous adage in policy-making that states that ``When a measure becomes a target, it ceases to be a good measure''. As machine learning models and the optimisation capacity to train them grow, growing empirical evidence reinforced the belief in the validity of this law without however being formalised. Recently, a few attempts were made to formalise Goodhart's law, either by categorising variants of it, or by looking at how optimising a proxy metric affects the optimisation of an intended goal. In this work, we alleviate the simplifying independence assumption, made in previous works, and the assumption on the learning paradigm made in most of them, to study the effect of the coupling between the proxy metric and the intended goal on Goodhart's law. Our results show that in the case of light tailed goal and light tailed discrepancy, dependence does not change the nature of Goodhart's effect. However, in the light tailed goal and heavy tailed discrepancy case, we exhibit an example where over-optimisation occurs at a rate inversely proportional to the heavy tailedness of the discrepancy between the goal and the metric. %
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning</title>
<link>https://arxiv.org/abs/2505.23475</link>
<guid>https://arxiv.org/abs/2505.23475</guid>
<content:encoded><![CDATA[
arXiv:2505.23475v1 Announce Type: cross 
Abstract: Fast and scalable alignment of time series is a fundamental challenge in many domains. The standard solution, Dynamic Time Warping (DTW), struggles with poor scalability and sensitivity to noise. We introduce TimePoint, a self-supervised method that dramatically accelerates DTW-based alignment while typically improving alignment accuracy by learning keypoints and descriptors from synthetic data. Inspired by 2D keypoint detection but carefully adapted to the unique challenges of 1D signals, TimePoint leverages efficient 1D diffeomorphisms, which effectively model nonlinear time warping, to generate realistic training data. This approach, along with fully convolutional and wavelet convolutional architectures, enables the extraction of informative keypoints and descriptors. Applying DTW to these sparse representations yield major speedups and typically higher alignment accuracy than standard DTW applied to the full signals. TimePoint demonstrates strong generalization to real-world time series when trained solely on synthetic data, and further improves with fine-tuning on real data. Extensive experiments demonstrate that TimePoint consistently achieves faster and more accurate alignments than standard DTW, making it a scalable solution for time-series analysis. Our code is available at https://github.com/BGU-CS-VIL/TimePoint
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking</title>
<link>https://arxiv.org/abs/2505.23495</link>
<guid>https://arxiv.org/abs/2505.23495</guid>
<content:encoded><![CDATA[
arXiv:2505.23495v1 Announce Type: cross 
Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality benchmarks to evaluate complex multi-hop reasoning. However, despite their widespread use, popular datasets such as WebQSP and CWQ suffer from critical quality issues, including inaccurate or incomplete ground-truth annotations, poorly constructed questions that are ambiguous, trivial, or unanswerable, and outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA datasets, including WebQSP and CWQ, we find that the average factual correctness rate is only 57 %. To address these issues, we introduce KGQAGen, an LLM-in-the-loop framework that systematically resolves these pitfalls. KGQAGen combines structured knowledge grounding, LLM-guided generation, and symbolic verification to produce challenging and verifiable QA instances. Using KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results demonstrate that even state-of-the-art systems struggle on this benchmark, highlighting its ability to expose limitations of existing models. Our findings advocate for more rigorous benchmark construction and position KGQAGen as a scalable framework for advancing KGQA evaluation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds</title>
<link>https://arxiv.org/abs/2505.23509</link>
<guid>https://arxiv.org/abs/2505.23509</guid>
<content:encoded><![CDATA[
arXiv:2505.23509v1 Announce Type: cross 
Abstract: Audio DNNs have demonstrated impressive performance on various machine listening tasks; however, most of their representations are computationally costly and uninterpretable, leaving room for optimization. Here, we propose a novel approach centered on spectrotemporal modulation (STM) features, a signal processing method that mimics the neurophysiological representation in the human auditory cortex. The classification performance of our STM-based model, without any pretraining, is comparable to that of pretrained audio DNNs across diverse naturalistic speech, music, and environmental sounds, which are essential categories for both human cognition and machine perception. These results show that STM is an efficient and interpretable feature representation for audio classification, advancing the development of machine listening and unlocking exciting new possibilities for basic understanding of speech and auditory sciences, as well as developing audio BCI and cognitive computing.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration</title>
<link>https://arxiv.org/abs/2505.23515</link>
<guid>https://arxiv.org/abs/2505.23515</guid>
<content:encoded><![CDATA[
arXiv:2505.23515v1 Announce Type: cross 
Abstract: In this work, we propose a full-band real-time speech enhancement system with GAN-based stochastic regeneration. Predictive models focus on estimating the mean of the target distribution, whereas generative models aim to learn the full distribution. This behavior of predictive models may lead to over-suppression, i.e. the removal of speech content. In the literature, it was shown that combining a predictive model with a generative one within the stochastic regeneration framework can reduce the distortion in the output. We use this framework to obtain a real-time speech enhancement system. With 3.58M parameters and a low latency, our system is designed for real-time streaming with a lightweight architecture. Experiments show that our system improves over the first stage in terms of NISQA-MOS metric. Finally, through an ablation study, we show the importance of noisy conditioning in our system. We participated in 2025 Urgent Challenge with our model and later made further improvements.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data</title>
<link>https://arxiv.org/abs/2505.23522</link>
<guid>https://arxiv.org/abs/2505.23522</guid>
<content:encoded><![CDATA[
arXiv:2505.23522v1 Announce Type: cross 
Abstract: Existing benchmarks for Earth science multimodal learning exhibit critical limitations in systematic coverage of geosystem components and cross-sphere interactions, often constrained to isolated subsystems (only in Human-activities sphere or atmosphere) with limited evaluation dimensions (less than 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first comprehensive multimodal benchmark spanning all six Earth science spheres (atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and Human-activities sphere) and cross-spheres with one hundred expert-curated evaluation dimensions. Leveraging observational data from satellite sensors and in-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four tiers: perception, general reasoning, scientific knowledge reasoning and chain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per sphere to establish authoritative evaluation dimensions and curate relevant observational datasets, 40 crowd-sourcing annotators to assist experts for annotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd workflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs reveal that even the most advanced models struggle with our benchmarks, where none of them reach 35\% accuracy. Especially, in some cross-spheres tasks, the performance of leading models like GPT-4o drops to 0.0\%. OmniEarth-Bench sets a new standard for geosystem-aware AI, advancing both scientific discovery and practical applications in environmental monitoring and disaster prediction. The dataset, source code, and trained models were released.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters</title>
<link>https://arxiv.org/abs/2505.23554</link>
<guid>https://arxiv.org/abs/2505.23554</guid>
<content:encoded><![CDATA[
arXiv:2505.23554v1 Announce Type: cross 
Abstract: In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and Gemini have been widely adopted in different areas. As the use of LLMs continues to grow, many efforts have focused on reducing the massive training overheads of these models. But it is the environmental impact of handling user requests to LLMs that is increasingly becoming a concern. Recent studies estimate that the costs of operating LLMs in their inference phase can exceed training costs by 25x per year. As LLMs are queried incessantly, the cumulative carbon footprint for the operational phase has been shown to far exceed the footprint during the training phase. Further, estimates indicate that 500 ml of fresh water is expended for every 20-50 requests to LLMs during inference. To address these important sustainability issues with LLMs, we propose a novel framework called SLIT to co-optimize LLM quality of service (time-to-first token), carbon emissions, water usage, and energy costs. The framework utilizes a machine learning (ML) based metaheuristic to enhance the sustainability of LLM hosting across geo-distributed cloud datacenters. Such a framework will become increasingly vital as LLMs proliferate.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Parametric Distributions from Samples and Preferences</title>
<link>https://arxiv.org/abs/2505.23557</link>
<guid>https://arxiv.org/abs/2505.23557</guid>
<content:encoded><![CDATA[
arXiv:2505.23557v1 Announce Type: cross 
Abstract: Recent advances in language modeling have underscored the role of preference feedback in enhancing model performance. This paper investigates the conditions under which preference feedback improves parameter estimation in classes of continuous parametric distributions. In our framework, the learner observes pairs of samples from an unknown distribution along with their relative preferences depending on the same unknown parameter. We show that preference-based M-estimators achieve a better asymptotic variance than sample-only M-estimators, further improved by deterministic preferences. Leveraging the hard constraints revealed by deterministic preferences, we propose an estimator achieving an estimation error scaling of $\mathcal{O}(1/n)$ -- a significant improvement over the $\Theta(1/\sqrt{n})$ rate attainable with samples alone. Next, we establish a lower bound that matches this accelerated rate; up to dimension and problem-dependent constants. While the assumptions underpinning our analysis are restrictive, they are satisfied by notable cases such as Gaussian or Laplace distributions for preferences based on the log-probability reward.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information</title>
<link>https://arxiv.org/abs/2505.23558</link>
<guid>https://arxiv.org/abs/2505.23558</guid>
<content:encoded><![CDATA[
arXiv:2505.23558v1 Announce Type: cross 
Abstract: Inference time scaling drives extended reasoning to enhance the performance of Vision-Language Models (VLMs), thus forming powerful Vision-Language Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens, causing visual information to receive less attention and may trigger hallucinations. Although introducing text-only reflection processes shows promise in language models, we demonstrate that it is insufficient to suppress hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain (Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a vision-text reflection process that guides the model to re-attention visual information during reasoning. We first propose a reinforcement learning method Balanced Reflective Policy Optimization (BRPO), which guides the model to decide when to generate vision-text reflection on its own and balance the number and length of reflections. Then, we formally prove that VLRMs lose attention to visual tokens as reasoning progresses, and demonstrate that supplementing visual information during reflection enhances visual attention. Therefore, during training and inference, Visual Token COPY and Visual Token ROUTE are introduced to force the model to re-attention visual information at the visual level, addressing the limitations of text-only reflection. Experiments on multiple visual QA datasets and hallucination metrics indicate that Qwen-LA achieves leading accuracy performance while reducing hallucinations. Our code is available at: https://github.com/Liar406/Look_Again.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring</title>
<link>https://arxiv.org/abs/2505.23575</link>
<guid>https://arxiv.org/abs/2505.23575</guid>
<content:encoded><![CDATA[
arXiv:2505.23575v1 Announce Type: cross 
Abstract: As AI models are deployed with increasing autonomy, it is important to ensure they do not take harmful actions unnoticed. As a potential mitigation, we investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor model continuously oversees the intermediate reasoning steps of a more powerful but untrusted model. We compare CoT monitoring to action-only monitoring, where only final outputs are reviewed, in a red-teaming setup where the untrusted model is instructed to pursue harmful side tasks while completing a coding problem. We find that CoT monitoring improves detection by up to 27 percentage points in scenarios where action-only monitoring fails to reliably identify sabotage. However, CoT traces can also contain misleading rationalizations that deceive the monitor, reducing performance in more obvious sabotage cases. To address this, we introduce a hybrid protocol that independently scores both reasoning and final outputs and combines them using a weighted average. This hybrid monitor consistently outperforms both CoT and action-only monitors across all tested models and tasks, with detection rates over four times higher than action-only monitoring for subtle deception scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation</title>
<link>https://arxiv.org/abs/2505.23587</link>
<guid>https://arxiv.org/abs/2505.23587</guid>
<content:encoded><![CDATA[
arXiv:2505.23587v1 Announce Type: cross 
Abstract: In medical image segmentation, limited external validity remains a critical obstacle when models are deployed across unseen datasets, an issue particularly pronounced in the ultrasound image domain. Existing solutions-such as domain adaptation and GAN-based style transfer-while promising, often fall short in the medical domain where datasets are typically small and diverse. This paper presents a novel application of principal component analysis (PCA) to address this limitation. PCA preprocessing reduces noise and emphasizes essential features by retaining approximately 90\% of the dataset variance. We evaluate our approach across six diverse breast tumor ultrasound datasets comprising 3,983 B-mode images and corresponding expert tumor segmentation masks. For each dataset, a corresponding dimensionality reduced PCA-dataset is created and U-Net-based segmentation models are trained on each of the twelve datasets. Each model trained on an original dataset was inferenced on the remaining five out-of-domain original datasets (baseline results), while each model trained on a PCA dataset was inferenced on five out-of-domain PCA datasets. Our experimental results indicate that using PCA reconstructed datasets, instead of original images, improves the model's recall and Dice scores, particularly for model-dataset pairs where baseline performance was lowest, achieving statistically significant gains in recall (0.57 $\pm$ 0.07 vs. 0.70 $\pm$ 0.05, $p = 0.0004$) and Dice scores (0.50 $\pm$ 0.06 vs. 0.58 $\pm$ 0.06, $p = 0.03$). Our method reduced the decline in recall values due to external validation by $33\%$. These findings underscore the potential of PCA reconstruction as a safeguard to mitigate declines in segmentation performance, especially in challenging cases, with implications for enhancing external validity in real-world medical applications.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilook Coherent Imaging: Theoretical Guarantees and Algorithms</title>
<link>https://arxiv.org/abs/2505.23594</link>
<guid>https://arxiv.org/abs/2505.23594</guid>
<content:encoded><![CDATA[
arXiv:2505.23594v1 Announce Type: cross 
Abstract: Multilook coherent imaging is a widely used technique in applications such as digital holography, ultrasound imaging, and synthetic aperture radar. A central challenge in these systems is the presence of multiplicative noise, commonly known as speckle, which degrades image quality. Despite the widespread use of coherent imaging systems, their theoretical foundations remain relatively underexplored. In this paper, we study both the theoretical and algorithmic aspects of likelihood-based approaches for multilook coherent imaging, providing a rigorous framework for analysis and method development. Our theoretical contributions include establishing the first theoretical upper bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under the deep image prior hypothesis. Our results capture the dependence of MSE on the number of parameters in the deep image prior, the number of looks, the signal dimension, and the number of measurements per look. On the algorithmic side, we employ projected gradient descent (PGD) as an efficient method for computing the maximum likelihood solution. Furthermore, we introduce two key ideas to enhance the practical performance of PGD. First, we incorporate the Newton-Schulz algorithm to compute matrix inverses within the PGD iterations, significantly reducing computational complexity. Second, we develop a bagging strategy to mitigate projection errors introduced during PGD updates. We demonstrate that combining these techniques with PGD yields state-of-the-art performance. Our code is available at https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory</title>
<link>https://arxiv.org/abs/2505.23617</link>
<guid>https://arxiv.org/abs/2505.23617</guid>
<content:encoded><![CDATA[
arXiv:2505.23617v1 Announce Type: cross 
Abstract: Effective video tokenization is critical for scaling transformer models for long videos. Current approaches tokenize videos using space-time patches, leading to excessive tokens and computational inefficiencies. The best token reduction strategies degrade performance and barely reduce the number of tokens when the camera moves. We introduce grounded video tokenization, a paradigm that organizes tokens based on panoptic sub-object trajectories rather than fixed patches. Our method aligns with fundamental perceptual principles, ensuring that tokenization reflects scene complexity rather than video duration. We propose TrajViT, a video encoder that extracts object trajectories and converts them into semantically meaningful tokens, significantly reducing redundancy while maintaining temporal coherence. Trained with contrastive learning, TrajViT significantly outperforms space-time ViT (ViT3D) across multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a large margin of 6% top-5 recall in average at video-text retrieval task with 10x token deduction. We also show TrajViT as a stronger model than ViT3D for being the video encoder for modern VideoLLM, obtaining an average of 5.2% performance improvement across 6 VideoQA benchmarks while having 4x faster training time and 18x less inference FLOPs. TrajViT is the first efficient encoder to consistently outperform ViT3D across diverse video analysis tasks, making it a robust and scalable solution.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.23619</link>
<guid>https://arxiv.org/abs/2505.23619</guid>
<content:encoded><![CDATA[
arXiv:2505.23619v1 Announce Type: cross 
Abstract: Recent advancements in Text-to-Speech (TTS) models, particularly in voice cloning, have intensified the demand for adaptable and efficient deepfake detection methods. As TTS systems continue to evolve, detection models must be able to efficiently adapt to previously unseen generation models with minimal data. This paper introduces ADD-GP, a few-shot adaptive framework based on a Gaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show how the combination of a powerful deep embedding model with the Gaussian processes flexibility can achieve strong performance and adaptability. Additionally, we show this approach can also be used for personalized detection, with greater robustness to new TTS models and one-shot adaptability. To support our evaluation, a benchmark dataset is constructed for this task using new state-of-the-art voice cloning models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance-Optimality for Private KL Distribution Estimation</title>
<link>https://arxiv.org/abs/2505.23620</link>
<guid>https://arxiv.org/abs/2505.23620</guid>
<content:encoded><![CDATA[
arXiv:2505.23620v1 Announce Type: cross 
Abstract: We study the fundamental problem of estimating an unknown discrete distribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the distribution. We are interested in minimizing the KL divergence between the true distribution and the algorithm's estimate. We first construct minimax optimal private estimators. Minimax optimality however fails to shed light on an algorithm's performance on individual (non-worst-case) instances $p$ and simple minimax-optimal DP estimators can have poor empirical performance on real distributions. We then study this problem from an instance-optimality viewpoint, where the algorithm's error on $p$ is compared to the minimum achievable estimation error over a small local neighborhood of $p$. Under natural notions of local neighborhood, we propose algorithms that achieve instance-optimality up to constant factors, with and without a differential privacy constraint. Our upper bounds rely on (private) variants of the Good-Turing estimator. Our lower bounds use additive local neighborhoods that more precisely captures the hardness of distribution estimation in KL divergence, compared to ones considered in prior works.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Reasoning Models More Prone to Hallucination?</title>
<link>https://arxiv.org/abs/2505.23646</link>
<guid>https://arxiv.org/abs/2505.23646</guid>
<content:encoded><![CDATA[
arXiv:2505.23646v1 Announce Type: cross 
Abstract: Recently evolved large reasoning models (LRMs) show powerful performance in solving complex tasks with long chain-of-thought (CoT) reasoning capability. As these LRMs are mostly developed by post-training on formal reasoning tasks, whether they generalize the reasoning capability to help reduce hallucination in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1 reports increased performance on SimpleQA, a fact-seeking benchmark, while OpenAI-o3 observes even severer hallucination. This discrepancy naturally raises the following research question: Are reasoning models more prone to hallucination? This paper addresses the question from three perspectives. (1) We first conduct a holistic evaluation for the hallucination in LRMs. Our analysis reveals that LRMs undergo a full post-training pipeline with cold start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate their hallucination. In contrast, both distillation alone and RL training without cold start fine-tuning introduce more nuanced hallucinations. (2) To explore why different post-training pipelines alters the impact on hallucination in LRMs, we conduct behavior analysis. We characterize two critical cognitive behaviors that directly affect the factuality of a LRM: Flaw Repetition, where the surface-level reasoning attempts repeatedly follow the same underlying flawed logic, and Think-Answer Mismatch, where the final answer fails to faithfully match the previous CoT process. (3) Further, we investigate the mechanism behind the hallucination of LRMs from the perspective of model uncertainty. We find that increased hallucination of LRMs is usually associated with the misalignment between model uncertainty and factual accuracy. Our work provides an initial understanding of the hallucination in LRMs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization-Free Diffusion Model -- A Perturbation Theory Approach</title>
<link>https://arxiv.org/abs/2505.23652</link>
<guid>https://arxiv.org/abs/2505.23652</guid>
<content:encoded><![CDATA[
arXiv:2505.23652v1 Announce Type: cross 
Abstract: Diffusion models have emerged as a powerful framework in generative modeling, typically relying on optimizing neural networks to estimate the score function via forward SDE simulations. In this work, we propose an alternative method that is both optimization-free and forward SDE-free. By expanding the score function in a sparse set of eigenbasis of the backward Kolmogorov operator associated with the diffusion process, we reformulate score estimation as the solution to a linear system, avoiding iterative optimization and time-dependent sample generation. We analyze the approximation error using perturbation theory and demonstrate the effectiveness of our method on high-dimensional Boltzmann distributions and real-world datasets.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation</title>
<link>https://arxiv.org/abs/2505.23657</link>
<guid>https://arxiv.org/abs/2505.23657</guid>
<content:encoded><![CDATA[
arXiv:2505.23657v1 Announce Type: cross 
Abstract: Recent decoding methods improve the factuality of large language models~(LLMs) by refining how the next token is selected during generation. These methods typically operate at the token level, leveraging internal representations to suppress superficial patterns. Nevertheless, LLMs remain prone to hallucinations, especially over longer contexts. In this paper, we propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy that actively decides when to apply contrasting layers during generation. By casting decoding as a sequential decision-making problem, ActLCD employs a reinforcement learning policy guided by a reward-aware classifier to optimize factuality beyond the token level. Our experiments demonstrate that ActLCD surpasses state-of-the-art methods across five benchmarks, showcasing its effectiveness in mitigating hallucinations in diverse generation scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Perspective on Memorization and Reconstruction</title>
<link>https://arxiv.org/abs/2505.23658</link>
<guid>https://arxiv.org/abs/2505.23658</guid>
<content:encoded><![CDATA[
arXiv:2505.23658v1 Announce Type: cross 
Abstract: We introduce a new Bayesian perspective on the concept of data reconstruction, and leverage this viewpoint to propose a new security definition that, in certain settings, provably prevents reconstruction attacks. We use our paradigm to shed new light on one of the most notorious attacks in the privacy and memorization literature - fingerprinting code attacks (FPC). We argue that these attacks are really a form of membership inference attacks, rather than reconstruction attacks. Furthermore, we show that if the goal is solely to prevent reconstruction (but not membership inference), then in some cases the impossibility results derived from FPC no longer apply.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoLA: Low-Rank Linear Attention With Sparse Caching</title>
<link>https://arxiv.org/abs/2505.23666</link>
<guid>https://arxiv.org/abs/2505.23666</guid>
<content:encoded><![CDATA[
arXiv:2505.23666v1 Announce Type: cross 
Abstract: Transformer-based large language models suffer from quadratic complexity at inference on long sequences. Linear attention methods are efficient alternatives, however, they fail to provide an accurate approximation of softmax attention. By additionally incorporating sliding window attention into each linear attention head, this gap can be closed for short context-length tasks. Unfortunately, these approaches cannot recall important information from long contexts due to "memory collisions". In this paper , we propose LoLA: Low-rank Linear Attention with sparse caching. LoLA separately stores additional key-value pairs that would otherwise interfere with past associative memories. Moreover, LoLA further closes the gap between linear attention models and transformers by distributing past key-value pairs into three forms of memory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize pairs in a sparse, global cache; and (iii) generic pairs in the recurrent hidden state of linear attention. As an inference-only strategy, LoLA enables pass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks from RULER. It boosts the accuracy of the base subquadratic model from 0.6% to 97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1 8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning tasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an extremely lightweight approach: Nearly all of our results can be reproduced on a single consumer GPU.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents</title>
<link>https://arxiv.org/abs/2505.23671</link>
<guid>https://arxiv.org/abs/2505.23671</guid>
<content:encoded><![CDATA[
arXiv:2505.23671v1 Announce Type: cross 
Abstract: Developing high-performance software is a complex task that requires specialized expertise. We introduce GSO, a benchmark for evaluating language models' capabilities in developing high-performance software. We develop an automated pipeline that generates and executes performance tests to analyze repository commit histories to identify 102 challenging optimization tasks across 10 codebases, spanning diverse domains and programming languages. An agent is provided with a codebase and performance test as a precise specification, and tasked to improve the runtime efficiency, which is measured against the expert developer optimization. Our quantitative evaluation reveals that leading SWE-Agents struggle significantly, achieving less than 5% success rate, with limited improvements even with inference-time scaling. Our qualitative analysis identifies key failure modes, including difficulties with low-level languages, practicing lazy optimization strategies, and challenges in accurately localizing bottlenecks. We release the code and artifacts of our benchmark along with agent trajectories to enable future research.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobi-$\pi$: Mobilizing Your Robot Learning Policy</title>
<link>https://arxiv.org/abs/2505.23692</link>
<guid>https://arxiv.org/abs/2505.23692</guid>
<content:encoded><![CDATA[
arXiv:2505.23692v1 Announce Type: cross 
Abstract: Learned visuomotor policies are capable of performing increasingly complex manipulation tasks. However, most of these policies are trained on data collected from limited robot positions and camera viewpoints. This leads to poor generalization to novel robot positions, which limits the use of these policies on mobile platforms, especially for precise tasks like pressing buttons or turning faucets. In this work, we formulate the policy mobilization problem: find a mobile robot base pose in a novel environment that is in distribution with respect to a manipulation policy trained on a limited set of camera viewpoints. Compared to retraining the policy itself to be more robust to unseen robot base pose initializations, policy mobilization decouples navigation from manipulation and thus does not require additional demonstrations. Crucially, this problem formulation complements existing efforts to improve manipulation policy robustness to novel viewpoints and remains compatible with them. To study policy mobilization, we introduce the Mobi-$\pi$ framework, which includes: (1) metrics that quantify the difficulty of mobilizing a given policy, (2) a suite of simulated mobile manipulation tasks based on RoboCasa to evaluate policy mobilization, (3) visualization tools for analysis, and (4) several baseline methods. We also propose a novel approach that bridges navigation and manipulation by optimizing the robot's base pose to align with an in-distribution base pose for a learned policy. Our approach utilizes 3D Gaussian Splatting for novel view synthesis, a score function to evaluate pose suitability, and sampling-based optimization to identify optimal robot poses. We show that our approach outperforms baselines in both simulation and real-world environments, demonstrating its effectiveness for policy mobilization.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.23709</link>
<guid>https://arxiv.org/abs/2505.23709</guid>
<content:encoded><![CDATA[
arXiv:2505.23709v1 Announce Type: cross 
Abstract: We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning rich representations of skin lesions through a novel nested contrastive learning approach that captures complex relationships between images and metadata. Melanoma detection and skin lesion classification based solely on images, pose significant challenges due to large variations in imaging conditions (lighting, color, resolution, distance, etc.) and lack of clinical and phenotypical context. Clinicians typically follow a holistic approach for assessing the risk level of the patient and for deciding which lesions may be malignant and need to be excised, by considering the patient's medical history as well as the appearance of other lesions of the patient. Inspired by this, SLIMP combines the appearance and the metadata of individual skin lesions with patient-level metadata relating to their medical record and other clinically relevant information. By fully exploiting all available data modalities throughout the learning process, the proposed pre-training strategy improves performance compared to other pre-training strategies on downstream skin lesions classification tasks highlighting the learned representations quality.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering</title>
<link>https://arxiv.org/abs/2505.23723</link>
<guid>https://arxiv.org/abs/2505.23723</guid>
<content:encoded><![CDATA[
arXiv:2505.23723v1 Announce Type: cross 
Abstract: The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Convergence Analysis of Muon</title>
<link>https://arxiv.org/abs/2505.23737</link>
<guid>https://arxiv.org/abs/2505.23737</guid>
<content:encoded><![CDATA[
arXiv:2505.23737v1 Announce Type: cross 
Abstract: The majority of parameters in neural networks are naturally represented as matrices. However, most commonly used optimizers treat these matrix parameters as flattened vectors during optimization, potentially overlooking their inherent structural properties. Recently, an optimizer called Muon has been proposed, specifically designed to optimize matrix-structured parameters. Extensive empirical evidence shows that Muon can significantly outperform traditional optimizers when training neural networks. Nonetheless, the theoretical understanding of Muon's convergence behavior and the reasons behind its superior performance remain limited. In this work, we present a comprehensive convergence rate analysis of Muon and its comparison with Gradient Descent (GD). We further characterize the conditions under which Muon can outperform GD. Our theoretical results reveal that Muon can benefit from the low-rank and approximate blockwise diagonal structure of Hessian matrices -- phenomena widely observed in practical neural network training. Our experimental results support and corroborate the theoretical findings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Trust Or Not To Trust Your Vision-Language Model's Prediction</title>
<link>https://arxiv.org/abs/2505.23745</link>
<guid>https://arxiv.org/abs/2505.23745</guid>
<content:encoded><![CDATA[
arXiv:2505.23745v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence</title>
<link>https://arxiv.org/abs/2505.23747</link>
<guid>https://arxiv.org/abs/2505.23747</guid>
<content:encoded><![CDATA[
arXiv:2505.23747v1 Announce Type: cross 
Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint</title>
<link>https://arxiv.org/abs/2505.23759</link>
<guid>https://arxiv.org/abs/2505.23759</guid>
<content:encoded><![CDATA[
arXiv:2505.23759v1 Announce Type: cross 
Abstract: Rebus puzzles, visual riddles that encode language through imagery, spatial arrangement, and symbolic substitution, pose a unique challenge to current vision-language models (VLMs). Unlike traditional image captioning or question answering tasks, rebus solving requires multi-modal abstraction, symbolic reasoning, and a grasp of cultural, phonetic and linguistic puns. In this paper, we investigate the capacity of contemporary VLMs to interpret and solve rebus puzzles by constructing a hand-generated and annotated benchmark of diverse English-language rebus puzzles, ranging from simple pictographic substitutions to spatially-dependent cues ("head" over "heels"). We analyze how different VLMs perform, and our findings reveal that while VLMs exhibit some surprising capabilities in decoding simple visual clues, they struggle significantly with tasks requiring abstract reasoning, lateral thinking, and understanding visual metaphors.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Chat Logs to Collective Insights: Aggregative Question Answering</title>
<link>https://arxiv.org/abs/2505.23765</link>
<guid>https://arxiv.org/abs/2505.23765</guid>
<content:encoded><![CDATA[
arXiv:2505.23765v1 Announce Type: cross 
Abstract: Conversational agents powered by large language models (LLMs) are rapidly becoming integral to our daily interactions, generating unprecedented amounts of conversational data. Such datasets offer a powerful lens into societal interests, trending topics, and collective concerns. Yet, existing approaches typically treat these interactions as independent and miss critical insights that could emerge from aggregating and reasoning across large-scale conversation logs. In this paper, we introduce Aggregative Question Answering, a novel task requiring models to reason explicitly over thousands of user-chatbot interactions to answer aggregative queries, such as identifying emerging concerns among specific demographics. To enable research in this direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative questions derived from 182,330 real-world chatbot conversations. Experiments show that existing methods either struggle to reason effectively or incur prohibitive computational costs, underscoring the need for new approaches capable of extracting collective insights from large-scale conversational data.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tutorial on Meta-Reinforcement Learning</title>
<link>https://arxiv.org/abs/2301.08028</link>
<guid>https://arxiv.org/abs/2301.08028</guid>
<content:encoded><![CDATA[
arXiv:2301.08028v4 Announce Type: replace 
Abstract: While deep reinforcement learning (RL) has fueled multiple high-profile successes in machine learning, it is held back from more widespread adoption by its often poor data efficiency and the limited generality of the policies it produces. A promising approach for alleviating these limitations is to cast the development of better RL algorithms as a machine learning problem itself in a process called meta-RL. Meta-RL is most commonly studied in a problem setting where, given a distribution of tasks, the goal is to learn a policy that is capable of adapting to any new task from the task distribution with as little data as possible. In this survey, we describe the meta-RL problem setting in detail as well as its major variations. We discuss how, at a high level, meta-RL research can be clustered based on the presence of a task distribution and the learning budget available for each individual task. Using these clusters, we then survey meta-RL algorithms and applications. We conclude by presenting the open problems on the path to making meta-RL part of the standard toolbox for a deep RL practitioner.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plug In and Learn: Federated Intelligence over a Smart Grid of Models</title>
<link>https://arxiv.org/abs/2302.04363</link>
<guid>https://arxiv.org/abs/2302.04363</guid>
<content:encoded><![CDATA[
arXiv:2302.04363v4 Announce Type: replace 
Abstract: We present a model-agnostic federated learning method that mirrors the operation of a smart power grid: diverse local models, like energy prosumers, train independently on their own data while exchanging lightweight signals to coordinate with statistically similar peers. This coordination is governed by a graph-based regularizer that encourages connected models to produce similar predictions on a shared, public unlabeled dataset. The resulting method is a flexible instance of regularized empirical risk minimization and supports a wide variety of local models - both parametric and non-parametric - provided they can be trained via regularized loss minimization. Such training is readily supported by standard ML libraries including scikit-learn, Keras, and PyTorch.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Collaborative Filtering is not Collaborative: Unfairness of PCA for Recommendations</title>
<link>https://arxiv.org/abs/2310.09687</link>
<guid>https://arxiv.org/abs/2310.09687</guid>
<content:encoded><![CDATA[
arXiv:2310.09687v2 Announce Type: replace 
Abstract: We study the fairness of dimensionality reduction methods for recommendations. We focus on the fundamental method of principal component analysis (PCA), which identifies latent components and produces a low-rank approximation via the leading components while discarding the trailing components. Prior works have defined notions of "fair PCA"; however, these definitions do not answer the following question: why is PCA unfair? We identify two underlying popularity mechanisms that induce item unfairness in PCA. The first negatively impacts less popular items because less popular items rely on trailing latent components to recover their values. The second negatively impacts highly popular items, since the leading PCA components specialize in individual popular items instead of capturing similarities between items. To address these issues, we develop a polynomial-time algorithm, Item-Weighted PCA, that flexibly up-weights less popular items when optimizing for leading principal components. We theoretically show that PCA, in all cases, and Normalized PCA, in cases of block-diagonal matrices, are instances of Item-Weighted PCA. We empirically show that there exist datasets for which Item-Weighted PCA yields the optimal solution while the baselines do not. In contrast to past dimensionality reduction re-weighting techniques, Item-Weighted PCA solves a convex optimization problem and enforces a hard rank constraint. Our evaluations on real-world datasets show that Item-Weighted PCA not only mitigates both unfairness mechanisms, but also produces recommendations that outperform those of PCA baselines.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hijacking Large Language Models via Adversarial In-Context Learning</title>
<link>https://arxiv.org/abs/2311.09948</link>
<guid>https://arxiv.org/abs/2311.09948</guid>
<content:encoded><![CDATA[
arXiv:2311.09948v3 Announce Type: replace 
Abstract: In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific downstream tasks by utilizing labeled examples as demonstrations (demos) in the preconditioned prompts. Despite its promising performance, crafted adversarial attacks pose a notable threat to the robustness of LLMs. Existing attacks are either easy to detect, require a trigger in user input, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable prompt injection attack against ICL, aiming to hijack LLMs to generate the target output or elicit harmful responses. In our threat model, the hacker acts as a model publisher who leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demos via prompt injection. We also propose effective defense strategies using a few shots of clean demos, enhancing the robustness of LLMs during ICL. Extensive experimental results across various classification and jailbreak tasks demonstrate the effectiveness of the proposed attack and defense strategies. This work highlights the significant security vulnerabilities of LLMs during ICL and underscores the need for further in-depth studies.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical guarantees on the best-of-n alignment policy</title>
<link>https://arxiv.org/abs/2401.01879</link>
<guid>https://arxiv.org/abs/2401.01879</guid>
<content:encoded><![CDATA[
arXiv:2401.01879v3 Announce Type: replace 
Abstract: A simple and effective method for the inference-time alignment and scaling test-time compute of generative models is best-of-$n$ sampling, where $n$ samples are drawn from a reference policy, ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the reference policy is equal to $\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes, and propose a new estimator for the KL divergence and empirically show that it provides a tight approximation. We also show that the win rate of the best-of-$n$ policy against the reference policy is upper bounded by $n/(n+1)$ and derive bounds on the tightness of this characterization. We conclude with analyzing the tradeoffs between win rate and KL divergence of the best-of-$n$ alignment policy, which demonstrate that very good tradeoffs are achievable with $n < 1000$.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks</title>
<link>https://arxiv.org/abs/2401.13330</link>
<guid>https://arxiv.org/abs/2401.13330</guid>
<content:encoded><![CDATA[
arXiv:2401.13330v2 Announce Type: replace 
Abstract: Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN) with Early Exit Classifiers (EECs), to provide predictions at intermediate points of the processing when enough confidence in classification is achieved. This leads to many benefits in terms of effectiveness and efficiency. Currently, the design of EENNs is carried out manually by experts, a complex and time-consuming task that requires accounting for many aspects, including the correct placement, the thresholding, and the computational overhead of the EECs. For this reason, the research is exploring the use of Neural Architecture Search (NAS) to automatize the design of EENNs. Currently, few comprehensive NAS solutions for EENNs have been proposed in the literature, and a fully automated, joint design strategy taking into consideration both the backbone and the EECs remains an open problem. To this end, this work presents Neural Architecture Search for Hardware Constrained Early Exit Neural Networks (NACHOS), the first NAS framework for the design of optimal EENNs satisfying constraints on the accuracy and the number of Multiply and Accumulate (MAC) operations performed by the EENNs at inference time. In particular, this provides the joint design of backbone and EECs to select a set of admissible (i.e., respecting the constraints) Pareto Optimal Solutions in terms of best tradeoff between the accuracy and number of MACs. The results show that the models designed by NACHOS are competitive with the state-of-the-art EENNs. Additionally, this work investigates the effectiveness of two novel regularization terms designed for the optimization of the auxiliary classifiers of the EENN
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimal Sufficient Views: A DNN model making predictions with more evidence has higher accuracy</title>
<link>https://arxiv.org/abs/2402.01095</link>
<guid>https://arxiv.org/abs/2402.01095</guid>
<content:encoded><![CDATA[
arXiv:2402.01095v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) exhibit high performance in image recognition; however, the reasons for their strong generalization abilities remain unclear. A plausible hypothesis is that DNNs achieve robust and accurate predictions by identifying multiple pieces of evidence from images. Thus, to test this hypothesis, this study proposed minimal sufficient views (MSVs). MSVs is defined as a set of minimal regions within an input image that are sufficient to preserve the prediction of DNNs, thus representing the evidence discovered by the DNN. We empirically demonstrated a strong correlation between the number of MSVs (i.e., the number of pieces of evidence) and the generalization performance of the DNN models. Remarkably, this correlation was found to hold within a single DNN as well as between different DNNs, including convolutional and transformer models. This suggested that a DNN model that makes its prediction based on more evidence has a higher generalization performance. We proposed a metric based on MSVs for DNN model selection that did not require label information. Consequently, we empirically showed that the proposed metric was less dependent on the degree of overfitting, rendering it a more reliable indicator of model performance than existing metrics, such as average confidence.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Poison Large Language Models for Downstream Manipulation</title>
<link>https://arxiv.org/abs/2402.13459</link>
<guid>https://arxiv.org/abs/2402.13459</guid>
<content:encoded><![CDATA[
arXiv:2402.13459v3 Announce Type: replace 
Abstract: The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where the adversary inserts backdoor triggers into training data to manipulate outputs. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the supervised fine-tuning (SFT) process. We propose a novel gradient-guided backdoor trigger learning (GBTL) algorithm to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various language model tasks, including sentiment analysis, domain generation, and question answering, our poisoning strategy demonstrates a high success rate in compromising various LLMs' outputs. We further propose two defense strategies against data poisoning attacks, including in-context learning (ICL) and continuous learning (CL), which effectively rectify the behavior of LLMs and significantly reduce the decline in performance. Our work highlights the significant security risks present during SFT of LLMs and the necessity of safeguarding LLMs against data poisoning attacks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniArch: Building Foundation Model For Scientific Computing</title>
<link>https://arxiv.org/abs/2402.16014</link>
<guid>https://arxiv.org/abs/2402.16014</guid>
<content:encoded><![CDATA[
arXiv:2402.16014v3 Announce Type: replace 
Abstract: Foundation models have revolutionized language modeling, while whether this success is replicated in scientific computing remains unexplored. We present OmniArch, the first prototype aiming at solving multi-scale and multi-physics scientific computing problems with physical alignment. We addressed all three challenges with one unified architecture. Its pre-training stage contains a Fourier Encoder-decoder fading out the disharmony across separated dimensions and a Transformer backbone integrating quantities through temporal dynamics, and the novel PDE-Aligner performs physics-informed fine-tuning under flexible conditions. As far as we know, we first conduct 1D-2D-3D united pre-training on the PDEBench, and it sets not only new performance benchmarks for 1D, 2D, and 3D PDEs but also demonstrates exceptional adaptability to new physics via in-context and zero-shot learning approaches, which supports realistic engineering applications and foresight physics discovery.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates</title>
<link>https://arxiv.org/abs/2402.17390</link>
<guid>https://arxiv.org/abs/2402.17390</guid>
<content:encoded><![CDATA[
arXiv:2402.17390v2 Announce Type: replace 
Abstract: Machine-learning models demand periodic updates to improve their average accuracy, exploiting novel architectures and additional data. However, a newly updated model may commit mistakes the previous model did not make. Such misclassifications are referred to as negative flips, experienced by users as a regression of performance. In this work, we show that this problem also affects robustness to adversarial examples, hindering the development of secure model update practices. In particular, when updating a model to improve its adversarial robustness, previously ineffective adversarial attacks on some inputs may become successful, causing a regression in the perceived security of the system. We propose a novel technique, named robustness-congruent adversarial training, to address this issue. It amounts to fine-tuning a model with adversarial training, while constraining it to retain higher robustness on the samples for which no adversarial example was found before the update. We show that our algorithm and, more generally, learning with non-regression constraints, provides a theoretically-grounded framework to train consistent estimators. Our experiments on robust models for computer vision confirm that both accuracy and robustness, even if improved after model update, can be affected by negative flips, and our robustness-congruent adversarial training can mitigate the problem, outperforming competing baseline methods.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Continual Learning Performance and Efficiency with Auxiliary Classifiers</title>
<link>https://arxiv.org/abs/2403.07404</link>
<guid>https://arxiv.org/abs/2403.07404</guid>
<content:encoded><![CDATA[
arXiv:2403.07404v4 Announce Type: replace 
Abstract: Continual learning is crucial for applying machine learning in challenging, dynamic, and often resource-constrained environments. However, catastrophic forgetting - overwriting previously learned knowledge when new information is acquired - remains a major challenge. In this work, we examine the intermediate representations in neural network layers during continual learning and find that such representations are less prone to forgetting, highlighting their potential to accelerate computation. Motivated by these findings, we propose to use auxiliary classifiers(ACs) to enhance performance and demonstrate that integrating ACs into various continual learning methods consistently improves accuracy across diverse evaluation settings, yielding an average 10% relative gain. We also leverage the ACs to reduce the average cost of the inference by 10-60% without compromising accuracy, enabling the model to return the predictions before computing all the layers. Our approach provides a scalable and efficient solution for continual learning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2404.05019</link>
<guid>https://arxiv.org/abs/2404.05019</guid>
<content:encoded><![CDATA[
arXiv:2404.05019v3 Announce Type: replace 
Abstract: Expert parallelism has emerged as a key strategy for distributing the computational workload of sparsely-gated mixture-of-experts (MoE) models across multiple devices, enabling the processing of increasingly large-scale models. However, the All-to-All communication inherent to expert parallelism poses a significant bottleneck, limiting the efficiency of MoE models. Although existing optimization methods partially mitigate this issue, they remain constrained by the sequential dependency between communication and computation operations. To address this challenge, we propose ScMoE, a novel shortcut-connected MoE architecture integrated with an overlapping parallelization strategy. ScMoE decouples communication from its conventional sequential ordering, enabling up to 100% overlap with computation. Compared to the prevalent top-2 MoE baseline, ScMoE achieves speedups of 1.49 times in training and 1.82 times in inference. Moreover, our experiments and analyses indicate that ScMoE not only achieves comparable but in some instances surpasses the model quality of existing approaches.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition</title>
<link>https://arxiv.org/abs/2404.08008</link>
<guid>https://arxiv.org/abs/2404.08008</guid>
<content:encoded><![CDATA[
arXiv:2404.08008v2 Announce Type: replace 
Abstract: Reliable evaluation of large language models (LLMs) is impeded by two key challenges: objective metrics often fail to reflect human perception of natural language, and exhaustive human labeling is prohibitively expensive. Here, we propose a sample-efficient human evaluation method for LLMs based on the principle of MAximum Discrepancy (MAD) Competition. Our method automatically and adaptively selects a compact set of input instructions that maximize semantic discrepancy between pairs of LLM responses. Human evaluators then perform three-alternative forced choices on these paired responses, which are aggregated into a global ranking using Elo rating. We apply our approach to compare eight widely used LLMs across four tasks: scientific knowledge understanding, mathematical reasoning, creative and functional writing, and code generation and explanation. Experimental results show that our sample-efficient evaluation method recovers "gold-standard" model rankings with a handful of MAD-selected instructions, reveals respective strengths and weaknesses of each LLM, and offers nuanced insights to guide future LLM development. Code is available at https://github.com/weiji-Feng/MAD-Eval .
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model</title>
<link>https://arxiv.org/abs/2405.14457</link>
<guid>https://arxiv.org/abs/2405.14457</guid>
<content:encoded><![CDATA[
arXiv:2405.14457v3 Announce Type: replace 
Abstract: Machine learning models can be trained with formal privacy guarantees via differentially private optimizers such as DP-SGD. In this work, we focus on a threat model where the adversary has access only to the final model, with no visibility into intermediate updates. In the literature, this hidden state threat model exhibits a significant gap between the lower bound from empirical privacy auditing and the theoretical upper bound provided by privacy accounting. To challenge this gap, we propose to audit this threat model with adversaries that craft a gradient sequence designed to maximize the privacy loss of the final model without relying on intermediate updates. Our experiments show that this approach consistently outperforms previous attempts at auditing the hidden state model. Furthermore, our results advance the understanding of achievable privacy guarantees within this threat model. Specifically, when the crafted gradient is inserted at every optimization step, we show that concealing the intermediate model updates in DP-SGD does not enhance the privacy guarantees. The situation is more complex when the crafted gradient is not inserted at every step: our auditing lower bound matches the privacy upper bound only for an adversarially-chosen loss landscape and a sufficiently large batch size. This suggests that existing privacy upper bounds can be improved in certain regimes.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closed-form Solutions: A New Perspective on Solving Differential Equations</title>
<link>https://arxiv.org/abs/2405.14620</link>
<guid>https://arxiv.org/abs/2405.14620</guid>
<content:encoded><![CDATA[
arXiv:2405.14620v3 Announce Type: replace 
Abstract: The quest for analytical solutions to differential equations has traditionally been constrained by the need for extensive mathematical expertise. Machine learning methods like genetic algorithms have shown promise in this domain, but are hindered by significant computational time and the complexity of their derived solutions. This paper introduces SSDE (Symbolic Solver for Differential Equations), a novel reinforcement learning-based approach that derives symbolic closed-form solutions for various differential equations. Evaluations across a diverse set of ordinary and partial differential equations demonstrate that SSDE outperforms existing machine learning methods, delivering superior accuracy and efficiency in obtaining analytical solutions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Perspectives: Plausible Counterfactual Explanations on Global, Group-wise, and Local Levels</title>
<link>https://arxiv.org/abs/2405.17642</link>
<guid>https://arxiv.org/abs/2405.17642</guid>
<content:encoded><![CDATA[
arXiv:2405.17642v2 Announce Type: replace 
Abstract: The growing complexity of AI systems has intensified the need for transparency through Explainable AI (XAI). Counterfactual explanations (CFs) offer actionable "what-if" scenarios on three levels: Local CFs providing instance-specific insights, Global CFs addressing broader trends, and Group-wise CFs (GWCFs) striking a balance and revealing patterns within cohesive groups. Despite the availability of methods for each granularity level, the field lacks a unified method that integrates these complementary approaches. We address this limitation by proposing a gradient-based optimization method for differentiable models that generates Local, Global, and Group-wise Counterfactual Explanations in a unified manner. We especially enhance GWCF generation by combining instance grouping and counterfactual generation into a single efficient process, replacing traditional two-step methods. Moreover, to ensure trustworthiness, we innovatively introduce the integration of plausibility criteria into the GWCF domain, making explanations both valid and realistic. Our results demonstrate the method's effectiveness in balancing validity, proximity, and plausibility while optimizing group granularity, with practical utility validated through practical use cases.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proper Dataset Valuation by Pointwise Mutual Information</title>
<link>https://arxiv.org/abs/2405.18253</link>
<guid>https://arxiv.org/abs/2405.18253</guid>
<content:encoded><![CDATA[
arXiv:2405.18253v3 Announce Type: replace 
Abstract: Data plays a central role in advancements in modern artificial intelligence, with high-quality data emerging as a key driver of model performance. This has prompted the development of principled and effective data curation methods in recent years. However, existing methods largely rely on heuristics, and whether they are truly effective remains unclear. For instance, standard evaluation methods that assess a trained model's performance on specific benchmarks may incentivize assigning high scores to data that merely resembles the test set. This issue exemplifies Goodhart's law: when a measure becomes a target, it ceases to be a good measure. To address this issue, we propose an information-theoretic framework for evaluating data curation methods. We define dataset quality in terms of its informativeness about the true model parameters, formalized using the Blackwell ordering of informativeness. Under this ordering, Blackwell's theorem ensures that more informative data yields optimal models with lower expected loss on the true underlying distribution. To measure informativeness, we show that the Blackwell order can be determined by the Shannon mutual information between the curated data and the test data. To estimate this mutual information, we introduce a novel method that trains Bayesian models on embedded datasets and computes mutual information from the posteriors of model parameters. Experiments on real-world data demonstrate that our mutual information-based evaluation assigns appropriately lower scores to data curation strategies that reduce dataset informativeness, while traditional test score-based evaluation methods may favor data curation strategies that overfit to the test set but compromise the training data's informativeness.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Diffusion: A Diffusion Based Model for Stochastic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2406.02827</link>
<guid>https://arxiv.org/abs/2406.02827</guid>
<content:encoded><![CDATA[
arXiv:2406.02827v2 Announce Type: replace 
Abstract: Recent innovations in diffusion probabilistic models have paved the way for significant progress in image, text and audio generation, leading to their applications in generative time series forecasting. However, leveraging such abilities to model highly stochastic time series data remains a challenge. In this paper, we propose a novel Stochastic Diffusion (StochDiff) model which learns data-driven prior knowledge at each time step by utilizing the representational power of the stochastic latent spaces to model the variability of the multivariate time series data. The learnt prior knowledge helps the model to capture complex temporal dynamics and the inherent uncertainty of the data. This improves its ability to model highly stochastic time series data. Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed model on stochastic time series forecasting. Additionally, we showcase an application of our model for real-world surgical guidance, highlighting its potential to benefit the medical community.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning</title>
<link>https://arxiv.org/abs/2406.09187</link>
<guid>https://arxiv.org/abs/2406.09187</guid>
<content:encoded><![CDATA[
arXiv:2406.09187v3 Announce Type: replace 
Abstract: The rapid advancement of large language model (LLM) agents has raised new concerns regarding their safety and security. In this paper, we propose GuardAgent, the first guardrail agent to protect target agents by dynamically checking whether their actions satisfy given safety guard requests. Specifically, GuardAgent first analyzes the safety guard requests to generate a task plan, and then maps this plan into guardrail code for execution. By performing the code execution, GuardAgent can deterministically follow the safety guard request and safeguard target agents. In both steps, an LLM is utilized as the reasoning component, supplemented by in-context demonstrations retrieved from a memory module storing experiences from previous tasks. In addition, we propose two novel benchmarks: EICU-AC benchmark to assess the access control for healthcare agents and Mind2Web-SC benchmark to evaluate the safety policies for web agents. We show that GuardAgent effectively moderates the violation actions for different types of agents on these two benchmarks with over 98% and 83% guardrail accuracies, respectively. Project page: https://guardagent.github.io/
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AI System for Continuous Knee Osteoarthritis Severity Grading Using Self-Supervised Anomaly Detection with Limited Data</title>
<link>https://arxiv.org/abs/2407.11500</link>
<guid>https://arxiv.org/abs/2407.11500</guid>
<content:encoded><![CDATA[
arXiv:2407.11500v2 Announce Type: replace 
Abstract: The diagnostic accuracy and subjectivity of existing Knee Osteoarthritis (OA) ordinal grading systems has been a subject of on-going debate and concern. Existing automated solutions are trained to emulate these imperfect systems, whilst also being reliant on large annotated databases for fully-supervised training. This work proposes a three stage approach for automated continuous grading of knee OA that is built upon the principles of Anomaly Detection (AD); learning a robust representation of healthy knee X-rays and grading disease severity based on its distance to the centre of normality. In the first stage, SS-FewSOME is proposed, a self-supervised AD technique that learns the 'normal' representation, requiring only examples of healthy subjects and <3% of the labels that existing methods require. In the second stage, this model is used to pseudo label a subset of unlabelled data as 'normal' or 'anomalous', followed by denoising of pseudo labels with CLIP. The final stage involves retraining on labelled and pseudo labelled data using the proposed Dual Centre Representation Learning (DCRL) which learns the centres of two representation spaces; normal and anomalous. Disease severity is then graded based on the distance to the learned centres. The proposed methodology outperforms existing techniques by margins of up to 24% in terms of OA detection and the disease severity scores correlate with the Kellgren-Lawrence grading system at the same level as human expert performance. Code available at https://github.com/niamhbelton/SS-FewSOME_Disease_Severity_Knee_Osteoarthritis.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Unlabeled Data Sharing through Kernel Function Approximation in Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.12307</link>
<guid>https://arxiv.org/abs/2408.12307</guid>
<content:encoded><![CDATA[
arXiv:2408.12307v3 Announce Type: replace 
Abstract: Offline reinforcement learning (RL) learns policies from a fixed dataset, but often requires large amounts of data. The challenge arises when labeled datasets are expensive, especially when rewards have to be provided by human labelers for large datasets. In contrast, unlabelled data tends to be less expensive. This situation highlights the importance of finding effective ways to use unlabelled data in offline RL, especially when labelled data is limited or expensive to obtain. In this paper, we present the algorithm to utilize the unlabeled data in the offline RL method with kernel function approximation and give the theoretical guarantee. We present various eigenvalue decay conditions of $\mathcal{H}_k$ which determine the complexity of the algorithm. In summary, our work provides a promising approach for exploiting the advantages offered by unlabeled data in offline RL, whilst maintaining theoretical assurances.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keep Everyone Happy: Online Fair Division of Numerous Items with Few Copies</title>
<link>https://arxiv.org/abs/2408.12845</link>
<guid>https://arxiv.org/abs/2408.12845</guid>
<content:encoded><![CDATA[
arXiv:2408.12845v2 Announce Type: replace 
Abstract: This paper considers a novel variant of the online fair division problem involving multiple agents in which a learner sequentially observes an indivisible item that has to be irrevocably allocated to one of the agents while satisfying a fairness and efficiency constraint. Existing algorithms assume a small number of items with a sufficiently large number of copies, which ensures a good utility estimation for all item-agent pairs from noisy bandit feedback. However, this assumption may not hold in many real-life applications, for example, an online platform that has a large number of users (items) who use the platform's service providers (agents) only a few times (a few copies of items), which makes it difficult to accurately estimate utilities for all item-agent pairs. To address this, we assume utility is an unknown function of item-agent features. We then propose algorithms that model online fair division as a contextual bandit problem, with sub-linear regret guarantees. Our experimental results further validate the effectiveness of the proposed algorithms.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning</title>
<link>https://arxiv.org/abs/2408.14774</link>
<guid>https://arxiv.org/abs/2408.14774</guid>
<content:encoded><![CDATA[
arXiv:2408.14774v4 Announce Type: replace 
Abstract: We introduce Instruct-SkillMix, an automated approach for creating diverse, high quality SFT data for instruction-following. The pipeline involves two stages, each leveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to extract core "skills" for instruction-following by directly prompting the model. This is inspired by ``LLM metacognition'' of Didolkar et al. (2024); (2) Data generation: uses the powerful LLM to generate (instruction, response) data that exhibit a randomly chosen pair of these skills. Here, the use of random skill combinations promotes diversity and difficulty. The estimated cost of creating the dataset is under $600.
  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from Instruct-SkillMix leads to strong gains on instruction following benchmarks such as AlpacaEval 2.0, MT-Bench, and WildBench. With just 4K examples, LLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0, a level similar to frontier models like Claude 3 Opus and LLaMA-3.1-405B-Instruct. Ablation studies also suggest plausible reasons for why creating open instruction-tuning datasets via naive crowd-sourcing has proved difficult. In our dataset, adding 20% low quality answers (``shirkers'') causes a noticeable degradation in performance. The Instruct-SkillMix pipeline seems flexible and adaptable to other settings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Data, My Model: Learning Who Really Helps in Federated Learning</title>
<link>https://arxiv.org/abs/2409.02064</link>
<guid>https://arxiv.org/abs/2409.02064</guid>
<content:encoded><![CDATA[
arXiv:2409.02064v3 Announce Type: replace 
Abstract: Many important machine learning applications involve networks of devices-such as wearables or smartphones-that generate local data and train personalized models. A key challenge is determining which peers are most beneficial for collaboration. We propose a simple and privacy-preserving method to select relevant collaborators by evaluating how much a model improves after a single gradient step using another devices data-without sharing raw data. This method naturally extends to non-parametric models by replacing the gradient step with a non-parametric generalization. Our approach enables model-agnostic, data-driven peer selection for personalized federated learning (PersFL).
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Filtration for RLHF to Mitigate Noise in Reward Models</title>
<link>https://arxiv.org/abs/2409.06957</link>
<guid>https://arxiv.org/abs/2409.06957</guid>
<content:encoded><![CDATA[
arXiv:2409.06957v4 Announce Type: replace 
Abstract: While direct policy optimization methods exist, pioneering LLMs are fine-tuned with reinforcement learning from human feedback (RLHF) to generate better responses under the supervision of a reward model learned from preference data. One major challenge of RLHF is the inaccuracy of the intermediate reward model, especially in the tasks that requires complex reasoning for the reward model to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve the signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtering strategy, we use the coefficient of determination (R2) between the rewards and actual scores on filtered samples as the metrics to help us find promising strategies since it measures how well the rewards filtered by PF-PPO indicate real performance. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation and math reasoning tasks. In code generation, PF-PPO achieves the state-of-the-art performance of 7-billion-parameter models on HumanEval (+7.9%), MBPP (+0.7%), and LeetCode Contest (+10.0%) which is a more challenging benchmark created by us. In math reasoning, PF-PPO yields performance increase using different reward models and benchmarks (Ape210K and CMATH). Code is available on https://github.com/DtYXs/verl/tree/pf-ppo.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-Device Collaborative Language Modeling via a Mixture of Generalists and Specialists</title>
<link>https://arxiv.org/abs/2409.13931</link>
<guid>https://arxiv.org/abs/2409.13931</guid>
<content:encoded><![CDATA[
arXiv:2409.13931v4 Announce Type: replace 
Abstract: On-device LLMs have gained increasing attention for their ability to enhance privacy and provide a personalized user experience. To facilitate private learning with scarce data, Federated Learning has become a standard approach. However, it faces challenges such as computational resource heterogeneity and data heterogeneity among end users. We propose CoMiGS ($\textbf{Co}$llaborative learning with a $\textbf{Mi}$xture of $\textbf{G}$eneralists and $\textbf{S}$pecialists), the first approach to address both challenges. A key innovation of our method is the bi-level optimization formulation of the Mixture-of-Experts learning objective, where the router is optimized using a separate validation set to ensure alignment with the target distribution. We solve our objective with alternating minimization, for which we provide a theoretical analysis. Our method shares generalist experts across users while localizing a varying number of specialist experts, thereby adapting to users' computational resources and preserving privacy. Through extensive experiments, we show CoMiGS effectively balances general and personalized knowledge for each token generation. We demonstrate that CoMiGS remains robust against overfitting-due to the generalists' regularizing effect-while adapting to local data through specialist expertise. We open source our codebase for collaborative LLMs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Protocols for Continual Learning via Statistical Physics and Control Theory</title>
<link>https://arxiv.org/abs/2409.18061</link>
<guid>https://arxiv.org/abs/2409.18061</guid>
<content:encoded><![CDATA[
arXiv:2409.18061v3 Announce Type: replace 
Abstract: Artificial neural networks often struggle with catastrophic forgetting when learning multiple tasks sequentially, as training on new tasks degrades the performance on previously learned tasks. Recent theoretical work has addressed this issue by analysing learning curves in synthetic frameworks under predefined training protocols. However, these protocols relied on heuristics and lacked a solid theoretical foundation assessing their optimality. In this paper, we fill this gap by combining exact equations for training dynamics, derived using statistical physics techniques, with optimal control methods. We apply this approach to teacher-student models for continual learning and multi-task problems, obtaining a theory for task-selection protocols maximising performance while minimising forgetting. Our theoretical analysis offers non-trivial yet interpretable strategies for mitigating catastrophic forgetting, shedding light on how optimal learning protocols modulate established effects, such as the influence of task similarity on forgetting. Finally, we validate our theoretical findings with experiments on real-world data.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2410.02681</link>
<guid>https://arxiv.org/abs/2410.02681</guid>
<content:encoded><![CDATA[
arXiv:2410.02681v4 Announce Type: replace 
Abstract: Confidence calibration is critical for the safe deployment of machine learning models in the real world. However, such issue in vision-language models like CLIP, particularly after fine-tuning, has not been fully addressed. In this work, we demonstrate that existing prompt tuning methods usually lead to a trade-off of calibration between base and new classes: the cross-entropy loss in CoOp causes overconfidence in new classes by increasing textual label divergence, whereas the regularization of KgCoOp maintains the confidence level but results in underconfidence in base classes due to the improved accuracy. Inspired by the observations, we introduce Dynamic Outlier Regularization (DOR) to ensure the confidence calibration on both base and new classes after fine-tuning. In particular, we propose to minimize the feature deviation of novel textual labels (instead of base classes) sampled from a large vocabulary. In effect, DOR prevents the increase in textual divergence for new labels while easing restrictions on base classes. Extensive experiments demonstrate that DOR can enhance the calibration performance of current fine-tuning methods on base and new classes.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning</title>
<link>https://arxiv.org/abs/2410.03348</link>
<guid>https://arxiv.org/abs/2410.03348</guid>
<content:encoded><![CDATA[
arXiv:2410.03348v4 Announce Type: replace 
Abstract: Neurosymbolic learning enables the integration of symbolic reasoning with deep learning but faces significant challenges in scaling to complex symbolic programs, large datasets, or both. We introduce DOLPHIN, a framework that tackles these challenges by supporting neurosymbolic programs in Python, executing complex symbolic reasoning on the CPU while vectorizing probabilistic computations and gradient propagation on the GPU. Across 13 benchmarks spanning tasks over text, image, and video data, with symbolic reasoning features like recursion and black-box functions, DOLPHIN converges to state-of-the-art accuracies on the more complex benchmarks while existing frameworks such as Scallop, ISED, and IndeCateR+ fail to converge within the time limit. On simpler benchmarks, DOLPHIN matches their performance, while achieving these results 1.71x to 62x faster than the baselines. Overall, DOLPHIN advances the scalability of neurosymbolic frameworks, achieving state-of-the-art efficiency and convergence on difficult benchmarks where existing frameworks struggle. The code is published at https://github.com/Dolphin-NeSy/Dolphin.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2410.03972</link>
<guid>https://arxiv.org/abs/2410.03972</guid>
<content:encoded><![CDATA[
arXiv:2410.03972v2 Announce Type: replace 
Abstract: Task-trained recurrent neural networks (RNNs) are widely used in neuroscience and machine learning to model dynamical computations. To gain mechanistic insight into how neural systems solve tasks, prior work often reverse-engineers individual trained networks. However, different RNNs trained on the same task and achieving similar performance can exhibit strikingly different internal solutions-a phenomenon known as solution degeneracy. Here, we develop a unified framework to systematically quantify and control solution degeneracy across three levels: behavior, neural dynamics, and weight space. We apply this framework to 3,400 RNNs trained on four neuroscience-relevant tasks-flip-flop memory, sine wave generation, delayed discrimination, and path integration-while systematically varying task complexity, learning regime, network size, and regularization. We find that higher task complexity and stronger feature learning reduce degeneracy in neural dynamics but increase it in weight space, with mixed effects on behavior. In contrast, larger networks and structural regularization reduce degeneracy at all three levels. These findings empirically validate the Contravariance Principle and provide practical guidance for researchers aiming to tailor RNN solutions-whether to uncover shared neural mechanisms or to model individual variability observed in biological systems. This work provides a principled framework for quantifying and controlling solution degeneracy in task-trained RNNs, offering new tools for building more interpretable and biologically grounded models of neural computation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bayesian Model Selection Criterion for Selecting Pretraining Checkpoints</title>
<link>https://arxiv.org/abs/2410.05612</link>
<guid>https://arxiv.org/abs/2410.05612</guid>
<content:encoded><![CDATA[
arXiv:2410.05612v2 Announce Type: replace 
Abstract: Recent advances in artificial intelligence have been fueled by the development of foundation models such as BERT, GPT, T5, and Vision Transformers. These models are first pretrained on vast and diverse datasets and then adapted to specific downstream tasks, often with significantly less data. However, the mechanisms behind the success of this ubiquitous pretrain-then-adapt paradigm remain underexplored, particularly the characteristics of pretraining checkpoints that enhance downstream adaptation. We introduce a Bayesian model selection criterion, called the downstream free energy, which quantifies a checkpoint's adaptability by measuring the concentration of nearby favorable parameters for the downstream task. We demonstrate that this Bayesian model selection criterion can be effectively implemented without access to the downstream data or prior knowledge of the downstream task. Furthermore, we provide empirical evidence that the criterion reliably correlates with improved finetuning performance, offering a principled approach to predicting model adaptability.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression</title>
<link>https://arxiv.org/abs/2410.09615</link>
<guid>https://arxiv.org/abs/2410.09615</guid>
<content:encoded><![CDATA[
arXiv:2410.09615v3 Announce Type: replace 
Abstract: Conventional model compression techniques for LLMs address high memory consumption and slow inference challenges but typically require computationally expensive retraining to preserve accuracy. In contrast, one-shot compression methods eliminate retraining cost, but struggle to achieve accuracy comparable to dense models. This paper presents SLIM, a new one-shot compression framework that holistically integrates hardware-friendly quantization, sparsity, and low-rank approximation into a unified process. First, we formulate the quantization process using a probabilistic approach (SLIM-Quant) that enables us to apply uniform quantization. Then, we use an existing one-shot pruning method to apply semi-structured sparsity on top of the quantized weights. Finally, to compensate for the introduced aggregated quantization and sparsity error, we use a novel saliency function with unique invertible and additive features that enables us to mathematically compute the value of low-rank adapters. SLIM improves model accuracy by up to 5.66% (LLaMA-2-7B) for 2:4 sparsity with 4-bit weight quantization, outperforming prior methods. Models compressed with SLIM achieve up to 4.3x and 3.8x on Nvidia RTX3060 and A100 GPUs, respectively. Additionally, they achieve up to 0.23x end-to-end memory reduction in comparison to their dense counterparts. We also propose an optional PEFT recipe that further improves accuracy by up to 1.66% (LLaMA-2-13B) compared to SLIM without fine-tuning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.09754</link>
<guid>https://arxiv.org/abs/2410.09754</guid>
<content:encoded><![CDATA[
arXiv:2410.09754v2 Announce Type: replace 
Abstract: Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms-including off-policy, on-policy, and unsupervised methods-is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Training Convergence of Transformers for In-Context Classification of Gaussian Mixtures</title>
<link>https://arxiv.org/abs/2410.11778</link>
<guid>https://arxiv.org/abs/2410.11778</guid>
<content:encoded><![CDATA[
arXiv:2410.11778v3 Announce Type: replace 
Abstract: Although transformers have demonstrated impressive capabilities for in-context learning (ICL) in practice, theoretical understanding of the underlying mechanism that allows transformers to perform ICL is still in its infancy. This work aims to theoretically study the training dynamics of transformers for in-context classification tasks. We demonstrate that, for in-context classification of Gaussian mixtures under certain assumptions, a single-layer transformer trained via gradient descent converges to a globally optimal model at a linear rate. We further quantify the impact of the training and testing prompt lengths on the ICL inference error of the trained transformer. We show that when the lengths of training and testing prompts are sufficiently large, the prediction of the trained transformer approaches the ground truth distribution of the labels. Experimental results corroborate the theoretical findings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are You Using Reliable Graph Prompts? Trojan Prompt Attacks on Graph Neural Networks</title>
<link>https://arxiv.org/abs/2410.13974</link>
<guid>https://arxiv.org/abs/2410.13974</guid>
<content:encoded><![CDATA[
arXiv:2410.13974v2 Announce Type: replace 
Abstract: Graph Prompt Learning (GPL) has been introduced as a promising approach that uses prompts to adapt pre-trained GNN models to specific downstream tasks without requiring fine-tuning of the entire model. Despite the advantages of GPL, little attention has been given to its vulnerability to backdoor attacks, where an adversary can manipulate the model's behavior by embedding hidden triggers. Existing graph backdoor attacks rely on modifying model parameters during training, but this approach is impractical in GPL as GNN encoder parameters are frozen after pre-training. Moreover, downstream users may fine-tune their own task models on clean datasets, further complicating the attack. In this paper, we propose TGPA, a backdoor attack framework designed specifically for GPL. TGPA injects backdoors into graph prompts without modifying pre-trained GNN encoders and ensures high attack success rates and clean accuracy. To address the challenge of model fine-tuning by users, we introduce a finetuning-resistant poisoning approach that maintains the effectiveness of the backdoor even after downstream model adjustments. Extensive experiments on multiple datasets under various settings demonstrate the effectiveness of TGPA in compromising GPL models with fixed GNN encoders.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGD Jittering: A Training Strategy for Robust and Accurate Model-Based Architectures</title>
<link>https://arxiv.org/abs/2410.14667</link>
<guid>https://arxiv.org/abs/2410.14667</guid>
<content:encoded><![CDATA[
arXiv:2410.14667v2 Announce Type: replace 
Abstract: Inverse problems aim to reconstruct unseen data from corrupted or perturbed measurements. While most work focuses on improving reconstruction quality, generalization accuracy and robustness are equally important, especially for safety-critical applications. Model-based architectures (MBAs), such as loop unrolling methods, are considered more interpretable and achieve better reconstructions. Empirical evidence suggests that MBAs are more robust to perturbations than black-box solvers, but the accuracy-robustness tradeoff in MBAs remains underexplored. In this work, we propose a simple yet effective training scheme for MBAs, called SGD jittering, which injects noise iteration-wise during reconstruction. We theoretically demonstrate that SGD jittering not only generalizes better than the standard mean squared error training but is also more robust to average-case attacks. We validate SGD jittering using denoising toy examples, seismic deconvolution, and single-coil MRI reconstruction. Both SGD jittering and its SPGD extension yield cleaner reconstructions for out-of-distribution data and demonstrates enhanced robustness against adversarial attacks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphNarrator: Generating Textual Explanations for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2410.15268</link>
<guid>https://arxiv.org/abs/2410.15268</guid>
<content:encoded><![CDATA[
arXiv:2410.15268v2 Announce Type: replace 
Abstract: Graph representation learning has garnered significant attention due to its broad applications in various domains, such as recommendation systems and social network analysis. Despite advancements in graph learning methods, challenges still remain in explainability when graphs are associated with semantic features. In this paper, we present GraphNarrator, the first method designed to generate natural language explanations for Graph Neural Networks. GraphNarrator employs a generative language model that maps input-output pairs to explanations reflecting the model's decision-making process. To address the lack of ground truth explanations to train the model, we propose first generating pseudo-labels that capture the model's decisions from saliency-based explanations, then using Expert Iteration to iteratively train the pseudo-label generator based on training objectives on explanation quality. The high-quality pseudo-labels are finally utilized to train an end-to-end explanation generator model. Extensive experiments are conducted to demonstrate the effectiveness of GraphNarrator in producing faithful, concise, and human-preferred natural language explanations.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces</title>
<link>https://arxiv.org/abs/2410.15625</link>
<guid>https://arxiv.org/abs/2410.15625</guid>
<content:encoded><![CDATA[
arXiv:2410.15625v3 Announce Type: replace 
Abstract: Modern scientific discovery increasingly relies on high-performance computing for complex modeling and simulation. A key challenge in improving parallel program performance is efficiently mapping tasks to processors and data to memory, a process dictated by intricate, low-level system code known as mappers. Developing high-performance mappers demands days of manual tuning, posing a significant barrier for domain scientists without systems expertise. We introduce a framework that automates mapper development with generative optimization, leveraging richer feedback beyond scalar performance metrics. Our approach features the Agent-System Interface, which includes a Domain-Specific Language (DSL) to abstract away the low-level complexity of system code and define a structured search space, as well as AutoGuide, a mechanism that interprets raw execution output into actionable feedback. Unlike traditional reinforcement learning methods such as OpenTuner, which rely solely on scalar feedback, our method finds superior mappers in far fewer iterations. With just 10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster performance. Our approach finds mappers that surpass expert-written mappers by up to 1.34X speedup across nine benchmarks while reducing tuning time from days to minutes.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing</title>
<link>https://arxiv.org/abs/2410.17194</link>
<guid>https://arxiv.org/abs/2410.17194</guid>
<content:encoded><![CDATA[
arXiv:2410.17194v4 Announce Type: replace 
Abstract: Knowledge Editing (KE) algorithms alter models' weights to perform targeted updates to incorrect, outdated, or otherwise unwanted factual associations. However, recent work has shown that applying KE can adversely affect models' broader factual recall accuracy and diminish their reasoning abilities. Although these studies give insights into the potential harms of KE algorithms, e.g., performance evaluations on benchmarks, little is understood about why such destructive failures occur. Motivated by this, we define a novel synthetic task in which a Transformer is trained from scratch to internalize a "structured" knowledge graph. The structure enforces relationships between entities of the graph, such that editing a factual association has "trickling effects" on other entities (e.g., altering X's parent is Y to Z affects who X's siblings' parent is). Through evaluations of edited models on this task, we show that KE inadvertently affects representations of entities beyond the targeted one, distorting relevant structures that allow a model to infer unseen knowledge about an entity. We call this phenomenon representation shattering and demonstrate that it degrades models' factual recall and reasoning performance. We further corroborate our findings in naturalistic settings with pre-trained Llama and Mamba models as well. Overall, our work yields a precise mechanistic hypothesis to explain why KE has adverse effects on model abilities.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Linear Bandits with Blocking Constraints</title>
<link>https://arxiv.org/abs/2410.20041</link>
<guid>https://arxiv.org/abs/2410.20041</guid>
<content:encoded><![CDATA[
arXiv:2410.20041v2 Announce Type: replace 
Abstract: We investigate the high-dimensional sparse linear bandits problem in a data-poor regime where the time horizon is much smaller than the ambient dimension and number of arms. We study the setting under the additional blocking constraint where each unique arm can be pulled only once. The blocking constraint is motivated by practical applications in personalized content recommendation and identification of data points to improve annotation efficiency for complex learning tasks. With mild assumptions on the arms, our proposed online algorithm (BSLB) achieves a regret guarantee of $\widetilde{\mathsf{O}}((1+\beta_k)^2k^{\frac{2}{3}} \mathsf{T}^{\frac{2}{3}})$ where the parameter vector has an (unknown) relative tail $\beta_k$ -- the ratio of $\ell_1$ norm of the top-$k$ and remaining entries of the parameter vector. To this end, we show novel offline statistical guarantees of the lasso estimator for the linear model that is robust to the sparsity modeling assumption. Finally, we propose a meta-algorithm (C-BSLB) based on corralling that does not need knowledge of optimal sparsity parameter $k$ at minimal cost to regret. Our experiments on multiple real-world datasets demonstrate the validity of our algorithms and theoretical framework.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GWQ: Gradient-Aware Weight Quantization for Large Language Models</title>
<link>https://arxiv.org/abs/2411.00850</link>
<guid>https://arxiv.org/abs/2411.00850</guid>
<content:encoded><![CDATA[
arXiv:2411.00850v4 Announce Type: replace 
Abstract: Large language models (LLMs) show impressive performance in solving complex language tasks. However, its large number of parameters presents significant challenges for the deployment. So, compressing LLMs to low bits can enable to deploy on resource-constrained devices. To address this problem, we propose gradient-aware weight quantization (GWQ), the first quantization approach for low-bit weight quantization that leverages gradients to localize outliers, requiring only a minimal amount of calibration data for outlier detection. GWQ retains the top 1\% outliers preferentially at FP16 precision, while the remaining non-outlier weights are stored in a low-bit. We widely evaluate GWQ on different task include language modeling, grounding detection, massive multitask language understanding and vision-language question and answering. Results show that models quantified by GWQ performs better than other quantization method. During quantization process, GWQ only need one calibration set to realize effective quant. Also, GWQ achieves 1.2x inference speedup in comparison to the original model and effectively reduces the inference memory.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Sampling Correction via Approximately 10 Parameters</title>
<link>https://arxiv.org/abs/2411.06503</link>
<guid>https://arxiv.org/abs/2411.06503</guid>
<content:encoded><![CDATA[
arXiv:2411.06503v3 Announce Type: replace 
Abstract: While powerful for generation, Diffusion Probabilistic Models (DPMs) face slow sampling challenges, for which various distillation-based methods have been proposed. However, they typically require significant additional training costs and model parameter storage, limiting their practicality. In this work, we propose PCA-based Adaptive Search (PAS), which optimizes existing solvers for DPMs with minimal additional costs. Specifically, we first employ PCA to obtain a few basis vectors to span the high-dimensional sampling space, which enables us to learn just a set of coordinates to correct the sampling direction; furthermore, based on the observation that the cumulative truncation error exhibits an ``S"-shape, we design an adaptive search strategy that further enhances the sampling efficiency and reduces the number of stored parameters to approximately 10. Extensive experiments demonstrate that PAS can significantly enhance existing fast solvers in a plug-and-play manner with negligible costs. E.g., on CIFAR10, PAS optimizes DDIM's FID from 15.69 to 4.37 (NFE=10) using only 12 parameters and sub-minute training on a single A100 GPU. Code is available at https://github.com/onefly123/PAS.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GrokFormer: Graph Fourier Kolmogorov-Arnold Transformers</title>
<link>https://arxiv.org/abs/2411.17296</link>
<guid>https://arxiv.org/abs/2411.17296</guid>
<content:encoded><![CDATA[
arXiv:2411.17296v3 Announce Type: replace 
Abstract: Graph Transformers (GTs) have demonstrated remarkable performance in graph representation learning over popular graph neural networks (GNNs). However, self--attention, the core module of GTs, preserves only low-frequency signals in graph features, leading to ineffectiveness in capturing other important signals like high-frequency ones. Some recent GT models help alleviate this issue, but their flexibility and expressiveness are still limited since the filters they learn are fixed on predefined graph spectrum or spectral order. To tackle this challenge, we propose a Graph Fourier Kolmogorov-Arnold Transformer (GrokFormer), a novel GT model that learns highly expressive spectral filters with adaptive graph spectrum and spectral order through a Fourier series modeling over learnable activation functions. We demonstrate theoretically and empirically that the proposed GrokFormer filter offers better expressiveness than other spectral methods. Comprehensive experiments on 10 real-world node classification datasets across various domains, scales, and graph properties, as well as 5 graph classification datasets, show that GrokFormer outperforms state-of-the-art GTs and GNNs. Our code is available at https://github.com/GGA23/GrokFormer
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCoTT: Strategic Chain-of-Thought Tasking for Wireless-Aware Robot Navigation in Digital Twins</title>
<link>https://arxiv.org/abs/2411.18212</link>
<guid>https://arxiv.org/abs/2411.18212</guid>
<content:encoded><![CDATA[
arXiv:2411.18212v2 Announce Type: replace 
Abstract: Path planning under wireless performance constraints is a complex challenge in robot navigation. However, naively incorporating such constraints into classical planning algorithms often incurs prohibitive search costs. In this paper, we propose SCoTT, a wireless-aware path planning framework that leverages vision-language models (VLMs) to co-optimize average path gains and trajectory length using wireless heatmap images and ray-tracing data from a digital twin (DT). At the core of our framework is Strategic Chain-of-Thought Tasking (SCoTT), a novel prompting paradigm that decomposes the exhaustive search problem into structured subtasks, each solved via chain-of-thought prompting. To establish strong baselines, we compare classical A* and wireless-aware extensions of it, and derive DP-WA*, an optimal, iterative dynamic programming algorithm that incorporates all path gains and distance metrics from the DT, but at significant computational cost. In extensive experiments, we show that SCoTT achieves path gains within 2% of DP-WA* while consistently generating shorter trajectories. Moreover, SCoTT's intermediate outputs can be used to accelerate DP-WA* by reducing its search space, saving up to 62% in execution time. We validate our framework using four VLMs, demonstrating effectiveness across both large and small models, thus making it applicable to a wide range of compact models at low inference cost. We also show the practical viability of our approach by deploying SCoTT as a ROS node within Gazebo simulations. Finally, we discuss data acquisition pipelines, compute requirements, and deployment considerations for VLMs in 6G-enabled DTs, underscoring the potential of natural language interfaces for wireless-aware navigation in real-world applications.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Model for One Graph: A New Perspective for Pretraining with Cross-domain Graphs</title>
<link>https://arxiv.org/abs/2412.00315</link>
<guid>https://arxiv.org/abs/2412.00315</guid>
<content:encoded><![CDATA[
arXiv:2412.00315v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have emerged as a powerful tool to capture intricate network patterns, achieving success across different domains. However, existing GNNs require careful domain-specific architecture designs and training from scratch on each dataset, leading to an expertise-intensive process with difficulty in generalizing across graphs from different domains. Therefore, it can be hard for practitioners to infer which GNN model can generalize well to graphs from their domains. To address this challenge, we propose a novel cross-domain pretraining framework, "one model for one graph," which overcomes the limitations of previous approaches that failed to use a single GNN to capture diverse graph patterns across domains with significant gaps. Specifically, we pretrain a bank of expert models, with each one corresponding to a specific dataset. When inferring to a new graph, gating functions choose a subset of experts to effectively integrate prior model knowledge while avoiding negative transfer. Extensive experiments consistently demonstrate the superiority of our proposed method on both link prediction and node classification tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combinatorial Rising Bandit</title>
<link>https://arxiv.org/abs/2412.00798</link>
<guid>https://arxiv.org/abs/2412.00798</guid>
<content:encoded><![CDATA[
arXiv:2412.00798v3 Announce Type: replace 
Abstract: Combinatorial online learning is a fundamental task for selecting the optimal action (or super arm) as a combination of base arms in sequential interactions with systems providing stochastic rewards. It is applicable to diverse domains such as robotics, social advertising, network routing, and recommendation systems. In many real-world scenarios, we often encounter rising rewards, where playing a base arm not only provides an instantaneous reward but also contributes to the enhancement of future rewards, e.g., robots enhancing proficiency through practice and social influence strengthening in the history of successful recommendations. Moreover, the enhancement of a single base arm may affect multiple super arms that include it, introducing complex dependencies that are not captured by existing rising bandit models. To address this, we introduce the Combinatorial Rising Bandit (CRB) framework and propose a provably efficient algorithm, Combinatorial Rising Upper Confidence Bound (CRUCB). We establish an upper bound on regret CRUCB and show that it is nearly tight by deriving a matching lower bound. In addition, we empirically demonstrate the effectiveness of CRUCB not only in synthetic environments but also in realistic applications of deep reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INRFlow: Flow Matching for INRs in Ambient Space</title>
<link>https://arxiv.org/abs/2412.03791</link>
<guid>https://arxiv.org/abs/2412.03791</guid>
<content:encoded><![CDATA[
arXiv:2412.03791v2 Announce Type: replace 
Abstract: Flow matching models have emerged as a powerful method for generative modeling on domains like images or videos, and even on irregular or unstructured data like 3D point clouds or even protein structures. These models are commonly trained in two stages: first, a data compressor is trained, and in a subsequent training stage a flow matching generative model is trained in the latent space of the data compressor. This two-stage paradigm sets obstacles for unifying models across data domains, as hand-crafted compressors architectures are used for different data modalities. To this end, we introduce INRFlow, a domain-agnostic approach to learn flow matching transformers directly in ambient space. Drawing inspiration from INRs, we introduce a conditionally independent point-wise training objective that enables INRFlow to make predictions continuously in coordinate space. Our empirical results demonstrate that INRFlow effectively handles different data modalities such as images, 3D point clouds and protein structure data, achieving strong performance in different domains and outperforming comparable approaches. INRFlow is a promising step towards domain-agnostic flow matching generative models that can be trivially adopted in different data domains.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition</title>
<link>https://arxiv.org/abs/2412.17333</link>
<guid>https://arxiv.org/abs/2412.17333</guid>
<content:encoded><![CDATA[
arXiv:2412.17333v2 Announce Type: replace 
Abstract: Shock waves caused by earthquakes can be devastating. Generating realistic earthquake-caused ground motion waveforms help reducing losses in lives and properties, yet generative models for the task tend to generate subpar waveforms. We present High-fidelity Earthquake Groundmotion Generation System (HEGGS) and demonstrate its superior performance using earthquakes from North American, East Asian, and European regions. HEGGS exploits the intrinsic characteristics of earthquake dataset and learns the waveforms using an end-to-end differentiable generator containing conditional latent diffusion model and hi-fidelity waveform construction model. We show the learning efficiency of HEGGS by training it on a single GPU machine and validate its performance using earthquake databases from North America, East Asia, and Europe, using diverse criteria from waveform generation tasks and seismology. Once trained, HEGGS can generate three dimensional E-N-Z seismic waveforms with accurate P/S phase arrivals, envelope correlation, signal-to-noise ratio, GMPE analysis, frequency content analysis, and section plot analysis.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing Cryptocurrency Trading Strategy Based on Autoencoder-CNN-GANs Algorithms</title>
<link>https://arxiv.org/abs/2412.18202</link>
<guid>https://arxiv.org/abs/2412.18202</guid>
<content:encoded><![CDATA[
arXiv:2412.18202v5 Announce Type: replace 
Abstract: This paper leverages machine learning algorithms to forecast and analyze financial time series. The process begins with a denoising autoencoder to filter out random noise fluctuations from the main contract price data. Then, one-dimensional convolution reduces the dimensionality of the filtered data and extracts key information. The filtered and dimensionality-reduced price data is fed into a GANs network, and its output serve as input of a fully connected network. Through cross-validation, a model is trained to capture features that precede large price fluctuations. The model predicts the likelihood and direction of significant price changes in real-time price sequences, placing trades at moments of high prediction accuracy. Empirical results demonstrate that using autoencoders and convolution to filter and denoise financial data, combined with GANs, achieves a certain level of predictive performance, validating the capabilities of machine learning algorithms to discover underlying patterns in financial sequences. Keywords - CNN;GANs; Cryptocurrency; Prediction.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment</title>
<link>https://arxiv.org/abs/2501.09620</link>
<guid>https://arxiv.org/abs/2501.09620</guid>
<content:encoded><![CDATA[
arXiv:2501.09620v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have demonstrated significant progress in performing complex tasks. While Reinforcement Learning from Human Feedback (RLHF) has been effective in aligning LLMs with human preferences, it is susceptible to spurious correlations in reward modeling. Consequently, it often introduces biases-such as length bias, sycophancy, conceptual bias, and discrimination-that hinder the model's ability to capture true causal relationships. To address this, we propose a novel causal reward modeling approach that integrates causality to mitigate these spurious correlations. Our method enforces counterfactual invariance, ensuring reward predictions remain consistent when irrelevant variables are altered. Through experiments on both synthetic and real-world datasets, we show that our approach mitigates various types of spurious correlations effectively, resulting in more reliable and fair alignment of LLMs with human preferences. As a drop-in enhancement to the existing RLHF workflow, our causal reward modeling provides a practical way to improve the trustworthiness and fairness of LLM finetuning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credit Risk Identification in Supply Chains Using Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2501.10348</link>
<guid>https://arxiv.org/abs/2501.10348</guid>
<content:encoded><![CDATA[
arXiv:2501.10348v4 Announce Type: replace 
Abstract: Credit risk management within supply chains has emerged as a critical research area due to its significant implications for operational stability and financial sustainability. The intricate interdependencies among supply chain participants mean that credit risks can propagate across networks, with impacts varying by industry. This study explores the application of Generative Adversarial Networks (GANs) to enhance credit risk identification in supply chains. GANs enable the generation of synthetic credit risk scenarios, addressing challenges related to data scarcity and imbalanced datasets. By leveraging GAN-generated data, the model improves predictive accuracy while effectively capturing dynamic and temporal dependencies in supply chain data. The research focuses on three representative industries-manufacturing (steel), distribution (pharmaceuticals), and services (e-commerce) to assess industry-specific credit risk contagion. Experimental results demonstrate that the GAN-based model outperforms traditional methods, including logistic regression, decision trees, and neural networks, achieving superior accuracy, recall, and F1 scores. The findings underscore the potential of GANs in proactive risk management, offering robust tools for mitigating financial disruptions in supply chains. Future research could expand the model by incorporating external market factors and supplier relationships to further enhance predictive capabilities. Keywords- Generative Adversarial Networks (GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data Augmentation
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Granger Causality Learning for Interdependent Clients with State Space Representation</title>
<link>https://arxiv.org/abs/2501.13890</link>
<guid>https://arxiv.org/abs/2501.13890</guid>
<content:encoded><![CDATA[
arXiv:2501.13890v4 Announce Type: replace 
Abstract: Advanced sensors and IoT devices have improved the monitoring and control of complex industrial enterprises. They have also created an interdependent fabric of geographically distributed process operations (clients) across these enterprises. Granger causality is an effective approach to detect and quantify interdependencies by examining how one client's state affects others over time. Understanding these interdependencies captures how localized events, such as faults and disruptions, can propagate throughout the system, possibly causing widespread operational impacts. However, the large volume and complexity of industrial data pose challenges in modeling these interdependencies. This paper develops a federated approach to learning Granger causality. We utilize a linear state space system framework that leverages low-dimensional state estimates to analyze interdependencies. This addresses bandwidth limitations and the computational burden commonly associated with centralized data processing. We propose augmenting the client models with the Granger causality information learned by the server through a Machine Learning (ML) function. We examine the co-dependence between the augmented client and server models and reformulate the framework as a standalone ML algorithm providing conditions for its sublinear and linear convergence rates. We also study the convergence of the framework to a centralized oracle model. Moreover, we include a differential privacy analysis to ensure data security while preserving causal insights. Using synthetic data, we conduct comprehensive experiments to demonstrate the robustness of our approach to perturbations in causality, the scalability to the size of communication, number of clients, and the dimensions of raw data. We also evaluate the performance on two real-world industrial control system datasets by reporting the volume of data saved by decentralization.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unified Attribution in Explainable AI, Data-Centric AI, and Mechanistic Interpretability</title>
<link>https://arxiv.org/abs/2501.18887</link>
<guid>https://arxiv.org/abs/2501.18887</guid>
<content:encoded><![CDATA[
arXiv:2501.18887v3 Announce Type: replace 
Abstract: The increasing complexity of AI systems has made understanding their behavior critical. Numerous interpretability methods have been developed to attribute model behavior to three key aspects: input features, training data, and internal model components, which emerged from explainable AI, data-centric AI, and mechanistic interpretability, respectively. However, these attribution methods are studied and applied rather independently, resulting in a fragmented landscape of methods and terminology. This position paper argues that feature, data, and component attribution methods share fundamental similarities, and a unified view of them benefits both interpretability and broader AI research. To this end, we first analyze popular methods for these three types of attributions and present a unified view demonstrating that these seemingly distinct methods employ similar techniques (such as perturbations, gradients, and linear approximations) over different aspects and thus differ primarily in their perspectives rather than techniques. Then, we demonstrate how this unified view enhances understanding of existing attribution methods, highlights shared concepts and evaluation criteria among these methods, and leads to new research directions both in interpretability research, by addressing common challenges and facilitating cross-attribution innovation, and in AI more broadly, with applications in model editing, steering, and regulation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Norm-Bounded Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2501.19050</link>
<guid>https://arxiv.org/abs/2501.19050</guid>
<content:encoded><![CDATA[
arXiv:2501.19050v3 Announce Type: replace 
Abstract: In this work, we propose norm-bounded low-rank adaptation (NB-LoRA) for parameter-efficient fine tuning. NB-LoRA is a novel parameterization of low-rank weight adaptations that admits explicit bounds on each singular value of the adaptation matrix, which can thereby satisfy any prescribed unitarily invariant norm bound, including the Schatten norms (e.g., nuclear, Frobenius, spectral norm). The proposed parameterization is unconstrained, smooth, and complete, i.e. it covers all matrices satisfying the prescribed rank and singular-value bounds. Comparative experiments on large language models show that NB-LoRA achieves superior adaptation performance and faster training over a range of models, tasks and ranks. Vision fine-tuning experiments show that NB-LoRA can achieve strong adaptation performance while avoiding model catastrophic forgetting, and compared to existing approaches it is substantially more robust to a hyper-parameters such as including adaptation rank, learning rate and number of training epochs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Learning for Combinatorial Multi-armed Bandits</title>
<link>https://arxiv.org/abs/2501.19300</link>
<guid>https://arxiv.org/abs/2501.19300</guid>
<content:encoded><![CDATA[
arXiv:2501.19300v2 Announce Type: replace 
Abstract: The combinatorial multi-armed bandit (CMAB) is a fundamental sequential decision-making framework, extensively studied over the past decade. However, existing work primarily focuses on the online setting, overlooking the substantial costs of online interactions and the readily available offline datasets. To overcome these limitations, we introduce Off-CMAB, the first offline learning framework for CMAB. Central to our framework is the combinatorial lower confidence bound (CLCB) algorithm, which combines pessimistic reward estimations with combinatorial solvers. To characterize the quality of offline datasets, we propose two novel data coverage conditions and prove that, under these conditions, CLCB achieves a near-optimal suboptimality gap, matching the theoretical lower bound up to a logarithmic factor. We validate Off-CMAB through practical applications, including learning to rank, large language model (LLM) caching, and social influence maximization, showing its ability to handle nonlinear reward functions, general feedback models, and out-of-distribution action samples that excludes optimal or even feasible actions. Extensive experiments on synthetic and real-world datasets further highlight the superior performance of CLCB.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion</title>
<link>https://arxiv.org/abs/2502.00264</link>
<guid>https://arxiv.org/abs/2502.00264</guid>
<content:encoded><![CDATA[
arXiv:2502.00264v2 Announce Type: replace 
Abstract: Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion. Our code is available on https://github.com/zhengzaiyi/RotationSymmetry.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Suboptimal Data in Continuous Control via Auto-Regressive Soft Q-Network</title>
<link>https://arxiv.org/abs/2502.00288</link>
<guid>https://arxiv.org/abs/2502.00288</guid>
<content:encoded><![CDATA[
arXiv:2502.00288v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) for continuous control often requires large amounts of online interaction data. Value-based RL methods can mitigate this burden by offering relatively high sample efficiency. Some studies further enhance sample efficiency by incorporating offline demonstration data to "kick-start" training, achieving promising results in continuous control. However, they typically compute the Q-function independently for each action dimension, neglecting interdependencies and making it harder to identify optimal actions when learning from suboptimal data, such as non-expert demonstration and online-collected data during the training process. To address these issues, we propose Auto-Regressive Soft Q-learning (ARSQ), a value-based RL algorithm that models Q-values in a coarse-to-fine, auto-regressive manner. First, ARSQ decomposes the continuous action space into discrete spaces in a coarse-to-fine hierarchy, enhancing sample efficiency for fine-grained continuous control tasks. Next, it auto-regressively predicts dimensional action advantages within each decision step, enabling more effective decision-making in continuous control tasks. We evaluate ARSQ on two continuous control benchmarks, RLBench and D4RL, integrating demonstration data into online training. On D4RL, which includes non-expert demonstrations, ARSQ achieves an average $1.62\times$ performance improvement over SOTA value-based baseline. On RLBench, which incorporates expert demonstrations, ARSQ surpasses various baselines, demonstrating its effectiveness in learning from suboptimal online-collected data. Project page is at https://sites.google.com/view/ar-soft-q
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn Singularly Perturbed Solutions via Homotopy Dynamics</title>
<link>https://arxiv.org/abs/2502.00488</link>
<guid>https://arxiv.org/abs/2502.00488</guid>
<content:encoded><![CDATA[
arXiv:2502.00488v3 Announce Type: replace 
Abstract: Solving partial differential equations (PDEs) using neural networks has become a central focus in scientific machine learning. Training neural networks for singularly perturbed problems is particularly challenging due to certain parameters in the PDEs that introduce near-singularities in the loss function. In this study, we overcome this challenge by introducing a novel method based on homotopy dynamics to effectively manipulate these parameters. From a theoretical perspective, we analyze the effects of these parameters on training difficulty in these singularly perturbed problems and establish the convergence of the proposed homotopy dynamics method. Experimentally, we demonstrate that our approach significantly accelerates convergence and improves the accuracy of these singularly perturbed problems. These findings present an efficient optimization strategy leveraging homotopy dynamics, offering a robust framework to extend the applicability of neural networks for solving singularly perturbed differential equations.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Statistical Learning Perspective on Semi-dual Adversarial Neural Optimal Transport Solvers</title>
<link>https://arxiv.org/abs/2502.01310</link>
<guid>https://arxiv.org/abs/2502.01310</guid>
<content:encoded><![CDATA[
arXiv:2502.01310v2 Announce Type: replace 
Abstract: Neural network-based optimal transport (OT) is a recent and fruitful direction in the generative modeling community. It finds its applications in various fields such as domain translation, image super-resolution, computational biology and others. Among the existing OT approaches, of considerable interest are adversarial minimax solvers based on semi-dual formulations of OT problems. While promising, these methods lack theoretical investigation from a statistical learning perspective. Our work fills this gap by establishing upper bounds on the generalization error of an approximate OT map recovered by the minimax quadratic OT solver. Importantly, the bounds we derive depend solely on some standard statistical and mathematical properties of the considered functional classes (neural nets). While our analysis focuses on the quadratic OT, we believe that similar bounds could be derived for general OT case, paving the promising direction for future research.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Inverse Attention Network with Intrinsic Discriminant Feature Exploitation for Fake News Detection</title>
<link>https://arxiv.org/abs/2502.01699</link>
<guid>https://arxiv.org/abs/2502.01699</guid>
<content:encoded><![CDATA[
arXiv:2502.01699v2 Announce Type: replace 
Abstract: Multimodal fake news detection has garnered significant attention due to its profound implications for social security. While existing approaches have contributed to understanding cross-modal consistency, they often fail to leverage modal-specific representations and explicit discrepant features. To address these limitations, we propose a Multimodal Inverse Attention Network (MIAN), a novel framework that explores intrinsic discriminative features based on news content to advance fake news detection. Specifically, MIAN introduces a hierarchical learning module that captures diverse intra-modal relationships through local-to-global and local-to-local interactions, thereby generating enhanced unimodal representations to improve the identification of fake news at the intra-modal level. Additionally, a cross-modal interaction module employs a co-attention mechanism to establish and model dependencies between the refined unimodal representations, facilitating seamless semantic integration across modalities. To explicitly extract inconsistency features, we propose an inverse attention mechanism that effectively highlights the conflicting patterns and semantic deviations introduced by fake news in both intra- and inter-modality. Extensive experiments on benchmark datasets demonstrate that MIAN significantly outperforms state-of-the-art methods, underscoring its pivotal contribution to advancing social security through enhanced multimodal fake news detection.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting</title>
<link>https://arxiv.org/abs/2502.02410</link>
<guid>https://arxiv.org/abs/2502.02410</guid>
<content:encoded><![CDATA[
arXiv:2502.02410v2 Announce Type: replace 
Abstract: Many forms of sensitive data, such as web traffic, mobility data, or hospital occupancy, are inherently sequential. The standard method for training machine learning models while ensuring privacy for units of sensitive information, such as individual hospital visits, is differentially private stochastic gradient descent (DP-SGD). However, we observe in this work that the formal guarantees of DP-SGD are incompatible with time-series-specific tasks like forecasting, since they rely on the privacy amplification attained by training on small, unstructured batches sampled from an unstructured dataset. In contrast, batches for forecasting are generated by (1) sampling sequentially structured time series from a dataset, (2) sampling contiguous subsequences from these series, and (3) partitioning them into context and ground-truth forecast windows. We theoretically analyze the privacy amplification attained by this structured subsampling to enable the training of forecasting models with sound and tight event- and user-level privacy guarantees. Towards more private models, we additionally prove how data augmentation amplifies privacy in self-supervised training of sequence models. Our empirical evaluation demonstrates that amplification by structured subsampling enables the training of forecasting models with strong formal privacy guarantees.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Exploration for Multi-Reward Multi-Policy Evaluation</title>
<link>https://arxiv.org/abs/2502.02516</link>
<guid>https://arxiv.org/abs/2502.02516</guid>
<content:encoded><![CDATA[
arXiv:2502.02516v2 Announce Type: replace 
Abstract: We study the policy evaluation problem in an online multi-reward multi-policy discounted setting, where multiple reward functions must be evaluated simultaneously for different policies. We adopt an $(\epsilon,\delta)$-PAC perspective to achieve $\epsilon$-accurate estimates with high confidence across finite or convex sets of rewards, a setting that has not been investigated in the literature. Building on prior work on Multi-Reward Best Policy Identification, we adapt the MR-NaS exploration scheme to jointly minimize sample complexity for evaluating different policies across different reward sets. Our approach leverages an instance-specific lower bound revealing how the sample complexity scales with a measure of value deviation, guiding the design of an efficient exploration policy. Although computing this bound entails a hard non-convex optimization, we propose an efficient convex approximation that holds for both finite and convex reward sets. Experiments in tabular domains demonstrate the effectiveness of this adaptive exploration scheme.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedRAX: Medical Reasoning Agent for Chest X-ray</title>
<link>https://arxiv.org/abs/2502.02673</link>
<guid>https://arxiv.org/abs/2502.02673</guid>
<content:encoded><![CDATA[
arXiv:2502.02673v2 Announce Type: replace 
Abstract: Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice. We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art CXR analysis tools and multimodal large language models into a unified framework. MedRAX dynamically leverages these models to address complex medical queries without requiring additional training. To rigorously evaluate its capabilities, we introduce ChestAgentBench, a comprehensive benchmark containing 2,500 complex medical queries across 7 diverse categories. Our experiments demonstrate that MedRAX achieves state-of-the-art performance compared to both open-source and proprietary models, representing a significant step toward the practical deployment of automated CXR interpretation systems. Data and code have been publicly available at https://github.com/bowang-lab/MedRAX
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.04778</link>
<guid>https://arxiv.org/abs/2502.04778</guid>
<content:encoded><![CDATA[
arXiv:2502.04778v2 Announce Type: replace 
Abstract: Behavior regularization, which constrains the policy to stay close to some behavior policy, is widely used in offline reinforcement learning (RL) to manage the risk of hazardous exploitation of unseen actions. Nevertheless, existing literature on behavior-regularized RL primarily focuses on explicit policy parameterizations, such as Gaussian policies. Consequently, it remains unclear how to extend this framework to more advanced policy parameterizations, such as diffusion models. In this paper, we introduce BDPO, a principled behavior-regularized RL framework tailored for diffusion-based policies, thereby combining the expressive power of diffusion policies and the robustness provided by regularization. The key ingredient of our method is to calculate the Kullback-Leibler (KL) regularization analytically as the accumulated discrepancies in reverse-time transition kernels along the diffusion trajectory. By integrating the regularization, we develop an efficient two-time-scale actor-critic RL algorithm that produces the optimal policy while respecting the behavior constraint. Comprehensive evaluations conducted on synthetic 2D tasks and continuous control tasks from the D4RL benchmark validate its effectiveness and superior performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Representation Dynamics of Diffusion Models via Low-Dimensional Modeling</title>
<link>https://arxiv.org/abs/2502.05743</link>
<guid>https://arxiv.org/abs/2502.05743</guid>
<content:encoded><![CDATA[
arXiv:2502.05743v2 Announce Type: replace 
Abstract: Diffusion models, though originally designed for generative tasks, have demonstrated impressive self-supervised representation learning capabilities. A particularly intriguing phenomenon in these models is the emergence of unimodal representation dynamics, where the quality of learned features peaks at an intermediate noise level. In this work, we conduct a comprehensive theoretical and empirical investigation of this phenomenon. Leveraging the inherent low-dimensionality structure of image data, we theoretically demonstrate that the unimodal dynamic emerges when the diffusion model successfully captures the underlying data distribution. The unimodality arises from an interplay between denoising strength and class confidence across noise scales. Empirically, we further show that, in classification tasks, the presence of unimodal dynamics reliably indicates generalization: it emerges when the model generalizes and gradually transitions to a monotonically decreasing curve as the model begins to memorize the training data.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Sequence Preconditioning</title>
<link>https://arxiv.org/abs/2502.06545</link>
<guid>https://arxiv.org/abs/2502.06545</guid>
<content:encoded><![CDATA[
arXiv:2502.06545v2 Announce Type: replace 
Abstract: We study the problem of preconditioning in the setting of sequential prediction. From the theoretical lens of linear dynamical systems, we show that applying a convolution to the input sequence translates to applying a polynomial to the unknown transition matrix in the hidden space. With this insight, we develop a novel preconditioning method that convolves the input sequence with the coefficients of the Chebyshev or Legendre polynomials. We formally prove that this improves the regret of two distinct prediction methods. Moreover, using this preconditioning technique on either method gives the first sublinear regret bounds that are also hidden dimension independent (up to logarithmic factors) even when the hidden transition matrix is asymmetric. From rigorous experiments on synthetic data we show that our simple preconditioning method generalizes to both 1) settings where the data is not from a linear dynamical system and 2) a broad range of learning algorithms, including recurrent neural networks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effects of Dropout on Performance in Long-range Graph Learning Tasks</title>
<link>https://arxiv.org/abs/2502.07364</link>
<guid>https://arxiv.org/abs/2502.07364</guid>
<content:encoded><![CDATA[
arXiv:2502.07364v2 Announce Type: replace 
Abstract: Message Passing Neural Networks (MPNNs) are a class of Graph Neural Networks (GNNs) that propagate information across the graph via local neighborhoods. The scheme gives rise to two key challenges: over-smoothing and over-squashing. While several Dropout-style algorithms, such as DropEdge and DropMessage, have successfully addressed over-smoothing, their impact on over-squashing remains largely unexplored. This represents a critical gap in the literature, as failure to mitigate over-squashing would make these methods unsuitable for long-range tasks -- the intended use case of deep MPNNs. In this work, we study the aforementioned algorithms, and closely related edge-dropping algorithms -- DropNode, DropAgg and DropGNN -- in the context of over-squashing. We present theoretical results showing that DropEdge-variants reduce sensitivity between distant nodes, limiting their suitability for long-range tasks. To address this, we introduce DropSens, a sensitivity-aware variant of DropEdge that explicitly controls the proportion of information lost due to edge-dropping, thereby increasing sensitivity to distant nodes despite dropping the same number of edges. Our experiments on long-range synthetic and real-world datasets confirm the predicted limitations of existing edge-dropping and feature-dropping methods. Moreover, DropSens consistently outperforms graph rewiring techniques designed to mitigate over-squashing, suggesting that simple, targeted modifications can substantially improve a model's ability to capture long-range interactions. Our conclusions highlight the need to re-evaluate and re-design existing methods for training deep GNNs, with a renewed focus on modelling long-range interactions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View</title>
<link>https://arxiv.org/abs/2502.11256</link>
<guid>https://arxiv.org/abs/2502.11256</guid>
<content:encoded><![CDATA[
arXiv:2502.11256v2 Announce Type: replace 
Abstract: Large language models (LLMs) offer powerful capabilities but come with significant environmental impact, particularly in carbon emissions. Existing studies benchmark carbon emissions but lack a standardized basis for comparison across different model configurations. To address this, we introduce the concept of functional unit (FU) as a standardized basis and develop FUEL, the first FU-based framework for evaluating LLM serving's environmental impact. Through three case studies, we uncover key insights and trade-offs in reducing carbon emissions by optimizing model size, quantization strategy, and hardware choice, paving the way for more sustainable LLM serving. The code is available at https://github.com/jojacola/FUEL.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Signed Graph Approach to Understanding and Mitigating Oversmoothing in GNNs</title>
<link>https://arxiv.org/abs/2502.11394</link>
<guid>https://arxiv.org/abs/2502.11394</guid>
<content:encoded><![CDATA[
arXiv:2502.11394v2 Announce Type: replace 
Abstract: Deep graph neural networks (GNNs) often suffer from oversmoothing, where node representations become overly homogeneous with increasing depth. While techniques like normalization, residual connections, and edge dropout have been proposed to mitigate oversmoothing, they are typically developed independently, with limited theoretical understanding of their underlying mechanisms. In this work, we present a unified theoretical perspective based on the framework of signed graphs, showing that many existing strategies implicitly introduce negative edges that alter message-passing to resist oversmoothing. However, we show that merely adding negative edges in an unstructured manner is insufficient-the asymptotic behavior of signed propagation depends critically on the strength and organization of positive and negative edges. To address this limitation, we leverage the theory of structural balance, which promotes stable, cluster-preserving dynamics by connecting similar nodes with positive edges and dissimilar ones with negative edges. We propose Structural Balanced Propagation (SBP), a plug-and-play method that assigns signed edges based on either labels or feature similarity to explicitly enhance structural balance in the constructed signed graphs. Experiments on nine benchmarks across both homophilic and heterophilic settings demonstrate that SBP consistently improves classification accuracy and mitigates oversmoothing, even at depths of up to 300 layers. Our results provide a principled explanation for prior oversmoothing remedies and introduce a new direction for signed message-passing design in deep GNNs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Semi-supervised Learning with Zero-shot Pseudolabels</title>
<link>https://arxiv.org/abs/2502.12584</link>
<guid>https://arxiv.org/abs/2502.12584</guid>
<content:encoded><![CDATA[
arXiv:2502.12584v2 Announce Type: replace 
Abstract: The high cost of data labeling presents a major barrier to deploying machine learning systems at scale. Semi-supervised learning (SSL) mitigates this challenge by utilizing unlabeled data alongside limited labeled examples, while the emergence of foundation models (FMs) offers powerful zero-shot capabilities that can further reduce labeling cost. However, directly fine-tuning large FMs is often impractical in resource-constrained settings, and na\"ively using their pseudo-labels for unlabeled data can degrade performance due to its unreliablity or domain mismatch with target task. In this work, we introduce ZeroMatch, a novel SSL framework that integrates knowledge distillation with consistency-based learning to jointly leverage labeled data, unlabeled data, and pseudo-labels from FMs. ZeroMatch enables training compact student models using only FM inference, making it suitable for low-resource environments such as personal devices with limited compute. Experiments on six vision and language classification benchmarks show that ZeroMatch consistently outperforms standard SSL and zero-shot augmented methods, demonstrating its effectiveness and robustness across a range of foundation model qualities.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning</title>
<link>https://arxiv.org/abs/2502.12913</link>
<guid>https://arxiv.org/abs/2502.12913</guid>
<content:encoded><![CDATA[
arXiv:2502.12913v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) fine-tuning technologies have achieved remarkable results. However, traditional LLM fine-tuning approaches face significant challenges: they require large Floating Point (FP) computation, raising privacy concerns when handling sensitive data, and are impractical for resource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT) techniques reduce trainable parameters, their reliance on floating-point arithmetic creates fundamental incompatibilities with edge hardware. In this work, we introduce a novel framework for on-device LLM fine-tuning that eliminates the need for floating-point operations in both inference and training, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer format, which efficiently represents model parameters in integer format using shared exponents among parameter groups. When combined with LoRA-like adapters, this enables fully integer-based fine-tuning that is both memory and compute efficient. We demonstrate that our approach achieves accuracy comparable to BF16-based fine-tuning while significantly reducing 1.85x memory usage. Moreover, compared to FP8, our method can reduce 5x power consumption and 11x chip area with same performance, making large-scale model adaptation feasible on edge devices.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction</title>
<link>https://arxiv.org/abs/2502.13344</link>
<guid>https://arxiv.org/abs/2502.13344</guid>
<content:encoded><![CDATA[
arXiv:2502.13344v3 Announce Type: replace 
Abstract: Biomedical knowledge graphs (KGs) encode rich, structured information critical for drug discovery tasks, but extracting meaningful insights from large-scale KGs remains challenging due to their complex structure. Existing biomedical subgraph retrieval methods are tailored for graph neural networks (GNNs), limiting compatibility with other paradigms, including large language models (LLMs). We introduce K-Paths, a model-agnostic retrieval framework that extracts structured, diverse, and biologically meaningful multi-hop paths from dense biomedical KGs. These paths enable the prediction of unobserved drug-drug and drug-disease interactions, including those involving entities not seen during training, thus supporting inductive reasoning. K-Paths is training-free and employs a diversity-aware adaptation of Yen's algorithm to extract the K shortest loopless paths between entities in a query, prioritizing biologically relevant and relationally diverse connections. These paths serve as concise, interpretable reasoning chains that can be directly integrated with LLMs or GNNs to improve generalization, accuracy, and enable explainable inference. Experiments on benchmark datasets show that K-Paths improves zero-shot reasoning across state-of-the-art LLMs. For instance, Tx-Gemma 27B improves by 19.8 and 4.0 F1 points on interaction severity prediction and drug repurposing tasks, respectively. Llama 70B achieves gains of 8.5 and 6.2 points on the same tasks. K-Paths also boosts the training efficiency of EmerGNN, a state-of-the-art GNN, by reducing the KG size by 90% while maintaining predictive performance. Beyond efficiency, K-Paths bridges the gap between KGs and LLMs, enabling scalable and explainable LLM-augmented scientific discovery. We release our code and the retrieved paths as a benchmark for inductive reasoning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STeCa: Step-level Trajectory Calibration for LLM Agent Learning</title>
<link>https://arxiv.org/abs/2502.14276</link>
<guid>https://arxiv.org/abs/2502.14276</guid>
<content:encoded><![CDATA[
arXiv:2502.14276v2 Announce Type: replace 
Abstract: Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations or preference learning through exploratory trajectory sampling. However, these methods often struggle to address long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. We finally leverage these calibrated trajectories with successful trajectories for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that timely calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperspherical Normalization for Scalable Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.15280</link>
<guid>https://arxiv.org/abs/2502.15280</guid>
<content:encoded><![CDATA[
arXiv:2502.15280v2 Announce Type: replace 
Abstract: Scaling up the model size and computation has brought consistent performance improvements in supervised learning. However, this lesson often fails to apply to reinforcement learning (RL) because training the model on non-stationary data easily leads to overfitting and unstable optimization. In response, we introduce SimbaV2, a novel RL architecture designed to stabilize optimization by (i) constraining the growth of weight and feature norm by hyperspherical normalization; and (ii) using a distributional value estimation with reward scaling to maintain stable gradients under varying reward magnitudes. Using the soft actor-critic as a base algorithm, SimbaV2 scales up effectively with larger models and greater compute, achieving state-of-the-art performance on 57 continuous control tasks across 4 domains. The code is available at https://dojeon-ai.github.io/SimbaV2.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason from Feedback at Test-Time</title>
<link>https://arxiv.org/abs/2502.15771</link>
<guid>https://arxiv.org/abs/2502.15771</guid>
<content:encoded><![CDATA[
arXiv:2502.15771v2 Announce Type: replace 
Abstract: Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, OpTune, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and OpTune achieve superior scalability and performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Aware Joint DNN Model Deployment and Partitioning Optimization for Collaborative Edge Inference Services</title>
<link>https://arxiv.org/abs/2502.16091</link>
<guid>https://arxiv.org/abs/2502.16091</guid>
<content:encoded><![CDATA[
arXiv:2502.16091v3 Announce Type: replace 
Abstract: Edge inference (EI) has emerged as a promising paradigm to address the growing limitations of cloud-based Deep Neural Network (DNN) inference services, such as high response latency, limited scalability, and severe data privacy exposure. However, deploying DNN models on resource-constrained edge devices introduces additional challenges, including limited computation/storage resources, dynamic service demands, and heightened privacy risks. To tackle these issues, this paper presents a novel privacy-aware optimization framework that jointly addresses DNN model deployment, user-server association, and model partitioning, with the goal of minimizing long-term average inference delay under resource and privacy constraints. The problem is formulated as a complex, NP-hard stochastic optimization. To efficiently handle system dynamics and computational complexity, we employ a Lyapunov-based approach to transform the long-term objective into tractable per-slot decisions. Furthermore, we introduce a coalition formation game to enable adaptive user-server association and design a greedy algorithm for model deployment within each coalition. Extensive simulations demonstrate that the proposed algorithm significantly reduces inference delay and consistently satisfies privacy constraints, outperforming state-of-the-art baselines across diverse scenarios.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Distribution Shifts For Machine Learning Force Fields</title>
<link>https://arxiv.org/abs/2503.08674</link>
<guid>https://arxiv.org/abs/2503.08674</guid>
<content:encoded><![CDATA[
arXiv:2503.08674v2 Announce Type: replace 
Abstract: Machine Learning Force Fields (MLFFs) are a promising alternative to expensive ab initio quantum mechanical molecular simulations. Given the diversity of chemical spaces that are of interest and the cost of generating new data, it is important to understand how MLFFs generalize beyond their training distributions. In order to characterize and better understand distribution shifts in MLFFs, we conduct diagnostic experiments on chemical datasets, revealing common shifts that pose significant challenges, even for large foundation models trained on extensive data. Based on these observations, we hypothesize that current supervised training methods inadequately regularize MLFFs, resulting in overfitting and learning poor representations of out-of-distribution systems. We then propose two new methods as initial steps for mitigating distribution shifts for MLFFs. Our methods focus on test-time refinement strategies that incur minimal computational cost and do not use expensive ab initio reference labels. The first strategy, based on spectral graph theory, modifies the edges of test graphs to align with graph structures seen during training. Our second strategy improves representations for out-of-distribution systems at test-time by taking gradient steps using an auxiliary objective, such as a cheap physical prior. Our test-time refinement strategies significantly reduce errors on out-of-distribution systems, suggesting that MLFFs are capable of and can move towards modeling diverse chemical spaces, but are not being effectively trained to do so. Our experiments establish clear benchmarks for evaluating the generalization capabilities of the next generation of MLFFs. Our code is available at https://tkreiman.github.io/projects/mlff_distribution_shifts/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers?</title>
<link>https://arxiv.org/abs/2503.10632</link>
<guid>https://arxiv.org/abs/2503.10632</guid>
<content:encoded><![CDATA[
arXiv:2503.10632v2 Announce Type: replace 
Abstract: Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep networks, including advanced architectures such as vision Transformers (ViTs). This work asks whether a similar replacement in the attention can bring benefits. In this paper, we design the first learnable attention called Kolmogorov-Arnold Attention (KArAt) for ViTs that can operate on any basis, ranging from Fourier, Wavelets, Splines, to Rational Functions. However, learnable activations in attention cause a memory explosion. To remedy this, we propose a modular version of KArAt that uses a low-rank approximation. By adopting the Fourier basis, Fourier-KArAt and its variants, in some cases, outperform their traditional softmax counterparts, or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We also deploy Fourier KArAt to ConViT and Swin-Transformer, and use it in detection and segmentation with ViT-Det. We dissect these architectures' performance by analyzing their loss landscapes, weight distributions, optimizer path, attention visualization, and transferability to other datasets. KArAt's learnable activation shows a better attention score across all ViTs, indicating better token-to-token interactions, contributing to better inference. Still, its generalizability does not scale with larger ViTs. However, many factors, including the present computing interface, affect the performance of parameter- and memory-heavy KArAts. We note that the goal of this paper is not to produce efficient attention or challenge the traditional activations; by designing KArAt, we are the first to show that attention can be learned and encourage researchers to explore KArAt in conjunction with more advanced architectures.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-regularized Gradient Estimators for Approximate Bayesian Inference</title>
<link>https://arxiv.org/abs/2503.11964</link>
<guid>https://arxiv.org/abs/2503.11964</guid>
<content:encoded><![CDATA[
arXiv:2503.11964v3 Announce Type: replace 
Abstract: Effective uncertainty quantification is important for training modern predictive models with limited data, enhancing both accuracy and robustness. While Bayesian methods are effective for this purpose, they can be challenging to scale. When employing approximate Bayesian inference, ensuring the quality of samples from the posterior distribution in a computationally efficient manner is essential. This paper addresses the estimation of the Bayesian posterior to generate diverse samples by approximating the gradient flow of the Kullback-Leibler (KL) divergence and the cross entropy of the target approximation under the metric induced by the Stein Operator. It presents empirical evaluations on classification tasks to assess the method's performance and discuss its effectiveness for Model-Based Reinforcement Learning that uses uncertainty-aware network dynamics models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation</title>
<link>https://arxiv.org/abs/2503.12600</link>
<guid>https://arxiv.org/abs/2503.12600</guid>
<content:encoded><![CDATA[
arXiv:2503.12600v2 Announce Type: replace 
Abstract: The powerful capabilities of Large Language Models (LLMs) have led to their growing use in evaluating human-generated content, particularly in evaluating research ideas within academic settings. Existing solutions primarily rely on prompt-based LLM methods or fine-tuned lightweight language models for idea evaluation. However, these methods are often unstable and struggle to comprehend the complex semantic information embedded in the ideas, impeding their ability to perform high-quality evaluations. To address the above challenges, we propose GraphEval, a lightweight graph-based LLM framework for idea evaluation. Our insight is that a complex idea can be broken down into comprehensible viewpoint nodes using prompts from small LLMs. These viewpoint nodes can then be linked together through edges created from LLM-based relation extraction and/or BERT similarity scores. The created viewpoint-graph can be used to conveniently propagate scores across view-nodes to improve the robustness of the idea evaluations. In particular, we propose two lightweight graph-based methods for idea evaluation: (1) GraphEval-LP: a training-free label propagation algorithm that propagates evaluation scores from known view-nodes to unknown nodes; (2) GraphEval-GNN: a Graph Neural Networks (GNN) that is trained to predict the evaluation scores given the observed graph with minimal computation resources. Moreover, to overcome LLM's limitation in objectively assessing the novelty of ideas, we further propose a novelty detection model to GraphEval-GNN to enhance its capability in judging idea novelty. Experiments on two datasets show GraphEval improves F1 scores by at least 14% with low computation and API costs. Additionally, GraphEval can effectively detect plagiarized ideas.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Bounds for Adversarial Constrained Online Convex Optimization</title>
<link>https://arxiv.org/abs/2503.13366</link>
<guid>https://arxiv.org/abs/2503.13366</guid>
<content:encoded><![CDATA[
arXiv:2503.13366v4 Announce Type: replace 
Abstract: Constrained Online Convex Optimization (COCO) can be seen as a generalization of the standard Online Convex Optimization (OCO) framework. At each round, a cost function and constraint function are revealed after a learner chooses an action. The goal is to minimize both the regret and cumulative constraint violation (CCV) against an adaptive adversary. We show for the first time that is possible to obtain the optimal $O(\sqrt{T})$ bound on both regret and CCV, improving the best known bounds of $O \left( \sqrt{T} \right)$ and $\tilde{O} \left( \sqrt{T} \right)$ for the regret and CCV, respectively. Based on a new surrogate loss function enforcing a minimum penalty on the constraint function, we demonstrate that both the Follow-the-Regularized-Leader and the Online Gradient Descent achieve the optimal bounds.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparseformer: a Transferable Transformer with Multi-granularity Token Sparsification for Medical Time Series Classification</title>
<link>https://arxiv.org/abs/2503.15578</link>
<guid>https://arxiv.org/abs/2503.15578</guid>
<content:encoded><![CDATA[
arXiv:2503.15578v2 Announce Type: replace 
Abstract: Medical time series (MedTS) classification is crucial for improved diagnosis in healthcare, and yet it is challenging due to the varying granularity of patterns, intricate inter-channel correlation, information redundancy, and label scarcity. While existing transformer-based models have shown promise in time series analysis, they mainly focus on forecasting and fail to fully exploit the distinctive characteristics of MedTS data. In this paper, we introduce Sparseformer, a transformer specifically designed for MedTS classification. We propose a sparse token-based dual-attention mechanism that enables global modeling and token compression, allowing dynamic focus on the most informative tokens while distilling redundant features. This mechanism is then applied to the multi-granularity, cross-channel encoding of medical signals, capturing intra- and inter-granularity correlations and inter-channel connections. The sparsification design allows our model to handle heterogeneous inputs of varying lengths and channels directly. Further, we introduce an adaptive label encoder to address label space misalignment across datasets, equipping our model with cross-dataset transferability to alleviate the medical label scarcity issue. Our model outperforms 12 baselines across seven medical datasets under supervised learning. In the few-shot learning experiments, our model also achieves superior average results. In addition, the in-domain and cross-domain experiments among three diagnostic scenarios demonstrate our model's zero-shot learning capability. Collectively, these findings underscore the robustness and transferability of our model in various medical applications.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Bias Reinforcement in LLM Agents Debate</title>
<link>https://arxiv.org/abs/2503.16814</link>
<guid>https://arxiv.org/abs/2503.16814</guid>
<content:encoded><![CDATA[
arXiv:2503.16814v2 Announce Type: replace 
Abstract: Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD's limitations, we propose $\textbf{DReaMAD}$ $($$\textbf{D}$iverse $\textbf{Rea}$soning via $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty</title>
<link>https://arxiv.org/abs/2503.18314</link>
<guid>https://arxiv.org/abs/2503.18314</guid>
<content:encoded><![CDATA[
arXiv:2503.18314v4 Announce Type: replace 
Abstract: We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the influence of training samples from pre-trained models, avoiding retraining from scratch. LoTUS smooths the prediction probabilities of the model up to an information-theoretic bound, mitigating its over-confidence stemming from data memorization. We evaluate LoTUS on Transformer and ResNet18 models against eight baselines across five public datasets. Beyond established MU benchmarks, we evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining is impractical, simulating real-world conditions. Moreover, we introduce the novel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable evaluation under real-world conditions. The experimental results show that LoTUS outperforms state-of-the-art methods in terms of both efficiency and effectiveness. Code: https://github.com/cspartalis/LoTUS.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Domain Generalization Benchmarks with Accuracy on the Line Misspecified?</title>
<link>https://arxiv.org/abs/2504.00186</link>
<guid>https://arxiv.org/abs/2504.00186</guid>
<content:encoded><![CDATA[
arXiv:2504.00186v2 Announce Type: replace 
Abstract: Spurious correlations are unstable statistical associations that hinder robust decision-making. Conventional wisdom suggests that models relying on such correlations will fail to generalize out-of-distribution (OOD), especially under strong distribution shifts. However, empirical evidence challenges this view as naive in-distribution empirical risk minimizers often achieve the best OOD accuracy across popular OOD generalization benchmarks. In light of these results, we propose a different perspective: many widely used benchmarks for evaluating robustness to spurious correlations are misspecified. Specifically, they fail to include shifts in spurious correlations that meaningfully impact OOD generalization, making them unsuitable for evaluating the benefit of removing such correlations. We establish conditions under which a distribution shift can reliably assess a model's reliance on spurious correlations. Crucially, under these conditions, we should not observe a strong positive correlation between in-distribution and OOD accuracy, often called "accuracy on the line." Yet, most state-of-the-art benchmarks exhibit this pattern, suggesting they do not effectively assess robustness. Our findings expose a key limitation in current benchmarks used to evaluate domain generalization algorithms, that is, models designed to avoid spurious correlations. We highlight the need to rethink how robustness to spurious correlations is assessed, identify well-specified benchmarks the field should prioritize, and enumerate strategies for designing future benchmarks that meaningfully reflect robustness under distribution shift.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2504.02298</link>
<guid>https://arxiv.org/abs/2504.02298</guid>
<content:encoded><![CDATA[
arXiv:2504.02298v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs), as a biologically plausible alternative to Artificial Neural Networks (ANNs), have demonstrated advantages in terms of energy efficiency, temporal processing, and biological plausibility. However, SNNs are highly sensitive to distribution shifts, which can significantly degrade their performance in real-world scenarios. Traditional test-time adaptation (TTA) methods designed for ANNs often fail to address the unique computational dynamics of SNNs, such as sparsity and temporal spiking behavior. To address these challenges, we propose SPike-Aware Consistency Enhancement (SPACE), the first source-free and single-instance TTA method specifically designed for SNNs. SPACE leverages the inherent spike dynamics of SNNs to maximize the consistency of spike-behavior-based local feature maps across augmented versions of a single test sample, enabling robust adaptation without requiring source data. We evaluate SPACE on multiple datasets. Furthermore, SPACE exhibits robust generalization across diverse network architectures, consistently enhancing the performance of SNNs on CNNs (such as VGG and ResNet), Transformer models, and ConvLSTM architectures. Experimental results show that SPACE outperforms state-of-the-art methods, highlighting its effectiveness and robustness in real-world settings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BECAME: BayEsian Continual Learning with Adaptive Model MErging</title>
<link>https://arxiv.org/abs/2504.02666</link>
<guid>https://arxiv.org/abs/2504.02666</guid>
<content:encoded><![CDATA[
arXiv:2504.02666v2 Announce Type: replace 
Abstract: Continual Learning (CL) strives to learn incrementally across tasks while mitigating catastrophic forgetting. A key challenge in CL is balancing stability (retaining prior knowledge) and plasticity (learning new tasks). While representative gradient projection methods ensure stability, they often limit plasticity. Model merging techniques offer promising solutions, but prior methods typically rely on empirical assumptions and carefully selected hyperparameters. In this paper, we explore the potential of model merging to enhance the stability-plasticity trade-off, providing theoretical insights that underscore its benefits. Specifically, we reformulate the merging mechanism using Bayesian continual learning principles and derive a closed-form solution for the optimal merging coefficient that adapts to the diverse characteristics of tasks. To validate our approach, we introduce a two-stage framework named BECAME, which synergizes the expertise of gradient projection and adaptive merging. Extensive experiments show that our approach outperforms state-of-the-art CL methods and existing merging strategies.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism</title>
<link>https://arxiv.org/abs/2504.11558</link>
<guid>https://arxiv.org/abs/2504.11558</guid>
<content:encoded><![CDATA[
arXiv:2504.11558v2 Announce Type: replace 
Abstract: We introduce Error Broadcast and Decorrelation (EBD), a novel learning framework for neural networks that addresses credit assignment by directly broadcasting output errors to individual layers, circumventing weight transport of backpropagation. EBD is rigorously grounded in the stochastic orthogonality property of Minimum Mean Square Error estimators. This fundamental principle states that the error of an optimal estimator is orthogonal to functions of the input. Guided by this insight, EBD defines layerwise loss functions that directly penalize correlations between layer activations and output errors, thereby establishing a principled foundation for error broadcasting. This theoretically sound mechanism naturally leads to the experimentally observed three-factor learning rule and integrates with biologically plausible frameworks to enhance performance and plausibility. Numerical experiments demonstrate EBD's competitive or better performance against other error-broadcast methods on benchmark datasets. Our findings establish EBD as an efficient, biologically plausible, and principled alternative for neural network training.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization</title>
<link>https://arxiv.org/abs/2504.14762</link>
<guid>https://arxiv.org/abs/2504.14762</guid>
<content:encoded><![CDATA[
arXiv:2504.14762v2 Announce Type: replace 
Abstract: We propose a combinatorial and graph-theoretic theory of dropout by modeling training as a random walk over a high-dimensional graph of binary subnetworks. Each node represents a masked version of the network, and dropout induces stochastic traversal across this space. We define a subnetwork contribution score that quantifies generalization and show that it varies smoothly over the graph. Using tools from spectral graph theory, PAC-Bayes analysis, and combinatorics, we prove that generalizing subnetworks form large, connected, low-resistance clusters, and that their number grows exponentially with network width. This reveals dropout as a mechanism for sampling from a robust, structured ensemble of well-generalizing subnetworks with built-in redundancy. Extensive experiments validate every theoretical claim across diverse architectures. Together, our results offer a unified foundation for understanding dropout and suggest new directions for mask-guided regularization and subnetwork optimization.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason under Off-Policy Guidance</title>
<link>https://arxiv.org/abs/2504.14945</link>
<guid>https://arxiv.org/abs/2504.14945</guid>
<content:encoded><![CDATA[
arXiv:2504.14945v4 Announce Type: replace 
Abstract: Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning with verifiable rewards~(\textit{RLVR}). However, existing \textit{RLVR} approaches are inherently ``on-policy'', limiting learning to a model's own outputs and failing to acquire reasoning abilities beyond its initial capabilities. To address this issue, we introduce \textbf{LUFFY} (\textbf{L}earning to reason \textbf{U}nder o\textbf{FF}-polic\textbf{Y} guidance), a framework that augments \textit{RLVR} with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Specifically, LUFFY combines the Mixed-Policy GRPO framework, which has a theoretically guaranteed convergence rate, alongside policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Compared with previous RLVR methods, LUFFY achieves an over \textbf{+6.4} average gain across six math benchmarks and an advantage of over \textbf{+6.2} points in out-of-distribution tasks. Most significantly, we show that LUFFY successfully trains weak models in scenarios where on-policy RLVR completely fails. These results provide compelling evidence that LUFFY transcends the fundamental limitations of on-policy RLVR and demonstrates the great potential of utilizing off-policy guidance in RLVR.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias</title>
<link>https://arxiv.org/abs/2505.00926</link>
<guid>https://arxiv.org/abs/2505.00926</guid>
<content:encoded><![CDATA[
arXiv:2505.00926v3 Announce Type: replace 
Abstract: Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of large language models (LLMs). These tasks also play a crucial role in explaining the working mechanisms of transformers. In this work, we focus on two representative tasks in the category of regular language recognition, known as `even pairs' and `parity check', the aim of which is to determine whether the occurrences of certain subsequences in a given sequence are even. Our goal is to explore how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to solve these tasks by theoretically analyzing its training dynamics under gradient descent. While even pairs can be solved directly by a one-layer transformer, parity check need to be solved by integrating Chain-of-Thought (CoT), either into the inference stage of a transformer well-trained for the even pairs task, or into the training of a one-layer transformer. For both problems, our analysis shows that the joint training of attention and linear layers exhibits two distinct phases. In the first phase, the attention layer grows rapidly, mapping data sequences into separable vectors. In the second phase, the attention layer becomes stable, while the linear layer grows logarithmically and approaches in direction to a max-margin hyperplane that correctly separates the attention layer outputs into positive and negative samples, and the loss decreases at a rate of $O(1/t)$. Our experiments validate those theoretical results.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection</title>
<link>https://arxiv.org/abs/2505.03793</link>
<guid>https://arxiv.org/abs/2505.03793</guid>
<content:encoded><![CDATA[
arXiv:2505.03793v2 Announce Type: replace 
Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse downstream tasks necessitates efficient model selection, given the impracticality of fine-tuning all candidates due to computational constraints. Despite the recent advances in LLM selection, a fundamental research question largely remains nascent: how can we model the dynamic behaviors of LLMs during fine-tuning, thereby enhancing our understanding of their generalization performance across diverse downstream tasks? In this work, we propose a novel theoretical framework that provides a proper lens to assess the generalization capabilities of LLMs, thereby enabling accurate and efficient LLM selection for downstream applications. In particular, we first derive a PAC-Bayesian Generalization Bound that unveils fine-tuning dynamics of LLMs and then introduce LENSLLM, a Neural Tangent Kernel (NTK)-based Rectified Scaling Model that enables accurate performance predictions across diverse tasks while maintaining computational efficiency. Extensive empirical results on 3 large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy and reduces up to 88.5% computational cost in LLM selection, outperforming 5 state-of-the-art methods. We open-source our proposed LENSLLM model and corresponding results at LensLLM.io.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dequantified Diffusion-Schr{\"o}dinger Bridge for Density Ratio Estimation</title>
<link>https://arxiv.org/abs/2505.05034</link>
<guid>https://arxiv.org/abs/2505.05034</guid>
<content:encoded><![CDATA[
arXiv:2505.05034v3 Announce Type: replace 
Abstract: Density ratio estimation is fundamental to tasks involving f-divergences, yet existing methods often fail under significantly different distributions or inadequately overlapping supports -- the density-chasm and the support-chasm problems. Additionally, prior approaches yield divergent time scores near boundaries, leading to instability. We design $\textbf{D}^3\textbf{RE}$, a unified framework for robust, stable and efficient density ratio estimation. We propose the dequantified diffusion bridge interpolant (DDBI), which expands support coverage and stabilizes time scores via diffusion bridges and Gaussian dequantization. Building on DDBI, the proposed dequantified Schr{\"o}dinger bridge interpolant (DSBI) incorporates optimal transport to solve the Schr{\"o}dinger bridge problem, enhancing accuracy and efficiency. Our method offers uniform approximation and bounded time scores in theory, and outperforms baselines empirically in mutual information and density estimation tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Survival Modeling in the Age of Foundation Models</title>
<link>https://arxiv.org/abs/2505.07683</link>
<guid>https://arxiv.org/abs/2505.07683</guid>
<content:encoded><![CDATA[
arXiv:2505.07683v2 Announce Type: replace 
Abstract: The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference through its harmonized genomics, clinical, and image data. Prior studies have trained bespoke cancer survival prediction models from unimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive meaningful feature embeddings, agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized. Here, we investigate the feasibility of training classical, multimodal survival models over zero-shot embeddings extracted by FMs. We show the ease and additive effect of multimodal fusion, outperforming unimodal models. We demonstrate the benefit of including pathology report text and rigorously evaluate the effect of model-based text summarization and hallucination. Overall, we modernize survival modeling by leveraging FMs and information extraction from pathology reports.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model</title>
<link>https://arxiv.org/abs/2505.10438</link>
<guid>https://arxiv.org/abs/2505.10438</guid>
<content:encoded><![CDATA[
arXiv:2505.10438v2 Announce Type: replace 
Abstract: Gas turbine engines represent complex highly nonlinear dynamical systems. Deriving their physics-based models can be challenging as it requires performance characteristics, that are not always available, and one often has to make many simplifying assumptions. In this paper, the limitations of conventional experimental methods used to derive component-level and locally linear parameter-varying models are discussed and addressed by employing identification techniques based on data collected from standard engine operation under closed-loop control. The rotor dynamics were estimated using the sparse identification of nonlinear dynamics. Subsequently, the autonomous part of the dynamics was mapped into an optimally constructed Koopman eigenfunction space. The process included eigenvalue optimization using metaheuristic algorithms and temporal projection, followed by gradient-based eigenfunction identification. The resulting Koopman model was validated against an in-house reference component-level model. A globally optimal nonlinear feedback controller and a Kalman estimator were then designed in the eigenfunction space and compared to the classical and gain-scheduled proportional-integral controllers, as well as a proposed internal model control approach. The eigenmode structure allowed targeting individual modes during the optimization process, resulting in a better performance tuning. The results showed that the Koopman-based controller outperformed the other benchmark controllers in both reference tracking and disturbance rejection, under sea-level and varying flight conditions, due to its global nature.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks</title>
<link>https://arxiv.org/abs/2505.11461</link>
<guid>https://arxiv.org/abs/2505.11461</guid>
<content:encoded><![CDATA[
arXiv:2505.11461v2 Announce Type: replace 
Abstract: Multi-agent reinforcement learning (MARL) methods typically require that agents enjoy global state observability, preventing development of decentralized algorithms and limiting scalability. Recent work has shown that, under assumptions on decaying inter-agent influence, global observability can be replaced by local neighborhood observability at each agent, enabling decentralization and scalability. Real-world applications enjoying such decay properties remain underexplored, however, despite the fact that signal power decay, or signal attenuation, due to path loss is an intrinsic feature of many problems in wireless communications and radar networks. In this paper, we show that signal attenuation enables decentralization in MARL by considering the illustrative special case of performing power allocation for target detection in a radar network. To achieve this, we propose two new constrained multi-agent Markov decision process formulations of this power allocation problem, derive local neighborhood approximations for global value function and policy gradient estimates and establish corresponding error bounds, and develop decentralized saddle point policy gradient algorithms for solving the proposed problems. Our approach, though oriented towards the specific radar network problem we consider, provides a useful model for extensions to additional problems in wireless communications and radar networks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Geometry of ReLU Networks through the ReLU Transition Graph</title>
<link>https://arxiv.org/abs/2505.11692</link>
<guid>https://arxiv.org/abs/2505.11692</guid>
<content:encoded><![CDATA[
arXiv:2505.11692v2 Announce Type: replace 
Abstract: We develop a novel theoretical framework for analyzing ReLU neural networks through the lens of a combinatorial object we term the ReLU Transition Graph (RTG). In this graph, each node corresponds to a linear region induced by the network's activation patterns, and edges connect regions that differ by a single neuron flip. Building on this structure, we derive a suite of new theoretical results connecting RTG geometry to expressivity, generalization, and robustness. Our contributions include tight combinatorial bounds on RTG size and diameter, a proof of RTG connectivity, and graph-theoretic interpretations of VC-dimension. We also relate entropy and average degree of the RTG to generalization error. Each theoretical result is rigorously validated via carefully controlled experiments across varied network depths, widths, and data regimes. This work provides the first unified treatment of ReLU network structure via graph theory and opens new avenues for compression, regularization, and complexity control rooted in RTG analysis.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Networks as Universal Finite-State Machines: A Constructive Deterministic Finite Automaton Theory</title>
<link>https://arxiv.org/abs/2505.11694</link>
<guid>https://arxiv.org/abs/2505.11694</guid>
<content:encoded><![CDATA[
arXiv:2505.11694v2 Announce Type: replace 
Abstract: We present a complete theoretical and empirical framework establishing feedforward neural networks as universal finite-state machines (N-FSMs). Our results prove that finite-depth ReLU and threshold networks can exactly simulate deterministic finite automata (DFAs) by unrolling state transitions into depth-wise neural layers, with formal characterizations of required depth, width, and state compression. We demonstrate that DFA transitions are linearly separable, binary threshold activations allow exponential compression, and Myhill-Nerode equivalence classes can be embedded into continuous latent spaces while preserving separability. We also formalize the expressivity boundary: fixed-depth feedforward networks cannot recognize non-regular languages requiring unbounded memory. Unlike prior heuristic or probing-based studies, we provide constructive proofs and design explicit DFA-unrolled neural architectures that empirically validate every claim. Our results bridge deep learning, automata theory, and neural-symbolic computation, offering a rigorous blueprint for how discrete symbolic processes can be realized in continuous neural systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Machine Learning in IoT-based Engineering Problems: A Tool Comparison in the Case of Household Energy Consumption</title>
<link>https://arxiv.org/abs/2505.12147</link>
<guid>https://arxiv.org/abs/2505.12147</guid>
<content:encoded><![CDATA[
arXiv:2505.12147v2 Announce Type: replace 
Abstract: The rapid increase in computing power and the ability to store Big Data in the infrastructure has enabled predictions in a large variety of domains by Machine Learning. However, in many cases, existing Machine Learning tools are considered insufficient or incorrect since they exploit only probabilistic dependencies rather than inference logic. Causal Machine Learning methods seem to close this gap. In this paper, two prevalent tools based on Causal Machine Learning methods are compared, as well as their mathematical underpinning background. The operation of the tools is demonstrated by examining their response to 18 queries, based on the IDEAL Household Energy Dataset, published by the University of Edinburgh. First, it was important to evaluate the causal relations assumption that allowed the use of this approach; this was based on the preexisting scientific knowledge of the domain and was implemented by use of the in-built validation tools. Results were encouraging and may easily be extended to other domains.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Of Mice and Machines: A Comparison of Learning Between Real World Mice and RL Agents</title>
<link>https://arxiv.org/abs/2505.12204</link>
<guid>https://arxiv.org/abs/2505.12204</guid>
<content:encoded><![CDATA[
arXiv:2505.12204v2 Announce Type: replace 
Abstract: Recent advances in reinforcement learning (RL) have demonstrated impressive capabilities in complex decision-making tasks. This progress raises a natural question: how do these artificial systems compare to biological agents, which have been shaped by millions of years of evolution? To help answer this question, we undertake a comparative study of biological mice and RL agents in a predator-avoidance maze environment. Through this analysis, we identify a striking disparity: RL agents consistently demonstrate a lack of self-preservation instinct, readily risking ``death'' for marginal efficiency gains. These risk-taking strategies are in contrast to biological agents, which exhibit sophisticated risk-assessment and avoidance behaviors. Towards bridging this gap between the biological and artificial, we propose two novel mechanisms that encourage more naturalistic risk-avoidance behaviors in RL agents. Our approach leads to the emergence of naturalistic behaviors, including strategic environment assessment, cautious path planning, and predator avoidance patterns that closely mirror those observed in biological systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the definition and importance of interpretability in scientific machine learning</title>
<link>https://arxiv.org/abs/2505.13510</link>
<guid>https://arxiv.org/abs/2505.13510</guid>
<content:encoded><![CDATA[
arXiv:2505.13510v2 Announce Type: replace 
Abstract: Though neural networks trained on large datasets have been successfully used to describe and predict many physical phenomena, there is a sense among scientists that, unlike traditional scientific models comprising simple mathematical expressions, their findings cannot be integrated into the body of scientific knowledge. Critics of machine learning's inability to produce human-understandable relationships have converged on the concept of "interpretability" as its point of departure from more traditional forms of science. As the growing interest in interpretability has shown, researchers in the physical sciences seek not just predictive models, but also to uncover the fundamental principles that govern a system of interest. However, clarity around a definition of interpretability and the precise role that it plays in science is lacking in the literature. In this work, we argue that researchers in equation discovery and symbolic regression tend to conflate the concept of sparsity with interpretability. We review key papers on interpretable machine learning from outside the scientific community and argue that, though the definitions and methods they propose can inform questions of interpretability for scientific machine learning (SciML), they are inadequate for this new purpose. Noting these deficiencies, we propose an operational definition of interpretability for the physical sciences. Our notion of interpretability emphasizes understanding of the mechanism over mathematical sparsity. Innocuous though it may seem, this emphasis on mechanism shows that sparsity is often unnecessary. It also questions the possibility of interpretable scientific discovery when prior knowledge is lacking. We believe a precise and philosophically informed definition of interpretability in SciML will help focus research efforts toward the most significant obstacles to realizing a data-driven scientific future.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quartet: Native FP4 Training Can Be Optimal for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14669</link>
<guid>https://arxiv.org/abs/2505.14669</guid>
<content:encoded><![CDATA[
arXiv:2505.14669v2 Announce Type: replace 
Abstract: Training large language models (LLMs) models directly in low-precision offers a way to address computational costs by improving both throughput and energy efficiency. For those purposes, NVIDIA's recent Blackwell architecture facilitates very low-precision operations using FP4 variants. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we investigate hardware-supported FP4 training and introduce a new approach for accurate, end-to-end FP4 training with all the major computations (i.e., linear layers) in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across bit-widths and training setups. Guided by this investigation, we design an "optimal" technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for Blackwell, demonstrating that fully FP4-based training is a competitive alternative to FP16 half-precision and to FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach</title>
<link>https://arxiv.org/abs/2505.16403</link>
<guid>https://arxiv.org/abs/2505.16403</guid>
<content:encoded><![CDATA[
arXiv:2505.16403v2 Announce Type: replace 
Abstract: Manipulation of local training data and local updates, i.e., the poisoning attack, is the main threat arising from the collaborative nature of the federated learning (FL) paradigm. Most existing poisoning attacks aim to manipulate local data/models in a way that causes denial-of-service (DoS) issues. In this paper, we introduce a novel attack method, named Federated Learning Sliding Attack (FedSA) scheme, aiming at precisely introducing the extent of poisoning in a subtle controlled manner. It operates with a predefined objective, such as reducing global model's prediction accuracy by 10%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC) theory with model poisoning attacks. It can manipulate the updates from malicious clients to drive the global model towards a compromised state, achieving this at a controlled and inconspicuous rate. Additionally, leveraging the robust control properties of FedSA allows precise control over the convergence bounds, enabling the attacker to set the global accuracy of the poisoned model to any desired level. Experimental results demonstrate that FedSA can accurately achieve a predefined global accuracy with fewer malicious clients while maintaining a high level of stealth and adjustable learning rates.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved and Oracle-Efficient Online $\ell_1$-Multicalibration</title>
<link>https://arxiv.org/abs/2505.17365</link>
<guid>https://arxiv.org/abs/2505.17365</guid>
<content:encoded><![CDATA[
arXiv:2505.17365v2 Announce Type: replace 
Abstract: We study \emph{online multicalibration}, a framework for ensuring calibrated predictions across multiple groups in adversarial settings, across $T$ rounds. Although online calibration is typically studied in the $\ell_1$ norm, prior approaches to online multicalibration have taken the indirect approach of obtaining rates in other norms (such as $\ell_2$ and $\ell_{\infty}$) and then transferred these guarantees to $\ell_1$ at additional loss. In contrast, we propose a direct method that achieves improved and oracle-efficient rates of $\widetilde{\mathcal{O}}(T^{-1/3})$ and $\widetilde{\mathcal{O}}(T^{-1/4})$ respectively, for online $\ell_1$-multicalibration. Our key insight is a novel reduction of online \(\ell_1\)-multicalibration to an online learning problem with product-based rewards, which we refer to as \emph{online linear-product optimization} ($\mathtt{OLPO}$).
  To obtain the improved rate of $\widetilde{\mathcal{O}}(T^{-1/3})$, we introduce a linearization of $\mathtt{OLPO}$ and design a no-regret algorithm for this linearized problem. Although this method guarantees the desired sublinear rate (nearly matching the best rate for online calibration), it is computationally expensive when the group family \(\mathcal{H}\) is large or infinite, since it enumerates all possible groups. To address scalability, we propose a second approach to $\mathtt{OLPO}$ that makes only a polynomial number of calls to an offline optimization (\emph{multicalibration evaluation}) oracle, resulting in \emph{oracle-efficient} online \(\ell_1\)-multicalibration with a rate of $\widetilde{\mathcal{O}}(T^{-1/4})$. Our framework also extends to certain infinite families of groups (e.g., all linear functions on the context space) by exploiting a $1$-Lipschitz property of the \(\ell_1\)-multicalibration error with respect to \(\mathcal{H}\).
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective</title>
<link>https://arxiv.org/abs/2505.17652</link>
<guid>https://arxiv.org/abs/2505.17652</guid>
<content:encoded><![CDATA[
arXiv:2505.17652v2 Announce Type: replace 
Abstract: Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces $\textbf{C}$ompetence-$\textbf{D}$ifficulty $\textbf{A}$lignment $\textbf{S}$ampling ($\textbf{CDAS}$), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is 2.33 times slower than CDAS.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network</title>
<link>https://arxiv.org/abs/2505.19423</link>
<guid>https://arxiv.org/abs/2505.19423</guid>
<content:encoded><![CDATA[
arXiv:2505.19423v2 Announce Type: replace 
Abstract: Evolutionary Reinforcement Learning (ERL), training the Reinforcement Learning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated enhanced exploration capabilities and greater robustness than using traditional policy gradient. However, ERL suffers from the high computational costs and low search efficiency, as EAs require evaluating numerous candidate policies with expensive simulations, many of which are ineffective and do not contribute meaningfully to the training. One intuitive way to reduce the ineffective evaluations is to adopt the surrogates. Unfortunately, existing ERL policies are often modeled as deep neural networks (DNNs) and thus naturally represented as high-dimensional vectors containing millions of weights, which makes the building of effective surrogates for ERL policies extremely challenging. This paper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE) and Hyperbolic Neural Networks (HNN). Specifically, AE compresses high-dimensional policies into low-dimensional representations while extracting key features as the inputs for the surrogate. HNN, functioning as a classification-based surrogate model, can learn complex nonlinear relationships from sampled data and enable more accurate pre-selection of the sampled policies without real evaluations. The experiments on 10 Atari and 4 Mujoco games have verified that the proposed method outperforms previous approaches significantly. The search trajectories guided by AE and HNN are also visually demonstrated to be more effective, in terms of both exploration and convergence. This paper not only presents the first learnable policy embedding and surrogate-modeling modules for high-dimensional ERL policies, but also empirically reveals when and why they can be successful.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Wave Networks</title>
<link>https://arxiv.org/abs/2505.20034</link>
<guid>https://arxiv.org/abs/2505.20034</guid>
<content:encoded><![CDATA[
arXiv:2505.20034v2 Announce Type: replace 
Abstract: Dynamics modeling has been introduced as a novel paradigm in message passing (MP) of graph neural networks (GNNs). Existing methods consider MP between nodes as a heat diffusion process, and leverage heat equation to model the temporal evolution of nodes in the embedding space. However, heat equation can hardly depict the wave nature of graph signals in graph signal processing. Besides, heat equation is essentially a partial differential equation (PDE) involving a first partial derivative of time, whose numerical solution usually has low stability, and leads to inefficient model training. In this paper, we would like to depict more wave details in MP, since graph signals are essentially wave signals that can be seen as a superposition of a series of waves in the form of eigenvector. This motivates us to consider MP as a wave propagation process to capture the temporal evolution of wave signals in the space. Based on wave equation in physics, we innovatively develop a graph wave equation to leverage the wave propagation on graphs. In details, we demonstrate that the graph wave equation can be connected to traditional spectral GNNs, facilitating the design of graph wave networks based on various Laplacians and enhancing the performance of the spectral GNNs. Besides, the graph wave equation is particularly a PDE involving a second partial derivative of time, which has stronger stability on graphs than the heat equation that involves a first partial derivative of time. Additionally, we theoretically prove that the numerical solution derived from the graph wave equation are constantly stable, enabling to significantly enhance model efficiency while ensuring its performance. Extensive experiments show that GWNs achieve SOTA and efficient performance on benchmark datasets, and exhibit outstanding performance in addressing challenging graph problems, such as over-smoothing and heterophily.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensuring User-side Fairness in Dynamic Recommender Systems</title>
<link>https://arxiv.org/abs/2308.15651</link>
<guid>https://arxiv.org/abs/2308.15651</guid>
<content:encoded><![CDATA[
arXiv:2308.15651v3 Announce Type: replace-cross 
Abstract: User-side group fairness is crucial for modern recommender systems, aiming to alleviate performance disparities among user groups defined by sensitive attributes like gender, race, or age. In the ever-evolving landscape of user-item interactions, continual adaptation to newly collected data is crucial for recommender systems to stay aligned with the latest user preferences. However, we observe that such continual adaptation often exacerbates performance disparities. This necessitates a thorough investigation into user-side fairness in dynamic recommender systems, an area that has been unexplored in the literature. This problem is challenging due to distribution shifts, frequent model updates, and non-differentiability of ranking metrics. To our knowledge, this paper presents the first principled study on ensuring user-side fairness in dynamic recommender systems. We start with theoretical analyses on fine-tuning v.s. retraining, showing that the best practice is incremental fine-tuning with restart. Guided by our theoretical analyses, we propose FAir Dynamic rEcommender (FADE), an end-to-end fine-tuning framework to dynamically ensure user-side fairness over time. To overcome the non-differentiability of recommendation metrics in the fairness loss, we further introduce Differentiable Hit (DH) as an improvement over the recent NeuralNDCG method, not only alleviating its gradient vanishing issue but also achieving higher efficiency. Besides that, we also address the instability issue of the fairness loss by leveraging the competing nature between the recommendation loss and the fairness loss. Through extensive experiments on real-world datasets, we demonstrate that FADE effectively and efficiently reduces performance disparities with little sacrifice in the overall recommendation performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game</title>
<link>https://arxiv.org/abs/2310.18940</link>
<guid>https://arxiv.org/abs/2310.18940</guid>
<content:encoded><![CDATA[
arXiv:2310.18940v4 Announce Type: replace-cross 
Abstract: Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model's training data and results in suboptimal performance. To develop strategic language agents, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidates to play in the game. Extensive experiments show that our agents overcome the intrinsic bias and outperform existing LLM-based agents in the Werewolf game. We also conduct human-agent experiments and find that our agents achieve human-level performance and demonstrate strong strategic play.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Straightforward Gradient-Based Approach for High-Tc Superconductor Design: Leveraging Domain Knowledge via Adaptive Constraints</title>
<link>https://arxiv.org/abs/2403.13627</link>
<guid>https://arxiv.org/abs/2403.13627</guid>
<content:encoded><![CDATA[
arXiv:2403.13627v2 Announce Type: replace-cross 
Abstract: Materials design aims to discover novel compounds with desired properties. However, prevailing strategies face critical trade-offs. Conventional element-substitution approaches readily and adaptively incorporate various domain knowledge but remain confined to a narrow search space. In contrast, deep generative models efficiently explore vast compositional landscapes, yet they struggle to flexibly integrate domain knowledge. To address these trade-offs, we propose a gradient-based material design framework that combines these strengths, offering both efficiency and adaptability. In our method, chemical compositions are optimised to achieve target properties by using property prediction models and their gradients. In order to seamlessly enforce diverse constraints, including those reflecting domain insights such as oxidation states, discretised compositional ratios, types of elements, and their abundance, we apply masks and employ a special loss function, namely the integer loss. Furthermore, we initialise the optimisation using promising candidates from existing dataset, effectively guiding the search away from unfavourable regions and thus helping to avoid poor solutions. Our approach demonstrates a more efficient exploration of superconductor candidates, uncovering candidate materials with higher critical temperature than conventional element-substitution and generative models. Importantly, it could propose new compositions beyond those found in existing databases, including new hydride superconductors absent from the training dataset but which share compositional similarities with materials found in literature. This synergy of domain knowledge and machine-learning-based scalability provides a robust foundation for rapid, adaptive, and comprehensive materials design for superconductors and beyond.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling the Interplay between Carryover Effects and Reward Autocorrelations in Switchback Experiments</title>
<link>https://arxiv.org/abs/2403.17285</link>
<guid>https://arxiv.org/abs/2403.17285</guid>
<content:encoded><![CDATA[
arXiv:2403.17285v3 Announce Type: replace-cross 
Abstract: A/B testing has become the gold standard for policy evaluation in modern technological industries. Motivated by the widespread use of switchback experiments in A/B testing, this paper conducts a comprehensive comparative analysis of various switchback designs in Markovian environments. Unlike many existing works which derive the optimal design based on specific and relatively simple estimators, our analysis covers a range of state-of-the-art estimators developed in the reinforcement learning (RL) literature. It reveals that the effectiveness of different switchback designs depends crucially on (i) the size of the carryover effect and (ii) the auto-correlations among reward errors over time. Meanwhile, these findings are estimator-agnostic, i.e., they apply to most RL estimators. Based on these insights, we provide a workflow to offer guidelines for practitioners on designing switchback experiments in A/B testing.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatHuman: Chatting about 3D Humans with Tools</title>
<link>https://arxiv.org/abs/2405.04533</link>
<guid>https://arxiv.org/abs/2405.04533</guid>
<content:encoded><![CDATA[
arXiv:2405.04533v2 Announce Type: replace-cross 
Abstract: Numerous methods have been proposed to detect, estimate, and analyze properties of people in images, including 3D pose, shape, contact, human-object interaction, and emotion. While widely applicable in vision and other areas, such methods require expert knowledge to select, use, and interpret the results. To address this, we introduce ChatHuman, a language-driven system that integrates the capabilities of specialized methods into a unified framework. ChatHuman functions as an assistant proficient in utilizing, analyzing, and interacting with tools specific to 3D human tasks, adeptly discussing and resolving related challenges. Built on a Large Language Model (LLM) framework, ChatHuman is trained to autonomously select, apply, and interpret a diverse set of tools in response to user inputs. Our approach overcomes significant hurdles in adapting LLMs to 3D human tasks, including the need for domain-specific knowledge and the ability to interpret complex 3D outputs. The innovations of ChatHuman include leveraging academic publications to instruct the LLM on tool usage, employing a retrieval-augmented generation model to create in-context learning examples for managing new tools, and effectively discriminating between and integrating tool results by transforming specialized 3D outputs into comprehensible formats. Experiments demonstrate that ChatHuman surpasses existing models in both tool selection accuracy and overall performance across various 3D human tasks, and it supports interactive chatting with users. ChatHuman represents a significant step toward consolidating diverse analytical methods into a unified, robust system for 3D human tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximate Thompson Sampling for Learning Linear Quadratic Regulators with $O(\sqrt{T})$ Regret</title>
<link>https://arxiv.org/abs/2405.19380</link>
<guid>https://arxiv.org/abs/2405.19380</guid>
<content:encoded><![CDATA[
arXiv:2405.19380v2 Announce Type: replace-cross 
Abstract: We propose a novel Thompson sampling algorithm that learns linear quadratic regulators (LQR) with a Bayesian regret bound of $O(\sqrt{T})$. Our method leverages Langevin dynamics with a carefully designed preconditioner and incorporates a simple excitation mechanism. We show that the excitation signal drives the minimum eigenvalue of the preconditioner to grow over time, thereby accelerating the approximate posterior sampling process. Furthermore, we establish nontrivial concentration properties of the approximate posteriors generated by our algorithm. These properties enable us to bound the moments of the system state and attain an $O(\sqrt{T})$ regret bound without relying on the restrictive assumptions that are often used in the literature.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JANET: Joint Adaptive predictioN-region Estimation for Time-series</title>
<link>https://arxiv.org/abs/2407.06390</link>
<guid>https://arxiv.org/abs/2407.06390</guid>
<content:encoded><![CDATA[
arXiv:2407.06390v2 Announce Type: replace-cross 
Abstract: Conformal prediction provides machine learning models with prediction sets that offer theoretical guarantees, but the underlying assumption of exchangeability limits its applicability to time series data. Furthermore, existing approaches struggle to handle multi-step ahead prediction tasks, where uncertainty estimates across multiple future time points are crucial. We propose JANET (Joint Adaptive predictioN-region Estimation for Time-series), a novel framework for constructing conformal prediction regions that are valid for both univariate and multivariate time series. JANET generalises the inductive conformal framework and efficiently produces joint prediction regions with controlled K-familywise error rates, enabling flexible adaptation to specific application needs. Our empirical evaluation demonstrates JANET's superior performance in multi-step prediction tasks across diverse time series datasets, highlighting its potential for reliable and interpretable uncertainty quantification in sequential data.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models</title>
<link>https://arxiv.org/abs/2408.04556</link>
<guid>https://arxiv.org/abs/2408.04556</guid>
<content:encoded><![CDATA[
arXiv:2408.04556v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable proficiency across various natural language processing (NLP) tasks. However, adapting LLMs to downstream applications requires computationally intensive and memory-demanding fine-tuning procedures. To alleviate these burdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a promising approach to tailor LLMs with minimal computational overhead. While PEFT methods offer substantial advantages, they do not fully address the pervasive issue of bias propagation from pre-training data. This work introduces Bias-Alleviating Low-Rank Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias inheritance. BA-LoRA incorporates three distinct regularization terms: (1) a consistency regularizer, (2) a diversity regularizer, and (3) a singular value decomposition regularizer. These regularizers aim to enhance the models' consistency, diversity, and generalization capabilities during fine-tuning. We conduct extensive experiments on natural language understanding (NLU) and natural language generation (NLG) tasks using prominent LLMs such as LLaMA, Mistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and its state-of-the-art variants. Moreover, the extended experiments demonstrate that our method effectively mitigates the adverse effects of pre-training bias, leading to more reliable and robust model outputs. The code is available at https://github.com/cyp-jlu-ai/BA-LoRA.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Empirical Mean is Minimax Optimal for Local Glivenko-Cantelli</title>
<link>https://arxiv.org/abs/2410.02835</link>
<guid>https://arxiv.org/abs/2410.02835</guid>
<content:encoded><![CDATA[
arXiv:2410.02835v2 Announce Type: replace-cross 
Abstract: We revisit the recently introduced Local Glivenko-Cantelli setting, which studies distribution-dependent uniform convergence rates of the Empirical Mean Estimator (EME). In this work, we investigate generalizations of this setting where arbitrary estimators are allowed rather than just the EME. Can a strictly larger class of measures be learned? Can better risk decay rates be obtained? We provide exhaustive answers to these questions, which are both negative, provided the learner is barred from exploiting some infinite-dimensional pathologies. On the other hand, allowing such exploits does lead to a strictly larger class of learnable measures.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3Bench: Benchmarking Whole-body Motion Generation for Mobile Manipulation in 3D Scenes</title>
<link>https://arxiv.org/abs/2410.06678</link>
<guid>https://arxiv.org/abs/2410.06678</guid>
<content:encoded><![CDATA[
arXiv:2410.06678v3 Announce Type: replace-cross 
Abstract: We propose M3Bench, a new benchmark for whole-body motion generation in mobile manipulation tasks. Given a 3D scene context, M3Bench requires an embodied agent to reason about its configuration, environmental constraints, and task objectives to generate coordinated whole-body motion trajectories for object rearrangement. M3Bench features 30,000 object rearrangement tasks across 119 diverse scenes, providing expert demonstrations generated by our newly developed M3BenchMaker, an automatic data generation tool that produces whole-body motion trajectories from high-level task instructions using only basic scene and robot information. Our benchmark includes various task splits to evaluate generalization across different dimensions and leverages realistic physics simulation for trajectory assessment. Extensive evaluation analysis reveals that state-of-the-art models struggle with coordinating base-arm motion while adhering to environmental and task-specific constraints, underscoring the need for new models to bridge this gap. By releasing M3Bench and M3BenchMaker we aim to advance robotics research toward more adaptive and capable mobile manipulation in diverse, real-world environments.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Gradient-Based Methods: Multi-Property Materials Design Beyond Differentiable Targets</title>
<link>https://arxiv.org/abs/2410.08562</link>
<guid>https://arxiv.org/abs/2410.08562</guid>
<content:encoded><![CDATA[
arXiv:2410.08562v4 Announce Type: replace-cross 
Abstract: Gradient-based methods offer a simple, efficient strategy for materials design by directly optimizing candidates using gradients from pretrained property predictors. However, their use in crystal structure optimization is hindered by two key challenges: handling non-differentiable constraints, such as charge neutrality and structural fidelity, and susceptibility to poor local minima. We revisit and extend the gradient-based methods to address these issues. We propose Simultaneous Multi-property Optimization using Adaptive Crystal Synthesizer (SMOACS), which integrates oxidation-number masks and template-based initialization to enforce non-differentiable constraints, avoid poor local minima, and flexibly incorporate additional constraints without retraining. SMOACS enables multi-property optimization. including exceptional targets such as high-temperature superconductivity, and scales to large crystal systems, both persistent challenges for generative models, even those enhanced with gradient-based guidance from property predictors. In experiments on five target properties and three datasets, SMOACS outperforms generative models and Bayesian optimization methods, successfully designing 135-atom perovskite structures that satisfy multiple property targets and constraints, a task at which the other methods fail entirely.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs</title>
<link>https://arxiv.org/abs/2410.11001</link>
<guid>https://arxiv.org/abs/2410.11001</guid>
<content:encoded><![CDATA[
arXiv:2410.11001v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has revitalized Large Language Models (LLMs) by injecting non-parametric factual knowledge. Compared with long-context LLMs, RAG is considered an effective summarization tool in a more concise and lightweight manner, which can interact with LLMs multiple times using diverse queries to get comprehensive responses. However, the LLM-generated historical responses, which contain potentially insightful information, are largely neglected and discarded by existing approaches, leading to suboptimal results. In this paper, we propose $\textit{graph of records}$ ($\textbf{GoR}$), which leverages historical responses generated by LLMs to enhance RAG for long-context global summarization. Inspired by the $\textit{retrieve-then-generate}$ paradigm of RAG, we construct a graph by establishing an edge between the retrieved text chunks and the corresponding LLM-generated response. To further uncover the intricate correlations between them, GoR features a $\textit{graph neural network}$ and an elaborately designed $\textit{BERTScore}$-based objective for self-supervised model training, enabling seamless supervision signal backpropagation between reference summaries and node embeddings. We comprehensively compare GoR with 12 baselines across four long-context summarization datasets, and the results indicate that our proposed method reaches the best performance ($\textit{e.g.}$, 15%, 8%, and 19% improvement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP dataset). Extensive experiments further demonstrate the effectiveness of GoR.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance-dependent Convergence Theory for Diffusion Models</title>
<link>https://arxiv.org/abs/2410.13738</link>
<guid>https://arxiv.org/abs/2410.13738</guid>
<content:encoded><![CDATA[
arXiv:2410.13738v2 Announce Type: replace-cross 
Abstract: Score-based diffusion models have demonstrated outstanding empirical performance in machine learning and artificial intelligence, particularly in generating high-quality new samples from complex probability distributions. Improving the theoretical understanding of diffusion models, with a particular focus on the convergence analysis, has attracted significant attention. In this work, we develop a convergence rate that is adaptive to the smoothness of different target distributions, referred to as instance-dependent bound. Specifically, we establish an iteration complexity of $\min\{d,d^{2/3}L^{1/3},d^{1/3}L\}\varepsilon^{-2/3}$ (up to logarithmic factors), where $d$ denotes the data dimension, and $\varepsilon$ quantifies the output accuracy in terms of total variation (TV) distance. In addition, $L$ represents a relaxed Lipschitz constant, which, in the case of Gaussian mixture models, scales only logarithmically with the number of components, the dimension and iteration number, demonstrating broad applicability.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guarantees of a Preconditioned Subgradient Algorithm for Overparameterized Asymmetric Low-rank Matrix Recovery</title>
<link>https://arxiv.org/abs/2410.16826</link>
<guid>https://arxiv.org/abs/2410.16826</guid>
<content:encoded><![CDATA[
arXiv:2410.16826v2 Announce Type: replace-cross 
Abstract: In this paper, we focus on a matrix factorization-based approach to recover low-rank {\it asymmetric} matrices from corrupted measurements. We propose an {\it Overparameterized Preconditioned Subgradient Algorithm (OPSA)} and provide, for the first time in the literature, linear convergence rates independent of the rank of the sought asymmetric matrix in the presence of gross corruptions. Our work goes beyond existing results in preconditioned-type approaches addressing their current limitation, i.e., the lack of convergence guarantees in the case of {\it asymmetric matrices of unknown rank}. By applying our approach to (robust) matrix sensing, we highlight its merits when the measurement operator satisfies a mixed-norm restricted isometry property. Lastly, we present extensive numerical experiments that validate our theoretical results and demonstrate the effectiveness of our approach for different levels of overparameterization and outlier corruptions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Positive Pairs in Contrastive Learning</title>
<link>https://arxiv.org/abs/2410.18200</link>
<guid>https://arxiv.org/abs/2410.18200</guid>
<content:encoded><![CDATA[
arXiv:2410.18200v2 Announce Type: replace-cross 
Abstract: The training methods in AI do involve semantically distinct pairs of samples. However, their role typically is to enhance the between class separability. The actual notion of similarity is normally learned from semantically identical pairs. This paper presents SimLAP: a simple framework for learning visual representation from arbitrary pairs. SimLAP explores the possibility of learning similarity from semantically distinct sample pairs. The approach is motivated by the observation that for any pair of classes there exists a subspace in which semantically distinct samples exhibit similarity. This phenomenon can be exploited for a novel method of learning, which optimises the similarity of an arbitrary pair of samples, while simultaneously learning the enabling subspace. The feasibility of the approach will be demonstrated experimentally and its merits discussed.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point Cloud Synthesis Using Inner Product Transforms</title>
<link>https://arxiv.org/abs/2410.18987</link>
<guid>https://arxiv.org/abs/2410.18987</guid>
<content:encoded><![CDATA[
arXiv:2410.18987v3 Announce Type: replace-cross 
Abstract: Point-cloud synthesis, i.e. the generation of novel point clouds from an input distribution, remains a challenging task, for which numerous complex machine-learning models have been devised. We develop a novel method that encodes geometrical-topological characteristics of point clouds using inner products, leading to a highly-efficient point cloud representation with provable expressivity properties. Integrated into deep learning models, our encoding exhibits high quality in typical tasks like reconstruction, generation, and interpolation, with inference times orders of magnitude faster than existing methods.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation</title>
<link>https://arxiv.org/abs/2411.04999</link>
<guid>https://arxiv.org/abs/2411.04999</guid>
<content:encoded><![CDATA[
arXiv:2411.04999v2 Announce Type: replace-cross 
Abstract: Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenarios where environments frequently change due to human intervention or the robot's own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot's environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70% on non-stationary objects, which is more than a 2x improvement over state-of-the-art static systems. Our code as well as our experiment and deployment videos are open sourced and can be found on our project website: https://dynamem.github.io/
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraints and Variables Reduction for Optimal Power Flow Using Hierarchical Graph Neural Networks with Virtual Node-Splitting</title>
<link>https://arxiv.org/abs/2411.06268</link>
<guid>https://arxiv.org/abs/2411.06268</guid>
<content:encoded><![CDATA[
arXiv:2411.06268v2 Announce Type: replace-cross 
Abstract: Power system networks are often modeled as homogeneous graphs, which limits the ability of graph neural network (GNN) to capture individual generator features at the same nodes. By introducing the proposed virtual node-splitting strategy, generator-level attributes like costs, limits, and ramp rates can be fully captured by GNN models, improving GNN's learning capacity and prediction accuracy. Optimal power flow (OPF) problem is used for real-time grid operations. Limited timeframe motivates studies to create size-reduced OPF (ROPF) models to relieve the computational complexity. In this paper, with virtual node-splitting, a novel two-stage adaptive hierarchical GNN is developed to (i) predict critical lines that would be congested, and then (ii) predict base generators that would operate at the maximum capacity. This will substantially reduce the constraints and variables needed for OPF, creating the proposed ROPFLG model with reduced monitor lines and reduced generator-specific variables and constraints. Two ROPF models, ROPFL and ROPFG, with just reduced lines or generators respectively, are also implemented as additional benchmark models. Case studies show that the proposed ROPFLG consistently outperforms the benchmark full OPF (FOPF) and the other two ROPF methods, achieving significant computational time savings while reliably finding optimal solutions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains</title>
<link>https://arxiv.org/abs/2411.06426</link>
<guid>https://arxiv.org/abs/2411.06426</guid>
<content:encoded><![CDATA[
arXiv:2411.06426v3 Announce Type: replace-cross 
Abstract: As the integration of the Large Language Models (LLMs) into various applications increases, so does their susceptibility to misuse, raising significant security concerns. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks mainly rely on scenario camouflage, prompt obfuscation, prompt optimization, and prompt iterative optimization to conceal malicious prompts. In particular, sequential prompt chains in a single query can lead LLMs to focus on certain prompts while ignoring others, facilitating context manipulation. This paper introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We discuss several scenarios, not limited to examples like Question Bank, Dialog Completion, and Game Environment, where the harmful prompt is embedded within benign ones that can fool LLMs into generating harmful responses. The distinct narrative structures of these scenarios show that SequentialBreak is flexible enough to adapt to various prompt formats beyond those discussed. Extensive experiments demonstrate that SequentialBreak uses only a single query to achieve a substantial gain of attack success rate over existing baselines against both open-source and closed-source models. Through our research, we highlight the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse. All the result files and website associated with this research are available in this GitHub repository: https://anonymous.4open.science/r/JailBreakAttack-4F3B/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surveying the space of descriptions of a composite system with machine learning</title>
<link>https://arxiv.org/abs/2411.18579</link>
<guid>https://arxiv.org/abs/2411.18579</guid>
<content:encoded><![CDATA[
arXiv:2411.18579v2 Announce Type: replace-cross 
Abstract: Multivariate information theory provides a general and principled framework for understanding how the components of a complex system are connected. Existing analyses are coarse in nature -- built up from characterizations of discrete subsystems -- and can be computationally prohibitive. In this work, we propose to study the continuous space of possible descriptions of a composite system as a window into its organizational structure. A description consists of specific information conveyed about each of the components, and the space of possible descriptions is equivalent to the space of lossy compression schemes of the components. We introduce a machine learning framework to optimize descriptions that extremize key information theoretic quantities used to characterize organization, such as total correlation and O-information. Through case studies on spin systems, sudoku boards, and letter sequences from natural language, we identify extremal descriptions that reveal how system-wide variation emerges from individual components. By integrating machine learning into a fine-grained information theoretic analysis of composite random variables, our framework opens a new avenues for probing the structure of real-world complex systems.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracking Progress Towards Sustainable Development Goal 6 Using Satellite Imagery</title>
<link>https://arxiv.org/abs/2411.19093</link>
<guid>https://arxiv.org/abs/2411.19093</guid>
<content:encoded><![CDATA[
arXiv:2411.19093v2 Announce Type: replace-cross 
Abstract: Clean water and sanitation are essential for health, well-being, and sustainable development, yet significant global disparities persist. Although the United Nations' Sustainable Development Goal (SDG) 6 clearly defines targets for universal access to clean water and sanitation, limitations in data coverage and openness impede accurate tracking of progress in many countries. To bridge these gaps, this study integrates Afrobarometer survey data, satellite imagery from Landsat 8 and Sentinel-2, and advanced deep learning techniques using Meta's self-supervised Distillation with No Labels (DINO) model to develop a modeling framework for evaluating access to piped water and sewage system across diverse African regions. The modeling framework achieved notable accuracy, with over 96% for piped water and 97% for sewage system access classification. When combined with geospatial population data, validation against official statistics from the United Nations Joint Monitoring Program demonstrated high concordance at the national scale (R2 of 0.95 for piped water access and R2 of 0.85 for sewage system access). The national-level estimates can represent SDG Indicators 6.1.1 and 6.2.1. This approach provides policymakers and stakeholders with an effective, scalable, and cost-efficient tool to pinpoint underserved areas requiring targeted intervention. The methodology developed herein can be adapted for assessing other infrastructure-related SDGs, promoting enhanced monitoring and informed decision-making towards achieving global sustainability objectives.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Tensor Motion Planning</title>
<link>https://arxiv.org/abs/2411.19393</link>
<guid>https://arxiv.org/abs/2411.19393</guid>
<content:encoded><![CDATA[
arXiv:2411.19393v3 Announce Type: replace-cross 
Abstract: Batch planning is increasingly necessary to quickly produce diverse and quality motion plans for downstream learning applications, such as distillation and imitation learning. This paper presents Global Tensor Motion Planning (GTMP) -- a sampling-based motion planning algorithm comprising only tensor operations. We introduce a novel discretization structure represented as a random multipartite graph, enabling efficient vectorized sampling, collision checking, and search. We provide a theoretical investigation showing that GTMP exhibits probabilistic completeness while supporting modern GPU/TPU. Additionally, by incorporating smooth structures into the multipartite graph, GTMP directly plans smooth splines without requiring gradient-based optimization. Experiments on lidar-scanned occupancy maps and the MotionBenchMarker dataset demonstrate GTMP's computation efficiency in batch planning compared to baselines, underscoring GTMP's potential as a robust, scalable planner for diverse applications and large-scale robot learning tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Circumventing shortcuts in audio-visual deepfake detection datasets with unsupervised learning</title>
<link>https://arxiv.org/abs/2412.00175</link>
<guid>https://arxiv.org/abs/2412.00175</guid>
<content:encoded><![CDATA[
arXiv:2412.00175v3 Announce Type: replace-cross 
Abstract: Good datasets are essential for developing and benchmarking any machine learning system. Their importance is even more extreme for safety critical applications such as deepfake detection - the focus of this paper. Here we reveal that two of the most widely used audio-video deepfake datasets suffer from a previously unidentified spurious feature: the leading silence. Fake videos start with a very brief moment of silence and based on this feature alone, we can separate the real and fake samples almost perfectly. As such, previous audio-only and audio-video models exploit the presence of silence in the fake videos and consequently perform worse when the leading silence is removed. To circumvent latching on such unwanted artifact and possibly other unrevealed ones we propose a shift from supervised to unsupervised learning by training models exclusively on real data. We show that by aligning self-supervised audio-video representations we remove the risk of relying on dataset-specific biases and improve robustness in deepfake detection.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AntiLeakBench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge</title>
<link>https://arxiv.org/abs/2412.13670</link>
<guid>https://arxiv.org/abs/2412.13670</guid>
<content:encoded><![CDATA[
arXiv:2412.13670v2 Announce Type: replace-cross 
Abstract: Data contamination hinders fair LLM evaluation by introducing test data into newer models' training sets. Existing studies solve this challenge by updating benchmarks with newly collected data. However, they fail to guarantee contamination-free evaluation as the newly collected data may contain pre-existing knowledge, and their benchmark updates rely on intensive human labor. To address these issues, we in this paper propose AntiLeak-Bench, an automated anti-leakage benchmarking framework. Instead of simply using newly collected data, we construct samples with explicitly new knowledge absent from LLMs' training sets, which thus ensures strictly contamination-free evaluation. We further design a fully automated workflow to build and update our benchmark without human labor. This significantly reduces the cost of benchmark maintenance to accommodate emerging LLMs. Through extensive experiments, we highlight that data contamination likely exists before LLMs' cutoff time and demonstrate AntiLeak-Bench effectively overcomes this challenge.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Representation Learning for fMRI-based Neurological Disorder Identification</title>
<link>https://arxiv.org/abs/2412.16197</link>
<guid>https://arxiv.org/abs/2412.16197</guid>
<content:encoded><![CDATA[
arXiv:2412.16197v2 Announce Type: replace-cross 
Abstract: Despite the impressive advances achieved using deep learning for functional brain activity analysis, the heterogeneity of functional patterns and the scarcity of imaging data still pose challenges in tasks such as identifying neurological disorders. For functional Magnetic Resonance Imaging (fMRI), while data may be abundantly available from healthy controls, clinical data is often scarce, especially for rare diseases, limiting the ability of models to identify clinically-relevant features. We overcome this limitation by introducing a novel representation learning strategy integrating meta-learning with self-supervised learning to improve the generalization from normal to clinical features. This approach enables generalization to challenging clinical tasks featuring scarce training data. We achieve this by leveraging self-supervised learning on the control dataset to focus on inherent features that are not limited to a particular supervised task and incorporating meta-learning to improve the generalization across domains. To explore the generalizability of the learned representations to unseen clinical applications, we apply the model to four distinct clinical datasets featuring scarce and heterogeneous data for neurological disorder classification. Results demonstrate the superiority of our representation learning strategy on diverse clinically-relevant tasks. Code is publicly available at https://github.com/wenhui0206/MeTSK/tree/main
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping</title>
<link>https://arxiv.org/abs/2412.19529</link>
<guid>https://arxiv.org/abs/2412.19529</guid>
<content:encoded><![CDATA[
arXiv:2412.19529v4 Announce Type: replace-cross 
Abstract: Recently, the study of heavy-tailed noises in first-order nonconvex stochastic optimization has gotten a lot of attention since it was recognized as a more realistic condition as suggested by many empirical observations. Specifically, the stochastic noise (the difference between the stochastic and true gradient) is considered to have only a finite $\mathfrak{p}$-th moment where $\mathfrak{p}\in\left(1,2\right]$ instead of assuming it always satisfies the classical finite variance assumption. To deal with this more challenging setting, people have proposed different algorithms and proved them to converge at an optimal $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{3\mathfrak{p}-2}})$ rate for smooth objectives after $T$ iterations. Notably, all these new-designed algorithms are based on the same technique - gradient clipping. Naturally, one may want to know whether the clipping method is a necessary ingredient and the only way to guarantee convergence under heavy-tailed noises. In this work, by revisiting the existing Batched Normalized Stochastic Gradient Descent with Momentum (Batched NSGDM) algorithm, we provide the first convergence result under heavy-tailed noises but without gradient clipping. Concretely, we prove that Batched NSGDM can achieve the optimal $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{3\mathfrak{p}-2}})$ rate even under the relaxed smooth condition. More interestingly, we also establish the first $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{2\mathfrak{p}}})$ convergence rate in the case where the tail index $\mathfrak{p}$ is unknown in advance, which is arguably the common scenario in practice.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An unsupervised method for MRI recovery: Deep image prior with structured sparsity</title>
<link>https://arxiv.org/abs/2501.01482</link>
<guid>https://arxiv.org/abs/2501.01482</guid>
<content:encoded><![CDATA[
arXiv:2501.01482v3 Announce Type: replace-cross 
Abstract: Objective: To propose and validate an unsupervised MRI reconstruction method that does not require fully sampled k-space data. Materials and Methods: The proposed method, deep image prior with structured sparsity (DISCUS), extends the deep image prior (DIP) by introducing group sparsity to frame-specific code vectors, enabling the discovery of a low-dimensional manifold for capturing temporal variations. \discus was validated using four studies: (I) simulation of a dynamic Shepp-Logan phantom to demonstrate its manifold discovery capabilities, (II) comparison with compressed sensing and DIP-based methods using simulated single-shot late gadolinium enhancement (LGE) image series from six distinct digital cardiac phantoms in terms of normalized mean square error (NMSE) and structural similarity index measure (SSIM), (III) evaluation on retrospectively undersampled single-shot LGE data from eight patients, and (IV) evaluation on prospectively undersampled single-shot LGE data from eight patients, assessed via blind scoring from two expert readers. Results: DISCUS outperformed competing methods, demonstrating superior reconstruction quality in terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study IV). Discussion: An unsupervised image reconstruction method is presented and validated on simulated and measured data. These developments can benefit applications where acquiring fully sampled data is challenging.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoRAG: Retrieval-Augmented Generation over Video Corpus</title>
<link>https://arxiv.org/abs/2501.05874</link>
<guid>https://arxiv.org/abs/2501.05874</guid>
<content:encoded><![CDATA[
arXiv:2501.05874v3 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) is a powerful strategy for improving the factual accuracy of models by retrieving external knowledge relevant to queries and incorporating it into the generation process. However, existing approaches primarily focus on text, with some recent advancements considering images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing contextual details more effectively than any other modality. While very recent studies explore the use of videos in response generation, they either predefine query-associated videos without retrieval or convert videos into textual descriptions losing multimodal richness. To tackle these, we introduce VideoRAG, a framework that not only dynamically retrieves videos based on their relevance with queries but also utilizes both visual and textual information. The operation of VideoRAG is powered by recent Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and the seamless integration of retrieved videos jointly with queries for response generation. Also, inspired by that the context size of LVLMs may not be sufficient to process all frames in extremely long videos and not all frames are equally important, we introduce a video frame selection mechanism to extract the most informative subset of frames, along with a strategy to extract textual information from videos (as it can aid the understanding of video content) when their subtitles are not available. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines. Code is available at https://github.com/starsuzi/VideoRAG.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Product Attention Is All You Need</title>
<link>https://arxiv.org/abs/2501.06425</link>
<guid>https://arxiv.org/abs/2501.06425</guid>
<content:encoded><![CDATA[
arXiv:2501.06425v4 Announce Type: replace-cross 
Abstract: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines, including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at the decoding stage enable processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Localization and Activation Editing for Low-Resource Fine-Tuning</title>
<link>https://arxiv.org/abs/2502.01179</link>
<guid>https://arxiv.org/abs/2502.01179</guid>
<content:encoded><![CDATA[
arXiv:2502.01179v4 Announce Type: replace-cross 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly used to adapt LLMs. However, the effectiveness of standard PEFT methods is limited in low-resource scenarios with only a few hundred examples. Recent advances in interpretability research have inspired the emergence of activation editing (or steering) techniques, which modify the activations of specific model components. Due to their extremely small parameter counts, these methods show promise for small datasets. However, their performance is highly dependent on identifying the correct modules to edit and often lacks stability across different datasets. In this paper, we propose Joint Localization and Activation Editing (JoLA), a method that jointly learns (1) which heads in the Transformer to edit (2) whether the intervention should be additive, multiplicative, or both and (3) the intervention parameters themselves - the vectors applied as additive offsets or multiplicative scalings to the head output. Through evaluations on three benchmarks spanning commonsense reasoning, natural language understanding, and natural language generation, we demonstrate that JoLA consistently outperforms existing methods. The code for the method is released at https://github.com/wenlai-lavine/jola.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Large Language Model Collaborative Decoding via Speculation</title>
<link>https://arxiv.org/abs/2502.01662</link>
<guid>https://arxiv.org/abs/2502.01662</guid>
<content:encoded><![CDATA[
arXiv:2502.01662v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) collaborative decoding techniques improve output quality by combining the outputs of multiple models at each generation step, but they incur high computational costs. In this paper, we introduce Collaborative decoding via Speculation (CoS), a novel framework that accelerates collaborative decoding without compromising performance. Inspired by Speculative Decoding--where a small proposal model generates tokens sequentially, and a larger target model verifies them in parallel, our approach builds on two key insights: (1) the verification distribution can be the combined distribution of both the proposal and target models, and (2) alternating each model as the proposer and verifier can further enhance efficiency. We generalize this method to collaboration among n models and theoretically prove that CoS is never slower than standard collaborative decoding, typically achieving faster speed. Extensive experiments demonstrate CoS is 1.11x-2.23x faster than standard collaborative decoding without compromising generation quality. Our code is available at https://github.com/Kamichanw/CoS/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wake-Informed 3D Path Planning for Autonomous Underwater Vehicles Using A* and Neural Network Approximations</title>
<link>https://arxiv.org/abs/2502.01918</link>
<guid>https://arxiv.org/abs/2502.01918</guid>
<content:encoded><![CDATA[
arXiv:2502.01918v2 Announce Type: replace-cross 
Abstract: Autonomous Underwater Vehicles (AUVs) encounter significant energy, control and navigation challenges in complex underwater environments, particularly during close-proximity operations, such as launch and recovery (LAR), where fluid interactions and wake effects present additional navigational and energy challenges. Traditional path planning methods fail to incorporate these detailed wake structures, resulting in increased energy consumption, reduced control stability, and heightened safety risks. This paper presents a novel wake-informed, 3D path planning approach that fully integrates localized wake effects and global currents into the planning algorithm. Two variants of the A* algorithm - a current-informed planner and a wake-informed planner - are created to assess its validity and two neural network models are then trained to approximate these planners for real-time applications. Both the A* planners and NN models are evaluated using important metrics such as energy expenditure, path length, and encounters with high-velocity and turbulent regions. The results demonstrate a wake-informed A* planner consistently achieves the lowest energy expenditure and minimizes encounters with high-velocity regions, reducing energy consumption by up to 11.3%. The neural network models are observed to offer computational speedup of 6 orders of magnitude, but exhibit 4.51 - 19.79% higher energy expenditures and 9.81 - 24.38% less optimal paths. These findings underscore the importance of incorporating detailed wake structures into traditional path planning algorithms and the benefits of neural network approximations to enhance energy efficiency and operational safety for AUVs in complex 3D domains.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance</title>
<link>https://arxiv.org/abs/2502.04350</link>
<guid>https://arxiv.org/abs/2502.04350</guid>
<content:encoded><![CDATA[
arXiv:2502.04350v2 Announce Type: replace-cross 
Abstract: Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-turn guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-turn supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0 and https://huggingface.co/yongchao98.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives</title>
<link>https://arxiv.org/abs/2502.04358</link>
<guid>https://arxiv.org/abs/2502.04358</guid>
<content:encoded><![CDATA[
arXiv:2502.04358v2 Announce Type: replace-cross 
Abstract: Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regression and Forecasting of U.S. Stock Returns Based on LSTM</title>
<link>https://arxiv.org/abs/2502.05210</link>
<guid>https://arxiv.org/abs/2502.05210</guid>
<content:encoded><![CDATA[
arXiv:2502.05210v3 Announce Type: replace-cross 
Abstract: This paper analyses the investment returns of three stock sectors, Manuf, Hitec, and Other, in the U.S. stock market, based on the Fama-French three-factor model, the Carhart four-factor model, and the Fama-French five-factor model, in order to test the validity of the Fama-French three-factor model, the Carhart four-factor model, and the Fama-French five-factor model for the three sectors of the market. French five-factor model for the three sectors of the market. Also, the LSTM model is used to explore the additional factors affecting stock returns. The empirical results show that the Fama-French five-factor model has better validity for the three segments of the market under study, and the LSTM model has the ability to capture the factors affecting the returns of certain industries, and can better regress and predict the stock returns of the relevant industries. Keywords- Fama-French model; Carhart model; Factor model; LSTM model.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Differentially Private Bayesian Optimization</title>
<link>https://arxiv.org/abs/2502.06044</link>
<guid>https://arxiv.org/abs/2502.06044</guid>
<content:encoded><![CDATA[
arXiv:2502.06044v2 Announce Type: replace-cross 
Abstract: In recent years, there has been much work on scaling Bayesian Optimization to high-dimensional problems, for example hyperparameter tuning in large machine learning models. These scalable methods have been successful, finding high objective values much more quickly than traditional global Bayesian Optimization or random search-based methods. At the same time, these large models often use sensitive data, but preservation of Differential Privacy has not scaled alongside these modern Bayesian Optimization procedures. Here we develop a method to privately optimize potentially high-dimensional parameter spaces using privatized Gradient Informative Bayesian Optimization. Our theoretical results show that under suitable conditions, our method converges exponentially fast to a locally optimal parameter configuration, up to a natural privacy error. Moreover, regardless of whether the assumptions are satisfied, we prove that our algorithm maintains privacy and empirically display superior performance to existing methods in the high-dimensional hyperparameter setting.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Value of Information in Human-AI Decision-making</title>
<link>https://arxiv.org/abs/2502.06152</link>
<guid>https://arxiv.org/abs/2502.06152</guid>
<content:encoded><![CDATA[
arXiv:2502.06152v4 Announce Type: replace-cross 
Abstract: Multiple agents -- including humans and AI models -- are increasingly combined to make decisions with the expectation of achieving complementary performance, where the decisions they make together outperform those made individually. However, knowing how to improve the performance of collaborating agents is often difficult without knowing more about what particular information and strategies each agent employs. With a focus on human-AI pairings, we contribute a decision-theoretic framework for characterizing the value of information -- and consequently, opportunities for agents to better exploit available information -- in AI-assisted decision workflows. We present a novel explanation technique (ILIV-SHAP) that adapts SHAP explanations to highlight human-complementing information. We validate the effectiveness of the framework and ILIV-SHAP through a study of human-AI decision-making. We show that our measure of complementary information can be used to identify which AI model will best complement human decisions. We also find that presenting ILIV-SHAP with AI predictions leads to reliably greater reductions in error over non-AI assisted decisions more than vanilla SHAP.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Individual Experience to Collective Evidence: A Reporting-Based Framework for Identifying Systemic Harms</title>
<link>https://arxiv.org/abs/2502.08166</link>
<guid>https://arxiv.org/abs/2502.08166</guid>
<content:encoded><![CDATA[
arXiv:2502.08166v2 Announce Type: replace-cross 
Abstract: When an individual reports a negative interaction with some system, how can their personal experience be contextualized within broader patterns of system behavior? We study the reporting database problem, where individual reports of adverse events arrive sequentially, and are aggregated over time. In this work, our goal is to identify whether there are subgroups--defined by any combination of relevant features--that are disproportionately likely to experience harmful interactions with the system. We formalize this problem as a sequential hypothesis test, and identify conditions on reporting behavior that are sufficient for making inferences about disparities in true rates of harm across subgroups. We show that algorithms for sequential hypothesis tests can be applied to this problem with a standard multiple testing correction. We then demonstrate our method on real-world datasets, including mortgage decisions and vaccine side effects; on each, our method (re-)identifies subgroups known to experience disproportionate harm using only a fraction of the data that was initially used to discover them.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Mental Health: Exploring College Students' Well-being through Smartphone Behaviors</title>
<link>https://arxiv.org/abs/2502.08766</link>
<guid>https://arxiv.org/abs/2502.08766</guid>
<content:encoded><![CDATA[
arXiv:2502.08766v2 Announce Type: replace-cross 
Abstract: The global mental health crisis is a pressing concern, with college students particularly vulnerable to rising mental health disorders. The widespread use of smartphones among young adults, while offering numerous benefits, has also been linked to negative outcomes such as addiction and regret, significantly impacting well-being. Leveraging the longest longitudinal dataset collected over four college years through passive mobile sensing, this study is the first to examine the relationship between students' smartphone unlocking behaviors and their mental health at scale in real-world settings. We provide the first evidence demonstrating the predictability of phone unlocking behaviors for mental health outcomes based on a large dataset, highlighting the potential of these novel features for future predictive models. Our findings reveal important variations in smartphone usage across genders and locations, offering a deeper understanding of the interplay between digital behaviors and mental health. We highlight future research directions aimed at mitigating adverse effects and promoting digital well-being in this population.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellFlux: Simulating Cellular Morphology Changes via Flow Matching</title>
<link>https://arxiv.org/abs/2502.09775</link>
<guid>https://arxiv.org/abs/2502.09775</guid>
<content:encoded><![CDATA[
arXiv:2502.09775v2 Announce Type: replace-cross 
Abstract: Building a virtual cell capable of accurately simulating cellular behaviors in silico has long been a dream in computational biology. We introduce CellFlux, an image-generative model that simulates cellular morphology changes induced by chemical and genetic perturbations using flow matching. Unlike prior methods, CellFlux models distribution-wise transformations from unperturbed to perturbed cell states, effectively distinguishing actual perturbation effects from experimental artifacts such as batch effects -- a major challenge in biological data. Evaluated on chemical (BBBC021), genetic (RxRx1), and combined perturbation (JUMP) datasets, CellFlux generates biologically meaningful cell images that faithfully capture perturbation-specific morphological changes, achieving a 35% improvement in FID scores and a 12% increase in mode-of-action prediction accuracy over existing methods. Additionally, CellFlux enables continuous interpolation between cellular states, providing a potential tool for studying perturbation dynamics. These capabilities mark a significant step toward realizing virtual cell modeling for biomedical research. Project page: https://yuhui-zh15.github.io/CellFlux/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Refined Analysis of UCBVI</title>
<link>https://arxiv.org/abs/2502.17370</link>
<guid>https://arxiv.org/abs/2502.17370</guid>
<content:encoded><![CDATA[
arXiv:2502.17370v2 Announce Type: replace-cross 
Abstract: In this work, we provide a refined analysis of the UCBVI algorithm (Azar et al., 2017), improving both the bonus terms and the regret analysis. Additionally, we compare our version of UCBVI with both its original version and the state-of-the-art MVP algorithm. Our empirical validation demonstrates that improving the multiplicative constants in the bounds has significant positive effects on the empirical performance of the algorithms.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Neuro-Symbolic Decision Transformer</title>
<link>https://arxiv.org/abs/2503.07148</link>
<guid>https://arxiv.org/abs/2503.07148</guid>
<content:encoded><![CDATA[
arXiv:2503.07148v3 Announce Type: replace-cross 
Abstract: We present a hierarchical neuro-symbolic control framework that tightly couples a classical symbolic planner with a transformer-based policy to address long-horizon decision-making under uncertainty. At the high level, the planner assembles an interpretable sequence of operators that guarantees logical coherence with task constraints, while at the low level each operator is rendered as a sub-goal token that conditions a decision transformer to generate fine-grained actions directly from raw observations. This bidirectional interface preserves the combinatorial efficiency and explainability of symbolic reasoning without sacrificing the adaptability of deep sequence models, and it permits a principled analysis that tracks how approximation errors from both planning and execution accumulate across the hierarchy. Empirical studies in stochastic grid-world domains demonstrate that the proposed method consistently surpasses purely symbolic, purely neural and existing hierarchical baselines in both success and efficiency, highlighting its robustness for sequential tasks.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Text to Image in Diffusion Models is Easier Than You Think</title>
<link>https://arxiv.org/abs/2503.08250</link>
<guid>https://arxiv.org/abs/2503.08250</guid>
<content:encoded><![CDATA[
arXiv:2503.08250v4 Announce Type: replace-cross 
Abstract: While recent advancements in generative modeling have significantly improved text-image alignment, some residual misalignment between text and image representations still remains. Some approaches address this issue by fine-tuning models in terms of preference optimization, etc., which require tailored datasets. Orthogonal to these methods, we revisit the challenge from the perspective of representation alignment-an approach that has gained popularity with the success of REPresentation Alignment (REPA). We first argue that conventional text-to-image (T2I) diffusion models, typically trained on paired image and text data (i.e., positive pairs) by minimizing score matching or flow matching losses, is suboptimal from the standpoint of representation alignment. Instead, a better alignment can be achieved through contrastive learning that leverages existing dataset as both positive and negative pairs. To enable efficient alignment with pretrained models, we propose SoftREPA- a lightweight contrastive fine-tuning strategy that leverages soft text tokens for representation alignment. This approach improves alignment with minimal computational overhead by adding fewer than 1M trainable parameters to the pretrained model. Our theoretical analysis demonstrates that our method explicitly increases the mutual information between text and image representations, leading to enhanced semantic consistency. Experimental results across text-to-image generation and text-guided image editing tasks validate the effectiveness of our approach in improving the semantic consistency of T2I generative models.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Cascade Ranking as One Network</title>
<link>https://arxiv.org/abs/2503.09492</link>
<guid>https://arxiv.org/abs/2503.09492</guid>
<content:encoded><![CDATA[
arXiv:2503.09492v2 Announce Type: replace-cross 
Abstract: Cascade Ranking is a prevalent architecture in large-scale top-k selection systems like recommendation and advertising platforms. Traditional training methods focus on single-stage optimization, neglecting interactions between stages. Recent advances have introduced interaction-aware training paradigms, but still struggle to 1) align training objectives with the goal of the entire cascade ranking (i.e., end-to-end recall of ground-truth items) and 2) learn effective collaboration patterns for different stages. To address these challenges, we propose LCRON, which introduces a novel surrogate loss function derived from the lower bound probability that ground truth items are selected by cascade ranking, ensuring alignment with the overall objective of the system. According to the properties of the derived bound, we further design an auxiliary loss for each stage to drive the reduction of this bound, leading to a more robust and effective top-k selection. LCRON enables end-to-end training of the entire cascade ranking system as a unified network. Experimental results demonstrate that LCRON achieves significant improvement over existing methods on public benchmarks and industrial applications, addressing key limitations in cascade ranking training and significantly enhancing system performance.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2503.12941</link>
<guid>https://arxiv.org/abs/2503.12941</guid>
<content:encoded><![CDATA[
arXiv:2503.12941v2 Announce Type: replace-cross 
Abstract: Instruction tuning is widely used to improve a pre-trained Multimodal Large Language Model (MLLM) by training it on curated task-specific datasets, enabling better comprehension of human instructions. However, it is infeasible to collect all possible instruction datasets simultaneously in real-world scenarios. Thus, enabling MLLM with continual instruction tuning is essential for maintaining their adaptability. However, existing methods often trade off memory efficiency for performance gains, significantly compromising overall efficiency. In this paper, we propose a task-specific expansion and task-general fusion framework based on the variations in Centered Kernel Alignment (CKA) similarity across different model layers when trained on diverse datasets. Furthermore, we analyze the information leakage present in the existing benchmark and propose a new and more challenging benchmark to rationally evaluate the performance of different methods. Comprehensive experiments showcase a significant performance improvement of our method compared to existing state-of-the-art methods. Code and dataset are released at https://github.com/Ghy0501/HiDe-LLaVA.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article</title>
<link>https://arxiv.org/abs/2503.16561</link>
<guid>https://arxiv.org/abs/2503.16561</guid>
<content:encoded><![CDATA[
arXiv:2503.16561v2 Announce Type: replace-cross 
Abstract: The future work section of a scientific article outlines potential research directions by identifying gaps and limitations of a current study. This section serves as a valuable resource for early-career researchers seeking unexplored areas and experienced researchers looking for new projects or collaborations. In this study, we generate future work suggestions from key sections of a scientific article alongside related papers and analyze how the trends have evolved. We experimented with various Large Language Models (LLMs) and integrated Retrieval-Augmented Generation (RAG) to enhance the generation process. We incorporate a LLM feedback mechanism to improve the quality of the generated content and propose an LLM-as-a-judge approach for evaluation. Our results demonstrated that the RAG-based approach with LLM feedback outperforms other methods evaluated through qualitative and quantitative metrics. Moreover, we conduct a human evaluation to assess the LLM as an extractor and judge. The code and dataset for this project are here, code: HuggingFace
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach</title>
<link>https://arxiv.org/abs/2503.18085</link>
<guid>https://arxiv.org/abs/2503.18085</guid>
<content:encoded><![CDATA[
arXiv:2503.18085v2 Announce Type: replace-cross 
Abstract: Temporal information extraction from unstructured text is essential for contextualizing events and deriving actionable insights, particularly in the medical domain. We address the task of extracting clinical events and their temporal relations using the well-studied I2B2 2012 Temporal Relations Challenge corpus. This task is inherently challenging due to complex clinical language, long documents, and sparse annotations. We introduce GRAPHTREX, a novel method integrating span-based entity-relation extraction, clinical large pre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT) to capture local and global dependencies. Our HGT component facilitates information propagation across the document through innovative global landmarks that bridge distant entities. Our method improves the state-of-the-art with 5.5% improvement in the tempeval $F_1$ score over the previous best and up to 8.9% improvement on long-range relations, which presents a formidable challenge. We further demonstrate generalizability by establishing a strong baseline on the E3C corpus. This work not only advances temporal information extraction but also lays the groundwork for improved diagnostic and prognostic models through enhanced temporal reasoning.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Framing Analysis of News</title>
<link>https://arxiv.org/abs/2503.20960</link>
<guid>https://arxiv.org/abs/2503.20960</guid>
<content:encoded><![CDATA[
arXiv:2503.20960v3 Announce Type: replace-cross 
Abstract: Automated frame analysis of political communication is a popular task in computational social science that is used to study how authors select aspects of a topic to frame its reception. So far, such studies have been narrow, in that they use a fixed set of pre-defined frames and focus only on the text, ignoring the visual contexts in which those texts appear. Especially for framing in the news, this leaves out valuable information about editorial choices, which include not just the written article but also accompanying photographs. To overcome such limitations, we present a method for conducting multi-modal, multi-label framing analysis at scale using large (vision-) language models. Grounding our work in framing theory, we extract latent meaning embedded in images used to convey a certain point and contrast that to the text by comparing the respective frames used. We also identify highly partisan framing of topics with issue-specific frame analysis found in prior qualitative work. We demonstrate a method for doing scalable integrative framing analysis of both text and image in news, providing a more complete picture for understanding media bias.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data</title>
<link>https://arxiv.org/abs/2503.24129</link>
<guid>https://arxiv.org/abs/2503.24129</guid>
<content:encoded><![CDATA[
arXiv:2503.24129v2 Announce Type: replace-cross 
Abstract: The platonic representation hypothesis suggests that vision and language embeddings become more homogeneous as model and dataset sizes increase. In particular, pairwise distances within each modality become more similar. This suggests that as foundation models mature, it may become possible to match vision and language embeddings in a fully unsupervised fashion, i.e. without parallel data. We present the first feasibility study, and investigate conformity of existing vision and language foundation models in the context of unsupervised, or "blind", matching. First, we formulate unsupervised matching as a quadratic assignment problem and introduce a novel heuristic that outperforms previous solvers. We also develop a technique to find optimal matching problems, for which a non-trivial match is very likely. Second, we conduct an extensive study deploying a range of vision and language models on four datasets. Our analysis reveals that for many problem instances, vision and language representations can be indeed matched without supervision. This finding opens up the exciting possibility of embedding semantic knowledge into other modalities virtually annotation-free. As a proof of concept, we showcase an unsupervised classifier, which achieves non-trivial classification accuracy without any image-text annotation.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Knowledgeable Self-awareness</title>
<link>https://arxiv.org/abs/2504.03553</link>
<guid>https://arxiv.org/abs/2504.03553</guid>
<content:encoded><![CDATA[
arXiv:2504.03553v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>am-ELO: A Stable Framework for Arena-based LLM Evaluation</title>
<link>https://arxiv.org/abs/2505.03475</link>
<guid>https://arxiv.org/abs/2505.03475</guid>
<content:encoded><![CDATA[
arXiv:2505.03475v2 Announce Type: replace-cross 
Abstract: Arena-based evaluation is a fundamental yet significant evaluation paradigm for modern AI models, especially large language models (LLMs). Existing framework based on ELO rating system suffers from the inevitable instability problem due to ranking inconsistency and the lack of attention to the varying abilities of annotators. In this paper, we introduce a novel stable arena framework to address these issues by enhancing the ELO Rating System. Specifically, we replace the iterative update method with a Maximum Likelihood Estimation (MLE) approach, m-ELO, and provide theoretical proof of the consistency and stability of the MLE approach for model ranking. Additionally, we proposed the am-ELO, which modify the Elo Rating's probability function to incorporate annotator abilities, enabling the simultaneous estimation of model scores and annotator reliability. Experiments demonstrate that this method ensures stability, proving that this framework offers a more robust, accurate, and stable evaluation method for LLMs.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay</title>
<link>https://arxiv.org/abs/2505.04209</link>
<guid>https://arxiv.org/abs/2505.04209</guid>
<content:encoded><![CDATA[
arXiv:2505.04209v2 Announce Type: replace-cross 
Abstract: E-commerce sellers are recommended keyphrases based on their inventory on which they advertise to increase buyer engagement (clicks/sales). The relevance of advertiser keyphrases plays an important role in preventing the inundation of search systems with numerous irrelevant items that compete for attention in auctions, in addition to maintaining a healthy seller perception. In this work, we describe the shortcomings of training Advertiser keyphrase relevance filter models on click/sales/search relevance signals and the importance of aligning with human judgment, as sellers have the power to adopt or reject said keyphrase recommendations. In this study, we frame Advertiser keyphrase relevance as a complex interaction between 3 dynamical systems -- seller judgment, which influences seller adoption of our product, Advertising, which provides the keyphrases to bid on, and Search, who holds the auctions for the same keyphrases. This study discusses the practicalities of using human judgment via a case study at eBay Advertising and demonstrate that using LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our relevance models achieves a better harmony across the three systems -- provided that they are bound by a meticulous evaluation framework grounded in business metrics.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEXam: Benchmarking Legal Reasoning on 340 Law Exams</title>
<link>https://arxiv.org/abs/2505.12864</link>
<guid>https://arxiv.org/abs/2505.12864</guid>
<content:encoded><![CDATA[
arXiv:2505.12864v2 Announce Type: replace-cross 
Abstract: Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Adopting an LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. Project page: https://lexam-benchmark.github.io/
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning</title>
<link>https://arxiv.org/abs/2505.14020</link>
<guid>https://arxiv.org/abs/2505.14020</guid>
<content:encoded><![CDATA[
arXiv:2505.14020v2 Announce Type: replace-cross 
Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs (KGs), incorporate the temporal feature to express the transience of knowledge by describing when facts occur. TKG extrapolation aims to infer possible future facts based on known history, which has garnered significant attention in recent years. Some existing methods treat TKG as a sequence of independent subgraphs to model temporal evolution patterns, demonstrating impressive reasoning performance. However, they still have limitations: 1) In modeling subgraph semantic evolution, they usually neglect the internal structural interactions between subgraphs, which are actually crucial for encoding TKGs. 2) They overlook the potential smooth features that do not lead to semantic changes, which should be distinguished from the semantic evolution process. Therefore, we propose a novel Disentangled Multi-span Evolutionary Network (DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution strategy that captures local neighbor features while perceiving historical neighbor semantic information, thus enabling internal interactions between subgraphs during the evolution process. To maximize the capture of semantic change patterns, we design a disentangle component that adaptively separates nodes' active and stable features, used to dynamically control the influence of historical semantics on future evolution. Extensive experiments conducted on four real-world TKG datasets show that DiMNet demonstrates substantial performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7% in MRR.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network</title>
<link>https://arxiv.org/abs/2505.16223</link>
<guid>https://arxiv.org/abs/2505.16223</guid>
<content:encoded><![CDATA[
arXiv:2505.16223v2 Announce Type: replace-cross 
Abstract: In this paper, we propose MADCluster, a novel model-agnostic anomaly detection framework utilizing self-supervised clustering. MADCluster is applicable to various deep learning architectures and addresses the 'hypersphere collapse' problem inherent in existing deep learning-based anomaly detection methods. The core idea is to cluster normal pattern data into a 'single cluster' while simultaneously learning the cluster center and mapping data close to this center. Also, to improve expressiveness and enable effective single clustering, we propose a new 'One-directed Adaptive loss'. The optimization of this loss is mathematically proven. MADCluster consists of three main components: Base Embedder capturing high-dimensional temporal dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous center updates. Its model-agnostic characteristics are achieved by applying various architectures to the Base Embedder. Experiments on four time series benchmark datasets demonstrate that applying MADCluster improves the overall performance of comparative models. In conclusion, the compatibility of MADCluster shows potential for enhancing model performance across various architectures.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary</title>
<link>https://arxiv.org/abs/2505.18325</link>
<guid>https://arxiv.org/abs/2505.18325</guid>
<content:encoded><![CDATA[
arXiv:2505.18325v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they often refuse to answer legitimate queries-a phenomenon known as overrefusal. Overrefusal typically stems from over-conservative safety alignment, causing models to treat many reasonable prompts as potentially risky. To systematically understand this issue, we probe and leverage the models'safety decision boundaries to analyze and mitigate overrefusal. Our findings reveal that overrefusal is closely tied to misalignment at these boundary regions, where models struggle to distinguish subtle differences between benign and harmful content. Building on these insights, we present RASS, an automated framework for prompt generation and selection that strategically targets overrefusal prompts near the safety boundary. By harnessing steering vectors in the representation space, RASS efficiently identifies and curates boundary-aligned prompts, enabling more effective and targeted mitigation of overrefusal. This approach not only provides a more precise and interpretable view of model safety decisions but also seamlessly extends to multilingual scenarios.We have explored the safety decision boundaries of various LLMs and construct the MORBench evaluation set to facilitate robust assessment of model safety and helpfulness across multiple languages. Code and datasets will be released at https://anonymous.4open.science/r/RASS-80D3.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SP2RINT: Spatially-Decoupled Physics-Inspired Progressive Inverse Optimization for Scalable, PDE-Constrained Meta-Optical Neural Network Training</title>
<link>https://arxiv.org/abs/2505.18377</link>
<guid>https://arxiv.org/abs/2505.18377</guid>
<content:encoded><![CDATA[
arXiv:2505.18377v2 Announce Type: replace-cross 
Abstract: DONNs leverage light propagation for efficient analog AI and signal processing. Advances in nanophotonic fabrication and metasurface-based wavefront engineering have opened new pathways to realize high-capacity DONNs across various spectral regimes. Training such DONN systems to determine the metasurface structures remains challenging. Heuristic methods are fast but oversimplify metasurfaces modulation, often resulting in physically unrealizable designs and significant performance degradation. Simulation-in-the-loop optimizes implementable metasurfaces via adjoint methods, but is computationally prohibitive and unscalable. To address these limitations, we propose SP2RINT, a spatially decoupled, progressive training framework that formulates DONN training as a PDE-constrained learning problem. Metasurface responses are first relaxed into freely trainable transfer matrices with a banded structure. We then progressively enforce physical constraints by alternating between transfer matrix training and adjoint-based inverse design, avoiding per-iteration PDE solves while ensuring final physical realizability. To further reduce runtime, we introduce a physics-inspired, spatially decoupled inverse design strategy based on the natural locality of field interactions. This approach partitions the metasurface into independently solvable patches, enabling scalable and parallel inverse design with system-level calibration. Evaluated across diverse DONN training tasks, SP2RINT achieves digital-comparable accuracy while being 1825 times faster than simulation-in-the-loop approaches. By bridging the gap between abstract DONN models and implementable photonic hardware, SP2RINT enables scalable, high-performance training of physically realizable meta-optical neural systems. Our code is available at https://github.com/ScopeX-ASU/SP2RINT
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Human and AI Rater Effects Using the Many-Facet Rasch Model</title>
<link>https://arxiv.org/abs/2505.18486</link>
<guid>https://arxiv.org/abs/2505.18486</guid>
<content:encoded><![CDATA[
arXiv:2505.18486v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been widely explored for automated scoring in low-stakes assessment to facilitate learning and instruction. Empirical evidence related to which LLM produces the most reliable scores and induces least rater effects needs to be collected before the use of LLMs for automated scoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4, ChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini 2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in scoring two types of writing tasks. The accuracy of the holistic and analytic scores from LLMs compared with human raters was evaluated in terms of Quadratic Weighted Kappa. Intra-rater consistency across prompts was compared in terms of Cronbach Alpha. Rater effects of LLMs were evaluated and compared with human raters using the Many-Facet Rasch model. The results in general supported the use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring accuracy, better rater reliability, and less rater effects.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations</title>
<link>https://arxiv.org/abs/2505.19164</link>
<guid>https://arxiv.org/abs/2505.19164</guid>
<content:encoded><![CDATA[
arXiv:2505.19164v2 Announce Type: replace-cross 
Abstract: In the domain of sponsored search advertising, the focus of Keyphrase recommendation has largely been on exact match types, which pose issues such as high management expenses, limited targeting scope, and evolving search query patterns. Alternatives like Broad match types can alleviate certain drawbacks of exact matches but present challenges like poor targeting accuracy and minimal supervisory signals owing to limited advertiser usage. This research defines the criteria for an ideal broad match, emphasizing on both efficiency and effectiveness, ensuring that a significant portion of matched queries are relevant. We propose BroadGen, an innovative framework that recommends efficient and effective broad match keyphrases by utilizing historical search query data. Additionally, we demonstrate that BroadGen, through token correspondence modeling, maintains better query stability over time. BroadGen's capabilities allow it to serve daily, millions of sellers at eBay with over 2.3 billion items.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change</title>
<link>https://arxiv.org/abs/2505.19328</link>
<guid>https://arxiv.org/abs/2505.19328</guid>
<content:encoded><![CDATA[
arXiv:2505.19328v2 Announce Type: replace-cross 
Abstract: Recognizing complex emotions linked to ambivalence and hesitancy (A/H) can play a critical role in the personalization and effectiveness of digital behaviour change interventions. These subtle and conflicting emotions are manifested by a discord between multiple modalities, such as facial and vocal expressions, and body language. Although experts can be trained to identify A/H, integrating them into digital interventions is costly and less effective. Automatic learning systems provide a cost-effective alternative that can adapt to individual users, and operate seamlessly within real-time, and resource-limited environments. However, there are currently no datasets available for the design of ML models to recognize A/H. This paper introduces a first Behavioural Ambivalence/Hesitancy (BAH) dataset collected for subject-based multimodal recognition of A/H in videos. It contains videos from 224 participants captured across 9 provinces in Canada, with different age, and ethnicity. Through our web platform, we recruited participants to answer 7 questions, some of which were designed to elicit A/H while recording themselves via webcam with microphone. BAH amounts to 1,118 videos for a total duration of 8.26 hours with 1.5 hours of A/H. Our behavioural team annotated timestamp segments to indicate where A/H occurs, and provide frame- and video-level annotations with the A/H cues. Video transcripts and their timestamps are also included, along with cropped and aligned faces in each frame, and a variety of participants meta-data. We include results baselines for BAH at frame- and video-level recognition in multi-modal setups, in addition to zero-shot prediction, and for personalization using unsupervised domain adaptation. The limited performance of baseline models highlights the challenges of recognizing A/H in real-world videos. The data, code, and pretrained weights are available.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization</title>
<link>https://arxiv.org/abs/2505.19205</link>
<guid>https://arxiv.org/abs/2505.19205</guid>
<content:encoded><![CDATA[
<div> Google, hyperparameter optimization, OptiMindTune, multi-agent framework, machine learning

Summary: 
The paper presents OptiMindTune, a multi-agent framework utilizing three specialized AI agents - Recommender Agent, Evaluator Agent, and Decision Agent - powered by Google's Gemini models for hyperparameter optimization. By enabling collaborative intelligence among these agents, OptiMindTune aims to efficiently optimize hyperparameters by addressing various aspects of the HPO problem such as model selection, hyperparameter suggestions, evaluation, and decision-making. The framework integrates principles from advanced large language models and adaptive search to achieve scalable and intelligent AutoML. It aims to converge to optimal hyperparameter configurations quickly and robustly compared to traditional methods. The multi-agent paradigm opens up new possibilities for tackling the complexities of modern machine learning model tuning. <div>
arXiv:2505.19205v2 Announce Type: replace 
Abstract: Hyperparameter optimization (HPO) is a critical yet challenging aspect of machine learning model development, significantly impacting model performance and generalization. Traditional HPO methods often struggle with high dimensionality, complex interdependencies, and computational expense. This paper introduces OptiMindTune, a novel multi-agent framework designed to intelligently and efficiently optimize hyperparameters. OptiMindTune leverages the collaborative intelligence of three specialized AI agents -- a Recommender Agent, an Evaluator Agent, and a Decision Agent -- each powered by Google's Gemini models. These agents address distinct facets of the HPO problem, from model selection and hyperparameter suggestion to robust evaluation and strategic decision-making. By fostering dynamic interactions and knowledge sharing, OptiMindTune aims to converge to optimal hyperparameter configurations more rapidly and robustly than existing single-agent or monolithic approaches. Our framework integrates principles from advanced large language models, and adaptive search to achieve scalable and intelligent AutoML. We posit that this multi-agent paradigm offers a promising avenue for tackling the increasing complexity of modern machine learning model tuning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use</title>
<link>https://arxiv.org/abs/2505.19255</link>
<guid>https://arxiv.org/abs/2505.19255</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning Finetuning, large language models, multimodal reasoning, Visual Sketchpad, VTool-R1 <br />
Summary: 
The article introduces VTool-R1, a framework that enhances the reasoning capabilities of large language models (LLMs) by incorporating visual reasoning steps. By interleaving text and visual steps during training, VTool-R1 enables LLMs to generate multimodal chains of thought. This approach integrates Python-based visual editing tools into the Reinforcement Learning Finetuning process, allowing LLMs to strategically utilize visual reasoning in their responses. The framework is trained using outcome-based rewards tied to task accuracy, enabling LLMs to "think with images" and improve reasoning performance. Experiments conducted on structured visual question answering tasks demonstrate that VTool-R1 significantly enhances the ability of LLMs to generate multimodal chains of thoughts by utilizing visual reasoning tools effectively. <div>
arXiv:2505.19255v2 Announce Type: replace 
Abstract: Reinforcement Learning Finetuning (RFT) has significantly advanced the reasoning capabilities of large language models (LLMs) by enabling long chains of thought, self-correction, and effective tool use. While recent works attempt to extend RFT to vision-language models (VLMs), these efforts largely produce text-only reasoning conditioned on static image inputs, falling short of true multimodal reasoning in the response. In contrast, test-time methods like Visual Sketchpad incorporate visual steps but lack training mechanisms.
  We introduce VTool-R1, the first framework that trains VLMs to generate multimodal chains of thought by interleaving text and intermediate visual reasoning steps. VTool-R1 integrates Python-based visual editing tools into the RFT process, enabling VLMs to learn when and how to generate visual reasoning steps that benefit final reasoning. Trained with outcome-based rewards tied to task accuracy, our approach elicits strategic visual tool use for reasoning without relying on process-based supervision. Experiments on structured visual question answering over charts and tables show that VTool-R1 enhances reasoning performance by teaching VLMs to "think with images" and generate multimodal chain of thoughts with tools.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Large Reasoning Models for Agriculture</title>
<link>https://arxiv.org/abs/2505.19259</link>
<guid>https://arxiv.org/abs/2505.19259</guid>
<content:encoded><![CDATA[
<div> Keywords: Agricultural reasoning, Large reasoning models, AgReason, AgThoughts, AgThinker

Summary:<br /><br />Agricultural decision-making is a complex process that relies on various factors such as geography, climate, and economics. Traditional large language models struggle to navigate this complexity, leading researchers to explore the use of large reasoning models. The AgReason project introduces an expert-curated benchmark with 100 questions for agricultural reasoning, showing that large reasoning models outperform conventional ones. The AgThoughts dataset, with 44.6K question-answer pairs, is presented to improve agricultural reasoning abilities in language models. The AgThinker suite of small reasoning models, designed for consumer-grade GPUs, demonstrates the effectiveness of the AgThoughts dataset in enhancing agricultural reasoning capabilities in language models. This research highlights the potential of using large reasoning models in agricultural decision-making processes. <br /><br />Summary: <div>
arXiv:2505.19259v2 Announce Type: replace 
Abstract: Agricultural decision-making involves complex, context-specific reasoning, where choices about crops, practices, and interventions depend heavily on geographic, climatic, and economic conditions. Traditional large language models (LLMs) often fall short in navigating this nuanced problem due to limited reasoning capacity. We hypothesize that recent advances in large reasoning models (LRMs) can better handle such structured, domain-specific inference. To investigate this, we introduce AgReason, the first expert-curated open-ended science benchmark with 100 questions for agricultural reasoning. Evaluations across thirteen open-source and proprietary models reveal that LRMs outperform conventional ones, though notable challenges persist, with the strongest Gemini-based baseline achieving 36% accuracy. We also present AgThoughts, a large-scale dataset of 44.6K question-answer pairs generated with human oversight and equipped with synthetically generated reasoning traces. Using AgThoughts, we develop AgThinker, a suite of small reasoning models that can be run on consumer-grade GPUs, and show that our dataset can be effective in unlocking agricultural reasoning abilities in LLMs. Our project page is here: https://baskargroup.github.io/Ag_reasoning/
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.19698</link>
<guid>https://arxiv.org/abs/2505.19698</guid>
<content:encoded><![CDATA[
<div> Keywords: model-based reinforcement learning, diffusion world models, Atari100k benchmark, Joint Embedding Diffusion, human-level performance

Summary:
Recent advancements in model-based reinforcement learning have led to super-human performance on the Atari100k benchmark. However, there exists a performance asymmetry where MBRL agents excel in some tasks but underperform in others, skewing aggregate metrics. Pixel-based agents trained with diffusion world models exhibit this asymmetry more prominently. To address this issue, tasks are categorized as Agent-Optimal or Human-Optimal, highlighting the need for balanced metrics. The lack of temporally-structured latent space in pixel-based methods is identified as a contributor to the asymmetry. A new model, Joint Embedding Diffusion (JEDI), is proposed to tackle this problem. JEDI outperforms current models in human-optimal tasks, remains competitive in the Atari100k benchmark, and boasts improved efficiency in terms of speed and memory usage. This work challenges the traditional definition of crossing human-level performance in the Atari100k domain.<br /><br />Summary: <div>
arXiv:2505.19698v2 Announce Type: replace 
Abstract: Recent advances in model-based reinforcement learning (MBRL) have achieved super-human level performance on the Atari100k benchmark, driven by reinforcement learning agents trained on powerful diffusion world models. However, we identify that the current aggregates mask a major performance asymmetry: MBRL agents dramatically outperform humans in some tasks despite drastically underperforming in others, with the former inflating the aggregate metrics. This is especially pronounced in pixel-based agents trained with diffusion world models. In this work, we address the pronounced asymmetry observed in pixel-based agents as an initial attempt to reverse the worrying upward trend observed in them. We address the problematic aggregates by delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal importance on metrics from both sets. Next, we hypothesize this pronounced asymmetry is due to the lack of temporally-structured latent space trained with the World Model objective in pixel-based methods. Lastly, to address this issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion world model trained end-to-end with the self-consistency objective. JEDI outperforms SOTA models in human-optimal tasks while staying competitive across the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the latest pixel-based diffusion baseline. Overall, our work rethinks what it truly means to cross human-level performance in Atari100k.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning</title>
<link>https://arxiv.org/abs/2505.20135</link>
<guid>https://arxiv.org/abs/2505.20135</guid>
<content:encoded><![CDATA[
<div> Keywords: Replay-based continual learning, memory buffer, dataset distillation, global information, forgetting mitigation

Summary:
Replay-based continual learning methods often struggle with consolidating past knowledge due to limited memory buffer capacity and heuristic data selection. To address this challenge, a new dataset distillation framework for continual learning is proposed in this study. The framework employs a learnable memory buffer to distill global information from current task data and accumulated knowledge from the previous memory buffer. A lightweight distillation module is introduced to generate soft labels for memory buffer data, minimizing computational overhead and overfitting risks. Experimental results demonstrate that the proposed method achieves competitive performance and effectively prevents forgetting across various datasets. The source code for the method will be made publicly available. <br /><br />Summary: <div>
arXiv:2505.20135v2 Announce Type: replace 
Abstract: Replay-based continual learning (CL) methods assume that models trained on a small subset can also effectively minimize the empirical risk of the complete dataset. These methods maintain a memory buffer that stores a sampled subset of data from previous tasks to consolidate past knowledge. However, this assumption is not guaranteed in practice due to the limited capacity of the memory buffer and the heuristic criteria used for buffer data selection. To address this issue, we propose a new dataset distillation framework tailored for CL, which maintains a learnable memory buffer to distill the global information from the current task data and accumulated knowledge preserved in the previous memory buffer. Moreover, to avoid the computational overhead and overfitting risks associated with parameterizing the entire buffer during distillation, we introduce a lightweight distillation module that can achieve global information distillation solely by generating learnable soft labels for the memory buffer data. Extensive experiments show that, our method can achieve competitive results and effectively mitigates forgetting across various datasets. The source code will be publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?</title>
<link>https://arxiv.org/abs/2505.19663</link>
<guid>https://arxiv.org/abs/2505.19663</guid>
<content:encoded><![CDATA[
<div> Benchmark, deep learning, audio watermarking, neural compression, audio attacks  
Summary:  
- The article introduces the Robust Audio Watermarking Benchmark (RAW-Bench) for evaluating deep learning-based audio watermarking methods with standardized comparisons.  
- A comprehensive audio attack pipeline is introduced to simulate real-world usage, including various distortions like compression, background noise, and reverberation.  
- Four existing watermarking methods were evaluated on RAW-bench, revealing challenges posed by neural compression techniques and the importance of training with audio attacks for improved robustness.  
- Specific distortions such as polarity inversion, time stretching, or reverb were found to significantly affect certain methods.  
- The evaluation framework is accessible on github.com/SonyResearch/raw_bench.  

<br /><br />Summary: <div>
arXiv:2505.19663v2 Announce Type: replace-cross 
Abstract: We introduce the Robust Audio Watermarking Benchmark (RAW-Bench), a benchmark for evaluating deep learning-based audio watermarking methods with standardized and systematic comparisons. To simulate real-world usage, we introduce a comprehensive audio attack pipeline with various distortions such as compression, background noise, and reverberation, along with a diverse test dataset including speech, environmental sounds, and music recordings. Evaluating four existing watermarking methods on RAW-bench reveals two main insights: (i) neural compression techniques pose the most significant challenge, even when algorithms are trained with such compressions; and (ii) training with audio attacks generally improves robustness, although it is insufficient in some cases. Furthermore, we find that specific distortions, such as polarity inversion, time stretching, or reverb, seriously affect certain methods. The evaluation framework is accessible at github.com/SonyResearch/raw_bench.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows</title>
<link>https://arxiv.org/abs/2505.21512</link>
<guid>https://arxiv.org/abs/2505.21512</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge graphs, Large language models, KG exploration, User trust, Visualization

Summary:<br /><br />
The study explores the interaction between users and knowledge graphs (KGs) using large language models (LLMs) for exploration. The LinkQ system was developed to convert natural language questions into structured queries with LLMs. Visual mechanisms were designed to help users assess the accuracy of KG queries and LLM responses. Findings from a qualitative evaluation with practitioners revealed that users tended to overtrust the system's outputs, even when incorrect, due to the helpful visualizations. Users showed distinct workflows based on their familiarity with KGs and LLMs, challenging the assumption of one-size-fits-all systems. The study highlights the risks of false trust in LLM-assisted tools and emphasizes the need for further research on visualization as a mitigation strategy. <div>
arXiv:2505.21512v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) are powerful data structures, but exploring them effectively remains difficult for even expert users. Large language models (LLMs) are increasingly used to address this gap, yet little is known empirically about how their usage with KGs shapes user trust, exploration strategies, or downstream decision-making - raising key design challenges for LLM-based KG visual analysis systems. To study these effects, we developed LinkQ, a KG exploration system that converts natural language questions into structured queries with an LLM. We collaborated with KG experts to design five visual mechanisms that help users assess the accuracy of both KG queries and LLM responses: an LLM-KG state diagram that illustrates which stage of the exploration pipeline LinkQ is in, a query editor displaying the generated query paired with an LLM explanation, an entity-relation ID table showing extracted KG entities and relations with semantic descriptions, a query structure graph that depicts the path traversed in the KG, and an interactive graph visualization of query results. From a qualitative evaluation with 14 practitioners, we found that users - even KG experts - tended to overtrust LinkQ's outputs due to its "helpful" visualizations, even when the LLM was incorrect. Users exhibited distinct workflows depending on their prior familiarity with KGs and LLMs, challenging the assumption that these systems are one-size-fits-all - despite often being designed as if they are. Our findings highlight the risks of false trust in LLM-assisted data analysis tools and the need for further investigation into the role of visualization as a mitigation technique.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation</title>
<link>https://arxiv.org/abs/2505.21514</link>
<guid>https://arxiv.org/abs/2505.21514</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, coding assistants, benchmark, Java, Python

Summary:<br />
The article introduces SIMCOPILOT, a benchmark that simulates the role of large language models as interactive coding assistants, targeting completion and infill tasks in Java and Python. The benchmark provides a comprehensive framework for evaluating LLM coding capabilities in practical scenarios, addressing critical factors like task-specific performance nuances and contextual understanding. Evaluations across different domains highlight model strengths and challenges in maintaining logical consistency within complex code structures. The study emphasizes the limitations of LLM-driven code generation and the evolving role of LLMs as intelligent software development partners. Overall, the benchmark offers detailed insights into the utility and performance of large language models in coding environments.<br /> <div>
arXiv:2505.21514v1 Announce Type: new 
Abstract: We introduce SIMCOPILOT, a benchmark that simulates the role of large language models (LLMs) as interactive, "copilot"-style coding assistants. Targeting both completion (finishing incomplete methods or code blocks) and infill tasks (filling missing segments within existing code), SIMCOPILOT provides a comprehensive framework for evaluating LLM coding capabilities. The benchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python (SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our key contributions include: (a) establishing a realistic, detailed evaluation environment to assess LLM utility in practical coding scenarios, and (b) providing fine-grained analyses that address critical factors frequently overlooked by existing benchmarks, such as task-specific performance nuances, contextual understanding across code segments, and sensitivity to variable scope. Evaluations conducted across domains-including algorithms, databases, computer vision, and neural networks-offer insights into model strengths and highlight persistent challenges in maintaining logical consistency within complex dependency structures. Beyond benchmarking, our study sheds light on the current limitations of LLM-driven code generation and underscores the ongoing transition of LLMs from merely syntax-aware generators toward reliable, intelligent software development partners.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.21525</link>
<guid>https://arxiv.org/abs/2505.21525</guid>
<content:encoded><![CDATA[
<div> Spatial-temporal feature encoder, Multivariate time series, Domain adaptation, Data privacy, Temporal restoration<br />
<br />
Summary:
The paper introduces a new Source-Free Domain Adaptation (SFDA) method called Temporal Restoration and Spatial Rewiring (TERSE) designed specifically for multivariate time series (MTS) data. Traditional SFDA methods struggle with MTS due to a lack of consideration for spatial correlations inherent in the data. TERSE addresses this challenge by utilizing a customized spatial-temporal feature encoder and tasks for temporal restoration and spatial rewiring to capture and transfer spatial-temporal dependencies across domains. By guiding the target encoder to produce consistent features with the source domain, TERSE facilitates implicit feature alignment. It can also be easily integrated into existing SFDA methods as a plug-and-play module. Extensive experiments on real-world time series datasets demonstrate the effectiveness and versatility of TERSE in preserving data privacy and accurately adapting pre-trained models to unlabelled target domains. <div>
arXiv:2505.21525v1 Announce Type: new 
Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from an annotated source domain to an unlabelled target domain without accessing the source data, thereby preserving data privacy. While existing SFDA methods have proven effective in reducing reliance on source data, they struggle to perform well on multivariate time series (MTS) due to their failure to consider the intrinsic spatial correlations inherent in MTS data. These spatial correlations are crucial for accurately representing MTS data and preserving invariant information across domains. To address this challenge, we propose Temporal Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method tailored for MTS data. Specifically, TERSE comprises a customized spatial-temporal feature encoder designed to capture the underlying spatial-temporal characteristics, coupled with both temporal restoration and spatial rewiring tasks to reinstate latent representations of the temporally masked time series and the spatially masked correlated structures. During the target adaptation phase, the target encoder is guided to produce spatially and temporally consistent features with the source domain by leveraging the source pre-trained temporal restoration and spatial rewiring networks. Therefore, TERSE can effectively model and transfer spatial-temporal dependencies across domains, facilitating implicit feature alignment. In addition, as the first approach to simultaneously consider spatial-temporal consistency in MTS-SFDA, TERSE can also be integrated as a versatile plug-and-play module into established SFDA methods. Extensive experiments on three real-world time series datasets demonstrate the effectiveness and versatility of our approach.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools</title>
<link>https://arxiv.org/abs/2505.21569</link>
<guid>https://arxiv.org/abs/2505.21569</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Chemistry tasks, Agent-stacking, Prediction errors, AI agent applications

Summary: ChemHAS (Chemical Hierarchical Agent Stacking) is a new method proposed in this paper to enhance chemistry tools by optimizing agent-stacking structures using limited data. By leveraging Large Language Model-based agents, ChemHAS aims to reduce prediction errors of chemistry tools, leading to state-of-the-art performance in fundamental chemistry tasks. The method identifies four distinct agent-stacking behaviors, improving interpretability and opening up new possibilities for AI agent applications in scientific research. The code and dataset for ChemHAS are publicly available for further research and development.<br /><br />Summary: <div>
arXiv:2505.21569v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based agents have demonstrated the ability to improve performance in chemistry-related tasks by selecting appropriate tools. However, their effectiveness remains limited by the inherent prediction errors of chemistry tools. In this paper, we take a step further by exploring how LLMbased agents can, in turn, be leveraged to reduce prediction errors of the tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking), a simple yet effective method that enhances chemistry tools through optimizing agent-stacking structures from limited data. ChemHAS achieves state-of-the-art performance across four fundamental chemistry tasks, demonstrating that our method can effectively compensate for prediction errors of the tools. Furthermore, we identify and characterize four distinct agent-stacking behaviors, potentially improving interpretability and revealing new possibilities for AI agent applications in scientific research. Our code and dataset are publicly available at https: //anonymous.4open.science/r/ChemHAS-01E4/README.md.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition</title>
<link>https://arxiv.org/abs/2505.21571</link>
<guid>https://arxiv.org/abs/2505.21571</guid>
<content:encoded><![CDATA[
<div> Keywords: wireless communications, deep learning, Automatic Modulation Recognition, model pruning, channel-level pruning

Summary:<br /><br />
The paper introduces FCOS, a novel Fine-to-Coarse two-Stage pruning framework for Automatic Modulation Recognition (AMR) in wireless communications. FCOS combines channel-level pruning with layer-level collapse diagnosis to achieve extreme compression and high performance while maintaining accuracy. In the first stage, hierarchical clustering and parameter fusion are used for channel-level pruning, followed by a Layer Collapse Diagnosis (LaCD) module to identify and remove collapsed layers. Experimental results show that FCOS outperforms existing methods, achieving a 95.51% reduction in FLOPs and 95.31% reduction in parameters while only experiencing a 0.46% drop in accuracy on Sig2019-12 benchmark. The code is available on GitHub for further exploration.  <div>
arXiv:2505.21571v1 Announce Type: new 
Abstract: With the rapid development of wireless communications and the growing complexity of digital modulation schemes, traditional manual modulation recognition methods struggle to extract reliable signal features and meet real-time requirements in modern scenarios. Recently, deep learning based Automatic Modulation Recognition (AMR) approaches have greatly improved classification accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained devices. Model pruning provides a general approach to reduce model complexity, but existing weight, channel, and layer pruning techniques each present a trade-off between compression rate, hardware acceleration, and accuracy preservation. To this end, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning framework that combines channel-level pruning with layer-level collapse diagnosis to achieve extreme compression, high performance and efficient inference. In the first stage of FCOS, hierarchical clustering and parameter fusion are applied to channel weights to achieve channel-level pruning. Then a Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer collapse and removes the collapsed layers due to high channel compression ratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms existing channel and layer pruning methods. Specifically, FCOS achieves 95.51% FLOPs reduction and 95.31% parameter reduction while still maintaining performance close to the original ResNet56, with only a 0.46% drop in accuracy on Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes</title>
<link>https://arxiv.org/abs/2505.21573</link>
<guid>https://arxiv.org/abs/2505.21573</guid>
<content:encoded><![CDATA[
<div> frequency domain, neural PDE solvers, physics-aware methods, spectral representations, Navier-Stokes equations <br />
Summary: 
The Spectral-inspired Neural Operator (SINO) framework is introduced for learning PDE operators from limited trajectories without known PDE terms. SINO operates in the frequency domain and utilizes a Frequency-to-Vector module for spectral representations. It incorporates a nonlinear operator block with a $\Pi$-Block for modeling nonlinear interactions and introduces operator distillation for efficient inference. SINO outperforms existing methods in PDE benchmarks, showing strong discretization invariance and generalization to out-of-distribution initial conditions. Notably, SINO can accurately simulate globally coupled systems, such as the Navier-Stokes equations, with limited data and no explicit PDE terms present. <div>
arXiv:2505.21573v1 Announce Type: new 
Abstract: Partial differential equations (PDEs) govern the spatiotemporal evolution of various physical systems. Classical numerical solvers, while accurate, require fine discretization and full knowledge of the governing PDEs, limiting their applicability when the physics is unknown or fast inference is required. Data-driven neural PDE solvers alleviate these constraints by learning from data but demand large training datasets and perform poorly in data-scarce regimes. Physics-aware methods mitigate data requirements by incorporating physical knowledge yet rely on known PDE terms or local numerical schemes, restricting their ability to handle unknown or globally coupled systems. In this work, we propose the Spectral-inspired Neural Operator (SINO), a novel framework that learns PDE operators from limited trajectories (as few as 2-5), without any known PDE terms. SINO operates in the frequency domain and introduces a Frequency-to-Vector module to learn spectral representations analogous to derivative multipliers. To model nonlinear physical interactions, we design a nonlinear operator block that includes a $\Pi$-Block with low-pass filtering to prevent aliasing. Finally, we introduce an operator distillation technique to distill the trained model for efficient inference. SINO achieves state-of-the-art results across multiple PDE benchmarks, demonstrating strong discretization invariance and robust generalization to out-of-distribution initial conditions. To our knowledge, SINO is the first physics-aware method capable of accurately simulating globally coupled systems (e.g., the Navier-Stokes equations) from limited data without any explicit PDE terms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concentration Distribution Learning from Label Distributions</title>
<link>https://arxiv.org/abs/2505.21576</link>
<guid>https://arxiv.org/abs/2505.21576</guid>
<content:encoded><![CDATA[
<div> Keywords: Label distribution learning, background concentration, concentration distribution learning, probabilistic methods, neural networks

Summary:<br /><br />The article introduces a new concept called background concentration to enhance label distribution learning (LDL) by incorporating the absolute intensity of each label. By considering the total description degree of hidden labels not in the label space, the proposed concentration distribution learning improves the representation of instances and minimizes information loss. A novel model combining probabilistic methods and neural networks is developed to learn both label distributions and background concentrations from existing LDL datasets. Experimental results demonstrate that the approach effectively extracts background concentrations and yields more accurate prediction results compared to existing LDL methods. The code for the proposed method is publicly available on GitHub for further research and implementation. <div>
arXiv:2505.21576v1 Announce Type: new 
Abstract: Label distribution learning (LDL) is an effective method to predict the relative label description degree (a.k.a. label distribution) of a sample. However, the label distribution is not a complete representation of an instance because it overlooks the absolute intensity of each label. Specifically, it's impossible to obtain the total description degree of hidden labels that not in the label space, which leads to the loss of information and confusion in instances. To solve the above problem, we come up with a new concept named background concentration to serve as the absolute description degree term of the label distribution and introduce it into the LDL process, forming the improved paradigm of concentration distribution learning. Moreover, we propose a novel model by probabilistic methods and neural networks to learn label distributions and background concentrations from existing LDL datasets. Extensive experiments prove that the proposed approach is able to extract background concentrations from label distributions while producing more accurate prediction results than the state-of-the-art LDL methods. The code is available in https://github.com/seutjw/CDL-LD.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness in Federated Learning: Fairness for Whom?</title>
<link>https://arxiv.org/abs/2505.21584</link>
<guid>https://arxiv.org/abs/2505.21584</guid>
<content:encoded><![CDATA[
<div> Keywords: fairness, federated learning, sociotechnical contexts, harms, stakeholders 

Summary: 
This paper discusses the limitations of current approaches to fairness in federated learning, highlighting five key pitfalls. Firstly, fairness is often defined solely through the server-client architecture perspective. Secondly, there is a mismatch between simulated scenarios and real-world contexts. Thirdly, definitions often prioritize protecting the system over protecting its users. Fourthly, interventions target specific stages of the lifecycle, neglecting broader impacts. Lastly, there is a lack of alignment among multiple stakeholders with different fairness definitions. The paper proposes a harm-centered framework to address these issues, connecting fairness definitions to concrete risks and stakeholder vulnerabilities. Recommendations are made for more holistic, context-aware, and accountable fairness research in federated learning. 

<br /><br />Summary: <div>
arXiv:2505.21584v1 Announce Type: new 
Abstract: Fairness in federated learning has emerged as a rapidly growing area of research, with numerous works proposing formal definitions and algorithmic interventions. Yet, despite this technical progress, fairness in FL is often defined and evaluated in ways that abstract away from the sociotechnical contexts in which these systems are deployed. In this paper, we argue that existing approaches tend to optimize narrow system level metrics, such as performance parity or contribution-based rewards, while overlooking how harms arise throughout the FL lifecycle and how they impact diverse stakeholders. We support this claim through a critical analysis of the literature, based on a systematic annotation of papers for their fairness definitions, design decisions, evaluation practices, and motivating use cases. Our analysis reveals five recurring pitfalls: 1) fairness framed solely through the lens of server client architecture, 2) a mismatch between simulations and motivating use-cases and contexts, 3) definitions that conflate protecting the system with protecting its users, 4) interventions that target isolated stages of the lifecycle while neglecting upstream and downstream effects, 5) and a lack of multi-stakeholder alignment where multiple fairness definitions can be relevant at once. Building on these insights, we propose a harm centered framework that links fairness definitions to concrete risks and stakeholder vulnerabilities. We conclude with recommendations for more holistic, context-aware, and accountable fairness research in FL.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.21587</link>
<guid>https://arxiv.org/abs/2505.21587</guid>
<content:encoded><![CDATA[
<div> Cellular Complex Contrastive Learning, Adaptive Trimming, self-supervised, topological deep learning, cellular complexes <br />
<br />
Summary: 
Cellular Complex Contrastive Learning with Adaptive Trimming (CellCLAT) is a novel framework for self-supervised topological deep learning on cellular complexes. It addresses challenges such as extrinsic structural constraints and intrinsic semantic redundancy in cellular representations. CellCLAT utilizes parameter perturbation-based augmentation to preserve cellular topology during contrastive learning while masking task-irrelevant cells through a cellular trimming scheduler. This approach enhances the representation of unlabeled graphs by capturing higher-order interactions in cellular complexes more effectively. The framework demonstrates significant improvements over existing self-supervised graph learning methods, making a valuable contribution to the field of topological deep learning. <div>
arXiv:2505.21587v1 Announce Type: new 
Abstract: Self-supervised topological deep learning (TDL) represents a nascent but underexplored area with significant potential for modeling higher-order interactions in simplicial complexes and cellular complexes to derive representations of unlabeled graphs. Compared to simplicial complexes, cellular complexes exhibit greater expressive power. However, the advancement in self-supervised learning for cellular TDL is largely hindered by two core challenges: \textit{extrinsic structural constraints} inherent to cellular complexes, and intrinsic semantic redundancy in cellular representations. The first challenge highlights that traditional graph augmentation techniques may compromise the integrity of higher-order cellular interactions, while the second underscores that topological redundancy in cellular complexes potentially diminish task-relevant information. To address these issues, we introduce Cellular Complex Contrastive Learning with Adaptive Trimming (CellCLAT), a twofold framework designed to adhere to the combinatorial constraints of cellular complexes while mitigating informational redundancy. Specifically, we propose a parameter perturbation-based augmentation method that injects controlled noise into cellular interactions without altering the underlying cellular structures, thereby preserving cellular topology during contrastive learning. Additionally, a cellular trimming scheduler is employed to mask gradient contributions from task-irrelevant cells through a bi-level meta-learning approach, effectively removing redundant topological elements while maintaining critical higher-order semantics. We provide theoretical justification and empirical validation to demonstrate that CellCLAT achieves substantial improvements over existing self-supervised graph learning methods, marking a significant attempt in this domain.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.21591</link>
<guid>https://arxiv.org/abs/2505.21591</guid>
<content:encoded><![CDATA[
<div> Keywords: model quantization, diffusion models, floating-point quantization, fine-tuning, denoising

Summary:
Model quantization aims to reduce the bit-width of weights and activations in diffusion models for improved memory efficiency and faster inference speed. However, achieving 4-bit quantization has been challenging. Existing methods based on integer quantization and post-training quantization fine-tuning have shown inconsistent performance. This study explores low-bit floating-point (FP) quantization for diffusion models and identifies key challenges such as handling asymmetric activation distributions and considering temporal complexity during the denoising process. To address these challenges, the mixup-sign floating-point quantization (MSFP) framework is proposed, which introduces unsigned FP quantization and incorporates timestep-aware techniques and denoising-factor loss alignment for precise and stable fine-tuning. Experimental results demonstrate superior performance in 4-bit FP quantization for diffusion models compared to existing methods in 4-bit integer quantization. 

<br /><br />Summary: <div>
arXiv:2505.21591v1 Announce Type: new 
Abstract: Model quantization reduces the bit-width of weights and activations, improving memory efficiency and inference speed in diffusion models. However, achieving 4-bit quantization remains challenging. Existing methods, primarily based on integer quantization and post-training quantization fine-tuning, struggle with inconsistent performance. Inspired by the success of floating-point (FP) quantization in large language models, we explore low-bit FP quantization for diffusion models and identify key challenges: the failure of signed FP quantization to handle asymmetric activation distributions, the insufficient consideration of temporal complexity in the denoising process during fine-tuning, and the misalignment between fine-tuning loss and quantization error. To address these challenges, we propose the mixup-sign floating-point quantization (MSFP) framework, first introducing unsigned FP quantization in model quantization, along with timestep-aware LoRA (TALoRA) and denoising-factor loss alignment (DFA), which ensure precise and stable fine-tuning. Extensive experiments show that we are the first to achieve superior performance in 4-bit FP quantization for diffusion models, outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relevance-driven Input Dropout: an Explanation-guided Regularization Technique</title>
<link>https://arxiv.org/abs/2505.21595</link>
<guid>https://arxiv.org/abs/2505.21595</guid>
<content:encoded><![CDATA[
<div> Keywords: overfitting, machine learning, data augmentation, occlusion, generalization 

Summary: 
The article addresses the issue of overfitting in machine learning models, proposing a novel data augmentation technique called Relevance-driven Input Dropout (RelDrop). Unlike existing methods that randomly mask input features, RelDrop selectively occludes the most relevant regions of the input to encourage the model to utilize other important features during training. Through qualitative and quantitative analyses, the study demonstrates that RelDrop improves model robustness towards occlusion, encourages the use of more features within the region of interest, and enhances generalization performance at inference time. Experimental results on benchmark datasets highlight the effectiveness of the proposed approach in mitigating overfitting and reducing the train-test performance gap. The code implementation for RelDrop is available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2505.21595v1 Announce Type: new 
Abstract: Overfitting is a well-known issue extending even to state-of-the-art (SOTA) Machine Learning (ML) models, resulting in reduced generalization, and a significant train-test performance gap. Mitigation measures include a combination of dropout, data augmentation, weight decay, and other regularization techniques. Among the various data augmentation strategies, occlusion is a prominent technique that typically focuses on randomly masking regions of the input during training. Most of the existing literature emphasizes randomness in selecting and modifying the input features instead of regions that strongly influence model decisions. We propose Relevance-driven Input Dropout (RelDrop), a novel data augmentation method which selectively occludes the most relevant regions of the input, nudging the model to use other important features in the prediction process, thus improving model generalization through informed regularization. We further conduct qualitative and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop) affects model decision-making. Through a series of experiments on benchmark datasets, we demonstrate that our approach improves robustness towards occlusion, results in models utilizing more features within the region of interest, and boosts inference time generalization performance. Our code is available at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge</title>
<link>https://arxiv.org/abs/2505.21605</link>
<guid>https://arxiv.org/abs/2505.21605</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, safety benchmark, hazardous scenarios, SOSBench, model alignment deficiencies

Summary:
The article introduces SOSBench, a new benchmark designed to evaluate the safety and resilience of large language models (LLMs) when handling high-risk scientific scenarios in domains such as chemistry, biology, medicine, pharmacology, physics, and psychology. The benchmark consists of 3,000 prompts derived from real-world regulations and laws, including detailed and complex misuse scenarios. The evaluation of state-of-the-art models using SOSBench reveals significant deficiencies in model alignment with safety regulations, with high rates of harmful responses detected across all domains. Advanced models like Deepseek-R1 and GPT-4.1 exhibit alarming levels of policy-violating content, raising urgent concerns about the responsible deployment of powerful LLMs. SOSBench highlights the need for improved safety benchmarks and increased attention to the risks associated with the misuse of advanced language models. 

<br /><br />Summary: <div>
arXiv:2505.21605v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit advancing capabilities in complex tasks, such as reasoning and graduate-level question answering, yet their resilience against misuse, particularly involving scientifically sophisticated risks, remains underexplored. Existing safety benchmarks typically focus either on instructions requiring minimal knowledge comprehension (e.g., ``tell me how to build a bomb") or utilize prompts that are relatively low-risk (e.g., multiple-choice or classification tasks about hazardous content). Consequently, they fail to adequately assess model safety when handling knowledge-intensive, hazardous scenarios.
  To address this critical gap, we introduce SOSBench, a regulation-grounded, hazard-focused benchmark encompassing six high-risk scientific domains: chemistry, biology, medicine, pharmacology, physics, and psychology. The benchmark comprises 3,000 prompts derived from real-world regulations and laws, systematically expanded via an LLM-assisted evolutionary pipeline that introduces diverse, realistic misuse scenarios (e.g., detailed explosive synthesis instructions involving advanced chemical formulas). We evaluate frontier models within a unified evaluation framework using our SOSBench. Despite their alignment claims, advanced models consistently disclose policy-violating content across all domains, demonstrating alarmingly high rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1). These results highlight significant safety alignment deficiencies and underscore urgent concerns regarding the responsible deployment of powerful LLMs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Where to Learn: Training Distribution Selection for Provable OOD Performance</title>
<link>https://arxiv.org/abs/2505.21626</link>
<guid>https://arxiv.org/abs/2505.21626</guid>
<content:encoded><![CDATA[
<div> distribution-aware training, out-of-distribution generalization, machine learning, optimization, probability measures<br />
Summary:<br />
- The study focuses on addressing the challenge of out-of-distribution (OOD) generalization in machine learning, where models often experience performance degradation on shifted or unseen domains. 
- A theoretical analysis establishes generalization bounds that quantify how the choice of training distribution affects OOD error. 
- Two algorithmic strategies are proposed: formulating OOD risk minimization as a bilevel optimization problem over probability measures and minimizing an upper bound on OOD error. 
- Evaluations across various examples show that the proposed methods significantly enhance OOD accuracy compared to standard empirical risk minimization. 
- Distribution-aware training emerges as a promising framework for robust OOD generalization in machine learning. <br /> 

Summary: <div>
arXiv:2505.21626v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in machine learning. Models trained on one data distribution often experience substantial performance degradation when evaluated on shifted or unseen domains. To address this challenge, the present paper studies the design of training data distributions that maximize average-case OOD performance. First, a theoretical analysis establishes a family of generalization bounds that quantify how the choice of training distribution influences OOD error across a predefined family of target distributions. These insights motivate the introduction of two complementary algorithmic strategies: (i) directly formulating OOD risk minimization as a bilevel optimization problem over the space of probability measures and (ii) minimizing a theoretical upper bound on OOD error. Last, the paper evaluates the two approaches across a range of function approximation and operator learning examples. The proposed methods significantly improve OOD accuracy over standard empirical risk minimization with a fixed distribution. These results highlight the potential of distribution-aware training as a principled and practical framework for robust OOD generalization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apprenticeship learning with prior beliefs using inverse optimization</title>
<link>https://arxiv.org/abs/2505.21639</link>
<guid>https://arxiv.org/abs/2505.21639</guid>
<content:encoded><![CDATA[
<div> framework, inverse reinforcement learning, inverse optimization, Markov decision processes, apprenticeship learning 
<br />
Summary: 
In this paper, the relationship between inverse reinforcement learning (IRL), inverse optimization (IO) for Markov decision processes (MDPs), and apprenticeship learning (AL) is investigated. The authors incorporate prior beliefs on the cost function structure into IRL and AL, showing that AL can be seen as a relaxation of their framework. By formulating AL as a regularized min-max problem, they address the ill-posedness of IRL using a regularizer to guide the search for plausible cost functions. Stochastic mirror descent (SMD) is employed to solve the resulting regularized-convex-concave-min-max problem, with convergence bounds established. Numerical experiments underscore the importance of regularization in learning cost vectors and apprentice policies. <div>
arXiv:2505.21639v1 Announce Type: new 
Abstract: The relationship between inverse reinforcement learning (IRL) and inverse optimization (IO) for Markov decision processes (MDPs) has been relatively underexplored in the literature, despite addressing the same problem. In this work, we revisit the relationship between the IO framework for MDPs, IRL, and apprenticeship learning (AL). We incorporate prior beliefs on the structure of the cost function into the IRL and AL problems, and demonstrate that the convex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a relaxation of our framework. Notably, the AL formalism is a special case in our framework when the regularization term is absent. Focusing on the suboptimal expert setting, we formulate the AL problem as a regularized min-max problem. The regularizer plays a key role in addressing the ill-posedness of IRL by guiding the search for plausible cost functions. To solve the resulting regularized-convex-concave-min-max problem, we use stochastic mirror descent (SMD) and establish convergence bounds for the proposed method. Numerical experiments highlight the critical role of regularization in learning cost vectors and apprentice policies.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Diffusion Models for Symmetric Manifolds</title>
<link>https://arxiv.org/abs/2505.21640</link>
<guid>https://arxiv.org/abs/2505.21640</guid>
<content:encoded><![CDATA[
<div> framework, efficient diffusion models, symmetric-space Riemannian manifolds, spatially-varying covariance, training algorithm 

Summary:<br /><br />Researchers introduce a framework for designing efficient diffusion models on symmetric-space Riemannian manifolds like the torus, sphere, special orthogonal group, and unitary group. They develop a new diffusion model with a spatially-varying covariance, allowing them to bypass heat kernel computations by leveraging a projection of Euclidean Brownian motion. The training algorithm minimizes a novel efficient objective derived through Ito's Lemma, running in O(1) gradient evaluations and nearly-linear-in-d (O(d^1.19)) arithmetic operations per step. The model's manifold symmetries ensure an "average-case" Lipschitz condition, facilitating accurate and efficient sample generation. Empirical results demonstrate that their model outperforms previous methods in training speed and enhances sample quality on synthetic datasets across various symmetric manifolds. <div>
arXiv:2505.21640v1 Announce Type: new 
Abstract: We introduce a framework for designing efficient diffusion models for $d$-dimensional symmetric-space Riemannian manifolds, including the torus, sphere, special orthogonal group and unitary group. Existing manifold diffusion models often depend on heat kernels, which lack closed-form expressions and require either $d$ gradient evaluations or exponential-in-$d$ arithmetic operations per training step. We introduce a new diffusion model for symmetric manifolds with a spatially-varying covariance, allowing us to leverage a projection of Euclidean Brownian motion to bypass heat kernel computations. Our training algorithm minimizes a novel efficient objective derived via Ito's Lemma, allowing each step to run in $O(1)$ gradient evaluations and nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap between diffusions on symmetric manifolds and Euclidean space. Manifold symmetries ensure the diffusion satisfies an "average-case" Lipschitz condition, enabling accurate and efficient sample generation. Empirically, our model outperforms prior methods in training speed and improves sample quality on synthetic datasets on the torus, special orthogonal group, and unitary group.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects</title>
<link>https://arxiv.org/abs/2505.21641</link>
<guid>https://arxiv.org/abs/2505.21641</guid>
<content:encoded><![CDATA[
<div> framework, CIs, ATE, treatment effects, differential privacy  
Summary:  
- The article introduces PrivATE, a machine learning framework for computing Confidence Intervals (CIs) for the Average Treatment Effect (ATE) under differential privacy.  
- Privacy is crucial in medical data analysis, and PrivATE allows valid uncertainty quantification while keeping sensitive data private.  
- PrivATE consists of three steps: estimating a differentially private ATE, estimating a differentially private variance, and constructing CIs that account for uncertainty from both steps.  
- The framework is model agnostic, doubly robust, and ensures valid CIs for the ATE.  
- The authors demonstrate the effectiveness of PrivATE using synthetic and real-world medical datasets, showing it is the first general, doubly robust framework for valid CIs of ATE under differential privacy.<br /><br />Summary: <div>
arXiv:2505.21641v1 Announce Type: new 
Abstract: The average treatment effect (ATE) is widely used to evaluate the effectiveness of drugs and other medical interventions. In safety-critical applications like medicine, reliable inferences about the ATE typically require valid uncertainty quantification, such as through confidence intervals (CIs). However, estimating treatment effects in these settings often involves sensitive data that must be kept private. In this work, we present PrivATE, a novel machine learning framework for computing CIs for the ATE under differential privacy. Specifically, we focus on deriving valid privacy-preserving CIs for the ATE from observational data. Our PrivATE framework consists of three steps: (i) estimating a differentially private ATE through output perturbation; (ii) estimating the differentially private variance through a truncated output perturbation mechanism; and (iii) constructing the CIs while accounting for the uncertainty from both the estimation and privatization steps. Our PrivATE framework is model agnostic, doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our framework using synthetic and real-world medical datasets. To the best of our knowledge, we are the first to derive a general, doubly robust framework for valid CIs of the ATE under ($\varepsilon$, $\delta$)-differential privacy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2505.21651</link>
<guid>https://arxiv.org/abs/2505.21651</guid>
<content:encoded><![CDATA[
<div> Keywords: learning rate, stochastic gradient descent, AutoSGD, convergence, optimization 

Summary: 
AutoSGD is a novel stochastic gradient descent (SGD) method that automatically adjusts the learning rate at each iteration, eliminating the need for manual tuning. It determines whether to increase or decrease the learning rate based on the current iteration's performance. The method is supported by theoretical convergence guarantees and has a deterministic counterpart for standard gradient descent. Empirical results demonstrate the effectiveness of AutoSGD across various optimization problems and machine learning tasks. This approach addresses the challenge of selecting an optimal learning rate schedule and shows promising performance in practice. <div>
arXiv:2505.21651v1 Announce Type: new 
Abstract: The learning rate is an important tuning parameter for stochastic gradient descent (SGD) and can greatly influence its performance. However, appropriate selection of a learning rate schedule across all iterations typically requires a non-trivial amount of user tuning effort. To address this, we introduce AutoSGD: an SGD method that automatically determines whether to increase or decrease the learning rate at a given iteration and then takes appropriate action. We introduce theory supporting the convergence of AutoSGD, along with its deterministic counterpart for standard gradient descent. Empirical results suggest strong performance of the method on a variety of traditional optimization problems and machine learning tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PreGenie: An Agentic Framework for High-quality Visual Presentation Generation</title>
<link>https://arxiv.org/abs/2505.21660</link>
<guid>https://arxiv.org/abs/2505.21660</guid>
<content:encoded><![CDATA[
<div> framework, multimodal input, visual presentations, automated creation, deep learning
Summary:
PreGenie is a new framework that aims to improve the automation of creating visual presentations using multimodal large language models (MLLMs). It addresses issues such as poorly organized layouts, inaccurate text summarization, and a lack of image understanding that have hindered previous attempts at automation. PreGenie operates in two stages: Analysis and Initial Generation, and Review and Re-generation, leveraging multiple MLLMs to collaborate and share information. Through comprehensive experiments, PreGenie has demonstrated superior performance in multimodal understanding, aesthetics, and content consistency compared to existing models. It also aligns closely with human design preferences, making it suitable for use in formal contexts like business and scientific research. <br /><br />Summary: <div>
arXiv:2505.21660v1 Announce Type: new 
Abstract: Visual presentations are vital for effective communication. Early attempts to automate their creation using deep learning often faced issues such as poorly organized layouts, inaccurate text summarization, and a lack of image understanding, leading to mismatched visuals and text. These limitations restrict their application in formal contexts like business and scientific research. To address these challenges, we propose PreGenie, an agentic and modular framework powered by multimodal large language models (MLLMs) for generating high-quality visual presentations.
  PreGenie is built on the Slidev presentation framework, where slides are rendered from Markdown code. It operates in two stages: (1) Analysis and Initial Generation, which summarizes multimodal input and generates initial code, and (2) Review and Re-generation, which iteratively reviews intermediate code and rendered slides to produce final, high-quality presentations. Each stage leverages multiple MLLMs that collaborate and share information. Comprehensive experiments demonstrate that PreGenie excels in multimodal understanding, outperforming existing models in both aesthetics and content consistency, while aligning more closely with human design preferences.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Controllable Diffusion via Optimal Classifier Guidance</title>
<link>https://arxiv.org/abs/2505.21666</link>
<guid>https://arxiv.org/abs/2505.21666</guid>
<content:encoded><![CDATA[
<div> diffusion models, controllable generation, reinforcement learning, supervised learning, classification<br />
<br />
Summary: 
The article introduces a method called Supervised Learning based Controllable Diffusion (SLCD) for the controllable generation of diffusion models. SLCD aims to optimize a KL-regularized objective function by iteratively generating online data and training a small classifier. It does not rely on complex concepts from reinforcement learning or control, making it more resource-efficient. The output from SLCD is proven to converge to the optimal solution of the objective under KL divergence. Empirical results show that SLCD can generate high-quality samples with minimal inference time, comparable to the base model, in tasks such as image generation and biological sequence generation. The code for SLCD is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2505.21666v1 Announce Type: new 
Abstract: The controllable generation of diffusion models aims to steer the model to generate samples that optimize some given objective functions. It is desirable for a variety of applications including image generation, molecule generation, and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of the base model is a popular approach but it can overfit the reward function while requiring significant resources. We frame controllable generation as a problem of finding a distribution that optimizes a KL-regularized objective function. We present SLCD -- Supervised Learning based Controllable Diffusion, which iteratively generates online data and trains a small classifier to guide the generation of the diffusion model. Similar to the standard classifier-guided diffusion, SLCD's key computation primitive is classification and does not involve any complex concepts from RL or control. Via a reduction to no-regret online learning analysis, we show that under KL divergence, the output from SLCD provably converges to the optimal solution of the KL-regularized objective. Further, we empirically demonstrate that SLCD can generate high quality samples with nearly the same inference time as the base model in both image generation with continuous diffusion and biological sequence generation with discrete diffusion. Our code is available at https://github.com/Owen-Oertell/slcd
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What happens when generative AI models train recursively on each others' generated outputs?</title>
<link>https://arxiv.org/abs/2505.21677</link>
<guid>https://arxiv.org/abs/2505.21677</guid>
<content:encoded><![CDATA[
<div> internet, AI-generated content, genAI models, training data, data-mediated interactions 

Summary:
Data-mediated interactions between generative AI (genAI) models and training data from other models on the internet have potential consequences. This study explores the effects of such interactions, showing that models trained on others' outputs can benefit from exposure to novel concepts but may also result in homogenization of performance on shared tasks. The empirical evidence and theoretical model provided shed light on how these interactions might unfold in practice, highlighting the importance of understanding the downstream effects of data-mediated model interactions in an increasingly genAI-dependent society. <div>
arXiv:2505.21677v1 Announce Type: new 
Abstract: The internet is full of AI-generated content while also serving as a common source of training data for generative AI (genAI) models. This duality raises the possibility that future genAI models may be trained on other models' generated outputs. Prior work has studied consequences of models training on their own generated outputs, but limited work has considered what happens if models ingest content produced by other models. Given society's increasing dependence on genAI tools, understanding downstream effects of such data-mediated model interactions is critical. To this end, we provide empirical evidence for how data-mediated interactions might unfold in practice, develop a theoretical model for this interactive training process, and show experimentally possible long-term results of such interactions. We find that data-mediated interactions can benefit models by exposing them to novel concepts perhaps missed in original training data, but also can homogenize their performance on shared tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data</title>
<link>https://arxiv.org/abs/2505.21680</link>
<guid>https://arxiv.org/abs/2505.21680</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariateGPT, mixed data, autoregressive sequence decomposition, likelihood estimation, transformer models <br />
<br />
Summary: <br />
The article introduces multivariateGPT, an architecture designed to model sequences of mixed categorical and numeric data. It addresses limitations in existing approaches by incorporating autoregressive sequence decomposition, embedding schemes, and a loss function that enables likelihood estimation of the joint distribution of next token class and value. By extending the next token prediction task, multivariateGPT efficiently generalizes patterns in simple physical systems and complex time series like electrocardiograms and multivariate electronic health record data. This advancement allows transformer models to effectively handle a wider range of data types, showcasing their versatility and potential for enhancing data analysis in real-world applications. <div>
arXiv:2505.21680v1 Announce Type: new 
Abstract: Real-world processes often generate data that are a mix of categorical and numeric values that are recorded at irregular and informative intervals. Discrete token-based approaches are limited in numeric representation capacity while methods like neural ordinary differential equations are not well suited for categorical data or informative sampling and require augmentation to handle certain classes of trajectories. Here, we present multivariateGPT, a single architecture for modeling sequences of mixed categorical (including tokenized text) and numeric data. This is accomplished with an autoregressive sequence decomposition, embedding scheme, and loss function that extend the next token prediction task to likelihood estimation of the joint distribution of next token class and value. We demonstrate how this approach can efficiently learn to generalize patterns in simple physical systems and model complex time series including electrocardiograms and multivariate electronic health record data. This work extends the utility of transformer based models to additional classes of data.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Permissionless Distributed Learning of LLMs</title>
<link>https://arxiv.org/abs/2505.21684</link>
<guid>https://arxiv.org/abs/2505.21684</guid>
<content:encoded><![CDATA[
<div> incentive system, distributed deep learning, Gauntlet, bittensor blockchain, pseudo-gradients

Summary:
The article introduces an incentive system called Gauntlet for distributed deep learning, implemented on the bittensor blockchain. It allows for the training of a 1.2B LLM model with permissionless contributions of pseudo-gradients from users with varying hardware. Gauntlet can be applied to synchronous distributed training schemes. The system utilizes a two-stage mechanism to filter peer reliability and synchronization, along with a core component for estimating loss before and after contributions. An OpenSkill rating system tracks the competitiveness of pseudo-gradient scores. A novel mechanism ensures unique computations by peers. The 1.2B run on the system has paid out real-valued tokens to participants based on their contributions, resulting in a competitive model. <div>
arXiv:2505.21684v1 Announce Type: new 
Abstract: We describe an incentive system for distributed deep learning of foundational models where peers are rewarded for contributions. The incentive system, \textit{Gauntlet}, has been deployed on the bittensor blockchain and used to train a 1.2B LLM with completely permissionless contributions of pseudo-gradients: no control over the users that can register or their hardware. \textit{Gauntlet} can be applied to any synchronous distributed training scheme that relies on aggregating updates or pseudo-gradients. We rely on a two-stage mechanism for fast filtering of peer uptime, reliability, and synchronization, combined with the core component that estimates the loss before and after individual pseudo-gradient contributions. We utilized an OpenSkill rating system to track competitiveness of pseudo-gradient scores across time. Finally, we introduce a novel mechanism to ensure peers on the network perform unique computations. Our live 1.2B run, which has paid out real-valued tokens to participants based on the value of their contributions, yielded a competitive (on a per-iteration basis) 1.2B model that demonstrates the utility of our incentive system.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling</title>
<link>https://arxiv.org/abs/2505.21695</link>
<guid>https://arxiv.org/abs/2505.21695</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, communication efficiency, model accuracy, Gradient Difference Approximation, Adaptive Multi-Step Federated Learning framework

Summary: 
The paper introduces a new method called Gradient Difference Approximation (GDA) to address the challenges of balancing communication efficiency and model accuracy in federated learning. GDA utilizes first-order information to estimate local error trends without the need to compute the full Hessian matrix, resulting in a lightweight yet effective solution. This method is a key component of the Adaptive Multi-Step Federated Learning (AMSFL) framework, offering a unified error modeling strategy for large-scale multi-step adaptive training environments. By leveraging GDA within the AMSFL framework, the proposed approach aims to improve the efficiency and accuracy of federated learning algorithms. <div>
arXiv:2505.21695v1 Announce Type: new 
Abstract: Federated learning faces critical challenges in balancing communication efficiency and model accuracy. One key issue lies in the approximation of update errors without incurring high computational costs. In this paper, we propose a lightweight yet effective method called Gradient Difference Approximation (GDA), which leverages first-order information to estimate local error trends without computing the full Hessian matrix. The proposed method forms a key component of the Adaptive Multi-Step Federated Learning (AMSFL) framework and provides a unified error modeling strategy for large-scale multi-step adaptive training environments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling</title>
<link>https://arxiv.org/abs/2505.21717</link>
<guid>https://arxiv.org/abs/2505.21717</guid>
<content:encoded><![CDATA[
<div> LrcSSM, nonlinear recurrent model, long sequences, fast processing, parallel solution, formal gradient-stability guarantee <br />
<br />
Summary: LrcSSM is a nonlinear recurrent model that efficiently processes long sequences with fast parallel computation. By utilizing a diagonal state-transition matrix learned at each step, it achieves $\mathcal{O}(TD)$ time and memory complexity and low $\mathcal{O}(\log T)$ sequential depth. The model offers a gradient-stability guarantee and outperforms other input-varying systems on long-range forecasting tasks. With a compute-optimal scaling law regime similar to Mamba, LrcSSM demonstrates superior performance compared to quadratic-attention Transformers while avoiding memory overhead. The forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, making it computationally efficient, and its parameter count scales optimally with depth. Overall, LrcSSM showcases impressive capabilities in processing long sequences and outperforming established models on forecasting tasks. <br /><br />Summary: <div>
arXiv:2505.21717v1 Announce Type: new 
Abstract: We present LrcSSM, a \textit{nonlinear} recurrent model that processes long sequences as fast as today's linear state-space layers. By forcing the state-transition matrix to be diagonal and learned at every step, the full sequence can be solved in parallel with a single prefix-scan, giving $\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential depth, for input-sequence length $T$ and a state dimension $D$. Moreover, LrcSSM offers a formal gradient-stability guarantee that other input-varying systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth $L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its low sequential depth and parameter count $\Theta(D\,L)$, the model follows the compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for Mamba, outperforming quadratic-attention Transformers at equal compute while avoiding the memory overhead of FFT-based long convolutions. We show that on a series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape</title>
<link>https://arxiv.org/abs/2505.21722</link>
<guid>https://arxiv.org/abs/2505.21722</guid>
<content:encoded><![CDATA[
<div> escape directions, ReLU networks, saddle points, low-rank bias, optimization 

Summary: 
The study investigates the behavior of gradient descent (GD) in deep ReLU networks when initialized with small weights. It focuses on the escape directions, analogous to the eigenvectors of the Hessian for strict saddles, and their importance in navigating the parameter space dominated by saddles. The optimal escape direction is characterized by a low-rank bias in deeper layers, with the first singular value of the weight matrix increasing at least as the square root of the layer depth. The research presents results related to these escape directions and suggests that this finding is a crucial step towards understanding Saddle-to-Saddle dynamics in deep ReLU networks, where GD encounters a sequence of saddles with increasing bottleneck rank. <div>
arXiv:2505.21722v1 Announce Type: new 
Abstract: When a deep ReLU network is initialized with small weights, GD is at first dominated by the saddle at the origin in parameter space. We study the so-called escape directions, which play a similar role as the eigenvectors of the Hessian for strict saddles. We show that the optimal escape direction features a low-rank bias in its deeper layers: the first singular value of the $\ell$-th layer weight matrix is at least $\ell^{\frac{1}{4}}$ larger than any other singular value. We also prove a number of related results about these escape directions. We argue that this result is a first step in proving Saddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of saddles with increasing bottleneck rank.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning Agents are not even close to Human Intelligence</title>
<link>https://arxiv.org/abs/2505.21731</link>
<guid>https://arxiv.org/abs/2505.21731</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, HackAtari, task variations, performance drops, generalization testing
<br />
Summary: 
The article introduces HackAtari, a set of task variations in the Arcade Learning Environments, to evaluate the zero-shot adaptation capabilities of deep RL agents. It highlights that RL agents struggle to maintain performance on simpler versions of training tasks, unlike humans. The study reveals a consistent reliance on shortcuts by RL agents, leading to significant performance drops on simplified tasks. The analysis across various algorithms and architectures emphasizes the gap between RL agents and human intelligence. The research underscores the importance of new benchmarks and methodologies for systematic generalization testing beyond standard evaluation protocols. It suggests that training and testing in the same environment are insufficient for developing RL agents with human-like intelligence. 
<br /> <div>
arXiv:2505.21731v1 Announce Type: new 
Abstract: Deep reinforcement learning (RL) agents achieve impressive results in a wide variety of tasks, but they lack zero-shot adaptation capabilities. While most robustness evaluations focus on tasks complexifications, for which human also struggle to maintain performances, no evaluation has been performed on tasks simplifications. To tackle this issue, we introduce HackAtari, a set of task variations of the Arcade Learning Environments. We use it to demonstrate that, contrary to humans, RL agents systematically exhibit huge performance drops on simpler versions of their training tasks, uncovering agents' consistent reliance on shortcuts. Our analysis across multiple algorithms and architectures highlights the persistent gap between RL agents and human behavioral intelligence, underscoring the need for new benchmarks and methodologies that enforce systematic generalization testing beyond static evaluation protocols. Training and testing in the same environment is not enough to obtain agents equipped with human-like intelligence.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing</title>
<link>https://arxiv.org/abs/2505.21732</link>
<guid>https://arxiv.org/abs/2505.21732</guid>
<content:encoded><![CDATA[
<div> Latent Crossing, Low-rank Factorization, ViTs, LLMs, Pre-training <br />
Summary: 
The research introduces Latent Crossing (LaX), a module enhancing low-rank models' capacity by enabling information flow across subspaces, overcoming limitations of traditional low-rank factorization methods. LaX significantly improves performance on pre-training tasks with ViT-Base/Large and LLaMA-like models, matching or surpassing full-rank baselines while using 2-3 times fewer parameters. When combined with low-rank adapters (LoRA) for fine-tuning larger models like LLaMA-7/13B, LaX consistently enhances performance on arithmetic and common sense reasoning tasks at minimal cost. The research showcases the effectiveness of LaX as a simple yet powerful plug-and-play solution for boosting the performance of low-rank models in various high-computational-cost training scenarios. <br /> <div>
arXiv:2505.21732v1 Announce Type: new 
Abstract: Training foundation models such as ViTs and LLMs requires tremendous computing cost. Low-rank matrix or tensor factorization offers a parameter-efficient alternative, but often downgrades performance due to the restricted parameter space. In this work, we introduce {\textbf{Latent Crossing (LaX)}} -- a simple yet effective plug-and-play module that enhances the capacity of low-rank models by enabling information flow across low-rank subspaces. We extensively validate the benefits of LaX on pre-training tasks with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters. LaX boosts low-rank model performance to match or exceed the full-rank baselines while using 2-3\(\times\) fewer parameters. When equipped with low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently improves performance on arithmetic and common sense reasoning tasks with negligible cost.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen</title>
<link>https://arxiv.org/abs/2505.21743</link>
<guid>https://arxiv.org/abs/2505.21743</guid>
<content:encoded><![CDATA[
<div> learning, counterfactual safety, near-miss events, crash prediction, Vision Zero<br />
Summary:<br />
Traffic safety science faces challenges due to limited crash data. To achieve Vision Zero, a paradigm shift towards counterfactual safety learning is needed. This involves considering potential dangerous scenarios that could have occurred. The proposed agenda combines crash-rate priors, generative scene engines, and diverse driver models to synthesize and explain near-miss events. A crash-focused digital twin testbed connects micro scenes to macro patterns, ensuring statistical realism. This transformation of sparse crash data enables proactive prevention of accidents through crash prediction and stress-testing of vehicles, roads, and policies. By learning from almost-crashes, traffic safety can shift from reactive forensics to proactive prevention, ultimately advancing Vision Zero. <br /><br />Summary: <div>
arXiv:2505.21743v1 Announce Type: new 
Abstract: Traffic safety science has long been hindered by a fundamental data paradox: the crashes we most wish to prevent are precisely those events we rarely observe. Existing crash-frequency models and surrogate safety metrics rely heavily on sparse, noisy, and under-reported records, while even sophisticated, high-fidelity simulations undersample the long-tailed situations that trigger catastrophic outcomes such as fatalities. We argue that the path to achieving Vision Zero, i.e., the complete elimination of traffic fatalities and severe injuries, requires a paradigm shift from traditional crash-only learning to a new form of counterfactual safety learning: reasoning not only about what happened, but also about the vast set of plausible yet perilous scenarios that could have happened under slightly different circumstances. To operationalize this shift, our proposed agenda bridges macro to micro. Guided by crash-rate priors, generative scene engines, diverse driver models, and causal learning, near-miss events are synthesized and explained. A crash-focused digital twin testbed links micro scenes to macro patterns, while a multi-objective validator ensures that simulations maintain statistical realism. This pipeline transforms sparse crash data into rich signals for crash prediction, enabling the stress-testing of vehicles, roads, and policies before deployment. By learning from crashes that almost happened, we can shift traffic safety from reactive forensics to proactive prevention, advancing Vision Zero.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Bi-Linear State Transitions in Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2505.21749</link>
<guid>https://arxiv.org/abs/2505.21749</guid>
<content:encoded><![CDATA[
<div> hidden units, recurrent neural networks, bi-linear operations, state tracking tasks, inductive bias<br />
<br />
Summary: In this new research, the role of hidden units in recurrent neural networks is examined from a different perspective. Instead of just being memory stores, hidden units are considered as active participants in network computation. The study focuses on bi-linear operations, which involve interactions between hidden units and input embeddings. The research shows that these operations serve as a natural inductive bias for representing the evolution of hidden states in state tracking tasks. It is demonstrated both theoretically and empirically that bi-linear state updates correspond to a hierarchy of increasing complexity in state tracking tasks, with popular linear recurrent networks like Mamba at the lowest level of complexity. This study sheds new light on the importance of hidden units in network behavior and provides insights into their active role in computations. <br /><br /> <div>
arXiv:2505.21749v1 Announce Type: new 
Abstract: The role of hidden units in recurrent neural networks is typically seen as modeling memory, with research focusing on enhancing information retention through gating mechanisms. A less explored perspective views hidden units as active participants in the computation performed by the network, rather than passive memory stores. In this work, we revisit bi-linear operations, which involve multiplicative interactions between hidden units and input embeddings. We demonstrate theoretically and empirically that they constitute a natural inductive bias for representing the evolution of hidden states in state tracking tasks. These are the simplest type of task that require hidden units to actively contribute to the behavior of the network. We also show that bi-linear state updates form a natural hierarchy corresponding to state tracking tasks of increasing complexity, with popular linear recurrent networks such as Mamba residing at the lowest-complexity center of that hierarchy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals</title>
<link>https://arxiv.org/abs/2505.21750</link>
<guid>https://arxiv.org/abs/2505.21750</guid>
<content:encoded><![CDATA[
<div> Hierarchical reinforcement learning, Diffusion model, Gaussian Process, subgoals, sample efficiency <br />
Summary: <br />
Hierarchical reinforcement learning (HRL) tackles the challenge of making decisions at multiple levels of temporal abstraction. The dynamic nature of low-level policies poses a difficulty for high-level policies to generate effective subgoals. This study introduces a novel approach that combines a conditional diffusion model with a Gaussian Process (GP) prior to generate a diverse set of subgoals while considering uncertainty. By incorporating GP uncertainty quantification, the proposed method surpasses existing HRL techniques in terms of both sample efficiency and performance on continuous control benchmarks. By selecting subgoals from the diffusion policy and GP's predictive mean, this strategy improves decision-making capabilities in complex environments. <div>
arXiv:2505.21750v1 Announce Type: new 
Abstract: Hierarchical reinforcement learning (HRL) learns to make decisions on multiple levels of temporal abstraction. A key challenge in HRL is that the low-level policy changes over time, making it difficult for the high-level policy to generate effective subgoals. To address this issue, the high-level policy must capture a complex subgoal distribution while also accounting for uncertainty in its estimates. We propose an approach that trains a conditional diffusion model regularized by a Gaussian Process (GP) prior to generate a complex variety of subgoals while leveraging principled GP uncertainty quantification. Building on this framework, we develop a strategy that selects subgoals from both the diffusion policy and GP's predictive mean. Our approach outperforms prior HRL methods in both sample efficiency and performance on challenging continuous control benchmarks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualSchool: How Reliable are LLMs for Optimization Education?</title>
<link>https://arxiv.org/abs/2505.21775</link>
<guid>https://arxiv.org/abs/2505.21775</guid>
<content:encoded><![CDATA[
<div> conversion, linear program, optimization, LLMs, DualSchool

Summary: 
DualSchool introduces a framework for assessing the performance of large language models (LLMs) in generating duals of linear programs. The framework uses a verification procedure based on Canonical Graph Edit Distance, providing more accurate evaluation compared to existing methods. Experiments show that while LLMs can accurately recite the conversion procedure, they struggle to consistently produce correct duals, even for small instances. This finding has implications for educators, highlighting the limitations of relying solely on LLMs for optimization tasks. It also raises questions about the development of large reasoning systems and the need for further research in the intersection of AI and operations research. <div>
arXiv:2505.21775v1 Announce Type: new 
Abstract: Consider the following task taught in introductory optimization courses which addresses challenges articulated by the community at the intersection of (generative) AI and OR: generate the dual of a linear program. LLMs, being trained at web-scale, have the conversion process and many instances of Primal to Dual Conversion (P2DC) at their disposal. Students may thus reasonably expect that LLMs would perform well on the P2DC task. To assess this expectation, this paper introduces DualSchool, a comprehensive framework for generating and verifying P2DC instances. The verification procedure of DualSchool uses the Canonical Graph Edit Distance, going well beyond existing evaluation methods for optimization models, which exhibit many false positives and negatives when applied to P2DC. Experiments performed by DualSchool reveal interesting findings. Although LLMs can recite the conversion procedure accurately, state-of-the-art open LLMs fail to consistently produce correct duals. This finding holds even for the smallest two-variable instances and for derivative tasks, such as correctness, verification, and error classification. The paper also discusses the implications for educators, students, and the development of large reasoning systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization to Generalization: Emergence of Diffusion Models from Associative Memory</title>
<link>https://arxiv.org/abs/2505.21777</link>
<guid>https://arxiv.org/abs/2505.21777</guid>
<content:encoded><![CDATA[
<div> Hopfield networks, associative memory, diffusion models, spurious states, training data<br />
Summary:<br />
This study explores the connection between diffusion models and associative memories (AMs), specifically focusing on the phenomenon of spurious states that occur in Hopfield networks when training data reaches its critical memory load. The training phase of the diffusion model is compared to memory encoding in AMs, while the generation phase represents memory retrieval. In the small data regime, the diffusion model behaves similarly to the Hopfield model below the critical memory load, creating distinct basins of attraction around each sample. In the large data regime, new attractor states are formed corresponding to manifolds of generated samples, with spurious states emerging at the transition boundary. These spurious states exhibit distinct basins of attraction despite not being in the training set. The study offers a new perspective on memorization-generalization in diffusion models, predicts the existence of spurious states, and provides empirical validation of these predictions in commonly-used diffusion models. <br /> <div>
arXiv:2505.21777v1 Announce Type: new 
Abstract: Hopfield networks are associative memory (AM) systems, designed for storing and retrieving patterns as local minima of an energy landscape. In the classical Hopfield model, an interesting phenomenon occurs when the amount of training data reaches its critical memory load $- spurious\,\,states$, or unintended stable points, emerge at the end of the retrieval dynamics, leading to incorrect recall. In this work, we examine diffusion models, commonly used in generative modeling, from the perspective of AMs. The training phase of diffusion model is conceptualized as memory encoding (training data is stored in the memory). The generation phase is viewed as an attempt of memory retrieval. In the small data regime the diffusion model exhibits a strong memorization phase, where the network creates distinct basins of attraction around each sample in the training set, akin to the Hopfield model below the critical memory load. In the large data regime, a different phase appears where an increase in the size of the training set fosters the creation of new attractor states that correspond to manifolds of the generated samples. Spurious states appear at the boundary of this transition and correspond to emergent attractor states, which are absent in the training set, but, at the same time, have distinct basins of attraction around them. Our findings provide: a novel perspective on the memorization-generalization phenomenon in diffusion models via the lens of AMs, theoretical prediction of existence of spurious states, empirical validation of this prediction in commonly-used diffusion models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P-DROP: Poisson-Based Dropout for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.21783</link>
<guid>https://arxiv.org/abs/2505.21783</guid>
<content:encoded><![CDATA[
<div> Node selection strategy, Poisson processes, Graph Neural Networks, over-smoothing, structure-aware updates

Summary:
The article addresses the issue of over-smoothing in Graph Neural Networks (GNNs) by proposing a novel node selection strategy based on Poisson processes. This strategy introduces stochastic and structure-aware updates for nodes, preventing node representations from converging too quickly and losing discriminative power. By equipping each node with an independent Poisson clock, the approach allows for asynchronous and localized updates that preserve structural diversity. The study explores two applications of the Poisson-based method: as a replacement for traditional dropout-based regularization and as a dynamic subgraph training scheme. Experimental results on standard benchmarks (Cora, Citeseer, Pubmed) demonstrate that the proposed method outperforms traditional approaches like Dropout, DropEdge, and DropNode, especially in later training stages. This showcases the effectiveness and competitiveness of the Poisson-based strategy in improving GNN performance. 

<br /><br />Summary: <div>
arXiv:2505.21783v1 Announce Type: new 
Abstract: Over-smoothing remains a major challenge in Graph Neural Networks (GNNs), where repeated message passing causes node representations to converge and lose discriminative power. To address this, we propose a novel node selection strategy based on Poisson processes, introducing stochastic but structure-aware updates. Specifically, we equip each node with an independent Poisson clock, enabling asynchronous and localized updates that preserve structural diversity. We explore two applications of this strategy: as a replacement for dropout-based regularization and as a dynamic subgraph training scheme. Experimental results on standard benchmarks (Cora, Citeseer, Pubmed) demonstrate that our Poisson-based method yields competitive or improved accuracy compared to traditional Dropout, DropEdge, and DropNode approaches, particularly in later training stages.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Born a Transformer -- Always a Transformer?</title>
<link>https://arxiv.org/abs/2505.21785</link>
<guid>https://arxiv.org/abs/2505.21785</guid>
<content:encoded><![CDATA[
<div> limitations, sequence-to-sequence tasks, Transformers, pretraining data, length generalization

Summary:
Pretraining of large-scale Language Models (LLMs) such as Transformers may not fully overcome theoretical limitations in modeling certain sequence-to-sequence tasks. A study of retrieval and copying tasks revealed an induction-versus-anti-induction asymmetry in pretrained models, with better performance in retrieving tokens to the right rather than the left of a query token. This asymmetry can be eliminated through targeted fine-tuning if length generalization is guaranteed. Analysis showed that the strength of induction versus anti-induction circuits within pretrained Transformers contributes to this asymmetry. Practical experiments on real-world tasks confirmed these findings and highlighted the need to consider reliability risks in utilizing pretrained models. Pretraining selectively enhances specific capabilities of Transformers but does not eliminate fundamental length-generalization limits.<br /><br />Summary: <div>
arXiv:2505.21785v1 Announce Type: new 
Abstract: Transformers have theoretical limitations in modeling certain sequence-to-sequence tasks, yet it remains largely unclear if these limitations play a role in large-scale pretrained LLMs, or whether LLMs might effectively overcome these constraints in practice due to the scale of both the models themselves and their pretraining data. We explore how these architectural constraints manifest after pretraining, by studying a family of $\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al. [2024]. We use the recently proposed C-RASP framework for studying length generalization [Huang et al., 2025b] to provide guarantees for each of our settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$ asymmetry, where pretrained models are better at retrieving tokens to the right (induction) rather than the left (anti-induction) of a query token. This asymmetry disappears upon targeted fine-tuning if length-generalization is guaranteed by theory. Mechanistic analysis reveals that this asymmetry is connected to the differences in the strength of induction versus anti-induction circuits within pretrained Transformers. We validate our findings through practical experiments on real-world tasks demonstrating reliability risks. Our results highlight that pretraining selectively enhances certain Transformer capabilities, but does not overcome fundamental length-generalization limits.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster Rates for Private Adversarial Bandits</title>
<link>https://arxiv.org/abs/2505.21790</link>
<guid>https://arxiv.org/abs/2505.21790</guid>
<content:encoded><![CDATA[
<div> bandits, differential privacy, regret upper bound, algorithms, adversarial 

Summary:<br />
- New differentially private algorithms are proposed for adversarial bandits and bandits with expert advice. 
- A simple and efficient conversion method is introduced for transforming non-private bandit algorithms into private ones in the adversarial bandits problem, improving regret upper bound to $O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$. 
- The algorithms allow for sublinear expected regret even with small values of epsilon, marking the first separation between central and local differential privacy for adversarial bandits. 
- The first differentially private algorithms for bandits with expert advice are presented, achieving expected regret rates of $O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right)$, $O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and $\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} + \frac{N^{1/2}\log(NT)}{\epsilon}\right)$. 
- These rates enable sublinear regret for various combinations of small and large numbers of actions, experts, and epsilon values. <div>
arXiv:2505.21790v1 Announce Type: new 
Abstract: We design new differentially private algorithms for the problems of adversarial bandits and bandits with expert advice. For adversarial bandits, we give a simple and efficient conversion of any non-private bandit algorithm to a private bandit algorithm. Instantiating our conversion with existing non-private bandit algorithms gives a regret upper bound of $O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all $\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first known separation between central and local differential privacy for this problem. For bandits with expert advice, we give the first differentially private algorithms, with expected regret $O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right), O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and $\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} + \frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of actions and experts respectively. These rates allow us to get sublinear regret for different combinations of small and large $K, N$ and $\epsilon.$
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms</title>
<link>https://arxiv.org/abs/2505.21792</link>
<guid>https://arxiv.org/abs/2505.21792</guid>
<content:encoded><![CDATA[
<div> paradigms, multimodal data, Federated Learning, challenges, taxonomy
Summary: 
In the paper "Multimodal Federated Learning: A Taxonomy of Challenges and Opportunities," the authors explore the intersection of two key research areas: leveraging multimodal data to enhance inference performance and enabling distributed training for efficiency and privacy preservation. The absence of a comprehensive taxonomy for Multimodal Federated Learning (MFL) is addressed, focusing on Horizontal FL (HFL), Vertical FL (VFL), and Hybrid FL paradigms. The paper examines the problem formulations, algorithms, and challenges associated with each paradigm in the context of multimodal data. Unique challenges such as modality heterogeneity, privacy heterogeneity, and communication inefficiency are discussed. The authors also highlight open challenges and provide insights for future research. This taxonomy sheds light on the novel challenges posed by multimodal data in various FL paradigms, offering a new perspective for advancing MFL research.<br /><br />Summary: <div>
arXiv:2505.21792v1 Announce Type: new 
Abstract: Multimodal Federated Learning (MFL) lies at the intersection of two pivotal research areas: leveraging complementary information from multiple modalities to improve downstream inference performance and enabling distributed training to enhance efficiency and preserve privacy. Despite the growing interest in MFL, there is currently no comprehensive taxonomy that organizes MFL through the lens of different Federated Learning (FL) paradigms. This perspective is important because multimodal data introduces distinct challenges across various FL settings. These challenges, including modality heterogeneity, privacy heterogeneity, and communication inefficiency, are fundamentally different from those encountered in traditional unimodal or non-FL scenarios. In this paper, we systematically examine MFL within the context of three major FL paradigms: horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we present the problem formulation, review representative training algorithms, and highlight the most prominent challenge introduced by multimodal data in distributed settings. We also discuss open challenges and provide insights for future research. By establishing this taxonomy, we aim to uncover the novel challenges posed by multimodal data from the perspective of different FL paradigms and to offer a new lens through which to understand and advance the development of MFL.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs</title>
<link>https://arxiv.org/abs/2505.21800</link>
<guid>https://arxiv.org/abs/2505.21800</guid>
<content:encoded><![CDATA[
<div> conversational abilities, falsehoods, truthfulness, multi-dimensional cones, concept cones<br />
Summary:<br />
This article explores the truthfulness of large language models (LLMs) and their ability to generate factual statements. By extending the concept cone framework to the domain of truth, the researchers identify multidimensional cones that influence how LLMs behave in response to true/false propositions. Through causal interventions, it is shown that these cones play a crucial role in mediating truth-related behavior across different LLM families. The study demonstrates that these cones can flip model responses to factual statements, generalize across model architectures, and preserve unrelated model behavior. These findings suggest a more complex, multidirectional geometry underlying the truthfulness of simple propositions in LLMs and highlight the potential of concept cones as a valuable tool for investigating abstract behaviors. <br /><br /> <div>
arXiv:2505.21800v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit strong conversational abilities but often generate falsehoods. Prior work suggests that the truthfulness of simple propositions can be represented as a single linear direction in a model's internal activations, but this may not fully capture its underlying geometry. In this work, we extend the concept cone framework, recently introduced for modeling refusal, to the domain of truth. We identify multi-dimensional cones that causally mediate truth-related behavior across multiple LLM families. Our results are supported by three lines of evidence: (i) causal interventions reliably flip model responses to factual statements, (ii) learned cones generalize across model architectures, and (iii) cone-based interventions preserve unrelated model behavior. These findings reveal the richer, multidirectional structure governing simple true/false propositions in LLMs and highlight concept cones as a promising tool for probing abstract behaviors.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Operational Automated Greenhouse Gas Plume Detection</title>
<link>https://arxiv.org/abs/2505.21806</link>
<guid>https://arxiv.org/abs/2505.21806</guid>
<content:encoded><![CDATA[
<div> Keywords: greenhouse gas detection, imaging spectroscopy, deep learning, convolutional neural networks, plume detection

Summary:
In the study, the challenges hindering the operational deployment of a fully automated greenhouse gas plume detection system using imaging spectroscopy were reviewed and addressed. Key obstacles such as data and label quality control, spatiotemporal biases, and modeling objectives alignment were tackled. The study demonstrated that convolutional neural networks (CNNs) can achieve operational detection performance when these obstacles are alleviated. A multitask model combining instance detection and pixelwise segmentation was shown to be effective in leading towards operational deployment. The model's ability to detect plumes across emission source types and regions was evaluated, establishing thresholds for operational readiness. The study also provided data, models, and source code for reproducibility, as well as defined best practices and validation standards for future contributions in the field. <div>
arXiv:2505.21806v1 Announce Type: new 
Abstract: Operational deployment of a fully automated greenhouse gas (GHG) plume detection system remains an elusive goal for imaging spectroscopy missions, despite recent advances in deep learning approaches. With the dramatic increase in data availability, however, automation continues to increase in importance for natural and anthropogenic emissions monitoring. This work reviews and addresses several key obstacles in the field: data and label quality control, prevention of spatiotemporal biases, and correctly aligned modeling objectives. We demonstrate through rigorous experiments using multicampaign data from airborne and spaceborne instruments that convolutional neural networks (CNNs) are able to achieve operational detection performance when these obstacles are alleviated. We demonstrate that a multitask model that learns both instance detection and pixelwise segmentation simultaneously can successfully lead towards an operational pathway. We evaluate the model's plume detectability across emission source types and regions, identifying thresholds for operational deployment. Finally, we provide analysis-ready data, models, and source code for reproducibility, and work to define a set of best practices and validation standards to facilitate future contributions to the field.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction</title>
<link>https://arxiv.org/abs/2505.21807</link>
<guid>https://arxiv.org/abs/2505.21807</guid>
<content:encoded><![CDATA[
<div> Interpretable modeling, Gradient boosting machines, Deep learning, Tabular data, Large language models (LLMs)<br />
Summary:<br />
The paper introduces a novel approach that utilizes reasoning-based Large Language Models (LLMs) trained with reinforcement learning to enhance prediction accuracy and interpretability on tabular data. Existing models like gradient boosting machines and deep models excel in performance but lack interpretability. By incorporating custom reward functions, the proposed model not only focuses on accurate predictions but also generates human-understandable explanations. Experimental results on financial benchmark datasets showcase superior performance compared to other LLMs. This approach seeks to bridge the gap between accurate predictions and interpretability, offering a promising solution for real-world applications. <br /> <div>
arXiv:2505.21807v1 Announce Type: new 
Abstract: Predictive modeling on tabular data is the cornerstone of many real-world applications. Although gradient boosting machines and some recent deep models achieve strong performance on tabular data, they often lack interpretability. On the other hand, large language models (LLMs) have demonstrated powerful capabilities to generate human-like reasoning and explanations, but remain under-performed for tabular data prediction. In this paper, we propose a new approach that leverages reasoning-based LLMs, trained using reinforcement learning, to perform more accurate and explainable predictions on tabular data. Our method introduces custom reward functions that guide the model not only toward high prediction accuracy but also toward human-understandable reasons for its predictions. Experimental results show that our model achieves promising performance on financial benchmark datasets, outperforming most existing LLMs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Data Augmentation through Bayesian Model Selection</title>
<link>https://arxiv.org/abs/2505.21813</link>
<guid>https://arxiv.org/abs/2505.21813</guid>
<content:encoded><![CDATA[
<div> Probabilistic View, Data Augmentation, Bayesian Optimization, Marginal Likelihood, Bayesian Principles
<br />
Summary: 
The paper introduces a novel framework for optimizing Data Augmentation (DA) strategies in machine learning. By viewing DA from a probabilistic standpoint, the authors treat augmentation parameters as model (hyper)parameters and optimize the marginal likelihood through a Bayesian model selection approach. A tractable Evidence Lower BOund (ELBO) is derived to enable joint optimization of augmentation parameters and model parameters. Theoretical results on variational approximation quality, generalization guarantees, invariance properties, and links to empirical Bayes are provided. Experimental results on computer vision tasks demonstrate improved calibration and robust performance compared to fixed or no augmentation strategies. The work establishes a rigorous foundation for optimizing DA using Bayesian principles, offering significant potential for enhancing robustness in machine learning. 
<br /> <div>
arXiv:2505.21813v1 Announce Type: new 
Abstract: Data Augmentation (DA) has become an essential tool to improve robustness and generalization of modern machine learning. However, when deciding on DA strategies it is critical to choose parameters carefully, and this can be a daunting task which is traditionally left to trial-and-error or expensive optimization based on validation performance. In this paper, we counter these limitations by proposing a novel framework for optimizing DA. In particular, we take a probabilistic view of DA, which leads to the interpretation of augmentation parameters as model (hyper)-parameters, and the optimization of the marginal likelihood with respect to these parameters as a Bayesian model selection problem. Due to its intractability, we derive a tractable Evidence Lower BOund (ELBO), which allows us to optimize augmentation parameters jointly with model parameters. We provide extensive theoretical results on variational approximation quality, generalization guarantees, invariance properties, and connections to empirical Bayes. Through experiments on computer vision tasks, we show that our approach improves calibration and yields robust performance over fixed or no augmentation. Our work provides a rigorous foundation for optimizing DA through Bayesian principles with significant potential for robust machine learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations</title>
<link>https://arxiv.org/abs/2505.21824</link>
<guid>https://arxiv.org/abs/2505.21824</guid>
<content:encoded><![CDATA[
<div> machine learning, diabetes risk prediction, Non-negative Matrix Factorization, multimorbidity, polypharmacy

Summary:
This article introduces a novel unsupervised framework for predicting the risk of developing type 2 diabetes mellitus (T2DM) by integrating Non-negative Matrix Factorization (NMF) with statistical techniques. T2DM poses significant health and economic challenges, making early detection crucial. Traditional supervised learning approaches are limited by the lack of confirmed negative cases, leading to the development of this new method. By identifying latent patterns of multimorbidity and polypharmacy among diagnosed T2DM patients, the framework estimates T2DM risk in undiagnosed individuals. This data-driven approach provides interpretable insights for healthcare providers to implement timely interventions, potentially improving patient outcomes and reducing the future health and economic burden of T2DM. <div>
arXiv:2505.21824v1 Announce Type: new 
Abstract: The global prevalence of diabetes, particularly type 2 diabetes mellitus (T2DM), is rapidly increasing, posing significant health and economic challenges. T2DM not only disrupts blood glucose regulation but also damages vital organs such as the heart, kidneys, eyes, nerves, and blood vessels, leading to substantial morbidity and mortality. In the US alone, the economic burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of individuals at risk is critical to mitigating these impacts. While machine learning approaches for T2DM prediction are increasingly adopted, many rely on supervised learning, which is often limited by the lack of confirmed negative cases. To address this limitation, we propose a novel unsupervised framework that integrates Non-negative Matrix Factorization (NMF) with statistical techniques to identify individuals at risk of developing T2DM. Our method identifies latent patterns of multimorbidity and polypharmacy among diagnosed T2DM patients and applies these patterns to estimate the T2DM risk in undiagnosed individuals. By leveraging data-driven insights from comorbidity and medication usage, our approach provides an interpretable and scalable solution that can assist healthcare providers in implementing timely interventions, ultimately improving patient outcomes and potentially reducing the future health and economic burden of T2DM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones</title>
<link>https://arxiv.org/abs/2505.21825</link>
<guid>https://arxiv.org/abs/2505.21825</guid>
<content:encoded><![CDATA[
<div> graph connectivity, inference-time computation, language models, reasoning strategies, parallel vs sequential scaling

Summary:
In the study, the researchers focused on the optimal allocation of inference-time computation in large language models for reasoning tasks. They explored whether sequential scaling or parallel scaling was more effective in improving performance. Through theoretical analysis and experiments on various language models, including those trained specifically for graph connectivity problems, they discovered scenarios where sequential scaling provided an exponential advantage over parallel scaling. These findings were particularly evident in challenging distributions of graphs. The research sheds light on the importance of considering different strategies for reasoning tasks and highlights the potential benefits of sequential scaling in certain scenarios. <div>
arXiv:2505.21825v1 Announce Type: new 
Abstract: Inference-time computation has emerged as a promising scaling axis for improving large language model reasoning. However, despite yielding impressive performance, the optimal allocation of inference-time computation remains poorly understood. A central question is whether to prioritize sequential scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority voting across multiple short chains of thought). In this work, we seek to illuminate the landscape of test-time scaling by demonstrating the existence of reasoning settings where sequential scaling offers an exponential advantage over parallel scaling. These settings are based on graph connectivity problems in challenging distributions of graphs. We validate our theoretical findings with comprehensive experiments across a range of language models, including models trained from scratch for graph connectivity with different chain of thought strategies as well as large reasoning models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In Search of Adam's Secret Sauce</title>
<link>https://arxiv.org/abs/2505.21829</link>
<guid>https://arxiv.org/abs/2505.21829</guid>
<content:encoded><![CDATA[
<div> Keywords: Adam, language models, optimization, signed momentum, gradient

Summary:
Adam optimization algorithm has shown remarkable efficacy in training transformer-based language models. Various simplified versions of Adam, such as signed momentum methods, have been proposed but consistently underperform relative to Adam. However, constraining the momentum parameters in Adam to be equal preserves near-optimal performance while offering new insights and formulations. This choice not only ensures robust performance but also allows for a statistical interpretation, showing that Adam can be viewed as an online algorithm for estimating gradient mean and variance from a mean-field Gaussian variational inference perspective. This study conducted over 1,300 language models highlights the significance of Adam in optimization and sheds light on the "secret sauce" behind its success. 

<br /><br />Summary: <div>
arXiv:2505.21829v1 Announce Type: new 
Abstract: Understanding the remarkable efficacy of Adam when training transformer-based language models has become a central research topic within the optimization community. To gain deeper insights, several simplifications of Adam have been proposed, such as the signed gradient and signed momentum methods. In this work, we conduct an extensive empirical study - training over 1,300 language models across different data configurations and scales - comparing Adam to several known simplified variants. We find that signed momentum methods are faster than SGD, but consistently underperform relative to Adam, even after careful tuning of momentum, clipping setting and learning rates. However, our analysis reveals a compelling option that preserves near-optimal performance while allowing for new insightful reformulations: constraining the Adam momentum parameters to be equal. Beyond robust performance, this choice affords new theoretical insights, highlights the "secret sauce" on top of signed momentum, and grants a precise statistical interpretation: we show that Adam in this setting implements a natural online algorithm for estimating the mean and variance of gradients-one that arises from a mean-field Gaussian variational inference perspective.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TuneComp: Joint Fine-tuning and Compression for Large Foundation Models</title>
<link>https://arxiv.org/abs/2505.21835</link>
<guid>https://arxiv.org/abs/2505.21835</guid>
<content:encoded><![CDATA[
<div> compression, knowledge distillation, low-rank approximation, pruning, model size reduction 

Summary: 
Joint fine-tuning and compression methods are proposed to reduce model size after training, aiming to create a smaller model directly guided by the downstream task. This approach avoids sacrificing performance and eliminates the need for a larger intermediate model. By gradually distilling the model to a pruned low-rank structure, the joint fine-tuning and compression method outperforms sequential compression techniques. Experimental results demonstrate the effectiveness of this approach in significantly reducing model size while maintaining performance in downstream tasks. <div>
arXiv:2505.21835v1 Announce Type: new 
Abstract: To reduce model size during post-training, compression methods, including knowledge distillation, low-rank approximation, and pruning, are often applied after fine-tuning the model. However, sequential fine-tuning and compression sacrifices performance, while creating a larger than necessary model as an intermediate step. In this work, we aim to reduce this gap, by directly constructing a smaller model while guided by the downstream task. We propose to jointly fine-tune and compress the model by gradually distilling it to a pruned low-rank structure. Experiments demonstrate that joint fine-tuning and compression significantly outperforms other sequential compression methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints</title>
<link>https://arxiv.org/abs/2505.21841</link>
<guid>https://arxiv.org/abs/2505.21841</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, safe learning, online learning, adversarial settings, optimal policy <br />
Summary: The article introduces the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, designed for online safe reinforcement learning in dynamic environments with unknown and adversarial constraints. OMDPD aims to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). The algorithm achieves optimal regret and strong constraint violation bounds without relying on strict assumptions about known safe policies. It is the first to address online CMDPs with adversarial constraints and can work effectively even in uncertain environments. The results demonstrate that OMDPD offers practical guarantees for safe decision-making in adversarial settings. Access to accurate estimates of rewards and transitions can further enhance the algorithm's performance, making it a valuable tool for applications in autonomous driving, robotics, and cybersecurity. <br /><br /> <div>
arXiv:2505.21841v1 Announce Type: new 
Abstract: Online safe reinforcement learning (RL) plays a key role in dynamic environments, with applications in autonomous driving, robotics, and cybersecurity. The objective is to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). Existing methods achieve sublinear regret under stochastic constraints but often fail in adversarial settings, where constraints are unknown, time-varying, and potentially adversarially designed. In this paper, we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the first to address online CMDPs with anytime adversarial constraints. OMDPD achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K)) without relying on Slater's condition or the existence of a strictly known safe policy. We further show that access to accurate estimates of rewards and transitions can further improve these bounds. Our results offer practical guarantees for safe decision-making in adversarial environments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Provable Approach for End-to-End Safe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.21852</link>
<guid>https://arxiv.org/abs/2505.21852</guid>
<content:encoded><![CDATA[
<div> safe reinforcement learning, Provably Lifetime Safe RL (PLS), offline learning, policy deployment, Gaussian processes

Summary:<br />
- This study introduces Provably Lifetime Safe RL (PLS) to address the challenge of ensuring policy safety from learning to operation.
- PLS combines offline safe RL with safe policy deployment and uses return-conditioned supervised learning.
- The method cautiously optimizes target returns using Gaussian processes, ensuring safety with high probability.
- The mathematical relationship between target and actual returns is analyzed and near-optimal target returns are guaranteed.
- Empirical results show that PLS outperforms baselines in safety and reward performance, achieving high rewards while maintaining policy safety throughout its lifetime.
<br /><br /> <div>
arXiv:2505.21852v1 Announce Type: new 
Abstract: A longstanding goal in safe reinforcement learning (RL) is a method to ensure the safety of a policy throughout the entire process, from learning to operation. However, existing safe RL paradigms inherently struggle to achieve this objective. We propose a method, called Provably Lifetime Safe RL (PLS), that integrates offline safe RL with safe policy deployment to address this challenge. Our proposed method learns a policy offline using return-conditioned supervised learning and then deploys the resulting policy while cautiously optimizing a limited set of parameters, known as target returns, using Gaussian processes (GPs). Theoretically, we justify the use of GPs by analyzing the mathematical relationship between target and actual returns. We then prove that PLS finds near-optimal target returns while guaranteeing safety with high probability. Empirically, we demonstrate that PLS outperforms baselines both in safety and reward performance, thereby achieving the longstanding goal to obtain high rewards while ensuring the safety of a policy throughout the lifetime from learning to operation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Bayesian Model Averaging in the Era of Foundation Models</title>
<link>https://arxiv.org/abs/2505.21857</link>
<guid>https://arxiv.org/abs/2505.21857</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian model averaging, ensemble learning, pre-trained models, classification, optimization

Summary:
In this study, the authors propose a novel approach to enhance classification performance on image and text data by leveraging Bayesian model averaging (BMA) with pre-trained or lightly-finetuned foundation models. They introduce trainable linear classifiers that take features from these models as inputs, allowing for model ensembling based on model posteriors. Additionally, they present a computationally efficient model averaging scheme (OMA) where ensemble weights are optimized to reduce prediction surprise. These methods provide a principled way to combine multiple models and enhance classification tasks, making it possible to incorporate future, improved foundation models for even better performance. <div>
arXiv:2505.21857v1 Announce Type: new 
Abstract: We revisit the classical, full-fledged Bayesian model averaging (BMA) paradigm to ensemble pre-trained and/or lightly-finetuned foundation models to enhance the classification performance on image and text data. To make BMA tractable under foundation models, we introduce trainable linear classifiers that take frozen features from the pre-trained foundation models as inputs. The model posteriors over the linear classifiers tell us which linear heads and frozen features are better suited for a given dataset, resulting in a principled model ensembling method. Furthermore, we propose a computationally cheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize the model ensemble weights, just like those weights based on model posterior distributions in BMA, by reducing the amount of surprise (expected entropy of the predictions) we get from predictions of ensembled models. With the rapid development of foundation models, these approaches will enable the incorporation of future, possibly significantly better foundation models to enhance the performance of challenging classification tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning</title>
<link>https://arxiv.org/abs/2505.21877</link>
<guid>https://arxiv.org/abs/2505.21877</guid>
<content:encoded><![CDATA[
<div> Keywords: Batch Normalisation, Federated Learning, Statistical Parameters, Hybrid Batch Normalisation, Global Statistics

Summary:
Hybrid Batch Normalisation (HBN) is introduced as a solution for the challenge of applying Batch Normalisation (BN) in federated learning, where data among client nodes is not identically distributed. HBN separates the update of statistical parameters from learnable parameters, allowing for unbiased estimation of global statistical parameters in distributed scenarios. By introducing a learnable hybrid distribution factor, HBN enables each computing node to adaptively mix current batch statistics with global statistics, improving federated learning performance. This approach emphasizes the importance of global statistics in shaping the learning process across varying federated learning settings, particularly beneficial for small batch sizes and heterogeneous data.<br /><br />Summary: <div>
arXiv:2505.21877v1 Announce Type: new 
Abstract: Batch Normalisation (BN) is widely used in conventional deep neural network training to harmonise the input-output distributions for each batch of data. However, federated learning, a distributed learning paradigm, faces the challenge of dealing with non-independent and identically distributed data among the client nodes. Due to the lack of a coherent methodology for updating BN statistical parameters, standard BN degrades the federated learning performance. To this end, it is urgent to explore an alternative normalisation solution for federated learning. In this work, we resolve the dilemma of the BN layer in federated learning by developing a customised normalisation approach, Hybrid Batch Normalisation (HBN). HBN separates the update of statistical parameters (i.e. , means and variances used for evaluation) from that of learnable parameters (i.e. , parameters that require gradient updates), obtaining unbiased estimates of global statistical parameters in distributed scenarios. In contrast with the existing solutions, we emphasise the supportive power of global statistics for federated learning. The HBN layer introduces a learnable hybrid distribution factor, allowing each computing node to adaptively mix the statistical parameters of the current batch with the global statistics. Our HBN can serve as a powerful plugin to advance federated learning performance. It reflects promising merits across a wide range of federated learning settings, especially for small batch sizes and heterogeneous data.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis</title>
<link>https://arxiv.org/abs/2505.21882</link>
<guid>https://arxiv.org/abs/2505.21882</guid>
<content:encoded><![CDATA[
<div> Keywords: tennis tournaments, momentum, HydraNet, multi-granularity analysis, state-space duality<br />
Summary:<br />
This study introduces a novel Momentum Score (MS) metric and HydraNet framework to model momentum in tennis tournaments. The framework integrates thirty-two dimensions of athlete performance and employs a Hydra module based on a state-space duality (SSD) framework to capture explicit and implicit momentum. It also includes a Versus Learning method to enhance the adversarial nature of momentum between athletes and a Collaborative-Adversarial Attention Mechanism (CAAM) for dynamic momentum integration. A million-level tennis cross-tournament dataset is used to validate the HydraNet framework's capabilities in modeling momentum at different granularities. Experimental results demonstrate that the MS metric provides valuable insights into the impact of momentum on match outcomes. This research establishes a new foundation for momentum modeling and sports analysis in tennis tournaments. Source code and datasets are available at https://github.com/ReyJerry/HydraNet. <br /> <div>
arXiv:2505.21882v1 Announce Type: new 
Abstract: In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects the dynamic shifts in performance of athletes that can decisively influence match outcomes. Despite its significance, momentum in terms of effective modeling and multi-granularity analysis across points, games, sets, and matches in tennis tournaments remains underexplored. In this study, we define a novel Momentum Score (MS) metric to quantify a player's momentum level in multi-granularity tennis tournaments, and design HydraNet, a momentum-driven state-space duality-based framework, to model MS by integrating thirty-two heterogeneous dimensions of athletes performance in serve, return, psychology and fatigue. HydraNet integrates a Hydra module, which builds upon a state-space duality (SSD) framework, capturing explicit momentum with a sliding-window mechanism and implicit momentum through cross-game state propagation. It also introduces a novel Versus Learning method to better enhance the adversarial nature of momentum between the two athletes at a macro level, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for capturing and integrating intra-player and inter-player dynamic momentum at a micro level. Additionally, we construct a million-level tennis cross-tournament dataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate the multi-granularity modeling capability of HydraNet for the MS metric on this dataset. Extensive experimental evaluations demonstrate that the MS metric constructed by the HydraNet framework provides actionable insights into how momentum impacts outcomes at different granularities, establishing a new foundation for momentum modeling and sports analysis. To the best of our knowledge. The source code and datasets are available at https://github.com/ReyJerry/HydraNet.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training</title>
<link>https://arxiv.org/abs/2505.21893</link>
<guid>https://arxiv.org/abs/2505.21893</guid>
<content:encoded><![CDATA[
<div> Keywords: Preference learning, generative models, diffusion models, optimization, off-policy bias

Summary: 
Preference learning is crucial for aligning generative models with human expectations, with recent advancements extending to diffusion models through methods like Direct Preference Optimization (DPO). However, challenges such as timestep-dependent instability and off-policy bias hinder the performance of existing approaches like Diffusion-DPO. To address these issues, a new practical strategy, DPO-C\&amp;M, is introduced to improve stability by clipping and masking uninformative timesteps while partially mitigating off-policy bias. Additionally, a principled framework, SDPO, incorporates importance sampling into the objective to fully correct off-policy bias and emphasize informative updates during the diffusion process. Experimental results on various datasets demonstrate that both DPO-C\&amp;M and SDPO outperform standard Diffusion-DPO in terms of VBench scores, human preference alignment, and training robustness. These findings underscore the significance of optimized, distribution-corrected techniques in diffusion-based preference learning.<br /><br />Summary: <div>
arXiv:2505.21893v1 Announce Type: new 
Abstract: Preference learning has become a central technique for aligning generative models with human expectations. Recently, it has been extended to diffusion models through methods like Direct Preference Optimization (DPO). However, existing approaches such as Diffusion-DPO suffer from two key challenges: timestep-dependent instability, caused by a mismatch between the reverse and forward diffusion processes and by high gradient variance in early noisy timesteps, and off-policy bias arising from the mismatch between optimization and data collection policies. We begin by analyzing the reverse diffusion trajectory and observe that instability primarily occurs at early timesteps with low importance weights. To address these issues, we first propose DPO-C\&amp;M, a practical strategy that improves stability by clipping and masking uninformative timesteps while partially mitigating off-policy bias. Building on this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a principled framework that incorporates importance sampling into the objective to fully correct for off-policy bias and emphasize informative updates during the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO, with SDPO achieving superior VBench scores, human preference alignment, and training robustness. These results highlight the importance of timestep-aware, distribution-corrected optimization in diffusion-based preference learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization</title>
<link>https://arxiv.org/abs/2505.21895</link>
<guid>https://arxiv.org/abs/2505.21895</guid>
<content:encoded><![CDATA[
arXiv:2505.21895v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) has become a standard approach for parameter-efficient fine-tuning, offering substantial reductions in trainable parameters by modeling updates as the product of two low-rank matrices. While effective, the low-rank constraint inherently limits representational capacity, often resulting in reduced performance compared to full-rank fine-tuning. Recent work by Ji et al. (2025) has addressed this limitation by applying a fixed-frequency sinusoidal transformation to low-rank adapters, increasing their stable rank without introducing additional parameters. This raises a crucial question: can the same sine-activated technique be successfully applied within the context of Post-Training Quantization to retain benefits even after model compression? In this paper, we investigate this question by extending the sinusoidal transformation framework to quantized LoRA adapters. We develop a theoretical analysis showing that the stable rank of a quantized adapter is tightly linked to that of its full-precision counterpart, motivating the use of such rank-enhancing functions even under quantization. Our results demonstrate that the expressivity gains from a sinusoidal non-linearity persist after quantization, yielding highly compressed adapters with negligible loss in performance. We validate our approach across a range of fine-tuning tasks for language, vision and text-to-image generation achieving significant memory savings while maintaining competitive accuracy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding</title>
<link>https://arxiv.org/abs/2505.21908</link>
<guid>https://arxiv.org/abs/2505.21908</guid>
<content:encoded><![CDATA[
arXiv:2505.21908v1 Announce Type: new 
Abstract: Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Transformer Without Using Learning Rate Warmup</title>
<link>https://arxiv.org/abs/2505.21910</link>
<guid>https://arxiv.org/abs/2505.21910</guid>
<content:encoded><![CDATA[
arXiv:2505.21910v1 Announce Type: new 
Abstract: Scaling Transformer to a large scale without using some technical tricks such as learning rate warump and using an obviously lower learning rate is an extremely challenging task, and is increasingly gaining more attention. In this paper, we provide a theoretical analysis for the process of training Transformer and reveal the rationale behind the model crash phenomenon in the training process, termed \textit{spectral energy concentration} of ${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse, where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the key in Transformer, respectively. To remedy this problem, motivated by \textit{Weyl's Inequality}, we present a novel optimization strategy, \ie, making the weight updating in successive steps smooth -- if the ratio $\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a threshold, we will automatically bound the learning rate to a weighted multiple of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla \bW_t$ is the updating quantity in step $t$. Such an optimization strategy can prevent spectral energy concentration to only a few directions, and thus can avoid malignant entropy collapse which will trigger the model crash. We conduct extensive experiments using ViT, Swin-Transformer and GPT, showing that our optimization strategy can effectively and stably train these Transformers without using learning rate warmup.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing</title>
<link>https://arxiv.org/abs/2505.21918</link>
<guid>https://arxiv.org/abs/2505.21918</guid>
<content:encoded><![CDATA[
arXiv:2505.21918v1 Announce Type: new 
Abstract: We developed a deep learning algorithm for human activity recognition using sensor signals as input. In this study, we built a pretrained language model based on the Transformer architecture, which is widely used in natural language processing. By leveraging this pretrained model, we aimed to improve performance on the downstream task of human activity recognition. While this task can be addressed using a vanilla Transformer, we propose an enhanced n-dimensional numerical processing Transformer that incorporates three key features: embedding n-dimensional numerical data through a linear layer, binning-based pre-processing, and a linear transformation in the output layer. We evaluated the effectiveness of our proposed model across five different datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15% improvements in accuracy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design</title>
<link>https://arxiv.org/abs/2505.21923</link>
<guid>https://arxiv.org/abs/2505.21923</guid>
<content:encoded><![CDATA[
arXiv:2505.21923v1 Announce Type: new 
Abstract: Designing analog circuits from performance specifications is a complex, multi-stage process encompassing topology selection, parameter inference, and layout feasibility. We introduce FALCON, a unified machine learning framework that enables fully automated, specification-driven analog circuit synthesis through topology selection and layout-constrained optimization. Given a target performance, FALCON first selects an appropriate circuit topology using a performance-driven classifier guided by human design heuristics. Next, it employs a custom, edge-centric graph neural network trained to map circuit topology and parameters to performance, enabling gradient-based parameter inference through the learned forward model. This inference is guided by a differentiable layout cost, derived from analytical equations capturing parasitic and frequency-dependent effects, and constrained by design rules. We train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave circuits, generated and simulated using Cadence Spectre across 20 expert-designed topologies. Through this evaluation, FALCON demonstrates >99\% accuracy in topology inference, <10\% relative error in performance prediction, and efficient layout-aware design that completes in under 1 second per instance. Together, these results position FALCON as a practical and extensible foundation model for end-to-end analog circuit design automation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets</title>
<link>https://arxiv.org/abs/2505.21930</link>
<guid>https://arxiv.org/abs/2505.21930</guid>
<content:encoded><![CDATA[
arXiv:2505.21930v1 Announce Type: new 
Abstract: This paper develops an ensemble method for fine-tuning a language model to multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are efficient when adapting to a single dataset. When training on multiple datasets of different tasks, a common setup in practice, it remains unclear how to design an efficient adaptation for fine-tuning language models. We propose to use an ensemble of multiple smaller adapters instead of a single adapter per task. We design an efficient algorithm that partitions $n$ datasets into $m$ groups, where $m$ is typically much smaller than $n$ in practice, and train one adapter for each group before taking a weighted combination to form the ensemble. The algorithm leverages a first-order approximation property of low-rank adaptation to quickly obtain the fine-tuning performances of dataset combinations since methods like LoRA stay close to the base model. Hence, we use the gradients of the base model to estimate its behavior during fine-tuning. Empirically, this approximation holds with less than $1\%$ error on models with up to $34$ billion parameters, leading to an estimation of true fine-tuning performances under $5\%$ error while speeding up computation compared to base fine-tuning by $105$ times. When applied to fine-tune Llama and GPT models on ten text classification tasks, our approach provides up to $10\%$ higher average test accuracy over QLoRA, with only $9\%$ more FLOPs. On a Llama model with $34$ billion parameters, an ensemble of QLoRA increases test accuracy by $3\%$ compared to QLoRA, with only $8\%$ more FLOPs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection</title>
<link>https://arxiv.org/abs/2505.21938</link>
<guid>https://arxiv.org/abs/2505.21938</guid>
<content:encoded><![CDATA[
arXiv:2505.21938v1 Announce Type: new 
Abstract: Adversarial attacks on stochastic bandits have traditionally relied on some unrealistic assumptions, such as per-round reward manipulation and unbounded perturbations, limiting their relevance to real-world systems. We propose a more practical threat model, Fake Data Injection, which reflects realistic adversarial constraints: the attacker can inject only a limited number of bounded fake feedback samples into the learner's history, simulating legitimate interactions. We design efficient attack strategies under this model, explicitly addressing both magnitude constraints (on reward values) and temporal constraints (on when and how often data can be injected). Our theoretical analysis shows that these attacks can mislead both Upper Confidence Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in nearly all rounds while incurring only sublinear attack cost. Experiments on synthetic and real-world datasets validate the effectiveness of our strategies, revealing significant vulnerabilities in widely used stochastic bandit algorithms under practical adversarial scenarios.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning Beyond Experience Rehearsal and Full Model Surrogates</title>
<link>https://arxiv.org/abs/2505.21942</link>
<guid>https://arxiv.org/abs/2505.21942</guid>
<content:encoded><![CDATA[
arXiv:2505.21942v1 Announce Type: new 
Abstract: Continual learning (CL) has remained a significant challenge for deep neural networks as learning new tasks erases previously acquired knowledge, either partially or completely. Existing solutions often rely on experience rehearsal or full model surrogates to mitigate CF. While effective, these approaches introduce substantial memory and computational overhead, limiting their scalability and applicability in real-world scenarios. To address this, we propose SPARC, a scalable CL approach that eliminates the need for experience rehearsal and full-model surrogates. By effectively combining task-specific working memories and task-agnostic semantic memory for cross-task knowledge consolidation, SPARC results in a remarkable parameter efficiency, using only 6% of the parameters required by full-model surrogates. Despite its lightweight design, SPARC achieves superior performance on Seq-TinyImageNet and matches rehearsal-based methods on various CL benchmarks. Additionally, weight re-normalization in the classification layer mitigates task-specific biases, establishing SPARC as a practical and scalable solution for CL under stringent efficiency constraints.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization</title>
<link>https://arxiv.org/abs/2505.21944</link>
<guid>https://arxiv.org/abs/2505.21944</guid>
<content:encoded><![CDATA[
arXiv:2505.21944v1 Announce Type: new 
Abstract: Two-way partial AUC (TPAUC) is a critical performance metric for binary classification with imbalanced data, as it focuses on specific ranges of the true positive rate (TPR) and false positive rate (FPR). However, stochastic algorithms for TPAUC optimization remain under-explored, with existing methods either limited to approximated TPAUC loss functions or burdened by sub-optimal complexities. To overcome these limitations, we introduce two innovative stochastic primal-dual double block-coordinate algorithms for TPAUC maximization. These algorithms utilize stochastic block-coordinate updates for both the primal and dual variables, catering to both convex and non-convex settings. We provide theoretical convergence rate analyses, demonstrating significant improvements over prior approaches. Our experimental results, based on multiple benchmark datasets, validate the superior performance of our algorithms, showcasing faster convergence and better generalization. This work advances the state of the art in TPAUC optimization and offers practical tools for real-world machine learning applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles</title>
<link>https://arxiv.org/abs/2505.21959</link>
<guid>https://arxiv.org/abs/2505.21959</guid>
<content:encoded><![CDATA[
arXiv:2505.21959v1 Announce Type: new 
Abstract: With Large Language Models (LLMs) rapidly approaching and potentially surpassing human-level performance, it has become imperative to develop approaches capable of effectively supervising and enhancing these powerful models using smaller, human-level models exposed to only human-level data. We address this critical weak-to-strong (W2S) generalization challenge by proposing a novel method aimed at improving weak experts, by training on the same limited human-level data, enabling them to generalize to complex, super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a token-level ensemble strategy that iteratively combines multiple weak experts, systematically addressing the shortcomings identified in preceding iterations. By continuously refining these weak models, we significantly enhance their collective ability to supervise stronger student models. We extensively evaluate the generalization performance of both the ensemble of weak experts and the subsequent strong student model across in-distribution (ID) and out-of-distribution (OOD) datasets. For OOD, we specifically introduce question difficulty as an additional dimension for defining distributional shifts. Our empirical results demonstrate notable improvements, achieving 4\%, and 3.2\% improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for experts and student models respectively, underscoring the effectiveness of our proposed method in advancing W2S generalization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging LLMs on a Simplex</title>
<link>https://arxiv.org/abs/2505.21972</link>
<guid>https://arxiv.org/abs/2505.21972</guid>
<content:encoded><![CDATA[
arXiv:2505.21972v1 Announce Type: new 
Abstract: Automated evaluation of free-form outputs from large language models (LLMs) is challenging because many distinct answers can be equally valid. A common practice is to use LLMs themselves as judges, but the theoretical properties of this approach are not yet well understood. We show that a geometric framework that represents both judges and candidates as points on a probability simplex can provide helpful insight on what is or is not identifiable using LLM judges. Our theoretical analysis uncovers a "phase transition" in ranking identifiability: for binary scoring systems, true rankings are identifiable even with weak judges under mild assumptions, while rankings become non-identifiable for three or more scoring levels even with infinite data, absent additional prior knowledge. This non-identifiability highlights how uncertainty in rankings stems from not only aleatoric uncertainty (i.e., inherent stochasticity in the data) but also epistemic uncertainty regarding which assumptions hold, an aspect that has received limited attention until now. To integrate both types of uncertainty, we use Bayesian inference to encode assumptions as priors and conduct sensitivity analysis of ranking estimates and credible intervals. Empirical evaluations across multiple benchmarks demonstrate that Bayesian inference yields more accurate rankings and substantially improves coverage rates. These results underscore the importance of taking a more holistic approach to uncertainty quantification when using LLMs as judges.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL</title>
<link>https://arxiv.org/abs/2505.21974</link>
<guid>https://arxiv.org/abs/2505.21974</guid>
<content:encoded><![CDATA[
arXiv:2505.21974v1 Announce Type: new 
Abstract: Bayesian optimization (BO) offers an efficient pipeline for optimizing black-box functions with the help of a Gaussian process prior and an acquisition function (AF). Recently, in the context of single-objective BO, learning-based AFs witnessed promising empirical results given its favorable non-myopic nature. Despite this, the direct extension of these approaches to multi-objective Bayesian optimization (MOBO) suffer from the \textit{hypervolume identifiability issue}, which results from the non-Markovian nature of MOBO problems. To tackle this, inspired by the non-Markovian RL literature and the success of Transformers in language modeling, we present a generalized deep Q-learning framework and propose \textit{BOFormer}, which substantiates this framework for MOBO via sequence modeling. Through extensive evaluation, we demonstrate that BOFormer constantly outperforms the benchmark rule-based and learning-based algorithms in various synthetic MOBO and real-world multi-objective hyperparameter optimization problems. We have made the source code publicly available to encourage further research in this direction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Feature Generation with Transformer and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.21978</link>
<guid>https://arxiv.org/abs/2505.21978</guid>
<content:encoded><![CDATA[
arXiv:2505.21978v1 Announce Type: new 
Abstract: Feature generation is a critical step in machine learning, aiming to enhance model performance by capturing complex relationships within the data and generating meaningful new features. Traditional feature generation methods heavily rely on domain expertise and manual intervention, making the process labor-intensive and challenging to adapt to different scenarios. Although automated feature generation techniques address these issues to some extent, they often face challenges such as feature redundancy, inefficiency in feature space exploration, and limited adaptability to diverse datasets and tasks. To address these problems, we propose a Two-Stage Feature Generation (TSFG) framework, which integrates a Transformer-based encoder-decoder architecture with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG leverages the Transformer's self-attention mechanism to efficiently represent and transform features, capturing complex dependencies within the data. PPO further enhances TSFG by dynamically adjusting the feature generation strategy based on task-specific feedback, optimizing the process for improved performance and adaptability. TSFG dynamically generates high-quality feature sets, significantly improving the predictive performance of machine learning models. Experimental results demonstrate that TSFG outperforms existing state-of-the-art methods in terms of feature quality and adaptability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning</title>
<link>https://arxiv.org/abs/2505.21987</link>
<guid>https://arxiv.org/abs/2505.21987</guid>
<content:encoded><![CDATA[
arXiv:2505.21987v1 Announce Type: new 
Abstract: With the rapid expansion of large language models (LLMs), the demand for memory and computational resources has grown significantly. Recent advances in LLM pruning aim to reduce the size and computational cost of these models. However, existing methods often suffer from either suboptimal pruning performance or low time efficiency during the pruning process. In this work, we propose an efficient and effective pruning method that simultaneously achieves high pruning performance and fast pruning speed with improved calibration efficiency. Our approach introduces two key innovations: (1) An activation cosine similarity loss-guided pruning metric, which considers the angular deviation of the output activation between the dense and pruned models. (2) An activation variance-guided pruning metric, which helps preserve semantic distinctions in output activations after pruning, enabling effective pruning with shorter input sequences. These two components can be readily combined to enhance LLM pruning in both accuracy and efficiency. Experimental results show that our method achieves up to an 18% reduction in perplexity and up to 63% decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning in Compact Spaces with Approximately Normalized Transformers</title>
<link>https://arxiv.org/abs/2505.22014</link>
<guid>https://arxiv.org/abs/2505.22014</guid>
<content:encoded><![CDATA[
arXiv:2505.22014v1 Announce Type: new 
Abstract: In deep learning, regularization and normalization are common solutions for challenges such as overfitting, numerical instabilities, and the increasing variance in the residual stream. An alternative approach is to force all parameters and representations to lie on a hypersphere. This removes the need for regularization and increases convergence speed, but comes with additional costs. In this work, we propose a more holistic but approximate normalization (anTransformer). Our approach constrains the norm of parameters and normalizes all representations via scalar multiplications motivated by the tight concentration of the norms of high-dimensional random vectors. When applied to GPT training, we observe a 40% faster convergence compared to models with QK normalization, with less than 3% additional runtime. Deriving scaling laws for anGPT, we found our method enables training with larger batch sizes and fewer hyperparameters, while matching the favorable scaling characteristics of classic GPT architectures.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly-Supervised Contrastive Learning for Imprecise Class Labels</title>
<link>https://arxiv.org/abs/2505.22028</link>
<guid>https://arxiv.org/abs/2505.22028</guid>
<content:encoded><![CDATA[
arXiv:2505.22028v1 Announce Type: new 
Abstract: Contrastive learning has achieved remarkable success in learning effective representations, with supervised contrastive learning often outperforming self-supervised approaches. However, in real-world scenarios, data annotations are often ambiguous or inaccurate, meaning that class labels may not reliably indicate whether two examples belong to the same class. This limitation restricts the applicability of supervised contrastive learning. To address this challenge, we introduce the concept of ``continuous semantic similarity'' to define positive and negative pairs. Instead of directly relying on imprecise class labels, we measure the semantic similarity between example pairs, which quantifies how closely they belong to the same category by iteratively refining weak supervisory signals. Based on this concept, we propose a graph-theoretic framework for weakly-supervised contrastive learning, where semantic similarity serves as the graph weights. Our framework is highly versatile and can be applied to many weakly-supervised learning scenarios. We demonstrate its effectiveness through experiments in two common settings, i.e., noisy label and partial label learning, where existing methods can be easily integrated to significantly improve performance. Theoretically, we establish an error bound for our approach, showing that it can approximate supervised contrastive learning under mild conditions. The implementation code is available at https://github.com/Speechless-10308/WSC.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2505.22041</link>
<guid>https://arxiv.org/abs/2505.22041</guid>
<content:encoded><![CDATA[
arXiv:2505.22041v1 Announce Type: new 
Abstract: Conformance checking techniques detect undesired process behavior by comparing process executions that are recorded in event logs to desired behavior that is captured in a dedicated process model. If such models are not available, conformance checking techniques are not applicable, but organizations might still be interested in detecting undesired behavior in their processes. To enable this, existing approaches use Large Language Models (LLMs), assuming that they can learn to distinguish desired from undesired behavior through fine-tuning. However, fine-tuning is highly resource-intensive and the fine-tuned LLMs often do not generalize well. To address these limitations, we propose an approach that requires neither a dedicated process model nor resource-intensive fine-tuning to detect undesired process behavior. Instead, we use Retrieval Augmented Generation (RAG) to provide an LLM with direct access to a knowledge base that contains both desired and undesired process behavior from other processes, assuming that the LLM can transfer this knowledge to the process at hand. Our evaluation shows that our approach outperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that RAG is a viable alternative to resource-intensive fine-tuning, particularly when enriched with relevant context from the event log, such as frequent traces and activities.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating the Effects of Sample Training Orders for Large Language Models without Retraining</title>
<link>https://arxiv.org/abs/2505.22042</link>
<guid>https://arxiv.org/abs/2505.22042</guid>
<content:encoded><![CDATA[
arXiv:2505.22042v1 Announce Type: new 
Abstract: The order of training samples plays a crucial role in large language models (LLMs), significantly impacting both their external performance and internal learning dynamics. Traditional methods for investigating this effect generally require retraining the model with various sample orders, which is computationally infeasible for LLMs. In this work, we improve traditional methods by designing a retraining-free framework. By approximating Adam optimizer updates with first- and second-order Taylor expansions and utilizing random projection methods to store intermediate checkpoints, our framework can efficiently estimate model parameters for arbitrary training sample orders. Next, we apply our framework to two downstream research problems: (1) Training curriculum design for LLMs -- we base our retraining-free framework to propose a novel curriculum learning strategy that augments curriculum proposals with estimated model performances, enabling more informed sample scheduling. (2) LLMs' memorization and generalization effect analysis -- we use our retraining-free framework to estimate how the positions of training samples influence LLMs' capacity for memorization and generalization. We conduct extensive experiments to validate the effectiveness of our retraining-free framework in reproducing the true model performances, and further demonstrate its potential in optimizing LLM training curricula and analyzing the memorization and generalization effects of LLMs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Generalized Sliced Wasserstein Plans</title>
<link>https://arxiv.org/abs/2505.22049</link>
<guid>https://arxiv.org/abs/2505.22049</guid>
<content:encoded><![CDATA[
arXiv:2505.22049v1 Announce Type: new 
Abstract: Optimal Transport (OT) has attracted significant interest in the machine learning community, not only for its ability to define meaningful distances between probability distributions -- such as the Wasserstein distance -- but also for its formulation of OT plans. Its computational complexity remains a bottleneck, though, and slicing techniques have been developed to scale OT to large datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a single one-dimensional plan back to the original multidimensional space, finally selecting the slice that yields the lowest Wasserstein distance as an approximation of the full OT plan. Despite its computational and theoretical advantages, min-SWGG inherits typical limitations of slicing methods: (i) the number of required slices grows exponentially with the data dimension, and (ii) it is constrained to linear projections. Here, we reformulate min-SWGG as a bilevel optimization problem and propose a differentiable approximation scheme to efficiently identify the optimal slice, even in high-dimensional settings. We furthermore define its generalized extension for accommodating to data living on manifolds. Finally, we demonstrate the practical value of our approach in various applications, including gradient flows on manifolds and high-dimensional spaces, as well as a novel sliced OT-based conditional flow matching for image generation -- where fast computation of transport plans is essential.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Resurrection of the ReLU</title>
<link>https://arxiv.org/abs/2505.22074</link>
<guid>https://arxiv.org/abs/2505.22074</guid>
<content:encoded><![CDATA[
arXiv:2505.22074v1 Announce Type: new 
Abstract: Modeling sophisticated activation functions within deep learning architectures has evolved into a distinct research direction. Functions such as GELU, SELU, and SiLU offer smooth gradients and improved convergence properties, making them popular choices in state-of-the-art models. Despite this trend, the classical ReLU remains appealing due to its simplicity, inherent sparsity, and other advantageous topological characteristics. However, ReLU units are prone to becoming irreversibly inactive - a phenomenon known as the dying ReLU problem - which limits their overall effectiveness. In this work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel, plug-and-play regularizer for deep architectures. SUGAR preserves the standard ReLU function during the forward pass but replaces its derivative in the backward pass with a smooth surrogate that avoids zeroing out gradients. We demonstrate that SUGAR, when paired with a well-chosen surrogate function, substantially enhances generalization performance over convolutional network architectures such as VGG-16 and ResNet-18, providing sparser activations while effectively resurrecting dead ReLUs. Moreover, we show that even in modern architectures like Conv2NeXt and Swin Transformer - which typically employ GELU - substituting these with SUGAR yields competitive and even slightly superior performance. These findings challenge the prevailing notion that advanced activation functions are necessary for optimal performance. Instead, they suggest that the conventional ReLU, particularly with appropriate gradient handling, can serve as a strong, versatile revived classic across a broad range of deep learning vision models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?</title>
<link>https://arxiv.org/abs/2505.22081</link>
<guid>https://arxiv.org/abs/2505.22081</guid>
<content:encoded><![CDATA[
arXiv:2505.22081v1 Announce Type: new 
Abstract: Symbolic regression aims to discover mathematical equations that fit given numerical data. It has been applied in various fields of scientific research, such as producing human-readable expressions that explain physical phenomena. Recently, Neural symbolic regression (NSR) methods that involve Transformers pre-trained on large-scale synthetic datasets have gained attention. While these methods offer advantages such as short inference time, they suffer from low performance, particularly when the number of input variables is large. In this study, we hypothesized that this limitation stems from the memorization bias of Transformers in symbolic regression. We conducted a quantitative evaluation of this bias in Transformers using a synthetic dataset and found that Transformers rarely generate expressions not present in the training data. Additional theoretical analysis reveals that this bias arises from the Transformer's inability to construct expressions compositionally while verifying their numerical validity. We finally examined if tailoring test-time strategies can lead to reduced memorization bias and better performance. We empirically demonstrate that providing additional information to the model at test time can significantly mitigate memorization bias. On the other hand, we also find that reducing memorization bias does not necessarily correlate with improved performance. These findings contribute to a deeper understanding of the limitations of NSR approaches and offer a foundation for designing more robust, generalizable symbolic regression methods. Code is available at https://github.com/Shun-0922/Mem-Bias-NSR .
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inclusive, Differentially Private Federated Learning for Clinical Data</title>
<link>https://arxiv.org/abs/2505.22108</link>
<guid>https://arxiv.org/abs/2505.22108</guid>
<content:encoded><![CDATA[
arXiv:2505.22108v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a promising approach for training clinical AI models without centralizing sensitive patient data. However, its real-world adoption is hindered by challenges related to privacy, resource constraints, and compliance. Existing Differential Privacy (DP) approaches often apply uniform noise, which disproportionately degrades model performance, even among well-compliant institutions. In this work, we propose a novel compliance-aware FL framework that enhances DP by adaptively adjusting noise based on quantifiable client compliance scores. Additionally, we introduce a compliance scoring tool based on key healthcare and security standards to promote secure, inclusive, and equitable participation across diverse clinical settings. Extensive experiments on public datasets demonstrate that integrating under-resourced, less compliant clinics with highly regulated institutions yields accuracy improvements of up to 15% over traditional FL. This work advances FL by balancing privacy, compliance, and performance, making it a viable solution for real-world clinical workflows in global healthcare.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The quest for the GRAph Level autoEncoder (GRALE)</title>
<link>https://arxiv.org/abs/2505.22109</link>
<guid>https://arxiv.org/abs/2505.22109</guid>
<content:encoded><![CDATA[
arXiv:2505.22109v1 Announce Type: new 
Abstract: Although graph-based learning has attracted a lot of attention, graph representation learning is still a challenging task whose resolution may impact key application fields such as chemistry or biology. To this end, we introduce GRALE, a novel graph autoencoder that encodes and decodes graphs of varying sizes into a shared embedding space. GRALE is trained using an Optimal Transport-inspired loss that compares the original and reconstructed graphs and leverages a differentiable node matching module, which is trained jointly with the encoder and decoder. The proposed attention-based architecture relies on Evoformer, the core component of AlphaFold, which we extend to support both graph encoding and decoding. We show, in numerical experiments on simulated and molecular data, that GRALE enables a highly general form of pre-training, applicable to a wide range of downstream tasks, from classification and regression to more complex tasks such as graph interpolation, editing, matching, and prediction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiMi Sheets: Infosheets for bias mitigation methods</title>
<link>https://arxiv.org/abs/2505.22114</link>
<guid>https://arxiv.org/abs/2505.22114</guid>
<content:encoded><![CDATA[
arXiv:2505.22114v1 Announce Type: new 
Abstract: Over the past 15 years, hundreds of bias mitigation methods have been proposed in the pursuit of fairness in machine learning (ML). However, algorithmic biases are domain-, task-, and model-specific, leading to a `portability trap': bias mitigation solutions in one context may not be appropriate in another. Thus, a myriad of design choices have to be made when creating a bias mitigation method, such as the formalization of fairness it pursues, and where and how it intervenes in the ML pipeline. This creates challenges in benchmarking and comparing the relative merits of different bias mitigation methods, and limits their uptake by practitioners.
  We propose BiMi Sheets as a portable, uniform guide to document the design choices of any bias mitigation method. This enables researchers and practitioners to quickly learn its main characteristics and to compare with their desiderata. Furthermore, the sheets' structure allow for the creation of a structured database of bias mitigation methods. In order to foster the sheets' adoption, we provide a platform for finding and creating BiMi Sheets at bimisheet.com.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL</title>
<link>https://arxiv.org/abs/2505.22151</link>
<guid>https://arxiv.org/abs/2505.22151</guid>
<content:encoded><![CDATA[
arXiv:2505.22151v1 Announce Type: new 
Abstract: A key challenge in offline multi-agent reinforcement learning (MARL) is achieving effective many-agent multi-step coordination in complex environments. In this work, we propose Oryx, a novel algorithm for offline cooperative MARL to directly address this challenge. Oryx adapts the recently proposed retention-based architecture Sable and combines it with a sequential form of implicit constraint Q-learning (ICQ), to develop a novel offline auto-regressive policy update scheme. This allows Oryx to solve complex coordination challenges while maintaining temporal coherence over lengthy trajectories. We evaluate Oryx across a diverse set of benchmarks from prior works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and continuous control, varying in scale and difficulty. Oryx achieves state-of-the-art performance on more than 80% of the 65 tested datasets, outperforming prior offline MARL methods and demonstrating robust generalisation across domains with many agents and long horizons. Finally, we introduce new datasets to push the limits of many-agent coordination in offline MARL, and demonstrate Oryx's superior ability to scale effectively in such settings. We will make all of our datasets, experimental data, and code available upon publication.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory</title>
<link>https://arxiv.org/abs/2505.22152</link>
<guid>https://arxiv.org/abs/2505.22152</guid>
<content:encoded><![CDATA[
arXiv:2505.22152v1 Announce Type: new 
Abstract: While uncertainty estimation for graphs recently gained traction, most methods rely on homophily and deteriorate in heterophilic settings. We address this by analyzing message passing neural networks from an information-theoretic perspective and developing a suitable analog to data processing inequality to quantify information throughout the model's layers. In contrast to non-graph domains, information about the node-level prediction target can increase with model depth if a node's features are semantically different from its neighbors. Therefore, on heterophilic graphs, the latent embeddings of an MPNN each provide different information about the data distribution - different from homophilic settings. This reveals that considering all node representations simultaneously is a key design principle for epistemic uncertainty estimation on graphs beyond homophily. We empirically confirm this with a simple post-hoc density estimator on the joint node embedding space that provides state-of-the-art uncertainty on heterophilic graphs. At the same time, it matches prior work on homophilic graphs without explicitly exploiting homophily through post-processing.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The informativeness of the gradient revisited</title>
<link>https://arxiv.org/abs/2505.22158</link>
<guid>https://arxiv.org/abs/2505.22158</guid>
<content:encoded><![CDATA[
arXiv:2505.22158v1 Announce Type: new 
Abstract: In the past decade gradient-based deep learning has revolutionized several applications. However, this rapid advancement has highlighted the need for a deeper theoretical understanding of its limitations. Research has shown that, in many practical learning tasks, the information contained in the gradient is so minimal that gradient-based methods require an exceedingly large number of iterations to achieve success. The informativeness of the gradient is typically measured by its variance with respect to the random selection of a target function from a hypothesis class.
  We use this framework and give a general bound on the variance in terms of a parameter related to the pairwise independence of the target function class and the collision entropy of the input distribution. Our bound scales as $ \tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $, where $ \tilde{\mathcal{O}} $ hides factors related to the regularity of the learning model and the loss function, $ \varepsilon $ measures the pairwise independence of the target function class and $\mathcal{E}_c$ is the collision entropy of the input distribution.
  To demonstrate the practical utility of our bound, we apply it to the class of Learning with Errors (LWE) mappings and high-frequency functions. In addition to the theoretical analysis, we present experiments to understand better the nature of recent deep learning-based attacks on LWE.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Augmentation-Aware Theory for Self-Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.22196</link>
<guid>https://arxiv.org/abs/2505.22196</guid>
<content:encoded><![CDATA[
arXiv:2505.22196v1 Announce Type: new 
Abstract: Self-supervised contrastive learning has emerged as a powerful tool in machine learning and computer vision to learn meaningful representations from unlabeled data. Meanwhile, its empirical success has encouraged many theoretical studies to reveal the learning mechanisms. However, in the existing theoretical research, the role of data augmentation is still under-exploited, especially the effects of specific augmentation types. To fill in the blank, we for the first time propose an augmentation-aware error bound for self-supervised contrastive learning, showing that the supervised risk is bounded not only by the unsupervised risk, but also explicitly by a trade-off induced by data augmentation. Then, under a novel semantic label assumption, we discuss how certain augmentation methods affect the error bound. Lastly, we conduct both pixel- and representation-level experiments to verify our proposed theoretical results.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer</title>
<link>https://arxiv.org/abs/2505.22199</link>
<guid>https://arxiv.org/abs/2505.22199</guid>
<content:encoded><![CDATA[
arXiv:2505.22199v1 Announce Type: new 
Abstract: Although deep neural networks have demonstrated significant success due to their powerful expressiveness, most models struggle to meet practical requirements for uncertainty estimation. Concurrently, the entangled nature of deep neural networks leads to a multifaceted problem, where various localized explanation techniques reveal that multiple unrelated features influence the decisions, thereby undermining interpretability. To address these challenges, we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates deep neural networks as a conditional Bayesian non-negative factor analysis. By leveraging stochastic latent variables, the BNDL can model complex dependencies and provide robust uncertainty estimation. Moreover, the sparsity and non-negativity of the latent variables encourage the model to learn disentangled representations and decision layers, thereby improving interpretability. We also offer theoretical guarantees that BNDL can achieve effective disentangled learning. In addition, we developed a corresponding variational inference method utilizing a Weibull variational inference network to approximate the posterior distribution of the latent variables. Our experimental results demonstrate that with enhanced disentanglement capabilities, BNDL not only improves the model's accuracy but also provides reliable uncertainty estimation and improved interpretability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.22203</link>
<guid>https://arxiv.org/abs/2505.22203</guid>
<content:encoded><![CDATA[
arXiv:2505.22203v1 Announce Type: new 
Abstract: Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL training results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct (i.e., false positives). This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique risks inherent to both rule-based and model-based verifiers, aiming to offer valuable insights to develop more robust reward systems in reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models</title>
<link>https://arxiv.org/abs/2505.22208</link>
<guid>https://arxiv.org/abs/2505.22208</guid>
<content:encoded><![CDATA[
arXiv:2505.22208v1 Announce Type: new 
Abstract: Neural network potentials (NNPs) are crucial for accelerating computational materials science by surrogating density functional theory (DFT) calculations. Improving their accuracy is possible through pre-training and fine-tuning, where an NNP model is first pre-trained on a large-scale dataset and then fine-tuned on a smaller target dataset. However, this approach is computationally expensive, mainly due to the cost of DFT-based dataset labeling and load imbalances during large-scale pre-training. To address this, we propose LaMM, a semi-supervised pre-training method incorporating improved denoising self-supervised learning and a load-balancing algorithm for efficient multi-node training. We demonstrate that our approach effectively leverages a large-scale dataset of $\sim$300 million semi-labeled samples to train a single NNP model, resulting in improved fine-tuning performance in terms of both speed and accuracy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solver-Free Decision-Focused Learning for Linear Optimization Problems</title>
<link>https://arxiv.org/abs/2505.22224</link>
<guid>https://arxiv.org/abs/2505.22224</guid>
<content:encoded><![CDATA[
arXiv:2505.22224v1 Announce Type: new 
Abstract: Mathematical optimization is a fundamental tool for decision-making in a wide range of applications. However, in many real-world scenarios, the parameters of the optimization problem are not known a priori and must be predicted from contextual features. This gives rise to predict-then-optimize problems, where a machine learning model predicts problem parameters that are then used to make decisions via optimization. A growing body of work on decision-focused learning (DFL) addresses this setting by training models specifically to produce predictions that maximize downstream decision quality, rather than accuracy. While effective, DFL is computationally expensive, because it requires solving the optimization problem with the predicted parameters at each loss evaluation. In this work, we address this computational bottleneck for linear optimization problems, a common class of problems in both DFL literature and real-world applications. We propose a solver-free training method that exploits the geometric structure of linear optimization to enable efficient training with minimal degradation in solution quality. Our method is based on the insight that a solution is optimal if and only if it achieves an objective value that is at least as good as that of its adjacent vertices on the feasible polytope. Building on this, our method compares the estimated quality of the ground-truth optimal solution with that of its precomputed adjacent vertices, and uses this as loss function. Experiments demonstrate that our method significantly reduces computational cost while maintaining high decision quality.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal kernel regression bounds under energy-bounded noise</title>
<link>https://arxiv.org/abs/2505.22235</link>
<guid>https://arxiv.org/abs/2505.22235</guid>
<content:encoded><![CDATA[
arXiv:2505.22235v1 Announce Type: new 
Abstract: Non-conservative uncertainty bounds are key for both assessing an estimation algorithm's accuracy and in view of downstream tasks, such as its deployment in safety-critical contexts. In this paper, we derive a tight, non-asymptotic uncertainty bound for kernel-based estimation, which can also handle correlated noise sequences. Its computation relies on a mild norm-boundedness assumption on the unknown function and the noise, returning the worst-case function realization within the hypothesis class at an arbitrary query input location. The value of this function is shown to be given in terms of the posterior mean and covariance of a Gaussian process for an optimal choice of the measurement noise covariance. By rigorously analyzing the proposed approach and comparing it with other results in the literature, we show its effectiveness in returning tight and easy-to-compute bounds for kernel-based estimates.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data</title>
<link>https://arxiv.org/abs/2505.22252</link>
<guid>https://arxiv.org/abs/2505.22252</guid>
<content:encoded><![CDATA[
arXiv:2505.22252v1 Announce Type: new 
Abstract: Understanding the reasoning behind deep learning model predictions is crucial in cheminformatics and drug discovery, where molecular design determines their properties. However, current evaluation frameworks for Explainable AI (XAI) in this domain often rely on artificial datasets or simplified tasks, employing data-derived metrics that fail to capture the complexity of real-world scenarios and lack a direct link to explanation faithfulness. To address this, we introduce B-XAIC, a novel benchmark constructed from real-world molecular data and diverse tasks with known ground-truth rationales for assigned labels. Through a comprehensive evaluation using B-XAIC, we reveal limitations of existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain. This benchmark provides a valuable resource for gaining deeper insights into the faithfulness of XAI, facilitating the development of more reliable and interpretable models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Online-Offline Framework for Co-Branding Campaign Recommendations</title>
<link>https://arxiv.org/abs/2505.22254</link>
<guid>https://arxiv.org/abs/2505.22254</guid>
<content:encoded><![CDATA[
arXiv:2505.22254v1 Announce Type: new 
Abstract: Co-branding has become a vital strategy for businesses aiming to expand market reach within recommendation systems. However, identifying effective cross-industry partnerships remains challenging due to resource imbalances, uncertain brand willingness, and ever-changing market conditions. In this paper, we provide the first systematic study of this problem and propose a unified online-offline framework to enable co-branding recommendations. Our approach begins by constructing a bipartite graph linking ``initiating'' and ``target'' brands to quantify co-branding probabilities and assess market benefits. During the online learning phase, we dynamically update the graph in response to market feedback, while striking a balance between exploring new collaborations for long-term gains and exploiting established partnerships for immediate benefits. To address the high initial co-branding costs, our framework mitigates redundant exploration, thereby enhancing short-term performance while ensuring sustainable strategic growth. In the offline optimization phase, our framework consolidates the interests of multiple sub-brands under the same parent brand to maximize overall returns, avoid excessive investment in single sub-brands, and reduce unnecessary costs associated with over-prioritizing a single sub-brand. We present a theoretical analysis of our approach, establishing a highly nontrivial sublinear regret bound for online learning in the complex co-branding problem, and enhancing the approximation guarantee for the NP-hard offline budget allocation optimization. Experiments on both synthetic and real-world co-branding datasets demonstrate the practical effectiveness of our framework, with at least 12\% improvement.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train Sparse Autoencoders Efficiently by Utilizing Features Correlation</title>
<link>https://arxiv.org/abs/2505.22255</link>
<guid>https://arxiv.org/abs/2505.22255</guid>
<content:encoded><![CDATA[
arXiv:2505.22255v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) have demonstrated significant promise in interpreting the hidden states of language models by decomposing them into interpretable latent directions. However, training SAEs at scale remains challenging, especially when large dictionary sizes are used. While decoders can leverage sparse-aware kernels for efficiency, encoders still require computationally intensive linear operations with large output dimensions. To address this, we propose KronSAE, a novel architecture that factorizes the latent representation via Kronecker product decomposition, drastically reducing memory and computational overhead. Furthermore, we introduce mAND, a differentiable activation function approximating the binary AND operation, which improves interpretability and performance in our factorized framework.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training</title>
<link>https://arxiv.org/abs/2505.22257</link>
<guid>https://arxiv.org/abs/2505.22257</guid>
<content:encoded><![CDATA[
arXiv:2505.22257v1 Announce Type: new 
Abstract: We revisit Group Relative Policy Optimization (GRPO) in both on-policy and off-policy optimization regimes. Our motivation comes from recent work on off-policy Proximal Policy Optimization (PPO), which improves training stability, sampling efficiency, and memory usage. In addition, a recent analysis of GRPO suggests that estimating the advantage function with off-policy samples could be beneficial. Building on these observations, we adapt GRPO to the off-policy setting. We show that both on-policy and off-policy GRPO objectives yield an improvement in the reward. This result motivates the use of clipped surrogate objectives in the off-policy version of GRPO. We then compare the empirical performance of reinforcement learning with verifiable rewards in post-training using both GRPO variants. Our results show that off-policy GRPO either significantly outperforms or performs on par with its on-policy counterpart.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Full Domain Analysis in Fluid Dynamics</title>
<link>https://arxiv.org/abs/2505.22275</link>
<guid>https://arxiv.org/abs/2505.22275</guid>
<content:encoded><![CDATA[
arXiv:2505.22275v1 Announce Type: new 
Abstract: Novel techniques in evolutionary optimization, simulation and machine learning allow for a broad analysis of domains like fluid dynamics, in which computation is expensive and flow behavior is complex. Under the term of full domain analysis we understand the ability to efficiently determine the full space of solutions in a problem domain, and analyze the behavior of those solutions in an accessible and interactive manner. The goal of full domain analysis is to deepen our understanding of domains by generating many examples of flow, their diversification, optimization and analysis. We define a formal model for full domain analysis, its current state of the art, and requirements of subcomponents. Finally, an example is given to show what we can learn by using full domain analysis. Full domain analysis, rooted in optimization and machine learning, can be a helpful tool in understanding complex systems in computational physics and beyond.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer</title>
<link>https://arxiv.org/abs/2505.22306</link>
<guid>https://arxiv.org/abs/2505.22306</guid>
<content:encoded><![CDATA[
arXiv:2505.22306v1 Announce Type: new 
Abstract: Cardiovascular signals such as photoplethysmography (PPG), electrocardiography (ECG), and blood pressure (BP) are inherently correlated and complementary, together reflecting the health of cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multi-modal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation, and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, while ensuring interpretability for human experts. These advantages position UniCardio as a promising avenue for advancing AI-assisted healthcare.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning</title>
<link>https://arxiv.org/abs/2505.22308</link>
<guid>https://arxiv.org/abs/2505.22308</guid>
<content:encoded><![CDATA[
arXiv:2505.22308v1 Announce Type: new 
Abstract: Pretraining on large, semantically rich datasets is key for developing language models. Surprisingly, recent studies have shown that even synthetic data, generated procedurally through simple semantic-free algorithms, can yield some of the same benefits as natural language pretraining. It is unclear what specific capabilities such simple synthetic data instils in a model, where these capabilities reside in the architecture, and how they manifest within its weights. In this short paper, we identify several beneficial forms of procedural data, together with specific algorithmic reasoning skills that improve in small transformers. Our core finding is that different procedural rules instil distinct but complementary inductive structures in the model. With extensive ablations and partial-transfer experiments, we discover that these structures reside in different parts of the model. Attention layers often carry the most transferable information, but some pretraining rules impart useful structure to MLP blocks instead. Most interestingly, the structures induced by multiple rules can be composed to jointly reinforce multiple capabilities. These results suggest an exciting possibility of disentangling the acquisition of knowledge from reasoning in language models, with the goal of improving their robustness and data efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization</title>
<link>https://arxiv.org/abs/2505.22310</link>
<guid>https://arxiv.org/abs/2505.22310</guid>
<content:encoded><![CDATA[
arXiv:2505.22310v1 Announce Type: new 
Abstract: Recent unlearning methods for LLMs are vulnerable to relearning attacks: knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of (even seemingly-unrelated) examples. We study this phenomenon in a controlled setting for example-level unlearning in vision classifiers. We make the surprising discovery that forget-set accuracy can recover from around 50% post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e., zero examples of the forget set. We observe this effect across a wide variety of unlearning methods, whereas for a model retrained from scratch excluding the forget set (gold standard), the accuracy remains at 50%. We observe that resistance to relearning attacks can be predicted by weight-space properties, specifically, $L_2$-distance and linear mode connectivity between the original and the unlearned model. Leveraging this insight, we propose a new class of methods that achieve state-of-the-art resistance to relearning attacks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skywork Open Reasoner 1 Technical Report</title>
<link>https://arxiv.org/abs/2505.22312</link>
<guid>https://arxiv.org/abs/2505.22312</guid>
<content:encoded><![CDATA[
arXiv:2505.22312v1 Announce Type: new 
Abstract: The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking BPS: A Utility-Based Evaluation Framework</title>
<link>https://arxiv.org/abs/2505.22316</link>
<guid>https://arxiv.org/abs/2505.22316</guid>
<content:encoded><![CDATA[
arXiv:2505.22316v1 Announce Type: new 
Abstract: Business process simulation (BPS) is a key tool for analyzing and optimizing organizational workflows, supporting decision-making by estimating the impact of process changes. The reliability of such estimates depends on the ability of a BPS model to accurately mimic the process under analysis, making rigorous accuracy evaluation essential. However, the state-of-the-art approach to evaluating BPS models has two key limitations. First, it treats simulation as a forecasting problem, testing whether models can predict unseen future events. This fails to assess how well a model captures the as-is process, particularly when process behavior changes from train to test period. Thus, it becomes difficult to determine whether poor results stem from an inaccurate model or the inherent complexity of the data, such as unpredictable drift. Second, the evaluation approach strongly relies on Earth Mover's Distance-based metrics, which can obscure temporal patterns and thus yield misleading conclusions about simulation quality. To address these issues, we propose a novel framework that evaluates simulation quality based on its ability to generate representative process behavior. Instead of comparing simulated logs to future real-world executions, we evaluate whether predictive process monitoring models trained on simulated data perform comparably to those trained on real data for downstream analysis tasks. Empirical results show that our framework not only helps identify sources of discrepancies but also distinguishes between model accuracy and data complexity, offering a more meaningful way to assess BPS quality.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective</title>
<link>https://arxiv.org/abs/2505.22322</link>
<guid>https://arxiv.org/abs/2505.22322</guid>
<content:encoded><![CDATA[
arXiv:2505.22322v1 Announce Type: new 
Abstract: Diffusion models have shown strong performance in generating high-quality tabular data, but they carry privacy risks by reproducing exact training samples. While prior work focuses on dataset-level augmentation to reduce memorization, little is known about which individual samples contribute most. We present the first data-centric study of memorization dynamics in tabular diffusion models. We quantify memorization for each real sample based on how many generated samples are flagged as replicas, using a relative distance ratio. Our empirical analysis reveals a heavy-tailed distribution of memorization counts: a small subset of samples contributes disproportionately to leakage, confirmed via sample-removal experiments. To understand this, we divide real samples into top- and non-top-memorized groups and analyze their training-time behaviors. We track when each sample is first memorized and monitor per-epoch memorization intensity (AUC). Memorized samples are memorized slightly earlier and show stronger signals in early training. Based on these insights, we propose DynamicCut, a two-stage, model-agnostic mitigation method: (a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and (c) retrain on the filtered dataset. Across multiple tabular datasets and models, DynamicCut reduces memorization with minimal impact on data diversity and downstream performance. It also complements augmentation-based defenses. Furthermore, DynamicCut enables cross-model transferability: high-ranked samples identified from one model (e.g., a diffusion model) are also effective for reducing memorization when removed from others, such as GANs and VAEs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.22355</link>
<guid>https://arxiv.org/abs/2505.22355</guid>
<content:encoded><![CDATA[
arXiv:2505.22355v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable to Full Fine-Tuning (FFT) while requiring significantly fewer computing resources, making it the go-to choice for researchers. We find that although PEFT can achieve competitive results on some benchmarks, its performance falls short of FFT in complex tasks, such as reasoning and instruction-based fine-tuning. In this paper, we compare the characteristics of PEFT and FFT in terms of representational capacity and robustness based on optimization theory. We theoretically demonstrate that PEFT is a strict subset of FFT. By providing theoretical upper bounds for PEFT, we show that the limited parameter space constrains the model's representational ability, making it more susceptible to perturbations. Experiments on 15 datasets encompassing classification, generation, reasoning, instruction fine-tuning tasks and 11 adversarial test sets validate our theories. We hope that these results spark further research beyond the realms of well established PEFT. The source code is in the anonymous Github repository\footnote{https://github.com/misonsky/PEFTEval}.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings</title>
<link>https://arxiv.org/abs/2505.22356</link>
<guid>https://arxiv.org/abs/2505.22356</guid>
<content:encoded><![CDATA[
arXiv:2505.22356v1 Announce Type: new 
Abstract: Deploying machine learning models in safety-critical domains poses a key challenge: ensuring reliable model performance on downstream user data without access to ground truth labels for direct validation. We propose the suitability filter, a novel framework designed to detect performance deterioration by utilizing suitability signals -- model output features that are sensitive to covariate shifts and indicative of potential prediction errors. The suitability filter evaluates whether classifier accuracy on unlabeled user data shows significant degradation compared to the accuracy measured on the labeled test dataset. Specifically, it ensures that this degradation does not exceed a pre-specified margin, which represents the maximum acceptable drop in accuracy. To achieve reliable performance evaluation, we aggregate suitability signals for both test and user data and compare these empirical distributions using statistical hypothesis testing, thus providing insights into decision uncertainty. Our modular method adapts to various models and domains. Empirical evaluations across different classification tasks demonstrate that the suitability filter reliably detects performance deviations due to covariate shift. This enables proactive mitigation of potential failures in high-stakes applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs</title>
<link>https://arxiv.org/abs/2505.22358</link>
<guid>https://arxiv.org/abs/2505.22358</guid>
<content:encoded><![CDATA[
arXiv:2505.22358v1 Announce Type: new 
Abstract: Large language models (LLMs) often suffer from catastrophic forgetting in continual learning (CL) scenarios, where performance on previously learned tasks degrades severely while training on sequentially arriving tasks. Although pioneering CL approaches using orthogonal subspaces can mitigate task interference, they typically employ fixed budget allocation, neglecting the varying complexity across tasks and layers. Besides, recent budget-adaptive tuning methods for LLMs often adopt multi-stage paradigms that decouple optimization and budget allocation. Such decoupling results in potential misalignment, which hinders those approaches' practical application in CL scenarios. To address these limitations, we propose OA-Adapter, a novel parameter-efficient approach for continual learning in LLMs that unifies dynamic budget adaptation with orthogonal subspace learning in a single end-to-end training stage. Specifically, OA-Adapter introduces a dynamic bottleneck dimension adaptation mechanism that simultaneously allocates an efficient parameter budget and optimizes task objectives without misalignment. To effectively preserve previously acquired knowledge while coordinating with the dynamic budget allocation, orthogonal constraints are applied specifically between the parameter subspace of the current task and the dynamically allocated parameter subspaces of historical tasks. Experimental results on continual learning benchmarks demonstrate that OA-Adapter outperforms state-of-the-art methods in both accuracy and parameter efficiency, achieving higher average accuracy while using 58.5% fewer parameters on the standard CL benchmark.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification</title>
<link>https://arxiv.org/abs/2505.22359</link>
<guid>https://arxiv.org/abs/2505.22359</guid>
<content:encoded><![CDATA[
arXiv:2505.22359v1 Announce Type: new 
Abstract: We study the generalization performance of unregularized gradient methods for separable linear classification. While previous work mostly deal with the binary case, we focus on the multiclass setting with $k$ classes and establish novel population risk bounds for Gradient Descent for loss functions that decay to zero. In this setting, we show risk bounds that reveal that convergence rates are crucially influenced by the geometry of the loss template, as formalized by Wang and Scott (2024), rather than of the loss function itself. Particularly, we establish risk upper bounds that holds for any decay rate of the loss whose template is smooth with respect to the $p$-norm. In the case of exponentially decaying losses, our results indicates a contrast between the $p=\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and $p=2$ where the risk scales linearly with $k$. To establish this separation formally, we also prove a lower bound in the latter scenario, demonstrating that the polynomial dependence on $k$ is unavoidable. Central to our analysis is a novel bound on the Rademacher complexity of low-noise vector-valued linear predictors with a loss template smooth w.r.t.~general $p$-norms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles</title>
<link>https://arxiv.org/abs/2505.22361</link>
<guid>https://arxiv.org/abs/2505.22361</guid>
<content:encoded><![CDATA[
arXiv:2505.22361v1 Announce Type: new 
Abstract: This paper studies a bandit optimization problem where the goal is to maximize a function $f(x)$ over $T$ periods for some unknown strongly concave function $f$. We consider a new pairwise comparison oracle, where the decision-maker chooses a pair of actions $(x, x')$ for a consecutive number of periods and then obtains an estimate of $f(x)-f(x')$. We show that such a pairwise comparison oracle finds important applications to joint pricing and inventory replenishment problems and network revenue management. The challenge in this bandit optimization is twofold. First, the decision-maker not only needs to determine a pair of actions $(x, x')$ but also a stopping time $n$ (i.e., the number of queries based on $(x, x')$). Second, motivated by our inventory application, the estimate of the difference $f(x)-f(x')$ is biased, which is different from existing oracles in stochastic optimization literature. To address these challenges, we first introduce a discretization technique and local polynomial approximation to relate this problem to linear bandits. Then we developed a tournament successive elimination technique to localize the discretized cell and run an interactive batched version of LinUCB algorithm on cells. We establish regret bounds that are optimal up to poly-logarithmic factors. Furthermore, we apply our proposed algorithm and analytical framework to the two operations management problems and obtain results that improve state-of-the-art results in the existing literature.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed Homophily-Aware Graph Neural Network</title>
<link>https://arxiv.org/abs/2505.22362</link>
<guid>https://arxiv.org/abs/2505.22362</guid>
<content:encoded><![CDATA[
arXiv:2505.22362v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have achieved significant success in various learning tasks on graph-structured data. Nevertheless, most GNNs struggle to generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the directional nature of real-world graphs, resulting in suboptimal performance on directed graphs with asymmetric structures. In this work, we propose Directed Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses these limitations by incorporating homophily-aware and direction-sensitive components. DHGNN employs a resettable gating mechanism to adaptively modulate message contributions based on homophily levels and informativeness, and a structure-aware noise-tolerant fusion module to effectively integrate node representations from the original and reverse directions. Extensive experiments on both homophilic and heterophilic directed graph datasets demonstrate that DHGNN outperforms state-of-the-art methods in node classification and link prediction. In particular, DHGNN improves over the best baseline by up to 15.07% in link prediction. Our analysis further shows that the gating mechanism captures directional homophily gaps and fluctuating homophily across layers, providing deeper insights into message-passing behavior on complex graph structures.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting</title>
<link>https://arxiv.org/abs/2505.22370</link>
<guid>https://arxiv.org/abs/2505.22370</guid>
<content:encoded><![CDATA[
arXiv:2505.22370v1 Announce Type: new 
Abstract: Continual Learning requires a model to learn multiple tasks in sequence while maintaining both stability:preserving knowledge from previously learned tasks, and plasticity:effectively learning new tasks. Gradient projection has emerged as an effective and popular paradigm in CL, where it partitions the gradient space of previously learned tasks into two orthogonal subspaces: a primary subspace and a minor subspace. New tasks are learned effectively within the minor subspace, thereby reducing interference with previously acquired knowledge. However, existing Gradient Projection methods struggle to achieve an optimal balance between plasticity and stability, as it is hard to appropriately partition the gradient space. In this work, we consider a continual learning paradigm based on Low-Rank Adaptation, which has gained considerable attention due to its efficiency and wide applicability, and propose a novel approach for continual learning, called SplitLoRA. We first provide a theoretical analysis of how subspace partitioning affects model stability and plasticity. Informed by this analysis, we then introduce an effective method that derives the optimal partition of the gradient space for previously learned tasks. This approach effectively balances stability and plasticity in continual learning. Experimental results on multiple datasets demonstrate that the proposed method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation</title>
<link>https://arxiv.org/abs/2505.22381</link>
<guid>https://arxiv.org/abs/2505.22381</guid>
<content:encoded><![CDATA[
arXiv:2505.22381v1 Announce Type: new 
Abstract: Business Process Simulation (BPS) is a critical tool for analyzing and improving organizational processes by estimating the impact of process changes. A key component of BPS is the case-arrival model, which determines the pattern of new case entries into a process. Although accurate case-arrival modeling is essential for reliable simulations, as it influences waiting and overall cycle times, existing approaches often rely on oversimplified static distributions of inter-arrival times. These approaches fail to capture the dynamic and temporal complexities inherent in organizational environments, leading to less accurate and reliable outcomes. To address this limitation, we propose Auto Time Kernel Density Estimation (AT-KDE), a divide-and-conquer approach that models arrival times of processes by incorporating global dynamics, day-of-week variations, and intraday distributional changes, ensuring both precision and scalability. Experiments conducted across 20 diverse processes demonstrate that AT-KDE is far more accurate and robust than existing approaches while maintaining sensible execution time efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning</title>
<link>https://arxiv.org/abs/2505.22389</link>
<guid>https://arxiv.org/abs/2505.22389</guid>
<content:encoded><![CDATA[
arXiv:2505.22389v1 Announce Type: new 
Abstract: Continual Learning (CL) aims to enable models to continuously acquire new knowledge from a sequence of tasks with avoiding the forgetting of learned information. However, existing CL methods only rely on the parameters of the most recent task for inference, which makes them susceptible to catastrophic forgetting. Inspired by the recent success of model merging techniques, we propose \textbf{Perturb-and-Merge (P\&amp;M)}, a novel continual learning framework that integrates model merging into the CL paradigm to mitigate forgetting. Specifically, after training on each task, P\&amp;M constructs a new model by forming a convex combination of the previous model and the newly trained task-specific model. Through theoretical analysis, we minimize the total loss increase across all tasks and derive an analytical solution for the optimal merging coefficient. To further improve the performance of the merged model, we observe that the degradation introduced during merging can be alleviated by a regularization term composed of the task vector and the Hessian matrix of the loss function. Interestingly, we show that this term can be efficiently approximated using second-order symmetric finite differences, and a stochastic perturbation strategy along the task vector direction is accordingly devised which incurs no additional forward or backward passes while providing an effective approximation of the regularization term. Finally, we combine P\&amp;M with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead. Our proposed approach achieves state-of-the-art performance on several continual learning benchmark datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation</title>
<link>https://arxiv.org/abs/2505.22391</link>
<guid>https://arxiv.org/abs/2505.22391</guid>
<content:encoded><![CDATA[
arXiv:2505.22391v1 Announce Type: new 
Abstract: Modeling physical systems in a generative manner offers several advantages, including the ability to handle partial observations, generate diverse solutions, and address both forward and inverse problems. Recently, diffusion models have gained increasing attention in the modeling of physical systems, particularly those governed by partial differential equations (PDEs). However, diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate steps, making it infeasible to directly enforce constraints on the clean sample $\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are typically applied to the expectation of clean samples $\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the learned score network. However, imposing PDE constraints on the expectation does not strictly represent the one on the true clean data, known as Jensen's Gap. This gap creates a trade-off: enforcing PDE constraints may come at the cost of reduced accuracy in generative modeling. To address this, we propose a simple yet effective post-hoc distillation approach, where PDE constraints are not injected directly into the diffusion process, but instead enforced during a post-hoc distillation stage. We term our method as Physics-Informed Distillation of Diffusion Models (PIDDM). This distillation not only facilitates single-step generation with improved PDE satisfaction, but also support both forward and inverse problem solving and reconstruction from randomly partial observation. Extensive experiments across various PDE benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over several recent and competitive baselines, such as PIDM, DiffusionPDE, and ECI-sampling, with less computation overhead. Our approach can shed light on more efficient and effective strategies for incorporating physical constraints into diffusion models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Overthinking in Large Reasoning Models via Manifold Steering</title>
<link>https://arxiv.org/abs/2505.22411</link>
<guid>https://arxiv.org/abs/2505.22411</guid>
<content:encoded><![CDATA[
arXiv:2505.22411v1 Announce Type: new 
Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in solving complex tasks such as mathematics and coding. However, these models frequently exhibit a phenomenon known as overthinking during inference, characterized by excessive validation loops and redundant deliberation, leading to substantial computational overheads. In this paper, we aim to mitigate overthinking by investigating the underlying mechanisms from the perspective of mechanistic interpretability. We first showcase that the tendency of overthinking can be effectively captured by a single direction in the model's activation space and the issue can be eased by intervening the activations along this direction. However, this efficacy soon reaches a plateau and even deteriorates as the intervention strength increases. We therefore systematically explore the activation space and find that the overthinking phenomenon is actually tied to a low-dimensional manifold, which indicates that the limited effect stems from the noises introduced by the high-dimensional steering direction. Based on this insight, we propose Manifold Steering, a novel approach that elegantly projects the steering direction onto the low-dimensional activation manifold given the theoretical approximation of the interference noise. Extensive experiments on DeepSeek-R1 distilled models validate that our method reduces output tokens by up to 71% while maintaining and even improving the accuracy on several mathematical benchmarks. Our method also exhibits robust cross-domain transferability, delivering consistent token reduction performance in code generation and knowledge-based QA tasks. Code is available at: https://github.com/Aries-iai/Manifold_Steering.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals</title>
<link>https://arxiv.org/abs/2505.22422</link>
<guid>https://arxiv.org/abs/2505.22422</guid>
<content:encoded><![CDATA[
arXiv:2505.22422v1 Announce Type: new 
Abstract: The construction of confidence intervals for the mean of a bounded random variable is a classical problem in statistics with numerous applications in machine learning and virtually all scientific fields. In particular, obtaining the tightest possible confidence intervals is vital every time the sampling of the random variables is expensive. The current state-of-the-art method to construct confidence intervals is by using betting algorithms. This is a very successful approach for deriving optimal confidence sequences, even matching the rate of law of iterated logarithms. However, in the fixed horizon setting, these approaches are either sub-optimal or based on heuristic solutions with strong empirical performance but without a finite-time guarantee. Hence, no betting-based algorithm guaranteeing the optimal $\mathcal{O}(\sqrt{\frac{\sigma^2\log\frac1\delta}{n}})$ width of the confidence intervals are known. This work bridges this gap. We propose a betting-based algorithm to compute confidence intervals that empirically outperforms the competitors. Our betting strategy uses the optimal strategy in every step (in a certain sense), whereas the standard betting methods choose a constant strategy in advance. Leveraging this fact results in strict improvements even for classical concentration inequalities, such as the ones of Hoeffding or Bernstein. Moreover, we also prove that the width of our confidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$. The code is available on~https://github.com/vvoracek/STaR-bets-confidence-interval.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Reasoning without Attention</title>
<link>https://arxiv.org/abs/2505.22425</link>
<guid>https://arxiv.org/abs/2505.22425</guid>
<content:encoded><![CDATA[
arXiv:2505.22425v1 Announce Type: new 
Abstract: Large language models (LLMs) have made significant advances in complex reasoning tasks, yet they remain bottlenecked by two core challenges: architectural inefficiency due to reliance on Transformers, and a lack of structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an attention-free language model that addresses both issues through architectural and data-centric innovations. Built on the state space dual (SSD) layers of Mamba-2, our model eliminates the need for self-attention and key-value caching, enabling fixed-memory, constant-time inference. To train it for complex reasoning, we propose a two-phase curriculum fine-tuning strategy based on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically structured problems via abstract concept selection and rationale-guided generation. On benchmark evaluations, \ourmodel-7B outperforms strong Transformer and hybrid models of comparable scale, and even surpasses the much larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on Livecodebench. These results highlight the potential of state space models as efficient and scalable alternatives to attention-based architectures for high-capacity reasoning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models</title>
<link>https://arxiv.org/abs/2505.22440</link>
<guid>https://arxiv.org/abs/2505.22440</guid>
<content:encoded><![CDATA[
arXiv:2505.22440v1 Announce Type: new 
Abstract: The rapid evolution of wireless technologies necessitates automated design frameworks to address antenna miniaturization and performance optimization within constrained development cycles. This study demonstrates a machine learning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm Optimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design. The QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds, achieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared to conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest, XGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds using 936 simulation datasets, with stacked models showing superior training accuracy (R2=0.9825) and SVM demonstrating optimal validation performance (R2=0.7197). The complete design cycle, encompassing optimization, prediction, and ANSYS validation, required 12.42 minutes on standard desktop hardware (Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of PSADEA-based approaches. This 240 times of acceleration eliminates traditional trial-and-error methods that often extend beyond seven expert-led days. The system enables precise specifications of performance targets with automated generation of fabrication-ready parameters, particularly benefiting compact consumer devices requiring rapid frequency tuning. By bridging AI-driven optimization with CAD validation, this framework reduces engineering workloads while ensuring production-ready designs, establishing a scalable paradigm for next-generation RF systems in 6G and IoT applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.22442</link>
<guid>https://arxiv.org/abs/2505.22442</guid>
<content:encoded><![CDATA[
arXiv:2505.22442v1 Announce Type: new 
Abstract: Sample efficiency remains a major obstacle for real world adoption of reinforcement learning (RL): success has been limited to settings where simulators provide access to essentially unlimited environment interactions, which in reality are typically costly or dangerous to obtain. Offline RL in principle offers a solution by exploiting offline data to learn a near-optimal policy before deployment. In practice, however, current offline RL methods rely on extensive online interactions for hyperparameter tuning, and have no reliable bound on their initial online performance. To address these two issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe offline reinforcement learning. Using only offline data, our Bayesian approach infers a posterior over environment dynamics to obtain a reliable estimate of the online performance via the posterior predictive uncertainty. Crucially, all hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a tuning for offline reinforcement learning algorithm that extends our information rate based offline hyperparameter tuning methods to general offline RL approaches. Our empirical evaluation confirms SOReL's ability to accurately estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter tuning achieves competitive performance with the best online hyperparameter tuning methods using only offline data. Thus, SOReL and TOReL make a significant step towards safe and reliable offline RL, unlocking the potential for RL in the real world. Our implementations are publicly available: https://github.com/CWibault/sorel\_torel.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: All Current Generative Fidelity and Diversity Metrics are Flawed</title>
<link>https://arxiv.org/abs/2505.22450</link>
<guid>https://arxiv.org/abs/2505.22450</guid>
<content:encoded><![CDATA[
arXiv:2505.22450v1 Announce Type: new 
Abstract: Any method's development and practical application is limited by our ability to measure its reliability. The popularity of generative modeling emphasizes the importance of good synthetic data metrics. Unfortunately, previous works have found many failure cases in current metrics, for example lack of outlier robustness and unclear lower and upper bounds. We propose a list of desiderata for synthetic data metrics, and a suite of sanity checks: carefully chosen simple experiments that aim to detect specific and known generative modeling failure modes. Based on these desiderata and the results of our checks, we arrive at our position: all current generative fidelity and diversity metrics are flawed. This significantly hinders practical use of synthetic data. Our aim is to convince the research community to spend more effort in developing metrics, instead of models. Additionally, through analyzing how current metrics fail, we provide practitioners with guidelines on how these metrics should (not) be used.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pure Exploration with Infinite Answers</title>
<link>https://arxiv.org/abs/2505.22473</link>
<guid>https://arxiv.org/abs/2505.22473</guid>
<content:encoded><![CDATA[
arXiv:2505.22473v1 Announce Type: new 
Abstract: We study pure exploration problems where the set of correct answers is possibly infinite, e.g., the regression of any continuous function of the means of the bandit. We derive an instance-dependent lower bound for these problems. By analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop) for finite answer problems fail at being asymptotically optimal in this more general setting. Finally, we present a framework, Sticky-Sequence Track-and-Stop, which generalizes both Track-and-Stop and Sticky Track-and-Stop, and that enjoys asymptotic optimality. Due to its generality, our analysis also highlights special cases where existing methods enjoy optimality.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis</title>
<link>https://arxiv.org/abs/2505.22474</link>
<guid>https://arxiv.org/abs/2505.22474</guid>
<content:encoded><![CDATA[
arXiv:2505.22474v1 Announce Type: new 
Abstract: The forecasting of multivariate urban data presents a complex challenge due to the intricate dependencies between various urban metrics such as weather, air pollution, carbon intensity, and energy demand. This paper introduces a novel multivariate time-series forecasting model that utilizes advanced Graph Neural Networks (GNNs) to capture spatial dependencies among different time-series variables. The proposed model incorporates a decomposition-based preprocessing step, isolating trend, seasonal, and residual components to enhance the accuracy and interpretability of forecasts. By leveraging the dynamic capabilities of GNNs, the model effectively captures interdependencies and improves the forecasting performance. Extensive experiments on real-world datasets, including electricity usage, weather metrics, carbon intensity, and air pollution data, demonstrate the effectiveness of the proposed approach across various forecasting scenarios. The results highlight the potential of the model to optimize smart infrastructure systems, contributing to energy-efficient urban development and enhanced public well-being.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Asymptotic Analysis of (Sticky) Track-and-Stop</title>
<link>https://arxiv.org/abs/2505.22475</link>
<guid>https://arxiv.org/abs/2505.22475</guid>
<content:encoded><![CDATA[
arXiv:2505.22475v1 Announce Type: new 
Abstract: In pure exploration problems, a statistician sequentially collects information to answer a question about some stochastic and unknown environment. The probability of returning a wrong answer should not exceed a maximum risk parameter $\delta$ and good algorithms make as few queries to the environment as possible. The Track-and-Stop algorithm is a pioneering method to solve these problems. Specifically, it is well-known that it enjoys asymptotic optimality sample complexity guarantees for $\delta\to 0$ whenever the map from the environment to its correct answers is single-valued (e.g., best-arm identification with a unique optimal arm). The Sticky Track-and-Stop algorithm extends these results to settings where, for each environment, there might exist multiple correct answers (e.g., $\epsilon$-optimal arm identification). Although both methods are optimal in the asymptotic regime, their non-asymptotic guarantees remain unknown. In this work, we fill this gap and provide non-asymptotic guarantees for both algorithms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look at Multimodal Representation Collapse</title>
<link>https://arxiv.org/abs/2505.22483</link>
<guid>https://arxiv.org/abs/2505.22483</guid>
<content:encoded><![CDATA[
arXiv:2505.22483v1 Announce Type: new 
Abstract: We aim to develop a fundamental understanding of modality collapse, a recently observed empirical phenomenon wherein models trained for multimodal fusion tend to rely only on a subset of the modalities, ignoring the rest. We show that modality collapse happens when noisy features from one modality are entangled, via a shared set of neurons in the fusion head, with predictive features from another, effectively masking out positive contributions from the predictive features of the former modality and leading to its collapse. We further prove that cross-modal knowledge distillation implicitly disentangles such representations by freeing up rank bottlenecks in the student encoder, denoising the fusion-head outputs without negatively impacting the predictive features from either modality. Based on the above findings, we propose an algorithm that prevents modality collapse through explicit basis reallocation, with applications in dealing with missing modalities. Extensive experiments on multiple multimodal benchmarks validate our theoretical claims. Project page: https://abhrac.github.io/mmcollapse/.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Adversarial Training with Energy-based Models</title>
<link>https://arxiv.org/abs/2505.22486</link>
<guid>https://arxiv.org/abs/2505.22486</guid>
<content:encoded><![CDATA[
arXiv:2505.22486v1 Announce Type: new 
Abstract: We aim at using Energy-based Model (EBM) framework to better understand adversarial training (AT) in classifiers, and additionally to analyze the intrinsic generative capabilities of robust classifiers. By viewing standard classifiers through an energy lens, we begin by analyzing how the energies of adversarial examples, generated by various attacks, differ from those of the natural samples. The central focus of our work is to understand the critical phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT from an energy perspective. We analyze the impact of existing AT approaches on the energy of samples during training and observe that the behavior of the ``delta energy' -- change in energy between original sample and its adversarial counterpart -- diverges significantly when CO or RO occurs. After a thorough analysis of these energy dynamics and their relationship with overfitting, we propose a novel regularizer, the Delta Energy Regularizer (DER), designed to smoothen the energy landscape during training. We demonstrate that DER is effective in mitigating both CO and RO across multiple benchmarks. We further show that robust classifiers, when being used as generative models, have limits in handling trade-off between image quality and variability. We propose an improved technique based on a local class-wise principal component analysis (PCA) and energy-based guidance for better class-specific initialization and adaptive stopping, enhancing sample diversity and generation quality. Considering that we do not explicitly train for generative modeling, we achieve a competitive Inception Score (IS) and Fr\'echet inception distance (FID) compared to hybrid discriminative-generative models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling</title>
<link>https://arxiv.org/abs/2505.22491</link>
<guid>https://arxiv.org/abs/2505.22491</guid>
<content:encoded><![CDATA[
arXiv:2505.22491v1 Announce Type: new 
Abstract: The dominant paradigm for training large-scale vision and language models is He initialization and a single global learning rate (\textit{standard parameterization}, SP). Despite its practical success, standard parametrization remains poorly understood from a theoretical perspective: Existing infinite-width theory would predict instability under large learning rates and vanishing feature learning under stable learning rates. However, empirically optimal learning rates consistently decay much slower than theoretically predicted. By carefully studying neural network training dynamics, we demonstrate that this discrepancy is not fully explained by finite-width phenomena such as catapult effects or a lack of alignment between weights and incoming activations. We instead show that the apparent contradiction can be fundamentally resolved by taking the loss function into account: In contrast to Mean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an intermediate \textit{controlled divergence} regime emerges, where logits diverge but loss, gradients, and activations remain stable. Stable training under large learning rates enables persistent feature evolution at scale in all hidden layers, which is crucial for the practical success of SP. In experiments across optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities (vision, language), we validate that neural networks operate in this controlled divergence regime under CE loss but not under MSE loss. Our empirical evidence suggests that width-scaling considerations are surprisingly useful for predicting empirically optimal learning rate exponents. Finally, our analysis clarifies the effectiveness and limitations of recently proposed layerwise learning rate scalings for standard initialization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation</title>
<link>https://arxiv.org/abs/2505.22492</link>
<guid>https://arxiv.org/abs/2505.22492</guid>
<content:encoded><![CDATA[
arXiv:2505.22492v1 Announce Type: new 
Abstract: This paper studies off-policy evaluation (OPE) in reinforcement learning with a focus on behavior policy estimation for importance sampling. Prior work has shown empirically that estimating a history-dependent behavior policy can lead to lower mean squared error (MSE) even when the true behavior policy is Markovian. However, the question of why the use of history should lower MSE remains open. In this paper, we theoretically demystify this paradox by deriving a bias-variance decomposition of the MSE of ordinary importance sampling (IS) estimators, demonstrating that history-dependent behavior policy estimation decreases their asymptotic variances while increasing their finite-sample biases. Additionally, as the estimated behavior policy conditions on a longer history, we show a consistent decrease in variance. We extend these findings to a range of other OPE estimators, including the sequential IS estimator, the doubly robust estimator and the marginalized IS estimator, with the behavior policy estimated either parametrically or non-parametrically.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods</title>
<link>https://arxiv.org/abs/2505.22494</link>
<guid>https://arxiv.org/abs/2505.22494</guid>
<content:encoded><![CDATA[
arXiv:2505.22494v1 Announce Type: new 
Abstract: Designing protein sequences of both high fitness and novelty is a challenging task in data-efficient protein engineering. Exploration beyond wild-type neighborhoods often leads to biologically implausible sequences or relies on surrogate models that lose fidelity in novel regions. Here, we propose ProSpero, an active learning framework in which a frozen pre-trained generative model is guided by a surrogate updated from oracle feedback. By integrating fitness-relevant residue selection with biologically-constrained Sequential Monte Carlo sampling, our approach enables exploration beyond wild-type neighborhoods while preserving biological plausibility. We show that our framework remains effective even when the surrogate is misspecified. ProSpero consistently outperforms or matches existing methods across diverse protein engineering tasks, retrieving sequences of both high fitness and novelty.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric GNNs for Charged Particle Tracking at GlueX</title>
<link>https://arxiv.org/abs/2505.22504</link>
<guid>https://arxiv.org/abs/2505.22504</guid>
<content:encoded><![CDATA[
arXiv:2505.22504v1 Announce Type: new 
Abstract: Nuclear physics experiments are aimed at uncovering the fundamental building blocks of matter. The experiments involve high-energy collisions that produce complex events with many particle trajectories. Tracking charged particles resulting from collisions in the presence of a strong magnetic field is critical to enable the reconstruction of particle trajectories and precise determination of interactions. It is traditionally achieved through combinatorial approaches that scale worse than linearly as the number of hits grows. Since particle hit data naturally form a 3-dimensional point cloud and can be structured as graphs, Graph Neural Networks (GNNs) emerge as an intuitive and effective choice for this task. In this study, we evaluate the GNN model for track finding on the data from the GlueX experiment at Jefferson Lab. We use simulation data to train the model and test on both simulation and real GlueX measurements. We demonstrate that GNN-based track finding outperforms the currently used traditional method at GlueX in terms of segment-based efficiency at a fixed purity while providing faster inferences. We show that the GNN model can achieve significant speedup by processing multiple events in batches, which exploits the parallel computation capability of Graphical Processing Units (GPUs). Finally, we compare the GNN implementation on GPU and FPGA and describe the trade-off.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsification and Reconstruction from the Perspective of Representation Geometry</title>
<link>https://arxiv.org/abs/2505.22506</link>
<guid>https://arxiv.org/abs/2505.22506</guid>
<content:encoded><![CDATA[
arXiv:2505.22506v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic interpretability, aiming to identify interpretable monosemantic features. However, how does sparse encoding organize the representations of activation vector from language models? What is the relationship between this organizational paradigm and feature disentanglement as well as reconstruction performance? To address these questions, we propose the SAEMA, which validates the stratified structure of the representation by observing the variability of the rank of the symmetric semipositive definite (SSPD) matrix corresponding to the modal tensor unfolded along the latent tensor with the level of noise added to the residual stream. To systematically investigate how sparse encoding alters representational structures, we define local and global representations, demonstrating that they amplify inter-feature distinctions by merging similar semantic features and introducing additional dimensionality. Furthermore, we intervene the global representation from an optimization perspective, proving a significant causal relationship between their separability and the reconstruction performance. This study explains the principles of sparsity from the perspective of representational geometry and demonstrates the impact of changes in representational structure on reconstruction performance. Particularly emphasizes the necessity of understanding representations and incorporating representational constraints, providing empirical references for developing new interpretable tools and improving SAEs. The code is available at \hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Optimization via Differentiable Stopping Time</title>
<link>https://arxiv.org/abs/2505.22509</link>
<guid>https://arxiv.org/abs/2505.22509</guid>
<content:encoded><![CDATA[
arXiv:2505.22509v1 Announce Type: new 
Abstract: Optimization is an important module of modern machine learning applications. Tremendous efforts have been made to accelerate optimization algorithms. A common formulation is achieving a lower loss at a given time. This enables a differentiable framework with respect to the algorithm hyperparameters. In contrast, its dual, minimizing the time to reach a target loss, is believed to be non-differentiable, as the time is not differentiable. As a result, it usually serves as a conceptual framework or is optimized using zeroth-order methods. To address this limitation, we propose a differentiable stopping time and theoretically justify it based on differential equations. An efficient algorithm is designed to backpropagate through it. As a result, the proposed differentiable stopping time enables a new differentiable formulation for accelerating algorithms. We further discuss its applications, such as online hyperparameter tuning and learning to optimize. Our proposed methods show superior performance in comprehensive experiments across various problems, which confirms their effectiveness.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data</title>
<link>https://arxiv.org/abs/2505.22521</link>
<guid>https://arxiv.org/abs/2505.22521</guid>
<content:encoded><![CDATA[
arXiv:2505.22521v1 Announce Type: new 
Abstract: Fraud detection remains a critical task in high-stakes domains such as finance and e-commerce, where undetected fraudulent transactions can lead to significant economic losses. In this study, we systematically compare the performance of four supervised learning models - Logistic Regression, Random Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit (GRU) network - on a large-scale, highly imbalanced online transaction dataset. While ensemble methods such as Random Forest and LightGBM demonstrated superior performance in both overall and class-specific metrics, Logistic Regression offered a reliable and interpretable baseline. The GRU model showed strong recall for the minority fraud class, though at the cost of precision, highlighting a trade-off relevant for real-world deployment. Our evaluation emphasizes not only weighted averages but also per-class precision, recall, and F1-scores, providing a nuanced view of each model's effectiveness in detecting rare but consequential fraudulent activity. The findings underscore the importance of choosing models based on the specific risk tolerance and operational needs of fraud detection systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo</title>
<link>https://arxiv.org/abs/2505.22524</link>
<guid>https://arxiv.org/abs/2505.22524</guid>
<content:encoded><![CDATA[
arXiv:2505.22524v1 Announce Type: new 
Abstract: Discrete diffusion models have become highly effective across various domains. However, real-world applications often require the generative process to adhere to certain constraints but without task-specific fine-tuning. To this end, we propose a training-free method based on Sequential Monte Carlo (SMC) to sample from the reward-aligned target distribution at the test time. Our approach leverages twisted SMC with an approximate locally optimal proposal, obtained via a first-order Taylor expansion of the reward function. To address the challenge of ill-defined gradients in discrete spaces, we incorporate a Gumbel-Softmax relaxation, enabling efficient gradient-based approximation within the discrete generative framework. Empirical results on both synthetic datasets and image modelling validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training RL Agents for Multi-Objective Network Defense Tasks</title>
<link>https://arxiv.org/abs/2505.22531</link>
<guid>https://arxiv.org/abs/2505.22531</guid>
<content:encoded><![CDATA[
arXiv:2505.22531v1 Announce Type: new 
Abstract: Open-ended learning (OEL) -- which emphasizes training agents that achieve broad capability over narrow competency -- is emerging as a paradigm to develop artificial intelligence (AI) agents to achieve robustness and generalization. However, despite promising results that demonstrate the benefits of OEL, applying OEL to develop autonomous agents for real-world cybersecurity applications remains a challenge.
  We propose a training approach, inspired by OEL, to develop autonomous network defenders. Our results demonstrate that like in other domains, OEL principles can translate into more robust and generalizable agents for cyber defense. To apply OEL to network defense, it is necessary to address several technical challenges. Most importantly, it is critical to provide a task representation approach over a broad universe of tasks that maintains a consistent interface over goals, rewards and action spaces. This way, the learning agent can train with varying network conditions, attacker behaviors, and defender goals while being able to build on previously gained knowledge.
  With our tools and results, we aim to fundamentally impact research that applies AI to solve cybersecurity problems. Specifically, as researchers develop gyms and benchmarks for cyber defense, it is paramount that they consider diverse tasks with consistent representations, such as those we propose in our work.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabularQGAN: A Quantum Generative Model for Tabular Data</title>
<link>https://arxiv.org/abs/2505.22533</link>
<guid>https://arxiv.org/abs/2505.22533</guid>
<content:encoded><![CDATA[
arXiv:2505.22533v1 Announce Type: new 
Abstract: In this paper, we introduce a novel quantum generative model for synthesizing tabular data. Synthetic data is valuable in scenarios where real-world data is scarce or private, it can be used to augment or replace existing datasets. Real-world enterprise data is predominantly tabular and heterogeneous, often comprising a mixture of categorical and numerical features, making it highly relevant across various industries such as healthcare, finance, and software. We propose a quantum generative adversarial network architecture with flexible data encoding and a novel quantum circuit ansatz to effectively model tabular data. The proposed approach is tested on the MIMIC III healthcare and Adult Census datasets, with extensive benchmarking against leading classical models, CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model outperforms classical models by an average of 8.5% with respect to an overall similarity score from SDMetrics, while using only 0.072% of the parameters of the classical models. Additionally, we evaluate the generalization capabilities of the models using two custom-designed metrics that demonstrate the ability of the proposed quantum model to generate useful and novel samples. To our knowledge, this is one of the first demonstrations of a successful quantum generative model for handling tabular data, indicating that this task could be well-suited to quantum computers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks</title>
<link>https://arxiv.org/abs/2505.22538</link>
<guid>https://arxiv.org/abs/2505.22538</guid>
<content:encoded><![CDATA[
arXiv:2505.22538v1 Announce Type: new 
Abstract: We address the problem of uncertainty quantification and propose measures of total, aleatoric, and epistemic uncertainty based on a known decomposition of (strictly) proper scoring rules, a specific type of loss function, into a divergence and an entropy component. This leads to a flexible framework for uncertainty quantification that can be instantiated with different losses (scoring rules), which makes it possible to tailor uncertainty quantification to the use case at hand. We show that this flexibility is indeed advantageous. In particular, we analyze the task of selective prediction and show that the scoring rule should ideally match the task loss. In addition, we perform experiments on two other common tasks. For out-of-distribution detection, our results confirm that a widely used measure of epistemic uncertainty, mutual information, performs best. Moreover, in the setting of active learning, our measure of epistemic uncertainty based on the zero-one-loss consistently outperforms other uncertainty measures.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Human-Centric Approach to Explainable AI for Personalized Education</title>
<link>https://arxiv.org/abs/2505.22541</link>
<guid>https://arxiv.org/abs/2505.22541</guid>
<content:encoded><![CDATA[
arXiv:2505.22541v1 Announce Type: new 
Abstract: Deep neural networks form the backbone of artificial intelligence research, with potential to transform the human experience in areas ranging from autonomous driving to personal assistants, healthcare to education. However, their integration into the daily routines of real-world classrooms remains limited. It is not yet common for a teacher to assign students individualized homework targeting their specific weaknesses, provide students with instant feedback, or simulate student responses to a new exam question. While these models excel in predictive performance, this lack of adoption can be attributed to a significant weakness: the lack of explainability of model decisions, leading to a lack of trust from students, parents, and teachers. This thesis aims to bring human needs to the forefront of eXplainable AI (XAI) research, grounded in the concrete use case of personalized learning and teaching. We frame the contributions along two verticals: technical advances in XAI and their aligned human studies. We investigate explainability in AI for education, revealing systematic disagreements between post-hoc explainers and identifying a need for inherently interpretable model architectures. We propose four novel technical contributions in interpretability with a multimodal modular architecture (MultiModN), an interpretable mixture-of-experts model (InterpretCC), adversarial training for explainer stability, and a theory-driven LLM-XAI framework to present explanations to students (iLLuMinaTE), which we evaluate in diverse settings with professors, teachers, learning scientists, and university students. By combining empirical evaluations of existing explainers with novel architectural designs and human studies, our work lays a foundation for human-centric AI systems that balance state-of-the-art performance with built-in transparency and trust.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models</title>
<link>https://arxiv.org/abs/2505.22549</link>
<guid>https://arxiv.org/abs/2505.22549</guid>
<content:encoded><![CDATA[
arXiv:2505.22549v1 Announce Type: new 
Abstract: Scaling foundation model training with Distributed Data Parallel (DDP) methods is bandwidth-limited. Existing infrequent communication methods like Local SGD were designed to synchronize only model parameters and cannot be trivially applied to adaptive optimizers due to additional optimizer states. Current approaches extending Local SGD either lack convergence guarantees or require synchronizing all optimizer states, tripling communication costs. We propose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of optimizers assigning independent synchronization periods to parameters and momenta, enabling lower communication costs while preserving convergence. Through extensive experiments on language models of up to 1.7B, we show that DES-LOC can communicate 170x less than DDP and 2x less than the previous state-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches, DES-LOC is suited for practical training scenarios prone to system failures. DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for foundation model training.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Hyena Networks for Large-scale Equivariant Learning</title>
<link>https://arxiv.org/abs/2505.22560</link>
<guid>https://arxiv.org/abs/2505.22560</guid>
<content:encoded><![CDATA[
arXiv:2505.22560v1 Announce Type: new 
Abstract: Processing global geometric context while preserving equivariance is crucial when modeling biological, chemical, and physical systems. Yet, this is challenging due to the computational demands of equivariance and global context at scale. Standard methods such as equivariant self-attention suffer from quadratic complexity, while local methods such as distance-based message passing sacrifice global information. Inspired by the recent success of state-space and long-convolutional models, we introduce Geometric Hyena, the first equivariant long-convolutional model for geometric systems. Geometric Hyena captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations. Evaluated on all-atom property prediction of large RNA molecules and full protein molecular dynamics, Geometric Hyena outperforms existing equivariant models while requiring significantly less memory and compute that equivariant self-attention. Notably, our model processes the geometric context of 30k tokens 20x faster than the equivariant transformer and allows 72x longer context within the same budget.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators</title>
<link>https://arxiv.org/abs/2505.22573</link>
<guid>https://arxiv.org/abs/2505.22573</guid>
<content:encoded><![CDATA[
arXiv:2505.22573v1 Announce Type: new 
Abstract: Simulation-based inference (SBI) is an established approach for performing Bayesian inference on scientific simulators. SBI so far works best on low-dimensional parametric models. However, it is difficult to infer function-valued parameters, which frequently occur in disciplines that model spatiotemporal processes such as the climate and earth sciences. Here, we introduce an approach for efficient posterior estimation, using a Fourier Neural Operator (FNO) architecture with a flow matching objective. We show that our approach, FNOPE, can perform inference of function-valued parameters at a fraction of the simulation budget of state of the art methods. In addition, FNOPE supports posterior evaluation at arbitrary discretizations of the domain, as well as simultaneous estimation of vector-valued parameters. We demonstrate the effectiveness of our approach on several benchmark tasks and a challenging spatial inference task from glaciology. FNOPE extends the applicability of SBI methods to new scientific domains by enabling the inference of function-valued parameters.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benignity of loss landscape with weight decay requires both large overparametrization and initialization</title>
<link>https://arxiv.org/abs/2505.22578</link>
<guid>https://arxiv.org/abs/2505.22578</guid>
<content:encoded><![CDATA[
arXiv:2505.22578v1 Announce Type: new 
Abstract: The optimization of neural networks under weight decay remains poorly understood from a theoretical standpoint. While weight decay is standard practice in modern training procedures, most theoretical analyses focus on unregularized settings. In this work, we investigate the loss landscape of the $\ell_2$-regularized training loss for two-layer ReLU networks. We show that the landscape becomes benign -- i.e., free of spurious local minima -- under large overparametrization, specifically when the network width $m$ satisfies $m \gtrsim \min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the input dimension. More precisely in this regime, almost all constant activation regions contain a global minimum and no spurious local minima. We further show that this level of overparametrization is not only sufficient but also necessary via the example of orthogonal data. Finally, we demonstrate that such loss landscape results primarily hold relevance in the large initialization regime. In contrast, for small initializations -- corresponding to the feature learning regime -- optimization can still converge to spurious local minima, despite the global benignity of the landscape.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Unlearning under Overparameterization</title>
<link>https://arxiv.org/abs/2505.22601</link>
<guid>https://arxiv.org/abs/2505.22601</guid>
<content:encoded><![CDATA[
arXiv:2505.22601v1 Announce Type: new 
Abstract: Machine unlearning algorithms aim to remove the influence of specific training samples, ideally recovering the model that would have resulted from training on the remaining data alone. We study unlearning in the overparameterized setting, where many models interpolate the data, and defining the unlearning solution as any loss minimizer over the retained set$\unicode{x2013}$as in prior work in the underparameterized setting$\unicode{x2013}$is inadequate, since the original model may already interpolate the retained data and satisfy this condition. In this regime, loss gradients vanish, rendering prior methods based on gradient perturbations ineffective, motivating both new unlearning definitions and algorithms. For this setting, we define the unlearning solution as the minimum-complexity interpolator over the retained data and propose a new algorithmic framework that only requires access to model gradients on the retained set at the original solution. We minimize a regularized objective over perturbations constrained to be orthogonal to these model gradients, a first-order relaxation of the interpolation condition. For different model classes, we provide exact and approximate unlearning guarantees, and we demonstrate that an implementation of our framework outperforms existing baselines across various unlearning experiments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Rank at a Time: Cascading Error Dynamics in Sequential Learning</title>
<link>https://arxiv.org/abs/2505.22602</link>
<guid>https://arxiv.org/abs/2505.22602</guid>
<content:encoded><![CDATA[
arXiv:2505.22602v1 Announce Type: new 
Abstract: Sequential learning -- where complex tasks are broken down into simpler, hierarchical components -- has emerged as a paradigm in AI. This paper views sequential learning through the lens of low-rank linear regression, focusing specifically on how errors propagate when learning rank-1 subspaces sequentially. We present an analysis framework that decomposes the learning process into a series of rank-1 estimation problems, where each subsequent estimation depends on the accuracy of previous steps. Our contribution is a characterization of the error propagation in this sequential process, establishing bounds on how errors -- e.g., due to limited computational budgets and finite precision -- affect the overall model accuracy. We prove that these errors compound in predictable ways, with implications for both algorithmic design and stability guarantees.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models</title>
<link>https://arxiv.org/abs/2505.22617</link>
<guid>https://arxiv.org/abs/2505.22617</guid>
<content:encoded><![CDATA[
arXiv:2505.22617v1 Announce Type: new 
Abstract: This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding (Un)Reliability of Steering Vectors in Language Models</title>
<link>https://arxiv.org/abs/2505.22637</link>
<guid>https://arxiv.org/abs/2505.22637</guid>
<content:encoded><![CDATA[
arXiv:2505.22637v1 Announce Type: new 
Abstract: Steering vectors are a lightweight method to control language model behavior by adding a learned bias to the activations at inference time. Although steering demonstrates promising performance, recent work shows that it can be unreliable or even counterproductive in some cases. This paper studies the influence of prompt types and the geometry of activation differences on steering reliability. First, we find that all seven prompt types used in our experiments produce a net positive steering effect, but exhibit high variance across samples, and often give an effect opposite of the desired one. No prompt type clearly outperforms the others, and yet the steering vectors resulting from the different prompt types often differ directionally (as measured by cosine similarity). Second, we show that higher cosine similarity between training set activation differences predicts more effective steering. Finally, we observe that datasets where positive and negative activations are better separated are more steerable. Our results suggest that vector steering is unreliable when the target behavior is not represented by a coherent direction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Survival Analysis</title>
<link>https://arxiv.org/abs/2505.22641</link>
<guid>https://arxiv.org/abs/2505.22641</guid>
<content:encoded><![CDATA[
arXiv:2505.22641v1 Announce Type: new 
Abstract: Survival analysis is widely deployed in a diverse set of fields, including healthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model is a semi-parametric model often encountered in the literature. Despite its popularity, wide deployment, and numerous variants, scaling CoxPH to large datasets and deep architectures poses a challenge, especially in the high-dimensional regime. We identify a fundamental connection between rank regression and the CoxPH model: this allows us to adapt and extend the so-called spectral method for rank regression to survival analysis. Our approach is versatile, naturally generalizing to several CoxPH variants, including deep models. We empirically verify our method's scalability on multiple real-world high-dimensional datasets; our method outperforms legacy methods w.r.t. predictive performance and efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Learning Verifiers for Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.22650</link>
<guid>https://arxiv.org/abs/2505.22650</guid>
<content:encoded><![CDATA[
arXiv:2505.22650v1 Announce Type: new 
Abstract: Chain-of-Thought reasoning has emerged as a powerful approach for solving complex mathematical and logical problems. However, it can often veer off track through incorrect or unsubstantiated inferences. Formal mathematical reasoning, which can be checked with a formal verifier, is one approach to addressing this issue. However, currently LLMs are simply not good enough to solve complex problems in a formal way, and even just formalizing an informal problem statement can be challenging. Motivated by this fact, in this work we consider the problem of learning reliable verifiers for natural language Chain-of-Thought reasoning. That is, given a problem statement and step-by-step solution in natural language, the aim of the verifier is to output [Yes] if the reasoning steps in the solution are all valid, and [No] otherwise. In this work we give a formal PAC-learning framework for studying this problem. We propose and analyze several natural verification goals, at different levels of strength, in this framework. We provide sample complexity upper-bounds for learning verifiers satisfying these goals, as well as lower-bound and impossibility results for learning other natural verification objectives without additional assumptions.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents</title>
<link>https://arxiv.org/abs/2505.22655</link>
<guid>https://arxiv.org/abs/2505.22655</guid>
<content:encoded><![CDATA[
arXiv:2505.22655v1 Announce Type: new 
Abstract: Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing Confidence Alone Improves Reasoning</title>
<link>https://arxiv.org/abs/2505.22660</link>
<guid>https://arxiv.org/abs/2505.22660</guid>
<content:encoded><![CDATA[
arXiv:2505.22660v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has enabled machine learning models to achieve significant advances in many fields. Most recently, RL has empowered frontier language models to solve challenging math, science, and coding problems. However, central to any RL algorithm is the reward function, and reward engineering is a notoriously difficult problem in any domain. In this paper, we propose RENT: Reinforcement Learning via Entropy Minimization -- a fully unsupervised RL method that requires no external reward or ground-truth answers, and instead uses the model's entropy of its underlying distribution as an intrinsic reward. We find that by reinforcing the chains of thought that yield high model confidence on its generated answers, the model improves its reasoning ability. In our experiments, we showcase these improvements on an extensive suite of commonly-used reasoning benchmarks, including GSM8K, MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and Mistral families. The generality of our unsupervised learning method lends itself to applicability in a wide range of domains where external supervision is limited or unavailable.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offset Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2404.11045</link>
<guid>https://arxiv.org/abs/2404.11045</guid>
<content:encoded><![CDATA[
arXiv:2404.11045v2 Announce Type: cross 
Abstract: Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, biased, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose {\delta}-Unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, {\delta}-Unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that {\delta}- Unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. {\delta}-Unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise</title>
<link>https://arxiv.org/abs/2505.18478</link>
<guid>https://arxiv.org/abs/2505.18478</guid>
<content:encoded><![CDATA[
arXiv:2505.18478v1 Announce Type: cross 
Abstract: Advancements in quantum computing have spurred significant interest in harnessing its potential for speedups over classical systems. However, noise remains a major obstacle to achieving reliable quantum algorithms. In this work, we present a provably noise-resilient training theory and algorithm to enhance the robustness of parameterized quantum circuit classifiers. Our method, with a natural connection to Evolutionary Strategies, guarantees resilience to parameter noise with minimal adjustments to commonly used optimization algorithms. Our approach is function-agnostic and adaptable to various quantum circuits, successfully demonstrated in quantum phase classification tasks. By developing provably guaranteed optimization theory with quantum circuits, our work opens new avenues for practical, robust applications of near-term quantum computers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capability-Based Scaling Laws for LLM Red-Teaming</title>
<link>https://arxiv.org/abs/2505.20162</link>
<guid>https://arxiv.org/abs/2505.20162</guid>
<content:encoded><![CDATA[
arXiv:2505.20162v1 Announce Type: cross 
Abstract: As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Genetic Influences on Brain Aging: Analyzing Sex Differences in the UK Biobank using Structural MRI</title>
<link>https://arxiv.org/abs/2505.20344</link>
<guid>https://arxiv.org/abs/2505.20344</guid>
<content:encoded><![CDATA[
arXiv:2505.20344v1 Announce Type: cross 
Abstract: Brain aging trajectories differ between males and females, yet the genetic factors underlying these differences remain underexplored. Using structural MRI and genotyping data from 40,940 UK Biobank participants (aged 45-83), we computed Brain Age Gap Estimates (BrainAGE) for total brain, hippocampal, and ventricular volumes. We conducted sex-stratified genome-wide association studies (GWAS) and Post-GWAS analyses to identify genetic variants associated with accelerated brain aging. Distinct gene sets emerged by sex: in females, neurotransmitter transport and mitochondrial stress response genes were implicated; in males, immune and inflammation-related genes dominated. Shared genes, including GMNC and OSTN, were consistently linked to brain volumes across sexes, suggesting core roles in neurostructural maintenance. Tissue expression analyses revealed sex-specific enrichment in pathways tied to neurodegeneration. These findings highlight the importance of sex-stratified approaches in aging research and suggest genetic targets for personalized interventions against age-related cognitive decline.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic detection of abnormal clinical EEG: comparison of a finetuned foundation model with two deep learning models</title>
<link>https://arxiv.org/abs/2505.21507</link>
<guid>https://arxiv.org/abs/2505.21507</guid>
<content:encoded><![CDATA[
arXiv:2505.21507v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) is commonly used by physicians for the diagnosis of numerous neurological disorders. Due to the large volume of EEGs requiring interpretation and the specific expertise involved, artificial intelligence-based tools are being developed to assist in their visual analysis. In this paper, we compare two deep learning models (CNN-LSTM and Transformer-based) with BioSerenity-E1, a recently proposed foundation model, in the task of classifying entire EEG recordings as normal or abnormal. The three models were trained or finetuned on 2,500 EEG recordings and their performances were evaluated on two private and one public datasets: a large multicenter dataset annotated by a single specialist (dataset A composed of n = 4,480 recordings), a small multicenter dataset annotated by three specialists (dataset B, n = 198), and the Temple University Abnormal (TUAB) EEG corpus evaluation dataset (n = 276). On dataset A, the three models achieved at least 86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest balanced accuracy (89.19% [88.36-90.41]). BioSerenity-E1 finetuned also achieved the best performance on dataset B, with 94.63% [92.32-98.12] balanced accuracy. The models were then validated on TUAB evaluation dataset, whose corresponding training set was not used during training, where they achieved at least 76% accuracy. Specifically, BioSerenity-E1 finetuned outperformed the other two models, reaching an accuracy of 82.25% [78.27-87.48]. Our results highlight the usefulness of leveraging pre-trained models for automatic EEG classification: enabling robust and efficient interpretation of EEG data with fewer resources and broader applicability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Vision Transformer Explainability Using Artificial Astrocytes</title>
<link>https://arxiv.org/abs/2505.21513</link>
<guid>https://arxiv.org/abs/2505.21513</guid>
<content:encoded><![CDATA[
arXiv:2505.21513v1 Announce Type: cross 
Abstract: Machine learning models achieve high precision, but their decision-making processes often lack explainability. Furthermore, as model complexity increases, explainability typically decreases. Existing efforts to improve explainability primarily involve developing new eXplainable artificial intelligence (XAI) techniques or incorporating explainability constraints during training. While these approaches yield specific improvements, their applicability remains limited. In this work, we propose the Vision Transformer with artificial Astrocytes (ViTA). This training-free approach is inspired by neuroscience and enhances the reasoning of a pretrained deep neural network to generate more human-aligned explanations. We evaluated our approach employing two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the similarity between the heatmaps produced by the XAI techniques and a (human-aligned) ground truth. Our results consistently demonstrate that incorporating artificial astrocytes enhances the alignment of model explanations with human perception, leading to statistically significant improvements across all XAI techniques and metrics utilized.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures</title>
<link>https://arxiv.org/abs/2505.21522</link>
<guid>https://arxiv.org/abs/2505.21522</guid>
<content:encoded><![CDATA[
arXiv:2505.21522v1 Announce Type: cross 
Abstract: While deep neural network (DNN)-based video denoising has demonstrated significant performance, deploying state-of-the-art models on edge devices remains challenging due to stringent real-time and energy efficiency requirements. Computing-in-Memory (CIM) chips offer a promising solution by integrating computation within memory cells, enabling rapid matrix-vector multiplication (MVM). However, existing DNN models are often designed without considering CIM architectural constraints, thus limiting their acceleration potential during inference. To address this, we propose a hardware-algorithm co-design framework incorporating two innovations: (1) a CIM-Aware Architecture, CIM-NET, optimized for large receptive field operation and CIM's crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator, CIM-CONV, used within CIM-NET to integrate slide-based processing with fully connected transformations for high-quality feature extraction and reconstruction. This framework significantly reduces the number of MVM operations, improving inference speed on CIM chips while maintaining competitive performance. Experimental results indicate that, compared to the conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM operations with a slight decrease in denoising performance. With a stride value of 8, CIM-NET reduces MVM operations to 1/77th of the original, while maintaining competitive PSNR (35.11 dB vs. 35.56 dB
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Shared Representations from Unpaired Data</title>
<link>https://arxiv.org/abs/2505.21524</link>
<guid>https://arxiv.org/abs/2505.21524</guid>
<content:encoded><![CDATA[
arXiv:2505.21524v1 Announce Type: cross 
Abstract: Learning shared representations is a primary area of multimodal representation learning. The current approaches to achieve a shared embedding space rely heavily on paired samples from each modality, which are significantly harder to obtain than unpaired ones. In this work, we demonstrate that shared representations can be learned almost exclusively from unpaired data. Our arguments are grounded in the spectral embeddings of the random walk matrices constructed independently from each unimodal representation. Empirical results in computer vision and natural language processing domains support its potential, revealing the effectiveness of unpaired data in capturing meaningful cross-modal relations, demonstrating high capabilities in retrieval tasks, generation, arithmetics, zero-shot, and cross-domain classification. This work, to the best of our knowledge, is the first to demonstrate these capabilities almost exclusively from unpaired samples, giving rise to a cross-modal embedding that could be viewed as universal, i.e., independent of the specific modalities of the data. Our code IS publicly available at https://github.com/shaham-lab/SUE.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniDB++: Fast Sampling of Unified Diffusion Bridge</title>
<link>https://arxiv.org/abs/2505.21528</link>
<guid>https://arxiv.org/abs/2505.21528</guid>
<content:encoded><![CDATA[
arXiv:2505.21528v1 Announce Type: cross 
Abstract: Diffusion Bridges enable transitions between arbitrary distributions, with the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's reliance on iterative Euler sampling methods results in slow, computationally expensive inference, while existing acceleration techniques for diffusion or diffusion bridge models fail to address its unique challenges: missing terminal mean constraints and SOC-specific penalty coefficients in its SDEs. We present UniDB++, a training-free sampling algorithm that significantly improves upon these limitations. The method's key advancement comes from deriving exact closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the error accumulation inherent in Euler approximations and enabling high-quality generation with up to 20$\times$ fewer sampling steps. This method is further complemented by replacing conventional noise prediction with a more stable data prediction model, along with an SDE-Corrector mechanism that maintains perceptual quality for low-step regimes (5-10 steps). Additionally, we demonstrate that UniDB++ aligns with existing diffusion bridge acceleration methods by evaluating their update rules, and UniDB++ can recover DBIMs as special cases under some theoretical conditions. Experiments demonstrate UniDB++'s state-of-the-art performance in image restoration tasks, outperforming Euler-based methods in fidelity and speed while reducing inference time significantly. This work bridges the gap between theoretical generality and practical efficiency in SOC-driven diffusion bridge models. Our code is available at https://github.com/2769433owo/UniDB-plusplus.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media</title>
<link>https://arxiv.org/abs/2505.21532</link>
<guid>https://arxiv.org/abs/2505.21532</guid>
<content:encoded><![CDATA[
arXiv:2505.21532v1 Announce Type: cross 
Abstract: Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology employed for distance and depth estimation across medical, automotive, and other fields, encounters significant computational challenges in scattering media. The complex nature of the acquired FLiDAR signal, particularly in such environments, makes isolating photon time-of-flight (related to target depth) and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the effectiveness of current analytical and computational methodologies. To overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE) framework tailored for specialized modeling of diverse temporal components. In contrast to the conventional MoE approaches our expert models are informed by underlying physics, such as the radiative transport equation governing photon propagation in scattering media. Central to our approach is EvidenceMoE, which integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess the reliability of each expert's output by providing per-expert quality scores and corrective feedback. A Decider Network then leverages this information to fuse expert predictions into a robust final estimate adaptively. We validate our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for non-invasive cancer cell depth detection generated from photon transport models in tissue. Our framework demonstrates strong performance, achieving a normalized root mean squared error (NRMSE) of 0.030 for depth estimation and 0.074 for fluorescence lifetime.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Organizing Visual Prototypes for Non-Parametric Representation Learning</title>
<link>https://arxiv.org/abs/2505.21533</link>
<guid>https://arxiv.org/abs/2505.21533</guid>
<content:encoded><![CDATA[
arXiv:2505.21533v1 Announce Type: cross 
Abstract: We present Self-Organizing Visual Prototypes (SOP), a new training technique for unsupervised visual feature learning. Unlike existing prototypical self-supervised learning (SSL) methods that rely on a single prototype to encode all relevant features of a hidden cluster in the data, we propose the SOP strategy. In this strategy, a prototype is represented by many semantically similar representations, or support embeddings (SEs), each containing a complementary set of features that together better characterize their region in space and maximize training performance. We reaffirm the feasibility of non-parametric SSL by introducing novel non-parametric adaptations of two loss functions that implement the SOP strategy. Notably, we introduce the SOP Masked Image Modeling (SOP-MIM) task, where masked representations are reconstructed from the perspective of multiple non-parametric local SEs. We comprehensively evaluate the representations learned using the SOP strategy on a range of benchmarks, including retrieval, linear evaluation, fine-tuning, and object detection. Our pre-trained encoders achieve state-of-the-art performance on many retrieval benchmarks and demonstrate increasing performance gains with more complex encoders.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement</title>
<link>https://arxiv.org/abs/2505.21535</link>
<guid>https://arxiv.org/abs/2505.21535</guid>
<content:encoded><![CDATA[
arXiv:2505.21535v1 Announce Type: cross 
Abstract: While transformers excel across vision and language pretraining tasks, their reliance on attention mechanisms poses challenges for inference efficiency, especially on edge and embedded accelerators with limited parallelism and memory bandwidth. Hinted by the observed redundancy of attention at inference time, we hypothesize that though the model learns complicated token dependency through pretraining, the inference-time sequence-to-sequence mapping in each attention layer is actually ''simple'' enough to be represented with a much cheaper function. In this work, we explore FAR, a Function-preserving Attention Replacement framework that replaces all attention blocks in pretrained transformers with learnable sequence-to-sequence modules, exemplified by an LSTM. FAR optimize a multi-head LSTM architecture with a block-wise distillation objective and a global structural pruning framework to achieve a family of efficient LSTM-based models from pretrained transformers. We validate FAR on the DeiT vision transformer family and demonstrate that it matches the accuracy of the original models on ImageNet and multiple downstream tasks with reduced parameters and latency. Further analysis shows that FAR preserves the semantic token relationships and the token-to-token correlation learned in the transformer's attention module.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero</title>
<link>https://arxiv.org/abs/2505.21536</link>
<guid>https://arxiv.org/abs/2505.21536</guid>
<content:encoded><![CDATA[
arXiv:2505.21536v1 Announce Type: cross 
Abstract: The demand of finite raw materials will keep increasing as they fuel modern society. Simultaneously, solutions for stopping carbon emissions in the short term are not available, thus making the net zero target extremely challenging to achieve at scale. The circular economy (CE) paradigm is gaining attention as a solution to address climate change and the uncertainties of supplies of critical materials. Hence, in this paper, we introduce CiRL, a deep reinforcement learning (DRL) library of environments focused on the circularity of both solid and fluid materials. The integration of DRL into the design of material circularity is possible thanks to the formalism of thermodynamical material networks, which is underpinned by compartmental dynamical thermodynamics. Along with the focus on circularity, this library has three more features: the new CE-oriented environments are in the state-space form, which is typically used in dynamical systems analysis and control designs; it is based on a state-of-the-art Python library of DRL algorithms, namely, Stable-Baselines3; and it is developed in Google Colaboratory to be accessible to researchers from different disciplines and backgrounds as is often the case for circular economy researchers and engineers. CiRL is publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2505.21545</link>
<guid>https://arxiv.org/abs/2505.21545</guid>
<content:encoded><![CDATA[
arXiv:2505.21545v1 Announce Type: cross 
Abstract: Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are sensitive to imperfect conditioning, which causes semantic drift and temporal incoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the first corruption-aware training framework for LVDMs that improves robustness through structured, data-aligned noise injection. Our method includes Batch-Centered Noise Injection (BCNI), which perturbs embeddings along intra-batch semantic directions to preserve temporal consistency. BCNI is especially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and MSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects noise along dominant spectral directions to improve low-frequency smoothness, showing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101. Ablation studies confirm the benefit of low-rank, data-aligned noise. Our theoretical analysis further explains how such perturbations tighten entropy, Wasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM establishes a principled, scalable training approach for robust video diffusion under multimodal noise. Code and models: https://github.com/chikap421/catlvdm
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper</title>
<link>https://arxiv.org/abs/2505.21551</link>
<guid>https://arxiv.org/abs/2505.21551</guid>
<content:encoded><![CDATA[
arXiv:2505.21551v1 Announce Type: cross 
Abstract: Whisper fails to correctly transcribe dementia speech because persons with dementia (PwDs) often exhibit irregular speech patterns and disfluencies such as pauses, repetitions, and fragmented sentences. It was trained on standard speech and may have had little or no exposure to dementia-affected speech. However, correct transcription is vital for dementia speech for cost-effective diagnosis and the development of assistive technology. In this work, we fine-tune Whisper with the open-source dementia speech dataset (DementiaBank) and our in-house dataset to improve its word error rate (WER). The fine-tuning also includes filler words to ascertain the filler inclusion rate (FIR) and F1 score. The fine-tuned models significantly outperformed the off-the-shelf models. The medium-sized model achieved a WER of 0.24, outperforming previous work. Similarly, there was a notable generalisability to unseen data and speech patterns.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the learned look-ahead behavior of chess neural networks</title>
<link>https://arxiv.org/abs/2505.21552</link>
<guid>https://arxiv.org/abs/2505.21552</guid>
<content:encoded><![CDATA[
arXiv:2505.21552v1 Announce Type: cross 
Abstract: We investigate the look-ahead capabilities of chess-playing neural networks, specifically focusing on the Leela Chess Zero policy network. We build on the work of Jenner et al. (2024) by analyzing the model's ability to consider future moves and alternative sequences beyond the immediate next move. Our findings reveal that the network's look-ahead behavior is highly context-dependent, varying significantly based on the specific chess position. We demonstrate that the model can process information about board states up to seven moves ahead, utilizing similar internal mechanisms across different future time steps. Additionally, we provide evidence that the network considers multiple possible move sequences rather than focusing on a single line of play. These results offer new insights into the emergence of sophisticated look-ahead capabilities in neural networks trained on strategic tasks, contributing to our understanding of AI reasoning in complex domains. Our work also showcases the effectiveness of interpretability techniques in uncovering cognitive-like processes in artificial intelligence systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction</title>
<link>https://arxiv.org/abs/2505.21553</link>
<guid>https://arxiv.org/abs/2505.21553</guid>
<content:encoded><![CDATA[
arXiv:2505.21553v1 Announce Type: cross 
Abstract: Network traffic prediction techniques have attracted much attention since they are valuable for network congestion control and user experience improvement. While existing prediction techniques can achieve favorable performance when there is sufficient training data, it remains a great challenge to make accurate predictions when only a small amount of training data is available. To tackle this problem, we propose a deep learning model, entitled MetaSTNet, based on a multimodal meta-learning framework. It is an end-to-end network architecture that trains the model in a simulator and transfers the meta-knowledge to a real-world environment, which can quickly adapt and obtain accurate predictions on a new task with only a small amount of real-world training data. In addition, we further employ cross conformal prediction to assess the calibrated prediction intervals. Extensive experiments have been conducted on real-world datasets to illustrate the efficiency and effectiveness of MetaSTNet.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification</title>
<link>https://arxiv.org/abs/2505.21558</link>
<guid>https://arxiv.org/abs/2505.21558</guid>
<content:encoded><![CDATA[
arXiv:2505.21558v1 Announce Type: cross 
Abstract: Agricultural research has accelerated in recent years, yet farmers often lack the time and resources for on-farm research due to the demands of crop production and farm operations. Seed classification offers valuable insights into quality control, production efficiency, and impurity detection. Early identification of seed types is critical to reducing the cost and risk associated with field emergence, which can lead to yield losses or disruptions in downstream processes like harvesting. Seed sampling supports growers in monitoring and managing seed quality, improving precision in determining seed purity levels, guiding management adjustments, and enhancing yield estimations. This study proposes a novel convolutional neural network (CNN)-based framework for the efficient classification of ten common Brassica seed types. The approach addresses the inherent challenge of texture similarity in seed images using a custom-designed CNN architecture. The model's performance was evaluated against several pre-trained state-of-the-art architectures, with adjustments to layer configurations for optimized classification. Experimental results using our collected Brassica seed dataset demonstrate that the proposed model achieved a high accuracy rate of 93 percent.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment</title>
<link>https://arxiv.org/abs/2505.21561</link>
<guid>https://arxiv.org/abs/2505.21561</guid>
<content:encoded><![CDATA[
arXiv:2505.21561v1 Announce Type: cross 
Abstract: We introduce a novel deep learning framework for the automated staging of spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in both orthodontics and forensic anthropology. Our approach leverages a dual-model architecture wherein a teacher model, trained on manually cropped images, transfers its precise spatial understanding to a student model that operates on full, uncropped images. This knowledge distillation is facilitated by a newly formulated loss function that aligns spatial logits as well as incorporates gradient-based attention spatial mapping, ensuring that the student model internalizes the anatomically relevant features without relying on external cropping or YOLO-based segmentation. By leveraging expert-curated data and feedback at each step, our framework attains robust diagnostic accuracy, culminating in a clinically viable end-to-end pipeline. This streamlined approach obviates the need for additional pre-processing tools and accelerates deployment, thereby enhancing both the efficiency and consistency of skeletal maturation assessment in diverse clinical settings.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model</title>
<link>https://arxiv.org/abs/2505.21564</link>
<guid>https://arxiv.org/abs/2505.21564</guid>
<content:encoded><![CDATA[
arXiv:2505.21564v1 Announce Type: cross 
Abstract: In deep multi-instance learning, the number of applicable instances depends on the data set. In histopathology images, deep learning multi-instance learners usually assume there are hundreds to thousands instances in a bag. However, when the number of instances in a bag increases to 256 in brain hematoma CT, learning becomes extremely difficult. In this paper, we address this drawback. To overcome this problem, we propose using a pre-trained model with self-supervised learning for the multi-instance learner as a downstream task. With this method, even when the original target task suffers from the spurious correlation problem, we show improvements of 5% to 13% in accuracy and 40% to 55% in the F1 measure for the hypodensity marker classification of brain hematoma CT.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Model-based Activity Completion for AI Motion Capture from Videos</title>
<link>https://arxiv.org/abs/2505.21566</link>
<guid>https://arxiv.org/abs/2505.21566</guid>
<content:encoded><![CDATA[
arXiv:2505.21566v1 Announce Type: cross 
Abstract: AI-based motion capture is an emerging technology that offers a cost-effective alternative to traditional motion capture systems. However, current AI motion capture methods rely entirely on observed video sequences, similar to conventional motion capture. This means that all human actions must be predefined, and movements outside the observed sequences are not possible. To address this limitation, we aim to apply AI motion capture to virtual humans, where flexible actions beyond the observed sequences are required. We assume that while many action fragments exist in the training data, the transitions between them may be missing. To bridge these gaps, we propose a diffusion-model-based action completion technique that generates complementary human motion sequences, ensuring smooth and continuous movements. By introducing a gate module and a position-time embedding module, our approach achieves competitive results on the Human3.6M dataset. Our experimental results show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size (16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural and coherent motion sequences. Additionally, we propose a method for extracting sensor data, including acceleration and angular velocity, from human motion sequences.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2505.21567</link>
<guid>https://arxiv.org/abs/2505.21567</guid>
<content:encoded><![CDATA[
arXiv:2505.21567v1 Announce Type: cross 
Abstract: With the development of Embodied Artificial intelligence, the end-to-end control policy such as Vision-Language-Action (VLA) model has become the mainstream. Existing VLA models faces expensive computing/storage cost, which need to be optimized. Quantization is considered as the most effective method which can not only reduce the memory cost but also achieve computation acceleration. However, we find the token alignment of VLA models hinders the application of existing quantization methods. To address this, we proposed an optimized framework called EaqVLA, which apply encoding-aligned quantization to VLA models. Specifically, we propose an complete analysis method to find the misalignment in various granularity. Based on the analysis results, we propose a mixed precision quantization with the awareness of encoding alignment. Experiments shows that the porposed EaqVLA achieves better quantization performance (with the minimal quantization loss for end-to-end action control and xxx times acceleration) than existing quantization methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks</title>
<link>https://arxiv.org/abs/2505.21572</link>
<guid>https://arxiv.org/abs/2505.21572</guid>
<content:encoded><![CDATA[
arXiv:2505.21572v1 Announce Type: cross 
Abstract: Mesh-based 3D static analysis methods have recently emerged as efficient alternatives to traditional computational numerical solvers, significantly reducing computational costs and runtime for various physics-based analyses. However, these methods primarily focus on surface topology and geometry, often overlooking the inherent thickness of real-world 3D objects, which exhibits high correlations and similar behavior between opposing surfaces. This limitation arises from the disconnected nature of these surfaces and the absence of internal edge connections within the mesh. In this work, we propose a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network (T-EMNN), that effectively integrates the thickness of 3D objects while maintaining the computational efficiency of surface meshes. Additionally, we introduce data-driven coordinates that encode spatial information while preserving E(3)-equivariance or invariance properties, ensuring consistent and robust analysis. Evaluations on a real-world industrial dataset demonstrate the superior performance of T-EMNN in accurately predicting node-level 3D deformations, effectively capturing thickness effects while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models</title>
<link>https://arxiv.org/abs/2505.21574</link>
<guid>https://arxiv.org/abs/2505.21574</guid>
<content:encoded><![CDATA[
arXiv:2505.21574v1 Announce Type: cross 
Abstract: Synthetically augmenting training datasets with diffusion models has been an effective strategy for improving generalization of image classifiers. However, existing techniques struggle to ensure the diversity of generation and increase the size of the data by up to 10-30x to improve the in-distribution performance. In this work, we show that synthetically augmenting part of the data that is not learned early in training outperforms augmenting the entire dataset. By analyzing a two-layer CNN, we prove that this strategy improves generalization by promoting homogeneity in feature learning speed without amplifying noise. Our extensive experiments show that by augmenting only 30%-40% of the data, our method boosts the performance by up to 2.8% in a variety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10, CIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM. Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on CIFAR-100 and TinyImageNet. It can also easily stack with existing weak and strong augmentation strategies to further boost the performance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Kernelised Stein Discrepancy for Assessing the Fit of Inhomogeneous Random Graph Models</title>
<link>https://arxiv.org/abs/2505.21580</link>
<guid>https://arxiv.org/abs/2505.21580</guid>
<content:encoded><![CDATA[
arXiv:2505.21580v1 Announce Type: cross 
Abstract: Complex data are often represented as a graph, which in turn can often be viewed as a realisation of a random graph, such as of an inhomogeneous random graph model (IRG). For general fast goodness-of-fit tests in high dimensions, kernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop, test, and analyse a KSD-type goodness-of-fit test for IRG models that can be carried out with a single observation of the network. The test is applicable to a network of any size and does not depend on the asymptotic distribution of the test statistic. We also provide theoretical guarantees.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI</title>
<link>https://arxiv.org/abs/2505.21589</link>
<guid>https://arxiv.org/abs/2505.21589</guid>
<content:encoded><![CDATA[
arXiv:2505.21589v1 Announce Type: cross 
Abstract: From uncertainty quantification to real-world object detection, we recognize the importance of machine learning algorithms, particularly in safety-critical domains such as autonomous driving or medical diagnostics. In machine learning, ambiguous data plays an important role in various machine learning domains. Optical illusions present a compelling area of study in this context, as they offer insight into the limitations of both human and machine perception. Despite this relevance, optical illusion datasets remain scarce. In this work, we introduce a novel dataset of optical illusions featuring intermingled animal pairs designed to evoke perceptual ambiguity. We identify generalizable visual concepts, particularly gaze direction and eye cues, as subtle yet impactful features that significantly influence model accuracy. By confronting models with perceptual ambiguity, our findings underscore the importance of concepts in visual learning and provide a foundation for studying bias and alignment between human and machine vision. To make this dataset useful for general purposes, we generate optical illusions systematically with different concepts discussed in our bias mitigation section. The dataset is accessible in Kaggle via https://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333. Our source code can be found at https://github.com/KDD-OpenSource/Ambivision.git.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment</title>
<link>https://arxiv.org/abs/2505.21592</link>
<guid>https://arxiv.org/abs/2505.21592</guid>
<content:encoded><![CDATA[
arXiv:2505.21592v1 Announce Type: cross 
Abstract: Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong function approximation capability. In our previous work, KAN and its variants were explored in score regression for blind image quality assessment (BIQA). However, these models encounter challenges when processing high-dimensional features, leading to limited performance gains and increased computational cost. To address these issues, we propose TaylorKAN that leverages the Taylor expansions as learnable activation functions to enhance local approximation capability. To improve the computational efficiency, network depth reduction and feature dimensionality compression are integrated into the TaylorKAN-based score regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and FLIVE) with authentic distortions, extensive experiments demonstrate that TaylorKAN consistently outperforms the other KAN-related models, indicating that the local approximation via Taylor expansions is more effective than global approximation using orthogonal functions. Its generalization capacity is validated through inter-database experiments. The findings highlight the potential of TaylorKAN as an efficient and robust model for high-dimensional score regression.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning optimal treatment strategies for intraoperative hypotension using deep reinforcement learning</title>
<link>https://arxiv.org/abs/2505.21596</link>
<guid>https://arxiv.org/abs/2505.21596</guid>
<content:encoded><![CDATA[
arXiv:2505.21596v1 Announce Type: cross 
Abstract: Traditional methods of surgical decision making heavily rely on human experience and prompt actions, which are variable. A data-driven system generating treatment recommendations based on patient states can be a substantial asset in perioperative decision-making, as in cases of intraoperative hypotension, for which suboptimal management is associated with acute kidney injury (AKI), a common and morbid postoperative complication. We developed a Reinforcement Learning (RL) model to recommend optimum dose of intravenous (IV) fluid and vasopressors during surgery to avoid intraoperative hypotension and postoperative AKI. We retrospectively analyzed 50,021 surgeries from 42,547 adult patients who underwent major surgery at a quaternary care hospital between June 2014 and September 2020. Of these, 34,186 surgeries were used for model training and 15,835 surgeries were reserved for testing. We developed a Deep Q-Networks based RL model using 16 variables including intraoperative physiologic time series, total dose of IV fluid and vasopressors extracted for every 15-minute epoch. The model replicated 69% of physician's decisions for the dosage of vasopressors and proposed higher or lower dosage of vasopressors than received in 10% and 21% of the treatments, respectively. In terms of IV fluids, the model's recommendations were within 0.05 ml/kg/15 min of the actual dose in 41% of the cases, with higher or lower doses recommended for 27% and 32% of the treatments, respectively. The model resulted in a higher estimated policy value compared to the physicians' actual treatments, as well as random and zero-drug policies. AKI prevalence was the lowest in patients receiving medication dosages that aligned with model's decisions. Our findings suggest that implementation of the model's policy has the potential to reduce postoperative AKI and improve other outcomes driven by intraoperative hypotension.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off</title>
<link>https://arxiv.org/abs/2505.21597</link>
<guid>https://arxiv.org/abs/2505.21597</guid>
<content:encoded><![CDATA[
arXiv:2505.21597v1 Announce Type: cross 
Abstract: The rapid advancement of deep learning in medical image analysis has greatly enhanced the accuracy of skin cancer classification. However, current state-of-the-art models, especially those based on transfer learning like ResNet50, come with significant computational overhead, rendering them impractical for deployment in resource-constrained environments. This study proposes a custom CNN model that achieves a 96.7\% reduction in parameters (from 23.9 million in ResNet50 to 692,000) while maintaining a classification accuracy deviation of less than 0.022\%. Our empirical analysis of the HAM10000 dataset reveals that although transfer learning models provide a marginal accuracy improvement of approximately 0.022\%, they result in a staggering 13,216.76\% increase in FLOPs, considerably raising computational costs and inference latency. In contrast, our lightweight CNN architecture, which encompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion, significantly reduces energy consumption, memory footprint, and inference time. These findings underscore the trade-off between the complexity of deep models and their real-world feasibility, positioning our optimized CNN as a practical solution for mobile and edge-based skin cancer diagnostics.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing</title>
<link>https://arxiv.org/abs/2505.21600</link>
<guid>https://arxiv.org/abs/2505.21600</guid>
<content:encoded><![CDATA[
arXiv:2505.21600v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging XP and CRISP-DM for Agile Data Science Projects</title>
<link>https://arxiv.org/abs/2505.21603</link>
<guid>https://arxiv.org/abs/2505.21603</guid>
<content:encoded><![CDATA[
arXiv:2505.21603v1 Announce Type: cross 
Abstract: This study explores the integration of eXtreme Programming (XP) and the Cross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data Science projects. We conducted a case study at the e-commerce company Elo7 to answer the research question: How can the agility of the XP method be integrated with CRISP-DM in Data Science projects? Data was collected through interviews and questionnaires with a Data Science team consisting of data scientists, ML engineers, and data product managers. The results show that 86% of the team frequently or always applies CRISP-DM, while 71% adopt XP practices in their projects. Furthermore, the study demonstrates that it is possible to combine CRISP-DM with XP in Data Science projects, providing a structured and collaborative approach. Finally, the study generated improvement recommendations for the company.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoMarkBench: Benchmarking Robustness of Video Watermarking</title>
<link>https://arxiv.org/abs/2505.21620</link>
<guid>https://arxiv.org/abs/2505.21620</guid>
<content:encoded><![CDATA[
arXiv:2505.21620v1 Announce Type: cross 
Abstract: The rapid development of video generative models has led to a surge in highly realistic synthetic videos, raising ethical concerns related to disinformation and copyright infringement. Recently, video watermarking has been proposed as a mitigation strategy by embedding invisible marks into AI-generated videos to enable subsequent detection. However, the robustness of existing video watermarking methods against both common and adversarial perturbations remains underexplored. In this work, we introduce VideoMarkBench, the first systematic benchmark designed to evaluate the robustness of video watermarks under watermark removal and watermark forgery attacks. Our study encompasses a unified dataset generated by three state-of-the-art video generative models, across three video styles, incorporating four watermarking methods and seven aggregation strategies used during detection. We comprehensively evaluate 12 types of perturbations under white-box, black-box, and no-box threat models. Our findings reveal significant vulnerabilities in current watermarking approaches and highlight the urgent need for more robust solutions. Our code is available at https://github.com/zhengyuan-jiang/VideoMarkBench.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives</title>
<link>https://arxiv.org/abs/2505.21627</link>
<guid>https://arxiv.org/abs/2505.21627</guid>
<content:encoded><![CDATA[
arXiv:2505.21627v1 Announce Type: cross 
Abstract: State-of-the-art large language models require specialized hardware and substantial energy to operate. As a consequence, cloud-based services that provide access to large language models have become very popular. In these services, the price users pay for an output provided by a model depends on the number of tokens the model uses to generate it -- they pay a fixed price per token. In this work, we show that this pricing mechanism creates a financial incentive for providers to strategize and misreport the (number of) tokens a model used to generate an output, and users cannot prove, or even know, whether a provider is overcharging them. However, we also show that, if an unfaithful provider is obliged to be transparent about the generative process used by the model, misreporting optimally without raising suspicion is hard. Nevertheless, as a proof-of-concept, we introduce an efficient heuristic algorithm that allows providers to significantly overcharge users without raising suspicion, highlighting the vulnerability of users under the current pay-per-token pricing mechanism. Further, to completely eliminate the financial incentive to strategize, we introduce a simple incentive-compatible token pricing mechanism. Under this mechanism, the price users pay for an output provided by a model depends on the number of characters of the output -- they pay a fixed price per character. Along the way, to illustrate and complement our theoretical results, we conduct experiments with several large language models from the $\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input prompts from the LMSYS Chatbot Arena platform.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuARI: Query Adaptive Retrieval Improvement</title>
<link>https://arxiv.org/abs/2505.21647</link>
<guid>https://arxiv.org/abs/2505.21647</guid>
<content:encoded><![CDATA[
arXiv:2505.21647v1 Announce Type: cross 
Abstract: Massive-scale pretraining has made vision-language models increasingly popular for image-to-image and text-to-image retrieval across a broad collection of domains. However, these models do not perform well when used for challenging retrieval tasks, such as instance retrieval in very large-scale image collections. Recent work has shown that linear transformations of VLM features trained for instance retrieval can improve performance by emphasizing subspaces that relate to the domain of interest. In this paper, we explore a more extreme version of this specialization by learning to map a given query to a query-specific feature space transformation. Because this transformation is linear, it can be applied with minimal computational cost to millions of image embeddings, making it effective for large-scale retrieval or re-ranking. Results show that this method consistently outperforms state-of-the-art alternatives, including those that require many orders of magnitude more computation at query time.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations</title>
<link>https://arxiv.org/abs/2505.21657</link>
<guid>https://arxiv.org/abs/2505.21657</guid>
<content:encoded><![CDATA[
arXiv:2505.21657v1 Announce Type: cross 
Abstract: Large language models like GPT, LLAMA, and Claude have become incredibly powerful at generating text, but they are still black boxes, so it is hard to understand how they decide what to say. That lack of transparency can be problematic, especially in fields where trust and accountability matter. To help with this, we introduce SMILE, a new method that explains how these models respond to different parts of a prompt. SMILE is model-agnostic and works by slightly changing the input, measuring how the output changes, and then highlighting which words had the most impact. Create simple visual heat maps showing which parts of a prompt matter the most. We tested SMILE on several leading LLMs and used metrics such as accuracy, consistency, stability, and fidelity to show that it gives clear and reliable explanations. By making these models easier to understand, SMILE brings us one step closer to making AI more transparent and trustworthy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STACI: Spatio-Temporal Aleatoric Conformal Inference</title>
<link>https://arxiv.org/abs/2505.21658</link>
<guid>https://arxiv.org/abs/2505.21658</guid>
<content:encoded><![CDATA[
arXiv:2505.21658v1 Announce Type: cross 
Abstract: Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty quantification for estimation of spatio-temporal fields. Spatio-temporal deep learning models, while scalable, typically assume a simplistic independent covariance matrix for the response, failing to capture the underlying correlation structure. However, spatio-temporal GPs suffer from issues of scalability and various forms of approximation bias resulting from restrictive assumptions of the covariance kernel function. We propose STACI, a novel framework consisting of a variational Bayesian neural network approximation of non-stationary spatio-temporal GP along with a novel spatio-temporal conformal inference algorithm. STACI is highly scalable, taking advantage of GPU training capabilities for neural network models, and provides statistically valid prediction intervals for uncertainty quantification. STACI outperforms competing GPs and deep methods in accurately approximating spatio-temporal processes and we show it easily scales to datasets with millions of observations.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing</title>
<link>https://arxiv.org/abs/2505.21671</link>
<guid>https://arxiv.org/abs/2505.21671</guid>
<content:encoded><![CDATA[
arXiv:2505.21671v1 Announce Type: cross 
Abstract: We study a sequential decision-making problem on a $n$-node graph $G$ where each node has an unknown label from a finite set $\mathbf{\Sigma}$, drawn from a joint distribution $P$ that is Markov with respect to $G$. At each step, selecting a node reveals its label and yields a label-dependent reward. The goal is to adaptively choose nodes to maximize expected accumulated discounted rewards. We impose a frontier exploration constraint, where actions are limited to neighbors of previously selected nodes, reflecting practical constraints in settings such as contact tracing and robotic exploration. We design a Gittins index-based policy that applies to general graphs and is provably optimal when $G$ is a forest. Our implementation runs in $O(n^2 \cdot |\mathbf{\Sigma}|^2)$ time while using $O(n \cdot |\mathbf{\Sigma}|^2)$ oracle calls to $P$ and $O(n^2 \cdot |\mathbf{\Sigma}|)$ space. Experiments on synthetic and real-world graphs show that our method consistently outperforms natural baselines, including in non-tree, budget-limited, and undiscounted settings. For example, in HIV testing simulations on real-world sexual interaction networks, our policy detects nearly all positive cases with only half the population tested, substantially outperforming other baselines.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>tenSVD algorithm for compression</title>
<link>https://arxiv.org/abs/2505.21686</link>
<guid>https://arxiv.org/abs/2505.21686</guid>
<content:encoded><![CDATA[
arXiv:2505.21686v1 Announce Type: cross 
Abstract: Tensors provide a robust framework for managing high-dimensional data. Consequently, tensor analysis has emerged as an active research area in various domains, including machine learning, signal processing, computer vision, graph analysis, and data mining. This study introduces an efficient image storage approach utilizing tensors, aiming to minimize memory to store, bandwidth to transmit and energy to processing. The proposed method organizes original data into a higher-order tensor and applies the Tucker model for compression. Implemented in R, this method is compared to a baseline algorithm. The evaluation focuses on efficient of algorithm measured in term of computational time and the quality of information preserved, using both simulated and real datasets. A detailed analysis of the results is conducted, employing established quantitative metrics, with significant attention paid to sustainability in terms of energy consumption across algorithms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model</title>
<link>https://arxiv.org/abs/2505.21689</link>
<guid>https://arxiv.org/abs/2505.21689</guid>
<content:encoded><![CDATA[
arXiv:2505.21689v1 Announce Type: cross 
Abstract: The persistent accumulation of unresolved legal cases, especially within the Indian judiciary, significantly hampers the timely delivery of justice. Manual methods of prioritizing petitions are often prone to inefficiencies and subjective biases further exacerbating delays. To address this issue, we propose LLMPR (Large Language Model-based Petition Ranking), an automated framework that utilizes transfer learning and machine learning to assign priority rankings to legal petitions based on their contextual urgency. Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process unstructured legal text and extract features through various embedding techniques, including DistilBERT, LegalBERT, and MiniLM. These textual embeddings are combined with quantitative indicators such as gap days, rank scores, and word counts to train multiple machine learning models, including Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments demonstrate that Random Forest and Decision Tree models yield superior performance, with accuracy exceeding 99% and a Spearman rank correlation of 0.99. Notably, models using only numerical features achieve nearly optimal ranking results (R2 = 0.988, \r{ho} = 0.998), while LLM-based embeddings offer only marginal gains. These findings suggest that automated petition ranking can effectively streamline judicial workflows, reduce case backlog, and improve fairness in legal prioritization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Data Enables Optimal Decisions? An Exact Characterization for Linear Optimization</title>
<link>https://arxiv.org/abs/2505.21692</link>
<guid>https://arxiv.org/abs/2505.21692</guid>
<content:encoded><![CDATA[
arXiv:2505.21692v1 Announce Type: cross 
Abstract: We study the fundamental question of how informative a dataset is for solving a given decision-making task. In our setting, the dataset provides partial information about unknown parameters that influence task outcomes. Focusing on linear programs, we characterize when a dataset is sufficient to recover an optimal decision, given an uncertainty set on the cost vector. Our main contribution is a sharp geometric characterization that identifies the directions of the cost vector that matter for optimality, relative to the task constraints and uncertainty set. We further develop a practical algorithm that, for a given task, constructs a minimal or least-costly sufficient dataset. Our results reveal that small, well-chosen datasets can often fully determine optimal decisions -- offering a principled foundation for task-aware data selection.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network classification through random walks</title>
<link>https://arxiv.org/abs/2505.21706</link>
<guid>https://arxiv.org/abs/2505.21706</guid>
<content:encoded><![CDATA[
arXiv:2505.21706v1 Announce Type: cross 
Abstract: Network models have been widely used to study diverse systems and analyze their dynamic behaviors. Given the structural variability of networks, an intriguing question arises: Can we infer the type of system represented by a network based on its structure? This classification problem involves extracting relevant features from the network. Existing literature has proposed various methods that combine structural measurements and dynamical processes for feature extraction. In this study, we introduce a novel approach to characterize networks using statistics from random walks, which can be particularly informative about network properties. We present the employed statistical metrics and compare their performance on multiple datasets with other state-of-the-art feature extraction methods. Our results demonstrate that the proposed method is effective in many cases, often outperforming existing approaches, although some limitations are observed across certain datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference</title>
<link>https://arxiv.org/abs/2505.21721</link>
<guid>https://arxiv.org/abs/2505.21721</guid>
<content:encoded><![CDATA[
arXiv:2505.21721v1 Announce Type: cross 
Abstract: We prove that, given a mean-field location-scale variational family, black-box variational inference (BBVI) with the reparametrization gradient converges at an almost dimension-independent rate. Specifically, for strongly log-concave and log-smooth targets, the number of iterations for BBVI with a sub-Gaussian family to achieve an objective $\epsilon$-close to the global optimum is $\mathrm{O}(\log d)$, which improves over the $\mathrm{O}(d)$ dependence of full-rank location-scale families. For heavy-tailed families, we provide a weaker $\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the number of finite moments. Additionally, if the Hessian of the target log-density is constant, the complexity is free of any explicit dimension dependence. We also prove that our bound on the gradient variance, which is key to our result, cannot be improved using only spectral bounds on the Hessian of the target log-density.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Statistical Methods Obsolete in the Era of Deep Learning?</title>
<link>https://arxiv.org/abs/2505.21723</link>
<guid>https://arxiv.org/abs/2505.21723</guid>
<content:encoded><![CDATA[
arXiv:2505.21723v1 Announce Type: cross 
Abstract: In the era of AI, neural networks have become increasingly popular for modeling, inference, and prediction, largely due to their potential for universal approximation. With the proliferation of such deep learning models, a question arises: are leaner statistical methods still relevant? To shed insight on this question, we employ the mechanistic nonlinear ordinary differential equation (ODE) inverse problem as a testbed, using physics-informed neural network (PINN) as a representative of the deep learning paradigm and manifold-constrained Gaussian process inference (MAGI) as a representative of statistically principled methods. Through case studies involving the SEIR model from epidemiology and the Lorenz model from chaotic dynamics, we demonstrate that statistical methods are far from obsolete, especially when working with sparse and noisy observations. On tasks such as parameter inference and trajectory reconstruction, statistically principled methods consistently achieve lower bias and variance, while using far fewer parameters and requiring less hyperparameter tuning. Statistical methods can also decisively outperform deep learning models on out-of-sample future prediction, where the absence of relevant data often leads overparameterized models astray. Additionally, we find that statistically principled approaches are more robust to accumulation of numerical imprecision and can represent the underlying system more faithful to the true governing ODEs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation</title>
<link>https://arxiv.org/abs/2505.21734</link>
<guid>https://arxiv.org/abs/2505.21734</guid>
<content:encoded><![CDATA[
arXiv:2505.21734v1 Announce Type: cross 
Abstract: Developing robust, efficient navigation algorithms is challenging. Rule-based methods offer interpretability and modularity but struggle with learning from large datasets, while end-to-end neural networks excel in learning but lack transparency and modularity. In this paper, we present MIND-Stack, a modular software stack consisting of a localization network and a Stanley Controller with intermediate human interpretable state representations and end-to-end differentiability. Our approach enables the upstream localization module to reduce the downstream control error, extending its role beyond state estimation. Unlike existing research on differentiable algorithms that either lack modules of the autonomous stack to span from sensor input to actuator output or real-world implementation, MIND-Stack offers both capabilities. We conduct experiments that demonstrate the ability of the localization module to reduce the downstream control loss through its end-to-end differentiability while offering better performance than state-of-the-art algorithms. We showcase sim-to-real capabilities by deploying the algorithm on a real-world embedded autonomous platform with limited computation power and demonstrate simultaneous training of both the localization and controller towards one goal. While MIND-Stack shows good results, we discuss the incorporation of additional modules from the autonomous navigation pipeline in the future, promising even greater stability and performance in the next iterations of the framework.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks</title>
<link>https://arxiv.org/abs/2505.21736</link>
<guid>https://arxiv.org/abs/2505.21736</guid>
<content:encoded><![CDATA[
arXiv:2505.21736v1 Announce Type: cross 
Abstract: The principle of translation equivariance (if an input image is translated an output image should be translated by the same amount), led to the development of convolutional neural networks that revolutionized machine vision. Other symmetries, like rotations and reflections, play a similarly critical role, especially in biomedical image analysis, but exploiting these symmetries has not seen wide adoption. We hypothesize that this is partially due to the mathematical complexity of methods used to exploit these symmetries, which often rely on representation theory, a bespoke concept in differential geometry and group theory. In this work, we show that the same equivariance can be achieved using a simple form of convolution kernels that we call ``moment kernels,'' and prove that all equivariant kernels must take this form. These are a set of radially symmetric functions of a spatial position $x$, multiplied by powers of the components of $x$ or the identity matrix. We implement equivariant neural networks using standard convolution modules, and provide architectures to execute several biomedical image analysis tasks that depend on equivariance principles: classification (outputs are invariant under orthogonal transforms), 3D image registration (outputs transform like a vector), and cell segmentation (quadratic forms defining ellipses transform like a matrix).
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is Adversarial Training for Diffusion Models?</title>
<link>https://arxiv.org/abs/2505.21742</link>
<guid>https://arxiv.org/abs/2505.21742</guid>
<content:encoded><![CDATA[
arXiv:2505.21742v1 Announce Type: cross 
Abstract: We answer the question in the title, showing that adversarial training (AT) for diffusion models (DMs) fundamentally differs from classifiers: while AT in classifiers enforces output invariance, AT in DMs requires equivariance to keep the diffusion process aligned with the data distribution. AT is a way to enforce smoothness in the diffusion flow, improving robustness to outliers and corrupted data. Unlike prior art, our method makes no assumptions about the noise model and integrates seamlessly into diffusion training by adding random noise, similar to randomized smoothing, or adversarial noise, akin to AT. This enables intrinsic capabilities such as handling noisy data, dealing with extreme variability such as outliers, preventing memorization, and improving robustness. We rigorously evaluate our approach with proof-of-concept datasets with known distributions in low- and high-dimensional space, thereby taking a perfect measure of errors; we further evaluate on standard benchmarks such as CIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe noise, data corruption, and iterative adversarial attacks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture</title>
<link>https://arxiv.org/abs/2505.21746</link>
<guid>https://arxiv.org/abs/2505.21746</guid>
<content:encoded><![CDATA[
arXiv:2505.21746v1 Announce Type: cross 
Abstract: Unmanned Aircraft Systems (UAS) and satellites are key data sources for precision agriculture, yet each presents trade-offs. Satellite data offer broad spatial, temporal, and spectral coverage but lack the resolution needed for many precision farming applications, while UAS provide high spatial detail but are limited by coverage and cost, especially for hyperspectral data. This study presents a novel framework that fuses satellite and UAS imagery using super-resolution methods. By integrating data across spatial, spectral, and temporal domains, we leverage the strengths of both platforms cost-effectively. We use estimation of cover crop biomass and nitrogen (N) as a case study to evaluate our approach. By spectrally extending UAS RGB data to the vegetation red edge and near-infrared regions, we generate high-resolution Sentinel-2 imagery and improve biomass and N estimation accuracy by 18% and 31%, respectively. Our results show that UAS data need only be collected from a subset of fields and time points. Farmers can then 1) enhance the spectral detail of UAS RGB imagery; 2) increase the spatial resolution by using satellite data; and 3) extend these enhancements spatially and across the growing season at the frequency of the satellite flights. Our SRCNN-based spectral extension model shows considerable promise for model transferability over other cropping systems in the Upper and Lower Chesapeake Bay regions. Additionally, it remains effective even when cloud-free satellite data are unavailable, relying solely on the UAS RGB input. The spatial extension model produces better biomass and N predictions than models built on raw UAS RGB images. Once trained with targeted UAS RGB data, the spatial extension model allows farmers to stop repeated UAS flights. While we introduce super-resolution advances, the core contribution is a lightweight and scalable system for affordable on-farm use.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.21755</link>
<guid>https://arxiv.org/abs/2505.21755</guid>
<content:encoded><![CDATA[
arXiv:2505.21755v1 Announce Type: cross 
Abstract: Visual question answering (VQA) systems face significant challenges when adapting to real-world data shifts, especially in multi-modal contexts. While robust fine-tuning strategies are essential for maintaining performance across in-distribution (ID) and out-of-distribution (OOD) scenarios, current evaluation settings are primarily unimodal or particular to some types of OOD, offering limited insight into the complexities of multi-modal contexts. In this work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA and others, and categorize them into ID, near and far OOD datasets covering uni-modal, multi-modal and adversarial distribution shifts. We first conduct a comprehensive comparison of existing robust fine-tuning methods. We then quantify the distribution shifts by calculating the Mahalanobis distance using uni-modal and multi-modal embeddings extracted from various models. Further, we perform an extensive analysis to explore the interactions between uni- and multi-modal shifts as well as modality importance for ID and OOD samples. These analyses offer valuable guidance on developing more robust fine-tuning methods to handle multi-modal distribution shifts. The code is available at https://github.com/chengyuehuang511/FRAMES-VQA .
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond 1D: Vision Transformers and Multichannel Signal Images for PPG-to-ECG Reconstruction</title>
<link>https://arxiv.org/abs/2505.21767</link>
<guid>https://arxiv.org/abs/2505.21767</guid>
<content:encoded><![CDATA[
arXiv:2505.21767v1 Announce Type: cross 
Abstract: Reconstructing ECG from PPG is a promising yet challenging task. While recent advancements in generative models have significantly improved ECG reconstruction, accurately capturing fine-grained waveform features remains a key challenge. To address this, we propose a novel PPG-to-ECG reconstruction method that leverages a Vision Transformer (ViT) as the core network. Unlike conventional approaches that rely on single-channel PPG, our method employs a four-channel signal image representation, incorporating the original PPG, its first-order difference, second-order difference, and area under the curve. This multi-channel design enriches feature extraction by preserving both temporal and physiological variations within the PPG. By leveraging the self-attention mechanism in ViT, our approach effectively captures both inter-beat and intra-beat dependencies, leading to more robust and accurate ECG reconstruction. Experimental results demonstrate that our method consistently outperforms existing 1D convolution-based approaches, achieving up to 29% reduction in PRD and 15% reduction in RMSE. The proposed approach also produces improvements in other evaluation metrics, highlighting its robustness and effectiveness in reconstructing ECG signals. Furthermore, to ensure a clinically relevant evaluation, we introduce new performance metrics, including QRS area error, PR interval error, RT interval error, and RT amplitude difference error. Our findings suggest that integrating a four-channel signal image representation with the self-attention mechanism of ViT enables more effective extraction of informative PPG features and improved modeling of beat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the potential of PPG as a viable alternative for heart activity monitoring, our approach opens new avenues for cyclic signal analysis and prediction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks</title>
<link>https://arxiv.org/abs/2505.21791</link>
<guid>https://arxiv.org/abs/2505.21791</guid>
<content:encoded><![CDATA[
arXiv:2505.21791v1 Announce Type: cross 
Abstract: Overparameterized neural networks can interpolate a given dataset in many different ways, prompting the fundamental question: which among these solutions should we prefer, and what explicit regularization strategies will provably yield these solutions? This paper addresses the challenge of finding the sparsest interpolating ReLU network -- i.e., the network with the fewest nonzero parameters or neurons -- a goal with wide-ranging implications for efficiency, generalization, interpretability, theory, and model compression. Unlike post hoc pruning approaches, we propose a continuous, almost-everywhere differentiable training objective whose global minima are guaranteed to correspond to the sparsest single-hidden-layer ReLU networks that fit the data. This result marks a conceptual advance: it recasts the combinatorial problem of sparse interpolation as a smooth optimization task, potentially enabling the use of gradient-based training methods. Our objective is based on minimizing $\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical sparsity-promoting strategy in finite-dimensional settings. However, applying these ideas to neural networks presents new challenges: the function class is infinite-dimensional, and the weights are learned using a highly nonconvex objective. We prove that, under our formulation, global minimizers correspond exactly to sparsest solutions. Our work lays a foundation for understanding when and how continuous sparsity-inducing objectives can be leveraged to recover sparse networks through training.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A General-Purpose Theorem for High-Probability Bounds of Stochastic Approximation with Polyak Averaging</title>
<link>https://arxiv.org/abs/2505.21796</link>
<guid>https://arxiv.org/abs/2505.21796</guid>
<content:encoded><![CDATA[
arXiv:2505.21796v1 Announce Type: cross 
Abstract: Polyak-Ruppert averaging is a widely used technique to achieve the optimal asymptotic variance of stochastic approximation (SA) algorithms, yet its high-probability performance guarantees remain underexplored in general settings. In this paper, we present a general framework for establishing non-asymptotic concentration bounds for the error of averaged SA iterates. Our approach assumes access to individual concentration bounds for the unaveraged iterates and yields a sharp bound on the averaged iterates. We also construct an example, showing the tightness of our result up to constant multiplicative factors. As direct applications, we derive tight concentration bounds for contractive SA algorithms and for algorithms such as temporal difference learning and Q-learning with averaging, obtaining new bounds in settings where traditional analysis is challenging.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective</title>
<link>https://arxiv.org/abs/2505.21799</link>
<guid>https://arxiv.org/abs/2505.21799</guid>
<content:encoded><![CDATA[
arXiv:2505.21799v1 Announce Type: cross 
Abstract: The ever-growing scale of deep learning models and datasets underscores the critical importance of efficient optimization methods. While preconditioned gradient methods such as Adam and AdamW are the de facto optimizers for training neural networks and large language models, structure-aware preconditioned optimizers like Shampoo and Muon, which utilize the matrix structure of gradients, have demonstrated promising evidence of faster convergence. In this paper, we introduce a unifying framework for analyzing "matrix-aware" preconditioned methods, which not only sheds light on the effectiveness of Muon and related optimizers but also leads to a class of new structure-aware preconditioned methods. A key contribution of this framework is its precise distinction between preconditioning strategies that treat neural network weights as vectors (addressing curvature anisotropy) versus those that consider their matrix structure (addressing gradient anisotropy). This perspective provides new insights into several empirical phenomena in language model pre-training, including Adam's training instabilities, Muon's accelerated convergence, and the necessity of learning rate warmup for Adam. Building upon this framework, we introduce PolarGrad, a new class of preconditioned optimization methods based on the polar decomposition of matrix-valued gradients. As a special instance, PolarGrad includes Muon with updates scaled by the nuclear norm of the gradients. We provide numerical implementations of these methods, leveraging efficient numerical polar decomposition algorithms for enhanced convergence. Our extensive evaluations across diverse matrix optimization problems and language model pre-training tasks demonstrate that PolarGrad outperforms both Adam and Muon.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect</title>
<link>https://arxiv.org/abs/2505.21809</link>
<guid>https://arxiv.org/abs/2505.21809</guid>
<content:encoded><![CDATA[
arXiv:2505.21809v1 Announce Type: cross 
Abstract: Perceptual voice quality dimensions describe key characteristics of atypical speech and other speech modulations. Here we develop and evaluate voice quality models for seven voice and speech dimensions (intelligibility, imprecise consonants, harsh voice, naturalness, monoloudness, monopitch, and breathiness). Probes were trained on the public Speech Accessibility (SAP) project dataset with 11,184 samples from 434 speakers, using embeddings from frozen pre-trained models as features. We found that our probes had both strong performance and strong generalization across speech elicitation categories in the SAP dataset. We further validated zero-shot performance on additional datasets, encompassing unseen languages and tasks: Italian atypical speech, English atypical speech, and affective speech. The strong zero-shot performance and the interpretability of results across an array of evaluations suggests the utility of using voice quality dimensions in speaking style-related tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representative Language Generation</title>
<link>https://arxiv.org/abs/2505.21819</link>
<guid>https://arxiv.org/abs/2505.21819</guid>
<content:encoded><![CDATA[
arXiv:2505.21819v1 Announce Type: cross 
Abstract: We introduce "representative generation," extending the theoretical framework for generation proposed by Kleinberg et al. (2024) and formalized by Li et al. (2024), to additionally address diversity and bias concerns in generative models. Our notion requires outputs of a generative model to proportionally represent groups of interest from the training data. We characterize representative uniform and non-uniform generation, introducing the "group closure dimension" as a key combinatorial quantity. For representative generation in the limit, we analyze both information-theoretic and computational aspects, demonstrating feasibility for countably infinite hypothesis classes and collections of groups under certain conditions, but proving a negative result for computability using only membership queries. This contrasts with Kleinberg et al.'s (2024) positive results for standard generation in the limit. Our findings provide a rigorous foundation for developing more diverse and representative generative models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Music Source Restoration</title>
<link>https://arxiv.org/abs/2505.21827</link>
<guid>https://arxiv.org/abs/2505.21827</guid>
<content:encoded><![CDATA[
arXiv:2505.21827v1 Announce Type: cross 
Abstract: We introduce Music Source Restoration (MSR), a novel task addressing the gap between idealized source separation and real-world music production. Current Music Source Separation (MSS) approaches assume mixtures are simple sums of sources, ignoring signal degradations employed during music production like equalization, compression, and reverb. MSR models mixtures as degraded sums of individually degraded sources, with the goal of recovering original, undegraded signals. Due to the lack of data for MSR, we present RawStems, a dataset annotation of 578 songs with unprocessed source signals organized into 8 primary and 17 secondary instrument groups, totaling 354.13 hours. To the best of our knowledge, RawStems is the first dataset that contains unprocessed music stems with hierarchical categories. We consider spectral filtering, dynamic range compression, harmonic distortion, reverb and lossy codec as possible degradations, and establish U-Former as a baseline method, demonstrating the feasibility of MSR on our dataset. We release the RawStems dataset annotations, degradation simulation pipeline, training code and pre-trained models to be publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniMoGen: Universal Motion Generation</title>
<link>https://arxiv.org/abs/2505.21837</link>
<guid>https://arxiv.org/abs/2505.21837</guid>
<content:encoded><![CDATA[
arXiv:2505.21837v1 Announce Type: cross 
Abstract: Motion generation is a cornerstone of computer graphics, animation, gaming, and robotics, enabling the creation of realistic and varied character movements. A significant limitation of existing methods is their reliance on specific skeletal structures, which restricts their versatility across different characters. To overcome this, we introduce UniMoGen, a novel UNet-based diffusion model designed for skeleton-agnostic motion generation. UniMoGen can be trained on motion data from diverse characters, such as humans and animals, without the need for a predefined maximum number of joints. By dynamically processing only the necessary joints for each character, our model achieves both skeleton agnosticism and computational efficiency. Key features of UniMoGen include controllability via style and trajectory inputs, and the ability to continue motions from past frames. We demonstrate UniMoGen's effectiveness on the 100style dataset, where it outperforms state-of-the-art methods in diverse character motion generation. Furthermore, when trained on both the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen achieves high performance and improved efficiency across both skeletons. These results highlight UniMoGen's potential to advance motion generation by providing a flexible, efficient, and controllable solution for a wide range of character animations.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physics-Informed Learning Framework to Solve the Infinite-Horizon Optimal Control Problem</title>
<link>https://arxiv.org/abs/2505.21842</link>
<guid>https://arxiv.org/abs/2505.21842</guid>
<content:encoded><![CDATA[
arXiv:2505.21842v1 Announce Type: cross 
Abstract: We propose a physics-informed neural networks (PINNs) framework to solve the infinite-horizon optimal control problem of nonlinear systems. In particular, since PINNs are generally able to solve a class of partial differential equations (PDEs), they can be employed to learn the value function of the infinite-horizon optimal control problem via solving the associated steady-state Hamilton-Jacobi-Bellman (HJB) equation. However, an issue here is that the steady-state HJB equation generally yields multiple solutions; hence if PINNs are directly employed to it, they may end up approximating a solution that is different from the optimal value function of the problem. We tackle this by instead applying PINNs to a finite-horizon variant of the steady-state HJB that has a unique solution, and which uniformly approximates the optimal value function as the horizon increases. An algorithm to verify if the chosen horizon is large enough is also given, as well as a method to extend it -- with reduced computations and robustness to approximation errors -- in case it is not. Unlike many existing methods, the proposed technique works well with non-polynomial basis functions, does not require prior knowledge of a stabilizing controller, and does not perform iterative policy evaluations. Simulations are performed, which verify and clarify theoretical findings.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral clustering for dependent community Hawkes process models of temporal networks</title>
<link>https://arxiv.org/abs/2505.21845</link>
<guid>https://arxiv.org/abs/2505.21845</guid>
<content:encoded><![CDATA[
arXiv:2505.21845v1 Announce Type: cross 
Abstract: Temporal networks observed continuously over time through timestamped relational events data are commonly encountered in application settings including online social media communications, financial transactions, and international relations. Temporal networks often exhibit community structure and strong dependence patterns among node pairs. This dependence can be modeled through mutual excitations, where an interaction event from a sender to a receiver node increases the possibility of future events among other node pairs.
  We provide statistical results for a class of models that we call dependent community Hawkes (DCH) models, which combine the stochastic block model with mutually exciting Hawkes processes for modeling both community structure and dependence among node pairs, respectively. We derive a non-asymptotic upper bound on the misclustering error of spectral clustering on the event count matrix as a function of the number of nodes and communities, time duration, and the amount of dependence in the model. Our result leverages recent results on bounding an appropriate distance between a multivariate Hawkes process count vector and a Gaussian vector, along with results from random matrix theory. We also propose a DCH model that incorporates only self and reciprocal excitation along with highly scalable parameter estimation using a Generalized Method of Moments (GMM) estimator that we demonstrate to be consistent for growing network size and time duration.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by treating action trajectories as flow trajectories</title>
<link>https://arxiv.org/abs/2505.21851</link>
<guid>https://arxiv.org/abs/2505.21851</guid>
<content:encoded><![CDATA[
arXiv:2505.21851v1 Announce Type: cross 
Abstract: Recent advances in diffusion$/$flow-matching policies have enabled imitation learning of complex, multi-modal action trajectories. However, they are computationally expensive because they sample a trajectory of trajectories: a diffusion$/$flow trajectory of action trajectories. They discard intermediate action trajectories, and must wait for the sampling process to complete before any actions can be executed on the robot. We simplify diffusion$/$flow policies by treating action trajectories as flow trajectories. Instead of starting from pure noise, our algorithm samples from a narrow Gaussian around the last action. Then, it incrementally integrates a velocity field learned via flow matching to produce a sequence of actions that constitute a single trajectory. This enables actions to be streamed to the robot on-the-fly during the flow sampling process, and is well-suited for receding horizon policy execution. Despite streaming, our method retains the ability to model multi-modal behavior. We train flows that stabilize around demonstration trajectories to reduce distribution shift and improve imitation learning performance. Streaming flow policy outperforms prior methods while enabling faster policy execution and tighter sensorimotor loops for learning-based robot control. Project website: https://streaming-flow-policy.github.io/
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images</title>
<link>https://arxiv.org/abs/2505.21872</link>
<guid>https://arxiv.org/abs/2505.21872</guid>
<content:encoded><![CDATA[
arXiv:2505.21872v1 Announce Type: cross 
Abstract: Machine unlearning aims to remove the influence of specific training samples from a trained model without full retraining. While prior work has largely focused on privacy-motivated settings, we recast unlearning as a general-purpose tool for post-deployment model revision. Specifically, we focus on utilizing unlearning in clinical contexts where data shifts, device deprecation, and policy changes are common. To this end, we propose a bilevel optimization formulation of boundary-based unlearning that can be solved using iterative algorithms. We provide convergence guarantees when first-order algorithms are used to unlearn. Our method introduces tunable loss design for controlling the forgetting-retention tradeoff and supports novel model composition strategies that merge the strengths of distinct unlearning runs. Across benchmark and real-world clinical imaging datasets, our approach outperforms baselines on both forgetting and retention metrics, including scenarios involving imaging devices and anatomical outliers. This work establishes machine unlearning as a modular, practical alternative to retraining for real-world model maintenance in clinical applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Foundation Regressor on Complex Networks</title>
<link>https://arxiv.org/abs/2505.21879</link>
<guid>https://arxiv.org/abs/2505.21879</guid>
<content:encoded><![CDATA[
arXiv:2505.21879v1 Announce Type: cross 
Abstract: In science, we are interested not only in forecasting but also in understanding how predictions are made, specifically what the interpretable underlying model looks like. Data-driven machine learning technology can significantly streamline the complex and time-consuming traditional manual process of discovering scientific laws, helping us gain insights into fundamental issues in modern science. In this work, we introduce a pre-trained symbolic foundation regressor that can effectively compress complex data with numerous interacting variables while producing interpretable physical representations. Our model has been rigorously tested on non-network symbolic regression, symbolic regression on complex networks, and the inference of network dynamics across various domains, including physics, biochemistry, ecology, and epidemiology. The results indicate a remarkable improvement in equation inference efficiency, being three times more effective than baseline approaches while maintaining accurate predictions. Furthermore, we apply our model to uncover more intuitive laws of interaction transmission from global epidemic outbreak data, achieving optimal data fitting. This model extends the application boundary of pre-trained symbolic regression models to complex networks, and we believe it provides a foundational solution for revealing the hidden mechanisms behind changes in complex phenomena, enhancing interpretability, and inspiring further scientific discoveries.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2505.21887</link>
<guid>https://arxiv.org/abs/2505.21887</guid>
<content:encoded><![CDATA[
arXiv:2505.21887v1 Announce Type: cross 
Abstract: Robust routing under uncertainty is central to real-world logistics, yet most benchmarks assume static, idealized settings. We present SVRPBench, the first open benchmark to capture high-fidelity stochastic dynamics in vehicle routing at urban scale. Spanning more than 500 instances with up to 1000 customers, it simulates realistic delivery conditions: time-dependent congestion, log-normal delays, probabilistic accidents, and empirically grounded time windows for residential and commercial clients. Our pipeline generates diverse, constraint-rich scenarios, including multi-depot and multi-vehicle setups. Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade by over 20% under distributional shift, while classical and metaheuristic methods remain robust. To enable reproducible research, we release the dataset and evaluation suite. SVRPBench challenges the community to design solvers that generalize beyond synthetic assumptions and adapt to real-world uncertainty.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Almost Linear Convergence under Minimal Score Assumptions: Quantized Transition Diffusion</title>
<link>https://arxiv.org/abs/2505.21892</link>
<guid>https://arxiv.org/abs/2505.21892</guid>
<content:encoded><![CDATA[
arXiv:2505.21892v1 Announce Type: cross 
Abstract: Continuous diffusion models have demonstrated remarkable performance in data generation across various domains, yet their efficiency remains constrained by two critical limitations: (1) the local adjacency structure of the forward Markov process, which restricts long-range transitions in the data space, and (2) inherent biases introduced during the simulation of time-inhomogeneous reverse denoising processes. To address these challenges, we propose Quantized Transition Diffusion (QTD), a novel approach that integrates data quantization with discrete diffusion dynamics. Our method first transforms the continuous data distribution $p_*$ into a discrete one $q_*$ via histogram approximation and binary encoding, enabling efficient representation in a structured discrete latent space. We then design a continuous-time Markov chain (CTMC) with Hamming distance-based transitions as the forward process, which inherently supports long-range movements in the original data space. For reverse-time sampling, we introduce a \textit{truncated uniformization} technique to simulate the reverse CTMC, which can provably provide unbiased generation from $q_*$ under minimal score assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we prove that QTD can generate samples with $O(d\ln^2(d/\epsilon))$ score evaluations in expectation to approximate the $d$--dimensional target distribution $p_*$ within an $\epsilon$ error tolerance. Our method not only establishes state-of-the-art inference efficiency but also advances the theoretical foundations of diffusion-based generative modeling by unifying discrete and continuous diffusion paradigms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination</title>
<link>https://arxiv.org/abs/2505.21925</link>
<guid>https://arxiv.org/abs/2505.21925</guid>
<content:encoded><![CDATA[
arXiv:2505.21925v1 Announce Type: cross 
Abstract: We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels. RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. We demonstrate and evaluate RenderFormer on scenes with varying complexity in shape and light transport.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology</title>
<link>https://arxiv.org/abs/2505.21928</link>
<guid>https://arxiv.org/abs/2505.21928</guid>
<content:encoded><![CDATA[
arXiv:2505.21928v1 Announce Type: cross 
Abstract: Gastrointestinal (GI) diseases represent a clinically significant burden, necessitating precise diagnostic approaches to optimize patient outcomes. Conventional histopathological diagnosis, heavily reliant on the subjective interpretation of pathologists, suffers from limited reproducibility and diagnostic variability. To overcome these limitations and address the lack of pathology-specific foundation models for GI diseases, we develop Digepath, a specialized foundation model for GI pathology. Our framework introduces a dual-phase iterative optimization strategy combining pretraining with fine-screening, specifically designed to address the detection of sparsely distributed lesion areas in whole-slide images. Digepath is pretrained on more than 353 million image patches from over 200,000 hematoxylin and eosin-stained slides of GI diseases. It attains state-of-the-art performance on 33 out of 34 tasks related to GI pathology, including pathological diagnosis, molecular prediction, gene mutation prediction, and prognosis evaluation, particularly in diagnostically ambiguous cases and resolution-agnostic tissue classification.We further translate the intelligent screening module for early GI cancer and achieve near-perfect 99.6% sensitivity across 9 independent medical institutions nationwide. The outstanding performance of Digepath highlights its potential to bridge critical gaps in histopathological practice. This work not only advances AI-driven precision pathology for GI diseases but also establishes a transferable paradigm for other pathology subspecialties.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Group Synchronization</title>
<link>https://arxiv.org/abs/2505.21932</link>
<guid>https://arxiv.org/abs/2505.21932</guid>
<content:encoded><![CDATA[
arXiv:2505.21932v1 Announce Type: cross 
Abstract: Group synchronization is the problem of determining reliable global estimates from noisy local measurements on networks. The typical task for group synchronization is to assign elements of a group to the nodes of a graph in a way that respects group elements given on the edges which encode information about local pairwise relationships between the nodes. In this paper, we introduce a novel higher-order group synchronization problem which operates on a hypergraph and seeks to synchronize higher-order local measurements on the hyperedges to obtain global estimates on the nodes. Higher-order group synchronization is motivated by applications to computer vision and image processing, among other computational problems. First, we define the problem of higher-order group synchronization and discuss its mathematical foundations. Specifically, we give necessary and sufficient synchronizability conditions which establish the importance of cycle consistency in higher-order group synchronization. Then, we propose the first computational framework for general higher-order group synchronization; it acts globally and directly on higher-order measurements using a message passing algorithm. We discuss theoretical guarantees for our framework, including convergence analyses under outliers and noise. Finally, we show potential advantages of our method through numerical experiments. In particular, we show that in certain cases our higher-order method applied to rotational and angular synchronization outperforms standard pairwise synchronization methods and is more robust to outliers. We also show that our method has comparable performance on simulated cryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM reconstruction package.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.21956</link>
<guid>https://arxiv.org/abs/2505.21956</guid>
<content:encoded><![CDATA[
arXiv:2505.21956v1 Announce Type: cross 
Abstract: Text-to-image generation increasingly demands access to domain-specific, fine-grained, and rapidly evolving knowledge that pretrained models cannot fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to address this by retrieving globally relevant images, but they fail when no single image contains all desired elements from a complex user query. We propose Cross-modal RAG, a novel framework that decomposes both queries and images into sub-dimensional components, enabling subquery-aware retrieval and generation. Our method introduces a hybrid retrieval strategy - combining a sub-dimensional sparse retriever with a dense retriever - to identify a Pareto-optimal set of images, each contributing complementary aspects of the query. During generation, a multimodal large language model is guided to selectively condition on relevant visual features aligned to specific subqueries, ensuring subquery-aware image synthesis. Extensive experiments on MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal RAG significantly outperforms existing baselines in both retrieval and generation quality, while maintaining high efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.21985</link>
<guid>https://arxiv.org/abs/2505.21985</guid>
<content:encoded><![CDATA[
arXiv:2505.21985v1 Announce Type: cross 
Abstract: In multi-agent reinforcement learning (MARL), effective communication improves agent performance, particularly under partial observability. We propose MARL-CPC, a framework that enables communication among fully decentralized, independent agents without parameter sharing. MARL-CPC incorporates a message learning model based on collective predictive coding (CPC) from emergent communication research. Unlike conventional methods that treat messages as part of the action space and assume cooperation, MARL-CPC links messages to state inference, supporting communication in non-cooperative, reward-independent settings. We introduce two algorithms -Bandit-CPC and IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that both outperform standard message-as-action approaches, establishing effective communication even when messages offer no direct benefit to the sender. These results highlight MARL-CPC's potential for enabling coordination in complex, decentralized environments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locking-Free Training of Physics-Informed Neural Network for Solving Nearly Incompressible Elasticity Equations</title>
<link>https://arxiv.org/abs/2505.21994</link>
<guid>https://arxiv.org/abs/2505.21994</guid>
<content:encoded><![CDATA[
arXiv:2505.21994v1 Announce Type: cross 
Abstract: Due to divergence instability, the accuracy of low-order conforming finite element methods for nearly incompressible homogeneous elasticity equations deteriorates as the Lam\'e coefficient $\lambda\to\infty$, or equivalently as the Poisson ratio $\nu\to1/2$. This phenomenon, known as locking or non-robustness, remains not fully understood despite extensive investigation. In this paper, we propose a robust method based on a fundamentally different, machine-learning-driven approach. Leveraging recently developed Physics-Informed Neural Networks (PINNs), we address the numerical solution of linear elasticity equations governing nearly incompressible materials. The core idea of our method is to appropriately decompose the given equations to alleviate the extreme imbalance in the coefficients, while simultaneously solving both the forward and inverse problems to recover the solutions of the decomposed systems as well as the associated external conditions. Through various numerical experiments, including constant, variable and parametric Lam\'e coefficients, we illustrate the efficiency of the proposed methodology.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences</title>
<link>https://arxiv.org/abs/2505.22008</link>
<guid>https://arxiv.org/abs/2505.22008</guid>
<content:encoded><![CDATA[
arXiv:2505.22008v1 Announce Type: cross 
Abstract: Data assimilation (DA) aims to estimate the full state of a dynamical system by combining partial and noisy observations with a prior model forecast, commonly referred to as the background. In atmospheric applications, this problem is fundamentally ill-posed due to the sparsity of observations relative to the high-dimensional state space. Traditional methods address this challenge by simplifying background priors to regularize the solution, which are empirical and require continual tuning for application. Inspired by alignment techniques in text-to-image diffusion models, we propose Align-DA, which formulates DA as a generative process and uses reward signals to guide background priors, replacing manual tuning with data-driven alignment. Specifically, we train a score-based model in the latent space to approximate the background-conditioned prior, and align it using three complementary reward signals for DA: (1) assimilation accuracy, (2) forecast skill initialized from the assimilated state, and (3) physical adherence of the analysis fields. Experiments with multiple reward signals demonstrate consistent improvements in analysis quality across different evaluation metrics and observation-guidance strategies. These results show that preference alignment, implemented as a soft constraint, can automatically adapt complex background priors tailored to DA, offering a promising new direction for advancing the field.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Curves of Stochastic Gradient Descent in Kernel Regression</title>
<link>https://arxiv.org/abs/2505.22048</link>
<guid>https://arxiv.org/abs/2505.22048</guid>
<content:encoded><![CDATA[
arXiv:2505.22048v1 Announce Type: cross 
Abstract: This paper considers a canonical problem in kernel regression: how good are the model performances when it is trained by the popular online first-order algorithms, compared to the offline ones, such as ridge and ridgeless regression? In this paper, we analyze the foundational single-pass Stochastic Gradient Descent (SGD) in kernel regression under source condition where the optimal predictor can even not belong to the RKHS, i.e. the model is misspecified. Specifically, we focus on the inner product kernel over the sphere and characterize the exact orders of the excess risk curves under different scales of sample sizes $n$ concerning the input dimension $d$. Surprisingly, we show that SGD achieves min-max optimal rates up to constants among all the scales, without suffering the saturation, a prevalent phenomenon observed in (ridge) regression, except when the model is highly misspecified and the learning is in a final stage where $n\gg d^{\gamma}$ with any constant $\gamma >0$. The main reason for SGD to overcome the curse of saturation is the exponentially decaying step size schedule, a common practice in deep neural network training. As a byproduct, we provide the \emph{first} provable advantage of the scheme over the iterative averaging method in the common setting.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Reasoning for Embodied Planning</title>
<link>https://arxiv.org/abs/2505.22050</link>
<guid>https://arxiv.org/abs/2505.22050</guid>
<content:encoded><![CDATA[
arXiv:2505.22050v1 Announce Type: cross 
Abstract: Embodied planning requires agents to make coherent multi-step decisions based on dynamic visual observations and natural language goals. While recent vision-language models (VLMs) excel at static perception tasks, they struggle with the temporal reasoning, spatial understanding, and commonsense grounding needed for planning in interactive environments. In this work, we introduce a reinforcement fine-tuning framework that brings R1-style reasoning enhancement into embodied planning. We first distill a high-quality dataset from a powerful closed-source model and perform supervised fine-tuning (SFT) to equip the model with structured decision-making priors. We then design a rule-based reward function tailored to multi-step action quality and optimize the policy via Generalized Reinforced Preference Optimization (GRPO). Our approach is evaluated on Embench, a recent benchmark for interactive embodied tasks, covering both in-domain and out-of-domain scenarios. Experimental results show that our method significantly outperforms models of similar or larger scale, including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong generalization to unseen environments. This work highlights the potential of reinforcement-driven reasoning to advance long-horizon planning in embodied AI.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz</title>
<link>https://arxiv.org/abs/2505.22083</link>
<guid>https://arxiv.org/abs/2505.22083</guid>
<content:encoded><![CDATA[
arXiv:2505.22083v1 Announce Type: cross 
Abstract: In this work, we introduce the first type of non-Euclidean neural quantum state (NQS) ansatz, in the form of the hyperbolic GRU (a variant of recurrent neural networks (RNNs)), to be used in the Variational Monte Carlo method of approximating the ground state wavefunction for quantum many-body systems. In particular, we examine the performances of NQS ansatzes constructed from both conventional or Euclidean RNN/GRU and from hyperbolic GRU in the prototypical settings of the one- and two-dimensional transverse field Ising models (TFIM) of up to 100 spins and the one-dimensional Heisenberg $J_1J_2$ and $J_1J_2J_3$ systems of up 50 spins. By virtue of the fact that, for all of the experiments performed in this work, hyperbolic GRU can yield performances comparable to or better than Euclidean RNNs, which have been extensively studied in these settings in the literature, our work is a proof-of-concept for the viability of hyperbolic GRU as the first type of non-Euclidean NQS ansatz for quantum many-body systems. Furthermore, in settings where the Hamiltonian displays a clear hierarchical interaction structure, such as the 1D Heisenberg $J_1J_2$ & $J_1J_2J_3$ systems with the 1st, 2nd and even 3rd nearest neighbor interactions, our results show that hyperbolic GRU definitively outperforms its Euclidean version in all instances. The fact that these results are reminiscent of the established ones from natural language processing where hyperbolic GRU almost always outperforms Euclidean RNNs when the training data exhibit a tree-like or hierarchical structure leads us to hypothesize that hyperbolic GRU NQS ansatz would likely outperform Euclidean RNN/GRU NQS ansatz in quantum spin systems that involve different degrees of nearest neighbor interactions. Finally, with this work, we hope to initiate future studies of other types of non-Euclidean NQS beyond hyperbolic GRU.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PADAM: Parallel averaged Adam reduces the error for stochastic optimization in scientific machine learning</title>
<link>https://arxiv.org/abs/2505.22085</link>
<guid>https://arxiv.org/abs/2505.22085</guid>
<content:encoded><![CDATA[
arXiv:2505.22085v1 Announce Type: cross 
Abstract: Averaging techniques such as Ruppert--Polyak averaging and exponential movering averaging (EMA) are powerful approaches to accelerate optimization procedures of stochastic gradient descent (SGD) optimization methods such as the popular ADAM optimizer. However, depending on the specific optimization problem under consideration, the type and the parameters for the averaging need to be adjusted to achieve the smallest optimization error. In this work we propose an averaging approach, which we refer to as parallel averaged ADAM (PADAM), in which we compute parallely different averaged variants of ADAM and during the training process dynamically select the variant with the smallest optimization error. A central feature of this approach is that this procedure requires no more gradient evaluations than the usual ADAM optimizer as each of the averaged trajectories relies on the same underlying ADAM trajectory and thus on the same underlying gradients. We test the proposed PADAM optimizer in 13 stochastic optimization and deep neural network (DNN) learning problems and compare its performance with known optimizers from the literature such as standard SGD, momentum SGD, Adam with and without EMA, and ADAMW. In particular, we apply the compared optimizers to physics-informed neural network, deep Galerkin, deep backward stochastic differential equation and deep Kolmogorov approximations for boundary value partial differential equation problems from scientific machine learning, as well as to DNN approximations for optimal control and optimal stopping problems. In nearly all of the considered examples PADAM achieves, sometimes among others and sometimes exclusively, essentially the smallest optimization error. This work thus strongly suggest to consider PADAM for scientific machine learning problems and also motivates further research for adaptive averaging procedures within the training of DNNs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.22090</link>
<guid>https://arxiv.org/abs/2505.22090</guid>
<content:encoded><![CDATA[
arXiv:2505.22090v1 Announce Type: cross 
Abstract: Three-dimensional ultrasound enables real-time volumetric visualization of anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the reliance on precise probe orientation, potentially making ultrasound more accessible to clinicians with varying levels of experience and improving automated measurements and post-exam analysis. However, achieving both high volume rates and high image quality remains a significant challenge. While 3D diverging waves can provide high volume rates, they suffer from limited tissue harmonic generation and increased multipath effects, which degrade image quality. One compromise is to retain the focusing in elevation while leveraging unfocused diverging waves in the lateral direction to reduce the number of transmissions per elevation plane. Reaching the volume rates achieved by full 3D diverging waves, however, requires dramatically undersampling the number of elevation planes. Subsequently, to render the full volume, simple interpolation techniques are applied. This paper introduces a novel approach to 3D ultrasound reconstruction from a reduced set of elevation planes by employing diffusion models (DMs) to achieve increased spatial and temporal resolution. We compare both traditional and supervised deep learning-based interpolation methods on a 3D cardiac ultrasound dataset. Our results show that DM-based reconstruction consistently outperforms the baselines in image quality and downstream task performance. Additionally, we accelerate inference by leveraging the temporal consistency inherent to ultrasound sequences. Finally, we explore the robustness of the proposed method by exploiting the probabilistic nature of diffusion posterior sampling to quantify reconstruction uncertainty and demonstrate improved recall on out-of-distribution data with synthetic anomalies under strong subsampling.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.22094</link>
<guid>https://arxiv.org/abs/2505.22094</guid>
<content:encoded><![CDATA[
arXiv:2505.22094v1 Announce Type: cross 
Abstract: We propose ReinFlow, a simple yet effective online reinforcement learning (RL) framework that fine-tunes a family of flow matching policies for continuous robotic control. Derived from rigorous RL theory, ReinFlow injects learnable noise into a flow policy's deterministic path, converting the flow into a discrete-time Markov Process for exact and straightforward likelihood computation. This conversion facilitates exploration and ensures training stability, enabling ReinFlow to fine-tune diverse flow model variants, including Rectified Flow [35] and Shortcut Models [19], particularly at very few or even one denoising step. We benchmark ReinFlow in representative locomotion and manipulation tasks, including long-horizon planning with visual input and sparse reward. The episode reward of Rectified Flow policies obtained an average net growth of 135.36% after fine-tuning in challenging legged locomotion tasks while saving denoising steps and 82.63% of wall time compared to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate of the Shortcut Model policies in state and visual manipulation tasks achieved an average net increase of 40.34% after fine-tuning with ReinFlow at four or even one denoising step, whose performance is comparable to fine-tuned DDIM policies while saving computation time for an average of 23.20%. Project Webpage: https://reinflow.github.io/
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Base Construction for Knowledge-Augmented Text-to-SQL</title>
<link>https://arxiv.org/abs/2505.22096</link>
<guid>https://arxiv.org/abs/2505.22096</guid>
<content:encoded><![CDATA[
arXiv:2505.22096v1 Announce Type: cross 
Abstract: Text-to-SQL aims to translate natural language queries into SQL statements, which is practical as it enables anyone to easily retrieve the desired information from databases. Recently, many existing approaches tackle this problem with Large Language Models (LLMs), leveraging their strong capability in understanding user queries and generating corresponding SQL code. Yet, the parametric knowledge in LLMs might be limited to covering all the diverse and domain-specific queries that require grounding in various database schemas, which makes generated SQLs less accurate oftentimes. To tackle this, we propose constructing the knowledge base for text-to-SQL, a foundational source of knowledge, from which we retrieve and generate the necessary knowledge for given queries. In particular, unlike existing approaches that either manually annotate knowledge or generate only a few pieces of knowledge for each query, our knowledge base is comprehensive, which is constructed based on a combination of all the available questions and their associated database schemas along with their relevant knowledge, and can be reused for unseen databases from different datasets and domains. We validate our approach on multiple text-to-SQL datasets, considering both the overlapping and non-overlapping database scenarios, where it outperforms relevant baselines substantially.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.22099</link>
<guid>https://arxiv.org/abs/2505.22099</guid>
<content:encoded><![CDATA[
arXiv:2505.22099v1 Announce Type: cross 
Abstract: In this paper, we addressed the limitation of relying solely on distribution alignment and source-domain empirical risk minimization in Unsupervised Domain Adaptation (UDA). Our information-theoretic analysis showed that this standard adversarial-based framework neglects the discriminability of target-domain features, leading to suboptimal performance. To bridge this theoretical-practical gap, we defined "good representation learning" as guaranteeing both transferability and discriminability, and proved that an additional loss term targeting target-domain discriminability is necessary. Building on these insights, we proposed a novel adversarial-based UDA framework that explicitly integrates a domain alignment objective with a discriminability-enhancing constraint. Instantiated as Domain-Invariant Representation Learning with Global and Local Consistency (RLGLC), our method leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD) to address class imbalance and semantic dimension weighting, and employs a local consistency mechanism to preserve fine-grained target-domain discriminative information. Extensive experiments across multiple benchmark datasets demonstrate that RLGLC consistently surpasses state-of-the-art methods, confirming the value of our theoretical perspective and underscoring the necessity of enforcing both transferability and discriminability in adversarial-based UDA.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Dynamic Shielding for Parametric Safety Specifications</title>
<link>https://arxiv.org/abs/2505.22104</link>
<guid>https://arxiv.org/abs/2505.22104</guid>
<content:encoded><![CDATA[
arXiv:2505.22104v1 Announce Type: cross 
Abstract: Shielding has emerged as a promising approach for ensuring safety of AI-controlled autonomous systems. The algorithmic goal is to compute a shield, which is a runtime safety enforcement tool that needs to monitor and intervene the AI controller's actions if safety could be compromised otherwise. Traditional shields are designed statically for a specific safety requirement. Therefore, if the safety requirement changes at runtime due to changing operating conditions, the shield needs to be recomputed from scratch, causing delays that could be fatal. We introduce dynamic shields for parametric safety specifications, which are succinctly represented sets of all possible safety specifications that may be encountered at runtime. Our dynamic shields are statically designed for a given safety parameter set, and are able to dynamically adapt as the true safety specification (permissible by the parameters) is revealed at runtime. The main algorithmic novelty lies in the dynamic adaptation procedure, which is a simple and fast algorithm that utilizes known features of standard safety shields, like maximal permissiveness. We report experimental results for a robot navigation problem in unknown territories, where the safety specification evolves as new obstacles are discovered at runtime. In our experiments, the dynamic shields took a few minutes for their offline design, and took between a fraction of a second and a few seconds for online adaptation at each step, whereas the brute-force online recomputation approach was up to 5 times slower.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curse of High Dimensionality Issue in Transformer for Long-context Modeling</title>
<link>https://arxiv.org/abs/2505.22107</link>
<guid>https://arxiv.org/abs/2505.22107</guid>
<content:encoded><![CDATA[
arXiv:2505.22107v1 Announce Type: cross 
Abstract: Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at https://github.com/bolixinyu/DynamicGroupAttention.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.22135</link>
<guid>https://arxiv.org/abs/2505.22135</guid>
<content:encoded><![CDATA[
arXiv:2505.22135v1 Announce Type: cross 
Abstract: Hybrid models combining Transformers and State Space Models (SSMs) are promising for balancing performance and efficiency. However, optimizing these hybrid models, particularly by addressing the potential redundancy inherent within the Transformer components, remains a significant challenge. In this paper, we propose RAD (Redundancy-Aware Distillation), a novel framework that uses self-speculative decoding as a diagnostic tool to identify redundant attention layers within the model. These identified layers are then selectively replaced with SSM components, followed by targeted (self-)distillation. Specifically, RAD focuses knowledge transfer on the components identified as redundant, considering architectural changes and specific weight initialization strategies. We experimentally demonstrate that self-distillation using RAD significantly surpasses the performance of the original base model on mathematical and coding tasks. Furthermore, RAD is also effective in standard knowledge distillation settings, achieving up to approximately 2x faster convergence compared to baseline methods. Notably, while a baseline model distilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and 22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and 28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers a new pathway for efficient optimization and performance enhancement in the distillation of hybrid models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments</title>
<link>https://arxiv.org/abs/2505.22137</link>
<guid>https://arxiv.org/abs/2505.22137</guid>
<content:encoded><![CDATA[
arXiv:2505.22137v1 Announce Type: cross 
Abstract: Identifying arguments is a necessary prerequisite for various tasks in automated discourse analysis, particularly within contexts such as political debates, online discussions, and scientific reasoning. In addition to theoretical advances in understanding the constitution of arguments, a significant body of research has emerged around practical argument mining, supported by a growing number of publicly available datasets. On these benchmarks, BERT-like transformers have consistently performed best, reinforcing the belief that such models are broadly applicable across diverse contexts of debate. This study offers the first large-scale re-evaluation of such state-of-the-art models, with a specific focus on their ability to generalize in identifying arguments. We evaluate four transformers, three standard and one enhanced with contrastive pre-training for better generalization, on 17 English sentence-level datasets as most relevant to the task. Our findings show that, to varying degrees, these models tend to rely on lexical shortcuts tied to content words, suggesting that apparent progress may often be driven by dataset-specific cues rather than true task alignment. While the models achieve strong results on familiar benchmarks, their performance drops markedly when applied to unseen datasets. Nonetheless, incorporating both task-specific pre-training and joint benchmark training proves effective in enhancing both robustness and generalization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes</title>
<link>https://arxiv.org/abs/2505.22165</link>
<guid>https://arxiv.org/abs/2505.22165</guid>
<content:encoded><![CDATA[
arXiv:2505.22165v1 Announce Type: cross 
Abstract: Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to continuous spaces and apply fine-grained noise, but the diffusion progress is uniform across tokens, limiting their ability to capture semantic nuances. To address these limitations, we propose \textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models (NeoDiff), a novel diffusion model that integrates the strengths of both discrete and continuous approaches. NeoDiff introduces a Poisson diffusion process for the forward process, enabling a flexible and fine-grained noising paradigm, and employs a time predictor for the reverse process to adaptively modulate the denoising progress based on token semantics. Furthermore, NeoDiff utilizes an optimized schedule for inference to ensure more precise noise control and improved performance. Our approach unifies the theories of discrete and continuous diffusion models, offering a more principled and effective framework for text generation. Experimental results on several text generation tasks demonstrate NeoDiff's superior performance compared to baselines of non-autoregressive continuous and discrete diffusion models, iterative-based methods and autoregressive diffusion-based methods. These results highlight NeoDiff's potential as a powerful tool for generating high-quality text and advancing the field of diffusion-based text generation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design</title>
<link>https://arxiv.org/abs/2505.22179</link>
<guid>https://arxiv.org/abs/2505.22179</guid>
<content:encoded><![CDATA[
arXiv:2505.22179v1 Announce Type: cross 
Abstract: Speculative decoding and quantization effectively accelerate memory-bound inference of large language models. Speculative decoding mitigates the memory bandwidth bottleneck by verifying multiple tokens within a single forward pass, which increases computational effort. Quantization achieves this optimization by compressing weights and activations into lower bit-widths and also reduces computations via low-bit matrix multiplications. To further leverage their strengths, we investigate the integration of these two techniques. Surprisingly, experiments applying the advanced speculative decoding method EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit weight quantization are diminished by the computational load from speculative decoding. Specifically, verifying a tree-style draft incurs significantly more time overhead than a single-token forward pass on 4-bit weight quantized models. This finding led to our new speculative decoding design: a hierarchical framework that employs a small model as an intermediate stage to turn tree-style drafts into sequence drafts, leveraging the memory access benefits of the target quantized model. Experimental results show that our hierarchical approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$. Code available at https://github.com/AI9Stars/SpecMQuant.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion</title>
<link>https://arxiv.org/abs/2505.22193</link>
<guid>https://arxiv.org/abs/2505.22193</guid>
<content:encoded><![CDATA[
arXiv:2505.22193v1 Announce Type: cross 
Abstract: Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI that aims to use quantum properties to improve the performances of their classical counterparts. However, existing algorithms are not easily scalable due to the limitations of near-term quantum devices. Following our previous work on QDMs, here we propose and implement two physics-inspired protocols. In the first, we use the formalism of quantum stochastic walks, showing that a specific interplay of quantum and classical dynamics in the forward process produces statistically more robust models generating sets of MNIST images with lower Fr\'echet Inception Distance (FID) than using totally classical dynamics. In the second approach, we realize an algorithm to generate images by exploiting the intrinsic noise of real IBM quantum hardware with only four qubits. Our work could be a starting point to pave the way for new scenarios for large-scale algorithms in quantum Generative AI, where quantum noise is neither mitigated nor corrected, but instead exploited as a useful resource.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models</title>
<link>https://arxiv.org/abs/2505.22232</link>
<guid>https://arxiv.org/abs/2505.22232</guid>
<content:encoded><![CDATA[
arXiv:2505.22232v1 Announce Type: cross 
Abstract: High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval</title>
<link>https://arxiv.org/abs/2505.22238</link>
<guid>https://arxiv.org/abs/2505.22238</guid>
<content:encoded><![CDATA[
arXiv:2505.22238v1 Announce Type: cross 
Abstract: We present Yambda-5B, a large-scale open dataset sourced from the Yandex.Music streaming platform. Yambda-5B contains 4.79 billion user-item interactions from 1 million users across 9.39 million tracks. The dataset includes two primary types of interactions: implicit feedback (listening events) and explicit feedback (likes, dislikes, unlikes and undislikes). In addition, we provide audio embeddings for most tracks, generated by a convolutional neural network trained on audio spectrograms. A key distinguishing feature of Yambda-5B is the inclusion of the is_organic flag, which separates organic user actions from recommendation-driven events. This distinction is critical for developing and evaluating machine learning algorithms, as Yandex.Music relies on recommender systems to personalize track selection for users. To support rigorous benchmarking, we introduce an evaluation protocol based on a Global Temporal Split, allowing recommendation algorithms to be assessed in conditions that closely mirror real-world use. We report benchmark results for standard baselines (ItemKNN, iALS) and advanced models (SANSA, SASRec) using a variety of evaluation metrics. By releasing Yambda-5B to the community, we aim to provide a readily accessible, industrial-scale resource to advance research, foster innovation, and promote reproducible results in recommender systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UDuo: Universal Dual Optimization Framework for Online Matching</title>
<link>https://arxiv.org/abs/2505.22243</link>
<guid>https://arxiv.org/abs/2505.22243</guid>
<content:encoded><![CDATA[
arXiv:2505.22243v1 Announce Type: cross 
Abstract: Online resource allocation under budget constraints critically depends on proper modeling of user arrival dynamics. Classical approaches employ stochastic user arrival models to derive near-optimal solutions through fractional matching formulations of exposed users for downstream allocation tasks. However, this is no longer a reasonable assumption when the environment changes dynamically. In this work, We propose the Universal Dual optimization framework UDuo, a novel paradigm that fundamentally rethinks online allocation through three key innovations: (i) a temporal user arrival representation vector that explicitly captures distribution shifts in user arrival patterns and resource consumption dynamics, (ii) a resource pacing learner with adaptive allocation policies that generalize to heterogeneous constraint scenarios, and (iii) an online time-series forecasting approach for future user arrival distributions that achieves asymptotically optimal solutions with constraint feasibility guarantees in dynamic environments. Experimental results show that UDuo achieves higher efficiency and faster convergence than the traditional stochastic arrival model in real-world pricing while maintaining rigorous theoretical validity for general online allocation problems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDAR Based Semantic Perception for Forklifts in Outdoor Environments</title>
<link>https://arxiv.org/abs/2505.22258</link>
<guid>https://arxiv.org/abs/2505.22258</guid>
<content:encoded><![CDATA[
arXiv:2505.22258v1 Announce Type: cross 
Abstract: In this study, we present a novel LiDAR-based semantic segmentation framework tailored for autonomous forklifts operating in complex outdoor environments. Central to our approach is the integration of a dual LiDAR system, which combines forward-facing and downward-angled LiDAR sensors to enable comprehensive scene understanding, specifically tailored for industrial material handling tasks. The dual configuration improves the detection and segmentation of dynamic and static obstacles with high spatial precision. Using high-resolution 3D point clouds captured from two sensors, our method employs a lightweight yet robust approach that segments the point clouds into safety-critical instance classes such as pedestrians, vehicles, and forklifts, as well as environmental classes such as driveable ground, lanes, and buildings. Experimental validation demonstrates that our approach achieves high segmentation accuracy while satisfying strict runtime requirements, establishing its viability for safety-aware, fully autonomous forklift navigation in dynamic warehouse and yard environments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling</title>
<link>https://arxiv.org/abs/2505.22290</link>
<guid>https://arxiv.org/abs/2505.22290</guid>
<content:encoded><![CDATA[
arXiv:2505.22290v1 Announce Type: cross 
Abstract: Recent research has highlighted that Large Language Models (LLMs), even when trained to generate extended long reasoning steps, still face significant challenges on hard reasoning problems. However, much of the existing literature relies on direct prompting with simple in-context learning examples for evaluation, which largely overlooks advanced techniques to elicit LLMs' deliberate reasoning before drawing conclusions that LLMs hit a performance ceiling. In this paper, we systematically explore the combined potential of in-context search and test-time scaling on super hard reasoning tasks. We find that by employing advanced in-context search prompting to LLMs augmented with internal scaling, one can achieve transformative performance breakthroughs on tasks previously deemed "unsolvable" (e.g., reported success rates below 5%). We provide both empirical results and theoretical analysis of how this combination can unleash LLM reasoning capabilities: i) Empirically, on controlled NP-hard tasks and complex real-world planning benchmarks, our approach achieves up to a 30x improvement in success rates compared to previously reported results without any external mechanisms; ii) Theoretically, we show that in-context search prompting, when combined with internal scaling, significantly extends the complexity class of solvable reasoning problems. These findings challenge prevailing assumptions about the limitations of LLMs on complex tasks, indicating that current evaluation paradigms systematically underestimate their true potential. Our work calls for a critical reassessment of how LLM reasoning is benchmarked and a more robust evaluation strategy that fully captures the true capabilities of contemporary LLMs, which can lead to a better understanding of their operational reasoning boundaries in real-world deployments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>360-LLaMA-Factory: Plug &amp; Play Sequence Parallelism for Long Post-Training</title>
<link>https://arxiv.org/abs/2505.22296</link>
<guid>https://arxiv.org/abs/2505.22296</guid>
<content:encoded><![CDATA[
arXiv:2505.22296v1 Announce Type: cross 
Abstract: Adding sequence parallelism into LLaMA-Factory, we open-sourced 360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory. 360-LLaMA-Factory has received wide recognition and used in models such as Light-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and also in large companies' training frameworks. This technical report delves deeper into the different sequence parallel modes behind 360-LLaMA-Factory and discusses our implementation insights.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?</title>
<link>https://arxiv.org/abs/2505.22318</link>
<guid>https://arxiv.org/abs/2505.22318</guid>
<content:encoded><![CDATA[
arXiv:2505.22318v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate impressive reasoning capabilities in familiar contexts, but struggle when the context conflicts with their parametric knowledge. To investigate this phenomenon, we introduce CounterLogic, a dataset containing 1,800 examples across 9 logical schemas, explicitly designed to evaluate logical reasoning through counterfactual (hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11 LLMs across 6 different datasets reveals a consistent performance degradation, with accuracies dropping by 27% on average when reasoning through counterfactual information. We propose Self-Segregate, a prompting method enabling metacognitive awareness (explicitly identifying knowledge conflicts) before reasoning. Our method dramatically narrows the average performance gaps from 27% to just 11%, while significantly increasing the overall accuracy (+7.5%). We discuss the implications of these findings and draw parallels to human cognitive processes, particularly on how humans disambiguate conflicting information during reasoning tasks. Our findings offer practical insights for understanding and enhancing LLMs reasoning capabilities in real-world applications, especially where models must logically reason independently of their factual knowledge.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Individualised Counterfactual Examples Using Conformal Prediction Intervals</title>
<link>https://arxiv.org/abs/2505.22326</link>
<guid>https://arxiv.org/abs/2505.22326</guid>
<content:encoded><![CDATA[
arXiv:2505.22326v1 Announce Type: cross 
Abstract: Counterfactual explanations for black-box models aim to pr ovide insight into an algorithmic decision to its recipient. For a binary classification problem an individual counterfactual details which features might be changed for the model to infer the opposite class. High-dimensional feature spaces that are typical of machine learning classification models admit many possible counterfactual examples to a decision, and so it is important to identify additional criteria to select the most useful counterfactuals. In this paper, we explore the idea that the counterfactuals should be maximally informative when considering the knowledge of a specific individual about the underlying classifier. To quantify this information gain we explicitly model the knowledge of the individual, and assess the uncertainty of predictions which the individual makes by the width of a conformal prediction interval. Regions of feature space where the prediction interval is wide correspond to areas where the confidence in decision making is low, and an additional counterfactual example might be more informative to an individual. To explore and evaluate our individualised conformal prediction interval counterfactuals (CPICFs), first we present a synthetic data set on a hypercube which allows us to fully visualise the decision boundary, conformal intervals via three different methods, and resultant CPICFs. Second, in this synthetic data set we explore the impact of a single CPICF on the knowledge of an individual locally around the original query. Finally, in both our synthetic data set and a complex real world dataset with a combination of continuous and discrete variables, we measure the utility of these counterfactuals via data augmentation, testing the performance on a held out set.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credal Prediction based on Relative Likelihood</title>
<link>https://arxiv.org/abs/2505.22332</link>
<guid>https://arxiv.org/abs/2505.22332</guid>
<content:encoded><![CDATA[
arXiv:2505.22332v1 Announce Type: cross 
Abstract: Predictions in the form of sets of probability distributions, so-called credal sets, provide a suitable means to represent a learner's epistemic uncertainty. In this paper, we propose a theoretically grounded approach to credal prediction based on the statistical notion of relative likelihood: The target of prediction is the set of all (conditional) probability distributions produced by the collection of plausible models, namely those models whose relative likelihood exceeds a specified threshold. This threshold has an intuitive interpretation and allows for controlling the trade-off between correctness and precision of credal predictions. We tackle the problem of approximating credal sets defined in this way by means of suitably modified ensemble learning techniques. To validate our approach, we illustrate its effectiveness by experiments on benchmark datasets demonstrating superior uncertainty representation without compromising predictive performance. We also compare our method against several state-of-the-art baselines in credal prediction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start</title>
<link>https://arxiv.org/abs/2505.22334</link>
<guid>https://arxiv.org/abs/2505.22334</guid>
<content:encoded><![CDATA[
arXiv:2505.22334v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While "aha moment" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at https://github.com/waltonfuture/RL-with-Cold-Start.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training</title>
<link>https://arxiv.org/abs/2505.22342</link>
<guid>https://arxiv.org/abs/2505.22342</guid>
<content:encoded><![CDATA[
arXiv:2505.22342v1 Announce Type: cross 
Abstract: The success of the machine learning field has reliably depended on training on large datasets. While effective, this trend comes at an extraordinary cost. This is due to two deeply intertwined factors: the size of models and the size of datasets. While promising research efforts focus on reducing the size of models, the other half of the equation remains fairly mysterious. Indeed, it is surprising that the standard approach to training remains to iterate over and over, uniformly sampling the training dataset. In this paper we explore a series of alternative training paradigms that leverage insights from hard-data-mining and dropout, simple enough to implement and use that can become the new training standard. The proposed Progressive Data Dropout reduces the number of effective epochs to as little as 12.4% of the baseline. This savings actually do not come at any cost for accuracy. Surprisingly, the proposed method improves accuracy by up to 4.82%. Our approach requires no changes to model architecture or optimizer, and can be applied across standard training pipelines, thus posing an excellent opportunity for wide adoption. Code can be found here: https://github.com/bazyagami/LearningWithRevision
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows</title>
<link>https://arxiv.org/abs/2505.22364</link>
<guid>https://arxiv.org/abs/2505.22364</guid>
<content:encoded><![CDATA[
arXiv:2505.22364v1 Announce Type: cross 
Abstract: We present a novel method for efficiently computing optimal transport maps and Wasserstein barycenters in high-dimensional spaces. Our approach uses conditional normalizing flows to approximate the input distributions as invertible pushforward transformations from a common latent space. This makes it possible to directly solve the primal problem using gradient-based minimization of the transport cost, unlike previous methods that rely on dual formulations and complex adversarial optimization. We show how this approach can be extended to compute Wasserstein barycenters by solving a conditional variance minimization problem. A key advantage of our conditional architecture is that it enables the computation of barycenters for hundreds of input distributions, which was computationally infeasible with previous methods. Our numerical experiments illustrate that our approach yields accurate results across various high-dimensional tasks and compares favorably with previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAM: Domain-Aware Module for Multi-Domain Dataset Condensation</title>
<link>https://arxiv.org/abs/2505.22387</link>
<guid>https://arxiv.org/abs/2505.22387</guid>
<content:encoded><![CDATA[
arXiv:2505.22387v1 Announce Type: cross 
Abstract: Dataset Condensation (DC) has emerged as a promising solution to mitigate the computational and storage burdens associated with training deep learning models. However, existing DC methods largely overlook the multi-domain nature of modern datasets, which are increasingly composed of heterogeneous images spanning multiple domains. In this paper, we extend DC and introduce Multi-Domain Dataset Condensation (MDDC), which aims to condense data that generalizes across both single-domain and multi-domain settings. To this end, we propose the Domain-Aware Module (DAM), a training-time module that embeds domain-related features into each synthetic image via learnable spatial masks. As explicit domain labels are mostly unavailable in real-world datasets, we employ frequency-based pseudo-domain labeling, which leverages low-frequency amplitude statistics. DAM is only active during the condensation process, thus preserving the same images per class (IPC) with prior methods. Experiments show that DAM consistently improves in-domain, out-of-domain, and cross-architecture performance over baseline dataset condensation methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synonymous Variational Inference for Perceptual Image Compression</title>
<link>https://arxiv.org/abs/2505.22438</link>
<guid>https://arxiv.org/abs/2505.22438</guid>
<content:encoded><![CDATA[
arXiv:2505.22438v1 Announce Type: cross 
Abstract: Recent contributions of semantic information theory reveal the set-element relationship between semantic and syntactic information, represented as synonymous relationships. In this paper, we propose a synonymous variational inference (SVI) method based on this synonymity viewpoint to re-analyze the perceptual image compression problem. It takes perceptual similarity as a typical synonymous criterion to build an ideal synonymous set (Synset), and approximate the posterior of its latent synonymous representation with a parametric density by minimizing a partial semantic KL divergence. This analysis theoretically proves that the optimization direction of perception image compression follows a triple tradeoff that can cover the existing rate-distortion-perception schemes. Additionally, we introduce synonymous image compression (SIC), a new image compression scheme that corresponds to the analytical process of SVI, and implement a progressive SIC codec to fully leverage the model's capabilities. Experimental results demonstrate comparable rate-distortion-perception performance using a single progressive SIC codec, thus verifying the effectiveness of our proposed analysis method.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO</title>
<link>https://arxiv.org/abs/2505.22453</link>
<guid>https://arxiv.org/abs/2505.22453</guid>
<content:encoded><![CDATA[
arXiv:2505.22453v1 Announce Type: cross 
Abstract: Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, a stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9 %$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at https://github.com/waltonfuture/MM-UPT.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Based Matrix Classification for the HHL Quantum Algorithm</title>
<link>https://arxiv.org/abs/2505.22454</link>
<guid>https://arxiv.org/abs/2505.22454</guid>
<content:encoded><![CDATA[
arXiv:2505.22454v1 Announce Type: cross 
Abstract: Under the nearing error-corrected era of quantum computing, it is necessary to understand the suitability of certain post-NISQ algorithms for practical problems. One of the most promising, applicable and yet difficult to implement in practical terms is the Harrow, Hassidim and Lloyd (HHL) algorithm for linear systems of equations. An enormous number of problems can be expressed as linear systems of equations, from Machine Learning to fluid dynamics. However, in most cases, HHL will not be able to provide a practical, reasonable solution to these problems. This paper's goal inquires about whether problems can be labeled using Machine Learning classifiers as suitable or unsuitable for HHL implementation when some numerical information about the problem is known beforehand. This work demonstrates that training on significantly representative data distributions is critical to achieve good classifications of the problems based on the numerical properties of the matrix representing the system of equations. Accurate classification is possible through Multi-Layer Perceptrons, although with careful design of the training data distribution and classifier parameters.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.22467</link>
<guid>https://arxiv.org/abs/2505.22467</guid>
<content:encoded><![CDATA[
arXiv:2505.22467v1 Announce Type: cross 
Abstract: Large Language Model-based Multi-Agent Systems (MASs) have emerged as a powerful paradigm for tackling complex tasks through collaborative intelligence. Nevertheless, the question of how agents should be structurally organized for optimal cooperation remains largely unexplored. In this position paper, we aim to gently redirect the focus of the MAS research community toward this critical dimension: develop topology-aware MASs for specific tasks. Specifically, the system consists of three core components - agents, communication links, and communication patterns - that collectively shape its coordination performance and efficiency. To this end, we introduce a systematic, three-stage framework: agent selection, structure profiling, and topology synthesis. Each stage would trigger new research opportunities in areas such as language models, reinforcement learning, graph learning, and generative modeling; together, they could unleash the full potential of MASs in complicated real-world applications. Then, we discuss the potential challenges and opportunities in the evaluation of multiple systems. We hope our perspective and framework can offer critical new insights in the era of agentic AI.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CPINN-ABPI: Physics-Informed Neural Networks for Accurate Power Estimation in MPSoCs</title>
<link>https://arxiv.org/abs/2505.22469</link>
<guid>https://arxiv.org/abs/2505.22469</guid>
<content:encoded><![CDATA[
arXiv:2505.22469v1 Announce Type: cross 
Abstract: Efficient thermal and power management in modern multiprocessor systems-on-chip (MPSoCs) demands accurate power consumption estimation. One of the state-of-the-art approaches, Alternative Blind Power Identification (ABPI), theoretically eliminates the dependence on steady-state temperatures, addressing a major shortcoming of previous approaches. However, ABPI performance has remained unverified in actual hardware implementations. In this study, we conduct the first empirical validation of ABPI on commercial hardware using the NVIDIA Jetson Xavier AGX platform. Our findings reveal that, while ABPI provides computational efficiency and independence from steady-state temperature, it exhibits considerable accuracy deficiencies in real-world scenarios. To overcome these limitations, we introduce a novel approach that integrates Custom Physics-Informed Neural Networks (CPINNs) with the underlying thermal model of ABPI. Our approach employs a specialized loss function that harmonizes physical principles with data-driven learning, complemented by multi-objective genetic algorithm optimization to balance estimation accuracy and computational cost. In experimental validation, CPINN-ABPI achieves a reduction of 84.7\% CPU and 73.9\% GPU in the mean absolute error (MAE) relative to ABPI, with the weighted mean absolute percentage error (WMAPE) improving from 47\%--81\% to $\sim$12\%. The method maintains real-time performance with 195.3~$\mu$s of inference time, with similar 85\%--99\% accuracy gains across heterogeneous SoCs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypothesis Testing in Imaging Inverse Problems</title>
<link>https://arxiv.org/abs/2505.22481</link>
<guid>https://arxiv.org/abs/2505.22481</guid>
<content:encoded><![CDATA[
arXiv:2505.22481v1 Announce Type: cross 
Abstract: This paper proposes a framework for semantic hypothesis testing tailored to imaging inverse problems. Modern imaging methods struggle to support hypothesis testing, a core component of the scientific method that is essential for the rigorous interpretation of experiments and robust interfacing with decision-making processes. There are three main reasons why image-based hypothesis testing is challenging. First, the difficulty of using a single observation to simultaneously reconstruct an image, formulate hypotheses, and quantify their statistical significance. Second, the hypotheses encountered in imaging are mostly of semantic nature, rather than quantitative statements about pixel values. Third, it is challenging to control test error probabilities because the null and alternative distributions are often unknown. Our proposed approach addresses these difficulties by leveraging concepts from self-supervised computational imaging, vision-language models, and non-parametric hypothesis testing with e-values. We demonstrate our proposed framework through numerical experiments related to image-based phenotyping, where we achieve excellent power while robustly controlling Type I errors.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Quantum Advantage for Gaussian Process Regression</title>
<link>https://arxiv.org/abs/2505.22502</link>
<guid>https://arxiv.org/abs/2505.22502</guid>
<content:encoded><![CDATA[
arXiv:2505.22502v1 Announce Type: cross 
Abstract: Gaussian Process Regression is a well-known machine learning technique for which several quantum algorithms have been proposed. We show here that in a wide range of scenarios these algorithms show no exponential speedup. We achieve this by rigorously proving that the condition number of a kernel matrix scales at least linearly with the matrix size under general assumptions on the data and kernel. We additionally prove that the sparsity and Frobenius norm of a kernel matrix scale linearly under similar assumptions. The implications for the quantum algorithms runtime are independent of the complexity of loading classical data on a quantum computer and also apply to dequantised algorithms. We supplement our theoretical analysis with numerical verification for popular kernels in machine learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas</title>
<link>https://arxiv.org/abs/2505.22518</link>
<guid>https://arxiv.org/abs/2505.22518</guid>
<content:encoded><![CDATA[
arXiv:2505.22518v1 Announce Type: cross 
Abstract: Parameter estimation for Archimedean copulas remains a challenging problem, particularly for the recently developed A1 and A2 families that exhibit complex dependency structures. Traditional methods, such as the Method of Moments (MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood (MPL), often struggle due to issues of non-monotonic relationship with dependency measures such as Kendall's tau (as in the case of A1) and numerical instability. In this paper, we present the IGNIS Network, a novel, unified neural framework that learns a direct mapping from observable dependency measures to copula parameters, thereby overcoming the limitations of classical approaches. Our approach is trained on simulated data spanning five Archimedean copula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its general applicability across the entire family. Extensive simulation studies demonstrate that the IGNIS Network reduces estimation errors compared to MoM, while inherently enforcing parameter constraints through theory-guided post-processing. We further validate the practical utility of our method on diverse real-world datasets, including financial returns (AAPL-MSFT), healthcare metrics (CDC Diabetes indicators), and environmental measurements (PM2.5 air quality). Our results underscore the transformative potential of neural methods for robust and accurate dependence modeling in modern applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling</title>
<link>https://arxiv.org/abs/2505.22527</link>
<guid>https://arxiv.org/abs/2505.22527</guid>
<content:encoded><![CDATA[
arXiv:2505.22527v1 Announce Type: cross 
Abstract: We introduce the Symplectic Generative Network (SGN), a deep generative model that leverages Hamiltonian mechanics to construct an invertible, volume-preserving mapping between a latent space and the data space. By endowing the latent space with a symplectic structure and modeling data generation as the time evolution of a Hamiltonian system, SGN achieves exact likelihood evaluation without incurring the computational overhead of Jacobian determinant calculations. In this work, we provide a rigorous mathematical foundation for SGNs through a comprehensive theoretical framework that includes: (i) complete proofs of invertibility and volume preservation, (ii) a formal complexity analysis with theoretical comparisons to Variational Autoencoders and Normalizing Flows, (iii) strengthened universal approximation results with quantitative error bounds, (iv) an information-theoretic analysis based on the geometry of statistical manifolds, and (v) an extensive stability analysis with adaptive integration guarantees. These contributions highlight the fundamental advantages of SGNs and establish a solid foundation for future empirical investigations and applications to complex, high-dimensional data.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting</title>
<link>https://arxiv.org/abs/2505.22535</link>
<guid>https://arxiv.org/abs/2505.22535</guid>
<content:encoded><![CDATA[
arXiv:2505.22535v1 Announce Type: cross 
Abstract: Recent deep learning approaches for river discharge forecasting have improved the accuracy and efficiency in flood forecasting, enabling more reliable early warning systems for risk management. Nevertheless, existing deep learning approaches in hydrology remain largely confined to local-scale applications and do not leverage the inherent spatial connections of bodies of water. Thus, there is a strong need for new deep learning methodologies that are capable of modeling spatio-temporal relations to improve river discharge and flood forecasting for scientific and operational applications. To address this, we present RiverMamba, a novel deep learning model that is pretrained with long-term reanalysis data and that can forecast global river discharge and floods on a $0.05^\circ$ grid up to 7 days lead time, which is of high relevance in early warning. To achieve this, RiverMamba leverages efficient Mamba blocks that enable the model to capture global-scale channel network routing and enhance its forecast capability for longer lead times. The forecast blocks integrate ECMWF HRES meteorological forecasts, while accounting for their inaccuracies through spatio-temporal modeling. Our analysis demonstrates that RiverMamba delivers reliable predictions of river discharge, including extreme floods across return periods and lead times, surpassing both operational AI- and physics-based models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Copulas Be Used for Feature Selection? A Machine Learning Study on Diabetes Risk Prediction</title>
<link>https://arxiv.org/abs/2505.22554</link>
<guid>https://arxiv.org/abs/2505.22554</guid>
<content:encoded><![CDATA[
arXiv:2505.22554v1 Announce Type: cross 
Abstract: Accurate diabetes risk prediction relies on identifying key features from complex health datasets, but conventional methods like mutual information (MI) filters and genetic algorithms (GAs) often overlook extreme dependencies critical for high-risk subpopulations. In this study we introduce a feature-selection framework using the upper-tail dependence coefficient ({\lambda}U) of the novel A2 copula, which quantifies how often extreme higher values of a predictor co-occur with diabetes diagnoses (target variable). Applied to the CDC Diabetes Health Indicators dataset (n=253,680), our method prioritizes five predictors (self-reported general health, high blood pressure, body mass index, mobility limitations, and high cholesterol levels) based on upper tail dependencies. These features match or outperform MI and GA selected subsets across four classifiers (Random Forest, XGBoost, Logistic Regression, Gradient Boosting), achieving accuracy up to 86.5% (XGBoost) and AUC up to 0.806 (Gradient Boosting), rivaling the full 21-feature model. Permutation importance confirms clinical relevance, with BMI and general health driving accuracy. To our knowledge, this is the first work to apply a copula's upper-tail dependence for supervised feature selection, bridging extreme-value theory and machine learning to deliver a practical toolkit for diabetes prevention.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion</title>
<link>https://arxiv.org/abs/2505.22564</link>
<guid>https://arxiv.org/abs/2505.22564</guid>
<content:encoded><![CDATA[
arXiv:2505.22564v1 Announce Type: cross 
Abstract: Video dataset condensation has emerged as a critical technique for addressing the computational challenges associated with large-scale video data processing in deep learning applications. While significant progress has been made in image dataset condensation, the video domain presents unique challenges due to the complex interplay between spatial content and temporal dynamics. This paper introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for video dataset condensation, a novel approach that fundamentally reconsiders how video data should be condensed. Unlike the previous method that separates static content from dynamic motion, our method preserves the essential interdependence between these elements. Our approach progressively refines and inserts frames to fully accommodate the motion in an action while achieving better performance but less storage, considering the relation of gradients for each frame. Extensive experiments across standard video action recognition benchmarks demonstrate that PRISM outperforms existing disentangled approaches while maintaining compact representations suitable for resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.22591</link>
<guid>https://arxiv.org/abs/2505.22591</guid>
<content:encoded><![CDATA[
arXiv:2505.22591v1 Announce Type: cross 
Abstract: Although large language models demonstrate strong performance across various domains, they still struggle with numerous bad cases in mathematical reasoning. Previous approaches to learning from errors synthesize training data by solely extrapolating from isolated bad cases, thereby failing to generalize the extensive patterns inherent within these cases. This paper presents Self-Error-Instruct (SEI), a framework that addresses these model weaknesses and synthesizes more generalized targeted training data. Specifically, we explore a target model on two mathematical datasets, GSM8K and MATH, to pinpoint bad cases. Then, we generate error keyphrases for these cases based on the instructor model's (GPT-4o) analysis and identify error types by clustering these keyphrases. Next, we sample a few bad cases during each generation for each identified error type and input them into the instructor model, which synthesizes additional training data using a self-instruct approach. This new data is refined through a one-shot learning process to ensure that only the most effective examples are kept. Finally, we use these curated data to fine-tune the target model, iteratively repeating the process to enhance performance. We apply our framework to various models and observe improvements in their reasoning abilities across both in-domain and out-of-domain mathematics datasets. These results demonstrate the effectiveness of self-error instruction in improving LLMs' mathematical reasoning through error generalization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym</title>
<link>https://arxiv.org/abs/2505.22597</link>
<guid>https://arxiv.org/abs/2505.22597</guid>
<content:encoded><![CDATA[
arXiv:2505.22597v1 Announce Type: cross 
Abstract: In recent years, reinforcement learning (RL) methods have been widely tested using tools like OpenAI Gym, though many tasks in these environments could also benefit from hierarchical planning. However, there is a lack of a tool that enables seamless integration of hierarchical planning with RL. Hierarchical Domain Definition Language (HDDL), used in classical planning, introduces a structured approach well-suited for model-based RL to address this gap. To bridge this integration, we introduce HDDLGym, a Python-based tool that automatically generates OpenAI Gym environments from HDDL domains and problems. HDDLGym serves as a link between RL and hierarchical planning, supporting multi-agent scenarios and enabling collaborative planning among agents. This paper provides an overview of HDDLGym's design and implementation, highlighting the challenges and design choices involved in integrating HDDL with the Gym interface, and applying RL policies to support hierarchical planning. We also provide detailed instructions and demonstrations for using the HDDLGym framework, including how to work with existing HDDL domains and problems from International Planning Competitions, exemplified by the Transport domain. Additionally, we offer guidance on creating new HDDL domains for multi-agent scenarios and demonstrate the practical use of HDDLGym in the Overcooked domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a valuable tool for studying RL in hierarchical planning, particularly in multi-agent contexts.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the performance of machine-learning assisted Monte Carlo in sampling from simple statistical physics models</title>
<link>https://arxiv.org/abs/2505.22598</link>
<guid>https://arxiv.org/abs/2505.22598</guid>
<content:encoded><![CDATA[
arXiv:2505.22598v1 Announce Type: cross 
Abstract: Recent years have seen a rise in the application of machine learning techniques to aid the simulation of hard-to-sample systems that cannot be studied using traditional methods. Despite the introduction of many different architectures and procedures, a wide theoretical understanding is still lacking, with the risk of suboptimal implementations. As a first step to address this gap, we provide here a complete analytic study of the widely-used Sequential Tempering procedure applied to a shallow MADE architecture for the Curie-Weiss model. The contribution of this work is twofold: firstly, we give a description of the optimal weights and of the training under Gradient Descent optimization. Secondly, we compare what happens in Sequential Tempering with and without the addition of local Metropolis Monte Carlo steps. We are thus able to give theoretical predictions on the best procedure to apply in this case. This work establishes a clear theoretical basis for the integration of machine learning techniques into Monte Carlo sampling and optimization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method</title>
<link>https://arxiv.org/abs/2505.22609</link>
<guid>https://arxiv.org/abs/2505.22609</guid>
<content:encoded><![CDATA[
arXiv:2505.22609v1 Announce Type: cross 
Abstract: In this work, we investigate the performance across multiple classification models to classify chest X-ray images into four categories of COVID-19, pneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning techniques with state-of-the-art pre-trained Convolutional Neural Networks (CNNs) models. We fine-tuned these pre-trained architectures on a labeled medical x-ray images. The initial results are promising with high accuracy and strong performance in key classification metrics such as precision, recall, and F1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for model interpretability to provide visual explanations for classification decisions, improving trust and transparency in clinical applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Out-of-Distribution Generalization via Simplicity</title>
<link>https://arxiv.org/abs/2505.22622</link>
<guid>https://arxiv.org/abs/2505.22622</guid>
<content:encoded><![CDATA[
arXiv:2505.22622v1 Announce Type: cross 
Abstract: Modern foundation models exhibit remarkable out-of-distribution (OOD) generalization, solving tasks far beyond the support of their training data. However, the theoretical principles underpinning this phenomenon remain elusive. This paper investigates this problem by examining the compositional generalization abilities of diffusion models in image generation. Our analysis reveals that while neural network architectures are expressive enough to represent a wide range of models -- including many with undesirable behavior on OOD inputs -- the true, generalizable model that aligns with human expectations typically corresponds to the simplest among those consistent with the training data.
  Motivated by this observation, we develop a theoretical framework for OOD generalization via simplicity, quantified using a predefined simplicity metric. We analyze two key regimes: (1) the constant-gap setting, where the true model is strictly simpler than all spurious alternatives by a fixed gap, and (2) the vanishing-gap setting, where the fixed gap is replaced by a smoothness condition ensuring that models close in simplicity to the true model yield similar predictions. For both regimes, we study the regularized maximum likelihood estimator and establish the first sharp sample complexity guarantees for learning the true, generalizable, simple model.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning</title>
<link>https://arxiv.org/abs/2505.22626</link>
<guid>https://arxiv.org/abs/2505.22626</guid>
<content:encoded><![CDATA[
arXiv:2505.22626v1 Announce Type: cross 
Abstract: Imitation learning advances robot capabilities by enabling the acquisition of diverse behaviors from human demonstrations. However, large-scale datasets used for policy training often introduce substantial variability in quality, which can negatively impact performance. As a result, automatically curating datasets by filtering low-quality samples to improve quality becomes essential. Existing robotic curation approaches rely on costly manual annotations and perform curation at a coarse granularity, such as the dataset or trajectory level, failing to account for the quality of individual state-action pairs. To address this, we introduce SCIZOR, a self-supervised data curation framework that filters out low-quality state-action pairs to improve the performance of imitation learning policies. SCIZOR targets two complementary sources of low-quality data: suboptimal data, which hinders learning with undesirable actions, and redundant data, which dilutes training with repetitive patterns. SCIZOR leverages a self-supervised task progress predictor for suboptimal data to remove samples lacking task progression, and a deduplication module operating on joint state-action representation for samples with redundant patterns. Empirically, we show that SCIZOR enables imitation learning policies to achieve higher performance with less data, yielding an average improvement of 15.4% across multiple benchmarks. More information is available at: https://ut-austin-rpl.github.io/SCIZOR/
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Knowledge Graph-Guided Multimodal Synthesis</title>
<link>https://arxiv.org/abs/2505.22633</link>
<guid>https://arxiv.org/abs/2505.22633</guid>
<content:encoded><![CDATA[
arXiv:2505.22633v1 Announce Type: cross 
Abstract: Recent advances in multimodal large language models (MLLMs) have significantly enhanced their capabilities; however, their spatial perception abilities remain a notable limitation. To address this challenge, multimodal data synthesis offers a promising solution. Yet, ensuring that synthesized data adhere to spatial common sense is a non-trivial task. In this work, we introduce SKG2Data, a novel multimodal synthesis approach guided by spatial knowledge graphs, grounded in the concept of knowledge-to-data generation. SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate human-like perception of spatial directions and distances, which is subsequently utilized to guide multimodal data synthesis. Extensive experiments demonstrate that data synthesized from diverse types of spatial knowledge, including direction and distance, not only enhance the spatial perception and reasoning abilities of MLLMs but also exhibit strong generalization capabilities. We hope that the idea of knowledge-based data synthesis can advance the development of spatial intelligence.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes</title>
<link>https://arxiv.org/abs/2505.22638</link>
<guid>https://arxiv.org/abs/2505.22638</guid>
<content:encoded><![CDATA[
arXiv:2505.22638v1 Announce Type: cross 
Abstract: Industrial Control Systems (ICS) manage critical infrastructures like power grids and water treatment plants. Cyberattacks on ICSs can disrupt operations, causing severe economic, environmental, and safety issues. For example, undetected pollution in a water plant can put the lives of thousands at stake. ICS researchers have increasingly turned to honeypots -- decoy systems designed to attract attackers, study their behaviors, and eventually improve defensive mechanisms. However, existing ICS honeypots struggle to replicate the ICS physical process, making them susceptible to detection. Accurately simulating the noise in ICS physical processes is challenging because different factors produce it, including sensor imperfections and external interferences.
  In this paper, we propose SimProcess, a novel framework to rank the fidelity of ICS simulations by evaluating how closely they resemble real-world and noisy physical processes. It measures the simulation distance from a target system by estimating the noise distribution with machine learning models like Random Forest. Unlike existing solutions that require detailed mathematical models or are limited to simple systems, SimProcess operates with only a timeseries of measurements from the real system, making it applicable to a broader range of complex dynamic systems. We demonstrate the framework's effectiveness through a case study using real-world power grid data from the EPIC testbed. We compare the performance of various simulation methods, including static and generative noise techniques. Our model correctly classifies real samples with a recall of up to 1.0. It also identifies Gaussian and Gaussian Mixture as the best distribution to simulate our power systems, together with a generative solution provided by an autoencoder, thereby helping developers to improve honeypot fidelity. Additionally, we make our code publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control</title>
<link>https://arxiv.org/abs/2505.22642</link>
<guid>https://arxiv.org/abs/2505.22642</guid>
<content:encoded><![CDATA[
arXiv:2505.22642v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-training for Recommendation Unlearning</title>
<link>https://arxiv.org/abs/2505.22649</link>
<guid>https://arxiv.org/abs/2505.22649</guid>
<content:encoded><![CDATA[
arXiv:2505.22649v1 Announce Type: cross 
Abstract: Modern recommender systems powered by Graph Neural Networks (GNNs) excel at modeling complex user-item interactions, yet increasingly face scenarios requiring selective forgetting of training data. Beyond user requests to remove specific interactions due to privacy concerns or preference changes, regulatory frameworks mandate recommender systems' ability to eliminate the influence of certain user data from models. This recommendation unlearning challenge presents unique difficulties as removing connections within interaction graphs creates ripple effects throughout the model, potentially impacting recommendations for numerous users. Traditional approaches suffer from significant drawbacks: fragmentation methods damage graph structure and diminish performance, while influence function techniques make assumptions that may not hold in complex GNNs, particularly with self-supervised or random architectures. To address these limitations, we propose a novel model-agnostic pre-training paradigm UnlearnRec that prepares systems for efficient unlearning operations. Our Influence Encoder takes unlearning requests together with existing model parameters and directly produces updated parameters of unlearned model with little fine-tuning, avoiding complete retraining while preserving model performance characteristics. Extensive evaluation on public benchmarks demonstrates that our method delivers exceptional unlearning effectiveness while providing more than 10x speedup compared to retraining approaches. We release our method implementation at: https://github.com/HKUDS/UnlearnRec.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sherlock: Self-Correcting Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.22651</link>
<guid>https://arxiv.org/abs/2505.22651</guid>
<content:encoded><![CDATA[
arXiv:2505.22651v1 Announce Type: cross 
Abstract: Reasoning Vision-Language Models (VLMs) have shown promising performance on complex multimodal tasks. However, they still face significant challenges: they are highly sensitive to reasoning errors, require large volumes of annotated data or accurate verifiers, and struggle to generalize beyond specific domains. To address these limitations, we explore self-correction as a strategy to enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning VLMs' self-correction abilities and identify key gaps. Based on our findings, we introduce Sherlock, a self-correction and self-improvement training framework. Sherlock introduces a trajectory-level self-correction objective, a preference data construction method based on visual perturbation, and a dynamic $\beta$ for preference tuning. Once the model acquires self-correction capabilities using only 20k randomly sampled annotated data, it continues to self-improve without external supervision. Built on the Llama3.2-Vision-11B model, Sherlock achieves remarkable results across eight benchmarks, reaching an average accuracy of 64.1 with direct generation and 65.4 after self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and LlamaV-o1 (63.4) while using less than 20% of the annotated data.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model</title>
<link>https://arxiv.org/abs/2505.22657</link>
<guid>https://arxiv.org/abs/2505.22657</guid>
<content:encoded><![CDATA[
arXiv:2505.22657v1 Announce Type: cross 
Abstract: Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models</title>
<link>https://arxiv.org/abs/2505.22662</link>
<guid>https://arxiv.org/abs/2505.22662</guid>
<content:encoded><![CDATA[
arXiv:2505.22662v1 Announce Type: cross 
Abstract: The reasoning-capable large language models (LLMs) demonstrate strong performance on complex reasoning tasks but often suffer from overthinking, generating unnecessarily long chain-of-thought (CoT) reasoning paths for easy reasoning questions, thereby increasing inference cost and latency. Recent approaches attempt to address this challenge by manually deciding when to apply long or short reasoning. However, they lack the flexibility to adapt CoT length dynamically based on question complexity. In this paper, we propose Auto Long-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that enables LLMs to dynamically compress their generated reasoning path based on the complexity of the reasoning question. AutoL2S enables a learned paradigm, in which LLMs themselves can decide when longer reasoning is necessary and when shorter reasoning suffices, by training on data annotated with our proposed method, which includes both long and short CoT paths and a special  token. We then use  token to indicate when the model can skip generating lengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs' ability to generate shorter CoT reasoning paths with improved quality after training. Extensive evaluation results show that AutoL2S reduces the length of reasoning generation by up to 57% without compromising performance, demonstrating the effectiveness of AutoL2S for scalable and efficient LLM reasoning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring Traffic Models in Terminal Airspace from Flight Tracks and Procedures</title>
<link>https://arxiv.org/abs/2303.09981</link>
<guid>https://arxiv.org/abs/2303.09981</guid>
<content:encoded><![CDATA[
arXiv:2303.09981v3 Announce Type: replace 
Abstract: Realistic aircraft trajectory models are useful in the design and validation of air traffic management (ATM) systems. Models of aircraft operated under instrument flight rules (IFR) require capturing the variability inherent in how aircraft follow standard flight procedures. The variability in aircraft behavior differs among flight stages. In this paper, we propose a simple probabilistic model that can learn this variability from procedural data and flight tracks collected from radar surveillance data. For each segment, we use a Gaussian mixture model to learn the deviations of aircraft trajectories from their procedures. Given new procedures, we generate synthetic trajectories by sampling a series of deviations from the Gaussian mixture model and reconstructing the aircraft trajectory using the deviations and the procedures. We extend this method to capture pairwise correlations between aircraft and show how a pairwise model can be used to generate traffic involving an arbitrary number of aircraft. We demonstrate the proposed models on the arrival tracks and procedures of the John F. Kennedy International Airport. Distributional similarity between the original and the synthetic trajectory dataset was evaluated using the Jensen-Shannon divergence between the empirical distributions of different variables and we provide qualitative analyses of the synthetic trajectories generated.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Evaluation of Rewiring Approaches in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2305.19717</link>
<guid>https://arxiv.org/abs/2305.19717</guid>
<content:encoded><![CDATA[
arXiv:2305.19717v2 Announce Type: replace 
Abstract: Graph neural networks compute node representations by performing multiple message-passing steps that consist in local aggregations of node features. Having deep models that can leverage longer-range interactions between nodes is hindered by the issues of over-smoothing and over-squashing. In particular, the latter is attributed to the graph topology which guides the message-passing, causing a node representation to become insensitive to information contained at distant nodes. Many graph rewiring methods have been proposed to remedy or mitigate this problem. However, properly evaluating the benefits of these methods is made difficult by the coupling of over-squashing with other issues strictly related to model training, such as vanishing gradients. Therefore, we propose an evaluation setting based on message-passing models that do not require training to compute node and graph representations. We perform a systematic experimental comparison on real-world node and graph classification tasks, showing that rewiring the underlying graph rarely does confer a practical benefit for message-passing.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Positive-incentive Noise: How Noise Benefits Models</title>
<link>https://arxiv.org/abs/2306.07651</link>
<guid>https://arxiv.org/abs/2306.07651</guid>
<content:encoded><![CDATA[
arXiv:2306.07651v2 Announce Type: replace 
Abstract: A large number of works aim to alleviate the impact of noise due to an underlying conventional assumption of the negative role of noise. However, some existing works show that the assumption does not always hold. In this paper, we investigate how to benefit the classical models by random noise under the framework of Positive-incentive Noise (Pi-Noise). Since the ideal objective of Pi-Noise is intractable, we propose to optimize its variational bound instead, namely variational Pi-Noise (VPN). With the variational inference, a VPN generator implemented by neural networks is designed for enhancing base models and simplifying the inference of base models, without changing the architecture of base models. Benefiting from the independent design of base models and VPN generators, the VPN generator can work with most existing models. From the experiments, it is shown that the proposed VPN generator can improve the base models. It is appealing that the trained variational VPN generator prefers to blur the irrelevant ingredients in complicated images, which meets our expectations.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models</title>
<link>https://arxiv.org/abs/2311.04378</link>
<guid>https://arxiv.org/abs/2311.04378</guid>
<content:encoded><![CDATA[
arXiv:2311.04378v5 Announce Type: replace 
Abstract: Watermarking generative models consists of planting a statistical signal (watermark) in a model's output so that it can be later verified that the output was generated by the given model. A strong watermarking scheme satisfies the property that a computationally bounded attacker cannot erase the watermark without causing significant quality degradation. In this paper, we study the (im)possibility of strong watermarking schemes. We prove that, under well-specified and natural assumptions, strong watermarking is impossible to achieve. This holds even in the private detection algorithm setting, where the watermark insertion and detection algorithms share a secret key, unknown to the attacker. To prove this result, we introduce a generic efficient watermark attack; the attacker is not required to know the private key of the scheme or even which scheme is used. Our attack is based on two assumptions: (1) The attacker has access to a "quality oracle" that can evaluate whether a candidate output is a high-quality response to a prompt, and (2) The attacker has access to a "perturbation oracle" which can modify an output with a nontrivial probability of maintaining quality, and which induces an efficiently mixing random walk on high-quality outputs. We argue that both assumptions can be satisfied in practice by an attacker with weaker computational capabilities than the watermarked model itself, to which the attacker has only black-box access. Furthermore, our assumptions will likely only be easier to satisfy over time as models grow in capabilities and modalities. We demonstrate the feasibility of our attack by instantiating it to attack three existing watermarking schemes for large language models: Kirchenbauer et al. (2023), Kuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully removes the watermarks planted by all three schemes, with only minor quality degradation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models</title>
<link>https://arxiv.org/abs/2401.10690</link>
<guid>https://arxiv.org/abs/2401.10690</guid>
<content:encoded><![CDATA[
arXiv:2401.10690v5 Announce Type: replace 
Abstract: Dyadic regression models, which output real-valued predictions for pairs of entities, are fundamental in many domains (e.g. obtaining user-product ratings in Recommender Systems) and promising and under exploration in others (e.g. tuning patient-drug dosages in precision pharmacology). In this work, we prove that non-uniform observed value distributions of individual entities lead to severe biases in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet crucial cases; we name this phenomenon eccentricity bias. We show that global error metrics like Root Mean Squared Error (RMSE) are insufficient to capture this bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a novel metric that can quantify it in all studied domains and models. We prove the intuitive interpretation of EAUC by experimenting with naive post-training bias corrections, and theorize other options to use EAUC to guide the construction of fair models. This work contributes a bias-aware evaluation of dyadic regression to prevent unfairness in critical real-world applications of such systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsic User-Centric Interpretability through Global Mixture of Experts</title>
<link>https://arxiv.org/abs/2402.02933</link>
<guid>https://arxiv.org/abs/2402.02933</guid>
<content:encoded><![CDATA[
arXiv:2402.02933v4 Announce Type: replace 
Abstract: In human-centric settings like education or healthcare, model accuracy and model explainability are key factors for user adoption. Towards these two goals, intrinsically interpretable deep learning models have gained popularity, focusing on accurate predictions alongside faithful explanations. However, there exists a gap in the human-centeredness of these approaches, which often produce nuanced and complex explanations that are not easily actionable for downstream users. We present InterpretCC (interpretable conditional computation), a family of intrinsically interpretable neural networks at a unique point in the design space that optimizes for ease of human understanding and explanation faithfulness, while maintaining comparable performance to state-of-the-art models. InterpretCC achieves this through adaptive sparse activation of features before prediction, allowing the model to use a different, minimal set of features for each instance. We extend this idea into an interpretable, global mixture-of-experts (MoE) model that allows users to specify topics of interest, discretely separates the feature space for each data point into topical subnetworks, and adaptively and sparsely activates these topical subnetworks for prediction. We apply InterpretCC for text, time series and tabular data across several real-world datasets, demonstrating comparable performance with non-interpretable baselines and outperforming intrinsically interpretable baselines. Through a user study involving 56 teachers, InterpretCC explanations are found to have higher actionability and usefulness over other intrinsically interpretable approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Subgraph Federated Learning</title>
<link>https://arxiv.org/abs/2402.19163</link>
<guid>https://arxiv.org/abs/2402.19163</guid>
<content:encoded><![CDATA[
arXiv:2402.19163v3 Announce Type: replace 
Abstract: We address the challenge of federated learning on graph-structured data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where interconnections between different clients play a critical role. We present a novel framework for this scenario, named FedStruct, that harnesses deep structural dependencies. To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients. Instead, it leverages explicit global graph structure information to capture inter-node dependencies. We validate the effectiveness of FedStruct through experimental results conducted on six datasets for semi-supervised node classification, showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of clients.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LC-Tsallis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits</title>
<link>https://arxiv.org/abs/2403.03219</link>
<guid>https://arxiv.org/abs/2403.03219</guid>
<content:encoded><![CDATA[
arXiv:2403.03219v3 Announce Type: replace 
Abstract: We investigate the \emph{linear contextual bandit problem} with independent and identically distributed (i.i.d.) contexts. In this problem, we aim to develop a \emph{Best-of-Both-Worlds} (BoBW) algorithm with regret upper bounds in both stochastic and adversarial regimes. We develop an algorithm based on \emph{Follow-The-Regularized-Leader} (FTRL) with Tsallis entropy, referred to as the $\alpha$-\emph{Linear-Contextual (LC)-Tsallis-INF}. We show that its regret is at most $O(\log(T))$ in the stochastic regime under the assumption that the suboptimality gap is uniformly bounded from below, and at most $O(\sqrt{T})$ in the adversarial regime. Furthermore, our regret analysis is extended to more general regimes characterized by the \emph{margin condition} with a parameter $\beta \in (1, \infty]$, which imposes a milder assumption on the suboptimality gap. We show that the proposed algorithm achieves $O\left(\log(T)^{\frac{1+\beta}{2+\beta}}T^{\frac{1}{2+\beta}}\right)$ regret under the margin condition.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polynomial Chaos Expanded Gaussian Process</title>
<link>https://arxiv.org/abs/2405.01052</link>
<guid>https://arxiv.org/abs/2405.01052</guid>
<content:encoded><![CDATA[
arXiv:2405.01052v2 Announce Type: replace 
Abstract: In complex and unknown processes, global models are initially generated over the entire experimental space but often fail to provide accurate predictions in local areas. A common approach is to use local models, which requires partitioning the experimental space and training multiple models, adding significant complexity. Recognizing this limitation, this study addresses the need for models that effectively represent both global and local experimental spaces. It introduces a novel machine learning (ML) approach: Polynomial Chaos Expanded Gaussian Process (PCEGP), leveraging polynomial chaos expansion (PCE) to calculate input-dependent hyperparameters of the Gaussian process (GP). This provides a mathematically interpretable approach that incorporates non-stationary covariance functions and heteroscedastic noise estimation to generate locally adapted models. The model performance is compared to different algorithms in benchmark tests for regression tasks. The results demonstrate low prediction errors of the PCEGP, highlighting model performance that is often competitive with or better than previous methods. A key advantage of the presented model is its interpretable hyperparameters along with training and prediction runtimes comparable to those of a GP.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast meta-solvers for 3D complex-shape scatterers using neural operators trained on a non-scattering problem</title>
<link>https://arxiv.org/abs/2405.12380</link>
<guid>https://arxiv.org/abs/2405.12380</guid>
<content:encoded><![CDATA[
arXiv:2405.12380v2 Announce Type: replace 
Abstract: Three-dimensional target identification using scattering techniques requires high accuracy solutions and very fast computations for real-time predictions in some critical applications. We first train a deep neural operator~(DeepONet) to solve wave propagation problems described by the Helmholtz equation in a domain \textit{without scatterers} but at different wavenumbers and with a complex absorbing boundary condition. We then design two classes of fast meta-solvers by combining DeepONet with either relaxation methods, such as Jacobi and Gauss-Seidel, or with Krylov methods, such as GMRES and BiCGStab, using the trunk basis of DeepONet as a coarse-scale preconditioner. We leverage the spectral bias of neural networks to account for the lower part of the spectrum in the error distribution while the upper part is handled inexpensively using relaxation methods or fine-scale preconditioners. The meta-solvers are then applied to solve scattering problems with different shape of scatterers, at no extra training cost. We first demonstrate that the resulting meta-solvers are shape-agnostic, fast, and robust, whereas the standard standalone solvers may even fail to converge without the DeepONet. We then apply both classes of meta-solvers to scattering from a submarine, a complex three-dimensional problem. We achieve very fast solutions, especially with the DeepONet-Krylov methods, which require orders of magnitude fewer iterations than any of the standalone solvers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Latent Graph Structures and their Uncertainty</title>
<link>https://arxiv.org/abs/2405.19933</link>
<guid>https://arxiv.org/abs/2405.19933</guid>
<content:encoded><![CDATA[
arXiv:2405.19933v2 Announce Type: replace 
Abstract: Graph neural networks use relational information as an inductive bias to enhance prediction performance. Not rarely, task-relevant relations are unknown and graph structure learning approaches have been proposed to learn them from data. Given their latent nature, no graph observations are available to provide a direct training signal to the learnable relations. Therefore, graph topologies are typically learned on the prediction task alongside the other graph neural network parameters. In this paper, we demonstrate that minimizing point-prediction losses does not guarantee proper learning of the latent relational information and its associated uncertainty. Conversely, we prove that suitable loss functions on the stochastic model outputs simultaneously grant solving two tasks: (i) learning the unknown distribution of the latent graph and (ii) achieving optimal predictions of the target variable. Finally, we propose a sampling-based method that solves this joint learning task. Empirical results validate our theoretical claims and demonstrate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTBENCH: A Library and Benchmark for Certified Training</title>
<link>https://arxiv.org/abs/2406.04848</link>
<guid>https://arxiv.org/abs/2406.04848</guid>
<content:encoded><![CDATA[
arXiv:2406.04848v4 Announce Type: replace 
Abstract: Training certifiably robust neural networks is an important but challenging task. While many algorithms for (deterministic) certified training have been proposed, they are often evaluated on different training schedules, certification methods, and systematically under-tuned hyperparameters, making it difficult to compare their performance. To address this challenge, we introduce CTBench, a unified library and a high-quality benchmark for certified training that evaluates all algorithms under fair settings and systematically tuned hyperparameters. We show that (1) almost all algorithms in CTBench surpass the corresponding reported performance in literature in the magnitude of algorithmic improvements, thus establishing new state-of-the-art, and (2) the claimed advantage of recent algorithms drops significantly when we enhance the outdated baselines with a fair training schedule, a fair certification method and well-tuned hyperparameters. Based on CTBench, we provide new insights into the current state of certified training, including (1) certified models have less fragmented loss surface, (2) certified models share many mistakes, (3) certified models have more sparse activations, (4) reducing regularization cleverly is crucial for certified training especially for large radii and (5) certified training has the potential to improve out-of-distribution generalization. We are confident that CTBench will serve as a benchmark and testbed for future research in certified training.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Heteroscedastic Count Regression with Deep Double Poisson Networks</title>
<link>https://arxiv.org/abs/2406.09262</link>
<guid>https://arxiv.org/abs/2406.09262</guid>
<content:encoded><![CDATA[
arXiv:2406.09262v4 Announce Type: replace 
Abstract: Neural networks capable of accurate, input-conditional uncertainty representation are essential for real-world AI systems. Deep ensembles of Gaussian networks have proven highly effective for continuous regression due to their ability to flexibly represent aleatoric uncertainty via unrestricted heteroscedastic variance, which in turn enables accurate epistemic uncertainty estimation. However, no analogous approach exists for count regression, despite many important applications. To address this gap, we propose the Deep Double Poisson Network (DDPN), a novel neural discrete count regression model that outputs the parameters of the Double Poisson distribution, enabling arbitrarily high or low predictive aleatoric uncertainty for count data and improving epistemic uncertainty estimation when ensembled. We formalize and prove that DDPN exhibits robust regression properties similar to heteroscedastic Gaussian models via learnable loss attenuation, and introduce a simple loss modification to control this behavior. Experiments on diverse datasets demonstrate that DDPN outperforms current baselines in accuracy, calibration, and out-of-distribution detection, establishing a new state-of-the-art in deep count regression.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Boosting Reinforcement Learning</title>
<link>https://arxiv.org/abs/2407.08250</link>
<guid>https://arxiv.org/abs/2407.08250</guid>
<content:encoded><![CDATA[
arXiv:2407.08250v2 Announce Type: replace 
Abstract: We present Gradient Boosting Reinforcement Learning (GBRL), a framework that adapts the strengths of gradient boosting trees (GBT) to reinforcement learning (RL) tasks. While neural networks (NNs) have become the de facto choice for RL, they face significant challenges with structured and categorical features and tend to generalize poorly to out-of-distribution samples. These are challenges for which GBTs have traditionally excelled in supervised learning. However, GBT's application in RL has been limited. The design of traditional GBT libraries is optimized for static datasets with fixed labels, making them incompatible with RL's dynamic nature, where both state distributions and reward signals evolve during training. GBRL overcomes this limitation by continuously interleaving tree construction with environment interaction. Through extensive experiments, we demonstrate that GBRL outperforms NNs in domains with structured observations and categorical features while maintaining competitive performance on standard continuous control benchmarks. Like its supervised learning counterpart, GBRL demonstrates superior robustness to out-of-distribution samples and better handles irregular state-action relationships.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures</title>
<link>https://arxiv.org/abs/2407.19580</link>
<guid>https://arxiv.org/abs/2407.19580</guid>
<content:encoded><![CDATA[
arXiv:2407.19580v4 Announce Type: replace 
Abstract: Training with larger mini-batches improves the convergence rate and can yield superior performance. However, training with large mini-batches becomes prohibitive for Large Language Models (LLMs), due to the large GPU memory requirement. To address this problem, an effective approach is finding small mini-batch coresets that closely match the gradient of larger mini-batches. However, this approach becomes infeasible and ineffective for LLMs, due to the highly imbalanced mixture of sources in language data, use of the Adam optimizer, and the very large gradient dimensionality of LLMs. In this work, we address the above challenges by proposing Coresets for Training LLMs (CoLM). First, we show that mini-batch coresets found by gradient matching do not contain representative examples of the small sources w.h.p., and thus including all examples of the small sources in the mini-batch coresets is crucial for optimal performance. Second, we normalize the gradients by their historical exponential to find mini-batch coresets for training with Adam. Finally, we leverage zeroth-order methods to find smooth gradient of the last V-projection matrix and sparsify it to keep the dimensions with the largest normalized gradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, Zephyr, and Llama-3 models with LoRA on MathInstruct and SuperGLUE benchmark. Remarkably, CoLM reduces the memory requirement of fine-tuning by 2x and even outperforms training with 4x larger mini-batches. Moreover, CoLM seamlessly integrates with existing memory-efficient training methods like LoRA, further reducing the memory requirements of training LLMs. Our code is available at https://github.com/BigML-CS-UCLA/CoLM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Kernel Learning for Small Dataset Modeling in Semiconductor Fabrication: Application to Ohmic Contact</title>
<link>https://arxiv.org/abs/2409.10803</link>
<guid>https://arxiv.org/abs/2409.10803</guid>
<content:encoded><![CDATA[
arXiv:2409.10803v3 Announce Type: replace 
Abstract: Modeling complex semiconductor fabrication processes such as Ohmic contact formation remains challenging due to high-dimensional parameter spaces and limited experimental data. While classical machine learning (CML) approaches have been successful in many domains, their performance degrades in small-sample, nonlinear scenarios. In this work, we investigate quantum machine learning (QML) as an alternative, exploiting quantum kernels to capture intricate correlations from compact datasets. Using only 159 experimental GaN HEMT samples, we develop a quantum kernel-aligned regressor (QKAR) combining a shallow Pauli-Z feature map with a trainable quantum kernel alignment (QKA) layer. All models, including seven baseline CML regressors, are evaluated under a unified PCA-based preprocessing pipeline to ensure a fair comparison. QKAR consistently outperforms classical baselines across multiple metrics (MAE, MSE, RMSE), achieving a mean absolute error of 0.338 Omega mm when validated on experimental data. We further assess noise robustness and generalization through cross-validation and new device fabrication. These findings suggest that carefully constructed QML models could provide predictive advantages in data-constrained semiconductor modeling, offering a foundation for practical deployment on near-term quantum hardware. While challenges remain for both QML and CML, this study demonstrates QML's potential as a complementary approach in complex process modeling tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Multi-Factor Network with Dynamic Sequence Modeling for Early Warning of Intraoperative Hypotension</title>
<link>https://arxiv.org/abs/2409.11064</link>
<guid>https://arxiv.org/abs/2409.11064</guid>
<content:encoded><![CDATA[
arXiv:2409.11064v3 Announce Type: replace 
Abstract: Intraoperative hypotension (IOH) prediction using past physiological signals is crucial, as IOH may lead to inadequate organ perfusion and significantly elevate the risk of severe complications and mortality. However, current methods often rely on static modeling, overlooking the complex temporal dependencies and the inherently non-stationary nature of physiological signals. We propose a Hybrid Multi-Factor (HMF) network that formulates IOH prediction as a dynamic sequence forecasting task, explicitly capturing both temporal dependencies and physiological non-stationarity. We represent signal dynamics as multivariate time series and decompose them into trend and seasonal components, enabling separate modeling of long-term and periodic variations. Each component is encoded with a patch-based Transformer to balance computational efficiency and feature representation. To address distributional drift from evolving signals, we introduce a symmetric normalization mechanism. Experiments on both public and real-world clinical datasets show that HMF significantly outperforms competitive baselines. We hope HMF offers new insights into IOH prediction and ultimately promotes safer surgical care. Our code is available at https://github.com/Mingyue-Cheng/HMF.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Criticality and Safety Margins for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2409.18289</link>
<guid>https://arxiv.org/abs/2409.18289</guid>
<content:encoded><![CDATA[
arXiv:2409.18289v2 Announce Type: replace 
Abstract: State of the art reinforcement learning methods sometimes encounter unsafe situations. Identifying when these situations occur is of interest both for post-hoc analysis and during deployment, where it might be advantageous to call out to a human overseer for help. Efforts to gauge the criticality of different points in time have been developed, but their accuracy is not well established due to a lack of ground truth, and they are not designed to be easily interpretable by end users. Therefore, we seek to define a criticality framework with both a quantifiable ground truth and a clear significance to users. We introduce true criticality as the expected drop in reward when an agent deviates from its policy for n consecutive random actions. We also introduce the concept of proxy criticality, a low-overhead metric that has a statistically monotonic relationship to true criticality. Safety margins make these interpretable, when defined as the number of random actions for which performance loss will not exceed some tolerance with high confidence. We demonstrate this approach in several environment-agent combinations; for an A3C agent in an Atari Beamrider environment, the lowest 5% of safety margins contain 47% of agent losses; i.e., supervising only 5% of decisions could potentially prevent roughly half of an agent's errors. This criticality framework measures the potential impacts of bad decisions, even before those decisions are made, allowing for more effective debugging and oversight of autonomous agents.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking GNN Expressive Power from a Distributed Computational Model Perspective</title>
<link>https://arxiv.org/abs/2410.01308</link>
<guid>https://arxiv.org/abs/2410.01308</guid>
<content:encoded><![CDATA[
arXiv:2410.01308v3 Announce Type: replace 
Abstract: The success of graph neural networks (GNNs) has motivated theoretical studies on their expressive power, often through alignments with the Weisfeiler-Lehman (WL) tests. However, such analyses typically focus on the ability of GNNs to distinguish between graph structures, rather than to compute or approximate specific function classes. The latter is more commonly studied in machine learning theory, including results such as the Turing completeness of recurrent networks and the universal approximation property of feedforward networks. We argue that using well-defined computational models, such as a modified CONGEST model with clearly specified preprocessing and postprocessing, offers a more sound framework for analyzing GNN expressiveness. Within this framework, we show that allowing unrestricted preprocessing or incorporating externally computed features, while claiming that these precomputations enhance the expressiveness, can sometimes lead to problems. We also show that the lower bound on a GNN's capacity (depth multiplied by width) to simulate one iteration of the WL test actually grows nearly linearly with graph size, indicating that the WL test is not locally computable and is misaligned with message-passing GNNs. Despite these negative results, we also present positive results that characterize the effects of virtual nodes and edges from a computational model perspective. Finally, we highlight several open problems regarding GNN expressiveness for further exploration.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diss-l-ECT: Dissecting Graph Data with Local Euler Characteristic Transforms</title>
<link>https://arxiv.org/abs/2410.02622</link>
<guid>https://arxiv.org/abs/2410.02622</guid>
<content:encoded><![CDATA[
arXiv:2410.02622v2 Announce Type: replace 
Abstract: The Euler Characteristic Transform (ECT) is an efficiently-computable geometrical-topological invariant that characterizes the global shape of data. In this paper, we introduce the Local Euler Characteristic Transform ($\ell$-ECT), a novel extension of the ECT particularly designed to enhance expressivity and interpretability in graph representation learning. Unlike traditional Graph Neural Networks (GNNs), which may lose critical local details through aggregation, the $\ell$-ECT provides a lossless representation of local neighborhoods. This approach addresses key limitations in GNNs by preserving nuanced local structures while maintaining global interpretability. Moreover, we construct a rotation-invariant metric based on $\ell$-ECTs for spatial alignment of data spaces. Our method exhibits superior performance compared to standard GNNs on a variety of node-classification tasks, while also offering theoretical guarantees that demonstrate its effectiveness.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Limitations of Mamba in COPY and CoT Reasoning</title>
<link>https://arxiv.org/abs/2410.03810</link>
<guid>https://arxiv.org/abs/2410.03810</guid>
<content:encoded><![CDATA[
arXiv:2410.03810v2 Announce Type: replace 
Abstract: Transformers have become the backbone of modern Large Language Models (LLMs); however, their inference overhead grows linearly with the sequence length, posing challenges for modeling long sequences. In light of this, Mamba has attracted attention for maintaining a constant inference size, with empirical evidence demonstrating that it can match Transformer performance in sequence modeling while significantly reducing computational costs. However, an open question remains: can Mamba always bring savings while achieving performance comparable to Transformers? In this paper, we focus on analyzing the expressive ability of Mamba to perform our defined COPY operation and Chain of Thought (CoT) reasoning. First, inspired by the connection between Mamba and linear attention, we show that constant-sized Mamba may struggle to perform COPY operations while Transformers can handle them more easily. However, when the size of Mamba grows linearly with the input sequence length, it can accurately perform COPY, but in this case, Mamba no longer provides overhead savings. Based on this observation, we further analyze Mamba's ability to tackle CoT tasks, which can be described by the Dynamic Programming (DP) problems. Our findings suggest that to solve arbitrary DP problems, the total cost of Mamba is still comparable to standard Transformers. However, similar to efficient Transformers, when facing DP problems with favorable properties such as locality, Mamba can provide savings in overhead. Our experiments on the copy and CoT tasks further demonstrate Mamba's limitations compared to Transformers in learning these tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Model Ensemble in Transferable Adversarial Attack</title>
<link>https://arxiv.org/abs/2410.06851</link>
<guid>https://arxiv.org/abs/2410.06851</guid>
<content:encoded><![CDATA[
arXiv:2410.06851v3 Announce Type: replace 
Abstract: Model ensemble adversarial attack has become a powerful method for generating transferable adversarial examples that can target even unknown models, but its theoretical foundation remains underexplored. To address this gap, we provide early theoretical insights that serve as a roadmap for advancing model ensemble adversarial attack. We first define transferability error to measure the error in adversarial transferability, alongside concepts of diversity and empirical model ensemble Rademacher complexity. We then decompose the transferability error into vulnerability, diversity, and a constant, which rigidly explains the origin of transferability error in model ensemble attack: the vulnerability of an adversarial example to ensemble components, and the diversity of ensemble components. Furthermore, we apply the latest mathematical tools in information theory to bound the transferability error using complexity and generalization terms, contributing to three practical guidelines for reducing transferability error: (1) incorporating more surrogate models, (2) increasing their diversity, and (3) reducing their complexity in cases of overfitting. Finally, extensive experiments with 54 models validate our theoretical framework, representing a significant step forward in understanding transferable model ensemble adversarial attacks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EventFlow: Forecasting Temporal Point Processes with Flow Matching</title>
<link>https://arxiv.org/abs/2410.07430</link>
<guid>https://arxiv.org/abs/2410.07430</guid>
<content:encoded><![CDATA[
arXiv:2410.07430v2 Announce Type: replace 
Abstract: Continuous-time event sequences, in which events occur at irregular intervals, are ubiquitous across a wide range of industrial and scientific domains. The contemporary modeling paradigm is to treat such data as realizations of a temporal point process, and in machine learning it is common to model temporal point processes in an autoregressive fashion using a neural network. While autoregressive models are successful in predicting the time of a single subsequent event, their performance can degrade when forecasting longer horizons due to cascading errors and myopic predictions. We propose EventFlow, a non-autoregressive generative model for temporal point processes. The model builds on the flow matching framework in order to directly learn joint distributions over event times, side-stepping the autoregressive process. EventFlow is simple to implement and achieves a 20%-53% lower error than the nearest baseline on standard TPP benchmarks while simultaneously using fewer model calls at sampling time.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NRFormer: Nationwide Nuclear Radiation Forecasting with Spatio-Temporal Transformer</title>
<link>https://arxiv.org/abs/2410.11924</link>
<guid>https://arxiv.org/abs/2410.11924</guid>
<content:encoded><![CDATA[
arXiv:2410.11924v3 Announce Type: replace 
Abstract: Nuclear radiation, which refers to the energy emitted from atomic nuclei during decay, poses significant risks to human health and environmental safety. Recently, advancements in monitoring technology have facilitated the effective recording of nuclear radiation levels and related factors, such as weather conditions. The abundance of monitoring data enables the development of accurate and reliable nuclear radiation forecasting models, which play a crucial role in informing decision-making for individuals and governments. However, this task is challenging due to the imbalanced distribution of monitoring stations over a wide spatial range and the non-stationary radiation variation patterns. In this study, we introduce NRFormer, a novel framework tailored for the nationwide prediction of nuclear radiation variations. By integrating a non-stationary temporal attention module, an imbalance-aware spatial attention module, and a radiation propagation prompting module, NRFormer collectively captures complex spatio-temporal dynamics of nuclear radiation. Extensive experiments on two real-world datasets demonstrate the superiority of our proposed framework against 11 baselines.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Weight Diffusion: Generating reactive policies instead of trajectories</title>
<link>https://arxiv.org/abs/2410.14040</link>
<guid>https://arxiv.org/abs/2410.14040</guid>
<content:encoded><![CDATA[
arXiv:2410.14040v2 Announce Type: replace 
Abstract: With the increasing availability of open-source robotic data, imitation learning has emerged as a viable approach for both robot manipulation and locomotion. Currently, large generalized policies are trained to predict controls or trajectories using diffusion models, which have the desirable property of learning multimodal action distributions. However, generalizability comes with a cost, namely, larger model size and slower inference. This is especially an issue for robotic tasks that require high control frequency. Further, there is a known trade-off between performance and action horizon for Diffusion Policy (DP), a popular model for generating trajectories: fewer diffusion queries accumulate greater trajectory tracking errors. For these reasons, it is common practice to run these models at high inference frequency, subject to robot computational constraints. To address these limitations, we propose Latent Weight Diffusion (LWD), a method that uses diffusion to generate closed-loop policies (weights for neural policies) for robotic tasks, rather than generating trajectories. Learning the behavior distribution through parameter space over trajectory space offers two key advantages: longer action horizons (fewer diffusion queries) & robustness to perturbations while retaining high performance; and a lower inference compute cost. To this end, we show that LWD has higher success rates than DP when the action horizon is longer and when stochastic perturbations exist in the environment. Furthermore, LWD achieves multitask performance comparable to DP while requiring just ~1/45th of the inference-time FLOPS
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Safety into RL: A New Take on Trust Region Methods</title>
<link>https://arxiv.org/abs/2411.02957</link>
<guid>https://arxiv.org/abs/2411.02957</guid>
<content:encoded><![CDATA[
arXiv:2411.02957v3 Announce Type: replace 
Abstract: Reinforcement Learning (RL) agents can solve diverse tasks but often exhibit unsafe behavior. Constrained Markov Decision Processes (CMDPs) address this by enforcing safety constraints, yet existing methods either sacrifice reward maximization or allow unsafe training. We introduce Constrained Trust Region Policy Optimization (C-TRPO), which reshapes the policy space geometry to ensure trust regions contain only safe policies, guaranteeing constraint satisfaction throughout training. We analyze its theoretical properties and connections to TRPO, Natural Policy Gradient (NPG), and Constrained Policy Optimization (CPO). Experiments show that C-TRPO reduces constraint violations while maintaining competitive returns.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot Learning</title>
<link>https://arxiv.org/abs/2411.12155</link>
<guid>https://arxiv.org/abs/2411.12155</guid>
<content:encoded><![CDATA[
arXiv:2411.12155v4 Announce Type: replace 
Abstract: Predicting a sequence of actions has been crucial in the success of recent behavior cloning algorithms in robotics. Can similar ideas improve reinforcement learning (RL)? We answer affirmatively by observing that incorporating action sequences when predicting ground-truth return-to-go leads to lower validation loss. Motivated by this, we introduce Coarse-to-fine Q-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that learns a critic network that outputs Q-values over a sequence of actions, i.e., explicitly training the value function to learn the consequence of executing action sequences. Our experiments show that CQN-AS outperforms several baselines on a variety of sparse-reward humanoid control and tabletop manipulation tasks from BiGym and RLBench.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Reinforcement Learning</title>
<link>https://arxiv.org/abs/2411.14251</link>
<guid>https://arxiv.org/abs/2411.14251</guid>
<content:encoded><![CDATA[
arXiv:2411.14251v3 Announce Type: replace 
Abstract: Artificial intelligence progresses towards the "Era of Experience," where agents are expected to learn from continuous, grounded interaction. We argue that traditional Reinforcement Learning (RL), which typically represents value as a scalar, can restrict agent's deep understanding of environments and hinders the active, deliberative learning crucial for navigating this new paradigm. To address the issue, we introduce Natural Language Reinforcement Learning (NLRL), a framework that extends RL principles into natural language counterparts. Central to NLRL is the Language Value Function (LVF), which redefines value as an interpretable linguistic narrative articulating the rationale behind an evaluation. NLRL further extends this concept to core RL components, including policy, the Bellman equation, and policy iteration. Leveraging recent advancements in Large Language Models (LLMs), NLRL can be practically implemented to achieve RL-like policy and value training through unsupervised environment interactions. Experiments over 4 multi-step agentic tasks demonstrate NLRL's effectiveness, efficiency, and its potential to foster deeper understanding and more active learning strategies.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling</title>
<link>https://arxiv.org/abs/2411.17284</link>
<guid>https://arxiv.org/abs/2411.17284</guid>
<content:encoded><![CDATA[
arXiv:2411.17284v5 Announce Type: replace 
Abstract: Large language models (LLMs) acquire a breadth of information across various domains. However, their computational complexity, cost, and lack of transparency often hinder their direct application for predictive tasks where privacy and interpretability are paramount. In fields such as healthcare, biology, and finance, specialised and interpretable linear models still hold considerable value. In such domains, labelled data may be scarce or expensive to obtain. Well-specified prior distributions over model parameters can reduce the sample complexity of learning through Bayesian inference; however, eliciting expert priors can be time-consuming. We therefore introduce AutoElicit to extract knowledge from LLMs and construct priors for predictive models. We show these priors are informative and can be refined using natural language. We perform a careful study contrasting AutoElicit with in-context learning and demonstrate how to perform model selection between the two methods. We find that AutoElicit yields priors that can substantially reduce error over uninformative priors, using fewer labels, and consistently outperform in-context learning. We show that AutoElicit saves over 6 months of labelling effort when building a new predictive model for urinary tract infections from sensor recordings of people living with dementia.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Label Bayesian Active Learning with Inter-Label Relationships</title>
<link>https://arxiv.org/abs/2411.17941</link>
<guid>https://arxiv.org/abs/2411.17941</guid>
<content:encoded><![CDATA[
arXiv:2411.17941v2 Announce Type: replace 
Abstract: The primary challenge of multi-label active learning, differing it from multi-class active learning, lies in assessing the informativeness of an indefinite number of labels while also accounting for the inherited label correlation. Existing studies either require substantial computational resources to leverage correlations or fail to fully explore label dependencies. Additionally, real-world scenarios often require addressing intrinsic biases stemming from imbalanced data distributions. In this paper, we propose a new multi-label active learning strategy to address both challenges. Our method incorporates progressively updated positive and negative correlation matrices to capture co-occurrence and disjoint relationships within the label space of annotated samples, enabling a holistic assessment of uncertainty rather than treating labels as isolated elements. Furthermore, alongside diversity, our model employs ensemble pseudo labeling and beta scoring rules to address data imbalances. Extensive experiments on four realistic datasets demonstrate that our strategy consistently achieves more reliable and superior performance, compared to several established methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Continual Graph Learning</title>
<link>https://arxiv.org/abs/2411.18919</link>
<guid>https://arxiv.org/abs/2411.18919</guid>
<content:encoded><![CDATA[
arXiv:2411.18919v3 Announce Type: replace 
Abstract: Managing evolving graph data presents substantial challenges in storage and privacy, and training graph neural networks (GNNs) on such data often leads to catastrophic forgetting, impairing performance on earlier tasks. Despite existing continual graph learning (CGL) methods mitigating this to some extent, they rely on centralized architectures and ignore the potential of distributed graph databases to leverage collective intelligence. To this end, we propose Federated Continual Graph Learning (FCGL) to adapt GNNs across multiple evolving graphs under storage and privacy constraints. Our empirical study highlights two core challenges: local graph forgetting (LGF), where clients lose prior knowledge when adapting to new tasks, and global expertise conflict (GEC), where the global GNN exhibits sub-optimal performance in both adapting to new tasks and retaining old ones, arising from inconsistent client expertise during server-side parameter aggregation. To address these, we introduce POWER, a framework that preserves experience nodes with maximum local-global coverage locally to mitigate LGF, and leverages pseudo-prototype reconstruction with trajectory-aware knowledge transfer to resolve GEC. Experiments on various graph datasets demonstrate POWER's superiority over federated adaptations of CGL baselines and vision-centric federated continual learning approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Go With the Flow: Fast Diffusion for Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2412.09059</link>
<guid>https://arxiv.org/abs/2412.09059</guid>
<content:encoded><![CDATA[
arXiv:2412.09059v4 Announce Type: replace 
Abstract: Schrodinger Bridges (SBs) are diffusion processes that steer, in finite time, a given initial distribution to another final one while minimizing a suitable cost functional. Although various methods for computing SBs have recently been proposed in the literature, most of these approaches require computationally expensive training schemes, even for solving low-dimensional problems. In this work, we propose an analytic parametrization of a set of feasible policies for steering the distribution of a dynamical system from one Gaussian Mixture Model (GMM) to another. Instead of relying on standard non-convex optimization techniques, the optimal policy within the set can be approximated as the solution of a low-dimensional linear program whose dimension scales linearly with the number of components in each mixture. The proposed method generalizes naturally to more general classes of dynamical systems, such as controllable linear time-varying systems, enabling efficient solutions to multi-marginal momentum SB between GMMs, a challenging distribution interpolation problem. We showcase the potential of this approach in low-to-moderate dimensional problems such as image-to-image translation in the latent space of an autoencoder, learning of cellular dynamics using multi-marginal momentum SB problems, and various other examples. We also test our approach on an Entropic Optimal Transport (EOT) benchmark problem and show that it outperforms state-of-the-art methods in cases where the boundary distributions are mixture models while requiring virtually no training.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Guidance Mechanisms for Discrete Diffusion Models</title>
<link>https://arxiv.org/abs/2412.10193</link>
<guid>https://arxiv.org/abs/2412.10193</guid>
<content:encoded><![CDATA[
arXiv:2412.10193v3 Announce Type: replace 
Abstract: Diffusion models for continuous data gained widespread adoption owing to their high quality generation and control mechanisms. However, controllable diffusion on discrete data faces challenges given that continuous guidance methods do not directly apply to discrete diffusion. Here, we provide a straightforward derivation of classifier-free and classifier-based guidance for discrete diffusion, as well as a new class of diffusion models that leverage uniform noise and that are more guidable because they can continuously edit their outputs. We improve the quality of these models with a novel continuous-time variational lower bound that yields state-of-the-art performance, especially in settings involving guidance or fast generation. Empirically, we demonstrate that our guidance mechanisms combined with uniform noise diffusion improve controllable generation relative to autoregressive and diffusion baselines on several discrete data domains, including genomic sequences, small molecule design, and discretized image generation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNNs-to-MLPs by Teacher Injection and Dirichlet Energy Distillation</title>
<link>https://arxiv.org/abs/2412.11180</link>
<guid>https://arxiv.org/abs/2412.11180</guid>
<content:encoded><![CDATA[
arXiv:2412.11180v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) are pivotal in graph-based learning, particularly excelling in node classification. However, their scalability is hindered by the need for multi-hop data during inference, limiting their application in latency-sensitive scenarios. Recent efforts to distill GNNs into multi-layer perceptrons (MLPs) for faster inference often underutilize the layer-level insights of GNNs. In this paper, we present TINED, a novel approach that distills GNNs to MLPs on a layer-by-layer basis using Teacher Injection and Dirichlet Energy Distillation techniques. We focus on two key operations in GNN layers: feature transformation (FT) and graph propagation (GP). We recognize that FT is computationally equivalent to a fully-connected (FC) layer in MLPs. Thus, we propose directly transferring teacher parameters from an FT in a GNN to an FC layer in the student MLP, enhanced by fine-tuning. In TINED, the FC layers in an MLP replicate the sequence of FTs and GPs in the GNN. We also establish a theoretical bound for GP approximation. Furthermore, we note that FT and GP operations in GNN layers often exhibit opposing smoothing effects: GP is aggressive, while FT is conservative. Using Dirichlet energy, we develop a DE ratio to measure these effects and propose Dirichlet Energy Distillation to convey these characteristics from GNN layers to MLP layers. Extensive experiments show that TINED outperforms GNNs and leading distillation methods across various settings and seven datasets. Source code are available at https://github.com/scottjiao/TINED_ICML25/.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint-Adaptive Policy Switching for Offline Safe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.18946</link>
<guid>https://arxiv.org/abs/2412.18946</guid>
<content:encoded><![CDATA[
arXiv:2412.18946v2 Announce Type: replace 
Abstract: Offline safe reinforcement learning (OSRL) involves learning a decision-making policy to maximize rewards from a fixed batch of training data to satisfy pre-defined safety constraints. However, adapting to varying safety constraints during deployment without retraining remains an under-explored challenge. To address this challenge, we introduce constraint-adaptive policy switching (CAPS), a wrapper framework around existing offline RL algorithms. During training, CAPS uses offline data to learn multiple policies with a shared representation that optimize different reward and cost trade-offs. During testing, CAPS switches between those policies by selecting at each state the policy that maximizes future rewards among those that satisfy the current cost constraint. Our experiments on 38 tasks from the DSRL benchmark demonstrate that CAPS consistently outperforms existing methods, establishing a strong wrapper-based baseline for OSRL. The code is publicly available at https://github.com/yassineCh/CAPS.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Innovative Data-Driven and Adaptive Reinforcement Learning Approach for Context-Aware Prescriptive Process Monitoring</title>
<link>https://arxiv.org/abs/2501.10543</link>
<guid>https://arxiv.org/abs/2501.10543</guid>
<content:encoded><![CDATA[
arXiv:2501.10543v2 Announce Type: replace 
Abstract: The application of artificial intelligence and machine learning in business process management has advanced significantly, however, the full potential of these technologies remains largely unexplored, primarily due to challenges related to data quality and availability. We present a novel framework called Fine-Tuned Offline Reinforcement Learning Augmented Process Sequence Optimization (FORLAPS), which aims to identify optimal execution paths in business processes by leveraging reinforcement learning enhanced with a state-dependent reward shaping mechanism, thereby enabling context-sensitive prescriptions. Additionally, to compare FORLAPS with the existing models (Permutation Feature Importance and multi-task Long Short Term Memory model), we experimented to evaluate its effectiveness in terms of resource savings and process time reduction. The experimental results on real-life event logs validate that FORLAPS achieves 31% savings in resource time spent and a 23% reduction in process time span. To further enhance learning, we introduce an innovative process-aware data augmentation technique that selectively increases the average estimated Q-values in sampled batches, enabling automatic fine-tuning of the reinforcement learning model. Robustness was assessed through both prefix-level and trace-level evaluations, using the Damerau-Levenshtein distance as the primary metric. Finally, the model's adaptability across industries was further validated through diverse case studies, including healthcare treatment pathways, financial services workflows, permit applications from regulatory bodies, and operations management. In each domain, the proposed model demonstrated exceptional performance, outperforming existing state-of-the-art approaches in prescriptive decision-making, demonstrating its capability to prescribe optimal next steps and predict the best next activities within a process trace.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Logit-based Knowledge Distillation of Deep Spiking Neural Networks for Full-Range Timestep Deployment</title>
<link>https://arxiv.org/abs/2501.15925</link>
<guid>https://arxiv.org/abs/2501.15925</guid>
<content:encoded><![CDATA[
arXiv:2501.15925v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) are emerging as a brain-inspired alternative to traditional Artificial Neural Networks (ANNs), prized for their potential energy efficiency on neuromorphic hardware. Despite this, SNNs often suffer from accuracy degradation compared to ANNs and face deployment challenges due to fixed inference timesteps, which require retraining for adjustments, limiting operational flexibility. To address these issues, our work considers the spatio-temporal property inherent in SNNs, and proposes a novel distillation framework for deep SNNs that optimizes performance across full-range timesteps without specific retraining, enhancing both efficacy and deployment adaptability. We provide both theoretical analysis and empirical validations to illustrate that training guarantees the convergence of all implicit models across full-range timesteps. Experimental results on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate state-of-the-art performance among distillation-based SNNs training methods. Our code is available at https://github.com/Intelli-Chip-Lab/snn\_temporal\_decoupling\_distillation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Dynamics of In-Context Learning in Linear Attention</title>
<link>https://arxiv.org/abs/2501.16265</link>
<guid>https://arxiv.org/abs/2501.16265</guid>
<content:encoded><![CDATA[
arXiv:2501.16265v2 Announce Type: replace 
Abstract: While attention-based models have demonstrated the remarkable ability of in-context learning (ICL), the theoretical understanding of how these models acquired this ability through gradient descent training is still preliminary. Towards answering this question, we study the gradient descent dynamics of multi-head linear self-attention trained for in-context linear regression. We examine two parametrizations of linear self-attention: one with the key and query weights merged as a single matrix (common in theoretical studies), and one with separate key and query matrices (closer to practical settings). For the merged parametrization, we show that the training dynamics has two fixed points and the loss trajectory exhibits a single, abrupt drop. We derive an analytical time-course solution for a certain class of datasets and initialization. For the separate parametrization, we show that the training dynamics has exponentially many fixed points and the loss exhibits saddle-to-saddle dynamics, which we reduce to scalar ordinary differential equations. During training, the model implements principal component regression in context with the number of principal components increasing over training time. Overall, we provide a theoretical description of how ICL abilities evolve during gradient descent training of linear attention, revealing abrupt acquisition or progressive improvements depending on how the key and query are parametrized.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk-Informed Diffusion Transformer for Long-Tail Trajectory Prediction in the Crash Scenario</title>
<link>https://arxiv.org/abs/2501.16349</link>
<guid>https://arxiv.org/abs/2501.16349</guid>
<content:encoded><![CDATA[
arXiv:2501.16349v2 Announce Type: replace 
Abstract: Trajectory prediction methods have been widely applied in autonomous driving technologies. Although the overall performance accuracy of trajectory prediction is relatively high, the lack of trajectory data in critical scenarios in the training data leads to the long-tail phenomenon. Normally, the trajectories of the tail data are more critical and more difficult to predict and may include rare scenarios such as crashes. To solve this problem, we extracted the trajectory data from real-world crash scenarios, which contain more long-tail data. Meanwhile, based on the trajectory data in this scenario, we integrated graph-based risk information and diffusion with transformer and proposed the Risk-Informed Diffusion Transformer (RI-DiT) trajectory prediction method. Extensive experiments were conducted on trajectory data in the real-world crash scenario, and the results show that the algorithm we proposed has good performance. When predicting the data of the tail 10\% (Top 10\%), the minADE and minFDE indicators are 0.016/2.667 m. At the same time, we showed the trajectory conditions of different long-tail distributions. The distribution of trajectory data is closer to the tail, the less smooth the trajectory is. Through the trajectory data in real-world crash scenarios, Our work expands the methods to overcome the long-tail challenges in trajectory prediction. Our method, RI-DiT, integrates inverse time to collision (ITTC) and the feature of traffic flow, which can predict long-tail trajectories more accurately and improve the safety of autonomous driving systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Variational Perspective on Generative Protein Fitness Optimization</title>
<link>https://arxiv.org/abs/2501.19200</link>
<guid>https://arxiv.org/abs/2501.19200</guid>
<content:encoded><![CDATA[
arXiv:2501.19200v2 Announce Type: replace 
Abstract: The goal of protein fitness optimization is to discover new protein variants with enhanced fitness for a given use. The vast search space and the sparsely populated fitness landscape, along with the discrete nature of protein sequences, pose significant challenges when trying to determine the gradient towards configurations with higher fitness. We introduce Variational Latent Generative Protein Optimization (VLGPO), a variational perspective on fitness optimization. Our method embeds protein sequences in a continuous latent space to enable efficient sampling from the fitness distribution and combines a (learned) flow matching prior over sequence mutations with a fitness predictor to guide optimization towards sequences with high fitness. VLGPO achieves state-of-the-art results on two different protein benchmarks of varying complexity. Moreover, the variational design with explicit prior and likelihood functions offers a flexible plug-and-play framework that can be easily customized to suit various protein design tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PUATE: Efficient Average Treatment Effect Estimation from Treated (Positive) and Unlabeled Units</title>
<link>https://arxiv.org/abs/2501.19345</link>
<guid>https://arxiv.org/abs/2501.19345</guid>
<content:encoded><![CDATA[
arXiv:2501.19345v2 Announce Type: replace 
Abstract: The estimation of average treatment effects (ATEs), defined as the difference in expected outcomes between treatment and control groups, is a central topic in causal inference. This study develops semiparametric efficient estimators for ATE in a setting where only a treatment group and an unlabeled group, consisting of units whose treatment status is unknown, are observed. This scenario constitutes a variant of learning from positive and unlabeled data (PU learning) and can be viewed as a special case of ATE estimation with missing data. For this setting, we derive the semiparametric efficiency bounds, which characterize the lowest achievable asymptotic variance for regular estimators. We then construct semiparametric efficient ATE estimators that attain these bounds. Our results contribute to the literature on causal inference with missing data and weakly supervised learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Online Reinforcement Learning for Diffusion Policy</title>
<link>https://arxiv.org/abs/2502.00361</link>
<guid>https://arxiv.org/abs/2502.00361</guid>
<content:encoded><![CDATA[
arXiv:2502.00361v3 Announce Type: replace 
Abstract: Diffusion policies have achieved superior performance in imitation learning and offline reinforcement learning (RL) due to their rich expressiveness. However, the conventional diffusion training procedure requires samples from target distribution, which is impossible in online RL since we cannot sample from the optimal policy. Backpropagating policy gradient through the diffusion process incurs huge computational costs and instability, thus being expensive and not scalable. To enable efficient training of diffusion policies in online RL, we generalize the conventional denoising score matching by reweighting the loss function. The resulting Reweighted Score Matching (RSM) preserves the optimal solution and low computational cost of denoising score matching, while eliminating the need to sample from the target distribution and allowing learning to optimize value functions. We introduce two tractable reweighted loss functions to solve two commonly used policy optimization problems, policy mirror descent and max-entropy policy, resulting in two practical algorithms named Diffusion Policy Mirror Descent (DPMD) and Soft Diffusion Actor-Critic (SDAC). We conducted comprehensive comparisons on MuJoCo benchmarks. The empirical results show that the proposed algorithms outperform recent diffusion-policy online RLs on most tasks, and the DPMD improves more than 120% over soft actor-critic on Humanoid and Ant.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Message-Passing GNNs Fail to Approximate Sparse Triangular Factorizations</title>
<link>https://arxiv.org/abs/2502.01397</link>
<guid>https://arxiv.org/abs/2502.01397</guid>
<content:encoded><![CDATA[
arXiv:2502.01397v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have been proposed as a tool for learning sparse matrix preconditioners, which are key components in accelerating linear solvers. This position paper argues that message-passing GNNs are fundamentally incapable of approximating sparse triangular factorizations. We demonstrate that message-passing GNNs fundamentally fail to approximate sparse triangular factorizations for classes of matrices for which high-quality preconditioners exist but require non-local dependencies. To illustrate this, we construct a set of baselines using both synthetic matrices and real-world examples from the SuiteSparse collection. Across a range of GNN architectures, including Graph Attention Networks and Graph Transformers, we observe severe performance degradation compared to exact or K-optimal factorizations, with cosine similarity dropping below $0.6$ in key cases. Our theoretical and empirical results suggest that architectural innovations beyond message-passing are necessary for applying GNNs to scientific computing tasks such as matrix factorization. Experiments demonstrate that overcoming non-locality alone is insufficient. Tailored architectures are necessary to capture the required dependencies since even a completely non-local Graph Transformer fails to match the proposed baselines.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints Internalization</title>
<link>https://arxiv.org/abs/2502.01562</link>
<guid>https://arxiv.org/abs/2502.01562</guid>
<content:encoded><![CDATA[
arXiv:2502.01562v2 Announce Type: replace 
Abstract: As the general capabilities of artificial intelligence (AI) agents continue to evolve, their ability to learn to master multiple complex tasks through experience remains a key challenge. Current LLM agents, particularly those based on proprietary language models, typically rely on prompts to incorporate knowledge about the target tasks. This approach does not allow the agent to internalize this information and instead relies on ever-expanding prompts to sustain its functionality in diverse scenarios. This resembles a system of notes used by a person affected by anterograde amnesia, the inability to form new memories. In this paper, we propose a novel method to train AI agents to incorporate knowledge and skills for multiple tasks without the need for either cumbersome note systems or prior high-quality demonstration data. Our approach employs an iterative process where the agent collects new experiences, receives corrective feedback from humans in the form of hints, and integrates this feedback into its weights via a context distillation training procedure. We demonstrate the efficacy of our approach by implementing it in a Llama-3-based agent that, after only a few rounds of feedback, outperforms advanced models GPT-4o and DeepSeek-V3 in tasksets requiring correct sequencing of information retrieval, tool use, and question answering.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Rule-based Reasoning in LLMs using Neurosymbolic Representations</title>
<link>https://arxiv.org/abs/2502.01657</link>
<guid>https://arxiv.org/abs/2502.01657</guid>
<content:encoded><![CDATA[
arXiv:2502.01657v3 Announce Type: replace 
Abstract: Large language models (LLMs) continue to face challenges in reliably solving reasoning tasks, particularly those that require precise rule following, as often found in mathematical reasoning. This paper introduces a novel neurosymbolic method that improves LLM reasoning by encoding hidden states into neurosymbolic vectors, enabling problem-solving within a neurosymbolic vector space. The results are decoded and merged with the original hidden state, significantly boosting the model's performance on numerical reasoning tasks. By offloading computation through neurosymbolic representations, this method enhances efficiency, reliability, and interpretability. Experimental results demonstrate an average of 88.6% lower cross-entropy loss and 15.4 times more problems correctly solved on a suite of mathematical reasoning tasks compared to chain-of-thought prompting and supervised fine-tuning (LoRA), without degrading performance on other tasks. We make our code available at: https://github.com/vdhanraj/Neurosymbolic-LLM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust LLM Alignment via Distributionally Robust Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2502.01930</link>
<guid>https://arxiv.org/abs/2502.01930</guid>
<content:encoded><![CDATA[
arXiv:2502.01930v2 Announce Type: replace 
Abstract: A major challenge in aligning large language models (LLMs) with human preferences is the issue of distribution shift. LLM alignment algorithms rely on static preference datasets, assuming that they accurately represent real-world user preferences. However, user preferences vary significantly across geographical regions, demographics, linguistic patterns, and evolving cultural trends. This preference distribution shift leads to catastrophic alignment failures in many real-world applications. We address this problem using the principled framework of distributionally robust optimization, and develop two novel distributionally robust direct preference optimization (DPO) algorithms, namely, Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO). We characterize the sample complexity of learning the optimal policy parameters for WDPO and KLDPO. Moreover, we propose scalable gradient descent-style learning algorithms by developing suitable approximations for the challenging minimax loss functions of WDPO and KLDPO. Our empirical experiments using benchmark data sets and LLMs demonstrate the superior performance of WDPO and KLDPO in substantially improving the alignment when there is a preference distribution shift.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BILBO: BILevel Bayesian Optimization</title>
<link>https://arxiv.org/abs/2502.02121</link>
<guid>https://arxiv.org/abs/2502.02121</guid>
<content:encoded><![CDATA[
arXiv:2502.02121v2 Announce Type: replace 
Abstract: Bilevel optimization is characterized by a two-level optimization structure, where the upper-level problem is constrained by optimal lower-level solutions, and such structures are prevalent in real-world problems. The constraint by optimal lower-level solutions poses significant challenges, especially in noisy, constrained, and derivative-free settings, as repeating lower-level optimizations is sample inefficient and predicted lower-level solutions may be suboptimal. We present BILevel Bayesian Optimization (BILBO), a novel Bayesian optimization algorithm for general bilevel problems with blackbox functions, which optimizes both upper- and lower-level problems simultaneously, without the repeated lower-level optimization required by existing methods. BILBO samples from confidence-bounds based trusted sets, which bounds the suboptimality on the lower level. Moreover, BILBO selects only one function query per iteration, where the function query selection strategy incorporates the uncertainty of estimated lower-level solutions and includes a conditional reassignment of the query to encourage exploration of the lower-level objective. The performance of BILBO is theoretically guaranteed with a sublinear regret bound for commonly used kernels and is empirically evaluated on several synthetic and real-world problems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGNet: Reciprocal Space-Aware Long-Range Modeling for Crystalline Property Prediction</title>
<link>https://arxiv.org/abs/2502.02748</link>
<guid>https://arxiv.org/abs/2502.02748</guid>
<content:encoded><![CDATA[
arXiv:2502.02748v2 Announce Type: replace 
Abstract: Predicting properties of crystals from their structures is a fundamental yet challenging task in materials science. Unlike molecules, crystal structures exhibit infinite periodic arrangements of atoms, requiring methods capable of capturing both local and global information effectively. However, most current works fall short of capturing long-range interactions within periodic structures. To address this limitation, we leverage \emph{reciprocal space} to efficiently encode long-range interactions with learnable filters within Fourier transforms. We introduce Reciprocal Geometry Network (ReGNet), a novel architecture that integrates geometric GNNs and reciprocal blocks to model short-range and long-range interactions, respectively. Experimental results on JARVIS, Materials Project, and MatBench demonstrate that ReGNet achieves state-of-the-art predictive accuracy across a range of crystal property prediction tasks. Additionally, we explore a model extension that employs the mixture-of-experts for multi-property prediction with promising results and high computational efficiency. These findings highlight the potential of our model as a scalable and accurate solution for crystal property prediction. The code will be released upon paper acceptance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Reward Alignment via Hypothesis Space Batch Cutting</title>
<link>https://arxiv.org/abs/2502.02921</link>
<guid>https://arxiv.org/abs/2502.02921</guid>
<content:encoded><![CDATA[
arXiv:2502.02921v3 Announce Type: replace 
Abstract: Reward design in reinforcement learning and optimal control is challenging. Preference-based alignment addresses this by enabling agents to learn rewards from ranked trajectory pairs provided by humans. However, existing methods often struggle from poor robustness to unknown false human preferences. In this work, we propose a robust and efficient reward alignment method based on a novel and geometrically interpretable perspective: hypothesis space batched cutting. Our method iteratively refines the reward hypothesis space through "cuts" based on batches of human preferences. Within each batch, human preferences, queried based on disagreement, are grouped using a voting function to determine the appropriate cut, ensuring a bounded human query complexity. To handle unknown erroneous preferences, we introduce a conservative cutting method within each batch, preventing erroneous human preferences from making overly aggressive cuts to the hypothesis space. This guarantees provable robustness against false preferences, while eliminating the need to explicitly identify them. We evaluate our method in a model predictive control setting across diverse tasks. The results demonstrate that our framework achieves comparable or superior performance to state-of-the-art methods in error-free settings while significantly outperforming existing methods when handling a high percentage of erroneous human preferences.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction of the Most Fire-Sensitive Point in Building Structures with Differentiable Agents for Thermal Simulators</title>
<link>https://arxiv.org/abs/2502.03424</link>
<guid>https://arxiv.org/abs/2502.03424</guid>
<content:encoded><![CDATA[
arXiv:2502.03424v4 Announce Type: replace 
Abstract: Fire safety is crucial for ensuring the stability of building structures, yet evaluating whether a structure meets fire safety requirement is challenging. Fires can originate at any point within a structure, and simulating every potential fire scenario is both expensive and time-consuming. To address this challenge, we propose the concept of the Most Fire-Sensitive Point (MFSP) and an efficient machine learning framework for its identification. The MFSP is defined as the location at which a fire, if initiated, would cause the most severe detrimental impact on the building's stability, effectively representing the worst-case fire scenario. In our framework, a Graph Neural Network (GNN) serves as an efficient and differentiable agent for conventional Finite Element Analysis (FEA) simulators by predicting the Maximum Interstory Drift Ratio (MIDR) under fire, which then guides the training and evaluation of the MFSP predictor. Additionally, we enhance our framework with a novel edge update mechanism and a transfer learning-based training scheme. Evaluations on a large-scale simulation dataset demonstrate the good performance of the proposed framework in identifying the MFSP, offering a transformative tool for optimizing fire safety assessments in structural design. All developed datasets and codes are open-sourced online.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path Planning for Masked Diffusion Model Sampling</title>
<link>https://arxiv.org/abs/2502.03540</link>
<guid>https://arxiv.org/abs/2502.03540</guid>
<content:encoded><![CDATA[
arXiv:2502.03540v4 Announce Type: replace 
Abstract: Any order generation of discrete data using masked diffusion models (MDMs) offers a compelling alternative to traditional autoregressive models, especially in domains that lack a natural causal ordering of data. However, current popular MDMs depart from their successful continuous diffusion model counterparts with simplified masked inference wherein unmasked tokens cannot be iteratively refined -- even if there is a mistake. In this paper, we extract the full power of MDMs by introducing a novel inference sampling strategy termed Path Planning (P2) that decomposes each generation step into two sub-stages: planning and denoising. Under P2, the planner at every step selects appropriate tokens that are marked to be updated, which can then be sampled using the denoiser. We demonstrate that P2 generalizes all existing sampling strategies for MDMs and critically enhances generative quality through the new capability of refining and updating existing unmasked tokens. We theoretically prove that P2 establishes a (new) expanded evidence lower bound (ELBO) on the log marginal likelihood of data. We instantiate P2 with a family of planners including: 1.) Self-Planning, 2.) BERT-Planning, and 3.) Trained-Planning with a learned planner leading to SOTA generative performance for MDMs on a suite of domains. Specifically, solely using P2 inference, we observe relative improvements of 22% in protein sequence foldability, 8% in RNA sequence pLDDT, 4% in math reasoning, 68% in story generation (ROUGE score), and 33% in code generation for the challenging pass@1 metric.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Devil is in the Details: Density Guidance for Detail-Aware Generation with Flow Models</title>
<link>https://arxiv.org/abs/2502.05807</link>
<guid>https://arxiv.org/abs/2502.05807</guid>
<content:encoded><![CDATA[
arXiv:2502.05807v2 Announce Type: replace 
Abstract: Diffusion models have emerged as a powerful class of generative models, capable of producing high-quality images by mapping noise to a data distribution. However, recent findings suggest that image likelihood does not align with perceptual quality: high-likelihood samples tend to be smooth, while lower-likelihood ones are more detailed. Controlling sample density is thus crucial for balancing realism and detail. In this paper, we analyze an existing technique, Prior Guidance, which scales the latent code to influence image detail. We introduce score alignment, a condition that explains why this method works and show that it can be tractably checked for any continuous normalizing flow model. We then propose Density Guidance, a principled modification of the generative ODE that enables exact log-density control during sampling. Finally, we extend Density Guidance to stochastic sampling, ensuring precise log-density control while allowing controlled variation in structure or fine details. Our experiments demonstrate that these techniques provide fine-grained control over image detail without compromising sample quality. Code is available at https://github.com/Aalto-QuML/density-guidance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Diffusion for Certifiable Few-shot Transfer Learning</title>
<link>https://arxiv.org/abs/2502.06970</link>
<guid>https://arxiv.org/abs/2502.06970</guid>
<content:encoded><![CDATA[
arXiv:2502.06970v2 Announce Type: replace 
Abstract: In contemporary deep learning, a prevalent and effective workflow for solving low-data problems is adapting powerful pre-trained foundation models (FMs) to new tasks via parameter-efficient fine-tuning (PEFT). However, while empirically effective, the resulting solutions lack generalisation guarantees to certify their accuracy - which may be required for ethical or legal reasons prior to deployment in high-importance applications. In this paper we develop a novel transfer learning approach that is designed to facilitate non-vacuous learning theoretic generalisation guarantees for downstream tasks, even in the low-shot regime. Specifically, we first use upstream tasks to train a distribution over PEFT parameters. We then learn the downstream task by a sample-and-evaluate procedure -- sampling plausible PEFTs from the trained diffusion model and selecting the one with the highest likelihood on the downstream data. Crucially, this confines our model hypothesis to a finite set of PEFT samples. In contrast to the typical continuous hypothesis spaces of neural network weights, this facilitates tighter risk certificates. We instantiate our bound and show non-trivial generalization guarantees compared to existing learning approaches which lead to vacuous bounds in the low-shot regime.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outsourced diffusion sampling: Efficient posterior inference in latent spaces of generative models</title>
<link>https://arxiv.org/abs/2502.06999</link>
<guid>https://arxiv.org/abs/2502.06999</guid>
<content:encoded><![CDATA[
arXiv:2502.06999v2 Announce Type: replace 
Abstract: Any well-behaved generative model over a variable $\mathbf{x}$ can be expressed as a deterministic transformation of an exogenous ('outsourced') Gaussian noise variable $\mathbf{z}$: $\mathbf{x}=f_\theta(\mathbf{z})$. In such a model (\eg, a VAE, GAN, or continuous-time flow-based model), sampling of the target variable $\mathbf{x} \sim p_\theta(\mathbf{x})$ is straightforward, but sampling from a posterior distribution of the form $p(\mathbf{x}\mid\mathbf{y}) \propto p_\theta(\mathbf{x})r(\mathbf{x},\mathbf{y})$, where $r$ is a constraint function depending on an auxiliary variable $\mathbf{y}$, is generally intractable. We propose to amortize the cost of sampling from such posterior distributions with diffusion models that sample a distribution in the noise space ($\mathbf{z}$). These diffusion samplers are trained by reinforcement learning algorithms to enforce that the transformed samples $f_\theta(\mathbf{z})$ are distributed according to the posterior in the data space ($\mathbf{x}$). For many models and constraints, the posterior in noise space is smoother than in data space, making it more suitable for amortized inference. Our method enables conditional sampling under unconditional GAN, (H)VAE, and flow-based priors, comparing favorably with other inference methods. We demonstrate the proposed outsourced diffusion sampling in several experiments with large pretrained prior models: conditional image generation, reinforcement learning with human feedback, and protein structure generation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Do Not Fully Utilize Transformer's Representation Capacity</title>
<link>https://arxiv.org/abs/2502.09245</link>
<guid>https://arxiv.org/abs/2502.09245</guid>
<content:encoded><![CDATA[
arXiv:2502.09245v2 Announce Type: replace 
Abstract: In contrast to RNNs, which compress their history into a single hidden state, Transformers can attend to all past tokens directly. However, standard Transformers rely solely on the hidden state from the previous layer to represent the entire context. We show that this design choice induces representation collapse and degrades performance. To address this issue, we introduce Layer-Integrated Memory (LIMe), a lightweight extension that leverages existing key-value buffers and learns per-head, per-layer routing weights to integrate representations from all previous layers with negligible overhead. Through extensive experiments-including language modeling, synthetic reasoning benchmarks, and very deep architectures-LIMe consistently achieves faster convergence, lower perplexity per FLOP, and substantial accuracy improvements on synthetic tasks while preserving higher value-vector entropy and improved token separability. Finally, our analysis of the learned routing weights reveals systematic reuse of both local and long-distance features, demonstrating how LIMe mitigates collapse, unlocks richer representations without increasing hidden-state size, and points to promising directions for future research.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When do neural networks learn world models?</title>
<link>https://arxiv.org/abs/2502.09297</link>
<guid>https://arxiv.org/abs/2502.09297</guid>
<content:encoded><![CDATA[
arXiv:2502.09297v3 Announce Type: replace 
Abstract: Humans develop world models that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we present the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions -- even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra</title>
<link>https://arxiv.org/abs/2502.09571</link>
<guid>https://arxiv.org/abs/2502.09571</guid>
<content:encoded><![CDATA[
arXiv:2502.09571v2 Announce Type: replace 
Abstract: Mass spectrometry plays a fundamental role in elucidating the structures of unknown molecules and subsequent scientific discoveries. One formulation of the structure elucidation task is the conditional de novo generation of molecular structure given a mass spectrum. Toward a more accurate and efficient scientific discovery pipeline for small molecules, we present DiffMS, a formula-restricted encoder-decoder generative network that achieves state-of-the-art performance on this task. The encoder utilizes a transformer architecture and models mass spectra domain knowledge such as peak formulae and neutral losses, and the decoder is a discrete graph diffusion model restricted by the heavy-atom composition of a known chemical formula. To develop a robust decoder that bridges latent embeddings and molecular structures, we pretrain the diffusion decoder with fingerprint-structure pairs, which are available in virtually infinite quantities, compared to structure-spectrum pairs that number in the tens of thousands. Extensive experiments on established benchmarks show that DiffMS outperforms existing models on de novo molecule generation. We provide several ablations to demonstrate the effectiveness of our diffusion and pretraining approaches and show consistent performance scaling with increasing pretraining dataset size. DiffMS code is publicly available at https://github.com/coleygroup/DiffMS.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Markovian Discrete Diffusion with Causal Language Models</title>
<link>https://arxiv.org/abs/2502.09767</link>
<guid>https://arxiv.org/abs/2502.09767</guid>
<content:encoded><![CDATA[
arXiv:2502.09767v2 Announce Type: replace 
Abstract: Discrete diffusion models offer a flexible, controllable approach to structured sequence generation, yet they still lag behind causal language models in expressive power. A key limitation lies in their reliance on the Markovian assumption, which restricts each step to condition only on the current state, leading to potential uncorrectable error accumulation. In this paper, we introduce CaDDi, a discrete diffusion model that conditions on the entire generative trajectory, thereby lifting the Markov constraint and allowing the model to revisit and improve past states. By unifying sequential (causal) and temporal (diffusion) reasoning in a single non-Markovian transformer, CaDDi also treats standard causal language models as a special case and permits the direct reuse of pretrained LLM weights with no architectural changes. Empirically, CaDDi outperforms state-of-the-art discrete diffusion baselines on natural-language benchmarks, substantially narrowing the remaining gap to large autoregressive transformers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Empirical Bayes via Transformers</title>
<link>https://arxiv.org/abs/2502.09844</link>
<guid>https://arxiv.org/abs/2502.09844</guid>
<content:encoded><![CDATA[
arXiv:2502.09844v2 Announce Type: replace 
Abstract: This work applies modern AI tools (transformers) to solving one of the oldest statistical problems: Poisson means under empirical Bayes (Poisson-EB) setting. In Poisson-EB a high-dimensional mean vector $\theta$ (with iid coordinates sampled from an unknown prior $\pi$) is estimated on the basis of $X=\mathrm{Poisson}(\theta)$. A transformer model is pre-trained on a set of synthetically generated pairs $(X,\theta)$ and learns to do in-context learning (ICL) by adapting to unknown $\pi$. Theoretically, we show that a sufficiently wide transformer can achieve vanishing regret with respect to an oracle estimator who knows $\pi$ as dimension grows to infinity. Practically, we discover that already very small models (100k parameters) are able to outperform the best classical algorithm (non-parametric maximum likelihood, or NPMLE) both in runtime and validation loss, which we compute on out-of-distribution synthetic data as well as real-world datasets (NHL hockey, MLB baseball, BookCorpusOpen). Finally, by using linear probes, we confirm that the transformer's EB estimator appears to internally work differently from either NPMLE or Robbins' estimators.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closed-Form Training Dynamics Reveal Learned Features and Linear Structure in Word2Vec-like Models</title>
<link>https://arxiv.org/abs/2502.09863</link>
<guid>https://arxiv.org/abs/2502.09863</guid>
<content:encoded><![CDATA[
arXiv:2502.09863v2 Announce Type: replace 
Abstract: Self-supervised word embedding algorithms such as word2vec provide a minimal setting for studying representation learning in language modeling. We examine the quartic Taylor approximation of the word2vec loss around the origin, and we show that both the resulting training dynamics and the final performance on downstream tasks are empirically very similar to those of word2vec. Our main contribution is to analytically solve for both the gradient flow training dynamics and the final word embeddings in terms of only the corpus statistics and training hyperparameters. The solutions reveal that these models learn orthogonal linear subspaces one at a time, each one incrementing the effective rank of the embeddings until model capacity is saturated. Training on Wikipedia, we find that each of the top linear subspaces represents an interpretable topic-level concept. Finally, we apply our theory to describe how linear representations of more abstract semantic concepts emerge during training; these can be used to complete analogies via vector addition.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse KL vs. Forward KL</title>
<link>https://arxiv.org/abs/2502.11107</link>
<guid>https://arxiv.org/abs/2502.11107</guid>
<content:encoded><![CDATA[
arXiv:2502.11107v3 Announce Type: replace 
Abstract: As large language models advance toward superhuman performance, ensuring their alignment with human values and abilities grows increasingly complex. Weak-to-strong generalization offers a promising approach by leveraging predictions from weaker models to guide stronger systems, but its effectiveness could be constrained by the inherent noise and inaccuracies in these weak predictions. To address this, we propose a theoretically grounded approach that replaces forward KL divergence-whose mass-covering behavior risks overfitting to imperfect weak signals-with reverse KL divergence. Reverse KL divergence's zero-forcing effect prioritizes high-confidence predictions, effectively mitigating the influence of unreliable weak supervision. Theoretically, we extend existing bounds and derive tighter lower bounds for both forward and reverse KL divergence, establishing that reverse KL achieves at least comparable guarantees to forward KL. Notably, when a sufficiently pre-trained strong model is fine-tuned on the last linear layer, reverse KL guarantees that it outperforms its weak supervisor by the magnitude of their disagreement. Empirically, we demonstrate that reverse KL and reverse cross-entropy enable strong models to successfully outperform those trained with forward KL and standard cross-entropy across most settings, highlighting the practical advantages of these reverse losses.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections</title>
<link>https://arxiv.org/abs/2502.12170</link>
<guid>https://arxiv.org/abs/2502.12170</guid>
<content:encoded><![CDATA[
arXiv:2502.12170v2 Announce Type: replace 
Abstract: We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer .
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation</title>
<link>https://arxiv.org/abs/2502.14637</link>
<guid>https://arxiv.org/abs/2502.14637</guid>
<content:encoded><![CDATA[
arXiv:2502.14637v3 Announce Type: replace 
Abstract: Protein backbone generation plays a central role in de novo protein design and is significant for many biological and medical applications. Although diffusion and flow-based generative models provide potential solutions to this challenging task, they often generate proteins with undesired designability and suffer computational inefficiency. In this study, we propose a novel rectified quaternion flow (ReQFlow) matching method for fast and high-quality protein backbone generation. In particular, our method generates a local translation and a 3D rotation from random noise for each residue in a protein chain, which represents each 3D rotation as a unit quaternion and constructs its flow by spherical linear interpolation (SLERP) in an exponential format. We train the model by quaternion flow (QFlow) matching with guaranteed numerical stability and rectify the QFlow model to accelerate its inference and improve the designability of generated protein backbones, leading to the proposed ReQFlow model. Experiments show that ReQFlow achieves on-par performance in protein backbone generation while requiring much fewer sampling steps and significantly less inference time (e.g., being 37x faster than RFDiffusion and 63x faster than Genie2 when generating a backbone of length 300), demonstrating its effectiveness and efficiency. The code is available at https://github.com/AngxiaoYue/ReQFlow.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Inverse Problems with Deep Linear Neural Networks: Global Convergence Guarantees for Gradient Descent with Weight Decay</title>
<link>https://arxiv.org/abs/2502.15522</link>
<guid>https://arxiv.org/abs/2502.15522</guid>
<content:encoded><![CDATA[
arXiv:2502.15522v2 Announce Type: replace 
Abstract: Machine learning methods are commonly used to solve inverse problems, wherein an unknown signal must be estimated from few measurements generated via a known acquisition procedure. In particular, neural networks perform well empirically but have limited theoretical guarantees. In this work, we study an underdetermined linear inverse problem that admits several possible solution mappings. A standard remedy (e.g., in compressed sensing) establishing uniqueness of the solution mapping is to assume knowledge of latent low-dimensional structure in the source signal. We ask the following question: do deep neural networks adapt to this low-dimensional structure when trained by gradient descent with weight decay regularization? We prove that mildly overparameterized deep linear networks trained in this manner converge to an approximate solution that accurately solves the inverse problem while implicitly encoding latent subspace structure. To our knowledge, this is the first result to rigorously show that deep linear networks trained with weight decay automatically adapt to latent subspace structure in the data under practical stepsize and weight initialization schemes. Our work highlights that regularization and overparameterization improve generalization, while overparameterization also accelerates convergence during training.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life Prediction</title>
<link>https://arxiv.org/abs/2502.18807</link>
<guid>https://arxiv.org/abs/2502.18807</guid>
<content:encoded><![CDATA[
arXiv:2502.18807v3 Announce Type: replace 
Abstract: Battery Life Prediction (BLP), which relies on time series data produced by battery degradation tests, is crucial for battery utilization, optimization, and production. Despite impressive advancements, this research area faces three key challenges. Firstly, the limited size of existing datasets impedes insights into modern battery life data. Secondly, most datasets are restricted to small-capacity lithium-ion batteries tested under a narrow range of diversity in labs, raising concerns about the generalizability of findings. Thirdly, inconsistent and limited benchmarks across studies obscure the effectiveness of baselines and leave it unclear if models popular in other time series fields are effective for BLP. To address these challenges, we propose BatteryLife, a comprehensive dataset and benchmark for BLP. BatteryLife integrates 16 datasets, offering a 2.5 times sample size compared to the previous largest dataset, and provides the most diverse battery life resource with batteries from 8 formats, 59 chemical systems, 9 operating temperatures, and 421 charge/discharge protocols, including both laboratory and industrial tests. Notably, BatteryLife is the first to release battery life datasets of zinc-ion batteries, sodium-ion batteries, and industry-tested large-capacity lithium-ion batteries. With the comprehensive dataset, we revisit the effectiveness of baselines popular in this and other time series fields. Furthermore, we propose CyclePatch, a plug-in technique that can be employed in various neural networks. Extensive benchmarking of 18 methods reveals that models popular in other time series fields can be unsuitable for BLP, and CyclePatch consistently improves model performance establishing state-of-the-art benchmarks. Moreover, BatteryLife evaluates model performance across aging conditions and domains. BatteryLife is available at https://github.com/Ruifeng-Tan/BatteryLife.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Multi-modal Time Series Prediction with LLM-in-the-Loop</title>
<link>https://arxiv.org/abs/2503.01013</link>
<guid>https://arxiv.org/abs/2503.01013</guid>
<content:encoded><![CDATA[
arXiv:2503.01013v2 Announce Type: replace 
Abstract: Time series analysis provides essential insights for real-world system dynamics and informs downstream decision-making, yet most existing methods often overlook the rich contextual signals present in auxiliary modalities. To bridge this gap, we introduce TimeXL, a multi-modal prediction framework that integrates a prototype-based time series encoder with three collaborating Large Language Models (LLMs) to deliver more accurate predictions and interpretable explanations. First, a multi-modal prototype-based encoder processes both time series and textual inputs to generate preliminary forecasts alongside case-based rationales. These outputs then feed into a prediction LLM, which refines the forecasts by reasoning over the encoder's predictions and explanations. Next, a reflection LLM compares the predicted values against the ground truth, identifying textual inconsistencies or noise. Guided by this feedback, a refinement LLM iteratively enhances text quality and triggers encoder retraining. This closed-loop workflow -- prediction, critique (reflect), and refinement -- continuously boosts the framework's performance and interpretability. Empirical evaluations on four real-world datasets demonstrate that TimeXL achieves up to 8.9\% improvement in AUC and produces human-centric, multi-modal explanations, highlighting the power of LLM-driven reasoning for time series prediction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wanda++: Pruning Large Language Models via Regional Gradients</title>
<link>https://arxiv.org/abs/2503.04992</link>
<guid>https://arxiv.org/abs/2503.04992</guid>
<content:encoded><![CDATA[
arXiv:2503.04992v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal accuracy impact. However, existing methods often suffer from accuracy degradation without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Moreover, despite updating weights with regional optimization, Wanda++ remains orthogonal to sparsity-aware fine-tuning, further reducing perplexity with LoRA in great extend. Our approach is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single H100 GPU.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Visualizations of Data Spaces for Classification Problems</title>
<link>https://arxiv.org/abs/2503.05861</link>
<guid>https://arxiv.org/abs/2503.05861</guid>
<content:encoded><![CDATA[
arXiv:2503.05861v2 Announce Type: replace 
Abstract: How do classification models "see" our data? Based on their success in delineating behaviors, there must be some lens through which it is easy to see the boundary between classes; however, our current set of visualization techniques makes this prospect difficult. In this work, we propose a hybrid supervised-unsupervised technique distinctly suited to visualizing the decision boundaries determined by classification problems. This method provides a human-interpretable map that can be analyzed qualitatively and quantitatively, which we demonstrate through visualizing and interpreting a decision boundary for chemical neurotoxicity. While we discuss this method in the context of chemistry-driven problems, its application can be generalized across subfields for "unboxing" the operations of machine-learning classification models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss, Dynamics, and Success Amplification</title>
<link>https://arxiv.org/abs/2503.06639</link>
<guid>https://arxiv.org/abs/2503.06639</guid>
<content:encoded><![CDATA[
arXiv:2503.06639v3 Announce Type: replace 
Abstract: Group Relative Policy Optimization (GRPO) was introduced recently and used successfully to train DeepSeek-R1 models for promoting reasoning capabilities of LLMs using verifiable or binary rewards. We show in this paper that GRPO with verifiable rewards can be written as a Kullback--Leibler (KL) regularized contrastive loss, where the contrastive samples are synthetic data sampled from the old policy. The optimal GRPO policy $\pi_{n}$ can be expressed explicitly in terms of the binary reward, as well as the first- and second-order statistics of the old policy ($\pi_{n-1}$) and the reference policy $\pi_{\text{ref}}$. Iterating this scheme, we obtain a sequence of policies $\pi_{n}$ for which we can quantify the probability of success $p_n$. We show that the probability of success of the policy satisfies a recurrence that converges to a fixed point of a function that depends on the initial probability of success $p_{\text{ref}}$ and the regularization parameter $\beta$ of the $KL$ regularizer. We show that the fixed point $p^*$ is guaranteed to be larger than $p_{\text{ref}}$, thereby demonstrating that GRPO effectively amplifies the probability of success of the policy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster</title>
<link>https://arxiv.org/abs/2503.07649</link>
<guid>https://arxiv.org/abs/2503.07649</guid>
<content:encoded><![CDATA[
arXiv:2503.07649v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) and Foundation Models (FMs) have recently become prevalent for time series forecasting tasks. While fine-tuning LLMs enables domain adaptation, they often struggle to generalize across diverse and unseen datasets. Moreover, existing Time Series Foundation Models (TSFMs) still face challenges in handling non-stationary dynamics and distribution shifts, largely due to the lack of effective mechanisms for adaptation. To this end, we present TS-RAG, a retrieval-augmented generation framework for time series forecasting that enhances the generalization and interpretability of TSFMs. Specifically, TS-RAG leverages pre-trained time series encoders to retrieve semantically relevant segments from a dedicated knowledge base, enriching the contextual representation of the input query. Furthermore, we propose an Adaptive Retrieval Mixer (ARM) module that dynamically fuses the retrieved patterns with the TSFM's internal representation, improving forecasting accuracy without requiring task-specific fine-tuning. Thorough empirical studies on seven public benchmark datasets demonstrate that TS-RAG achieves state-of-the-art zero-shot forecasting performance, outperforming the existing TSFMs by up to 6.84% across diverse domains while also providing desirable interpretability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Enhanced Representation Learning for Single-Cell Transcriptomics</title>
<link>https://arxiv.org/abs/2503.09427</link>
<guid>https://arxiv.org/abs/2503.09427</guid>
<content:encoded><![CDATA[
arXiv:2503.09427v3 Announce Type: replace 
Abstract: Single-cell RNA sequencing (scRNA-seq) offers detailed insights into cellular heterogeneity. Recent advancements leverage single-cell large language models (scLLMs) for effective representation learning. These models focus exclusively on transcriptomic data, neglecting complementary biological knowledge from textual descriptions. To overcome this limitation, we propose scMMGPT, a novel multimodal framework designed for language-enhanced representation learning in single-cell transcriptomics. Unlike existing methods, scMMGPT employs robust cell representation extraction, preserving quantitative gene expression data, and introduces an innovative two-stage pre-training strategy combining discriminative precision with generative flexibility. Extensive experiments demonstrate that scMMGPT significantly outperforms unimodal and multimodal baselines across key downstream tasks, including cell annotation and clustering, and exhibits superior generalization in out-of-distribution scenarios.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Linear Regression Demystified: Training Dynamics and Mechanistic Interpretability of Multi-Head Softmax Attention</title>
<link>https://arxiv.org/abs/2503.12734</link>
<guid>https://arxiv.org/abs/2503.12734</guid>
<content:encoded><![CDATA[
arXiv:2503.12734v2 Announce Type: replace 
Abstract: We study how multi-head softmax attention models are trained to perform in-context learning on linear data. Through extensive empirical experiments and rigorous theoretical analysis, we demystify the emergence of elegant attention patterns: a diagonal and homogeneous pattern in the key-query (KQ) weights, and a last-entry-only and zero-sum pattern in the output-value (OV) weights. Remarkably, these patterns consistently appear from gradient-based training starting from random initialization. Our analysis reveals that such emergent structures enable multi-head attention to approximately implement a debiased gradient descent predictor -- one that outperforms single-head attention and nearly achieves Bayesian optimality up to proportional factor. Furthermore, compared to linear transformers, the softmax attention readily generalizes to sequences longer than those seen during training. We also extend our study to scenarios with anisotropic covariates and multi-task linear regression. In the former, multi-head attention learns to implement a form of pre-conditioned gradient descent. In the latter, we uncover an intriguing regime where the interplay between head number and task number triggers a superposition phenomenon that efficiently resolves multi-task in-context learning. Our results reveal that in-context learning ability emerges from the trained transformer as an aggregated effect of its architecture and the underlying data distribution, paving the way for deeper understanding and broader applications of in-context learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiScale Contextual Bandits for Long Term Objectives</title>
<link>https://arxiv.org/abs/2503.17674</link>
<guid>https://arxiv.org/abs/2503.17674</guid>
<content:encoded><![CDATA[
arXiv:2503.17674v2 Announce Type: replace 
Abstract: The feedback that AI systems (e.g., recommender systems, chatbots) collect from user interactions is a crucial source of training data. While short-term feedback (e.g., clicks, engagement) is widely used for training, there is ample evidence that optimizing short-term feedback does not necessarily achieve the desired long-term objectives. Unfortunately, directly optimizing for long-term objectives is challenging, and we identify the disconnect in the timescales of short-term interventions (e.g., rankings) and the long-term feedback (e.g., user retention) as one of the key obstacles. To overcome this disconnect, we introduce the framework of MultiScale Policy Learning to contextually reconcile that AI systems need to act and optimize feedback at multiple interdependent timescales. Following a PAC-Bayes motivation, we show how the lower timescales with more plentiful data can provide a data-dependent hierarchical prior for faster learning at higher scales, where data is more scarce. As a result, the policies at all levels effectively optimize for the long-term. We instantiate the framework with MultiScale Off-Policy Bandit Learning (MSBL) and demonstrate its effectiveness on three tasks relating to recommender and conversational systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language Models to Unverifiable Data</title>
<link>https://arxiv.org/abs/2503.19618</link>
<guid>https://arxiv.org/abs/2503.19618</guid>
<content:encoded><![CDATA[
arXiv:2503.19618v2 Announce Type: replace 
Abstract: We propose to scale RL to unverifiable data with a novel algorithm JEPO (Jensen's Evidence lower bound Policy Optimization). While most prior efforts on scaling RL for LLMs focus on verifiable data where ground truth answers are typically short-form and can be matched easily; we investigate the case where such assumptions are less valid (e.g., when answers are long-form such as mathematical proofs). To scale RL training to unverifiable data with contemporary training constraints, we propose JEPO. JEPO applies Jensen's evidence lower bound, a pragmatic simplification of the evidence lower bound which views chain-of-thought as a latent variable in the generative process. We show that on verifiable data (math), JEPO is as effective as RL with verifiable rewards; on semi-verifiable data (numina), JEPO improves on soft-match based evaluations compared to RL with verifiable rewards which can only leverage a subset of the data source; finally, on unverifiable data (numina-proof), JEPO outperforms SFT and a few ablation baselines on likelihood evaluations.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving User Behavior Prediction: Leveraging Annotator Metadata in Supervised Machine Learning Models</title>
<link>https://arxiv.org/abs/2503.21000</link>
<guid>https://arxiv.org/abs/2503.21000</guid>
<content:encoded><![CDATA[
arXiv:2503.21000v2 Announce Type: replace 
Abstract: Supervised machine-learning models often underperform in predicting user behaviors from conversational text, hindered by poor crowdsourced label quality and low NLP task accuracy. We introduce the Metadata-Sensitive Weighted-Encoding Ensemble Model (MSWEEM), which integrates annotator meta-features like fatigue and speeding. First, our results show MSWEEM outperforms standard ensembles by 14% on held-out data and 12% on an alternative dataset. Second, we find that incorporating signals of annotator behavior, such as speed and fatigue, significantly boosts model performance. Third, we find that annotators with higher qualifications, such as Master's, deliver more consistent and faster annotations. Given the increasing uncertainty over annotation quality, our experiments show that understanding annotator patterns is crucial for enhancing model accuracy in user behavior prediction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of the impact of expert knowledge: How decision support scores impact the effectiveness of automatic knowledge-driven feature engineering (aKDFE)</title>
<link>https://arxiv.org/abs/2504.05928</link>
<guid>https://arxiv.org/abs/2504.05928</guid>
<content:encoded><![CDATA[
arXiv:2504.05928v2 Announce Type: replace 
Abstract: Adverse Drug Events (ADEs), harmful medication effects, pose significant healthcare challenges, impacting patient safety and costs. This study evaluates automatic Knowledge-Driven Feature Engineering (aKDFE) for improved ADE prediction from Electronic Health Record (EHR) data, comparing it with automated event-based Knowledge Discovery in Databases (KDD). We investigated how incorporating domain-specific ADE risk scores for prolonged heart QT interval, extracted from the Janusmed Riskprofile (Janusmed) Clinical Decision Support System (CDSS), affects prediction performance using EHR data and medication handling events. Results indicate that, while aKDFE step 1 (event-based feature generation) alone did not significantly improve ADE prediction performance, aKDFE step 2 (patient-centric transformation) enhances the prediction performance. High Area Under the Receiver Operating Characteristic curve (AUROC) values suggest strong feature correlations to the outcome, aligning with the predictive power of patients' prior healthcare history for ADEs. Statistical analysis did not confirm that incorporating the Janusmed information (i) risk scores and (ii) medication route of administration into the model's feature set enhanced predictive performance. However, the patient-centric transformation applied by aKDFE proved to be a highly effective feature engineering approach. Limitations include a single-project focus, potential bias from machine learning pipeline methods, and reliance on AUROC. In conclusion, aKDFE, particularly with patient-centric transformation, improves ADE prediction from EHR data. Future work will explore attention-based models, event feature sequences, and automatic methods for incorporating domain knowledge into the aKDFE framework.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching</title>
<link>https://arxiv.org/abs/2504.11713</link>
<guid>https://arxiv.org/abs/2504.11713</guid>
<content:encoded><![CDATA[
arXiv:2504.11713v3 Announce Type: replace 
Abstract: We introduce Adjoint Sampling, a highly scalable and efficient algorithm for learning diffusion processes that sample from unnormalized densities, or energy functions. It is the first on-policy approach that allows significantly more gradient updates than the number of energy evaluations and model samples, allowing us to scale to much larger problem settings than previously explored by similar methods. Our framework is theoretically grounded in stochastic optimal control and shares the same theoretical guarantees as Adjoint Matching, being able to train without the need for corrective measures that push samples towards the target distribution. We show how to incorporate key symmetries, as well as periodic boundary conditions, for modeling molecules in both cartesian and torsional coordinates. We demonstrate the effectiveness of our approach through extensive experiments on classical energy functions, and further scale up to neural network-based energy models where we perform amortized conformer generation across many molecular systems. To encourage further research in developing highly scalable sampling methods, we plan to open source these challenging benchmarks, where successful methods can directly impact progress in computational chemistry.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large Language Models on Graph-theoretic Tasks</title>
<link>https://arxiv.org/abs/2504.12764</link>
<guid>https://arxiv.org/abs/2504.12764</guid>
<content:encoded><![CDATA[
arXiv:2504.12764v3 Announce Type: replace 
Abstract: This paper introduces GraphOmni, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs on graph-theoretic tasks articulated in natural language. GraphOmni encompasses diverse graph types, serialization formats, and prompting schemes, significantly exceeding prior efforts in both scope and depth. Through extensive systematic evaluation, we identify critical interactions among these dimensions, demonstrating their substantial impact on model performance. Our experiments reveal that state-of-the-art models like Claude-3.5 and o4-mini consistently outperform other models, yet even these leading models exhibit substantial room for improvement. Performance variability is evident depending on the specific combinations of factors we considered, underscoring the necessity of comprehensive evaluations across these interconnected dimensions. Additionally, we observe distinct impacts of serialization and prompting strategies between open-source and closed-source models, encouraging the development of tailored approaches. Motivated by the findings, we also propose a reinforcement learning-inspired framework that adaptively selects the optimal factors influencing LLM reasoning capabilities. This flexible and extendable benchmark not only deepens our understanding of LLM performance on structured tasks but also provides a robust foundation for advancing research in LLM-based graph reasoning. The code and datasets are available at https://github.com/GAI-Community/GraphOmni.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization</title>
<link>https://arxiv.org/abs/2504.18026</link>
<guid>https://arxiv.org/abs/2504.18026</guid>
<content:encoded><![CDATA[
arXiv:2504.18026v2 Announce Type: replace 
Abstract: Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI systems by constraining their decisions on a set of human understandable concepts. However, CBMs typically assume that datasets contains accurate concept labels an assumption often violated in practice, which we show can significantly degrade performance (by 25% in some cases). To address this, we introduce the Concept Preference Optimization (CPO) objective, a new loss function based on Direct Preference Optimization, which effectively mitigates the negative impact of concept mislabeling on CBM performance. We provide an analysis on some key properties of the CPO objective showing it directly optimizes for the concept's posterior distribution, and contrast it against Binary Cross Entropy (BCE) where we show CPO is inherently less sensitive to concept noise. We empirically confirm our analysis finding that CPO consistently outperforms BCE in three real world datasets with and without added label noise.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Informed Neural Operator Transformer</title>
<link>https://arxiv.org/abs/2504.19452</link>
<guid>https://arxiv.org/abs/2504.19452</guid>
<content:encoded><![CDATA[
arXiv:2504.19452v3 Announce Type: replace 
Abstract: Machine-learning-based surrogate models offer significant computational efficiency and faster simulations compared to traditional numerical methods, especially for problems requiring repeated evaluations of partial differential equations. This work introduces the Geometry-Informed Neural Operator Transformer (GINOT), which integrates the transformer architecture with the neural operator framework to enable forward predictions for arbitrary geometries. GINOT encodes the surface points cloud of a geometry using a sampling and grouping mechanism combined with an attention mechanism, ensuring invariance to point order and padding while maintaining robustness to variations in point density. The geometry information is seamlessly integrated with query points in the solution decoder through the attention mechanism. The performance of GINOT is validated on multiple challenging datasets, showcasing its high accuracy and strong generalization capabilities for complex and arbitrary 2D and 3D geometries.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model</title>
<link>https://arxiv.org/abs/2505.05082</link>
<guid>https://arxiv.org/abs/2505.05082</guid>
<content:encoded><![CDATA[
arXiv:2505.05082v3 Announce Type: replace 
Abstract: Generative modeling of non-negative, discrete data, such as symbolic music, remains challenging due to two persistent limitations in existing methods. Firstly, many approaches rely on modeling continuous embeddings, which is suboptimal for inherently discrete data distributions. Secondly, most models optimize variational bounds rather than exact data likelihood, resulting in inaccurate likelihood estimates and degraded sampling quality. While recent diffusion-based models have addressed these issues separately, we tackle them jointly. In this work, we introduce the Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM), inspired by photon arrival process, which combines exact likelihood estimation with fully discrete-state modeling. Central to our approach is an information-theoretic Poisson Reconstruction Loss (PRL) that has a provable exact relationship with the true data likelihood. ItDPDM achieves improved likelihood and sampling performance over prior discrete and continuous diffusion models on a variety of synthetic discrete datasets. Furthermore, on real-world datasets such as symbolic music and images, ItDPDM attains superior likelihood estimates and competitive generation quality-demonstrating a proof of concept for distribution-robust discrete generative modeling.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Thought Machines</title>
<link>https://arxiv.org/abs/2505.05522</link>
<guid>https://arxiv.org/abs/2505.05522</guid>
<content:encoded><![CDATA[
arXiv:2505.05522v3 Announce Type: replace 
Abstract: Biological brains demonstrate complex neural activity, where the timing and interplay between neurons is critical to how brains process information. Most deep learning architectures simplify neural activity by abstracting away temporal dynamics. In this paper we challenge that paradigm. By incorporating neuron-level processing and synchronization, we can effectively reintroduce neural timing as a foundational element. We present the Continuous Thought Machine (CTM), a model designed to leverage neural dynamics as its core representation. The CTM has two core innovations: (1) neuron-level temporal processing, where each neuron uses unique weight parameters to process a history of incoming signals; and (2) neural synchronization employed as a latent representation. The CTM aims to strike a balance between oversimplified neuron abstractions that improve computational efficiency, and biological realism. It operates at a level of abstraction that effectively captures essential temporal dynamics while remaining computationally tractable for deep learning. We demonstrate the CTM's strong performance and versatility across a range of challenging tasks, including ImageNet-1K classification, solving 2D mazes, sorting, parity computation, question-answering, and RL tasks. Beyond displaying rich internal representations and offering a natural avenue for interpretation owing to its internal process, the CTM is able to perform tasks that require complex sequential reasoning. The CTM can also leverage adaptive compute, where it can stop earlier for simpler tasks, or keep computing when faced with more challenging instances. The goal of this work is to share the CTM and its associated innovations, rather than pushing for new state-of-the-art results. To that end, we believe the CTM represents a significant step toward developing more biologically plausible and powerful artificial intelligence systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Causal Direction via Variational Bayesian Compression</title>
<link>https://arxiv.org/abs/2505.07503</link>
<guid>https://arxiv.org/abs/2505.07503</guid>
<content:encoded><![CDATA[
arXiv:2505.07503v3 Announce Type: replace 
Abstract: Telling apart the cause and effect between two random variables with purely observational data is a challenging problem that finds applications in various scientific disciplines. A key principle utilized in this task is the algorithmic Markov condition, which postulates that the joint distribution, when factorized according to the causal direction, yields a more succinct codelength compared to the anti-causal direction. Previous approaches approximate these codelengths by relying on simple functions or Gaussian processes (GPs) with easily evaluable complexity, compromising between model fitness and computational complexity. To overcome these limitations, we propose leveraging the variational Bayesian learning of neural networks as an interpretation of the codelengths. Consequently, we can enhance the model fitness while promoting the succinctness of the codelengths, while avoiding the significant computational complexity of the GP-based approaches. Extensive experiments on both synthetic and real-world benchmarks in cause-effect identification demonstrate the effectiveness of our proposed method, surpassing the overall performance of related complexity-based and structural causal model regression-based approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning</title>
<link>https://arxiv.org/abs/2505.11953</link>
<guid>https://arxiv.org/abs/2505.11953</guid>
<content:encoded><![CDATA[
arXiv:2505.11953v2 Announce Type: replace 
Abstract: Loss reweighting has shown significant benefits for machine unlearning with large language models (LLMs). However, their exact functionalities are left unclear and the optimal strategy remains an open question, thus impeding the understanding and improvement of existing methodologies. In this paper, we identify two distinct goals of loss reweighting, namely, Saturation and Importance -- the former indicates that those insufficiently optimized data should be emphasized, while the latter stresses some critical data that are most influential for loss minimization. To study their usefulness, we design specific reweighting strategies for each goal and evaluate their respective effects on unlearning. We conduct extensive empirical analyses on well-established benchmarks, and summarize some important observations as follows: (i) Saturation enhances efficacy more than importance-based reweighting, and their combination can yield additional improvements. (ii) Saturation typically allocates lower weights to data with lower likelihoods, whereas importance-based reweighting does the opposite. (iii) The efficacy of unlearning is also largely influenced by the smoothness and granularity of the weight distributions. Based on these findings, we propose SatImp, a simple reweighting method that combines the advantages of both saturation and importance. Empirical results on extensive datasets validate the efficacy of our method, potentially bridging existing research gaps and indicating directions for future research. Our code is available at https://github.com/tmlr-group/SatImp.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backdoors in DRL: Four Environments Focusing on In-distribution Triggers</title>
<link>https://arxiv.org/abs/2505.17248</link>
<guid>https://arxiv.org/abs/2505.17248</guid>
<content:encoded><![CDATA[
arXiv:2505.17248v2 Announce Type: replace 
Abstract: Backdoor attacks, or trojans, pose a security risk by concealing undesirable behavior in deep neural network models. Open-source neural networks are downloaded from the internet daily, possibly containing backdoors, and third-party model developers are common. To advance research on backdoor attack mitigation, we develop several trojans for deep reinforcement learning (DRL) agents. We focus on in-distribution triggers, which occur within the agent's natural data distribution, since they pose a more significant security threat than out-of-distribution triggers due to their ease of activation by the attacker during model deployment. We implement backdoor attacks in four reinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld, Colorful Memory, and Modified Safety Gymnasium. We train various models, both clean and backdoored, to characterize these attacks. We find that in-distribution triggers can require additional effort to implement and be more challenging for models to learn, but are nevertheless viable threats in DRL even using basic data poisoning attacks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models</title>
<link>https://arxiv.org/abs/2505.17773</link>
<guid>https://arxiv.org/abs/2505.17773</guid>
<content:encoded><![CDATA[
arXiv:2505.17773v2 Announce Type: replace 
Abstract: Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning large language models (LLMs), but it often produces overconfident predictions in data-scarce few-shot settings. To address this issue, several classical statistical learning approaches have been repurposed for scalable uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input characteristics affect the predictive uncertainty estimates. To address this limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a novel uncertainty-aware and parameter efficient fine-tuning approach, by developing new lightweight LoRA modules contextualized to each input data sample to dynamically adapt uncertainty estimates. Incorporating data-driven contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves well-calibrated uncertainties, and yields robust predictions. Extensive experiments demonstrate that C-LoRA consistently outperforms the state-of-the-art uncertainty-aware LoRA methods in both uncertainty quantification and model generalization. Ablation studies further confirm the critical role of our contextual modules in capturing sample-specific uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot regimes.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Supervised Learning and Reinforcement Learning in Math Reasoning</title>
<link>https://arxiv.org/abs/2505.18116</link>
<guid>https://arxiv.org/abs/2505.18116</guid>
<content:encoded><![CDATA[
arXiv:2505.18116v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has played a central role in the recent surge of LLMs' math abilities by enabling self-improvement through binary verifier signals. In contrast, Supervised Learning (SL) is rarely considered for such verification-driven training, largely due to its heavy reliance on reference answers and inability to reflect on mistakes. In this work, we challenge the prevailing notion that self-improvement is exclusive to RL and propose Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to reflect on their failures and improve autonomously with no external teachers. In online training, instead of throwing away self-generated negative answers, NFT constructs an implicit negative policy to model them. This implicit policy is parameterized with the same positive LLM we target to optimize on positive data, enabling direct policy optimization on all LLMs' generations. We conduct experiments on 7B and 32B models in math reasoning tasks. Results consistently show that through the additional leverage of negative feedback, NFT significantly improves over SL baselines like Rejection sampling Fine-Tuning, matching or even surpassing leading RL algorithms like GRPO and DAPO. Furthermore, we demonstrate that NFT and GRPO are actually equivalent in strict-on-policy training, even though they originate from entirely different theoretical foundations. Our experiments and theoretical findings bridge the gap between SL and RL methods in binary-feedback learning systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning in Stackelberg Games with Non-myopic Agents</title>
<link>https://arxiv.org/abs/2208.09407</link>
<guid>https://arxiv.org/abs/2208.09407</guid>
<content:encoded><![CDATA[
arXiv:2208.09407v3 Announce Type: replace-cross 
Abstract: We study Stackelberg games where a principal repeatedly interacts with a non-myopic long-lived agent, without knowing the agent's payoff function. Although learning in Stackelberg games is well-understood when the agent is myopic, dealing with non-myopic agents poses additional complications. In particular, non-myopic agents may strategize and select actions that are inferior in the present in order to mislead the principal's learning algorithm and obtain better outcomes in the future.
  We provide a general framework that reduces learning in presence of non-myopic agents to robust bandit optimization in the presence of myopic agents. Through the design and analysis of minimally reactive bandit algorithms, our reduction trades off the statistical efficiency of the principal's learning algorithm against its effectiveness in inducing near-best-responses. We apply this framework to Stackelberg security games (SSGs), pricing with unknown demand curve, general finite Stackelberg games, and strategic classification. In each setting, we characterize the type and impact of misspecifications present in near-best responses and develop a learning algorithm robust to such misspecifications.
  On the way, we improve the state-of-the-art query complexity of learning in SSGs with $n$ targets from $O(n^3)$ to a near-optimal $\widetilde{O}(n)$ by uncovering a fundamental structural property of these games. The latter result is of independent interest beyond learning with non-myopic agents.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Variance of Differentially Private Randomized Experiments through Clustering</title>
<link>https://arxiv.org/abs/2308.00957</link>
<guid>https://arxiv.org/abs/2308.00957</guid>
<content:encoded><![CDATA[
arXiv:2308.00957v3 Announce Type: replace-cross 
Abstract: Estimating causal effects from randomized experiments is only possible if participants are willing to disclose their potentially sensitive responses. Differential privacy, a widely used framework for ensuring an algorithms privacy guarantees, can encourage participants to share their responses without the risk of de-anonymization. However, many mechanisms achieve differential privacy by adding noise to the original dataset, which reduces the precision of causal effect estimation. This introduces a fundamental trade-off between privacy and variance when performing causal analyses on differentially private data. In this work, we propose a new differentially private mechanism, "Cluster-DP", which leverages a given cluster structure in the data to improve the privacy-variance trade-off. While our results apply to any clustering, we demonstrate that selecting higher-quality clusters, according to a quality metric we introduce, can decrease the variance penalty without compromising privacy guarantees. Finally, we evaluate the theoretical and empirical performance of our Cluster-DP algorithm on both real and simulated data, comparing it to common baselines, including two special cases of our algorithm: its unclustered version and a uniform-prior version.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novelty Detection in Reinforcement Learning with World Models</title>
<link>https://arxiv.org/abs/2310.08731</link>
<guid>https://arxiv.org/abs/2310.08731</guid>
<content:encoded><![CDATA[
arXiv:2310.08731v4 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) using world models has found significant recent successes. However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline. We refer to the sudden change in visual properties or state transitions as novelties. Implementing novelty detection within generated world model frameworks is a crucial task for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents, by utilizing the misalignment of the world model's hallucinated states and the true observed states as an anomaly score. We provide effective approaches to detecting novelties in a distribution of transitions learned by an agent in a world model. Finally, we show the advantage of our work in a novel environment compared to traditional machine learning novelty detection methods as well as currently accepted RL focused novelty detection algorithms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Breast Cancer Radiotherapy Planning via LMMs with Consistency Embedding</title>
<link>https://arxiv.org/abs/2311.15876</link>
<guid>https://arxiv.org/abs/2311.15876</guid>
<content:encoded><![CDATA[
arXiv:2311.15876v4 Announce Type: replace-cross 
Abstract: Recent advances in AI foundation models have significant potential for lightening the clinical workload by mimicking the comprehensive and multi-faceted approaches used by medical professionals. In the field of radiation oncology, the integration of multiple modalities holds great importance, so the opportunity of foundational model is abundant. Inspired by this, here we present RO-LMM, a multi-purpose, comprehensive large multimodal model (LMM) tailored for the field of radiation oncology. This model effectively manages a series of tasks within the clinical workflow, including clinical context summarization, radiation treatment plan suggestion, and plan-guided target volume segmentation by leveraging the capabilities of LMM. In particular, to perform consecutive clinical tasks without error accumulation, we present a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which boosts LMM's robustness to noisy inputs while preserving the consistency of handling clean inputs. We further extend this concept to LMM-driven segmentation framework, leading to a novel Consistency Embedding Segmentation (CESEG) techniques. Experimental results including multi-centre validation confirm that our RO-LMM with CEFTune and CESEG results in promising performance for multiple clinical tasks with generalization capabilities.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta Co-Training: Two Views are Better than One</title>
<link>https://arxiv.org/abs/2311.18083</link>
<guid>https://arxiv.org/abs/2311.18083</guid>
<content:encoded><![CDATA[
arXiv:2311.18083v5 Announce Type: replace-cross 
Abstract: In many critical computer vision scenarios unlabeled data is plentiful, but labels are scarce and difficult to obtain. As a result, semi-supervised learning which leverages unlabeled data to boost the performance of supervised classifiers have received significant attention in recent literature. One representative class of semi-supervised algorithms are co-training algorithms. Co-training algorithms leverage two different models which have access to different independent and sufficient representations or "views" of the data to jointly make better predictions. Each of these models creates pseudo-labels on unlabeled points which are used to improve the other model. We show that in the common case where independent views are not available, we can construct such views inexpensively using pre-trained models. Co-training on the constructed views yields a performance improvement over any of the individual views we construct and performance comparable with recent approaches in semi-supervised learning. We present Meta Co-Training, a novel semi-supervised learning algorithm, which has two advantages over co-training: (i) learning is more robust when there is large discrepancy between the information content of the different views, and (ii) does not require retraining from scratch on each iteration. Our method achieves new state-of-the-art performance on ImageNet-10% achieving a ~4.7% reduction in error rate over prior work. Our method also outperforms prior semi-supervised work on several other fine-grained image classification datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks</title>
<link>https://arxiv.org/abs/2404.01932</link>
<guid>https://arxiv.org/abs/2404.01932</guid>
<content:encoded><![CDATA[
arXiv:2404.01932v2 Announce Type: replace-cross 
Abstract: In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced outputs. A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%. Moreover, we systematically evaluate the challenges raised by the individual tasks such as object or robot position variability, number of distractors or the task length. Our work thus also sheds light on the potential benefits and limitations of using the current multimodal VAEs for unsupervised learning of robotic motion trajectories based on vision and language.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decision-Focused Forecasting: A Differentiable Multistage Optimisation Architecture</title>
<link>https://arxiv.org/abs/2405.14719</link>
<guid>https://arxiv.org/abs/2405.14719</guid>
<content:encoded><![CDATA[
arXiv:2405.14719v2 Announce Type: replace-cross 
Abstract: Most decision-focused learning work has focused on single stage problems whereas many real-world decision problems are more appropriately modelled using multistage optimisation. In multistage problems contextual information is revealed over time, decisions have to be taken sequentially, and decisions now have an intertemporal effect on future decisions. Decision-focused forecasting is a recurrent differentiable optimisation architecture that expresses a fully differentiable multistage optimisation approach. This architecture enables us to account for the intertemporal decision effects of forecasts. We show what gradient adjustments are made to account for the state-path caused by forecasting. We apply the model to multistage problems in energy storage arbitrage and portfolio optimisation and report that our model outperforms existing approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edit Distance Robust Watermarks via Indexing Pseudorandom Codes</title>
<link>https://arxiv.org/abs/2406.02633</link>
<guid>https://arxiv.org/abs/2406.02633</guid>
<content:encoded><![CDATA[
arXiv:2406.02633v2 Announce Type: replace-cross 
Abstract: Motivated by the problem of detecting AI-generated text, we consider the problem of watermarking the output of language models with provable guarantees. We aim for watermarks which satisfy: (a) undetectability, a cryptographic notion introduced by Christ, Gunn & Zamir (2024) which stipulates that it is computationally hard to distinguish watermarked language model outputs from the model's actual output distribution; and (b) robustness to channels which introduce a constant fraction of adversarial insertions, substitutions, and deletions to the watermarked text. Earlier schemes could only handle stochastic substitutions and deletions, and thus we are aiming for a more natural and appealing robustness guarantee that holds with respect to edit distance.
  Our main result is a watermarking scheme which achieves both undetectability and robustness to edits when the alphabet size for the language model is allowed to grow as a polynomial in the security parameter. To derive such a scheme, we follow an approach introduced by Christ & Gunn (2024), which proceeds via first constructing pseudorandom codes satisfying undetectability and robustness properties analogous to those above; our key idea is to handle adversarial insertions and deletions by interpreting the symbols as indices into the codeword, which we call indexing pseudorandom codes. Additionally, our codes rely on weaker computational assumptions than used in previous work. Then we show that there is a generic transformation from such codes over large alphabets to watermarking schemes for arbitrary language models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical analysis of binding precedent efficiency in Brazilian Supreme Court via case classification</title>
<link>https://arxiv.org/abs/2407.07004</link>
<guid>https://arxiv.org/abs/2407.07004</guid>
<content:encoded><![CDATA[
arXiv:2407.07004v3 Announce Type: replace-cross 
Abstract: Binding precedents (s\'umulas vinculantes) constitute a juridical instrument unique to the Brazilian legal system and whose objectives include the protection of the Federal Supreme Court against repetitive demands. Studies of the effectiveness of these instruments in decreasing the Court's exposure to similar cases, however, indicate that they tend to fail in such a direction, with some of the binding precedents seemingly creating new demands. We empirically assess the legal impact of five binding precedents, 11, 14, 17, 26, and 37, at the highest Court level through their effects on the legal subjects they address. This analysis is only possible through the comparison of the Court's ruling about the precedents' themes before they are created, which means that these decisions should be detected through techniques of Similar Case Retrieval, which we tackle from the angle of Case Classification. The contributions of this article are therefore twofold: on the mathematical side, we compare the use of different methods of Natural Language Processing -- TF-IDF, LSTM, Longformer, and regex -- for Case Classification, whereas on the legal side, we contrast the inefficiency of these binding precedents with a set of hypotheses that may justify their repeated usage. We observe that the TF-IDF models performed slightly better than LSTM and Longformer when compared through common metrics; however, the deep learning models were able to detect certain important legal events that TF-IDF missed. On the legal side, we argue that the reasons for binding precedents to fail in responding to repetitive demand are heterogeneous and case-dependent, making it impossible to single out a specific cause. We identify five main hypotheses, which are found in different combinations in each of the precedents studied.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infinite-dimensional Mahalanobis Distance with Applications to Kernelized Novelty Detection</title>
<link>https://arxiv.org/abs/2407.11873</link>
<guid>https://arxiv.org/abs/2407.11873</guid>
<content:encoded><![CDATA[
arXiv:2407.11873v2 Announce Type: replace-cross 
Abstract: The Mahalanobis distance is a classical tool used to measure the covariance-adjusted distance between points in $\bbR^d$. In this work, we extend the concept of Mahalanobis distance to separable Banach spaces by reinterpreting it as a Cameron-Martin norm associated with a probability measure. This approach leads to a basis-free, data-driven notion of anomaly distance through the so-called variance norm, which can naturally be estimated using empirical measures of a sample. Our framework generalizes the classical $\bbR^d$, functional $(L^2[0,1])^d$, and kernelized settings; importantly, it incorporates non-injective covariance operators. We prove that the variance norm is invariant under invertible bounded linear transformations of the data, extending previous results which are limited to unitary operators. In the Hilbert space setting, we connect the variance norm to the RKHS of the covariance operator and establish consistency and convergence results for estimation using empirical measures. Using the variance norm, we introduce the notion of a kernelized nearest-neighbour Mahalanobis distance. In an empirical study on 12 real-world data sets, we demonstrate that the kernelized nearest-neighbour Mahalanobis distance outperforms the traditional kernelized Mahalanobis distance for multivariate time series novelty detection, using state-of-the-art time series kernels such as the signature, global alignment, and Volterra reservoir kernels.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Base and Exponent Prediction in Mathematical Expressions using Multi-Output CNN</title>
<link>https://arxiv.org/abs/2407.14967</link>
<guid>https://arxiv.org/abs/2407.14967</guid>
<content:encoded><![CDATA[
arXiv:2407.14967v2 Announce Type: replace-cross 
Abstract: The use of neural networks and deep learning techniques in image processing has significantly advanced the field, enabling highly accurate recognition results. However, achieving high recognition rates often necessitates complex network models, which can be challenging to train and require substantial computational resources. This research presents a simplified yet effective approach to predicting both the base and exponent from images of mathematical expressions using a multi-output Convolutional Neural Network (CNN). The model is trained on 10,900 synthetically generated images containing exponent expressions, incorporating random noise, font size variations, and blur intensity to simulate real-world conditions. The proposed CNN model demonstrates robust performance with efficient training time. The experimental results indicate that the model achieves high accuracy in predicting the base and exponent values, proving the efficacy of this approach in handling noisy and varied input images.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAMBDA: A Large Model Based Data Agent</title>
<link>https://arxiv.org/abs/2407.17535</link>
<guid>https://arxiv.org/abs/2407.17535</guid>
<content:encoded><![CDATA[
arXiv:2407.17535v3 Announce Type: replace-cross 
Abstract: We introduce LArge Model Based Data Agent (LAMBDA), a novel open-source, code-free multi-agent data analysis system that leverages the power of large language models. LAMBDA is designed to address data analysis challenges in data-driven applications through innovatively designed data agents using natural language. At the core of LAMBDA are two key agent roles: the programmer and the inspector, which are engineered to work together seamlessly. Specifically, the programmer generates code based on the user's instructions and domain-specific knowledge, while the inspector debugs the code when necessary. To ensure robustness and handle adverse scenarios, LAMBDA features a user interface that allows direct user intervention. Moreover, LAMBDA can flexibly integrate external models and algorithms through our proposed Knowledge Integration Mechanism, catering to the needs of customized data analysis. LAMBDA has demonstrated strong performance on various data analysis tasks. It has the potential to enhance data analysis paradigms by seamlessly integrating human and artificial intelligence, making it more accessible, effective, and efficient for users from diverse backgrounds. The strong performance of LAMBDA in solving data analysis problems is demonstrated using real-world data examples. The code for LAMBDA is available at https://github.com/AMA-CMFAI/LAMBDA and videos of three case studies can be viewed at https://www.polyu.edu.hk/ama/cmfai/lambda.html.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Eigenvalue Theorems for Tensor Analysis in Multi-Modal Data Fusion</title>
<link>https://arxiv.org/abs/2409.09392</link>
<guid>https://arxiv.org/abs/2409.09392</guid>
<content:encoded><![CDATA[
arXiv:2409.09392v3 Announce Type: replace-cross 
Abstract: This paper presents a novel framework for tensor eigenvalue analysis in the context of multi-modal data fusion, leveraging topological invariants such as Betti numbers. Traditional approaches to tensor eigenvalue analysis often extend matrix theory, whereas this work introduces a topological perspective to enhance the understanding of tensor structures. By establishing new theorems that link eigenvalues to topological features, the proposed framework provides deeper insights into the latent structure of data, improving both interpretability and robustness. Applications in data fusion demonstrate the theoretical and practical significance of this approach, with potential for broad impact in machine learning and data science.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Within-class Variation Issue in Alzheimer's Disease Detection</title>
<link>https://arxiv.org/abs/2409.16322</link>
<guid>https://arxiv.org/abs/2409.16322</guid>
<content:encoded><![CDATA[
arXiv:2409.16322v2 Announce Type: replace-cross 
Abstract: Alzheimer's Disease (AD) detection employs machine learning classification models to distinguish between individuals with AD and those without. Different from conventional classification tasks, we identify within-class variation as a critical challenge in AD detection: individuals with AD exhibit a spectrum of cognitive impairments. Therefore, simplistic binary AD classification may overlook two crucial aspects: within-class heterogeneity and instance-level imbalance. In this work, we found using a sample score estimator can generate sample-specific soft scores aligning with cognitive scores. We subsequently propose two simple yet effective methods: Soft Target Distillation (SoTD) and Instance-level Re-balancing (InRe), targeting two problems respectively. Based on the ADReSS and CU-MARVEL corpora, we demonstrated and analyzed the advantages of the proposed approaches in detection performance. These findings provide insights for developing robust and reliable AD detection models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shielded Diffusion: Generating Novel and Diverse Images using Sparse Repellency</title>
<link>https://arxiv.org/abs/2410.06025</link>
<guid>https://arxiv.org/abs/2410.06025</guid>
<content:encoded><![CDATA[
arXiv:2410.06025v3 Announce Type: replace-cross 
Abstract: The adoption of text-to-image diffusion models raises concerns over reliability, drawing scrutiny under the lens of various metrics like calibration, fairness, or compute efficiency. We focus in this work on two issues that arise when deploying these models: a lack of diversity when prompting images, and a tendency to recreate images from the training set. To solve both problems, we propose a method that coaxes the sampled trajectories of pretrained diffusion models to land on images that fall outside of a reference set. We achieve this by adding repellency terms to the diffusion SDE throughout the generation trajectory, which are triggered whenever the path is expected to land too closely to an image in the shielded reference set. Our method is sparse in the sense that these repellency terms are zero and inactive most of the time, and even more so towards the end of the generation trajectory. Our method, named SPELL for sparse repellency, can be used either with a static reference set that contains protected images, or dynamically, by updating the set at each timestep with the expected images concurrently generated within a batch, and with the images of previously generated batches. We show that adding SPELL to popular diffusion models improves their diversity while impacting their FID only marginally, and performs comparatively better than other recent training-free diversity methods. We also demonstrate how SPELL can ensure a shielded generation away from a very large set of protected images by considering all 1.2M images from ImageNet as the protected set.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy for Free in the Overparameterized Regime</title>
<link>https://arxiv.org/abs/2410.14787</link>
<guid>https://arxiv.org/abs/2410.14787</guid>
<content:encoded><![CDATA[
arXiv:2410.14787v2 Announce Type: replace-cross 
Abstract: Differentially private gradient descent (DP-GD) is a popular algorithm to train deep learning models with provable guarantees on the privacy of the training data. In the last decade, the problem of understanding its performance cost with respect to standard GD has received remarkable attention from the research community, which formally derived upper bounds on the excess population risk $R_{P}$ in different learning settings. However, existing bounds typically degrade with over-parameterization, i.e., as the number of parameters $p$ gets larger than the number of training samples $n$ -- a regime which is ubiquitous in current deep-learning practice. As a result, the lack of theoretical insights leaves practitioners without clear guidance, leading some to reduce the effective number of trainable parameters to improve performance, while others use larger models to achieve better results through scale. In this work, we show that in the popular random features model with quadratic loss, for any sufficiently large $p$, privacy can be obtained for free, i.e., $\left|R_{P} \right| = o(1)$, not only when the privacy parameter $\varepsilon$ has constant order, but also in the strongly private setting $\varepsilon = o(1)$. This challenges the common wisdom that over-parameterization inherently hinders performance in private learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simultaneously Solving FBSDEs and their Associated Semilinear Elliptic PDEs with Small Neural Operators</title>
<link>https://arxiv.org/abs/2410.14788</link>
<guid>https://arxiv.org/abs/2410.14788</guid>
<content:encoded><![CDATA[
arXiv:2410.14788v2 Announce Type: replace-cross 
Abstract: Forward-backwards stochastic differential equations (FBSDEs) play an important role in optimal control, game theory, economics, mathematical finance, and in reinforcement learning. Unfortunately, the available FBSDE solvers operate on \textit{individual} FBSDEs, meaning that they cannot provide a computationally feasible strategy for solving large families of FBSDEs, as these solvers must be re-run several times. \textit{Neural operators} (NOs) offer an alternative approach for \textit{simultaneously solving} large families of decoupled FBSDEs by directly approximating the solution operator mapping \textit{inputs:} terminal conditions and dynamics of the backwards process to \textit{outputs:} solutions to the associated FBSDE. Though universal approximation theorems (UATs) guarantee the existence of such NOs, these NOs are unrealistically large. Upon making only a few simple theoretically-guided tweaks to the standard convolutional NO build, we confirm that ``small'' NOs can uniformly approximate the solution operator to structured families of FBSDEs with random terminal time, uniformly on suitable compact sets determined by Sobolev norms using a logarithmic depth, a constant width, and a polynomial rank in the reciprocal approximation error.
  This result is rooted in our second result, and main contribution to the NOs for PDE literature, showing that our convolutional NOs of similar depth and width but grow only \textit{quadratically} (at a dimension-free rate) when uniformly approximating the solution operator of the associated class of semilinear Elliptic PDEs to these families of FBSDEs. A key insight into how NOs work we uncover is that the convolutional layers of our NO can approximately implement the fixed point iteration used to prove the existence of a unique solution to these semilinear Elliptic PDEs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Characterization of the Population Area Under the Risk Coverage Curve (AURC) and Rates of Finite Sample Estimators</title>
<link>https://arxiv.org/abs/2410.15361</link>
<guid>https://arxiv.org/abs/2410.15361</guid>
<content:encoded><![CDATA[
arXiv:2410.15361v3 Announce Type: replace-cross 
Abstract: The selective classifier (SC) has been proposed for rank based uncertainty thresholding, which could have applications in safety critical areas such as medical diagnostics, autonomous driving, and the justice system. The Area Under the Risk-Coverage Curve (AURC) has emerged as the foremost evaluation metric for assessing the performance of SC systems. In this work, we present a formal statistical formulation of population AURC, presenting an equivalent expression that can be interpreted as a reweighted risk function. Through Monte Carlo methods, we derive empirical AURC plug-in estimators for finite sample scenarios. The weight estimators associated with these plug-in estimators are shown to be consistent, with low bias and tightly bounded mean squared error (MSE). The plug-in estimators are proven to converge at a rate of $\mathcal{O}(\sqrt{\ln(n)/n})$ demonstrating statistical consistency. We empirically validate the effectiveness of our estimators through experiments across multiple datasets, model architectures, and confidence score functions (CSFs), demonstrating consistency and effectiveness in fine-tuning AURC performance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference for Temporal Difference Learning with Linear Function Approximation</title>
<link>https://arxiv.org/abs/2410.16106</link>
<guid>https://arxiv.org/abs/2410.16106</guid>
<content:encoded><![CDATA[
arXiv:2410.16106v3 Announce Type: replace-cross 
Abstract: We investigate the statistical properties of Temporal Difference (TD) learning with Polyak-Ruppert averaging, arguably one of the most widely used algorithms in reinforcement learning, for the task of estimating the parameters of the optimal linear approximation to the value function. We make three significant contributions that improve the current state-of-the-art results: (i) we derive sharper high probability convergence guarantee that depend explicitly on the asymptotic variance and hold under weaker conditions than those normally assumed; (ii) we establish refined high-dimensional Berry-Esseen bounds over the class of convex sets, achieving faster rates than those previously established in the literature, and (iii) we propose and analyze a novel, computationally efficient online plug-in estimator of the asymptotic covariance matrix.These results enable the construction of confidence regions and simultaneous confidence intervals for the linear parameters of the value function approximation, with guaranteed finite-sample coverage. We demonstrate the applicability of our theoretical findings through numerical experiments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Approximation of Mean-Field Models via Transformers</title>
<link>https://arxiv.org/abs/2410.16295</link>
<guid>https://arxiv.org/abs/2410.16295</guid>
<content:encoded><![CDATA[
arXiv:2410.16295v2 Announce Type: replace-cross 
Abstract: This paper investigates the use of transformers to approximate the mean-field dynamics of interacting particle systems exhibiting collective behavior. Such systems are fundamental in modeling phenomena across physics, biology, and engineering, including opinion formation, biological networks, and swarm robotics. The key characteristic of these systems is that the particles are indistinguishable, leading to permutation-equivariant dynamics. First, we empirically demonstrate that transformers are well-suited for approximating a variety of mean field models, including the Cucker-Smale model for flocking and milling, and the mean-field system for training two-layer neural networks. We validate our numerical experiments via mathematical theory. Specifically, we prove that if a finite-dimensional transformer effectively approximates the finite-dimensional vector field governing the particle system, then the $L_2$ distance between the \textit{expected transformer} and the infinite-dimensional mean-field vector field can be uniformly bounded by a function of the number of particles observed during training. Leveraging this result, we establish theoretical bounds on the distance between the true mean-field dynamics and those obtained using the transformer.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stochastic Approximation Approach for Efficient Decentralized Optimization on Random Networks</title>
<link>https://arxiv.org/abs/2410.18774</link>
<guid>https://arxiv.org/abs/2410.18774</guid>
<content:encoded><![CDATA[
arXiv:2410.18774v2 Announce Type: replace-cross 
Abstract: A challenging problem in decentralized optimization is to develop algorithms with fast convergence on random and time varying topologies under unreliable and bandwidth-constrained communication network. This paper studies a stochastic approximation approach with a Fully Stochastic Primal Dual Algorithm (FSPDA) framework. Our framework relies on a novel observation that randomness in time varying topology can be incorporated in a stochastic augmented Lagrangian formulation, whose expected value admits saddle points that coincide with stationary solutions of the decentralized optimization problem. With the FSPDA framework, we develop two new algorithms supporting efficient sparsified communication on random time varying topologies -- FSPDA-SA allows agents to execute multiple local gradient steps depending on the time varying topology to accelerate convergence, and FSPDA-STORM further incorporates a variance reduction step to improve sample complexity. For problems with smooth (possibly non-convex) objective function, within $T$ iterations, we show that FSPDA-SA (resp. FSPDA-STORM) finds an $\mathcal{O}( 1/\sqrt{T} )$-stationary (resp. $\mathcal{O}( 1/T^{2/3} )$) solution. Numerical experiments show the benefits of the FSPDA algorithms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models as Cartoonists: The Curious Case of High Density Regions</title>
<link>https://arxiv.org/abs/2411.01293</link>
<guid>https://arxiv.org/abs/2411.01293</guid>
<content:encoded><![CDATA[
arXiv:2411.01293v4 Announce Type: replace-cross 
Abstract: We investigate what kind of images lie in the high-density regions of diffusion models. We introduce a theoretical mode-tracking process capable of pinpointing the exact mode of the denoising distribution, and we propose a practical high-density sampler that consistently generates images of higher likelihood than usual samplers. Our empirical findings reveal the existence of significantly higher likelihood samples that typical samplers do not produce, often manifesting as cartoon-like drawings or blurry images depending on the noise level. Curiously, these patterns emerge in datasets devoid of such examples. We also present a novel approach to track sample likelihoods in diffusion SDEs, which remarkably incurs no additional computational cost. Code is available at https://github.com/Aalto-QuML/high-density-diffusion.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an Auxiliary Head</title>
<link>https://arxiv.org/abs/2411.08937</link>
<guid>https://arxiv.org/abs/2411.08937</guid>
<content:encoded><![CDATA[
arXiv:2411.08937v2 Announce Type: replace-cross 
Abstract: Traditional knowledge distillation focuses on aligning the student's predicted probabilities with both ground-truth labels and the teacher's predicted probabilities. However, the transition to predicted probabilities from logits would obscure certain indispensable information. To address this issue, it is intuitive to additionally introduce a logit-level loss function as a supplement to the widely used probability-level loss function, for exploiting the latent information of logits. Unfortunately, we empirically find that the amalgamation of the newly introduced logit-level loss and the previous probability-level loss will lead to performance degeneration, even trailing behind the performance of employing either loss in isolation. We attribute this phenomenon to the collapse of the classification head, which is verified by our theoretical analysis based on the neural collapse theory. Specifically, the gradients of the two loss functions exhibit contradictions in the linear classifier yet display no such conflict within the backbone. Drawing from the theoretical analysis, we propose a novel method called dual-head knowledge distillation, which partitions the linear classifier into two classification heads responsible for different losses, thereby preserving the beneficial effects of both losses on the backbone while eliminating adverse influences on the classification head. Extensive experiments validate that our method can effectively exploit the information inside the logits and achieve superior performance against state-of-the-art counterparts. Our code is available at: https://github.com/penghui-yang/DHKD.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LL\"aMmlein: Compact and Competitive German-Only Language Models from Scratch</title>
<link>https://arxiv.org/abs/2411.11171</link>
<guid>https://arxiv.org/abs/2411.11171</guid>
<content:encoded><![CDATA[
arXiv:2411.11171v4 Announce Type: replace-cross 
Abstract: We create two German-only decoder models, LL\"aMmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use. The model training involved several key steps, including extensive data preprocessing, the creation of a custom German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks. Throughout the training process, multiple checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor the models' learning dynamics. Compared to state-of-the-art models on the SuperGLEBer benchmark, both LL\"aMmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models' quality scales with size as expected, but performance improvements on some tasks plateaued early, offering valuable insights into resource allocation for future model development.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample Efficient Robot Learning in Supervised Effect Prediction Tasks</title>
<link>https://arxiv.org/abs/2412.02331</link>
<guid>https://arxiv.org/abs/2412.02331</guid>
<content:encoded><![CDATA[
arXiv:2412.02331v2 Announce Type: replace-cross 
Abstract: In self-supervised robotic learning, agents acquire data through active interaction with their environment, incurring costs such as energy use, human oversight, and experimental time. To mitigate these, sample-efficient exploration is essential. While intrinsic motivation (IM) methods like learning progress (LP) are widely used in robotics, and active learning (AL) is well established for classification in machine learning, few frameworks address continuous, high-dimensional regression tasks typical of world model learning. We propose MUSEL (Model Uncertainty for Sample-Efficient Learning), a novel AL framework tailored for regression tasks in robotics, such as action-effect prediction. MUSEL introduces a model uncertainty metric that combines total predictive uncertainty, learning progress, and input diversity to guide data acquisition. We validate our approach using a Stochastic Variational Deep Kernel Learning (SVDKL) model in two robotic tabletop tasks. Experimental results demonstrate that MUSEL improves both learning accuracy and sample efficiency, validating its effectiveness in learning action effects and selecting informative samples.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Mono-to-Binaural Speech Synthesis</title>
<link>https://arxiv.org/abs/2412.08356</link>
<guid>https://arxiv.org/abs/2412.08356</guid>
<content:encoded><![CDATA[
arXiv:2412.08356v2 Announce Type: replace-cross 
Abstract: We present ZeroBAS, a neural method to synthesize binaural audio from monaural audio recordings and positional information without training on any binaural data. To our knowledge, this is the first published zero-shot neural approach to mono-to-binaural audio synthesis. Specifically, we show that a parameter-free geometric time warping and amplitude scaling based on source location suffices to get an initial binaural synthesis that can be refined by iteratively applying a pretrained denoising vocoder. Furthermore, we find this leads to generalization across room conditions, which we measure by introducing a new dataset, TUT Mono-to-Binaural, to evaluate state-of-the-art monaural-to-binaural synthesis methods on unseen conditions. Our zero-shot method is perceptually on-par with the performance of supervised methods on the standard mono-to-binaural dataset, and even surpasses them on our out-of-distribution TUT Mono-to-Binaural dataset. Our results highlight the potential of pretrained generative audio models and zero-shot learning to unlock robust binaural audio synthesis.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference Adaptive and Sequential Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2412.10419</link>
<guid>https://arxiv.org/abs/2412.10419</guid>
<content:encoded><![CDATA[
arXiv:2412.10419v2 Announce Type: replace-cross 
Abstract: We address the problem of interactive text-to-image (T2I) generation, designing a reinforcement learning (RL) agent which iteratively improves a set of generated images for a user through a sequence of prompt expansions. Using human raters, we create a novel dataset of sequential preferences, which we leverage, together with large-scale open-source (non-sequential) datasets. We construct user-preference and user-choice models using an EM strategy and identify varying user preference types. We then leverage a large multimodal language model (LMM) and a value-based RL approach to suggest an adaptive and diverse slate of prompt expansions to the user. Our Preference Adaptive and Sequential Text-to-image Agent (PASTA) extends T2I models with adaptive multi-turn capabilities, fostering collaborative co-creation and addressing uncertainty or underspecification in a user's intent. We evaluate PASTA using human raters, showing significant improvement compared to baseline methods. We also open-source our sequential rater dataset and simulated user-rater interactions to support future research in user-centric multi-turn T2I systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Core Context Aware Transformers for Long Context Language Modeling</title>
<link>https://arxiv.org/abs/2412.12465</link>
<guid>https://arxiv.org/abs/2412.12465</guid>
<content:encoded><![CDATA[
arXiv:2412.12465v2 Announce Type: replace-cross 
Abstract: Transformer-based Large Language Models (LLMs) have exhibited remarkable success in extensive tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute attention. However, when the context length L becomes very large (e.g., 128K), the amount of potentially redundant information in the context tends to increase. The redundant context not only hampers the modeling representation performance but also incurs unnecessary computational and storage overhead. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-context modeling, comprising two complementary modules: 1) Globality-aware pooling module groups input tokens and dynamically compresses each group into one core token based on their significance. In this way, our method automatically focuses and strengthens core context while diminishing redundancy during the learning process, leading to effective long-term dependency modeling. 2) Locality-preserving module incorporates neighboring tokens to preserve local context for detailed representation. Notably, our CCA-Attention is able to replace the self-attention module in existing LLMs with minimal fine-tuning cost. Extensive experimental results show the superiority of our method in both long-context modeling and computational efficiency over state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Synthesize Text Data without Model Collapse?</title>
<link>https://arxiv.org/abs/2412.14689</link>
<guid>https://arxiv.org/abs/2412.14689</guid>
<content:encoded><![CDATA[
arXiv:2412.14689v3 Announce Type: replace-cross 
Abstract: Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-$\{n\}$ models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves model performance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting In-Context Learning with Long Context Language Models</title>
<link>https://arxiv.org/abs/2412.16926</link>
<guid>https://arxiv.org/abs/2412.16926</guid>
<content:encoded><![CDATA[
arXiv:2412.16926v3 Announce Type: replace-cross 
Abstract: In-Context Learning (ICL) is a technique by which language models make predictions based on examples provided in their input context. Previously, their context window size imposed a limit on the number of examples that can be shown, making example selection techniques crucial for identifying the maximally effective set of examples. However, the recent advent of Long Context Language Models (LCLMs) has significantly increased the number of examples that can be included in context, raising an important question of whether ICL performance in a many-shot regime is still sensitive to the method of sample selection. To answer this, we revisit these approaches in the context of LCLMs through extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we observe that sophisticated example selection techniques do not yield significant improvements over a simple random sample selection method. Instead, we discover that the advent of LCLMs has fundamentally shifted the challenge of ICL from that of selecting the most effective examples to that of collecting sufficient examples to fill the context window. Specifically, in certain datasets, including all available examples does not fully utilize the context window; however, by augmenting the examples in context with a simple data augmentation approach, we substantially improve ICL performance by 5%.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum framework for Reinforcement Learning: Integrating Markov decision process, quantum arithmetic, and trajectory search</title>
<link>https://arxiv.org/abs/2412.18208</link>
<guid>https://arxiv.org/abs/2412.18208</guid>
<content:encoded><![CDATA[
arXiv:2412.18208v3 Announce Type: replace-cross 
Abstract: This paper introduces a quantum framework for addressing reinforcement learning (RL) tasks, grounded in the quantum principles and leveraging a fully quantum model of the classical Markov decision process (MDP). By employing quantum concepts and a quantum search algorithm, this work presents the implementation and optimization of the agent-environment interactions entirely within the quantum domain, eliminating reliance on classical computations. Key contributions include the quantum-based state transitions, return calculation, and trajectory search mechanism that utilize quantum principles to demonstrate the realization of RL processes through quantum phenomena. The implementation emphasizes the fundamental role of quantum superposition in enhancing computational efficiency for RL tasks. Results demonstrate the capacity of a quantum model to achieve quantum enhancement in RL, highlighting the potential of fully quantum implementations in decision-making tasks. This work not only underscores the applicability of quantum computing in machine learning but also contributes to the field of quantum reinforcement learning (QRL) by offering a robust framework for understanding and exploiting quantum computing in RL systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outlier-Robust Linear System Identification Under Heavy-tailed Noise</title>
<link>https://arxiv.org/abs/2501.00421</link>
<guid>https://arxiv.org/abs/2501.00421</guid>
<content:encoded><![CDATA[
arXiv:2501.00421v2 Announce Type: replace-cross 
Abstract: We consider the problem of estimating the state transition matrix of a linear time-invariant (LTI) system, given access to multiple independent trajectories sampled from the system. Several recent papers have conducted a non-asymptotic analysis of this problem, relying crucially on the assumption that the process noise is either Gaussian or sub-Gaussian, i.e., "light-tailed". In sharp contrast, we work under a significantly weaker noise model, assuming nothing more than the existence of the fourth moment of the noise distribution. For this setting, we provide the first set of results demonstrating that one can obtain sample-complexity bounds for linear system identification that are nearly of the same order as under sub-Gaussian noise. To achieve such results, we develop a novel robust system identification algorithm that relies on constructing multiple weakly-concentrated estimators, and then boosting their performance using suitable tools from high-dimensional robust statistics. Interestingly, our analysis reveals how the kurtosis of the noise distribution, a measure of heavy-tailedness, affects the number of trajectories needed to achieve desired estimation error bounds. Finally, we show that our algorithm and analysis technique can be easily extended to account for scenarios where an adversary can arbitrarily corrupt a small fraction of the collected trajectory data. Our work takes the first steps towards building a robust statistical learning theory for control under non-ideal assumptions on the data-generating process.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation</title>
<link>https://arxiv.org/abs/2501.00777</link>
<guid>https://arxiv.org/abs/2501.00777</guid>
<content:encoded><![CDATA[
arXiv:2501.00777v3 Announce Type: replace-cross 
Abstract: Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF's core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals, which we hope will serve as an important finding for future research in this direction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models</title>
<link>https://arxiv.org/abs/2501.03124</link>
<guid>https://arxiv.org/abs/2501.03124</guid>
<content:encoded><![CDATA[
arXiv:2501.03124v4 Announce Type: replace-cross 
Abstract: Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Adversarial Post-Training for One-Step Video Generation</title>
<link>https://arxiv.org/abs/2501.08316</link>
<guid>https://arxiv.org/abs/2501.08316</guid>
<content:encoded><![CDATA[
arXiv:2501.08316v2 Announce Type: replace-cross 
Abstract: The diffusion models are widely used for image and video generation, but their iterative generation process is slow and expansive. While existing distillation approaches have demonstrated the potential for one-step generation in the image domain, they still suffer from significant quality degradation. In this work, we propose Adversarial Post-Training (APT) against real data following diffusion pre-training for one-step video generation. To improve the training stability and quality, we introduce several improvements to the model architecture and training procedures, along with an approximated R1 regularization objective. Empirically, our experiments show that our adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720, 24fps videos in real time using a single forward evaluation step. Additionally, our model is capable of generating 1024px images in a single step, achieving quality comparable to state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kimi k1.5: Scaling Reinforcement Learning with LLMs</title>
<link>https://arxiv.org/abs/2501.12599</link>
<guid>https://arxiv.org/abs/2501.12599</guid>
<content:encoded><![CDATA[
arXiv:2501.12599v3 Announce Type: replace-cross 
Abstract: Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains</title>
<link>https://arxiv.org/abs/2501.14431</link>
<guid>https://arxiv.org/abs/2501.14431</guid>
<content:encoded><![CDATA[
arXiv:2501.14431v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are widely applied to downstream domains. However, current LLMs for high-stakes domain tasks, such as financial investment and legal QA, typically generate brief answers without reasoning processes and explanations. This limits users' confidence in making decisions based on their responses. While original CoT shows promise, it lacks self-correction mechanisms during reasoning. This work introduces Domain$o1$s, which enhances LLMs' reasoning capabilities on domain tasks through supervised fine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k datasets for fine-tuning models that activate domain-specific reasoning steps based on their judgment. Additionally, we propose Selective Tree Exploration to spontaneously explore solution spaces and sample optimal reasoning paths to improve performance. We also introduce PROOF-Score, a new metric for evaluating domain models' explainability, complementing traditional accuracy metrics with richer assessment dimensions. Extensive experiments on stock investment recommendation and legal reasoning QA tasks demonstrate Domaino1s's leading performance and explainability. Our code is available at https://github.com/Hyalinesky/Domaino1s.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Feature Representation Boosting</title>
<link>https://arxiv.org/abs/2501.18283</link>
<guid>https://arxiv.org/abs/2501.18283</guid>
<content:encoded><![CDATA[
arXiv:2501.18283v3 Announce Type: replace-cross 
Abstract: We introduce Random Feature Representation Boosting (RFRBoost), a novel method for constructing deep residual random feature neural networks (RFNNs) using boosting theory. RFRBoost uses random features at each layer to learn the functional gradient of the network representation, enhancing performance while preserving the convex optimization benefits of RFNNs. In the case of MSE loss, we obtain closed-form solutions to greedy layer-wise boosting with random features. For general loss functions, we show that fitting random feature residual blocks reduces to solving a quadratically constrained least squares problem. Through extensive numerical experiments on tabular datasets for both regression and classification, we show that RFRBoost significantly outperforms RFNNs and end-to-end trained MLP ResNets in the small- to medium-scale regime where RFNNs are typically applied. Moreover, RFRBoost offers substantial computational benefits, and theoretical guarantees stemming from boosting theory.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing</title>
<link>https://arxiv.org/abs/2502.00602</link>
<guid>https://arxiv.org/abs/2502.00602</guid>
<content:encoded><![CDATA[
arXiv:2502.00602v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved remarkable performance on various natural language tasks. However, they are trained on static corpora and their knowledge can become outdated quickly in the fast-changing world. This motivates the development of knowledge editing (KE) to update specific knowledge in LLMs without changing unrelated others or compromising their pre-trained capabilities. Previous efforts sought to update a small amount of parameters of a LLM and proved effective for making selective updates. Nonetheless, the edited LLM often exhibits degraded ability to reason about the new knowledge. In this work, we identify a key issue: heterogeneous token overfitting (HTO), where the LLM overfits different tokens in the provided knowledge at varying rates. To tackle this, we propose OVERTONE, a token-level smoothing method that mitigates HTO by adaptively refining the target distribution. Theoretically, OVERTONE offers better parameter updates with negligible computation overhead. It also induces an implicit DPO but does not require preference data pairs. Extensive experiments across four editing methods, two LLMs, and diverse scenarios demonstrate the effectiveness and versatility of our method.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, Provably and Efficiently</title>
<link>https://arxiv.org/abs/2502.01235</link>
<guid>https://arxiv.org/abs/2502.01235</guid>
<content:encoded><![CDATA[
arXiv:2502.01235v2 Announce Type: replace-cross 
Abstract: This paper explores how theory can guide and enhance practical algorithms, using Low-Rank Adaptation (LoRA, Hu et al. 2022) in large language models as a case study. We rigorously prove that, under gradient descent, LoRA adapters align with specific singular subspaces of the one-step full fine-tuning gradient. This result suggests that, by properly initializing the adapters using the one-step full gradient, subspace alignment can be achieved immediately and applicable to both linear and nonlinear models. Building on our theory, we propose a theory-driven algorithm, LoRA-One, where the linear convergence (as well as generalization) is built and incorporating preconditioners theoretically helps mitigate the effects of ill-conditioning. Besides, our theory reveals connections between LoRA-One and other gradient-alignment-based methods, helping to clarify misconceptions in the design of such algorithms. LoRA-One achieves significant empirical improvements over LoRA and its variants across benchmarks in natural language understanding, mathematical reasoning, and code generation. Code is available at: https://github.com/YuanheZ/LoRA-One.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spurious Correlations in High Dimensional Regression: The Roles of Regularization, Simplicity Bias and Over-Parameterization</title>
<link>https://arxiv.org/abs/2502.01347</link>
<guid>https://arxiv.org/abs/2502.01347</guid>
<content:encoded><![CDATA[
arXiv:2502.01347v2 Announce Type: replace-cross 
Abstract: Learning models have been shown to rely on spurious correlations between non-predictive features and the associated labels in the training data, with negative implications on robustness, bias and fairness. In this work, we provide a statistical characterization of this phenomenon for high-dimensional regression, when the data contains a predictive core feature $x$ and a spurious feature $y$. Specifically, we quantify the amount of spurious correlations $C$ learned via linear regression, in terms of the data covariance and the strength $\lambda$ of the ridge regularization. As a consequence, we first capture the simplicity of $y$ through the spectrum of its covariance, and its correlation with $x$ through the Schur complement of the full data covariance. Next, we prove a trade-off between $C$ and the in-distribution test loss $L$, by showing that the value of $\lambda$ that minimizes $L$ lies in an interval where $C$ is increasing. Finally, we investigate the effects of over-parameterization via the random features model, by showing its equivalence to regularized linear regression. Our theoretical results are supported by numerical experiments on Gaussian, Color-MNIST, and CIFAR-10 datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2502.02406</link>
<guid>https://arxiv.org/abs/2502.02406</guid>
<content:encoded><![CDATA[
arXiv:2502.02406v3 Announce Type: replace-cross 
Abstract: Cross-attention is commonly adopted in multimodal large language models (MLLMs) for integrating visual information into the language backbone. However, in applications with large visual inputs, such as video understanding, processing a large number of visual tokens in cross-attention layers leads to high memory demands and often necessitates distributed computation across multiple GPUs. Existing distributed attention mechanisms face significant communication overheads, making cross-attention layers a critical bottleneck for efficient training and inference of MLLMs. To address this, we propose LV-XAttn, a distributed, exact cross-attention mechanism with minimal communication overhead. We observe that in applications involving large visual inputs, the size of the query block is typically much smaller than that of the key-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally on each GPU and exchange smaller query blocks across GPUs. We also introduce an efficient activation recomputation technique to support longer visual context. We theoretically analyze the communication benefits of LV-XAttn and show that it can achieve speedups for a wide range of models. Our evaluations with Llama 3-V, mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to 10.62$\times$ end-to-end speedup compared to existing approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Kernels to Features: A Multi-Scale Adaptive Theory of Feature Learning</title>
<link>https://arxiv.org/abs/2502.03210</link>
<guid>https://arxiv.org/abs/2502.03210</guid>
<content:encoded><![CDATA[
arXiv:2502.03210v2 Announce Type: replace-cross 
Abstract: Feature learning in neural networks is crucial for their expressive power and inductive biases, motivating various theoretical approaches. Some approaches describe network behavior after training through a change in kernel scale from initialization, resulting in a generalization power comparable to a Gaussian process. Conversely, in other approaches training results in the adaptation of the kernel to the data, involving directional changes to the kernel. The relationship and respective strengths of these two views have so far remained unresolved. This work presents a theoretical framework of multi-scale adaptive feature learning bridging these two views. Using methods from statistical mechanics, we derive analytical expressions for network output statistics which are valid across scaling regimes and in the continuum between them. A systematic expansion of the network's probability distribution reveals that mean-field scaling requires only a saddle-point approximation, while standard scaling necessitates additional correction terms. Remarkably, we find across regimes that kernel adaptation can be reduced to an effective kernel rescaling when predicting the mean network output in the special case of a linear network. However, for linear and non-linear networks, the multi-scale adaptive approach captures directional feature learning effects, providing richer insights than what could be recovered from a rescaling of the kernel alone.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring</title>
<link>https://arxiv.org/abs/2502.05242</link>
<guid>https://arxiv.org/abs/2502.05242</guid>
<content:encoded><![CDATA[
arXiv:2502.05242v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are becoming increasingly capable, but the mechanisms of their thinking and decision-making process remain unclear. Chain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this strategy fails to accurately reflect LLMs' thinking process. Techniques based on LLMs' hidden representations provide an inner perspective to monitor their latent thinking. However, previous methods only try to develop external monitors instead of making LLMs themselves easier to monitor. In this paper, we propose a novel method TELLME, improving the transparency of LLMs and helping monitors identify unsuitable and sensitive behaviors. Furthermore, we showcase the applications of TELLME on trustworthiness tasks (\eg, safety risks monitoring tasks and detoxification tasks), where LLMs achieve consistent improvement in transparency and task performance. More crucially, we theoretically analyze the improvement of TELLME on LLMs' generalization ability through optimal transport theory.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth Sailing: Lipschitz-Driven Uncertainty Quantification for Spatial Association</title>
<link>https://arxiv.org/abs/2502.06067</link>
<guid>https://arxiv.org/abs/2502.06067</guid>
<content:encoded><![CDATA[
arXiv:2502.06067v2 Announce Type: replace-cross 
Abstract: Estimating associations between spatial covariates and responses - rather than merely predicting responses - is central to environmental science, epidemiology, and economics. For instance, public health officials might be interested in whether air pollution has a strictly positive association with a health outcome, and the magnitude of any effect. Standard machine learning methods often provide accurate predictions but offer limited insight into covariate-response relationships. And we show that existing methods for constructing confidence (or credible) intervals for associations fail to provide nominal coverage in the face of model misspecification and distribution shift - despite both being essentially always present in spatial problems. We introduce a method that constructs valid frequentist confidence intervals for associations in spatial settings. Our method requires minimal assumptions beyond a form of spatial smoothness. In particular, we do not require model correctness or covariate overlap between training and target locations. Our approach is the first to guarantee nominal coverage in this setting and outperforms existing techniques in both real and simulated experiments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation</title>
<link>https://arxiv.org/abs/2502.07365</link>
<guid>https://arxiv.org/abs/2502.07365</guid>
<content:encoded><![CDATA[
arXiv:2502.07365v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have gained extended context windows through scaling positional encodings and lightweight continual pre-training. However, this often leads to degraded performance on short-text tasks, while the reasons for this degradation remain insufficiently explored. In this work, we identify two primary factors contributing to this issue: distribution drift in hidden states and attention scores, and catastrophic forgetting during continual pre-training. To address these challenges, we propose Long Context Pre-training with Restoration Distillation (LongReD), a novel approach designed to mitigate short-text performance degradation through minimizing the distribution discrepancy between the extended and original models. Besides training on long texts, LongReD distills the hidden state of selected layers from the original model on short texts. Additionally, LongReD also introduces a short-to-long distillation, aligning the output distribution on short texts with that on long texts by leveraging skipped positional indices. Experiments on common text benchmarks demonstrate that LongReD effectively preserves the model's short-text performance while maintaining comparable or even better capacity to handle long texts than baselines. Our code is available at https://github.com/RUCAIBox/LongReD.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combinatorial Reinforcement Learning with Preference Feedback</title>
<link>https://arxiv.org/abs/2502.10158</link>
<guid>https://arxiv.org/abs/2502.10158</guid>
<content:encoded><![CDATA[
arXiv:2502.10158v2 Announce Type: replace-cross 
Abstract: In this paper, we consider combinatorial reinforcement learning with preference feedback, where a learning agent sequentially offers an action--an assortment of multiple items to--a user, whose preference feedback follows a multinomial logistic (MNL) model. This framework allows us to model real-world scenarios, particularly those involving long-term user engagement, such as in recommender systems and online advertising. However, this framework faces two main challenges: (1) the unknown value of each item, unlike traditional MNL bandits that only address single-step preference feedback, and (2) the difficulty of ensuring optimism while maintaining tractable assortment selection in the combinatorial action space with unknown values. In this paper, we assume a contextual MNL preference model, where the mean utilities are linear, and the value of each item is approximated by a general function. We propose an algorithm, MNL-VQL, that addresses these challenges, making it both computationally and statistically efficient. As a special case, for linear MDPs (with the MNL preference feedback), we establish the first regret lower bound in this framework and show that MNL-VQL achieves nearly minimax-optimal regret. To the best of our knowledge, this is the first work to provide statistical guarantees in combinatorial RL with preference feedback.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReLearn: Unlearning via Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2502.11190</link>
<guid>https://arxiv.org/abs/2502.11190</guid>
<content:encoded><![CDATA[
arXiv:2502.11190v3 Announce Type: replace-cross 
Abstract: Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2502.11882</link>
<guid>https://arxiv.org/abs/2502.11882</guid>
<content:encoded><![CDATA[
arXiv:2502.11882v5 Announce Type: replace-cross 
Abstract: Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. DPT-Agent can effectively help LLMs convert correct slow thinking and reasoning into executable actions, thereby improving performance. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails</title>
<link>https://arxiv.org/abs/2502.13458</link>
<guid>https://arxiv.org/abs/2502.13458</guid>
<content:encoded><![CDATA[
arXiv:2502.13458v2 Announce Type: replace-cross 
Abstract: Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose ThinkGuard, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail's cautiousness and interpretability. Evaluated on multiple safety benchmarks, ThinkGuard achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimize Cardinality Estimation Model Pretraining by Simplifying the Training Datasets</title>
<link>https://arxiv.org/abs/2502.14350</link>
<guid>https://arxiv.org/abs/2502.14350</guid>
<content:encoded><![CDATA[
arXiv:2502.14350v2 Announce Type: replace-cross 
Abstract: The cardinality estimation is a key aspect of query optimization research, and its performance has significantly improved with the integration of machine learning. To overcome the "cold start" problem or the lack of model transferability in learned cardinality estimators, some pre-training cardinality estimation models have been proposed that use learning across multiple datasets and corresponding workloads. These models typically train on a dataset created by uniformly sampling from many datasets, but this approach may not be optimal. By applying the Group Distributionally Robust Optimization (Group DRO) algorithm to training datasets, we find that some specific training datasets contribute more significantly to model performance than others. Based on this observation, we conduct extensive experiments to delve deeper into pre-training cardinality estimators. Our results show how the performance of these models can be influenced by the datasets and corresponding workloads. Finally, we introduce a simplified training dataset, which has been reduced to a fraction of the size of existing pretraining datasets. Sufficient experimental results demonstrate that the pre-trained cardinality estimator based on this simplified dataset can still achieve comparable performance to existing models in zero-shot setups.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness and Cybersecurity in the EU Artificial Intelligence Act</title>
<link>https://arxiv.org/abs/2502.16184</link>
<guid>https://arxiv.org/abs/2502.16184</guid>
<content:encoded><![CDATA[
arXiv:2502.16184v2 Announce Type: replace-cross 
Abstract: The EU Artificial Intelligence Act (AIA) establishes different legal principles for different types of AI systems. While prior work has sought to clarify some of these principles, little attention has been paid to robustness and cybersecurity. This paper aims to fill this gap. We identify legal challenges and shortcomings in provisions related to robustness and cybersecurity for high-risk AI systems(Art. 15 AIA) and general-purpose AI models (Art. 55 AIA). We show that robustness and cybersecurity demand resilience against performance disruptions. Furthermore, we assess potential challenges in implementing these provisions in light of recent advancements in the machine learning (ML) literature. Our analysis informs efforts to develop harmonized standards, guidelines by the European Commission, as well as benchmarks and measurement methodologies under Art. 15(2) AIA. With this, we seek to bridge the gap between legal terminology and ML research, fostering a better alignment between research and implementation efforts.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting CLIP with Hierarchical Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2502.20578</link>
<guid>https://arxiv.org/abs/2502.20578</guid>
<content:encoded><![CDATA[
arXiv:2502.20578v2 Announce Type: replace-cross 
Abstract: Sparse autoencoders (SAEs) are useful for detecting and steering interpretable features in neural networks, with particular potential for understanding complex multimodal representations. Given their ability to uncover interpretable features, SAEs are particularly valuable for analyzing large-scale vision-language models (e.g., CLIP and SigLIP), which are fundamental building blocks in modern systems yet remain challenging to interpret and control. However, current SAE methods are limited by optimizing both reconstruction quality and sparsity simultaneously, as they rely on either activation suppression or rigid sparsity constraints. To this end, we introduce Matryoshka SAE (MSAE), a new architecture that learns hierarchical representations at multiple granularities simultaneously, enabling a direct optimization of both metrics without compromise. MSAE establishes a new state-of-the-art Pareto frontier between reconstruction quality and sparsity for CLIP, achieving 0.99 cosine similarity and less than 0.1 fraction of variance unexplained while maintaining ~80% sparsity. Finally, we demonstrate the utility of MSAE as a tool for interpreting and controlling CLIP by extracting over 120 semantic concepts from its representation to perform concept-based similarity search and bias analysis in downstream tasks like CelebA. We make the codebase available at https://github.com/WolodjaZ/MSAE.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Steer Learners in Games</title>
<link>https://arxiv.org/abs/2502.20770</link>
<guid>https://arxiv.org/abs/2502.20770</guid>
<content:encoded><![CDATA[
arXiv:2502.20770v2 Announce Type: replace-cross 
Abstract: We consider the problem of learning to exploit learning algorithms through repeated interactions in games. Specifically, we focus on the case of repeated two player, finite-action games, in which an optimizer aims to steer a no-regret learner to a Stackelberg equilibrium without knowledge of its payoffs. We first show that this is impossible if the optimizer only knows that the learner is using an algorithm from the general class of no-regret algorithms. This suggests that the optimizer requires more information about the learner's objectives or algorithm to successfully exploit them. Building on this intuition, we reduce the problem for the optimizer to that of recovering the learner's payoff structure. We demonstrate the effectiveness of this approach if the learner's algorithm is drawn from a smaller class by analyzing two examples: one where the learner uses an ascent algorithm, and another where the learner uses stochastic mirror ascent with known regularizer and step sizes.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred Datapoints</title>
<link>https://arxiv.org/abs/2503.01747</link>
<guid>https://arxiv.org/abs/2503.01747</guid>
<content:encoded><![CDATA[
arXiv:2503.01747v3 Announce Type: replace-cross 
Abstract: Rigorous statistical evaluations of large language models (LLMs), including valid error bars and significance testing, are essential for meaningful and reliable performance assessment. Currently, when such statistical measures are reported, they typically rely on the Central Limit Theorem (CLT). In this position paper, we argue that while CLT-based methods for uncertainty quantification are appropriate when benchmarks consist of thousands of examples, they fail to provide adequate uncertainty estimates for LLM evaluations that rely on smaller, highly specialized benchmarks. In these small-data settings, we demonstrate that CLT-based methods perform very poorly, usually dramatically underestimating uncertainty (i.e. producing error bars that are too small). We give recommendations for alternative frequentist and Bayesian methods that are both easy to implement and more appropriate in these increasingly common scenarios. We provide a simple Python library for these Bayesian methods at https://github.com/sambowyer/bayes_evals .
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Reasoning with LLMs for k-anonymity Estimation</title>
<link>https://arxiv.org/abs/2503.09674</link>
<guid>https://arxiv.org/abs/2503.09674</guid>
<content:encoded><![CDATA[
arXiv:2503.09674v3 Announce Type: replace-cross 
Abstract: Probabilistic reasoning is a key aspect of both human and artificial intelligence that allows for handling uncertainty and ambiguity in decision-making. In this paper, we introduce a new numerical reasoning task under uncertainty for large language models, focusing on estimating the privacy risk of user-generated documents containing privacy-sensitive information. We propose BRANCH, a new LLM methodology that estimates the k-privacy value of a text-the size of the population matching the given information. BRANCH factorizes a joint probability distribution of personal information as random variables. The probability of each factor in a population is estimated separately using a Bayesian network and combined to compute the final k-value. Our experiments show that this method successfully estimates the k-value 73% of the time, a 13% increase compared to o3-mini with chain-of-thought reasoning. We also find that LLM uncertainty is a good indicator for accuracy, as high-variance predictions are 37.47% less accurate on average.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Discrete Diffusion</title>
<link>https://arxiv.org/abs/2503.09790</link>
<guid>https://arxiv.org/abs/2503.09790</guid>
<content:encoded><![CDATA[
arXiv:2503.09790v2 Announce Type: replace-cross 
Abstract: Discrete diffusion models are a class of generative models that construct sequences by progressively denoising samples from a categorical noise distribution. Beyond their rapidly growing ability to generate coherent natural language, these models present a new and important opportunity to enforce sequence-level constraints, a capability that current autoregressive models cannot natively provide. This paper capitalizes on this opportunity by introducing Constrained Discrete Diffusion (CDD), a novel integration of differentiable constraint optimization within the diffusion process to ensure adherence to constraints, logic rules, or safety requirements for generated sequences. Unlike conventional text generators that often rely on post-hoc filtering or model retraining for controllable generation, CDD directly imposes constraints into the discrete diffusion sampling process, resulting in a training-free and effective approach. Experiments in toxicity-controlled text generation, property-constrained molecule design, and instruction-constrained text completion demonstrate that CDD achieves zero constraint violations in a diverse array of tasks while preserving fluency, novelty, and coherence while outperforming autoregressive and existing discrete diffusion approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond</title>
<link>https://arxiv.org/abs/2503.10460</link>
<guid>https://arxiv.org/abs/2503.10460</guid>
<content:encoded><![CDATA[
arXiv:2503.10460v4 Announce Type: replace-cross 
Abstract: This paper introduces Light-R1, an open-source suite for training long reasoning models using reproducible and cost-effective methodology. Given the proprietary nature of data used in the DeepSeek-R1 series, we develop an alternative approach leveraging exclusively public data and models. Our curriculum training progressively increases data difficulty, combined with multi-staged post-training. Our Light-R1-32B model, trained from Qwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math reasoning.
  Experimental results show that this curriculum approach becomes more effective when distinct, diverse datasets are available for different training stages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on proprietary data) with 3,000 challenging examples from our curriculum dataset yielded state-of-the-art 7B and 14B models, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.
  Furthermore, we extend our work by applying GRPO on long reasoning models. Our final Light-R1-14B-DS achieves SOTA performance among 14B models in math, with AIME24 & 25 scores of 74.0 and 60.2 respectively, surpassing many 32B models and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training, Light-R1-14B-DS demonstrates strong cross-domain generalization.
  Light-R1 represents a significant advancement in making sophisticated reasoning models more accessible and implementable in real-world applications. Our models, training data and code have been made available at https://github.com/Qihoo360/Light-R1.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Resilient and Sustainable Global Industrial Systems: An Evolutionary-Based Approach</title>
<link>https://arxiv.org/abs/2503.11688</link>
<guid>https://arxiv.org/abs/2503.11688</guid>
<content:encoded><![CDATA[
arXiv:2503.11688v2 Announce Type: replace-cross 
Abstract: This paper presents a new complex optimization problem in the field of automatic design of advanced industrial systems and proposes a hybrid optimization approach to solve the problem. The problem is multi-objective as it aims at finding solutions that minimize CO2 emissions, transportation time, and costs. The optimization approach combines an evolutionary algorithm and classical mathematical programming to design resilient and sustainable global manufacturing networks. Further, it makes use of the OWL ontology for data consistency and constraint management. The experimental validation demonstrates the effectiveness of the approach in both single and double sourcing scenarios. The proposed methodology, in general, can be applied to any industry case with complex manufacturing and supply chain challenges.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing</title>
<link>https://arxiv.org/abs/2503.19385</link>
<guid>https://arxiv.org/abs/2503.19385</guid>
<content:encoded><![CDATA[
arXiv:2503.19385v4 Announce Type: replace-cross 
Abstract: We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2503.22194</link>
<guid>https://arxiv.org/abs/2503.22194</guid>
<content:encoded><![CDATA[
arXiv:2503.22194v2 Announce Type: replace-cross 
Abstract: We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices</title>
<link>https://arxiv.org/abs/2504.03312</link>
<guid>https://arxiv.org/abs/2504.03312</guid>
<content:encoded><![CDATA[
arXiv:2504.03312v2 Announce Type: replace-cross 
Abstract: Large Language Models have significantly advanced natural language processing, achieving remarkable performance in tasks such as language generation, translation, and reasoning. However, their substantial computational requirements restrict deployment to high-end systems, limiting accessibility on consumer-grade devices. This challenge is especially pronounced for under-resourced languages like those spoken in the Iberian Peninsula, where relatively limited linguistic resources and benchmarks hinder effective evaluation. This work presents a comprehensive evaluation of compact state-of-the-art LLMs across several essential NLP tasks tailored for Iberian languages. The results reveal that while some models consistently excel in certain tasks, significant performance gaps remain, particularly for languages such as Basque. These findings highlight the need for further research on balancing model compactness with robust multilingual performance
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement</title>
<link>https://arxiv.org/abs/2504.03561</link>
<guid>https://arxiv.org/abs/2504.03561</guid>
<content:encoded><![CDATA[
arXiv:2504.03561v2 Announce Type: replace-cross 
Abstract: In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations</title>
<link>https://arxiv.org/abs/2504.06792</link>
<guid>https://arxiv.org/abs/2504.06792</guid>
<content:encoded><![CDATA[
arXiv:2504.06792v2 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) models achieve a favorable trade-off between performance and inference efficiency by activating only a subset of experts. However, the memory overhead of storing all experts remains a major limitation, especially in large-scale MoE models such as DeepSeek-R1(671B). In this study, we investigate domain specialization and expert redundancy in large-scale MoE models and uncover a consistent behavior we term few-shot expert localization, with only a few in-domain demonstrations, the model consistently activates a sparse and stable subset of experts on tasks within the same domain. Building on this observation, we propose a simple yet effective pruning framework, EASY-EP, that leverages a few domain-specific demonstrations to identify and retain only the most relevant experts. EASY-EP comprises two key components: output-aware expert importance assessment and expert-level token contribution estimation. The former evaluates the importance of each expert for the current token by considering the gating scores and L2 norm of the outputs of activated experts, while the latter assesses the contribution of tokens based on representation similarities before and after routed experts. Experiments on DeepSeek-R1 and DeepSeek-V3-0324 show that our method can achieve comparable performances and $2.99\times$ throughput under the same memory budget with full model with only half the experts.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment</title>
<link>https://arxiv.org/abs/2504.15585</link>
<guid>https://arxiv.org/abs/2504.15585</guid>
<content:encoded><![CDATA[
arXiv:2504.15585v3 Announce Type: replace-cross 
Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs. To address this gap, this paper introduces, for the first time, the concept of "full-stack" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Enhanced Ensemble Filters</title>
<link>https://arxiv.org/abs/2504.17836</link>
<guid>https://arxiv.org/abs/2504.17836</guid>
<content:encoded><![CDATA[
arXiv:2504.17836v2 Announce Type: replace-cross 
Abstract: The filtering distribution in hidden Markov models evolves according to the law of a mean-field model in state-observation space. The ensemble Kalman filter (EnKF) approximates this mean-field model with an ensemble of interacting particles, employing a Gaussian ansatz for the joint distribution of the state and observation at each observation time. These methods are robust, but the Gaussian ansatz limits accuracy. This shortcoming is addressed by approximating the mean-field evolution using a novel form of neural operator taking probability distributions as input: a measure neural mapping (MNM). A MNM is used to design a novel approach to filtering, the MNM-enhanced ensemble filter (MNMEF), which is defined in both the mean-field limit and for interacting ensemble particle approximations. The ensemble approach uses empirical measures as input to the MNM and is implemented using the set transformer, which is invariant to ensemble permutation and allows for different ensemble sizes. The derivation of methods from a mean-field formulation allows a single parameterization of the algorithm to be deployed at different ensemble sizes. In practice fine-tuning of a small number of parameters, for specific ensemble sizes, further enhances the accuracy of the scheme. The promise of the approach is demonstrated by its superior root mean-square-error performance relative to leading methods in filtering the Lorenz 96 and Kuramoto-Sivashinsky models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Dimensional Factorization Limits in Discrete Diffusion Models through Quantum Joint Distribution Learning</title>
<link>https://arxiv.org/abs/2505.05151</link>
<guid>https://arxiv.org/abs/2505.05151</guid>
<content:encoded><![CDATA[
arXiv:2505.05151v2 Announce Type: replace-cross 
Abstract: This study explores quantum-enhanced discrete diffusion models to overcome classical limitations in learning high-dimensional distributions. We rigorously prove that classical discrete diffusion models, which calculate per-dimension transition probabilities to avoid exponential computational cost, exhibit worst-case linear scaling of Kullback-Leibler (KL) divergence with data dimension. To address this, we propose a Quantum Discrete Denoising Diffusion Probabilistic Model (QD3PM), which enables joint probability learning through diffusion and denoising in exponentially large Hilbert spaces. By deriving posterior states through quantum Bayes' theorem, similar to the crucial role of posterior probabilities in classical diffusion models, and by learning the joint probability, we establish a solid theoretical foundation for quantum-enhanced diffusion models. For denoising, we design a quantum circuit using temporal information for parameter sharing and learnable classical-data-controlled rotations for encoding. Exploiting joint distribution learning, our approach enables single-step sampling from pure noise, eliminating iterative requirements of existing models. Simulations demonstrate the proposed model's superior accuracy in modeling complex distributions compared to factorization methods. Hence, this paper establishes a new theoretical paradigm in generative models by leveraging the quantum advantage in joint distribution learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Sequential Numerical Prediction in Autoregressive Models</title>
<link>https://arxiv.org/abs/2505.13077</link>
<guid>https://arxiv.org/abs/2505.13077</guid>
<content:encoded><![CDATA[
arXiv:2505.13077v2 Announce Type: replace-cross 
Abstract: Autoregressive models have become the de facto choice for sequence generation tasks, but standard approaches treat digits as independent tokens and apply cross-entropy loss, overlooking the coherent structure of numerical sequences. This paper introduces Numerical Token Integrity Loss (NTIL) to address this gap. NTIL operates at two levels: (1) token-level, where it extends the Earth Mover's Distance (EMD) to preserve ordinal relationships between numerical values, and (2) sequence-level, where it penalizes the overall discrepancy between the predicted and actual sequences. This dual approach improves numerical prediction and integrates effectively with LLMs/MLLMs. Extensive experiments show significant performance improvements with NTIL.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Recommendation Bias with Causal Intervention on Evolving Personal Popularity</title>
<link>https://arxiv.org/abs/2505.14310</link>
<guid>https://arxiv.org/abs/2505.14310</guid>
<content:encoded><![CDATA[
arXiv:2505.14310v2 Announce Type: replace-cross 
Abstract: Popularity bias occurs when popular items are recommended far more frequently than they should be, negatively impacting both user experience and recommendation accuracy. Existing debiasing methods mitigate popularity bias often uniformly across all users and only partially consider the time evolution of users or items. However, users have different levels of preference for item popularity, and this preference is evolving over time. To address these issues, we propose a novel method called CausalEPP (Causal Intervention on Evolving Personal Popularity) for taming recommendation bias, which accounts for the evolving personal popularity of users. Specifically, we first introduce a metric called {Evolving Personal Popularity} to quantify each user's preference for popular items. Then, we design a causal graph that integrates evolving personal popularity into the conformity effect, and apply deconfounded training to mitigate the popularity bias of the causal graph. During inference, we consider the evolution consistency between users and items to achieve a better recommendation. Empirical studies demonstrate that CausalEPP outperforms baseline methods in reducing popularity bias while improving recommendation accuracy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BACON: A fully explainable AI model with graded logic for decision making problems</title>
<link>https://arxiv.org/abs/2505.14510</link>
<guid>https://arxiv.org/abs/2505.14510</guid>
<content:encoded><![CDATA[
arXiv:2505.14510v3 Announce Type: replace-cross 
Abstract: As machine learning models and autonomous agents are increasingly deployed in high-stakes, real-world domains such as healthcare, security, finance, and robotics, the need for transparent and trustworthy explanations has become critical. To ensure end-to-end transparency of AI decisions, we need models that are not only accurate but also fully explainable and human-tunable. We introduce BACON, a novel framework for automatically training explainable AI models for decision making problems using graded logic. BACON achieves high predictive accuracy while offering full structural transparency and precise, logic-based symbolic explanations, enabling effective human-AI collaboration and expert-guided refinement. We evaluate BACON with a diverse set of scenarios: classic Boolean approximation, Iris flower classification, house purchasing decisions and breast cancer diagnosis. In each case, BACON provides high-performance models while producing compact, human-verifiable decision logic. These results demonstrate BACON's potential as a practical and principled approach for delivering crisp, trustworthy explainable AI.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings</title>
<link>https://arxiv.org/abs/2505.14664</link>
<guid>https://arxiv.org/abs/2505.14664</guid>
<content:encoded><![CDATA[
arXiv:2505.14664v2 Announce Type: replace-cross 
Abstract: Cross-modal embeddings form the foundation for multi-modal models. However, visualization methods for interpreting cross-modal embeddings have been primarily confined to traditional dimensionality reduction (DR) techniques like PCA and t-SNE. These DR methods primarily focus on feature distributions within a single modality, whilst failing to incorporate metrics (e.g., CLIPScore) across multiple modalities. This paper introduces AKRMap, a new DR technique designed to visualize cross-modal embeddings metric with enhanced accuracy by learning kernel regression of the metric landscape in the projection space. Specifically, AKRMap constructs a supervised projection network guided by a post-projection kernel regression loss, and employs adaptive generalized kernels that can be jointly optimized with the projection. This approach enables AKRMap to efficiently generate visualizations that capture complex metric distributions, while also supporting interactive features such as zoom and overlay for deeper exploration. Quantitative experiments demonstrate that AKRMap outperforms existing DR methods in generating more accurate and trustworthy visualizations. We further showcase the effectiveness of AKRMap in visualizing and comparing cross-modal embeddings for text-to-image models. Code and demo are available at https://github.com/yilinye/AKRMap.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Reasoning-Driven Pedagogical Visualization</title>
<link>https://arxiv.org/abs/2505.16832</link>
<guid>https://arxiv.org/abs/2505.16832</guid>
<content:encoded><![CDATA[
arXiv:2505.16832v2 Announce Type: replace-cross 
Abstract: While foundation models (FMs), such as diffusion models and large vision-language models (LVLMs), have been widely applied in educational contexts, their ability to generate pedagogically effective visual explanations remains limited. Most existing approaches focus primarily on textual reasoning, overlooking the critical role of structured and interpretable visualizations in supporting conceptual understanding. To better assess the visual reasoning capabilities of FMs in educational settings, we introduce EduVisBench, a multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem sets requiring visually grounded solutions, along with a fine-grained evaluation rubric informed by pedagogical theory. Our empirical analysis reveals that existing models frequently struggle with the inherent challenge of decomposing complex reasoning and translating it into visual representations aligned with human cognitive processes. To address these limitations, we propose EduVisAgent, a multi-agent collaborative framework that coordinates specialized agents for instructional planning, reasoning decomposition, metacognitive prompting, and visualization design. Experimental results show that EduVisAgent substantially outperforms all baselines, achieving a 40.2% improvement and delivering more educationally aligned visualizations. EduVisBench and EduVisAgent are available at https://github.com/aiming-lab/EduVisBench and https://github.com/aiming-lab/EduVisAgent.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Practical Defect-Focused Automated Code Review</title>
<link>https://arxiv.org/abs/2505.17928</link>
<guid>https://arxiv.org/abs/2505.17928</guid>
<content:encoded><![CDATA[
arXiv:2505.17928v2 Announce Type: replace-cross 
Abstract: The complexity of code reviews has driven efforts to automate review comments, but prior approaches oversimplify this task by treating it as snippet-level code-to-text generation and relying on text similarity metrics like BLEU for evaluation. These methods overlook repository context, real-world merge request evaluation, and defect detection, limiting their practicality. To address these issues, we explore the full automation pipeline within the online recommendation service of a company with nearly 400 million daily active users, analyzing industry-grade C++ codebases comprising hundreds of thousands of lines of code. We identify four key challenges: 1) capturing relevant context, 2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and 4) integrating human workflows. To tackle these, we propose 1) code slicing algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a filtering mechanism for FAR reduction, and 4) a novel prompt design for better human interaction. Our approach, validated on real-world merge requests from historical fault reports, achieves a 2x improvement over standard LLMs and a 10x gain over previous baselines. While the presented results focus on C++, the underlying framework design leverages language-agnostic principles (e.g., AST-based analysis), suggesting potential for broader applicability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Feature Interactions from the Perspective of Quadratic Neural Networks for Click-through Rate Prediction</title>
<link>https://arxiv.org/abs/2505.17999</link>
<guid>https://arxiv.org/abs/2505.17999</guid>
<content:encoded><![CDATA[
arXiv:2505.17999v2 Announce Type: replace-cross 
Abstract: Hadamard Product (HP) has long been a cornerstone in click-through rate (CTR) prediction tasks due to its simplicity, effectiveness, and ability to capture feature interactions without additional parameters. However, the underlying reasons for its effectiveness remain unclear. In this paper, we revisit HP from the perspective of Quadratic Neural Networks (QNN), which leverage quadratic interaction terms to model complex feature relationships. We further reveal QNN's ability to expand the feature space and provide smooth nonlinear approximations without relying on activation functions. Meanwhile, we find that traditional post-activation does not further improve the performance of the QNN. Instead, mid-activation is a more suitable alternative. Through theoretical analysis and empirical evaluation of 25 QNN neuron formats, we identify a good-performing variant and make further enhancements on it. Specifically, we propose the Multi-Head Khatri-Rao Product as a superior alternative to HP and a Self-Ensemble Loss with dynamic ensemble capability within the same network to enhance computational efficiency and performance. Ultimately, we propose a novel neuron format, QNN-alpha, which is tailored for CTR prediction tasks. Experimental results show that QNN-alpha achieves new state-of-the-art performance on six public datasets while maintaining low inference latency, good scalability, and excellent compatibility. The code, running logs, and detailed hyperparameter configurations are available at: https://github.com/salmon1802/QNN.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GMoE: Empowering LLMs Fine-Tuning via MoE Graph Collaboration</title>
<link>https://arxiv.org/abs/2412.16216</link>
<guid>https://arxiv.org/abs/2412.16216</guid>
<content:encoded><![CDATA[
<div> Graph-based MoE, load imbalance, collaboration, fine-tuning, LLMs <br />
Summary: 
The paper introduces a novel Graph-based Mixture-of-Experts (GMoE) framework to address the load imbalance issue in large language models (LLMs). By utilizing a graph router function, GMoE enhances collaboration among experts, allowing them to dynamically allocate information from input data. Two coordination strategies, Poisson distribution-based distinction, and Normal distribution-based balance, further enhance the model stability during fine-tuning of LLMs. The implementation of GMoE using the Low-Rank Adaptation (LoRA) technique shows promising results on real-world datasets, demonstrating the effectiveness of facilitating collaborations among multiple experts in LLM fine-tuning. The experimental code is available at https://github.com/BAI-LAB/GMoE <br /><br /> <div>
arXiv:2412.16216v3 Announce Type: replace 
Abstract: The sparse Mixture-of-Experts (MoE) architecture of large language models (LLMs) confronts an inherent issue of load imbalance arising from the simplistic linear router strategy, which ultimately causes the instability and inefficient learning of LLMs. To address this challenge, we introduce a novel MoE graph-based framework $\textbf{GMoE}$, aimed at enhancing the collaboration among multiple experts. In GMoE, a graph router function is designed to capture the collaboration signals among experts. This enables all experts to dynamically allocate information derived from input data by sharing information with their neighboring experts. Moreover, we put forward two coordination strategies in GMoE: the $\textit{Poisson distribution-based distinction strategy}$ and the $\textit{Normal distribution-based balance strategy}$, to further release the capacity of each expert and increase the model stability in the fine-tuning of LLMs. Specifically, we leverage a parameter-efficient fine-tuning technique, i.e., Low-Rank Adaptation (LoRA), to implement the graph MoE architecture. Extensive experiments on four real-world benchmark datasets demonstrate the effectiveness of GMoE, showing the benefits of facilitating collaborations of multiple experts in LLM fine-tuning. The code of experimental implementation is available at https://github.com/BAI-LAB/GMoE
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Boundary of Diffusion-based Methods for Solving Constrained Optimization</title>
<link>https://arxiv.org/abs/2502.10330</link>
<guid>https://arxiv.org/abs/2502.10330</guid>
<content:encoded><![CDATA[
<div> Diffusion models, Continuous Constrained Optimization, DiOpt, supervised learning, multimodal distributions <br />
Summary: <br />
This paper explores the application of diffusion models to Continuous Constrained Optimization problems, specifically focusing on a two-dimensional constrained quadratic optimization example. The researchers identify challenges and propose the DiOpt framework, which consists of a warm-start phase and a bootstrapping phase to iteratively refine solutions while meeting constraints. The framework samples multiple candidate solutions and selects the optimal one through a screening process. Extensive experiments demonstrate the training dynamics, performance on various optimization problems, and the impact of hyperparameters on DiOpt's effectiveness. <div>
arXiv:2502.10330v3 Announce Type: replace 
Abstract: Diffusion models have achieved remarkable success in generative tasks such as image and video synthesis, and in control domains like robotics, owing to their strong generalization capabilities and proficiency in fitting complex multimodal distributions. However, their full potential in solving Continuous Constrained Optimization problems remains largely underexplored. Our work commences by investigating a two-dimensional constrained quadratic optimization problem as an illustrative example to explore the inherent challenges and issues when applying diffusion models to such optimization tasks and providing theoretical analyses for these observations. To address the identified gaps and harness diffusion models for Continuous Constrained Optimization, we build upon this analysis to propose a novel diffusion-based framework for optimization problems called DiOpt. This framework operates in two distinct phases: an initial warm-start phase, implemented via supervised learning, followed by a bootstrapping phase. This dual-phase architecture is designed to iteratively refine solutions, thereby improving the objective function while rigorously satisfying problem constraints. Finally, multiple candidate solutions are sampled, and the optimal one is selected through a screening process. We present extensive experiments detailing the training dynamics of DiOpt, its performance across a diverse set of Continuous Constrained Optimization problems, and an analysis of the impact of DiOpt's various hyperparameters.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIREN: Beyond Trajectories -- A Spectral Lens on Time</title>
<link>https://arxiv.org/abs/2505.17370</link>
<guid>https://arxiv.org/abs/2505.17370</guid>
<content:encoded><![CDATA[
<div> Forecasting, LTSF, Geometric structure, Wasserstein-2 distance, FRIREN <br />
Summary:
The study challenges the conventional approach of pointwise prediction in long-term time-series forecasting models and emphasizes the importance of capturing geometric changes. The proposed FRIREN model incorporates a normalizing flow block to embed data into a latent representation and generate optimal paths that preserve geometry. By focusing on the Wasserstein-2 distance and spectral analysis, FRIREN achieves high accuracy on chaotic systems like Lorenz-63 and Rossler, outperforming existing models like TimeMixer. The model offers interpretable predictions by identifying local and global dynamics, making it a promising solution for various forecasting tasks. FRIREN sets a new benchmark by combining generative flows with spectral analysis, providing accurate and interpretable long-term forecasting capabilities. <div>
arXiv:2505.17370v2 Announce Type: replace 
Abstract: Long-term time-series forecasting (LTSF) models are often presented as general-purpose solutions that can be applied across domains, implicitly assuming that all data is pointwise predictable. Using chaotic systems such as Lorenz-63 as a case study, we argue that geometric structure - not pointwise prediction - is the right abstraction for a dynamic-agnostic foundational model. Minimizing the Wasserstein-2 distance (W2), which captures geometric changes, and providing a spectral view of dynamics are essential for long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via Interpretable Eigen-networks), implements an augmented normalizing-flow block that embeds data into a normally distributed latent representation. It then generates a W2-efficient optimal path that can be decomposed into rotation, scaling, inverse rotation, and translation. This architecture yields locally generated, geometry-preserving predictions that are independent of the underlying dynamics, and a global spectral representation that functions as a finite Koopman operator with a small modification. This enables practitioners to identify which modes grow, decay, or oscillate, both locally and system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE 27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out), FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170, outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065. FRIREN is also competitive on standard LTSF datasets such as ETT and Weather. By connecting modern generative flows with classical spectral analysis, FRIREN makes long-term forecasting both accurate and interpretable, setting a new benchmark for LTSF model design.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeUQI: Near-Optimal Uniform Quantization Parameter Initialization</title>
<link>https://arxiv.org/abs/2505.17595</link>
<guid>https://arxiv.org/abs/2505.17595</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Post-training quantization, Uniform quantization, NeUQI, Model performance

Summary:
NeUQI is a method proposed to efficiently determine initial parameters for uniform quantization in Large language models (LLMs). It aims to reduce memory consumption and decoding latency, making deployment on consumer-grade GPUs and personal devices more feasible. NeUQI stands out for its ability to integrate with existing quantization methodologies and consistently outperform them in experiments with the LLaMA and Qwen families across various tasks. The method offers near-optimal parameter initialization for uniform quantization, enhancing the performance of post-quantization models. When combined with a lightweight distillation strategy, NeUQI surpasses the resource-intensive PV-tuning approach, showcasing superior performance in reducing memory footprint and inference costs. <div>
arXiv:2505.17595v2 Announce Type: replace 
Abstract: Large language models (LLMs) achieve impressive performance across domains but face significant challenges when deployed on consumer-grade GPUs or personal devices such as laptops, due to high memory consumption and inference costs. Post-training quantization (PTQ) of LLMs offers a promising solution that reduces their memory footprint and decoding latency. In practice, PTQ with uniform quantization representation is favored for its efficiency and ease of deployment since uniform quantization is widely supported by mainstream hardware and software libraries. Recent studies on $\geq 2$-bit uniform quantization have led to noticeable improvements in post-quantization model performance; however, they primarily focus on quantization methodologies, while the initialization of quantization parameters is underexplored and still relies on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method devoted to efficiently determining near-optimal initial parameters for uniform quantization. NeUQI is orthogonal to prior quantization methodologies and can seamlessly integrate with them. The experiments with the LLaMA and Qwen families on various tasks demonstrate that our NeUQI consistently outperforms existing methods. Furthermore, when combined with a lightweight distillation strategy, NeUQI can achieve superior performance to PV-tuning, a much more resource-intensive approach.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration</title>
<link>https://arxiv.org/abs/2505.17621</link>
<guid>https://arxiv.org/abs/2505.17621</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, Large Language Models, i-MENTOR, dense rewards, exploration<br />
Summary:<br />
The article discusses the limitations of existing reinforcement learning approaches for Large Language Models (LLMs) and introduces i-MENTOR as a novel method to address these challenges. i-MENTOR focuses on delivering dense rewards and promoting exploration in RL training by introducing trajectory-aware exploration rewards, dynamic reward scaling, and advantage-preserving reward implementation. These innovations aim to mitigate bias in token-level strategies, stabilize exploration and exploitation in large action spaces, and maintain advantage distribution integrity while providing exploratory guidance. Experimental results on three public datasets demonstrate i-MENTOR's effectiveness, showing a significant 22.39% improvement on the challenging Countdown-4 dataset. <div>
arXiv:2505.17621v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has emerged as a pivotal method for improving the reasoning capabilities of Large Language Models (LLMs). However, prevalent RL approaches such as Proximal Policy Optimization (PPO) and Group-Regularized Policy Optimization (GRPO) face critical limitations due to their reliance on sparse outcome-based rewards and inadequate mechanisms for incentivizing exploration. These limitations result in inefficient guidance for multi-step reasoning processes. Specifically, sparse reward signals fail to deliver effective or sufficient feedback, particularly for challenging problems. Furthermore, such reward structures induce systematic biases that prioritize exploitation of familiar trajectories over novel solution discovery. These shortcomings critically hinder performance in complex reasoning tasks, which inherently demand iterative refinement across ipntermediate steps. To address these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense rewards and amplify explorations in the RL-based training paradigm. i-MENTOR introduces three key innovations: trajectory-aware exploration rewards that mitigate bias in token-level strategies while maintaining computational efficiency; dynamic reward scaling to stabilize exploration and exploitation in large action spaces; and advantage-preserving reward implementation that maintains advantage distribution integrity while incorporating exploratory guidance. Experiments across three public datasets demonstrate i-MENTOR's effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations</title>
<link>https://arxiv.org/abs/2505.17708</link>
<guid>https://arxiv.org/abs/2505.17708</guid>
<content:encoded><![CDATA[
<div> Causal reasoning, discovery, causal analysis, causal representation learning, downstream tasks<br />
<br />
Summary: Causal analysis faces challenges due to data complexity. This paper reinterprets causal representation learning using a measurement model framework where learned representations are proxy measurements of latent causal variables. This approach clarifies conditions for supporting downstream causal reasoning and introduces a Test-based Measurement EXclusivity (T-MEX) score to evaluate representation quality. The T-MEX score is validated through diverse causal inference scenarios, including simulations and ecological video analysis, demonstrating its effectiveness in assessing learned representations for causal tasks. <div>
arXiv:2505.17708v2 Announce Type: replace 
Abstract: Causal reasoning and discovery, two fundamental tasks of causal analysis, often face challenges in applications due to the complexity, noisiness, and high-dimensionality of real-world data. Despite recent progress in identifying latent causal structures using causal representation learning (CRL), what makes learned representations useful for causal downstream tasks and how to evaluate them are still not well understood. In this paper, we reinterpret CRL using a measurement model framework, where the learned representations are viewed as proxy measurements of the latent causal variables. Our approach clarifies the conditions under which learned representations support downstream causal reasoning and provides a principled basis for quantitatively assessing the quality of representations using a new Test-based Measurement EXclusivity (T-MEX) score. We validate T-MEX across diverse causal inference scenarios, including numerical simulations and real-world ecological video analysis, demonstrating that the proposed framework and corresponding score effectively assess the identification of learned representations and their usefulness for causal downstream tasks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out of the Shadows: Exploring a Latent Space for Neural Network Verification</title>
<link>https://arxiv.org/abs/2505.17854</link>
<guid>https://arxiv.org/abs/2505.17854</guid>
<content:encoded><![CDATA[
<div> Neural networks; Formal verification; Latent space; Projection-based set representation; Zonotopes <br />
Summary:<br />
This study introduces a novel approach for formal verification of neural networks by designing a latent space that enables iterative input refinement based on output specifications. By leveraging projection-based set representations like zonotopes, the approach allows for efficient transfer of constraints between input and output spaces. The verification tool developed based on this approach significantly reduces the number of subproblems in a branch-and-bound procedure, achieving competitive performance with potential to rank highly in neural network verification competitions. Unlike many existing methods, the tool relies solely on matrix operations, enabling efficient GPU acceleration and enhancing overall speed. This innovative approach addresses the conservatism issues often encountered in neural network verification, providing a promising solution for ensuring the safety of neural networks in critical applications. <br /> <div>
arXiv:2505.17854v2 Announce Type: replace 
Abstract: Neural networks are ubiquitous. However, they are often sensitive to small input changes. Hence, to prevent unexpected behavior in safety-critical applications, their formal verification -- a notoriously hard problem -- is necessary. Many state-of-the-art verification algorithms use reachability analysis or abstract interpretation to enclose the set of possible outputs of a neural network. Often, the verification is inconclusive due to the conservatism of the enclosure. To address this problem, we design a novel latent space for formal verification that enables the transfer of output specifications to the input space for an iterative specification-driven input refinement, i.e., we iteratively reduce the set of possible inputs to only enclose the unsafe ones. The latent space is constructed from a novel view of projection-based set representations, e.g., zonotopes, which are commonly used in reachability analysis of neural networks. A projection-based set representation is a "shadow" of a higher-dimensional set -- a latent space -- that does not change during a set propagation through a neural network. Hence, the input set and the output enclosure are "shadows" of the same latent space that we can use to transfer constraints. We present an efficient verification tool for neural networks that uses our iterative refinement to significantly reduce the number of subproblems in a branch-and-bound procedure. Using zonotopes as a set representation, unlike many other state-of-the-art approaches, our approach can be realized by only using matrix operations, which enables a significant speed-up through efficient GPU acceleration. We demonstrate that our tool achieves competitive performance, which would place it among the top-ranking tools of the last neural network verification competition (VNN-COMP'24).
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models</title>
<link>https://arxiv.org/abs/2505.17871</link>
<guid>https://arxiv.org/abs/2505.17871</guid>
<content:encoded><![CDATA[
<div> Keywords: universal time series forecasting, data diversity, BLAST, pre-training corpus, grid sampling

Summary:
BLAST is a pre-training corpus designed to enhance data diversity for universal time series forecasting models. It incorporates 321 billion observations from various datasets and uses statistical metrics to characterize time series patterns. The data is clustered using grid-based partitioning for pattern-oriented sampling. By integrating grid sampling and mixup techniques, BLAST ensures a balanced coverage of diverse patterns. Models pre-trained on BLAST achieve state-of-the-art performance with fewer computational resources. The study highlights the importance of data diversity in improving training efficiency and model performance for the universal forecasting task. <div>
arXiv:2505.17871v2 Announce Type: replace 
Abstract: The advent of universal time series forecasting models has revolutionized zero-shot forecasting across diverse domains, yet the critical role of data diversity in training these models remains underexplored. Existing large-scale time series datasets often suffer from inherent biases and imbalanced distributions, leading to suboptimal model performance and generalization. To address this gap, we introduce BLAST, a novel pre-training corpus designed to enhance data diversity through a balanced sampling strategy. First, BLAST incorporates 321 billion observations from publicly available datasets and employs a comprehensive suite of statistical metrics to characterize time series patterns. Then, to facilitate pattern-oriented sampling, the data is implicitly clustered using grid-based partitioning. Furthermore, by integrating grid sampling and grid mixup techniques, BLAST ensures a balanced and representative coverage of diverse patterns. Experimental results demonstrate that models pre-trained on BLAST achieve state-of-the-art performance with a fraction of the computational resources and training tokens required by existing methods. Our findings highlight the pivotal role of data diversity in improving both training efficiency and model performance for the universal forecasting task.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.17872</link>
<guid>https://arxiv.org/abs/2505.17872</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-task forecasting, Expressiveness Bottleneck, LoRA modules, Mixture-of-LoRA (MoLA) model, Time-series forecasting

Summary:
Multi-task forecasting in time-series forecasting (TSF) faces the Expressiveness Bottleneck, leading to errors even with optimal representations. To overcome this limitation, a two-stage framework is proposed: first pre-training a foundation model for one-step-ahead prediction, then adapting it using step-specific LoRA modules. The Mixture-of-LoRA (MoLA) model uses adaptively weighted LoRA experts to achieve partial parameter sharing across steps, enhancing efficiency and forecasting performance by exploiting interdependencies between forecast steps. Experimental results demonstrate that MoLA significantly improves model expressiveness and outperforms existing time-series forecasting methods. The code for MoLA is available at https://anonymous.4open.science/r/MoLA-BC92. 

<br /><br />Summary: <div>
arXiv:2505.17872v2 Announce Type: replace 
Abstract: Multi-task forecasting has become the standard approach for time-series forecasting (TSF). However, we show that it suffers from an Expressiveness Bottleneck, where predictions at different time steps share the same representation, leading to unavoidable errors even with optimal representations. To address this issue, we propose a two-stage framework: first, pre-train a foundation model for one-step-ahead prediction; then, adapt it using step-specific LoRA modules.This design enables the foundation model to handle any number of forecast steps while avoiding the expressiveness bottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which employs adaptively weighted LoRA experts to achieve partial parameter sharing across steps. This approach enhances both efficiency and forecasting performance by exploiting interdependencies between forecast steps. Experiments show that MoLA significantly improves model expressiveness and outperforms state-of-the-art time-series forecasting methods. Code is available at https://anonymous.4open.science/r/MoLA-BC92.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding</title>
<link>https://arxiv.org/abs/2505.17939</link>
<guid>https://arxiv.org/abs/2505.17939</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Topological Deep Learning, Semi-Simplicial Neural Networks, Routing-SSNs, Brain dynamics representation learning

Summary:
Semi-Simplicial Neural Networks (SSNs) are introduced as a class of Topological Deep Learning (TDL) models that operate on semi-simplicial sets to capture directed higher-order motifs in complex systems such as brain networks. Routing-SSNs dynamically select informative relations in a learnable way, enhancing scalability. SSNs are proven to be more expressive than standard graph and TDL models. A new framework for brain dynamics representation learning based on SSNs' ability to recover topological descriptors is presented, achieving state-of-the-art performance in brain dynamics classification tasks. Empirical results show that SSNs outperform other models by up to 50% in accuracy. Additionally, SSNs demonstrate competitive performance in node classification and edge regression tasks. The code and data will be made publicly available.<br /><br />Summary: <div>
arXiv:2505.17939v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) excel at learning from pairwise interactions but often overlook multi-way and hierarchical relationships. Topological Deep Learning (TDL) addresses this limitation by leveraging combinatorial topological spaces. However, existing TDL models are restricted to undirected settings and fail to capture the higher-order directed patterns prevalent in many complex systems, e.g., brain networks, where such interactions are both abundant and functionally significant. To fill this gap, we introduce Semi-Simplicial Neural Networks (SSNs), a principled class of TDL models that operate on semi-simplicial sets -- combinatorial structures that encode directed higher-order motifs and their directional relationships. To enhance scalability, we propose Routing-SSNs, which dynamically select the most informative relations in a learnable manner. We prove that SSNs are strictly more expressive than standard graph and TDL models. We then introduce a new principled framework for brain dynamics representation learning, grounded in the ability of SSNs to provably recover topological descriptors shown to successfully characterize brain activity. Empirically, SSNs achieve state-of-the-art performance on brain dynamics classification tasks, outperforming the second-best model by up to 27%, and message passing GNNs by up to 50% in accuracy. Our results highlight the potential of principled topological models for learning from structured brain data, establishing a unique real-world case study for TDL. We also test SSNs on standard node classification and edge regression tasks, showing competitive performance. We will make the code and data publicly available.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17988</link>
<guid>https://arxiv.org/abs/2505.17988</guid>
<content:encoded><![CDATA[
<div> RL, SFT, efficiency, re-distillation, empirical success  
Summary:  
- Small-scale SFT influences RL but is inefficient.  
- A framework comparing SFT and RL efficiency using sample effect measurement is proposed.  
- SFT efficiency is limited by training data based on hypothetical analysis.  
- Re-distillation technique is introduced to fine-tune pretrain model through distillation from RL-trained policy.  
- Empirical verification on Knight & Knave and MATH datasets shows re-distillation's efficiency, surpassing DeepSeek-V3-0324 with fewer samples and less computation, and matching instruct-tuned variant without RL on MATH with only 500 samples.  
<br /><br />Summary: <div>
arXiv:2505.17988v2 Announce Type: replace 
Abstract: R1-style Reinforcement Learning (RL) significantly enhances Large Language Models' reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&amp;K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: https://github.com/on1262/deep-reasoning
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective</title>
<link>https://arxiv.org/abs/2505.17997</link>
<guid>https://arxiv.org/abs/2505.17997</guid>
<content:encoded><![CDATA[
<div> Keywords: VAPO framework, reinforcement learning, long chain-of-thought reasoning tasks, large language models, value function approximation

Summary:<br />
The paper discusses the VAPO framework, which has shown significant empirical success in enhancing the efficiency and reliability of reinforcement learning for tasks requiring long chain-of-thought reasoning with large language models. It addresses challenges such as value model bias, sequence length variations, and sparse reward signals, leading to state-of-the-art performance. Theoretical exploration of VAPO's mechanisms and limitations is crucial for future advancements. Key areas of discussion include value function approximation in complex reasoning spaces, the optimality of adaptive advantage estimation, the impact of token-level optimization, and challenges related to exploration and generalization. This theoretical perspective aims to guide future research efforts in developing more robust and generalizable reasoning agents.<br /><br />Summary: <div>
arXiv:2505.17997v2 Announce Type: replace 
Abstract: The VAPO framework has demonstrated significant empirical success in enhancing the efficiency and reliability of reinforcement learning for long chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By systematically addressing challenges such as value model bias, heterogeneous sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art performance. While its practical benefits are evident, a deeper theoretical understanding of its underlying mechanisms and potential limitations is crucial for guiding future advancements. This paper aims to initiate such a discussion by exploring VAPO from a theoretical perspective, highlighting areas where its assumptions might be challenged and where further investigation could yield more robust and generalizable reasoning agents. We delve into the intricacies of value function approximation in complex reasoning spaces, the optimality of adaptive advantage estimation, the impact of token-level optimization, and the enduring challenges of exploration and generalization.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System</title>
<link>https://arxiv.org/abs/2410.09403</link>
<guid>https://arxiv.org/abs/2410.09403</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, large language models, multi-agent system, scientific research, collaboration <br />
Summary: <br />
The article introduces Virtual Scientists (VirSci), a multi-agent system based on large language models (LLMs) designed to simulate the collaborative nature of scientific research. VirSci utilizes a team of agents to collectively generate, evaluate, and refine research ideas, surpassing existing methods in producing novel scientific concepts. Through extensive experimentation, the article demonstrates the effectiveness of this approach in fostering innovation. The study also delves into the collaborative mechanisms that enhance idea novelty, offering valuable insights for future research endeavors. The code for VirSci is openly available for further exploration and development. In summary, VirSci represents a significant leap forward in the quest for autonomous scientific discovery by leveraging AI technologies and collaborative teamwork. <br /> <div>
arXiv:2410.09403v4 Announce Type: replace-cross 
Abstract: The rapid advancement of scientific progress requires innovative tools that can accelerate knowledge discovery. Although recent AI methods, particularly large language models (LLMs), have shown promise in tasks such as hypothesis generation and experimental design, they fall short of replicating the collaborative nature of real-world scientific practices, where diverse experts work together in teams to tackle complex problems. To address the limitations, we propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci), designed to mimic the teamwork inherent in scientific research. VirSci organizes a team of agents to collaboratively generate, evaluate, and refine research ideas. Through comprehensive experiments, we demonstrate that this multi-agent approach outperforms the state-of-the-art method in producing novel scientific ideas. We further investigate the collaboration mechanisms that contribute to its tendency to produce ideas with higher novelty, offering valuable insights to guide future research and illuminating pathways toward building a robust system for autonomous scientific discovery. The code is available at https://github.com/open-sciencelab/Virtual-Scientists.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When More is Less: Understanding Chain-of-Thought Length in LLMs</title>
<link>https://arxiv.org/abs/2502.07266</link>
<guid>https://arxiv.org/abs/2502.07266</guid>
<content:encoded><![CDATA[
<div> Optimal CoT Length, Chain-of-Thought Reasoning, Large Language Models, Performance, Simplicity Bias<br />
<br />Summary: This paper challenges the assumption that longer Chain-of-Thought (CoT) reasoning is always better for Large Language Models (LLMs). Through real-world observations, experiments, and analysis, it is shown that task accuracy follows an inverted U-shaped curve with CoT length, with performance initially improving but eventually declining with more CoT steps. The optimal CoT length scales with task difficulty and model capability, revealing a simplicity bias where more capable models prefer shorter CoTs. This bias is also observed in Reinforcement Learning training where models gravitate towards shorter CoTs as accuracy improves. A theoretical model is proposed to explain these phenomena and guide the calibration of CoTs for optimal reasoning performance tailored to task complexity and model capability. Practical benefits are demonstrated by training with optimally-lengthed CoTs and employing length-aware filtering during inference. Overall, this study offers insights into the "overthinking" phenomenon and provides guidelines for adaptive CoT calibration in LLMs. <br /><br /> <div>
arXiv:2502.07266v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) employ Chain-of-Thought (CoT) reasoning to deconstruct complex problems. While longer CoTs are often presumed superior, this paper challenges that notion, arguing that longer is not always better. Drawing on combined evidence from real-world observations, controlled experiments, and theoretical analysis, we demonstrate that task accuracy typically follows an inverted U-shaped curve with CoT length, where performance initially improves but eventually decreases as the number of CoT steps increases. With controlled experiments, we further uncover the scaling behaviors of the optimal CoT length: it increases with task difficulty but decreases with model capability, exposing an inherent simplicity bias where more capable models favor shorter, more efficient CoT reasoning. This bias is also evident in Reinforcement Learning (RL) training, where models gravitate towards shorter CoTs as their accuracy improves. To have a deep understanding of these dynamics, we establish a simple theoretical model that formally proves these phenomena, including the optimal length's scaling laws and the emergence of simplicity bias during RL. Guided by this framework, we demonstrate significant practical benefits from training with optimally-lengthed CoTs and employing length-aware filtering at inference. These findings offer both a principled understanding of the "overthinking" phenomenon and multiple practical guidelines for CoT calibration, enabling LLMs to achieve optimal reasoning performance with adaptive CoTs tailored to task complexity and model capability.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines</title>
<link>https://arxiv.org/abs/2502.12051</link>
<guid>https://arxiv.org/abs/2502.12051</guid>
<content:encoded><![CDATA[
<div> scaling laws, AI models, model performance, computational resources, neural networks

Summary:
Neural scaling laws have transformed the design and optimization of large-scale AI models, revealing predictable relationships between model size, dataset volume, and computational resources. While power-law relationships in model performance have led to compute-optimal scaling strategies, recent studies have shown limitations across different architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Scaling behaviors also differ across domains like vision, reinforcement learning, and fine-tuning. It is crucial to develop more nuanced approaches in scaling strategies to address challenges such as data efficiency, inference scaling, and architecture-specific constraints. Adaptive scaling strategies tailored to real-world applications are recommended, as scaling laws may not always generalize across all architectures and training strategies. <div>
arXiv:2502.12051v3 Announce Type: replace-cross 
Abstract: Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMEnets: Flow, Material, and Energy networks for non-ideal plug flow reactor design</title>
<link>https://arxiv.org/abs/2505.20300</link>
<guid>https://arxiv.org/abs/2505.20300</guid>
<content:encoded><![CDATA[
<div> framework, machine learning, plug flow reactors, FMEnets, physics-informed<br />
<br />
Summary: 
The article introduces FMEnets, a physics-informed machine learning framework for non-ideal plug flow reactors. FMEnets integrates fundamental equations to create a multi-scale network model, enabling both forward and inverse problem-solving. It predicts various profiles using limited input data and can infer unknown parameters and states. FMEnets can be implemented as FME-PINNs or FME-KANs, with the latter being more robust to noise. The framework is tested on different reaction scenarios, showing accuracy comparable to finite element simulations. With relative errors below 2.5% for unknown parameters, FMEnets provides an efficient alternative for reactor design and optimization. It also allows the integration of empirical correlations and limited experimental data, guiding reactor design effectively. <div>
arXiv:2505.20300v1 Announce Type: new 
Abstract: We propose FMEnets, a physics-informed machine learning framework for the design and analysis of non-ideal plug flow reactors. FMEnets integrates the fundamental governing equations (Navier-Stokes for fluid flow, material balance for reactive species transport, and energy balance for temperature distribution) into a unified multi-scale network model. The framework is composed of three interconnected sub-networks with independent optimizers that enable both forward and inverse problem-solving. In the forward mode, FMEnets predicts velocity, pressure, species concentrations, and temperature profiles using only inlet and outlet information. In the inverse mode, FMEnets utilizes sparse multi-residence-time measurements to simultaneously infer unknown kinetic parameters and states. FMEnets can be implemented either as FME-PINNs, which employ conventional multilayer perceptrons, or as FME-KANs, based on Kolmogorov-Arnold Networks. Comprehensive ablation studies highlight the critical role of the FMEnets architecture in achieving accurate predictions. Specifically, FME-KANs are more robust to noise than FME-PINNs, although both representations are comparable in accuracy and speed in noise-free conditions. The proposed framework is applied to three different sets of reaction scenarios and is compared with finite element simulations. FMEnets effectively captures the complex interactions, achieving relative errors less than 2.5% for the unknown kinetic parameters. The new network framework not only provides a computationally efficient alternative for reactor design and optimization, but also opens new avenues for integrating empirical correlations, limited and noisy experimental data, and fundamental physical equations to guide reactor design.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint-stochastic-approximation Random Fields with Application to Semi-supervised Learning</title>
<link>https://arxiv.org/abs/2505.20330</link>
<guid>https://arxiv.org/abs/2505.20330</guid>
<content:encoded><![CDATA[
<div> Keywords: deep generative models, semi-supervised learning, mode missing, mode covering, joint-stochastic-approximation random fields 

Summary: 
The study explores issues in deep generative models (DGMs) like GANs and VAEs used in semi-supervised learning (SSL). It identifies problems like mode missing and mode covering in generation and the conflict between classification and generation in SSL with directed generative models. To overcome these challenges, the researchers introduce joint-stochastic-approximation random fields (JRFs) as a new algorithm for building deep undirected generative models. Synthetic experiments show that JRFs effectively balance mode covering and mode missing, accurately reflect empirical data distribution, and achieve competitive classification results on popular datasets like MNIST, SVHN, and CIFAR-10 in SSL. Additionally, JRFs also demonstrate good generation capabilities. Overall, JRFs offer a promising solution for improving the performance of deep generative models in semi-supervised learning tasks. 

<br /><br />Summary: <div>
arXiv:2505.20330v1 Announce Type: new 
Abstract: Our examination of deep generative models (DGMs) developed for semi-supervised learning (SSL), mainly GANs and VAEs, reveals two problems. First, mode missing and mode covering phenomenons are observed in genertion with GANs and VAEs. Second, there exists an awkward conflict between good classification and good generation in SSL by employing directed generative models. To address these problems, we formally present joint-stochastic-approximation random fields (JRFs) -- a new family of algorithms for building deep undirected generative models, with application to SSL. It is found through synthetic experiments that JRFs work well in balancing mode covering and mode missing, and match the empirical data distribution well. Empirically, JRFs achieve good classification results comparable to the state-of-art methods on widely adopted datasets -- MNIST, SVHN, and CIFAR-10 in SSL, and simultaneously perform good generation.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PDFBench: A Benchmark for De novo Protein Design from Function</title>
<link>https://arxiv.org/abs/2505.20346</link>
<guid>https://arxiv.org/abs/2505.20346</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language processing, multimodal learning, de novo protein design, PDFBench, evaluation metrics

Summary:
PDFBench introduces a comprehensive benchmark for evaluating de novo protein design, addressing the challenges of proprietary datasets and evaluation rubrics. The benchmark supports description-guided design and keyword-guided design tasks, encompassing 22 metrics covering sequence plausibility, structural fidelity, language-protein alignment, novelty, and diversity. Five state-of-the-art baselines are evaluated, highlighting their strengths and weaknesses across tasks. The analysis of inter-metric correlations provides insights into the relationships between different metric categories and suggests guidelines for metric selection. PDFBench aims to drive future advancements in function-driven de novo protein design by offering a unified evaluation framework. 

<br /><br />Summary: <div>
arXiv:2505.20346v1 Announce Type: new 
Abstract: In recent years, while natural language processing and multimodal learning have seen rapid advancements, the field of de novo protein design has also experienced significant growth. However, most current methods rely on proprietary datasets and evaluation rubrics, making fair comparisons between different approaches challenging. Moreover, these methods often employ evaluation metrics that capture only a subset of the desired properties of designed proteins, lacking a comprehensive assessment framework. To address these, we introduce PDFBench, the first comprehensive benchmark for evaluating de novo protein design from function. PDFBench supports two tasks: description-guided design and keyword-guided design. To ensure fair and multifaceted evaluation, we compile 22 metrics covering sequence plausibility, structural fidelity, and language-protein alignment, along with measures of novelty and diversity. We evaluate five state-of-the-art baselines, revealing their respective strengths and weaknesses across tasks. Finally, we analyze inter-metric correlations, exploring the relationships between four categories of metrics, and offering guidelines for metric selection. PDFBench establishes a unified framework to drive future advances in function-driven de novo protein design.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decision Flow Policy Optimization</title>
<link>https://arxiv.org/abs/2505.20350</link>
<guid>https://arxiv.org/abs/2505.20350</guid>
<content:encoded><![CDATA[
<div> Flow-based models, decision-making, reinforcement learning, multi-modal action distributions, robotic control<br />
<br />
Summary: 
Generative models have shown significant progress in various fields. By incorporating flow-based models into reinforcement learning, complex multi-modal action distributions can be effectively modeled, leading to better robotic control in continuous action spaces. Existing methods often treat generative models as behavior models for fitting action distributions from datasets, separate from policy optimization. This separation limits training and performance. The proposed Decision Flow framework integrates multi-modal action distribution modeling and policy optimization. It formulates action generation as a flow decision-making process, enabling seamless optimization of the flow policy while capturing multi-modal action distributions. Rigorous proofs support Decision Flow's effectiveness, validated through extensive experiments in offline RL environments. Results show competitive or superior performance compared to established baselines.<br /><br /> <div>
arXiv:2505.20350v1 Announce Type: new 
Abstract: In recent years, generative models have shown remarkable capabilities across diverse fields, including images, videos, language, and decision-making. By applying powerful generative models such as flow-based models to reinforcement learning, we can effectively model complex multi-modal action distributions and achieve superior robotic control in continuous action spaces, surpassing the limitations of single-modal action distributions with traditional Gaussian-based policies. Previous methods usually adopt the generative models as behavior models to fit state-conditioned action distributions from datasets, with policy optimization conducted separately through additional policies using value-based sample weighting or gradient-based updates. However, this separation prevents the simultaneous optimization of multi-modal distribution fitting and policy improvement, ultimately hindering the training of models and degrading the performance. To address this issue, we propose Decision Flow, a unified framework that integrates multi-modal action distribution modeling and policy optimization. Specifically, our method formulates the action generation procedure of flow-based models as a flow decision-making process, where each action generation step corresponds to one flow decision. Consequently, our method seamlessly optimizes the flow policy while capturing multi-modal action distributions. We provide rigorous proofs of Decision Flow and validate the effectiveness through extensive experiments across dozens of offline RL environments. Compared with established offline RL baselines, the results demonstrate that our method achieves or matches the SOTA performance.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation</title>
<link>https://arxiv.org/abs/2505.20353</link>
<guid>https://arxiv.org/abs/2505.20353</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion Transformers, FastCache, generative models, inference acceleration, compression

Summary: 
FastCache is a novel framework designed to accelerate the inference process of Diffusion Transformers (DiT), which are known for their computational intensity. This framework leverages hidden-state-level caching and compression techniques to optimize the generation process while maintaining output quality. It incorporates a spatial-aware token selection mechanism to filter out redundant tokens based on their saliency in the hidden states. Additionally, a transformer-level cache is utilized to reuse latent activations across timesteps when statistically insignificant changes occur. These modules work together to reduce unnecessary computations and memory usage without compromising the fidelity of generation. Theoretical analysis demonstrates that FastCache maintains a bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across different DiT variants confirm significant reductions in latency and memory usage compared to other caching methods, while still producing high-quality generation outputs. The code implementation of FastCache is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2505.20353v1 Announce Type: new 
Abstract: Diffusion Transformers (DiT) are powerful generative models but remain computationally intensive due to their iterative structure and deep transformer stacks. To alleviate this inefficiency, we propose FastCache, a hidden-state-level caching and compression framework that accelerates DiT inference by exploiting redundancy within the model's internal representations. FastCache introduces a dual strategy: (1) a spatial-aware token selection mechanism that adaptively filters redundant tokens based on hidden state saliency, and (2) a transformer-level cache that reuses latent activations across timesteps when changes are statistically insignificant. These modules work jointly to reduce unnecessary computation while preserving generation fidelity through learnable linear approximation. Theoretical analysis shows that FastCache maintains bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across multiple DiT variants demonstrate substantial reductions in latency and memory usage, with best generation output quality compared to other cache methods, as measured by FID and t-FID. Code implementation of FastCache is available on GitHub at https://github.com/NoakLiu/FastCache-xDiT.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.20355</link>
<guid>https://arxiv.org/abs/2505.20355</guid>
<content:encoded><![CDATA[
<div> adaptation, low-rank, generative models, fine-tuning, GraLoRA

Summary:
Granular Low-Rank Adaptation (GraLoRA) is introduced as a novel method to address the limitations of Low-Rank Adaptation (LoRA) in fine-tuning generative models. The fundamental issue of overfitting when widening the bottleneck in LoRA is identified as stemming from gradient entanglement in unrelated input channels due to its structural bottleneck. GraLoRA partitions weight matrices into sub-blocks, each with its own low-rank adapter, effectively increasing representational capacity and approximating full fine-tuning (FFT) behavior. Experimental results on code generation and commonsense reasoning benchmarks demonstrate GraLoRA's superior performance over LoRA and other baselines, with up to an 8.5% absolute gain in Pass@1 on HumanEval+. This improvement is consistent across different model sizes and rank settings, making GraLoRA a scalable and robust solution for parameter-efficient fine-tuning. <div>
arXiv:2505.20355v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning and Interpreting Gravitational-Wave Features from CNNs with a Random Forest Approach</title>
<link>https://arxiv.org/abs/2505.20357</link>
<guid>https://arxiv.org/abs/2505.20357</guid>
<content:encoded><![CDATA[
<div> Keywords: convolutional neural networks, gravitational wave detection, interpretability, random forest classifier, feature extraction<br />
Summary:<br />
- The article introduces a hybrid architecture for gravitational wave detection, combining a CNN-based feature extractor with a random forest classifier for improved interpretability and performance.
- Physically interpretable metrics such as variance, SNR, waveform overlap, and peak amplitude are computed from the final convolutional layer, enhancing decision boundaries in the RF classifier.
- The hybrid model outperforms a baseline CNN model, achieving a 21% relative improvement in sensitivity and showing better detection of low-SNR signals.
- Feature attribution analysis via the RF model highlights the importance of both CNN-extracted and handcrafted features, with learned variance and CNN outputs being particularly informative.
- This approach demonstrates the value of incorporating physically motivated post-processing of CNN feature maps for interpretable and efficient gravitational wave detection, effectively leveraging deep learning techniques with domain knowledge.<br /> 
Summary: <div>
arXiv:2505.20357v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) have become widely adopted in gravitational wave (GW) detection pipelines due to their ability to automatically learn hierarchical features from raw strain data. However, the physical meaning of these learned features remains underexplored, limiting the interpretability of such models. In this work, we propose a hybrid architecture that combines a CNN-based feature extractor with a random forest (RF) classifier to improve both detection performance and interpretability. Unlike prior approaches that directly connect classifiers to CNN outputs, our method introduces four physically interpretable metrics - variance, signal-to-noise ratio (SNR), waveform overlap, and peak amplitude - computed from the final convolutional layer. These are jointly used with the CNN output in the RF classifier to enable more informed decision boundaries. Tested on long-duration strain datasets, our hybrid model outperforms a baseline CNN model, achieving a relative improvement of 21\% in sensitivity at a fixed false alarm rate of 10 events per month. Notably, it also shows improved detection of low-SNR signals (SNR $\le$ 10), which are especially vulnerable to misclassification in noisy environments. Feature attribution via the RF model reveals that both CNN-extracted and handcrafted features contribute significantly to classification decisions, with learned variance and CNN outputs ranked among the most informative. These findings suggest that physically motivated post-processing of CNN feature maps can serve as a valuable tool for interpretable and efficient GW detection, bridging the gap between deep learning and domain knowledge.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk-aware Direct Preference Optimization under Nested Risk Measure</title>
<link>https://arxiv.org/abs/2505.20359</link>
<guid>https://arxiv.org/abs/2505.20359</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, fine-tuning, human values, risk-awareness, preferences<br />
Summary:<br />
When fine-tuning Large Language Models to align with human values, maximizing reward can lead to better performance but also introduces risks. Existing methods use KL divergence to constrain deviations, but it may not be sufficient for tight risk control. This paper introduces Ra-DPO, a novel approach that incorporates risk-awareness using nested risk measures. It formulates a constrained risk-aware advantage function maximization problem and converts the Bradley-Terry model into a token-level representation. The objective function maximizes policy likelihood while suppressing deviation between trained and reference models using sequential risk ratio. Experimental results on three datasets show Ra-DPO's superior performance in balancing alignment and model drift. The code is available at https://github.com/zlj123-max/Ra-DPO. <br /> <div>
arXiv:2505.20359v1 Announce Type: new 
Abstract: When fine-tuning pre-trained Large Language Models (LLMs) to align with human values and intentions, maximizing the estimated reward can lead to superior performance, but it also introduces potential risks due to deviations from the reference model's intended behavior. Most existing methods typically introduce KL divergence to constrain deviations between the trained model and the reference model; however, this may not be sufficient in certain applications that require tight risk control. In this paper, we introduce Risk-aware Direct Preference Optimization (Ra-DPO), a novel approach that incorporates risk-awareness by employing a class of nested risk measures. This approach formulates a constrained risk-aware advantage function maximization problem and then converts the Bradley-Terry model into a token-level representation. The objective function maximizes the likelihood of the policy while suppressing the deviation between a trained model and the reference model using a sequential risk ratio, thereby enhancing the model's risk-awareness. Experimental results across three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and AlpacaEval, demonstrate the proposed method's superior performance in balancing alignment performance and model drift. Our code is opensourced at https://github.com/zlj123-max/Ra-DPO.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining</title>
<link>https://arxiv.org/abs/2505.20380</link>
<guid>https://arxiv.org/abs/2505.20380</guid>
<content:encoded><![CDATA[
<div> domain reweighting, language models, pretraining, adaptive, multi-target

Summary: 
The paper introduces GRAPE, a multi-source-multi-target domain reweighting framework for optimizing pretraining data mixtures across multiple target tasks simultaneously. GRAPE dynamically adjusts domain and task weights to prioritize tasks based on their learning difficulty during training. This adaptive process is formulated as a minimax optimization problem, utilizing group distributed-robust-optimization to adjust task weights and optimize domain weights for maximum loss reduction on prioritized tasks. Experimental results on ClimbLab and SlimPajama datasets show that GRAPE outperforms baseline methods in reasoning performance across 6 benchmarks. Moreover, when applied to multilingual targets, GRAPE effectively identifies optimal training mixtures from mainstream languages, enhancing language modeling capabilities across 8 low-resource target languages. <br /><br />Summary: <div>
arXiv:2505.20380v1 Announce Type: new 
Abstract: The performance of large language models (LLMs) across diverse downstream applications is fundamentally governed by the quality and composition of their pretraining corpora. Existing domain reweighting algorithms primarily optimize data mixtures for a single target task, thereby resulting in models that overfit to specialized objectives while exhibiting substantial performance degradation on other benchmarks. This paper introduces Group Robust Multi-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target domain reweighting framework designed to calibrate pretraining data mixtures for robust performance across multiple target tasks simultaneously. GRAPE dynamically adjusts sampling weights across source domains (domain weights) while concurrently modulating task weights that quantify the relative importance of each individual target task. This adaptive process prioritizes tasks based on their learning difficulty throughout training. We formulate this interleaved reweighting mechanism as a minimax optimization problem: The inner maximization adjusts task weights leveraging group distributed-robust-optimization (DRO), where those tasks demonstrating the least improvement under the current data mixture are prioritized with higher weights; The outer minimization then optimizes domain weights to maximize loss reduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama datasets demonstrate that GRAPE consistently outperforms baseline methods in terms of reasoning performance across 6 benchmarks. Furthermore, when applied to multilingual targets, GRAPE effectively identifies optimal training mixtures from mainstream languages, achieving superior language modeling capabilities across 8 low-resource target languages.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Holes in Latent Space: Topological Signatures Under Adversarial Influence</title>
<link>https://arxiv.org/abs/2505.20435</link>
<guid>https://arxiv.org/abs/2505.20435</guid>
<content:encoded><![CDATA[
<div> topological data analysis, language models, adversarial conditions, latent space dynamics, neuron-level framework
Summary: 
This study introduces the use of persistent homology (PH), a tool from topological data analysis, to examine the effects of adversarial conditions on language models (LLMs). The research focuses on two attack modes  backdoor fine-tuning and indirect prompt injection  and analyzes six state-of-the-art LLMs. The findings reveal that adversarial conditions lead to a compression of latent topologies, reducing structural diversity at smaller scales while emphasizing dominant features at larger scales. These topological changes are consistent across different layers, architectures, and model sizes, indicating a deep-seated impact of adversarial effects within the network. Additionally, a neuron-level PH framework is introduced to better understand information flow and transformation within and between layers, providing insights into the underlying mechanisms of shifts in LLM representations under distributional shift. This study highlights how PH can offer a systematic and comprehensive approach to interpreting representational dynamics in LLMs under adversarial conditions. 
<br /><br />Summary: <div>
arXiv:2505.20435v1 Announce Type: new 
Abstract: Understanding how adversarial conditions affect language models requires techniques that capture both global structure and local detail within high-dimensional activation spaces. We propose persistent homology (PH), a tool from topological data analysis, to systematically characterize multiscale latent space dynamics in LLMs under two distinct attack modes -- backdoor fine-tuning and indirect prompt injection. By analyzing six state-of-the-art LLMs, we show that adversarial conditions consistently compress latent topologies, reducing structural diversity at smaller scales while amplifying dominant features at coarser ones. These topological signatures are statistically robust across layers, architectures, model sizes, and align with the emergence of adversarial effects deeper in the network. To capture finer-grained mechanisms underlying these shifts, we introduce a neuron-level PH framework that quantifies how information flows and transforms within and across layers. Together, our findings demonstrate that PH offers a principled and unifying approach to interpreting representational dynamics in LLMs, particularly under distributional shift.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.20444</link>
<guid>https://arxiv.org/abs/2505.20444</guid>
<content:encoded><![CDATA[
<div> RoPE, VLMs, long-context capabilities, HoPE, video benchmarks <br />
Summary: <br />
Vision-Language Models (VLMs) have shown progress in multimodal tasks but struggle with long-context scenarios, especially in videos. Current methods for extending RoPE to capture spatial-temporal dependencies lack theoretical depth. The proposed Hybrid of Position Embedding (HoPE) addresses this issue by introducing a hybrid frequency allocation strategy and dynamic temporal scaling mechanism. It enhances semantic modeling over long contexts, leading to improved performance in long video understanding and retrieval tasks. Experiments on four video benchmarks demonstrate the effectiveness of HoPE, outperforming existing methods consistently. The code for HoPE is publicly available. <br /> <div>
arXiv:2505.20444v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the long-context capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, a Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces a hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long context, and a dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness. Code is available at https://github.com/hrlics/HoPE.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach</title>
<link>https://arxiv.org/abs/2505.20446</link>
<guid>https://arxiv.org/abs/2505.20446</guid>
<content:encoded><![CDATA[
<div> Generative modeling, time series, data-scarce conditions, performance evaluation, diffusion-based framework<br />
Summary:<br />
A study evaluated generative models in data-scarce settings, revealing a performance gap. A unified diffusion-based framework was proposed, pre-trained on diverse time series datasets for generalizable representations. The model includes dynamic convolutional layers and dataset token conditioning for domain-specific generation. It achieves state-of-the-art performance in few-shot settings without requiring abundant supervision, outperforming domain-specific baselines and even full dataset benchmarks. The model's strength lies in pre-training and cross-domain generalization. The work emphasizes few-shot generative modeling as a pivotal issue in time series research and advocates for unified solutions that efficiently scale across domains.  <br />Summary: <div>
arXiv:2505.20446v1 Announce Type: new 
Abstract: Generative modeling of time series is a central challenge in time series analysis, particularly under data-scarce conditions. Despite recent advances in generative modeling, a comprehensive understanding of how state-of-the-art generative models perform under limited supervision remains lacking. In this work, we conduct the first large-scale study evaluating leading generative models in data-scarce settings, revealing a substantial performance gap between full-data and data-scarce regimes. To close this gap, we propose a unified diffusion-based generative framework that can synthesize high-fidelity time series across diverse domains using just a few examples. Our model is pre-trained on a large, heterogeneous collection of time series datasets, enabling it to learn generalizable temporal representations. It further incorporates architectural innovations such as dynamic convolutional layers for flexible channel adaptation and dataset token conditioning for domain-aware generation. Without requiring abundant supervision, our unified model achieves state-of-the-art performance in few-shot settings-outperforming domain-specific baselines across a wide range of subset sizes. Remarkably, it also surpasses all baselines even when tested on full datasets benchmarks, highlighting the strength of pre-training and cross-domain generalization. We hope this work encourages the community to revisit few-shot generative modeling as a key problem in time series research and pursue unified solutions that scale efficiently across domains. Code is available at https://github.com/azencot-group/ImagenFew.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning for Multiple Change Point Detection in Non-stationary Time Series with Deep Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.20452</link>
<guid>https://arxiv.org/abs/2505.20452</guid>
<content:encoded><![CDATA[
<div> Active Learning, Deep Gaussian Processes, Multiple change point detection, Non-stationary time series, Spectral analysis

Summary:
The study introduces a novel algorithm that combines Active Learning (AL) with Deep Gaussian Processes (DGPs) to improve multiple change point (MCP) detection in non-stationary time series. By utilizing spectral analysis to identify potential changes and employing AL to strategically select new sampling points, the algorithm enhances efficiency. The integration of DGPs and spectral methods allows for adaptability to various spectral change behaviors and accurate localization of multiple change points. Experimental results on simulated and real-world data demonstrate the algorithm's superiority in detection accuracy and sampling efficiency for non-stationary time series.<br /><br />Summary: <div>
arXiv:2505.20452v1 Announce Type: new 
Abstract: Multiple change point (MCP) detection in non-stationary time series is challenging due to the variety of underlying patterns. To address these challenges, we propose a novel algorithm that integrates Active Learning (AL) with Deep Gaussian Processes (DGPs) for robust MCP detection. Our method leverages spectral analysis to identify potential changes and employs AL to strategically select new sampling points for improved efficiency. By incorporating the modeling flexibility of DGPs with the change-identification capabilities of spectral methods, our approach adapts to diverse spectral change behaviors and effectively localizes multiple change points. Experiments on both simulated and real-world data demonstrate that our method outperforms existing techniques in terms of detection accuracy and sampling efficiency for non-stationary time series.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction</title>
<link>https://arxiv.org/abs/2505.20454</link>
<guid>https://arxiv.org/abs/2505.20454</guid>
<content:encoded><![CDATA[
<div> Transformer, blast pressure prediction, surrogate model, CFD simulations, real-time alternative<br />
<br />
Summary: 
The article introduces BlastOFormer, a Transformer-based surrogate model for accurately predicting blast pressure fields. Traditional methods like empirical models and CFD simulations have limitations in capturing complex interactions and are computationally expensive. BlastOFormer, trained on a dataset from blastFoam CFD solver, outperforms CNNs and FNOs, achieving the highest R2 score (0.9516) and low error metrics. It requires only 6.4 milliseconds for inference, significantly faster than CFD simulations. The model shows superior spatial coherence and generalization capabilities, making it a real-time alternative for blast pressure estimation in complex environments. <div>
arXiv:2505.20454v1 Announce Type: new 
Abstract: Accurate prediction of blast pressure fields is essential for applications in structural safety, defense planning, and hazard mitigation. Traditional methods such as empirical models and computational fluid dynamics (CFD) simulations offer limited trade offs between speed and accuracy; empirical models fail to capture complex interactions in cluttered environments, while CFD simulations are computationally expensive and time consuming. In this work, we introduce BlastOFormer, a novel Transformer based surrogate model for full field maximum pressure prediction from arbitrary obstacle and charge configurations. BlastOFormer leverages a signed distance function (SDF) encoding and a grid to grid attention based architecture inspired by OFormer and Vision Transformer (ViT) frameworks. Trained on a dataset generated using the open source blastFoam CFD solver, our model outperforms convolutional neural networks (CNNs) and Fourier Neural Operators (FNOs) across both log transformed and unscaled domains. Quantitatively, BlastOFormer achieves the highest R2 score (0.9516) and lowest error metrics, while requiring only 6.4 milliseconds for inference, more than 600,000 times faster than CFD simulations. Qualitative visualizations and error analyses further confirm BlastOFormer's superior spatial coherence and generalization capabilities. These results highlight its potential as a real time alternative to conventional CFD approaches for blast pressure estimation in complex environments.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data</title>
<link>https://arxiv.org/abs/2505.20485</link>
<guid>https://arxiv.org/abs/2505.20485</guid>
<content:encoded><![CDATA[
<div> Analysis, Federated learning, Data heterogeneity, Global decision boundary, Forgetting

Summary:
In the study, the challenges of data heterogeneity in federated learning are addressed. Existing methods suffer from forgetting, where clients forget the global decision boundary and only learn the perfect local one. This forgetting occurs irrespective of the initial weights, leading to suboptimal performance. A new federated learning framework, FedProj, is introduced to overcome this issue. FedProj robustly learns the global decision boundary and prevents forgetting during local training. The framework incorporates a server-side ensemble knowledge transfer loss for better ensemble knowledge fusion and utilizes an episodic memory technique to regulate gradient updates and maintain the learned global decision boundary. Experimental results demonstrate that FedProj outperforms current state-of-the-art methods significantly, showcasing its effectiveness in handling data heterogeneity and improving model performance in federated learning.<br /><br />Summary: <div>
arXiv:2505.20485v1 Announce Type: new 
Abstract: The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints</title>
<link>https://arxiv.org/abs/2505.20515</link>
<guid>https://arxiv.org/abs/2505.20515</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific machine learning, neural differential equations, conservation laws, manifold projection, constraint violation error<br />
Summary:<br />
- The article introduces Manifold-Projected Neural ODEs (PNODEs) as a method to incorporate hard constraints in neural differential equations for modeling physical systems with conservation laws.
- PNODEs enforce algebraic constraints by projecting each ODE step onto the constraint manifold, showing superior performance in constraint violation error compared to baseline methods.
- The framework is derived from semi-explicit differential-algebraic equations (DAEs) and includes robust iterative and fast approximation variants.
- PNODEs outperform previous relaxation methods and achieve physically consistent long-horizon dynamics with low error tolerance and runtime.
- The results demonstrate that constraint projection provides a simple strategy for combining data-driven techniques with mechanistic modeling in scientific machine learning.<br /><br />Summary: <div>
arXiv:2505.20515v1 Announce Type: new 
Abstract: Despite the promise of scientific machine learning (SciML) in combining data-driven techniques with mechanistic modeling, existing approaches for incorporating hard constraints in neural differential equations (NDEs) face significant limitations. Scalability issues and poor numerical properties prevent these neural models from being used for modeling physical systems with complicated conservation laws. We propose Manifold-Projected Neural ODEs (PNODEs), a method that explicitly enforces algebraic constraints by projecting each ODE step onto the constraint manifold. This framework arises naturally from semi-explicit differential-algebraic equations (DAEs), and includes both a robust iterative variant and a fast approximation requiring a single Jacobian factorization. We further demonstrate that prior works on relaxation methods are special cases of our approach. PNODEs consistently outperform baselines across six benchmark problems achieving a mean constraint violation error below $10^{-10}$. Additionally, PNODEs consistently achieve lower runtime compared to other methods for a given level of error tolerance. These results show that constraint projection offers a simple strategy for learning physically consistent long-horizon dynamics.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Fully FP8 GEMM LLM Training at Scale</title>
<link>https://arxiv.org/abs/2505.20524</link>
<guid>https://arxiv.org/abs/2505.20524</guid>
<content:encoded><![CDATA[
<div> FP8, large language model, LLM architectures, matrix multiplications, low-precision training<br />
<br />
Summary: 
This study introduces a new class of large language model (LLM) architectures that support FP8 computation for all matrix multiplications (GEMMs) within transformer blocks, enabling significant throughput gains without compromising downstream performance. By reducing large outlier activations, stable long-term FP8 training is promoted. Key metrics are identified to monitor low-precision training and predict potential divergences in the future. Through these advancements, the adoption of FP8 data formats for LLM pre-training is expected to increase, offering improved efficiency and scalability in large-scale language modeling tasks. <div>
arXiv:2505.20524v1 Announce Type: new 
Abstract: Despite the significant potential of FP8 data formats for large language model (LLM) pre-training, their adoption has been limited due to challenges in maintaining stability at scale. Existing approaches often rely on suboptimal fine-grained FP8 kernels or fall back to higher-precision matrix multiplications (GEMMs) in sensitive components, such as attention projections, compromising potential throughput gains. We introduce a new class of LLM architectures that, for the first time, support FP8 computation for all GEMMs within transformer blocks during both forward and backward passes. This enables unprecedented throughput gains, particularly at scale, while matching the downstream performance of standard BF16 training. Our architecture design reduces large outlier activations, promoting stable long-term FP8 training. In addition, we identify key metrics to monitor low-precision training and predict potential future divergences.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-shot Robust Federated Learning of Independent Component Analysis</title>
<link>https://arxiv.org/abs/2505.20532</link>
<guid>https://arxiv.org/abs/2505.20532</guid>
<content:encoded><![CDATA[
<div> aggregation algorithm, independent component analysis, federated learning, geometric median, k-means clustering

Summary: 
This paper presents a novel robust one-shot aggregation framework for distributed and federated Independent Component Analysis (ICA) problems. The proposed algorithm utilizes a geometric median-based approach combined with k-means clustering to address the issue of permutation ambiguity in local client estimations. By partitioning client-provided estimators into clusters using k-means and then aggregating them within each cluster using the geometric median, the method proves to be effective even in highly heterogeneous scenarios where some clients have minimal sample sizes. The theoretical analysis incorporates error bounds of the geometric median with sample quantiles and misclustering rates of k-means. Simulation studies demonstrate the effectiveness of the approach in various heterogeneous settings. <div>
arXiv:2505.20532v1 Announce Type: new 
Abstract: This paper investigates a general robust one-shot aggregation framework for distributed and federated Independent Component Analysis (ICA) problem. We propose a geometric median-based aggregation algorithm that leverages $k$-means clustering to resolve the permutation ambiguity in local client estimations. Our method first performs k-means to partition client-provided estimators into clusters and then aggregates estimators within each cluster using the geometric median. This approach provably remains effective even in highly heterogeneous scenarios where at most half of the clients can observe only a minimal number of samples. The key theoretical contribution lies in the combined analysis of the geometric median's error bound-aided by sample quantiles-and the maximum misclustering rates of the aforementioned solution of $k$-means. The effectiveness of the proposed approach is further supported by simulation studies conducted under various heterogeneous settings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rotary Masked Autoencoders are Versatile Learners</title>
<link>https://arxiv.org/abs/2505.20535</link>
<guid>https://arxiv.org/abs/2505.20535</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, time-series, RoMAE, positional embeddings, representation learning

Summary: 
The article introduces the Rotary Masked Autoencoder (RoMAE) as a method for applying Transformers to irregular time-series without the need for specialized architectural modifications. RoMAE leverages Rotary Positional Embeddings (RoPE) to incorporate continuous positional information in multidimensional form, enhancing representation learning across various modalities like images, audio, and multivariate time-series. By outperforming specialized time-series architectures on challenging datasets such as the DESC ELAsTiCC Challenge, RoMAE showcases its versatility and efficiency. The study also delves into the reconstruction of embedded continuous positions, revealing that introducing learned embeddings in the input sequence disrupts RoPE's relative position property. RoMAE successfully achieves superior performance while maintaining the simplicity and effectiveness of Masked Autoencoders (MAE) in non-time-series contexts.<br /><br />Summary: <div>
arXiv:2505.20535v1 Announce Type: new 
Abstract: Applying Transformers to irregular time-series typically requires specializations to their baseline architecture, which can result in additional computational overhead and increased method complexity. We present the Rotary Masked Autoencoder (RoMAE), which utilizes the popular Rotary Positional Embedding (RoPE) method for continuous positions. RoMAE is an extension to the Masked Autoencoder (MAE) that enables representation learning with multidimensional continuous positional information while avoiding any time-series-specific architectural specializations. We showcase RoMAE's performance on a variety of modalities including irregular and multivariate time-series, images, and audio, demonstrating that RoMAE surpasses specialized time-series architectures on difficult datasets such as the DESC ELAsTiCC Challenge while maintaining MAE's usual performance across other modalities. In addition, we investigate RoMAE's ability to reconstruct the embedded continuous positions, demonstrating that including learned embeddings in the input sequence breaks RoPE's relative position property.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A ZeNN architecture to avoid the Gaussian trap</title>
<link>https://arxiv.org/abs/2505.20553</link>
<guid>https://arxiv.org/abs/2505.20553</guid>
<content:encoded><![CDATA[
<div> architecture, Zeta Neural Networks, ZeNNs, multi-layer perceptrons, MLPs, harmonic analysis <br />
<br />
Summary: The article introduces a new architecture called Zeta Neural Networks (ZeNNs) that addresses shortcomings of standard multi-layer perceptrons (MLPs). Inspired by principles from harmonic analysis, ZeNNs enumerate perceptrons with non-learnable weights, introduce scaling factors, and use activation functions that lead to near orthogonal systems. In the infinite width limit, ZeNNs converge pointwise, exhibit a rich asymptotic structure beyond Gaussianity, and enable feature learning. Additionally, finite width ZeNNs excel at learning high-frequency features with low-dimensional domains when suitable activation functions are selected. These innovations in architecture allow ZeNNs to overcome limitations of MLPs and enhance their performance in handling various types of data. <br /><br /> <div>
arXiv:2505.20553v1 Announce Type: new 
Abstract: We propose a new simple architecture, Zeta Neural Networks (ZeNNs), in order to overcome several shortcomings of standard multi-layer perceptrons (MLPs). Namely, in the large width limit, MLPs are non-parametric, they do not have a well-defined pointwise limit, they lose non-Gaussian attributes and become unable to perform feature learning; moreover, finite width MLPs perform poorly in learning high frequencies. The new ZeNN architecture is inspired by three simple principles from harmonic analysis:
  i) Enumerate the perceptons and introduce a non-learnable weight to enforce convergence;
  ii) Introduce a scaling (or frequency) factor;
  iii) Choose activation functions that lead to near orthogonal systems.
  We will show that these ideas allow us to fix the referred shortcomings of MLPs. In fact, in the infinite width limit, ZeNNs converge pointwise, they exhibit a rich asymptotic structure beyond Gaussianity, and perform feature learning. Moreover, when appropriate activation functions are chosen, (finite width) ZeNNs excel at learning high-frequency features of functions with low dimensional domains.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning a Pessimistic Reward Model in RLHF</title>
<link>https://arxiv.org/abs/2505.20556</link>
<guid>https://arxiv.org/abs/2505.20556</guid>
<content:encoded><![CDATA[
<div> method, reward hacking, offline reinforcement learning, human feedback, pessimistic reward<br />
Summary:<br />
This work introduces PET, a new method for fine-tuning pessimistic rewards in offline reinforcement learning from human feedback (RLHF). Traditional reward modeling techniques in RLHF are prone to reward hacking, but PET aims to prevent this issue without using regularization. By optimizing policies on a pessimistic reward model, reward hacking can be mitigated effectively. The study conducted experiments on a summarization dataset and showed that high-quality policies can be learned without the need for regularization. These policies exhibit high performance despite having a high KL divergence from the dataset distribution. This research demonstrates the feasibility of using a pessimistic reward model to guide policy optimization and prevent reward hacking in RLHF settings. <br />Summary: <div>
arXiv:2505.20556v1 Announce Type: new 
Abstract: This work proposes `PET', a novel pessimistic reward fine-tuning method, to learn a pessimistic reward model robust against reward hacking in offline reinforcement learning from human feedback (RLHF). Traditional reward modeling techniques in RLHF train an imperfect reward model, on which a KL regularization plays a pivotal role in mitigating reward hacking when optimizing a policy. Such an intuition-based method still suffers from reward hacking, and the policies with large KL divergence from the dataset distribution are excluded during learning. In contrast, we show that when optimizing a policy on a pessimistic reward model fine-tuned through PET, reward hacking can be prevented without relying on any regularization. We test our methods on the standard TL;DR summarization dataset. We find that one can learn a high-quality policy on our pessimistic reward without using any regularization. Such a policy has a high KL divergence from the dataset distribution while having high performance in practice. In summary, our work shows the feasibility of learning a pessimistic reward model against reward hacking. The agent can greedily search for the policy with a high pessimistic reward without suffering from reward hacking.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.20561</link>
<guid>https://arxiv.org/abs/2505.20561</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reinforcement Learning, Reflective Reasoning, Bayes-Adaptive RL, BARL

Summary:
Large Language Models (LLMs) trained using Reinforcement Learning (RL) have shown strong reasoning abilities and emergent reflective behaviors like backtracking and error correction. However, traditional Markovian RL limits exploration to the training phase, leading to uncertainty about the emergence of reflective reasoning. To address this, reflective exploration is reframed in the Bayes-Adaptive RL framework, optimizing expected return under a posterior distribution over Markov decision processes. The resulting algorithm, BARL, guides the LLM to adapt strategies based on observed outcomes, balancing reward-maximizing exploitation and information-gathering exploration. BARL outperforms standard Markovian RL at test time, offering superior token efficiency and improved exploration effectiveness. Empirical results on synthetic and mathematical reasoning tasks demonstrate the effectiveness of BARL. The code for BARL is available on GitHub. 

<br /><br />Summary: <div>
arXiv:2505.20561v1 Announce Type: new 
Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-Level Unsupervised Feature Selection</title>
<link>https://arxiv.org/abs/2505.20563</link>
<guid>https://arxiv.org/abs/2505.20563</guid>
<content:encoded><![CDATA[
<div> feature selection, unsupervised, bi-level, clustering, projection matrix

Summary:
The article introduces a novel bi-level unsupervised feature selection (BLUFS) method that addresses the limitations of existing UFS methods by combining a clustering level and a feature level. Spectral clustering generates pseudo-labels to represent the data structure, while a linear regression model learns the projection matrix. The $\ell_{2,0}$-norm constraint is applied to the projection matrix for improved feature selection. A proximal alternating minimization (PAM) algorithm is designed to solve the bi-level model efficiently, with established convergence and computational complexity. Experimental results on synthetic and real datasets demonstrate BLUFS' superiority in clustering and classification tasks. <div>
arXiv:2505.20563v1 Announce Type: new 
Abstract: Unsupervised feature selection (UFS) is an important task in data engineering. However, most UFS methods construct models from a single perspective and often fail to simultaneously evaluate feature importance and preserve their inherent data structure, thus limiting their performance. To address this challenge, we propose a novel bi-level unsupervised feature selection (BLUFS) method, including a clustering level and a feature level. Specifically, at the clustering level, spectral clustering is used to generate pseudo-labels for representing the data structure, while a continuous linear regression model is developed to learn the projection matrix. At the feature level, the $\ell_{2,0}$-norm constraint is imposed on the projection matrix for more effectively selecting features. To the best of our knowledge, this is the first work to combine a bi-level framework with the $\ell_{2,0}$-norm. To solve the proposed bi-level model, we design an efficient proximal alternating minimization (PAM) algorithm, whose subproblems either have explicit solutions or can be computed by fast solvers. Furthermore, we establish the convergence result and computational complexity. Finally, extensive experiments on two synthetic datasets and eight real datasets demonstrate the superiority of BLUFS in clustering and classification tasks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL</title>
<link>https://arxiv.org/abs/2505.20578</link>
<guid>https://arxiv.org/abs/2505.20578</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, Cell-type specificity, Regulatory DNA sequences, Transformer-based language models, Synthetic biology<br />
Summary:<br />
Designing precise cell-type-specific gene expression regulatory DNA sequences is vital for various fields like synthetic biology. Existing generative models struggle with reliably producing novel sequences. This study introduces Ctrl-DNA, a constrained reinforcement learning framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating the problem as a constrained optimization one and applying reinforcement learning to genomic language models, Ctrl-DNA outperforms existing approaches in generating high-fitness regulatory sequences while achieving state-of-the-art cell-type specificity. The generated sequences also contain key cell-type-specific transcription factor binding sites, demonstrating biological plausibility. <div>
arXiv:2505.20578v1 Announce Type: new 
Abstract: Designing regulatory DNA sequences that achieve precise cell-type-specific gene expression is crucial for advancements in synthetic biology, gene therapy and precision medicine. Although transformer-based language models (LMs) can effectively capture patterns in regulatory DNA, their generative approaches often struggle to produce novel sequences with reliable cell-specific activity. Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL) framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating regulatory sequence design as a biologically informed constrained optimization problem, we apply RL to autoregressive genomic LMs, enabling the models to iteratively refine sequences that maximize regulatory activity in targeted cell types while constraining off-target effects. Our evaluation on human promoters and enhancers demonstrates that Ctrl-DNA consistently outperforms existing generative and RL-based approaches, generating high-fitness regulatory sequences and achieving state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences capture key cell-type-specific transcription factor binding sites (TFBS), short DNA motifs recognized by regulatory proteins that control gene expression, demonstrating the biological plausibility of the generated sequences.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The challenge of hidden gifts in multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2505.20579</link>
<guid>https://arxiv.org/abs/2505.20579</guid>
<content:encoded><![CDATA[
<div> Keywords: hidden gifts, multi-agent reinforcement learning, grid-world environment, credit assignment, independent agents

Summary:
In the study of hidden gifts in multi-agent reinforcement learning (MARL), researchers explore the challenge of assigning credit when beneficial actions of others are concealed. A simple MARL task in a grid-world environment is used to illustrate this concept, where agents must collectively unlock doors using a shared key to receive rewards. Despite individual success, achieving the collective reward requires agents to drop the key for others, a hidden gift. Existing state-of-the-art RL algorithms struggle to learn this task, but independent model-free policy gradient agents succeed when provided with their action history. Introducing a correction term inspired by learning aware approaches enhances the performance of independent agents, reducing learning variance and promoting collective success. This study highlights the difficulty of credit assignment in multi-agent scenarios with hidden gifts and suggests that learning awareness in independent agents can improve performance. 

<br /><br />Summary: <div>
arXiv:2505.20579v1 Announce Type: new 
Abstract: Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These "hidden gifts" represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus the act of dropping the key for others is a "hidden gift". We show that several different state-of-the-art RL algorithms, including MARL algorithms, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that independent model-free policy gradient agents can solve the task when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for these independent agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of "hidden gifts", and demonstrate that learning awareness in independent agents can benefit these settings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction</title>
<link>https://arxiv.org/abs/2505.20589</link>
<guid>https://arxiv.org/abs/2505.20589</guid>
<content:encoded><![CDATA[
<div> Framework, Protein Language Models, Multi-task learning, Autoregressive decoder, Benchmark

Summary:<br />
- Prot2Token is introduced as a unified framework for protein prediction tasks, converting various predictions into a standardized next-token format.
- The model utilizes an autoregressive decoder and pre-trained protein encoders for multi-task learning, showcasing strong predictive power across different benchmarks.
- It offers significant speedups and performance comparable to or exceeding specialized models.
- An auxiliary self-supervised decoder pre-training approach is introduced to enhance spatially sensitive task performance.
- Prot2Token represents a versatile and high-throughput paradigm for protein modeling, promising to accelerate biological discovery and therapeutics development.<br /> <div>
arXiv:2505.20589v1 Announce Type: new 
Abstract: The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, a unified framework that overcomes these challenges by converting a wide spectrum of protein-related predictions, from sequence-level properties and residue-specific attributes to complex inter-protein interactions, into a standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling a single model to master numerous tasks with improved efficiency. We present extensive experimental validation across a variety of benchmarks, demonstrating Prot2Tokens strong predictive power in different types of protein-prediction tasks. Key results include significant speedups (e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or exceeding specialized approaches. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers a significant step towards a versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at https://github.com/mahdip72/prot2token .
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.20621</link>
<guid>https://arxiv.org/abs/2505.20621</guid>
<content:encoded><![CDATA[
<div> machine learning, offline reinforcement learning, poisoning attacks, differential privacy, certified defenses 
<br /> 
Summary:
This study introduces a novel approach to enhance the robustness of Offline Reinforcement Learning (RL) against poisoning attacks. By extending certified defenses, the framework provides larger guarantees for per-state actions and overall expected cumulative rewards in both continuous and discrete spaces, and in stochastic and deterministic environments. Leveraging properties of Differential Privacy, the approach significantly expands the scope and applicability of achievable guarantees. Empirical evaluations show that the performance drop is limited to 50% with up to 7% of the training data poisoned, a notable improvement over previous work. Additionally, the certified radii produced by the framework are 5 times larger. These results signify the potential of this framework to improve safety and reliability in offline RL. 
<br /> 
Summary: <div>
arXiv:2505.20621v1 Announce Type: new 
Abstract: Similar to other machine learning frameworks, Offline Reinforcement Learning (RL) is shown to be vulnerable to poisoning attacks, due to its reliance on externally sourced datasets, a vulnerability that is exacerbated by its sequential nature. To mitigate the risks posed by RL poisoning, we extend certified defenses to provide larger guarantees against adversarial manipulation, ensuring robustness for both per-state actions, and the overall expected cumulative reward. Our approach leverages properties of Differential Privacy, in a manner that allows this work to span both continuous and discrete spaces, as well as stochastic and deterministic environments -- significantly expanding the scope and applicability of achievable guarantees. Empirical evaluations demonstrate that our approach ensures the performance drops to no more than $50\%$ with up to $7\%$ of the training data poisoned, significantly improving over the $0.008\%$ in prior work~\citep{wu_copa_2022}, while producing certified radii that is $5$ times larger as well. This highlights the potential of our framework to enhance safety and reliability in offline RL.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Adopt Constraints Over Penalties in Deep Learning</title>
<link>https://arxiv.org/abs/2505.20628</link>
<guid>https://arxiv.org/abs/2505.20628</guid>
<content:encoded><![CDATA[
<div> Machine learning, Trustworthy AI, Constrained optimization, Penalization, Lagrangian approach

Summary:
Tailored constrained optimization methods are proposed for developing trustworthy AI systems with accountability guarantees, integrating external requirements or constraints seamlessly into deep learning pipelines. The traditional approach of penalization through fixed-weight terms is deemed ineffective, as it often requires costly tuning of penalty coefficients and may not yield optimal solutions. Alternatively, tailored constrained methods, like the Lagrangian approach, optimize the penalization coefficients (Lagrange multipliers) alongside the model, ensuring solutions that satisfy constraints and achieve good performance. These methods not only solve the constrained problem but also add accountability without the need for extensive penalty tuning. By adopting tailored constrained optimization methods, researchers can enhance the robustness and reliability of AI systems while streamlining the integration of external requirements. <br /><br />Summary: <div>
arXiv:2505.20628v1 Announce Type: new 
Abstract: Recent efforts toward developing trustworthy AI systems with accountability guarantees have led to a growing reliance on machine learning formulations that incorporate external requirements, or constraints. These requirements are often enforced through penalization--adding fixed-weight terms to the task loss. We argue that this approach is ill-suited, and that tailored constrained optimization methods should be adopted instead. In particular, no penalty coefficient may yield a solution that both satisfies the constraints and achieves good performance--i.e., one solving the constrained problem. Moreover, tuning these coefficients is costly, incurring significant time and computational overhead. In contrast, tailored constrained methods--such as the Lagrangian approach, which optimizes the penalization "coefficients" (the Lagrange multipliers) alongside the model--(i) truly solve the constrained problem and add accountability, (ii) eliminate the need for extensive penalty tuning, and (iii) integrate seamlessly with modern deep learning pipelines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Concept Shift with Interpretable Feature Attribution</title>
<link>https://arxiv.org/abs/2505.20634</link>
<guid>https://arxiv.org/abs/2505.20634</guid>
<content:encoded><![CDATA[
<div> shift detection; concept shift; tabular data; machine learning; feature selection

Summary:<br /><br />Concept shift occurs when the distribution of labels conditioned on the features changes, leading to reduced model performance even for well-trained machine learning models. In this paper, SGShift, a model for detecting concept shift in tabular data, is proposed. SGShift utilizes a Generalized Additive Model (GAM) to model concept shift, followed by feature selection to identify shifted features. Extensions of SGShift include incorporating knockoffs to control false discoveries and an absorption term for models with poor data fit. Extensive experiments on synthetic and real data show that SGShift can identify shifted features with high AUC and recall values, outperforming baseline methods. The proposed approach provides valuable insight into understanding dataset differences and helps in attributing reduced model performance to specific shifted features. <div>
arXiv:2505.20634v1 Announce Type: new 
Abstract: Regardless the amount of data a machine learning (ML) model is trained on, there will inevitably be data that differs from their training set, lowering model performance. Concept shift occurs when the distribution of labels conditioned on the features changes, making even a well-tuned ML model to have learned a fundamentally incorrect representation. Identifying these shifted features provides unique insight into how one dataset differs from another, considering the difference may be across a scientifically relevant dimension, such as time, disease status, population, etc. In this paper, we propose SGShift, a model for detecting concept shift in tabular data and attributing reduced model performance to a sparse set of shifted features. SGShift models concept shift with a Generalized Additive Model (GAM) and performs subsequent feature selection to identify shifted features. We propose further extensions of SGShift by incorporating knockoffs to control false discoveries and an absorption term to account for models with poor fit to the data. We conduct extensive experiments in synthetic and real data across various ML models and find SGShift can identify shifted features with AUC $>0.9$ and recall $>90\%$, often 2 or 3 times as high as baseline methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Past Experience Accelerate LLM Reasoning?</title>
<link>https://arxiv.org/abs/2505.20643</link>
<guid>https://arxiv.org/abs/2505.20643</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning speedup, compute allocation, memory mechanisms, benchmarking

Summary: 
The paper explores the concept of making large language models (LLMs) faster at reasoning through recurrent exposure to relevant tasks. It formalizes the problem of LLM reasoning speedup and proposes the SpeedupLLM framework to implement and benchmark this behavior. The framework is based on adaptive compute allocation and memory mechanisms. Comprehensive experiments are conducted to assess the reasoning speedup across various question similarity levels, memory methods, and reasoning techniques. Results indicate that LLMs can indeed reason faster with past experience, leading to up to a 56% reduction in compute cost when equipped with suitable memory and reasoning methods. The study sheds light on the potential for LLMs to improve their efficiency through repeated exposure to relevant tasks, showcasing the importance of memory retention and adaptive compute allocation in enhancing reasoning abilities. 

<br /><br />Summary: <div>
arXiv:2505.20643v1 Announce Type: new 
Abstract: Allocating more compute to large language models (LLMs) reasoning has generally been demonstrated to improve their effectiveness, but also results in increased inference time. In contrast, humans can perform tasks faster and better with increased experience and exposure. Hence, this paper aims to investigate the question: Can LLMs also become faster at reasoning through recurrent exposure on relevant tasks, and if so, how can it be achieved? To address these questions, we first formalize the problem setting of LLM reasoning speedup systematically in the dimensions of task relevancy and compute budget calculation. We then propose SpeedupLLM, a theoretically guaranteed framework to implement and benchmark such reasoning speedup behaviour based on adaptive compute allocation and memory mechanisms. We further conduct comprehensive experiments to benchmark such behaviour across different question similarity levels, memory methods, and reasoning methods. Results show that LLMs can generally reason faster with past experience, achieving up to a 56% reduction in compute cost when equipped with appropriate memory and reasoning methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory</title>
<link>https://arxiv.org/abs/2505.20646</link>
<guid>https://arxiv.org/abs/2505.20646</guid>
<content:encoded><![CDATA[
<div> algorithmic information theory, Binarized Neural Networks, Block Decomposition Method, learning dynamics, algorithmic compression<br />
<br />Summary:
This study explores the use of algorithmic information theory to understand and control the informational complexity of neural networks. By focusing on Binarized Neural Networks (BNNs) and using the Block Decomposition Method (BDM) to approximate algorithmic complexity, the researchers show that this approach better captures structural changes during training compared to traditional entropy-based methods. The results suggest that training can be viewed as a process of algorithmic compression, where learning involves internalizing structured regularities. This perspective offers a principled estimate of learning progression and provides a framework for complexity-aware learning and regularization based on fundamental principles from information theory, complexity, and computability. <div>
arXiv:2505.20646v1 Announce Type: new 
Abstract: Understanding and controlling the informational complexity of neural networks is a central challenge in machine learning, with implications for generalization, optimization, and model capacity. While most approaches rely on entropy-based loss functions and statistical metrics, these measures often fail to capture deeper, causally relevant algorithmic regularities embedded in network structure. We propose a shift toward algorithmic information theory, using Binarized Neural Networks (BNNs) as a first proxy. Grounded in algorithmic probability (AP) and the universal distribution it defines, our approach characterizes learning dynamics through a formal, causally grounded lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation of algorithmic complexity based on AP -- and demonstrate that it more closely tracks structural changes during training than entropy, consistently exhibiting stronger correlations with training loss across varying model sizes and randomized training runs. These results support the view of training as a process of algorithmic compression, where learning corresponds to the progressive internalization of structured regularities. In doing so, our work offers a principled estimate of learning progression and suggests a framework for complexity-aware learning and regularization, grounded in first principles from information theory, complexity, and computability.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning</title>
<link>https://arxiv.org/abs/2505.20648</link>
<guid>https://arxiv.org/abs/2505.20648</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-objective optimization, Pareto front, Hypernetworks, Genetic algorithm, Federated learning

Summary:
Multi-objective optimization is crucial in machine learning, particularly in areas like federated learning. Pareto-Front Learning using Hypernetworks (PHN) is a powerful method to approximate the Pareto front. However, existing approaches face challenges in sampling rays in high-dimensional spaces and covering the entire Pareto front. To address these issues, a novel framework called PHN-HVVS is introduced, which uses Voronoi grids and genetic algorithm for partitioning the design space. A new loss function is proposed to enhance coverage of the Pareto front and maximize the HV Indicator. Experimental results show that PHN-HVVS outperforms baselines in generating Pareto front and advances methodologies in Federated Learning. The code for PHN-HVVS is available on GitHub, enabling further research and application in the field. 

<br /><br />Summary: <div>
arXiv:2505.20648v1 Announce Type: new 
Abstract: Multi-objective optimization (MOO) exists extensively in machine learning, and aims to find a set of Pareto-optimal solutions, called the Pareto front, e.g., it is fundamental for multiple avenues of research in federated learning (FL). Pareto-Front Learning (PFL) is a powerful method implemented using Hypernetworks (PHNs) to approximate the Pareto front. This method enables the acquisition of a mapping function from a given preference vector to the solutions on the Pareto front. However, most existing PFL approaches still face two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to cover the entire Pareto Front which has a convex shape. Here, we introduce a novel PFL framework, called as PHN-HVVS, which decomposes the design space into Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid partitioning within high-dimensional space. We put forward a new loss function, which effectively contributes to more extensive coverage of the resultant Pareto front and maximizes the HV Indicator. Experimental results on multiple MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines significantly in generating Pareto front. Also, we illustrate that PHN-HVVS advances the methodologies of several recent problems in the FL field. The code is available at https://github.com/buptcmm/phnhvvs}{https://github.com/buptcmm/phnhvvs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Optimisation Framework for Unsupervised Environment Design</title>
<link>https://arxiv.org/abs/2505.20659</link>
<guid>https://arxiv.org/abs/2505.20659</guid>
<content:encoded><![CDATA[
<div> optimisation, reinforcement learning, robustness, unsupervised environment design, generalisability

Summary:
The paper introduces an approach called unsupervised environment design (UED), which aims to enhance the robustness of reinforcement learning agents in high-risk settings. It focuses on optimizing a nonconvex-strongly-concave objective to improve generalisability across different environment configurations. The method provides theoretical guarantees for practical applications, unlike previous techniques that rely on convergence. An algorithm is proposed for the zero-sum setting, ensuring provable convergence. Empirical results demonstrate the effectiveness of the approach, showcasing superior performance compared to existing methods in various challenging environments. This research contributes to advancing the field of reinforcement learning by enhancing agent robustness through innovative optimization strategies. <div>
arXiv:2505.20659v1 Announce Type: new 
Abstract: For reinforcement learning agents to be deployed in high-risk settings, they must achieve a high level of robustness to unfamiliar scenarios. One method for improving robustness is unsupervised environment design (UED), a suite of methods aiming to maximise an agent's generalisability across configurations of an environment. In this work, we study UED from an optimisation perspective, providing stronger theoretical guarantees for practical settings than prior work. Whereas previous methods relied on guarantees if they reach convergence, our framework employs a nonconvex-strongly-concave objective for which we provide a provably convergent algorithm in the zero-sum setting. We empirically verify the efficacy of our method, outperforming prior methods in a number of environments with varying difficulties.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers</title>
<link>https://arxiv.org/abs/2505.20666</link>
<guid>https://arxiv.org/abs/2505.20666</guid>
<content:encoded><![CDATA[
<div> Keywords: Continuous_Time Attention, partial differential equations, Transformer, long input sequences, optimization landscapes

Summary:
Continuous_Time Attention is a novel framework that incorporates partial differential equations (PDEs) into the Transformer's attention mechanism to handle long input sequences. By allowing attention weights to evolve over a pseudo_time dimension using diffusion, wave, or reaction_diffusion dynamics, this mechanism smooths local noise, enhances long-range dependencies, and stabilizes gradient flow. The PDE_based attention leads to improved optimization landscapes and polynomial decay of distant interactions. Empirical benchmarks show consistent improvements over standard Transformer models on various experiments. This approach showcases the potential of PDE_based formulations in enhancing attention mechanisms with continuous_time dynamics and global coherence.<br /><br />Summary: <div>
arXiv:2505.20666v1 Announce Type: new 
Abstract: We propose a novel framework, Continuous_Time Attention, which infuses partial differential equations (PDEs) into the Transformer's attention mechanism to address the challenges of extremely long input sequences. Instead of relying solely on a static attention matrix, we allow attention weights to evolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion dynamics. This mechanism systematically smooths local noise, enhances long_range dependencies, and stabilizes gradient flow. Theoretically, our analysis shows that PDE_based attention leads to better optimization landscapes and polynomial rather than exponential decay of distant interactions. Empirically, we benchmark our method on diverse experiments_demonstrating consistent gains over both standard and specialized long sequence Transformer variants. Our findings highlight the potential of PDE_based formulations to enrich attention mechanisms with continuous_time dynamics and global coherence.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating RL for LLM Reasoning with Optimal Advantage Regression</title>
<link>https://arxiv.org/abs/2505.20686</link>
<guid>https://arxiv.org/abs/2505.20686</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, language models, policy optimization, reasoning tasks, A*-PO

Summary:
Reinforcement learning (RL) is utilized to enhance large language models (LLMs) for complex reasoning tasks. The A*-PO framework proposed in this paper streamlines policy optimization for LLM training by approximating the optimal advantage function. It involves a two-stage process where offline sampling estimates the optimal value function $V$* in the first stage, eliminating the need for expensive online value estimation. The second stage uses a simple least-squares regression loss with only one generation per prompt for on-policy updates. The framework provides performance guarantees and simplifies the optimization of the KL-regularized RL objective without complex exploration strategies. Empirically, A*-PO outperforms existing methods such as PPO, GRPO, and REBEL in mathematical reasoning benchmarks, while reducing training time by up to 2 times and peak memory usage by over 30%. The implementation of A*-PO is available at https://github.com/ZhaolinGao/A-PO.

<br /><br />Summary: A*-PO is a novel policy optimization framework that enhances LLM training efficiency by approximating the optimal advantage function through a two-stage process. It eliminates costly online value estimation, simplifies optimization with a simple loss function, and achieves competitive performance with reduced training time and memory usage. <div>
arXiv:2505.20686v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning large language models (LLMs) to improve complex reasoning abilities. However, state-of-the-art policy optimization methods often suffer from high computational overhead and memory consumption, primarily due to the need for multiple generations per prompt and the reliance on critic networks or advantage estimates of the current policy. In this paper, we propose $A$*-PO, a novel two-stage policy optimization framework that directly approximates the optimal advantage function and enables efficient training of LLMs for reasoning tasks. In the first stage, we leverage offline sampling from a reference policy to estimate the optimal value function $V$*, eliminating the need for costly online value estimation. In the second stage, we perform on-policy updates using a simple least-squares regression loss with only a single generation per prompt. Theoretically, we establish performance guarantees and prove that the KL-regularized RL objective can be optimized without requiring complex exploration strategies. Empirically, $A$*-PO achieves competitive performance across a wide range of mathematical reasoning benchmarks, while reducing training time by up to 2$\times$ and peak memory usage by over 30% compared to PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at https://github.com/ZhaolinGao/A-PO.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evidential Deep Active Learning for Semi-Supervised Classification</title>
<link>https://arxiv.org/abs/2505.20691</link>
<guid>https://arxiv.org/abs/2505.20691</guid>
<content:encoded><![CDATA[
<div> active learning, semi-supervised classification, uncertainty estimation, evidential deep learning, sample selection strategy

Summary:
In this paper, the authors propose an evidential deep active learning approach for semi-supervised classification (EDALSSC) that takes into account uncertainty estimation during the learning process. EDALSSC integrates evidential deep learning to quantify uncertainty in labeled data and combines ignorance and conflict information for unlabeled data. A heuristic method balances evidence and number of classes to avoid counter-intuitive results. The sample selection strategy prioritizes samples with high uncertainty estimation as training loss increases. Experimental results show that EDALSSC surpasses existing active learning and supervised approaches on image classification tasks. The proposed method addresses the limitations of existing methods by incorporating uncertainty estimation effectively throughout the learning process. <br /><br />Summary: <div>
arXiv:2505.20691v1 Announce Type: new 
Abstract: Semi-supervised classification based on active learning has made significant progress, but the existing methods often ignore the uncertainty estimation (or reliability) of the prediction results during the learning process, which makes it questionable whether the selected samples can effectively update the model. Hence, this paper proposes an evidential deep active learning approach for semi-supervised classification (EDALSSC). EDALSSC builds a semi-supervised learning framework to simultaneously quantify the uncertainty estimation of labeled and unlabeled data during the learning process. The uncertainty estimation of the former is associated with evidential deep learning, while that of the latter is modeled by combining ignorance information and conflict information of the evidence from the perspective of the T-conorm operator. Furthermore, this article constructs a heuristic method to dynamically balance the influence of evidence and the number of classes on uncertainty estimation to ensure that it does not produce counter-intuitive results in EDALSSC. For the sample selection strategy, EDALSSC selects the sample with the greatest uncertainty estimation that is calculated in the form of a sum when the training loss increases in the latter half of the learning process. Experimental results demonstrate that EDALSSC outperforms existing semi-supervised and supervised active learning approaches on image classification datasets.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series</title>
<link>https://arxiv.org/abs/2505.20697</link>
<guid>https://arxiv.org/abs/2505.20697</guid>
<content:encoded><![CDATA[
<div> Keywords: hypothesis generation, neuroscience, machine learning, dynamic graphs, causal relationships 

Summary:
Our novel method in hypothesis generation in neuroscience leverages machine learning techniques to model dynamic graphs as a combination of static graphs. This allows us to capture complex, time-varying relationships beyond linear patterns in systems with dynamic, state-dependent behavior like the brain. Our approach improves the prediction of dynamic causal patterns significantly, with f1-scores increasing by 22-28% on average over baselines and up to over 60% in some cases. A case study on real brain data showcases the method's effectiveness in uncovering relationships associated with specific behavioral states, providing valuable insights into neural dynamics. This innovative approach offers a promising way to reduce costs in neuroscience research by narrowing down the range of interventional studies needed and enhancing our understanding of complex brain dynamics. 

Summary: <div>
arXiv:2505.20697v1 Announce Type: new 
Abstract: The field of hypothesis generation promises to reduce costs in neuroscience by narrowing the range of interventional studies needed to study various phenomena. Existing machine learning methods can generate scientific hypotheses from complex datasets, but many approaches assume causal relationships are static over time, limiting their applicability to systems with dynamic, state-dependent behavior, such as the brain. While some techniques attempt dynamic causal discovery through factor models, they often restrict relationships to linear patterns or impose other simplifying assumptions. We propose a novel method that models dynamic graphs as a conditionally weighted superposition of static graphs, where each static graph can capture nonlinear relationships. This approach enables the detection of complex, time-varying interactions between variables beyond linear limitations. Our method improves f1-scores of predicted dynamic causal patterns by roughly 22-28% on average over baselines in some of our experiments, with some improvements reaching well over 60%. A case study on real brain data demonstrates our method's ability to uncover relationships linked to specific behavioral states, offering valuable insights into neural dynamics.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsified State-Space Models are Efficient Highway Networks</title>
<link>https://arxiv.org/abs/2505.20698</link>
<guid>https://arxiv.org/abs/2505.20698</guid>
<content:encoded><![CDATA[
<div> Keywords: State-space models, sequence modeling, sparsification, token pruning, natural language tasks

Summary: 
State-space models (SSMs) offer an efficient alternative to Transformers for sequence modeling by utilizing linear recurrences instead of self-attention. This paper introduces Simba, a hierarchical sparsification method for SSMs that enhances their performance within given computational constraints. By sparsifying tokens based on a novel pruning criterion that measures global impact through accumulated local recurrences, Simba optimizes information flow in SSMs. The approach focuses on reducing redundancy in upper layers while retaining important global context, resembling the behavior of highways. Experimental results demonstrate that Simba outperforms the baseline model, Mamba, in various natural language tasks while maintaining the same computational complexity. The code for implementing Simba is publicly available, allowing for easy adoption and further research in the field. 

<br /><br />Summary: <div>
arXiv:2505.20698v1 Announce Type: new 
Abstract: State-space models (SSMs) offer a promising architecture for sequence modeling, providing an alternative to Transformers by replacing expensive self-attention with linear recurrences. In this paper, we propose a simple yet effective trick to enhance SSMs within given computational budgets by sparsifying them. Our intuition is that tokens in SSMs are highly redundant due to gradual recurrent updates, and dense recurrence operations block the delivery of past information. In particular, we observe that upper layers of SSMs tend to be more redundant as they encode global information, while lower layers encode local information. Motivated by this, we introduce Simba, a hierarchical sparsification method for SSMs based on token pruning. Simba sparsifies upper layers more than lower layers, encouraging the upper layers to behave like highways. To achieve this, we propose a novel token pruning criterion for SSMs, measuring the global impact of tokens on the final output by accumulating local recurrences. We demonstrate that Simba outperforms the baseline model, Mamba, with the same FLOPS in various natural language tasks. Moreover, we illustrate the effect of highways, showing that Simba not only enhances efficiency but also improves the information flow across long sequences. Code is available at https://github.com/woominsong/Simba.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Data Embeddings effective in time series forecasting?</title>
<link>https://arxiv.org/abs/2505.20716</link>
<guid>https://arxiv.org/abs/2505.20716</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, data embedding, state-of-the-art models, ablation studies, computational efficiency <br />
Summary: 
Data embedding techniques in time series forecasting models are not as effective as previously believed. Extensive ablation studies on fifteen state-of-the-art models and four benchmark datasets show that removing data embedding layers does not decrease performance; in fact, it can improve both accuracy and computational efficiency. The gains from removing these layers often surpass the differences in performance typically seen between competing models. This research challenges the notion that complex data embedding layers are necessary for enhancing forecasting accuracy, suggesting a potential simplification of model architectures for improved efficiency. The study's findings open up new avenues for developing more streamlined and effective forecasting models. The code for replicating the study is available on GitHub for further exploration and validation. <br /><br />Summary: <div>
arXiv:2505.20716v1 Announce Type: new 
Abstract: Time series forecasting plays a crucial role in many real-world applications, and numerous complex forecasting models have been proposed in recent years. Despite their architectural innovations, most state-of-the-art models report only marginal improvements -- typically just a few thousandths in standard error metrics. These models often incorporate complex data embedding layers to transform raw inputs into higher-dimensional representations to enhance accuracy. But are data embedding techniques actually effective in time series forecasting? Through extensive ablation studies across fifteen state-of-the-art models and four benchmark datasets, we find that removing data embedding layers from many state-of-the-art models does not degrade forecasting performance. In many cases, it improves both accuracy and computational efficiency. The gains from removing embedding layers often exceed the performance differences typically reported between competing models. Code available at: https://github.com/neuripsdataembedidng/DataEmbedding
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrent Neural Operators: Stable Long-Term PDE Prediction</title>
<link>https://arxiv.org/abs/2505.20721</link>
<guid>https://arxiv.org/abs/2505.20721</guid>
<content:encoded><![CDATA[
<div> Neural operators, recurrent training, partial differential equations, temporal dynamics, long-term predictions <br />
<br />
Summary: <br />
Neural operators have shown promise in solving partial differential equations, but face challenges with long-term predictions due to a mismatch between training and inference methods. Recurrent Neural Operators (RNOs) address this by incorporating recurrent training into neural operator architectures, simulating inference-time dynamics during training. This alignment reduces error accumulation and improves accuracy and stability in long-term predictions. Theoretical analysis suggests that recurrent training can mitigate exponential error growth seen in standard training strategies. Empirical results demonstrate that Multigrid Neural Operators trained with recurrence significantly outperform teacher-forced counterparts on standard benchmarks. This research highlights the importance of aligning training with inference dynamics for robust temporal generalization in neural operator learning. <div>
arXiv:2505.20721v1 Announce Type: new 
Abstract: Neural operators have emerged as powerful tools for learning solution operators of partial differential equations. However, in time-dependent problems, standard training strategies such as teacher forcing introduce a mismatch between training and inference, leading to compounding errors in long-term autoregressive predictions. To address this issue, we propose Recurrent Neural Operators (RNOs)-a novel framework that integrates recurrent training into neural operator architectures. Instead of conditioning each training step on ground-truth inputs, RNOs recursively apply the operator to their own predictions over a temporal window, effectively simulating inference-time dynamics during training. This alignment mitigates exposure bias and enhances robustness to error accumulation. Theoretically, we show that recurrent training can reduce the worst-case exponential error growth typical of teacher forcing to linear growth. Empirically, we demonstrate that recurrently trained Multigrid Neural Operators significantly outperform their teacher-forced counterparts in long-term accuracy and stability on standard benchmarks. Our results underscore the importance of aligning training with inference dynamics for robust temporal generalization in neural operator learning.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A reinforcement learning agent for maintenance of deteriorating systems with increasingly imperfect repairs</title>
<link>https://arxiv.org/abs/2505.20725</link>
<guid>https://arxiv.org/abs/2505.20725</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, maintenance optimization, Industry 4.0, degradation process, Double Deep Q-Network

Summary:
- Efficient maintenance is crucial for engineering systems, particularly in the context of Industry 4.0.
- Machine learning techniques, such as reinforcement learning, are being increasingly utilized in maintenance practices.
- A novel maintenance model is proposed, incorporating a gamma degradation process and imperfect repairs to reflect real-world system behavior.
- A reinforcement-learning-based agent, utilizing a Double Deep Q-Network architecture, is developed to generate maintenance policies without a predefined preventive threshold.
- The agent demonstrates flexibility in adapting to different scenarios and significantly improves long-run cost compared to common maintenance strategies. 

<br /><br />Summary: <div>
arXiv:2505.20725v1 Announce Type: new 
Abstract: Efficient maintenance has always been essential for the successful application of engineering systems. However, the challenges to be overcome in the implementation of Industry 4.0 necessitate new paradigms of maintenance optimization. Machine learning techniques are becoming increasingly used in engineering and maintenance, with reinforcement learning being one of the most promising. In this paper, we propose a gamma degradation process together with a novel maintenance model in which repairs are increasingly imperfect, i.e., the beneficial effect of system repairs decreases as more repairs are performed, reflecting the degradational behavior of real-world systems. To generate maintenance policies for this system, we developed a reinforcement-learning-based agent using a Double Deep Q-Network architecture. This agent presents two important advantages: it works without a predefined preventive threshold, and it can operate in a continuous degradation state space. Our agent learns to behave in different scenarios, showing great flexibility. In addition, we performed an analysis of how changes in the main parameters of the environment affect the maintenance policy proposed by the agent. The proposed approach is demonstrated to be appropriate and to significatively improve long-run cost as compared with other common maintenance strategies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial bandit optimization for approximately linear functions</title>
<link>https://arxiv.org/abs/2505.20734</link>
<guid>https://arxiv.org/abs/2505.20734</guid>
<content:encoded><![CDATA[
<div> bandit optimization, nonconvex functions, non-smooth functions, regret bounds, linear optimization <br />
<br />
Summary: 
The article addresses a bandit optimization problem focusing on nonconvex and non-smooth functions. It examines scenarios where the loss function is a combination of a linear function and a small arbitrary perturbation chosen after observing the players choice. The study presents expected and high probability regret bounds for this problem, which also leads to an enhanced high-probability regret bound for bandit linear optimization  a special case lacking perturbations. Additionally, a lower bound on the expected regret is provided. The findings offer valuable insights into optimizing bandit problems with complex function forms and uncertainties, enhancing the understanding of efficient decision-making strategies in dynamic and turbulent environments. <div>
arXiv:2505.20734v1 Announce Type: new 
Abstract: We consider a bandit optimization problem for nonconvex and non-smooth functions, where in each trial the loss function is the sum of a linear function and a small but arbitrary perturbation chosen after observing the player's choice. We give both expected and high probability regret bounds for the problem. Our result also implies an improved high-probability regret bound for the bandit linear optimization, a special case with no perturbation. We also give a lower bound on the expected regret.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Informative Channels: ActionFormer</title>
<link>https://arxiv.org/abs/2505.20739</link>
<guid>https://arxiv.org/abs/2505.20739</guid>
<content:encoded><![CDATA[
<div> Transformer-based models have advanced Human Activity Recognition (HAR), with ActionFormer offering new insights by detecting activity borders and labels. While originally designed for image/video inputs, it has been adapted for sensor signals. However, challenges include capturing subtle changes effectively due to high temporal dynamics and managing spatial-temporal feature interdependencies. To address these issues, a modified ActionFormer approach is proposed for sensor signals. The method utilizes the Sequence-and-Excitation strategy to minimize additional parameters and incorporates the swish activation function for directional information retention. Experiments on the WEAR dataset demonstrate a significant 16.01% average mean Average Precision (mAP) improvement for inertial data.<br /><br />Keywords: Human Activity Recognition, Transformer-based models, ActionFormer, sensor signals, Sequence-and-Excitation strategy<br /><br />Summary: Transformer-based models like ActionFormer have enhanced HAR by detecting activity borders and labels. Adapting ActionFormer for sensor signals addresses challenges of capturing subtle changes due to high temporal dynamics and managing spatial-temporal feature interdependencies. The proposed modified approach shows a substantial 16.01% improvement in average mAP for inertial data on the WEAR dataset. <div>
arXiv:2505.20739v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) has recently witnessed advancements with Transformer-based models. Especially, ActionFormer shows us a new perspectives for HAR in the sense that this approach gives us additional outputs which detect the border of the activities as well as the activity labels. ActionFormer was originally proposed with its input as image/video. However, this was converted to with its input as sensor signals as well. We analyze this extensively in terms of deep learning architectures. Based on the report of high temporal dynamics which limits the model's ability to capture subtle changes effectively and of the interdependencies between the spatial and temporal features. We propose the modified ActionFormer which will decrease these defects for sensor signals. The key to our approach lies in accordance with the Sequence-and-Excitation strategy to minimize the increase in additional parameters and opt for the swish activation function to retain the information about direction in the negative range. Experiments on the WEAR dataset show that our method achieves substantial improvement of a 16.01\% in terms of average mAP for inertial data.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>'Hello, World!': Making GNNs Talk with LLMs</title>
<link>https://arxiv.org/abs/2505.20742</link>
<guid>https://arxiv.org/abs/2505.20742</guid>
<content:encoded><![CDATA[
<div> Graph Lingual Network, GNN, hidden representations, large language models, node classification<br />
<br />
Summary:<br />
Graph Lingual Network (GLN) is a graph neural network (GNN) that utilizes large language models (LLMs) to create human-readable text representations for improved interpretability. By incorporating GNN techniques like graph attention and initial residual connection, GLN allows for intuitive analysis of how node representations change across layers and under different GNN techniques. The comprehensibility of GLN's hidden representations offers insights into the inner workings of GNNs. Additionally, GLN demonstrates impressive zero-shot performance on tasks like node classification and link prediction, surpassing existing LLM-based methods. <div>
arXiv:2505.20742v1 Announce Type: new 
Abstract: While graph neural networks (GNNs) have shown remarkable performance across diverse graph-related tasks, their high-dimensional hidden representations render them black boxes. In this work, we propose Graph Lingual Network (GLN), a GNN built on large language models (LLMs), with hidden representations in the form of human-readable text. Through careful prompt design, GLN incorporates not only the message passing module of GNNs but also advanced GNN techniques, including graph attention and initial residual connection. The comprehensibility of GLN's hidden representations enables an intuitive analysis of how node representations change (1) across layers and (2) under advanced GNN techniques, shedding light on the inner workings of GNNs. Furthermore, we demonstrate that GLN achieves strong zero-shot performance on node classification and link prediction, outperforming existing LLM-based baseline methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction</title>
<link>https://arxiv.org/abs/2505.20755</link>
<guid>https://arxiv.org/abs/2505.20755</guid>
<content:encoded><![CDATA[
<div> Keywords: Uni-Instruct, one-step diffusion distillation, $f$-divergence family, generation benchmarks, text-to-3D generation <br />
<br />
Summary: 
The paper introduces Uni-Instruct, a framework that unifies multiple existing one-step diffusion distillation approaches within the $f$-divergence family theory. By overcoming the intractability issue of the original expanded $f$-divergence, Uni-Instruct offers a tractable loss for training one-step diffusion models effectively. This unification not only provides new theoretical insights into existing approaches but also achieves state-of-the-art performance on generation benchmarks. Uni-Instruct sets record-breaking Frechet Inception Distance values for both unconditional and conditional generation on CIFAR10 and achieves a new state-of-the-art FID on the ImageNet-$64\times 64$ benchmark. Additionally, Uni-Instruct is applied successfully to text-to-3D generation tasks, outperforming previous methods in terms of quality and diversity. The theoretical and empirical contributions of Uni-Instruct are poised to advance research in one-step diffusion distillation and knowledge transfer for diffusion models. <br /> <div>
arXiv:2505.20755v1 Announce Type: new 
Abstract: In this paper, we unify more than 10 existing one-step diffusion distillation approaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside a theory-driven framework which we name the \textbf{\emph{Uni-Instruct}}. Uni-Instruct is motivated by our proposed diffusion expansion theory of the $f$-divergence family. Then we introduce key theories that overcome the intractability issue of the original expanded $f$-divergence, resulting in an equivalent yet tractable loss that effectively trains one-step diffusion models by minimizing the expanded $f$-divergence family. The novel unification introduced by Uni-Instruct not only offers new theoretical contributions that help understand existing approaches from a high-level perspective but also leads to state-of-the-art one-step diffusion generation performances. On the CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet Inception Distance (FID) values of \textbf{\emph{1.46}} for unconditional generation and \textbf{\emph{1.38}} for conditional generation. On the ImageNet-$64\times 64$ generation benchmark, Uni-Instruct achieves a new SoTA one-step generation FID of \textbf{\emph{1.02}}, which outperforms its 79-step teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35). We also apply Uni-Instruct on broader tasks like text-to-3D generation. For text-to-3D generation, Uni-Instruct gives decent results, which slightly outperforms previous methods, such as SDS and VSD, in terms of both generation quality and diversity. Both the solid theoretical and empirical contributions of Uni-Instruct will potentially help future studies on one-step diffusion distillation and knowledge transferring of diffusion models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical estimation of the optimal classification error with soft labels and calibration</title>
<link>https://arxiv.org/abs/2505.20761</link>
<guid>https://arxiv.org/abs/2505.20761</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, binary classification, Bayes error estimation, soft labels, isotonic calibration <br />
Summary: 
This paper addresses the question of improving machine learning models in the context of binary classification. The authors extend previous work on estimating the Bayes error using soft labels in two key ways. They investigate the bias of the estimator based on hard labels and show that it decays faster as the number of hard labels per instance increases. Additionally, they tackle the issue of estimating with corrupted soft labels and demonstrate that isotonic calibration can provide a statistically consistent estimator. Importantly, their method is instance-free, making it suitable for scenarios where input instances are not available. Experimental results on synthetic and real-world datasets confirm the effectiveness of their approach and theoretical findings. <br /><br />Summary: <div>
arXiv:2505.20761v1 Announce Type: new 
Abstract: While the performance of machine learning systems has experienced significant improvement in recent years, relatively little attention has been paid to the fundamental question: to what extent can we improve our models? This paper provides a means of answering this question in the setting of binary classification, which is practical and theoretically supported. We extend a previous work that utilizes soft labels for estimating the Bayes error, the optimal error rate, in two important ways. First, we theoretically investigate the properties of the bias of the hard-label-based estimator discussed in the original work. We reveal that the decay rate of the bias is adaptive to how well the two class-conditional distributions are separated, and it can decay significantly faster than the previous result suggested as the number of hard labels per instance grows. Second, we tackle a more challenging problem setting: estimation with corrupted soft labels. One might be tempted to use calibrated soft labels instead of clean ones. However, we reveal that calibration guarantee is not enough, that is, even perfectly calibrated soft labels can result in a substantially inaccurate estimate. Then, we show that isotonic calibration can provide a statistically consistent estimator under an assumption weaker than that of the previous work. Our method is instance-free, i.e., we do not assume access to any input instances. This feature allows it to be adopted in practical scenarios where the instances are not available due to privacy issues. Experiments with synthetic and real-world datasets show the validity of our methods and theory.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies</title>
<link>https://arxiv.org/abs/2505.20765</link>
<guid>https://arxiv.org/abs/2505.20765</guid>
<content:encoded><![CDATA[
<div> unsupervised anomaly detection, time series, data augmentation, multiclass classification, robustness

Summary: 
RedLamp proposes a novel approach to unsupervised anomaly detection in time series data. Unlike existing methods that focus on learning normality, RedLamp utilizes diverse data augmentations to generate multiclass pseudo-anomalies, covering a wide range of time series anomalies. By employing soft labels in multiclass classification, the model prevents overconfidence and ensures robustness against contaminated or false anomalies. The learned latent space is inherently explainable as it separates pseudo-anomalies into multiclasses. Experimental results demonstrate the effectiveness of RedLamp in anomaly detection and its ability to handle anomaly contamination, making it a promising approach in the field of unsupervised anomaly detection in time series data.  <div>
arXiv:2505.20765v1 Announce Type: new 
Abstract: Unsupervised anomaly detection in time series has been a pivotal research area for decades. Current mainstream approaches focus on learning normality, on the assumption that all or most of the samples in the training set are normal. However, anomalies in the training set (i.e., anomaly contamination) can be misleading. Recent studies employ data augmentation to generate pseudo-anomalies and learn the boundary separating the training samples from the augmented samples. Although this approach mitigates anomaly contamination if augmented samples mimic unseen real anomalies, it suffers from several limitations. (1) Covering a wide range of time series anomalies is challenging. (2) It disregards augmented samples that resemble normal samples (i.e., false anomalies). (3) It places too much trust in the labels of training and augmented samples. In response, we propose RedLamp, which employs diverse data augmentations to generate multiclass pseudo-anomalies and learns the multiclass boundary. Such multiclass pseudo-anomalies cover a wide variety of time series anomalies. We conduct multiclass classification using soft labels, which prevents the model from being overconfident and ensures its robustness against contaminated/false anomalies. The learned latent space is inherently explainable as it is trained to separate pseudo-anomalies into multiclasses. Extensive experiments demonstrate the effectiveness of RedLamp in anomaly detection and its robustness against anomaly contamination.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimePro: Efficient Multivariate Long-term Time Series Forecasting with Variable- and Time-Aware Hyper-state</title>
<link>https://arxiv.org/abs/2505.20774</link>
<guid>https://arxiv.org/abs/2505.20774</guid>
<content:encoded><![CDATA[
<div> Keywords: long-term time series forecasting, multi-delay issue, TimePro, variate tokens, real-world benchmarks

Summary:<br />
- The article introduces TimePro, a novel Mamba-based model designed to address the multi-delay issue in long-term time series forecasting by considering the variable relationships and temporal information.
- Traditional models often struggle with capturing distinct variable influences over different time intervals, limiting their forecasting capabilities.
- TimePro constructs variate- and time-aware hyper-states, allowing for the preservation of fine-grained temporal features of variate tokens and adaptive selection of focused time points.
- By reconstructing hyper-states that capture variable relationships and important temporal information, TimePro achieves accurate forecasting results on eight real-world benchmarks.
- The model demonstrates competitive performance with linear complexity and the code is available for deployment and further research on the GitHub repository. 

<br /><br />Summary: TimePro is an innovative Mamba-based model that addresses the multi-delay issue in long-term time series forecasting by creating variate- and time-aware hyper-states. By preserving fine-grained temporal features of variate tokens and selecting focused time points, TimePro improves forecasting accuracy on real-world benchmarks. The model's competitive performance with linear complexity and availability of code on GitHub make it a promising tool for accurate long-term forecasting. <div>
arXiv:2505.20774v1 Announce Type: new 
Abstract: In long-term time series forecasting, different variables often influence the target variable over distinct time intervals, a challenge known as the multi-delay issue. Traditional models typically process all variables or time points uniformly, which limits their ability to capture complex variable relationships and obtain non-trivial time representations. To address this issue, we propose TimePro, an innovative Mamba-based model that constructs variate- and time-aware hyper-states. Unlike conventional approaches that merely transfer plain states across variable or time dimensions, TimePro preserves the fine-grained temporal features of each variate token and adaptively selects the focused time points to tune the plain state. The reconstructed hyper-state can perceive both variable relationships and salient temporal information, which helps the model make accurate forecasting. In experiments, TimePro performs competitively on eight real-world long-term forecasting benchmarks with satisfactory linear complexity. Code is available at https://github.com/xwmaxwma/TimePro.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-invasive maturity assessment of iPSC-CMs based on optical maturity characteristics using interpretable AI</title>
<link>https://arxiv.org/abs/2505.20775</link>
<guid>https://arxiv.org/abs/2505.20775</guid>
<content:encoded><![CDATA[
<div> Keywords: iPSC-CMs, maturation, artificial intelligence, motion analysis, therapeutic targets <br />
<br />
Summary: 
The study focused on developing a non-invasive method for automatically assessing the maturity level of induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) using artificial intelligence (AI) and motion analysis. The researchers analyzed video recordings of immature iPSC-CMs and more mature ones cultured in maturation medium, extracting 10 features from each recording. An optimized support vector machine (SVM) model achieved a high accuracy of 99.5% on a test set. Important features for maturity assessment were identified as displacement, relaxation-rise time, and beating duration using Shapley Additive Explanations (SHAP). This non-invasive approach could potentially improve the reproducibility of experimental studies by providing a quick and reliable method to assess iPSC-CMs maturity before conducting functional tests or drug screenings. <div>
arXiv:2505.20775v1 Announce Type: new 
Abstract: Human induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) are an important resource for the identification of new therapeutic targets and cardioprotective drugs. After differentiation iPSC-CMs show an immature, fetal-like phenotype. Cultivation of iPSC-CMs in lipid-supplemented maturation medium (MM) strongly enhances their structural, metabolic and functional phenotype. Nevertheless, assessing iPSC-CM maturation state remains challenging as most methods are time consuming and go in line with cell damage or loss of the sample. To address this issue, we developed a non-invasive approach for automated classification of iPSC-CM maturity through interpretable artificial intelligence (AI)-based analysis of beat characteristics derived from video-based motion analysis. In a prospective study, we evaluated 230 video recordings of early-state, immature iPSC-CMs on day 21 after differentiation (d21) and more mature iPSC-CMs cultured in MM (d42, MM). For each recording, 10 features were extracted using Maia motion analysis software and entered into a support vector machine (SVM). The hyperparameters of the SVM were optimized in a grid search on 80 % of the data using 5-fold cross-validation. The optimized model achieved an accuracy of 99.5 $\pm$ 1.1 % on a hold-out test set. Shapley Additive Explanations (SHAP) identified displacement, relaxation-rise time and beating duration as the most relevant features for assessing maturity level. Our results suggest the use of non-invasive, optical motion analysis combined with AI-based methods as a tool to assess iPSC-CMs maturity and could be applied before performing functional readouts or drug testing. This may potentially reduce the variability and improve the reproducibility of experimental studies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-VQC: A Novel QML Approach for Enhancing Healthcare Classification</title>
<link>https://arxiv.org/abs/2505.20797</link>
<guid>https://arxiv.org/abs/2505.20797</guid>
<content:encoded><![CDATA[
<div> Keywords: diagnosis, diseases, Machine Learning, Quantum models, class imbalances 

Summary: 
Machine Learning has revolutionized disease diagnosis by creating accurate classification models. However, class imbalances in these problems can hinder their effectiveness. Quantum models have emerged as a promising solution to overcome these limitations, as they can map data in a higher-dimensional computational space and express complex patterns. Accurate and reliable diagnosis of diseases is crucial for timely medical treatment and improving patient survival rates. The interest in Quantum models has been driven by their potential to address the challenges faced by traditional classification models due to class imbalances. Quantum models offer a new approach to disease diagnosis, leveraging their ability to handle complex patterns and increase the accuracy of results. <div>
arXiv:2505.20797v1 Announce Type: new 
Abstract: Accurate and reliable diagnosis of diseases is crucial in enabling timely medical treatment and enhancing patient survival rates. In recent years, Machine Learning has revolutionized diagnostic practices by creating classification models capable of identifying diseases. However, these classification problems often suffer from significant class imbalances, which can inhibit the effectiveness of traditional models. Therefore, the interest in Quantum models has arisen, driven by the captivating promise of overcoming the limitations of the classical counterpart thanks to their ability to express complex patterns by mapping data in a higher-dimensional computational space.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leaner Transformers: More Heads, Less Depth</title>
<link>https://arxiv.org/abs/2505.20802</link>
<guid>https://arxiv.org/abs/2505.20802</guid>
<content:encoded><![CDATA[
arXiv:2505.20802v1 Announce Type: new 
Abstract: Transformers have reshaped machine learning by utilizing attention mechanisms to capture complex patterns in large datasets, leading to significant improvements in performance. This success has contributed to the belief that "bigger means better", leading to ever-increasing model sizes. This paper challenge this ideology by showing that many existing transformers might be unnecessarily oversized. We discover a theoretical principle that redefines the role of multi-head attention. An important benefit of the multiple heads is in improving the conditioning of the attention block. We exploit this theoretical insight and redesign popular architectures with an increased number of heads. The improvement in the conditioning proves so significant in practice that model depth can be decreased, reducing the parameter count by up to 30-50% while maintaining accuracy. We obtain consistent benefits across a variety of transformer-based architectures of various scales, on tasks in computer vision (ImageNet-1k) as well as language and sequence modeling (GLUE benchmark, TinyStories, and the Long-Range Arena benchmark).
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Machine Learning in Healthcare: Evaluating QNN and QSVM Models</title>
<link>https://arxiv.org/abs/2505.20804</link>
<guid>https://arxiv.org/abs/2505.20804</guid>
<content:encoded><![CDATA[
arXiv:2505.20804v1 Announce Type: new 
Abstract: Effective and accurate diagnosis of diseases such as cancer, diabetes, and heart failure is crucial for timely medical intervention and improving patient survival rates. Machine learning has revolutionized diagnostic methods in recent years by developing classification models that detect diseases based on selected features. However, these classification tasks are often highly imbalanced, limiting the performance of classical models. Quantum models offer a promising alternative, exploiting their ability to express complex patterns by operating in a higher-dimensional computational space through superposition and entanglement. These unique properties make quantum models potentially more effective in addressing the challenges of imbalanced datasets. This work evaluates the potential of quantum classifiers in healthcare, focusing on Quantum Neural Networks (QNNs) and Quantum Support Vector Machines (QSVMs), comparing them with popular classical models. The study is based on three well-known healthcare datasets -- Prostate Cancer, Heart Failure, and Diabetes. The results indicate that QSVMs outperform QNNs across all datasets due to their susceptibility to overfitting. Furthermore, quantum models prove the ability to overcome classical models in scenarios with high dataset imbalance. Although preliminary, these findings highlight the potential of quantum models in healthcare classification tasks and lead the way for further research in this domain.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple yet Effective Graph Distillation via Clustering</title>
<link>https://arxiv.org/abs/2505.20807</link>
<guid>https://arxiv.org/abs/2505.20807</guid>
<content:encoded><![CDATA[
arXiv:2505.20807v1 Announce Type: new 
Abstract: Despite plentiful successes achieved by graph representation learning in various domains, the training of graph neural networks (GNNs) still remains tenaciously challenging due to the tremendous computational overhead needed for sizable graphs in practice. Recently, graph data distillation (GDD), which seeks to distill large graphs into compact and informative ones, has emerged as a promising technique to enable efficient GNN training. However, most existing GDD works rely on heuristics that align model gradients or representation distributions on condensed and original graphs, leading to compromised result quality, expensive training for distilling large graphs, or both. Motivated by this, this paper presents an efficient and effective GDD approach, ClustGDD. Under the hood, ClustGDD resorts to synthesizing the condensed graph and node attributes through fast and theoretically-grounded clustering that minimizes the within-cluster sum of squares and maximizes the homophily on the original graph. The fundamental idea is inspired by our empirical and theoretical findings unveiling the connection between clustering and empirical condensation quality using Fr\'echet Inception Distance, a well-known quality metric for synthetic images. Furthermore, to mitigate the adverse effects caused by the homophily-based clustering, ClustGDD refines the nodal attributes of the condensed graph with a small augmentation learned via class-aware graph sampling and consistency loss. Our extensive experiments exhibit that GNNs trained over condensed graphs output by ClustGDD consistently achieve superior or comparable performance to state-of-the-art GDD methods in terms of node classification on five benchmark datasets, while being orders of magnitude faster.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Credit Default Prediction with Ensemble Learning and SHAP</title>
<link>https://arxiv.org/abs/2505.20815</link>
<guid>https://arxiv.org/abs/2505.20815</guid>
<content:encoded><![CDATA[
arXiv:2505.20815v1 Announce Type: new 
Abstract: This study focuses on the problem of credit default prediction, builds a modeling framework based on machine learning, and conducts comparative experiments on a variety of mainstream classification algorithms. Through preprocessing, feature engineering, and model training of the Home Credit dataset, the performance of multiple models including logistic regression, random forest, XGBoost, LightGBM, etc. in terms of accuracy, precision, and recall is evaluated. The results show that the ensemble learning method has obvious advantages in predictive performance, especially in dealing with complex nonlinear relationships between features and data imbalance problems. It shows strong robustness. At the same time, the SHAP method is used to analyze the importance and dependency of features, and it is found that the external credit score variable plays a dominant role in model decision making, which helps to improve the model's interpretability and practical application value. The research results provide effective reference and technical support for the intelligent development of credit risk control systems.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling</title>
<link>https://arxiv.org/abs/2505.20836</link>
<guid>https://arxiv.org/abs/2505.20836</guid>
<content:encoded><![CDATA[
arXiv:2505.20836v1 Announce Type: new 
Abstract: Inspired by the great success of Masked Language Modeling (MLM) in the natural language domain, the paradigm of self-supervised pre-training and fine-tuning has also achieved remarkable progress in the field of DNA sequence modeling. However, previous methods often relied on massive pre-training data or large-scale base models with huge parameters, imposing a significant computational burden. To address this, many works attempted to use more compact models to achieve similar outcomes but still fell short by a considerable margin. In this work, we propose a Hybrid Architecture Distillation (HAD) approach, leveraging both distillation and reconstruction tasks for more efficient and effective pre-training. Specifically, we employ the NTv2-500M as the teacher model and devise a grouping masking strategy to align the feature embeddings of visible tokens while concurrently reconstructing the invisible tokens during MLM pre-training. To validate the effectiveness of our proposed method, we conducted comprehensive experiments on the Nucleotide Transformer Benchmark and Genomic Benchmark. Compared to models with similar parameters, our model achieved excellent performance. More surprisingly, it even surpassed the distillation ceiling-teacher model on some sub-tasks, which is more than 500 $\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization, which shows that our model can gain a sophisticated understanding of the intrinsic representation pattern in genomic sequences.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration</title>
<link>https://arxiv.org/abs/2505.20839</link>
<guid>https://arxiv.org/abs/2505.20839</guid>
<content:encoded><![CDATA[
arXiv:2505.20839v1 Announce Type: new 
Abstract: As large language models become increasingly prevalent, memory bandwidth constraints significantly limit inference throughput, motivating post-training quantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ framework and an INT4-FP8 matrix multiplication kernel that accelerates LLM inference across all linear layers. Specifically, FireQ quantizes linear layer weights and key-values to INT4, and activations and queries to FP8, significantly enhancing throughput. Additionally, we introduce a three-stage pipelining for the prefill phase, which modifies the FlashAttention-3 kernel, effectively reducing time-to-first-token in the prefill phase. To minimize accuracy loss from quantization, we develop novel outlier smoothing techniques tailored separately for linear and attention layers. In linear layers, we explicitly use per-tensor scaling to prevent underflow caused by the FP8 quantization scaling factor of INT4 quantization, and channel-wise scaling to compensate for coarse granularity of INT4. In attention layers, we address quantization challenges posed by rotary positional embeddings (RoPE) by combining pre-RoPE and post-RoPE scaling strategies. FireQ significantly outperforms state-of-the-art methods, achieving 1.68x faster inference in feed-forward network layers on Llama2-7B and 1.26x faster prefill phase performance on Llama3-8B compared to QServe, with negligible accuracy loss.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aggregation Buffer: Revisiting DropEdge with a New Parameter Block</title>
<link>https://arxiv.org/abs/2505.20840</link>
<guid>https://arxiv.org/abs/2505.20840</guid>
<content:encoded><![CDATA[
arXiv:2505.20840v1 Announce Type: new 
Abstract: We revisit DropEdge, a data augmentation technique for GNNs which randomly removes edges to expose diverse graph structures during training. While being a promising approach to effectively reduce overfitting on specific connections in the graph, we observe that its potential performance gain in supervised learning tasks is significantly limited. To understand why, we provide a theoretical analysis showing that the limited performance of DropEdge comes from the fundamental limitation that exists in many GNN architectures. Based on this analysis, we propose Aggregation Buffer, a parameter block specifically designed to improve the robustness of GNNs by addressing the limitation of DropEdge. Our method is compatible with any GNN model, and shows consistent performance improvements on multiple datasets. Moreover, our method effectively addresses well-known problems such as degree bias or structural disparity as a unifying solution. Code and datasets are available at https://github.com/dooho00/agg-buffer.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperation of Experts: Fusing Heterogeneous Information with Large Margin</title>
<link>https://arxiv.org/abs/2505.20853</link>
<guid>https://arxiv.org/abs/2505.20853</guid>
<content:encoded><![CDATA[
arXiv:2505.20853v1 Announce Type: new 
Abstract: Fusing heterogeneous information remains a persistent challenge in modern data analysis. While significant progress has been made, existing approaches often fail to account for the inherent heterogeneity of object patterns across different semantic spaces. To address this limitation, we propose the Cooperation of Experts (CoE) framework, which encodes multi-typed information into unified heterogeneous multiplex networks. By overcoming modality and connection differences, CoE provides a powerful and flexible model for capturing the intricate structures of real-world complex data. In our framework, dedicated encoders act as domain-specific experts, each specializing in learning distinct relational patterns in specific semantic spaces. To enhance robustness and extract complementary knowledge, these experts collaborate through a novel large margin mechanism supported by a tailored optimization strategy. Rigorous theoretical analyses guarantee the framework's feasibility and stability, while extensive experiments across diverse benchmarks demonstrate its superior performance and broad applicability. Our code is available at https://github.com/strangeAlan/CoE.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization</title>
<link>https://arxiv.org/abs/2505.20881</link>
<guid>https://arxiv.org/abs/2505.20881</guid>
<content:encoded><![CDATA[
arXiv:2505.20881v1 Announce Type: new 
Abstract: Heuristic design with large language models (LLMs) has emerged as a promising approach for tackling combinatorial optimization problems (COPs). However, existing approaches often rely on manually predefined evolutionary computation (EC) optimizers and single-task training schemes, which may constrain the exploration of diverse heuristic algorithms and hinder the generalization of the resulting heuristics. To address these issues, we propose Meta-Optimization of Heuristics (MoH), a novel framework that operates at the optimizer level, discovering effective optimizers through the principle of meta-learning. Specifically, MoH leverages LLMs to iteratively refine a meta-optimizer that autonomously constructs diverse optimizers through (self-)invocation, thereby eliminating the reliance on a predefined EC optimizer. These constructed optimizers subsequently evolve heuristics for downstream tasks, enabling broader heuristic exploration. Moreover, MoH employs a multi-task training scheme to promote its generalization capability. Experiments on classic COPs demonstrate that MoH constructs an effective and interpretable meta-optimizer, achieving state-of-the-art performance across various downstream tasks, particularly in cross-size settings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fedivertex: a Graph Dataset based on Decentralized Social Networks for Trustworthy Machine Learning</title>
<link>https://arxiv.org/abs/2505.20882</link>
<guid>https://arxiv.org/abs/2505.20882</guid>
<content:encoded><![CDATA[
arXiv:2505.20882v1 Announce Type: new 
Abstract: Decentralized machine learning - where each client keeps its own data locally and uses its own computational resources to collaboratively train a model by exchanging peer-to-peer messages - is increasingly popular, as it enables better scalability and control over the data. A major challenge in this setting is that learning dynamics depend on the topology of the communication graph, which motivates the use of real graph datasets for benchmarking decentralized algorithms. Unfortunately, existing graph datasets are largely limited to for-profit social networks crawled at a fixed point in time and often collected at the user scale, where links are heavily influenced by the platform and its recommendation algorithms. The Fediverse, which includes several free and open-source decentralized social media platforms such as Mastodon, Misskey, and Lemmy, offers an interesting real-world alternative. We introduce Fedivertex, a new dataset of 182 graphs, covering seven social networks from the Fediverse, crawled weekly over 14 weeks. We release the dataset along with a Python package to facilitate its use, and illustrate its utility on several tasks, including a new defederation task, which captures a process of link deletion observed on these networks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Bounds for Swap Multicalibration and Swap Omniprediction</title>
<link>https://arxiv.org/abs/2505.20885</link>
<guid>https://arxiv.org/abs/2505.20885</guid>
<content:encoded><![CDATA[
arXiv:2505.20885v1 Announce Type: new 
Abstract: In this paper, we consider the related problems of multicalibration -- a multigroup fairness notion and omniprediction -- a simultaneous loss minimization paradigm, both in the distributional and online settings. The recent work of Garg et al. (2024) raised the open problem of whether it is possible to efficiently achieve $O(\sqrt{T})$ $\ell_{2}$-multicalibration error against bounded linear functions. In this paper, we answer this question in a strongly affirmative sense. We propose an efficient algorithm that achieves $O(T^{\frac{1}{3}})$ $\ell_{2}$-swap multicalibration error (both in high probability and expectation). On propagating this bound onward, we obtain significantly improved rates for $\ell_{1}$-swap multicalibration and swap omniprediction for a loss class of convex Lipschitz functions. In particular, we show that our algorithm achieves $O(T^{\frac{2}{3}})$ $\ell_{1}$-swap multicalibration and swap omniprediction errors, thereby improving upon the previous best-known bound of $O(T^{\frac{7}{8}})$. As a consequence of our improved online results, we further obtain several improved sample complexity rates in the distributional setting. In particular, we establish a $O(\varepsilon ^ {-3})$ sample complexity of efficiently learning an $\varepsilon$-swap omnipredictor for the class of convex and Lipschitz functions, $O(\varepsilon ^{-2.5})$ sample complexity of efficiently learning an $\varepsilon$-swap agnostic learner for the squared loss, and $O(\varepsilon ^ {-5}), O(\varepsilon ^ {-2.5})$ sample complexities of learning $\ell_{1}, \ell_{2}$-swap multicalibrated predictors against linear functions, all of which significantly improve on the previous best-known bounds.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Time Soft Alignment Enables Resilient Learning without Weight Transport</title>
<link>https://arxiv.org/abs/2505.20892</link>
<guid>https://arxiv.org/abs/2505.20892</guid>
<content:encoded><![CDATA[
arXiv:2505.20892v1 Announce Type: new 
Abstract: Backpropagation is the cornerstone of deep learning, but its reliance on symmetric weight transport and global synchronization makes it computationally expensive and biologically implausible. Feedback alignment offers a promising alternative by approximating error gradients through fixed random feedback, thereby avoiding symmetric weight transport. However, this approach often struggles with poor learning performance and instability, especially in deep networks. Here, we show that a one-time soft alignment between forward and feedback weights at initialization enables deep networks to achieve performance comparable to backpropagation, without requiring weight transport during learning. This simple initialization condition guides stable error minimization in the loss landscape, improving network trainability. Spectral analyses further reveal that initial alignment promotes smoother gradient flow and convergence to flatter minima, resulting in better generalization and robustness. Notably, we also find that allowing moderate deviations from exact weight symmetry can improve adversarial robustness compared to standard backpropagation. These findings demonstrate that a simple initialization strategy can enable effective learning in deep networks in a biologically plausible and resource-efficient manner.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepConvContext: A Multi-Scale Approach to Timeseries Classification in Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.20894</link>
<guid>https://arxiv.org/abs/2505.20894</guid>
<content:encoded><![CDATA[
arXiv:2505.20894v1 Announce Type: new 
Abstract: Despite recognized limitations in modeling long-range temporal dependencies, Human Activity Recognition (HAR) has traditionally relied on a sliding window approach to segment labeled datasets. Deep learning models like the DeepConvLSTM typically classify each window independently, thereby restricting learnable temporal context to within-window information. To address this constraint, we propose DeepConvContext, a multi-scale time series classification framework for HAR. Drawing inspiration from the vision-based Temporal Action Localization community, DeepConvContext models both intra- and inter-window temporal patterns by processing sequences of time-ordered windows. Unlike recent HAR models that incorporate attention mechanisms, DeepConvContext relies solely on LSTMs -- with ablation studies demonstrating the superior performance of LSTMs over attention-based variants for modeling inertial sensor data. Across six widely-used HAR benchmarks, DeepConvContext achieves an average 10% improvement in F1-score over the classic DeepConvLSTM, with gains of up to 21%. Code to reproduce our experiments is publicly available via github.com/mariusbock/context_har.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Transformers Learn Variable Binding in Symbolic Programs?</title>
<link>https://arxiv.org/abs/2505.20896</link>
<guid>https://arxiv.org/abs/2505.20896</guid>
<content:encoded><![CDATA[
arXiv:2505.20896v1 Announce Type: new 
Abstract: Variable binding -- the ability to associate variables with values -- is fundamental to symbolic computation and cognition. Although classical architectures typically implement variable binding via addressable memory, it is not well understood how modern neural networks lacking built-in binding operations may acquire this capacity. We investigate this by training a Transformer to dereference queried variables in symbolic programs where variables are assigned either numerical constants or other variables. Each program requires following chains of variable assignments up to four steps deep to find the queried value, and also contains irrelevant chains of assignments acting as distractors. Our analysis reveals a developmental trajectory with three distinct phases during training: (1) random prediction of numerical constants, (2) a shallow heuristic prioritizing early variable assignments, and (3) the emergence of a systematic mechanism for dereferencing assignment chains. Using causal interventions, we find that the model learns to exploit the residual stream as an addressable memory space, with specialized attention heads routing information across token positions. This mechanism allows the model to dynamically track variable bindings across layers, resulting in accurate dereferencing. Our results show how Transformer models can learn to implement systematic variable binding without explicit architectural support, bridging connectionist and symbolic approaches.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humble AI in the real-world: the case of algorithmic hiring</title>
<link>https://arxiv.org/abs/2505.20918</link>
<guid>https://arxiv.org/abs/2505.20918</guid>
<content:encoded><![CDATA[
arXiv:2505.20918v1 Announce Type: new 
Abstract: Humble AI (Knowles et al., 2023) argues for cautiousness in AI development and deployments through scepticism (accounting for limitations of statistical learning), curiosity (accounting for unexpected outcomes), and commitment (accounting for multifaceted values beyond performance). We present a real-world case study for humble AI in the domain of algorithmic hiring. Specifically, we evaluate virtual screening algorithms in a widely used hiring platform that matches candidates to job openings. There are several challenges in misrecognition and stereotyping in such contexts that are difficult to assess through standard fairness and trust frameworks; e.g., someone with a non-traditional background is less likely to rank highly. We demonstrate technical feasibility of how humble AI principles can be translated to practice through uncertainty quantification of ranks, entropy estimates, and a user experience that highlights algorithmic unknowns. We describe preliminary discussions with focus groups made up of recruiters. Future user studies seek to evaluate whether the higher cognitive load of a humble AI system fosters a climate of trust in its outcomes.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label Leakage in Federated Inertial-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.20924</link>
<guid>https://arxiv.org/abs/2505.20924</guid>
<content:encoded><![CDATA[
arXiv:2505.20924v1 Announce Type: new 
Abstract: While prior work has shown that Federated Learning updates can leak sensitive information, label reconstruction attacks, which aim to recover input labels from shared gradients, have not yet been examined in the context of Human Activity Recognition (HAR). Given the sensitive nature of activity labels, this study evaluates the effectiveness of state-of-the-art gradient-based label leakage attacks on HAR benchmark datasets. Our findings show that the number of activity classes, sampling strategy, and class imbalance are critical factors influencing the extent of label leakage, with reconstruction accuracies reaching up to 90% on two benchmark datasets, even for trained models. Moreover, we find that Local Differential Privacy techniques such as gradient noise and clipping offer only limited protection, as certain attacks still reliably infer both majority and minority class labels. We conclude by offering practical recommendations for the privacy-aware deployment of federated HAR systems and identify open challenges for future research. Code to reproduce our experiments is publicly available via github.com/mariusbock/leakage_har.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLMC-based Resource Adequacy Assessment with Active Learning Trained Surrogate Models</title>
<link>https://arxiv.org/abs/2505.20930</link>
<guid>https://arxiv.org/abs/2505.20930</guid>
<content:encoded><![CDATA[
arXiv:2505.20930v1 Announce Type: new 
Abstract: Multilevel Monte Carlo (MLMC) is a flexible and effective variance reduction technique for accelerating reliability assessments of complex power system. Recently, data-driven surrogate models have been proposed as lower-level models in the MLMC framework due to their high correlation and negligible execution time once trained. However, in resource adequacy assessments, pre-labeled datasets are typically unavailable. For large-scale systems, the efficiency gains from surrogate models are often offset by the substantial time required for labeling training data. Therefore, this paper introduces a speed metric that accounts for training time in evaluating MLMC efficiency. Considering the total time budget is limited, a vote-by-committee active learning approach is proposed to reduce the required labeling calls. A case study demonstrates that, within practical variance thresholds, active learning enables significantly improved MLMC efficiency with reduced training effort, compared to regular surrogate modelling approaches.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion</title>
<link>https://arxiv.org/abs/2505.20934</link>
<guid>https://arxiv.org/abs/2505.20934</guid>
<content:encoded><![CDATA[
arXiv:2505.20934v1 Announce Type: new 
Abstract: Adversarial samples exploit irregularities in the manifold ``learned'' by deep learning models to cause misclassifications. The study of these adversarial samples provides insight into the features a model uses to classify inputs, which can be leveraged to improve robustness against future attacks. However, much of the existing literature focuses on constrained adversarial samples, which do not accurately reflect test-time errors encountered in real-world settings. To address this, we propose `NatADiff', an adversarial sampling scheme that leverages denoising diffusion to generate natural adversarial samples. Our approach is based on the observation that natural adversarial samples frequently contain structural elements from the adversarial class. Deep learning models can exploit these structural elements to shortcut the classification process, rather than learning to genuinely distinguish between classes. To leverage this behavior, we guide the diffusion trajectory towards the intersection of the true and adversarial classes, combining time-travel sampling with augmented classifier guidance to enhance attack transferability while preserving image fidelity. Our method achieves comparable attack success rates to current state-of-the-art techniques, while exhibiting significantly higher transferability across model architectures and better alignment with natural test-time errors as measured by FID. These results demonstrate that NatADiff produces adversarial samples that not only transfer more effectively across models, but more faithfully resemble naturally occurring test-time errors.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Sparsity Constraint Under High-Rank Property in Partial Multi-Label Learning</title>
<link>https://arxiv.org/abs/2505.20938</link>
<guid>https://arxiv.org/abs/2505.20938</guid>
<content:encoded><![CDATA[
arXiv:2505.20938v1 Announce Type: new 
Abstract: Partial Multi-Label Learning (PML) extends the multi-label learning paradigm to scenarios where each sample is associated with a candidate label set containing both ground-truth labels and noisy labels. Existing PML methods commonly rely on two assumptions: sparsity of the noise label matrix and low-rankness of the ground-truth label matrix. However, these assumptions are inherently conflicting and impractical for real-world scenarios, where the true label matrix is typically full-rank or close to full-rank. To address these limitations, we demonstrate that the sparsity constraint contributes to the high-rank property of the predicted label matrix. Based on this, we propose a novel method Schirn, which introduces a sparsity constraint on the noise label matrix while enforcing a high-rank property on the predicted label matrix. Extensive experiments demonstrate the superior performance of Schirn compared to state-of-the-art methods, validating its effectiveness in tackling real-world PML challenges.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Spectral Control of Partially Observed Linear Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.20943</link>
<guid>https://arxiv.org/abs/2505.20943</guid>
<content:encoded><![CDATA[
arXiv:2505.20943v1 Announce Type: new 
Abstract: We propose a new method for the problem of controlling linear dynamical systems under partial observation and adversarial disturbances. Our new algorithm, Double Spectral Control (DSC), matches the best known regret guarantees while exponentially improving runtime complexity over previous approaches in its dependence on the system's stability margin. Our key innovation is a two-level spectral approximation strategy, leveraging double convolution with a universal basis of spectral filters, enabling efficient and accurate learning of the best linear dynamical controllers.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Communication meets System 2 ML: How Abstraction, Compositionality and Emergent Languages Shape Intelligence</title>
<link>https://arxiv.org/abs/2505.20964</link>
<guid>https://arxiv.org/abs/2505.20964</guid>
<content:encoded><![CDATA[
arXiv:2505.20964v1 Announce Type: new 
Abstract: The trajectories of 6G and AI are set for a creative collision. However, current visions for 6G remain largely incremental evolutions of 5G, while progress in AI is hampered by brittle, data-hungry models that lack robust reasoning capabilities. This paper argues for a foundational paradigm shift, moving beyond the purely technical level of communication toward systems capable of semantic understanding and effective, goal-oriented interaction. We propose a unified research vision rooted in the principles of System-2 cognition, built upon three pillars: Abstraction, enabling agents to learn meaningful world models from raw sensorimotor data; Compositionality, providing the algebraic tools to combine learned concepts and subsystems; and Emergent Communication, allowing intelligent agents to create their own adaptive and grounded languages. By integrating these principles, we lay the groundwork for truly intelligent systems that can reason, adapt, and collaborate, unifying advances in wireless communications, machine learning, and robotics under a single coherent framework.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the behavior of representation forgetting in continual learning</title>
<link>https://arxiv.org/abs/2505.20970</link>
<guid>https://arxiv.org/abs/2505.20970</guid>
<content:encoded><![CDATA[
arXiv:2505.20970v1 Announce Type: new 
Abstract: In continual learning scenarios, catastrophic forgetting of previously learned tasks is a critical issue, making it essential to effectively measure such forgetting. Recently, there has been growing interest in focusing on representation forgetting, the forgetting measured at the hidden layer. In this paper, we provide the first theoretical analysis of representation forgetting and use this analysis to better understand the behavior of continual learning. First, we introduce a new metric called representation discrepancy, which measures the difference between representation spaces constructed by two snapshots of a model trained through continual learning. We demonstrate that our proposed metric serves as an effective surrogate for the representation forgetting while remaining analytically tractable. Second, through mathematical analysis of our metric, we derive several key findings about the dynamics of representation forgetting: the forgetting occurs more rapidly to a higher degree as the layer index increases, while increasing the width of the network slows down the forgetting process. Third, we support our theoretical findings through experiments on real image datasets, including Split-CIFAR100 and ImageNet1K.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs</title>
<link>https://arxiv.org/abs/2505.20972</link>
<guid>https://arxiv.org/abs/2505.20972</guid>
<content:encoded><![CDATA[
arXiv:2505.20972v1 Announce Type: new 
Abstract: Along with AI computing shining in scientific discovery, its potential in the combinatorial optimization (CO) domain has also emerged in recent years. Yet, existing unsupervised neural network solvers struggle to solve $k$-grouping problems (e.g., coloring, partitioning) on large-scale graphs and hypergraphs, due to limited computational frameworks. In this work, we propose Deep $k$-grouping, an unsupervised learning-based CO framework. Specifically, we contribute: Novel one-hot encoded polynomial unconstrained binary optimization (OH-PUBO), a formulation for modeling k-grouping problems on graphs and hypergraphs (e.g., graph/hypergraph coloring and partitioning); GPU-accelerated algorithms for large-scale k-grouping CO problems. Deep $k$-grouping employs the relaxation of large-scale OH-PUBO objectives as differentiable loss functions and trains to optimize them in an unsupervised manner. To ensure scalability, it leverages GPU-accelerated algorithms to unify the training pipeline; A Gini coefficient-based continuous relaxation annealing strategy to enforce discreteness of solutions while preventing convergence to local optima. Experimental results demonstrate that Deep $k$-grouping outperforms existing neural network solvers and classical heuristics such as SCIP and Tabu.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation</title>
<link>https://arxiv.org/abs/2505.20992</link>
<guid>https://arxiv.org/abs/2505.20992</guid>
<content:encoded><![CDATA[
arXiv:2505.20992v1 Announce Type: new 
Abstract: Graph neural networks (GNNs), which capture graph structures via a feature aggregation mechanism following the graph embedding framework, have demonstrated a powerful ability to support various tasks. According to the topology properties (e.g., structural roles or community memberships of nodes) to be preserved, graph embedding can be categorized into identity and position embedding. However, it is unclear for most GNN-based methods which property they can capture. Some of them may also suffer from low efficiency and scalability caused by several time- and space-consuming procedures (e.g., feature extraction and training). From a perspective of graph signal processing, we find that high- and low-frequency information in the graph spectral domain may characterize node identities and positions, respectively. Based on this investigation, we propose random feature aggregation (RFA) for efficient identity and position embedding, serving as an extreme ablation study regarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without learnable parameters as its backbone, (ii) only uses random noises as inputs, and (iii) derives embeddings via just one feed-forward propagation (FFP). Inspired by degree-corrected spectral clustering, we further introduce a degree correction mechanism to the GNN backbone. Surprisingly, our experiments demonstrate that two variants of RFA with high- and low-pass filters can respectively derive informative identity and position embeddings via just one FFP (i.e., without any training). As a result, RFA can achieve a better trade-off between quality and efficiency for both identity and position embedding over various baselines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks</title>
<link>https://arxiv.org/abs/2505.20997</link>
<guid>https://arxiv.org/abs/2505.20997</guid>
<content:encoded><![CDATA[
arXiv:2505.20997v1 Announce Type: new 
Abstract: Binary (0-1) integer programming (BIP) is pivotal in scientific domains requiring discrete decision-making. As the advance of AI computing, recent works explore neural network-based solvers for integer linear programming (ILP) problems. Yet, they lack scalability for tackling nonlinear challenges. To handle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear relaxations, leading to exponential growth in auxiliary variables and severe computation limitations. To overcome these limitations, we propose BIPNN (Binary Integer Programming Neural Network), an unsupervised learning framework to solve nonlinear BIP problems via hypergraph neural networks (HyperGNN). Specifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear (sin, log, exp) optimization problems-into unconstrained, differentiable, and polynomial loss functions. The reformulation stems from the observation of a precise one-to-one mapping between polynomial BIP objectives and hypergraph structures, enabling the unsupervised training of HyperGNN to optimize BIP problems in an end-to-end manner. On this basis, we propose a GPU-accelerated and continuous-annealing-enhanced training pipeline for BIPNN. The pipeline enables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel via straightforward gradient descent, thus significantly reducing the training cost while ensuring the generation of discrete, high-quality solutions. Extensive experiments on synthetic and real-world datasets highlight the superiority of our approach.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models</title>
<link>https://arxiv.org/abs/2505.21005</link>
<guid>https://arxiv.org/abs/2505.21005</guid>
<content:encoded><![CDATA[
arXiv:2505.21005v1 Announce Type: new 
Abstract: Score-based diffusion models (SBDMs) are powerful amortized samplers for Boltzmann distributions; however, imperfect score estimates bias downstream Monte Carlo estimates. Classical importance sampling (IS) can correct this bias, but computing exact likelihoods requires solving the probability-flow ordinary differential equation (PF-ODE), a procedure that is prohibitively costly and scales poorly with dimensionality. We introduce Variance-Tuned Diffusion Importance Sampling (VT-DIS), a lightweight post-training method that adapts the per-step noise covariance of a pretrained SBDM by minimizing the $\alpha$-divergence ($\alpha=2$) between its forward diffusion and reverse denoising trajectories. VT-DIS assigns a single trajectory-wise importance weight to the joint forward-reverse process, yielding unbiased expectation estimates at test time with negligible overhead compared to standard sampling. On the DW-4, LJ-13, and alanine-dipeptide benchmarks, VT-DIS achieves effective sample sizes of approximately 80 %, 35 %, and 3.5 %, respectively, while using only a fraction of the computational budget required by vanilla diffusion + IS or PF-ODE-based IS.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Instrumental Variable Analysis via Federated Generalized Method of Moments</title>
<link>https://arxiv.org/abs/2505.21012</link>
<guid>https://arxiv.org/abs/2505.21012</guid>
<content:encoded><![CDATA[
arXiv:2505.21012v1 Announce Type: new 
Abstract: Instrumental variables (IV) analysis is an important applied tool for areas such as healthcare and consumer economics. For IV analysis in high-dimensional settings, the Generalized Method of Moments (GMM) using deep neural networks offers an efficient approach. With non-i.i.d. data sourced from scattered decentralized clients, federated learning is a popular paradigm for training the models while promising data privacy. However, to our knowledge, no federated algorithm for either GMM or IV analysis exists to date. In this work, we introduce federated instrumental variables analysis (FedIV) via federated generalized method of moments (FedGMM). We formulate FedGMM as a federated zero-sum game defined by a federated non-convex non-concave minimax optimization problem, which is solved using federated gradient descent ascent (FedGDA) algorithm. One key challenge arises in theoretically characterizing the federated local optimality. To address this, we present properties and existence results of clients' local equilibria via FedGDA limit points. Thereby, we show that the federated solution consistently estimates the local moment conditions of every participating client. The proposed algorithm is backed by extensive experiments to demonstrate the efficacy of our approach.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation</title>
<link>https://arxiv.org/abs/2505.21020</link>
<guid>https://arxiv.org/abs/2505.21020</guid>
<content:encoded><![CDATA[
arXiv:2505.21020v1 Announce Type: new 
Abstract: Accurate Subseasonal-to-Seasonal (S2S) ocean simulation is critically important for marine research, yet remains challenging due to its substantial thermal inertia and extended time delay. Machine learning (ML)-based models have demonstrated significant advancements in simulation accuracy and computational efficiency compared to traditional numerical methods. Nevertheless, a significant limitation of current ML models for S2S ocean simulation is their inadequate incorporation of physical consistency and the slow-changing properties of the ocean system. In this work, we propose a neural ocean model (NeuralOM) for S2S ocean simulation with a multi-scale interactive graph neural network to emulate diverse physical phenomena associated with ocean systems effectively. Specifically, we propose a multi-stage framework tailored to model the ocean's slowly changing nature. Additionally, we introduce a multi-scale interactive messaging module to capture complex dynamical behaviors, such as gradient changes and multiplicative coupling relationships inherent in ocean dynamics. Extensive experimental evaluations confirm that our proposed NeuralOM outperforms state-of-the-art models in S2S and extreme event simulation. The codes are available at https://github.com/YuanGao-YG/NeuralOM.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers</title>
<link>https://arxiv.org/abs/2505.21024</link>
<guid>https://arxiv.org/abs/2505.21024</guid>
<content:encoded><![CDATA[
arXiv:2505.21024v1 Announce Type: new 
Abstract: Pause tokens, simple filler symbols such as "...", consistently improve Transformer performance on both language and mathematical tasks, yet their theoretical effect remains unexplained. We provide the first formal separation result, proving that adding pause tokens to constant-depth, logarithmic-width Transformers strictly increases their computational expressivity. With bounded-precision activations, Transformers without pause tokens compute only a strict subset of $\mathsf{AC}^0$ functions, while adding a polynomial number of pause tokens allows them to express the entire class. For logarithmic-precision Transformers, we show that adding pause tokens achieves expressivity equivalent to $\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate that two-layer causally masked Transformers can learn parity when supplied with pause tokens, a function that they appear unable to learn without them. Our results provide a rigorous theoretical explanation for prior empirical findings, clarify how pause tokens interact with width, depth, and numeric precision, and position them as a distinct mechanism, complementary to chain-of-thought prompting, for enhancing Transformer reasoning.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data</title>
<link>https://arxiv.org/abs/2505.21027</link>
<guid>https://arxiv.org/abs/2505.21027</guid>
<content:encoded><![CDATA[
arXiv:2505.21027v1 Announce Type: new 
Abstract: Adversarial attacks pose a significant threat to machine learning models by inducing incorrect predictions through imperceptible perturbations to input data. While these attacks have been extensively studied in unstructured data like images, their application to tabular data presents new challenges. These challenges arise from the inherent heterogeneity and complex feature interdependencies in tabular data, which differ significantly from those in image data. To address these differences, it is crucial to consider imperceptibility as a key criterion specific to tabular data. Most current research focuses primarily on achieving effective adversarial attacks, often overlooking the importance of maintaining imperceptibility. To address this gap, we propose a new benchmark for adversarial attacks on tabular data that evaluates both effectiveness and imperceptibility. In this study, we assess the effectiveness and imperceptibility of five adversarial attacks across four models using eleven tabular datasets, including both mixed and numerical-only datasets. Our analysis explores how these factors interact and influence the overall performance of the attacks. We also compare the results across different dataset types to understand the broader implications of these findings. The findings from this benchmark provide valuable insights for improving the design of adversarial attack algorithms, thereby advancing the field of adversarial machine learning on tabular data.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms</title>
<link>https://arxiv.org/abs/2505.21034</link>
<guid>https://arxiv.org/abs/2505.21034</guid>
<content:encoded><![CDATA[
arXiv:2505.21034v1 Announce Type: new 
Abstract: Bayesian optimization (BO) is a powerful class of algorithms for optimizing expensive black-box functions, but designing effective BO algorithms remains a manual, expertise-driven task. Recent advancements in Large Language Models (LLMs) have opened new avenues for automating scientific discovery, including the automatic design of optimization algorithms. While prior work has used LLMs within optimization loops or to generate non-BO algorithms, we tackle a new challenge: Using LLMs to automatically generate full BO algorithm code. Our framework uses an evolution strategy to guide an LLM in generating Python code that preserves the key components of BO algorithms: An initial design, a surrogate model, and an acquisition function. The LLM is prompted to produce multiple candidate algorithms, which are evaluated on the established Black-Box Optimization Benchmarking (BBOB) test suite from the COmparing Continuous Optimizers (COCO) platform. Based on their performance, top candidates are selected, combined, and mutated via controlled prompt variations, enabling iterative refinement. Despite no additional fine-tuning, the LLM-generated algorithms outperform state-of-the-art BO baselines in 19 (out of 24) BBOB functions in dimension 5 and generalize well to higher dimensions, and different tasks (from the Bayesmark framework). This work demonstrates that LLMs can serve as algorithmic co-designers, offering a new paradigm for automating BO development and accelerating the discovery of novel algorithmic combinations. The source code is provided at https://github.com/Ewendawi/LLaMEA-BO.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable and adaptive prediction bands with kernel sum-of-squares</title>
<link>https://arxiv.org/abs/2505.21039</link>
<guid>https://arxiv.org/abs/2505.21039</guid>
<content:encoded><![CDATA[
arXiv:2505.21039v1 Announce Type: new 
Abstract: Conformal Prediction (CP) is a popular framework for constructing prediction bands with valid coverage in finite samples, while being free of any distributional assumption. A well-known limitation of conformal prediction is the lack of adaptivity, although several works introduced practically efficient alternate procedures. In this work, we build upon recent ideas that rely on recasting the CP problem as a statistical learning problem, directly targeting coverage and adaptivity. This statistical learning problem is based on reproducible kernel Hilbert spaces (RKHS) and kernel sum-of-squares (SoS) methods. First, we extend previous results with a general representer theorem and exhibit the dual formulation of the learning problem. Crucially, such dual formulation can be solved efficiently by accelerated gradient methods with several hundreds or thousands of samples, unlike previous strategies based on off-the-shelf semidefinite programming algorithms. Second, we introduce a new hyperparameter tuning strategy tailored specifically to target adaptivity through bounds on test-conditional coverage. This strategy, based on the Hilbert-Schmidt Independence Criterion (HSIC), is introduced here to tune kernel lengthscales in our framework, but has broader applicability since it could be used in any CP algorithm where the score function is learned. Finally, extensive experiments are conducted to show how our method compares to related work. All figures can be reproduced with the accompanying code.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A domain adaptation neural network for digital twin-supported fault diagnosis</title>
<link>https://arxiv.org/abs/2505.21046</link>
<guid>https://arxiv.org/abs/2505.21046</guid>
<content:encoded><![CDATA[
arXiv:2505.21046v1 Announce Type: new 
Abstract: Digital twins offer a promising solution to the lack of sufficient labeled data in deep learning-based fault diagnosis by generating simulated data for model training. However, discrepancies between simulation and real-world systems can lead to a significant drop in performance when models are applied in real scenarios. To address this issue, we propose a fault diagnosis framework based on Domain-Adversarial Neural Networks (DANN), which enables knowledge transfer from simulated (source domain) to real-world (target domain) data. We evaluate the proposed framework using a publicly available robotics fault diagnosis dataset, which includes 3,600 sequences generated by a digital twin model and 90 real sequences collected from physical systems. The DANN method is compared with commonly used lightweight deep learning models such as CNN, TCN, Transformer, and LSTM. Experimental results show that incorporating domain adaptation significantly improves the diagnostic performance. For example, applying DANN to a baseline CNN model improves its accuracy from 70.00% to 80.22% on real-world test data, demonstrating the effectiveness of domain adaptation in bridging the sim-to-real gap.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity</title>
<link>https://arxiv.org/abs/2505.21073</link>
<guid>https://arxiv.org/abs/2505.21073</guid>
<content:encoded><![CDATA[
arXiv:2505.21073v1 Announce Type: new 
Abstract: Trees and the associated shortest-path tree metrics provide a powerful framework for representing hierarchical and combinatorial structures in data. Given an arbitrary metric space, its deviation from a tree metric can be quantified by Gromov's $\delta$-hyperbolicity. Nonetheless, designing algorithms that bridge an arbitrary metric to its closest tree metric is still a vivid subject of interest, as most common approaches are either heuristical and lack guarantees, or perform moderately well. In this work, we introduce a novel differentiable optimization framework, coined DeltaZero, that solves this problem. Our method leverages a smooth surrogate for Gromov's $\delta$-hyperbolicity which enables a gradient-based optimization, with a tractable complexity. The corresponding optimization procedure is derived from a problem with better worst case guarantees than existing bounds, and is justified statistically. Experiments on synthetic and real-world datasets demonstrate that our method consistently achieves state-of-the-art distortion.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling</title>
<link>https://arxiv.org/abs/2505.21074</link>
<guid>https://arxiv.org/abs/2505.21074</guid>
<content:encoded><![CDATA[
arXiv:2505.21074v1 Announce Type: new 
Abstract: Text-to-image (T2I) models raise ethical and safety concerns due to their potential to generate inappropriate or harmful images. Evaluating these models' security through red-teaming is vital, yet white-box approaches are limited by their need for internal access, complicating their use with closed-source models. Moreover, existing black-box methods often assume knowledge about the model's specific defense mechanisms, limiting their utility in real-world commercial API scenarios. A significant challenge is how to evade unknown and diverse defense mechanisms. To overcome this difficulty, we propose a novel Rule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively employs LLM to modify prompts to query and leverages feedback from T2I systems for fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a prior, enabling the LLM to dynamically adapt to unknown defense mechanisms. Given that the feedback is often labeled and coarse-grained, making it difficult to utilize directly, we further propose rule-based preference modeling, which employs a set of rules to evaluate desired or undesired feedback, facilitating finer-grained control over the LLM's dynamic adaptation process. Extensive experiments on nineteen T2I systems with varied safety mechanisms, three online commercial API services, and T2V models verify the superiority and practicality of our approach.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Large Language Model Inference with Neural Block Linearization</title>
<link>https://arxiv.org/abs/2505.21077</link>
<guid>https://arxiv.org/abs/2505.21077</guid>
<content:encoded><![CDATA[
arXiv:2505.21077v1 Announce Type: new 
Abstract: The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce Neural Block Linearization (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Impossible Tuning and Lipschitz-Adaptive Universal Online Learning with Gradient Variations</title>
<link>https://arxiv.org/abs/2505.21095</link>
<guid>https://arxiv.org/abs/2505.21095</guid>
<content:encoded><![CDATA[
arXiv:2505.21095v1 Announce Type: new 
Abstract: A central goal in online learning is to achieve adaptivity to unknown problem characteristics, such as environmental changes captured by gradient variation (GV), function curvature (universal online learning, UOL), and gradient scales (Lipschitz adaptivity, LA). Simultaneously achieving these with optimal performance is a major challenge, partly due to limitations in algorithms for prediction with expert advice. These algorithms often serve as meta-algorithms in online ensemble frameworks, and their sub-optimality hinders overall UOL performance. Specifically, existing algorithms addressing the ``impossible tuning'' issue incur an excess $\sqrt{\log T}$ factor in their regret bound compared to the lower bound. To solve this problem, we propose a novel optimistic online mirror descent algorithm with an auxiliary initial round using large learning rates. This design enables a refined analysis where a generated negative term cancels the gap-related factor, resolving the impossible tuning issue up to $\log\log T$ factors. Leveraging our improved algorithm as a meta-algorithm, we develop the first UOL algorithm that simultaneously achieves state-of-the-art GV bounds and LA under standard assumptions. Our UOL result overcomes key limitations of prior works, notably resolving the conflict between LA mechanisms and regret analysis for GV bounds -- an open problem highlighted by Xie et al.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance</title>
<link>https://arxiv.org/abs/2505.21101</link>
<guid>https://arxiv.org/abs/2505.21101</guid>
<content:encoded><![CDATA[
arXiv:2505.21101v1 Announce Type: new 
Abstract: Classifier-Free Guidance (CFG) is a widely used technique for improving conditional diffusion models by linearly combining the outputs of conditional and unconditional denoisers. While CFG enhances visual quality and improves alignment with prompts, it often reduces sample diversity, leading to a challenging trade-off between quality and diversity. To address this issue, we make two key contributions. First, CFG generally does not correspond to a well-defined denoising diffusion model (DDM). In particular, contrary to common intuition, CFG does not yield samples from the target distribution associated with the limiting CFG score as the noise level approaches zero -- where the data distribution is tilted by a power $w \gt 1$ of the conditional distribution. We identify the missing component: a R\'enyi divergence term that acts as a repulsive force and is required to correct CFG and render it consistent with a proper DDM. Our analysis shows that this correction term vanishes in the low-noise limit. Second, motivated by this insight, we propose a Gibbs-like sampling procedure to draw samples from the desired tilted distribution. This method starts with an initial sample from the conditional diffusion model without CFG and iteratively refines it, preserving diversity while progressively enhancing sample quality. We evaluate our approach on both image and text-to-audio generation tasks, demonstrating substantial improvements over CFG across all considered metrics. The code is available at https://github.com/yazidjanati/cfgig
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Value-Function Uncertainties</title>
<link>https://arxiv.org/abs/2505.21119</link>
<guid>https://arxiv.org/abs/2505.21119</guid>
<content:encoded><![CDATA[
arXiv:2505.21119v1 Announce Type: new 
Abstract: Estimating epistemic uncertainty in value functions is a crucial challenge for many aspects of reinforcement learning (RL), including efficient exploration, safe decision-making, and offline RL. While deep ensembles provide a robust method for quantifying value uncertainty, they come with significant computational overhead. Single-model methods, while computationally favorable, often rely on heuristics and typically require additional propagation mechanisms for myopic uncertainty estimates. In this work we introduce universal value-function uncertainties (UVU), which, similar in spirit to random network distillation (RND), quantify uncertainty as squared prediction errors between an online learner and a fixed, randomly initialized target network. Unlike RND, UVU errors reflect policy-conditional value uncertainty, incorporating the future uncertainties any given policy may encounter. This is due to the training procedure employed in UVU: the online network is trained using temporal difference learning with a synthetic reward derived from the fixed, randomly initialized target network. We provide an extensive theoretical analysis of our approach using neural tangent kernel (NTK) theory and show that in the limit of infinite network width, UVU errors are exactly equivalent to the variance of an ensemble of independent universal value functions. Empirically, we show that UVU achieves equal performance to large ensembles on challenging multi-task offline RL settings, while offering simplicity and substantial computational savings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Computation-Aware Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.21133</link>
<guid>https://arxiv.org/abs/2505.21133</guid>
<content:encoded><![CDATA[
arXiv:2505.21133v1 Announce Type: new 
Abstract: Gaussian processes (GPs) are widely used for regression and optimization tasks such as Bayesian optimization (BO) due to their expressiveness and principled uncertainty estimates. However, in settings with large datasets corrupted by outliers, standard GPs and their sparse approximations struggle with computational tractability and robustness. We introduce Robust Computation-aware Gaussian Process (RCaGP), a novel GP model that jointly addresses these challenges by combining a principled treatment of approximation-induced uncertainty with robust generalized Bayesian updating. The key insight is that robustness and approximation-awareness are not orthogonal but intertwined: approximations can exacerbate the impact of outliers, and mitigating one without the other is insufficient. Unlike previous work that focuses narrowly on either robustness or approximation quality, RCaGP combines both in a principled and scalable framework, thus effectively managing both outliers and computational uncertainties introduced by approximations such as low-rank matrix multiplications. Our model ensures more conservative and reliable uncertainty estimates, a property we rigorously demonstrate. Additionally, we establish a robustness property and show that the mean function is key to preserving it, motivating a tailored model selection scheme for robust mean functions. Empirical results confirm that solving these challenges jointly leads to superior performance across both clean and outlier-contaminated settings, both on regression and high-throughput Bayesian optimization benchmarks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Single Index Models with Diffusion Priors</title>
<link>https://arxiv.org/abs/2505.21135</link>
<guid>https://arxiv.org/abs/2505.21135</guid>
<content:encoded><![CDATA[
arXiv:2505.21135v1 Announce Type: new 
Abstract: Diffusion models (DMs) have demonstrated remarkable ability to generate diverse and high-quality images by efficiently modeling complex data distributions. They have also been explored as powerful generative priors for signal recovery, resulting in a substantial improvement in the quality of reconstructed signals. However, existing research on signal recovery with diffusion models either focuses on specific reconstruction problems or is unable to handle nonlinear measurement models with discontinuous or unknown link functions. In this work, we focus on using DMs to achieve accurate recovery from semi-parametric single index models, which encompass a variety of popular nonlinear models that may have {\em discontinuous} and {\em unknown} link functions. We propose an efficient reconstruction method that only requires one round of unconditional sampling and (partial) inversion of DMs. Theoretical analysis on the effectiveness of the proposed methods has been established under appropriate conditions. We perform numerical experiments on image datasets for different nonlinear measurement models. We observe that compared to competing methods, our approach can yield more accurate reconstructions while utilizing significantly fewer neural function evaluations.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SageAttention2++: A More Efficient Implementation of SageAttention2</title>
<link>https://arxiv.org/abs/2505.21136</link>
<guid>https://arxiv.org/abs/2505.21136</guid>
<content:encoded><![CDATA[
arXiv:2505.21136v1 Announce Type: new 
Abstract: The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at https://github.com/thu-ml/SageAttention.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs</title>
<link>https://arxiv.org/abs/2505.21140</link>
<guid>https://arxiv.org/abs/2505.21140</guid>
<content:encoded><![CDATA[
arXiv:2505.21140v1 Announce Type: new 
Abstract: Heterogeneous graph neural networks (HGNNs) have recently drawn increasing attention for modeling complex multi-relational data in domains such as recommendation, finance, and social networks. While existing research has been largely focused on enhancing HGNNs' predictive performance, their robustness and security, especially under backdoor attacks, remain underexplored. In this paper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework for node classification tasks on heterogeneous graphs. HeteroBA inserts carefully crafted trigger nodes with realistic features and targeted structural connections, leveraging attention-based and clustering-based strategies to select influential auxiliary nodes for effective trigger propagation, thereby causing the model to misclassify specific nodes into a target label while maintaining accuracy on clean data. Experimental results on three datasets and various HGNN architectures demonstrate that HeteroBA achieves high attack success rates with minimal impact on the clean accuracy. Our method sheds light on potential vulnerabilities in HGNNs and calls for more robust defenses against backdoor threats in multi-relational graph scenarios.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Predicting Phishing Websites Using Support Vector Machine and MultiClass Classification Based on Association Rule Techniques</title>
<link>https://arxiv.org/abs/2505.21141</link>
<guid>https://arxiv.org/abs/2505.21141</guid>
<content:encoded><![CDATA[
arXiv:2505.21141v1 Announce Type: new 
Abstract: Phishing is a semantic attack which targets the user rather than the computer. It is a new Internet crime in comparison with other forms such as virus and hacking. Considering the damage phishing websites has caused to various economies by collapsing organizations, stealing information and financial diversion, various researchers have embarked on different ways of detecting phishing websites but there has been no agreement about the best algorithm to be used for prediction. This study is interested in integrating the strengths of two algorithms, Support Vector Machines (SVM) and Multi-Class Classification Rules based on Association Rules (MCAR) to establish a strong and better means of predicting phishing websites. A total of 11,056 websites were used from both PhishTank and yahoo directory to verify the effectiveness of this approach. Feature extraction and rules generation were done by the MCAR technique; classification and prediction were done by SVM technique. The result showed that the technique achieved 98.30% classification accuracy with a computation time of 2205.33s with minimum error rate. It showed a total of 98% Area under the Curve (AUC) which showed the proportion of accuracy in classifying phishing websites. The model showed 82.84% variance in the prediction of phishing websites based on the coefficient of determination. The use of two techniques together in detecting phishing websites produced a more accurate result as it combined the strength of both techniques respectively. This research work centralized on this advantage by building a hybrid of two techniques to help produce a more accurate result.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score</title>
<link>https://arxiv.org/abs/2505.21147</link>
<guid>https://arxiv.org/abs/2505.21147</guid>
<content:encoded><![CDATA[
arXiv:2505.21147v1 Announce Type: new 
Abstract: Conformal prediction (CP) is a powerful framework for uncertainty quantification, providing prediction sets with coverage guarantees when calibrated on sufficient labeled data. However, in real-world applications where labeled data is often limited, standard CP can lead to coverage deviation and output overly large prediction sets. In this paper, we extend CP to the semi-supervised setting and propose SemiCP, leveraging both labeled data and unlabeled data for calibration. Specifically, we introduce a novel nonconformity score function, NNM, designed for unlabeled data. This function selects labeled data with similar pseudo-label scores to estimate nonconformity scores, integrating them into the calibration process to overcome sample size limitations. We theoretically demonstrate that, under mild assumptions, SemiCP provide asymptotically coverage guarantee for prediction sets. Extensive experiments further validate that our approach effectively reduces instability and inefficiency under limited calibration data, can be adapted to conditional coverage settings, and integrates seamlessly with existing CP methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STEB: In Search of the Best Evaluation Approach for Synthetic Time Series</title>
<link>https://arxiv.org/abs/2505.21160</link>
<guid>https://arxiv.org/abs/2505.21160</guid>
<content:encoded><![CDATA[
arXiv:2505.21160v1 Announce Type: new 
Abstract: The growing need for synthetic time series, due to data augmentation or privacy regulations, has led to numerous generative models, frameworks, and evaluation measures alike. Objectively comparing these measures on a large scale remains an open challenge. We propose the Synthetic Time series Evaluation Benchmark (STEB) -- the first benchmark framework that enables comprehensive and interpretable automated comparisons of synthetic time series evaluation measures. Using 10 diverse datasets, randomness injection, and 13 configurable data transformations, STEB computes indicators for measure reliability and score consistency. It tracks running time, test errors, and features sequential and parallel modes of operation. In our experiments, we determine a ranking of 41 measures from literature and confirm that the choice of upstream time series embedding heavily impacts the final score.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Deep Learning for Speech Data</title>
<link>https://arxiv.org/abs/2505.21173</link>
<guid>https://arxiv.org/abs/2505.21173</guid>
<content:encoded><![CDATA[
arXiv:2505.21173v1 Announce Type: new 
Abstract: Topological data analysis (TDA) offers novel mathematical tools for deep learning. Inspired by Carlsson et al., this study designs topology-aware convolutional kernels that significantly improve speech recognition networks. Theoretically, by investigating orthogonal group actions on kernels, we establish a fiber-bundle decomposition of matrix spaces, enabling new filter generation methods. Practically, our proposed Orthogonal Feature (OF) layer achieves superior performance in phoneme recognition, particularly in low-noise scenarios, while demonstrating cross-domain adaptability. This work reveals TDA's potential in neural network optimization, opening new avenues for mathematics-deep learning interdisciplinary studies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent label distribution grid representation for modeling uncertainty</title>
<link>https://arxiv.org/abs/2505.21180</link>
<guid>https://arxiv.org/abs/2505.21180</guid>
<content:encoded><![CDATA[
arXiv:2505.21180v1 Announce Type: new 
Abstract: Although \textbf{L}abel \textbf{D}istribution \textbf{L}earning (LDL) has promising representation capabilities for characterizing the polysemy of an instance, the complexity and high cost of the label distribution annotation lead to inexact in the construction of the label space. The existence of a large number of inexact labels generates a label space with uncertainty, which misleads the LDL algorithm to yield incorrect decisions. To alleviate this problem, we model the uncertainty of label distributions by constructing a \textbf{L}atent \textbf{L}abel \textbf{D}istribution \textbf{G}rid (LLDG) to form a low-noise representation space. Specifically, we first construct a label correlation matrix based on the differences between labels, and then expand each value of the matrix into a vector that obeys a Gaussian distribution, thus building a LLDG to model the uncertainty of the label space. Finally, the LLDG is reconstructed by the LLDG-Mixer to generate an accurate label distribution. Note that we enforce a customized low-rank scheme on this grid, which assumes that the label relations may be noisy and it needs to perform noise-reduction with the help of a Tucker reconstruction technique. Furthermore, we attempt to evaluate the effectiveness of the LLDG by considering its generation as an upstream task to achieve the classification of the objects. Extensive experimental results show that our approach performs competitively on several benchmarks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations</title>
<link>https://arxiv.org/abs/2505.21182</link>
<guid>https://arxiv.org/abs/2505.21182</guid>
<content:encoded><![CDATA[
arXiv:2505.21182v1 Announce Type: new 
Abstract: Offline imitation learning typically learns from expert and unlabeled demonstrations, yet often overlooks the valuable signal in explicitly undesirable behaviors. In this work, we study offline imitation learning from contrasting behaviors, where the dataset contains both expert and undesirable demonstrations. We propose a novel formulation that optimizes a difference of KL divergences over the state-action visitation distributions of expert and undesirable (or bad) data. Although the resulting objective is a DC (Difference-of-Convex) program, we prove that it becomes convex when expert demonstrations outweigh undesirable demonstrations, enabling a practical and stable non-adversarial training objective. Our method avoids adversarial training and handles both positive and negative demonstrations in a unified framework. Extensive experiments on standard offline imitation learning benchmarks demonstrate that our approach consistently outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing</title>
<link>https://arxiv.org/abs/2505.21184</link>
<guid>https://arxiv.org/abs/2505.21184</guid>
<content:encoded><![CDATA[
arXiv:2505.21184v1 Announce Type: new 
Abstract: To construct responsible and secure AI applications, harmful information data is widely utilized for adversarial testing and the development of safeguards. Existing studies mainly leverage Large Language Models (LLMs) to synthesize data to obtain high-quality task datasets at scale, thereby avoiding costly human annotation. However, limited by the safety alignment mechanisms of LLMs, the synthesis of harmful data still faces challenges in generation reliability and content diversity. In this study, we propose a novel harmful information synthesis framework, PoisonSwarm, which applies the model crowdsourcing strategy to generate diverse harmful data while maintaining a high success rate. Specifically, we generate abundant benign data as the based templates in a counterfactual manner. Subsequently, we decompose each based template into multiple semantic units and perform unit-by-unit toxification and final refinement through dynamic model switching, thus ensuring the success of synthesis. Experimental results demonstrate that PoisonSwarm achieves state-of-the-art performance in synthesizing different categories of harmful data with high scalability and diversity.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crop recommendation with machine learning: leveraging environmental and economic factors for optimal crop selection</title>
<link>https://arxiv.org/abs/2505.21201</link>
<guid>https://arxiv.org/abs/2505.21201</guid>
<content:encoded><![CDATA[
arXiv:2505.21201v1 Announce Type: new 
Abstract: Agriculture constitutes a primary source of food production, economic growth and employment in India, but the sector is confronted with low farm productivity and yields aggravated by increased pressure on natural resources and adverse climate change variability. Efforts involving green revolution, land irrigations, improved seeds and organic farming have yielded suboptimal outcomes. The adoption of computational tools like crop recommendation systems offers a new way to provide insights and help farmers tackle low productivity. However, most agricultural recommendation systems in India focus narrowly on environmental factors and regions, limiting accurate predictions of high-yield, profitable crops. This study uses environmental and economic factors with 19 crops across 15 states to develop and evaluate Random Forest and SVM models using 10-fold Cross Validation, Time-series Split, and Lag Variables. The 10-fold cross validation showed high accuracy (RF: 99.96%, SVM: 94.71%) but raised overfitting concerns. Introducing temporal order, better reflecting real-world conditions, reduced performance (RF: 78.55%, SVM: 71.18%) in the Time-series Split.To further increase the model accuracy while maintaining the temporal order, the Lag Variables approach was employed, which resulted in improved performance (RF: 83.62%, SVM: 74.38%) compared to the 10-fold cross validation approach. Overall, the models in the Time-series Split and Lag Variable Approaches offer practical insights by handling temporal dependencies and enhancing its adaptability to changing agricultural conditions over time. Consequently, the study shows the Random Forest model developed based on the Lag Variables as the most preferred algorithm for optimal crop recommendation in the Indian context.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing hybrid mechanistic and data-driven personalized prediction models for platelet dynamics</title>
<link>https://arxiv.org/abs/2505.21204</link>
<guid>https://arxiv.org/abs/2505.21204</guid>
<content:encoded><![CDATA[
arXiv:2505.21204v1 Announce Type: new 
Abstract: Hematotoxicity, drug-induced damage to the blood-forming system, is a frequent side effect of cytotoxic chemotherapy and poses a significant challenge in clinical practice due to its high inter-patient variability and limited predictability. Current mechanistic models often struggle to accurately forecast outcomes for patients with irregular or atypical trajectories. In this study, we develop and compare hybrid mechanistic and data-driven approaches for individualized time series modeling of platelet counts during chemotherapy. We consider hybrid models that combine mechanistic models with neural networks, known as universal differential equations. As a purely data-driven alternative, we utilize a nonlinear autoregressive exogenous model using gated recurrent units as the underlying architecture. These models are evaluated across a range of real patient scenarios, varying in data availability and sparsity, to assess predictive performance. Our findings demonstrate that data-driven methods, when provided with sufficient data, significantly improve prediction accuracy, particularly for high-risk patients with irregular platelet dynamics. This highlights the potential of data-driven approaches in enhancing clinical decision-making. In contrast, hybrid and mechanistic models are superior in scenarios with limited or sparse data. The proposed modeling and comparison framework is generalizable and could be extended to predict other treatment-related toxicities, offering broad applicability in personalized medicine.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection</title>
<link>https://arxiv.org/abs/2505.21219</link>
<guid>https://arxiv.org/abs/2505.21219</guid>
<content:encoded><![CDATA[
arXiv:2505.21219v1 Announce Type: new 
Abstract: In cross-silo Federated Learning (FL), client selection is critical to ensure high model performance, yet it remains challenging due to data quality decompensation, budget constraints, and incentive compatibility. As training progresses, these factors exacerbate client heterogeneity and degrade global performance. Most existing approaches treat these challenges in isolation, making jointly optimizing multiple factors difficult. To address this, we propose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a unified framework integrating dynamic bidding, reputation modeling, and cost-aware selection. Clients submit bids based on their perceived data quality, and their contributions are evaluated using Shapley values to quantify their marginal impact on the global model. A reputation system, inspired by prospect theory, captures historical performance while penalizing inconsistency. The client selection problem is formulated as a 0-1 integer program that maximizes reputation-weighted utility under budget constraints. Experiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that SBRO-FL improves accuracy, convergence speed, and robustness, even in adversarial and low-bid interference scenarios. Our results highlight the importance of balancing data reliability, incentive compatibility, and cost efficiency to enable scalable and trustworthy FL deployments.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Do More Experts Fail? A Theoretical Analysis of Model Merging</title>
<link>https://arxiv.org/abs/2505.21226</link>
<guid>https://arxiv.org/abs/2505.21226</guid>
<content:encoded><![CDATA[
arXiv:2505.21226v1 Announce Type: new 
Abstract: Model merging dramatically reduces storage and computational resources by combining multiple expert models into a single multi-task model. Although recent model merging methods have shown promising results, they struggle to maintain performance gains as the number of merged models increases. In this paper, we investigate the key obstacles that limit the scalability of model merging when integrating a large number of expert models. First, we prove that there is an upper bound on model merging. Further theoretical analysis reveals that the limited effective parameter space imposes a strict constraint on the number of models that can be successfully merged. Gaussian Width shows that the marginal benefit of merging additional models diminishes according to a strictly concave function. This implies that the effective parameter space becomes rapidly saturated as the number of merged models increases. Furthermore, using Approximate Kinematics Theory, we prove the existence of a unique optimal threshold beyond which adding more models does not yield significant performance improvements. At the same time, we introduce a straightforward Reparameterized Heavy-Tailed method (RHT) to extend the coverage of the merged model, thereby enhancing its performance. Empirical results on 12 benchmarks, including both knowledge-intensive and general-purpose tasks, validate our theoretical analysis. We believe that these results spark further research beyond the current scope of model merging. The source code is in the anonymous Github repository https://github.com/wzj1718/ModelMergingAnalysis.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies</title>
<link>https://arxiv.org/abs/2505.21236</link>
<guid>https://arxiv.org/abs/2505.21236</guid>
<content:encoded><![CDATA[
arXiv:2505.21236v1 Announce Type: new 
Abstract: Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at https://sites.google.com/view/inf-marl.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design</title>
<link>https://arxiv.org/abs/2505.21241</link>
<guid>https://arxiv.org/abs/2505.21241</guid>
<content:encoded><![CDATA[
arXiv:2505.21241v1 Announce Type: new 
Abstract: Protein binder design has been transformed by hallucination-based methods that optimize structure prediction confidence metrics, such as the interface predicted TM-score (ipTM), via backpropagation. However, these metrics do not reflect the statistical likelihood of a binder-target complex under the learned distribution and yield sparse gradients for optimization. In this work, we propose a method to extract such likelihoods from structure predictors by reinterpreting their confidence outputs as an energy-based model (EBM). By leveraging the Joint Energy-based Modeling (JEM) framework, we introduce pTMEnergy, a statistical energy function derived from predicted inter-residue error distributions. We incorporate pTMEnergy into BindEnergyCraft (BECraft), a design pipeline that maintains the same optimization framework as BindCraft but replaces ipTM with our energy-based objective. BECraft outperforms BindCraft, RFDiffusion, and ESM3 across multiple challenging targets, achieving higher in silico binder success rates while reducing structural clashes. Furthermore, pTMEnergy establishes a new state-of-the-art in structure-based virtual screening tasks for miniprotein and RNA aptamer binders.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework</title>
<link>https://arxiv.org/abs/2505.21251</link>
<guid>https://arxiv.org/abs/2505.21251</guid>
<content:encoded><![CDATA[
arXiv:2505.21251v1 Announce Type: new 
Abstract: We introduce copresheaf topological neural networks (CTNNs), a powerful and unifying framework that encapsulates a wide spectrum of deep learning architectures, designed to operate on structured data: including images, point clouds, graphs, meshes, and topological manifolds. While deep learning has profoundly impacted domains ranging from digital assistants to autonomous systems, the principled design of neural architectures tailored to specific tasks and data types remains one of the field's most persistent open challenges. CTNNs address this gap by grounding model design in the language of copresheaves, a concept from algebraic topology that generalizes and subsumes most practical deep learning models in use today. This abstract yet constructive formulation yields a rich design space from which theoretically sound and practically effective solutions can be derived to tackle core challenges in representation learning: long-range dependencies, oversmoothing, heterophily, and non-Euclidean domains. Our empirical results on structured data benchmarks demonstrate that CTNNs consistently outperform conventional baselines, particularly in tasks requiring hierarchical or localized sensitivity. These results underscore CTNNs as a principled, multi-scale foundation for the next generation of deep learning architectures.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learnable Kernel Density Estimation for Graphs</title>
<link>https://arxiv.org/abs/2505.21285</link>
<guid>https://arxiv.org/abs/2505.21285</guid>
<content:encoded><![CDATA[
arXiv:2505.21285v1 Announce Type: new 
Abstract: This work proposes a framework LGKDE that learns kernel density estimation for graphs. The key challenge in graph density estimation lies in effectively capturing both structural patterns and semantic variations while maintaining theoretical guarantees. Combining graph kernels and kernel density estimation (KDE) is a standard approach to graph density estimation, but has unsatisfactory performance due to the handcrafted and fixed features of kernels. Our method LGKDE leverages graph neural networks to represent each graph as a discrete distribution and utilizes maximum mean discrepancy to learn the graph metric for multi-scale KDE, where all parameters are learned by maximizing the density of graphs relative to the density of their well-designed perturbed counterparts. The perturbations are conducted on both node features and graph spectra, which helps better characterize the boundary of normal density regions. Theoretically, we establish consistency and convergence guarantees for LGKDE, including bounds on the mean integrated squared error, robustness, and complexity. We validate LGKDE by demonstrating its effectiveness in recovering the underlying density of synthetic graph distributions and applying it to graph anomaly detection across diverse benchmark datasets. Extensive empirical evaluation shows that LGKDE demonstrates superior performance compared to state-of-the-art baselines on most benchmark datasets.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSAT: Graph Structure Attention Networks</title>
<link>https://arxiv.org/abs/2505.21288</link>
<guid>https://arxiv.org/abs/2505.21288</guid>
<content:encoded><![CDATA[
arXiv:2505.21288v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as a powerful tool for processing data represented in graph structures, achieving remarkable success across a wide range of applications. However, to further improve the performance on graph classification benchmarks, structural representation of each node that encodes rich local topological information in the neighbourhood of nodes is an important type of feature that is often overlooked in the modeling. The consequence of neglecting the structural information has resulted high number of layers to connect messages from distant nodes which by itself produces other problems such as oversmoothing. In the present paper, we leverage these structural information that are modeled by anonymous random walks (ARWs) and introduce graph structure attention network (GSAT) which is a generalization of graph attention network(GAT) to integrate the original attribute and the structural representation to enforce the model to automatically find patterns for attending to different edges in the node neighbourhood to enrich graph representation. Our experiments show GSAT slightly improves SOTA on some graph classification benchmarks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.21289</link>
<guid>https://arxiv.org/abs/2505.21289</guid>
<content:encoded><![CDATA[
arXiv:2505.21289v1 Announce Type: new 
Abstract: Large pre-trained models are commonly adapted to downstream tasks using parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA), which injects small trainable low-rank matrices instead of updating all weights. While LoRA dramatically reduces trainable parameters with little overhead, it can still underperform full fine-tuning in accuracy and often converges more slowly. We introduce LoFT, a novel low-rank adaptation method that behaves like full fine-tuning by aligning the optimizer's internal dynamics with those of updating all model weights. LoFT not only learns weight updates in a low-rank subspace (like LoRA) but also properly projects the optimizer's first and second moments (Adam's momentum and variance) into the same subspace, mirroring full-model updates. By aligning the low-rank update itself with the full update, LoFT eliminates the need for tuning extra hyperparameters, e.g., LoRA scaling factor $\alpha$. Empirically, this approach substantially narrows the performance gap between adapter-based tuning and full fine-tuning and consistently outperforms standard LoRA-style methods, all without increasing inference cost.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cross Modal Knowledge Distillation &amp; Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features</title>
<link>https://arxiv.org/abs/2505.21317</link>
<guid>https://arxiv.org/abs/2505.21317</guid>
<content:encoded><![CDATA[
arXiv:2505.21317v1 Announce Type: new 
Abstract: Understanding cellular responses to stimuli is crucial for biological discovery and drug development. Transcriptomics provides interpretable, gene-level insights, while microscopy imaging offers rich predictive features but is harder to interpret. Weakly paired datasets, where samples share biological states, enable multimodal learning but are scarce, limiting their utility for training and multimodal inference. We propose a framework to enhance transcriptomics by distilling knowledge from microscopy images. Using weakly paired data, our method aligns and binds modalities, enriching gene expression representations with morphological information. To address data scarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal distillation using pretrained foundation models, achieving state-of-the-art results, and (2) PEA (Perturbation Embedding Augmentation), a novel augmentation technique that enhances transcriptomics data while preserving inherent biological information. These strategies improve the predictive power and retain the interpretability of transcriptomics, enabling rich unimodal representations for complex biological tasks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bencher: Simple and Reproducible Benchmarking for Black-Box Optimization</title>
<link>https://arxiv.org/abs/2505.21321</link>
<guid>https://arxiv.org/abs/2505.21321</guid>
<content:encoded><![CDATA[
arXiv:2505.21321v1 Announce Type: new 
Abstract: We present Bencher, a modular benchmarking framework for black-box optimization that fundamentally decouples benchmark execution from optimization logic. Unlike prior suites that focus on combining many benchmarks in a single project, Bencher introduces a clean abstraction boundary: each benchmark is isolated in its own virtual Python environment and accessed via a unified, version-agnostic remote procedure call (RPC) interface. This design eliminates dependency conflicts and simplifies the integration of diverse, real-world benchmarks, which often have complex and conflicting software requirements. Bencher can be deployed locally or remotely via Docker or on high-performance computing (HPC) clusters via Singularity, providing a containerized, reproducible runtime for any benchmark. Its lightweight client requires minimal setup and supports drop-in evaluation of 80 benchmarks across continuous, categorical, and binary domains.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UGCE: User-Guided Incremental Counterfactual Exploration</title>
<link>https://arxiv.org/abs/2505.21330</link>
<guid>https://arxiv.org/abs/2505.21330</guid>
<content:encoded><![CDATA[
arXiv:2505.21330v1 Announce Type: new 
Abstract: Counterfactual explanations (CFEs) are a popular approach for interpreting machine learning predictions by identifying minimal feature changes that alter model outputs. However, in real-world settings, users often refine feasibility constraints over time, requiring counterfactual generation to adapt dynamically. Existing methods fail to support such iterative updates, instead recomputing explanations from scratch with each change, an inefficient and rigid approach. We propose User-Guided Incremental Counterfactual Exploration (UGCE), a genetic algorithm-based framework that incrementally updates counterfactuals in response to evolving user constraints. Experimental results across five benchmark datasets demonstrate that UGCE significantly improves computational efficiency while maintaining high-quality solutions compared to a static, non-incremental approach. Our evaluation further shows that UGCE supports stable performance under varying constraint sequences, benefits from an efficient warm-start strategy, and reveals how different constraint types may affect search behavior.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Learning in the Gaussian Single Index Model</title>
<link>https://arxiv.org/abs/2505.21336</link>
<guid>https://arxiv.org/abs/2505.21336</guid>
<content:encoded><![CDATA[
arXiv:2505.21336v1 Announce Type: new 
Abstract: We consider the problem of jointly learning a one-dimensional projection and a univariate function in high-dimensional Gaussian models. Specifically, we study predictors of the form $f(x)=\varphi^\star(\langle w^\star, x \rangle)$, where both the direction $w^\star \in \mathcal{S}_{d-1}$, the sphere of $\mathbb{R}^d$, and the function $\varphi^\star: \mathbb{R} \to \mathbb{R}$ are learned from Gaussian data. This setting captures a fundamental non-convex problem at the intersection of representation learning and nonlinear regression. We analyze the gradient flow dynamics of a natural alternating scheme and prove convergence, with a rate controlled by the information exponent reflecting the \textit{Gaussian regularity} of the function $\varphi^\star$. Strikingly, our analysis shows that convergence still occurs even when the initial direction is negatively correlated with the target. On the practical side, we demonstrate that such joint learning can be effectively implemented using a Reproducing Kernel Hilbert Space (RKHS) adapted to the structure of the problem, enabling efficient and flexible estimation of the univariate function. Our results offer both theoretical insight and practical methodology for learning low-dimensional structure in high-dimensional settings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction</title>
<link>https://arxiv.org/abs/2505.21339</link>
<guid>https://arxiv.org/abs/2505.21339</guid>
<content:encoded><![CDATA[
arXiv:2505.21339v1 Announce Type: new 
Abstract: Suffix prediction of business processes forecasts the remaining sequence of events until process completion. Current approaches focus on predicting a single, most likely suffix. However, if the future course of a process is exposed to uncertainty or has high variability, the expressiveness of a single suffix prediction can be limited. To address this limitation, we propose probabilistic suffix prediction, a novel approach that approximates a probability distribution of suffixes. The proposed approach is based on an Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC) suffix sampling algorithm. We capture epistemic uncertainties via MC dropout and aleatoric uncertainties as learned loss attenuation. This technical report provides a detailed evaluation of the U-ED-LSTM's predictive performance and assesses its calibration on four real-life event logs with three different hyperparameter settings. The results show that i) the U-ED-LSTM has reasonable predictive performance across various datasets, ii) aggregating probabilistic suffix predictions into mean values can outperform most likely predictions, particularly for rare prefixes or longer suffixes, and iii) the approach effectively captures uncertainties present in event logs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models</title>
<link>https://arxiv.org/abs/2505.21347</link>
<guid>https://arxiv.org/abs/2505.21347</guid>
<content:encoded><![CDATA[
arXiv:2505.21347v1 Announce Type: new 
Abstract: Text-to-Image (T2I) models have achieved remarkable success in generating visual content from text inputs. Although multiple safety alignment strategies have been proposed to prevent harmful outputs, they often lead to overly cautious behavior -- rejecting even benign prompts -- a phenomenon known as $\textit{over-refusal}$ that reduces the practical utility of T2I models. Despite over-refusal having been observed in practice, there is no large-scale benchmark that systematically evaluates this phenomenon for T2I models. In this paper, we present an automatic workflow to construct synthetic evaluation data, resulting in OVERT ($\textbf{OVE}$r-$\textbf{R}$efusal evaluation on $\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing over-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful but benign prompts across nine safety-related categories, along with 1,785 genuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility trade-off. Using OVERT, we evaluate several leading T2I models and find that over-refusal is a widespread issue across various categories (Figure 1), underscoring the need for further research to enhance the safety alignment of T2I models without compromising their functionality.As a preliminary attempt to reduce over-refusal, we explore prompt rewriting; however, we find it often compromises faithfulness to the meaning of the original prompts. Finally, we demonstrate the flexibility of our generation framework in accommodating diverse safety requirements by generating customized evaluation data adapting to user-defined policies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models</title>
<link>https://arxiv.org/abs/2505.21360</link>
<guid>https://arxiv.org/abs/2505.21360</guid>
<content:encoded><![CDATA[
arXiv:2505.21360v1 Announce Type: new 
Abstract: Competing risks are crucial considerations in survival modelling, particularly in healthcare domains where patients may experience multiple distinct event types. We propose CRISP-NAM (Competing Risks Interpretable Survival Prediction with Neural Additive Models), an interpretable neural additive model for competing risks survival analysis which extends the neural additive architecture to model cause-specific hazards while preserving feature-level interpretability. Each feature contributes independently to risk estimation through dedicated neural networks, allowing for visualization of complex non-linear relationships between covariates and each competing risk. We demonstrate competitive performance on multiple datasets compared to existing approaches.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subgroups Matter for Robust Bias Mitigation</title>
<link>https://arxiv.org/abs/2505.21363</link>
<guid>https://arxiv.org/abs/2505.21363</guid>
<content:encoded><![CDATA[
arXiv:2505.21363v1 Announce Type: new 
Abstract: Despite the constant development of new bias mitigation methods for machine learning, no method consistently succeeds, and a fundamental question remains unanswered: when and why do bias mitigation techniques fail? In this paper, we hypothesise that a key factor may be the often-overlooked but crucial step shared by many bias mitigation methods: the definition of subgroups. To investigate this, we conduct a comprehensive evaluation of state-of-the-art bias mitigation methods across multiple vision and language classification tasks, systematically varying subgroup definitions, including coarse, fine-grained, intersectional, and noisy subgroups. Our results reveal that subgroup choice significantly impacts performance, with certain groupings paradoxically leading to worse outcomes than no mitigation at all. Our findings suggest that observing a disparity between a set of subgroups is not a sufficient reason to use those subgroups for mitigation. Through theoretical analysis, we explain these phenomena and uncover a counter-intuitive insight that, in some cases, improving fairness with respect to a particular set of subgroups is best achieved by using a different set of subgroups for mitigation. Our work highlights the importance of careful subgroup definition in bias mitigation and suggest it as a alternative lever for improving the robustness and fairness of machine learning models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders</title>
<link>https://arxiv.org/abs/2505.21364</link>
<guid>https://arxiv.org/abs/2505.21364</guid>
<content:encoded><![CDATA[
arXiv:2505.21364v1 Announce Type: new 
Abstract: Multilayer perceptrons (MLPs) are an integral part of large language models, yet their dense representations render them difficult to understand, edit, and steer. Recent methods learn interpretable approximations via neuron-level sparsity, yet fail to faithfully reconstruct the original mapping--significantly increasing model's next-token cross-entropy loss. In this paper, we advocate for moving to layer-level sparsity to overcome the accuracy trade-off in sparse layer approximation. Under this paradigm, we introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear Units, expanding pre-trained dense layers into tens of thousands of specialized sublayers. Through a flexible form of tensor factorization, each sparsely activating MxD sublayer implements a linear transformation with full-rank weights--preserving the original decoders' expressive capacity even under heavy sparsity. Experimentally, we show that MxDs significantly outperform state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier in language models with up to 3B parameters. Further evaluations on sparse probing and feature steering demonstrate that MxDs learn similarly specialized features of natural language--opening up a promising new avenue for designing interpretable yet faithful decompositions. Our code is included at: https://github.com/james-oldfield/MxD/.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLANETALIGN: A Comprehensive Python Library for Benchmarking Network Alignment</title>
<link>https://arxiv.org/abs/2505.21366</link>
<guid>https://arxiv.org/abs/2505.21366</guid>
<content:encoded><![CDATA[
arXiv:2505.21366v1 Announce Type: new 
Abstract: Network alignment (NA) aims to identify node correspondence across different networks and serves as a critical cornerstone behind various downstream multi-network learning tasks. Despite growing research in NA, there lacks a comprehensive library that facilitates the systematic development and benchmarking of NA methods. In this work, we introduce PLANETALIGN, a comprehensive Python library for network alignment that features a rich collection of built-in datasets, methods, and evaluation pipelines with easy-to-use APIs. Specifically, PLANETALIGN integrates 18 datasets and 14 NA methods with extensible APIs for easy use and development of NA methods. Our standardized evaluation pipeline encompasses a wide range of metrics, enabling a systematic assessment of the effectiveness, scalability, and robustness of NA methods. Through extensive comparative studies, we reveal practical insights into the strengths and limitations of existing NA methods. We hope that PLANETALIGN can foster a deeper understanding of the NA problem and facilitate the development and benchmarking of more effective, scalable, and robust methods in the future. The source code of PLANETALIGN is available at https://github.com/yq-leo/PlanetAlign.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLM-based Global Optimization with Search Space Partitioning</title>
<link>https://arxiv.org/abs/2505.21372</link>
<guid>https://arxiv.org/abs/2505.21372</guid>
<content:encoded><![CDATA[
arXiv:2505.21372v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently emerged as effective surrogate models and candidate generators within global optimization frameworks for expensive blackbox functions. Despite promising results, LLM-based methods often struggle in high-dimensional search spaces or when lacking domain-specific priors, leading to sparse or uninformative suggestions. To overcome these limitations, we propose HOLLM, a novel global optimization algorithm that enhances LLM-driven sampling by partitioning the search space into promising subregions. Each subregion acts as a ``meta-arm'' selected via a bandit-inspired scoring mechanism that effectively balances exploration and exploitation. Within each selected subregion, an LLM then proposes high-quality candidate points, without any explicit domain knowledge. Empirical evaluation on standard optimization benchmarks shows that HOLLM consistently matches or surpasses leading Bayesian optimization and trust-region methods, while substantially outperforming global LLM-based sampling strategies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models</title>
<link>https://arxiv.org/abs/2505.21382</link>
<guid>https://arxiv.org/abs/2505.21382</guid>
<content:encoded><![CDATA[
arXiv:2505.21382v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) has emerged as one of the most effective, computationally tractable fine-tuning approaches for training Vision-Language Models (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by freezing the pre-trained model weights and injecting trainable low-rank matrices, allowing for efficient learning of these foundation models even on edge devices. However, LoRA in decentralized settings still remains under explored, particularly for the theoretical underpinnings due to the lack of smoothness guarantee and model consensus interference (defined formally below). This work improves the convergence rate of decentralized LoRA (DLoRA) to match the rate of decentralized SGD by ensuring gradient smoothness. We also introduce DeCAF, a novel algorithm integrating DLoRA with truncated singular value decomposition (TSVD)-based matrix factorization to resolve consensus interference. Theoretical analysis shows TSVD's approximation error is bounded and consensus differences between DLoRA and DeCAF vanish as rank increases, yielding DeCAF's matching convergence rate. Extensive experiments across vision/language tasks demonstrate our algorithms outperform local training and rivals federated learning under both IID and non-IID data distributions.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features</title>
<link>https://arxiv.org/abs/2505.21391</link>
<guid>https://arxiv.org/abs/2505.21391</guid>
<content:encoded><![CDATA[
arXiv:2505.21391v1 Announce Type: new 
Abstract: Linear TD($\lambda$) is one of the most fundamental reinforcement learning algorithms for policy evaluation. Previously, convergence rates are typically established under the assumption of linearly independent features, which does not hold in many practical scenarios. This paper instead establishes the first $L^2$ convergence rates for linear TD($\lambda$) operating under arbitrary features, without making any algorithmic modification or additional assumptions. Our results apply to both the discounted and average-reward settings. To address the potential non-uniqueness of solutions resulting from arbitrary features, we develop a novel stochastic approximation result featuring convergence rates to the solution set instead of a single point.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits</title>
<link>https://arxiv.org/abs/2505.21393</link>
<guid>https://arxiv.org/abs/2505.21393</guid>
<content:encoded><![CDATA[
arXiv:2505.21393v1 Announce Type: new 
Abstract: Conversational recommender systems proactively query users with relevant "key terms" and leverage the feedback to elicit users' preferences for personalized recommendations. Conversational contextual bandits, a prevalent approach in this domain, aim to optimize preference learning by balancing exploitation and exploration. However, several limitations hinder their effectiveness in real-world scenarios. First, existing algorithms employ key term selection strategies with insufficient exploration, often failing to thoroughly probe users' preferences and resulting in suboptimal preference estimation. Second, current algorithms typically rely on deterministic rules to initiate conversations, causing unnecessary interactions when preferences are well-understood and missed opportunities when preferences are uncertain. To address these limitations, we propose three novel algorithms: CLiSK, CLiME, and CLiSK-ME. CLiSK introduces smoothed key term contexts to enhance exploration in preference learning, CLiME adaptively initiates conversations based on preference uncertainty, and CLiSK-ME integrates both techniques. We theoretically prove that all three algorithms achieve a tighter regret upper bound of $O(\sqrt{dT\log{T}})$ with respect to the time horizon $T$, improving upon existing methods. Additionally, we provide a matching lower bound $\Omega(\sqrt{dT})$ for conversational bandits, demonstrating that our algorithms are nearly minimax optimal. Extensive evaluations on both synthetic and real-world datasets show that our approaches achieve at least a 14.6% improvement in cumulative regret.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Square$\chi$PO: Differentially Private and Robust $\chi^2$-Preference Optimization in Offline Direct Alignment</title>
<link>https://arxiv.org/abs/2505.21395</link>
<guid>https://arxiv.org/abs/2505.21395</guid>
<content:encoded><![CDATA[
arXiv:2505.21395v1 Announce Type: new 
Abstract: In this paper, we theoretically study the offline alignment of language models with human preference feedback, under both preference label corruption and privacy protections. To this end, we propose Square$\chi$PO, a simple one-line change to $\chi$PO where the standard log-loss is replaced by a new square loss over probability. Thanks to the inherent properties of this new loss, we have advanced the state-of-the-art of differentially private and robust offline direct alignment. Specifically, for the local model of label privacy, Square$\chi$PO is the first algorithm that attains an optimal rate based on single-policy concentrability even with general function approximations. It also gives the first result under the central model of privacy protection over both prompts (responses) and labels. On the robustness side against Huber label corruption, Square$\chi$PO is the first alignment method that has a meaningful theoretical guarantee under general function approximations. More importantly, Square$\chi$PO can address privacy protection and corruption simultaneously, where an interesting separation is observed, implying that the order of privacy and corruption matters. Furthermore, we show that Square$\chi$PO can also be easily extended to handle the scenario of the general preference model with state-of-the-art guarantees under corruption and privacy. Last but not least, all of our theoretical guarantees enjoy a unified analysis, building upon a new result on the generalization error bounds of least-square regression under corruption and privacy constraints, which we believe is of independent interest to the community.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective</title>
<link>https://arxiv.org/abs/2505.21400</link>
<guid>https://arxiv.org/abs/2505.21400</guid>
<content:encoded><![CDATA[
arXiv:2505.21400v1 Announce Type: new 
Abstract: Diffusion models have emerged as a powerful paradigm for modern generative modeling, demonstrating strong potential for large language models (LLMs). Unlike conventional autoregressive (AR) models that generate tokens sequentially, diffusion models enable parallel token sampling, leading to faster generation and eliminating left-to-right generation constraints. Despite their empirical success, the theoretical understanding of diffusion model approaches remains underdeveloped. In this work, we develop convergence guarantees for diffusion language models from an information-theoretic perspective. Our analysis demonstrates that the sampling error, measured by the Kullback-Leibler (KL) divergence, decays inversely with the number of iterations $T$ and scales linearly with the mutual information between tokens in the target text sequence. In particular, we establish matching upper and lower bounds, up to some constant factor, to demonstrate the tightness of our convergence analysis. These results offer novel theoretical insights into the practical effectiveness of diffusion language models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Natural Gradient Descent for Scalable Training of Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.21404</link>
<guid>https://arxiv.org/abs/2505.21404</guid>
<content:encoded><![CDATA[
arXiv:2505.21404v1 Announce Type: new 
Abstract: Natural-gradient methods markedly accelerate the training of Physics-Informed Neural Networks (PINNs), yet their Gauss--Newton update must be solved in the parameter space, incurring a prohibitive $O(n^3)$ time complexity, where $n$ is the number of network trainable weights. We show that exactly the same step can instead be formulated in a generally smaller residual space of size $m = \sum_{\gamma} N_{\gamma} d_{\gamma}$, where each residual class $\gamma$ (e.g. PDE interior, boundary, initial data) contributes $N_{\gamma}$ collocation points of output dimension $d_{\gamma}$.
  Building on this insight, we introduce \textit{Dual Natural Gradient Descent} (D-NGD). D-NGD computes the Gauss--Newton step in residual space, augments it with a geodesic-acceleration correction at negligible extra cost, and provides both a dense direct solver for modest $m$ and a Nystrom-preconditioned conjugate-gradient solver for larger $m$.
  Experimentally, D-NGD scales second-order PINN optimization to networks with up to 12.8 million parameters, delivers one- to three-order-of-magnitude lower final error $L^2$ than first-order methods (Adam, SGD) and quasi-Newton methods, and -- crucially -- enables natural-gradient training of PINNs at this scale on a single GPU.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment</title>
<link>https://arxiv.org/abs/2505.21414</link>
<guid>https://arxiv.org/abs/2505.21414</guid>
<content:encoded><![CDATA[
arXiv:2505.21414v1 Announce Type: new 
Abstract: This paper introduces a comprehensive framework designed to analyze and secure decision-support systems trained with Deep Reinforcement Learning (DRL), prior to deployment, by providing insights into learned behavior patterns and vulnerabilities discovered through simulation. The introduced framework aids in the development of precisely timed and targeted observation perturbations, enabling researchers to assess adversarial attack outcomes within a strategic decision-making context. We validate our framework, visualize agent behavior, and evaluate adversarial outcomes within the context of a custom-built strategic game, CyberStrike. Utilizing the proposed framework, we introduce a method for systematically discovering and ranking the impact of attacks on various observation indices and time-steps, and we conduct experiments to evaluate the transferability of adversarial attacks across agent architectures and DRL training algorithms. The findings underscore the critical need for robust adversarial defense mechanisms to protect decision-making policies in high-stakes environments.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Shift Happens - Confounding Is to Blame</title>
<link>https://arxiv.org/abs/2505.21422</link>
<guid>https://arxiv.org/abs/2505.21422</guid>
<content:encoded><![CDATA[
arXiv:2505.21422v1 Announce Type: new 
Abstract: Distribution shifts introduce uncertainty that undermines the robustness and generalization capabilities of machine learning models. While conventional wisdom suggests that learning causal-invariant representations enhances robustness to such shifts, recent empirical studies present a counterintuitive finding: (i) empirical risk minimization (ERM) can rival or even outperform state-of-the-art out-of-distribution (OOD) generalization methods, and (ii) its OOD generalization performance improves when all available covariates, not just causal ones, are utilized. Drawing on both empirical and theoretical evidence, we attribute this phenomenon to hidden confounding. Shifts in hidden confounding induce changes in data distributions that violate assumptions commonly made by existing OOD generalization approaches. Under such conditions, we prove that effective generalization requires learning environment-specific relationships, rather than relying solely on invariant ones. Furthermore, we show that models augmented with proxies for hidden confounders can mitigate the challenges posed by hidden confounding shifts. These findings offer new theoretical insights and practical guidance for designing robust OOD generalization algorithms and principled covariate selection strategies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>