<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>Generalizing Large Language Model Usability Across Resource-Constrained</title>
<link>https://arxiv.org/abs/2505.17040</link>
<guid>https://arxiv.org/abs/2505.17040</guid>
<content:encoded><![CDATA[
<div> alignment framework, multimodal, robustness, low-resource, adaptability

Summary:
- The dissertation focuses on enhancing the usability of Large Language Models (LLMs) in real-world scenarios.
- It introduces a text-centric alignment framework to integrate diverse modalities seamlessly.
- An adversarial prompting technique is proposed to improve robustness against noisy and missing modalities.
- Inference-time optimization strategies, such as prompt search and uncertainty quantification, are investigated to enhance LLM performance.
- The work also addresses low-resource domains, achieving state-of-the-art performance in Verilog code generation using synthetic data pipelines and logic-enhanced reasoning models.
- Overall, the dissertation aims to improve the adaptability, scalability, and efficiency of LLMs under practical constraints.<br><br> <div>
arXiv:2505.17040v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language tasks, and recent efforts have sought to extend their capabilities to multimodal domains and resource-constrained environments. However, existing approaches often rely on costly supervised fine-tuning or assume fixed training conditions, limiting their generalization when facing unseen modalities, limited data, or restricted compute resources. This dissertation presents a systematic study toward generalizing LLM usability under real-world constraints. First, it introduces a robust text-centric alignment framework that enables LLMs to seamlessly integrate diverse modalities-including text, images, tables, and any modalities - via natural language interfaces. This approach supports in-context adaptation to unseen or dynamically changing modalities without requiring retraining. To enhance robustness against noisy and missing modalities, an adversarial prompting technique is proposed, generating semantically challenging perturbations at the prompt level to stress-test model reliability. Beyond multimodal setting, the dissertation investigates inference-time optimization strategies for LLMs, leveraging prompt search and uncertainty quantification to improve performance without additional model training. This perspective offers an efficient alternative to scaling model parameters or retraining from scratch. Additionally, the work addresses low-resource domains such as Verilog code generation by designing correct-by-construction synthetic data pipelines and logic-enhanced reasoning models, achieving state-of-the-art performance with minimal data. Together, these contributions form a unified effort to enhance the adaptability, scalability, and efficiency of large language models under practical constraints.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAP: Runtime-Adaptive Pruning for LLM Inference</title>
<link>https://arxiv.org/abs/2505.17138</link>
<guid>https://arxiv.org/abs/2505.17138</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, compression, reinforcement learning, adaptive pruning, runtime memory

Summary:
- Large language models (LLMs) are powerful but computationally intensive, hindering deployment.
- Existing compression methods for LLMs often fail to adapt to runtime memory variations or heterogeneous KV-cache demands.
- RAP is a dynamic pruning framework driven by reinforcement learning (RL) that adjusts compression strategies based on immediate memory constraints and workload demands.
- RAP tracks the evolving ratio between model parameters and KV-cache during execution, focusing on retaining components that maximize utility within the current memory budget.
- Experimental results show that RAP surpasses state-of-the-art baselines by dynamically considering both model weights and KV-cache in real-time. 

<br><br>Summary: 

Large language models have significant computational and memory requirements that limit their deployment. Current compression methods for these models struggle to adapt to changing memory conditions and diverse user requests. To address these limitations, RAP, a novel framework driven by reinforcement learning, dynamically adjusts compression strategies based on real-time memory constraints and workload demands. By tracking the ratio between model parameters and KV-cache and retaining components that maximize utility within current memory limits, RAP outperforms existing methods by considering both model weights and KV-cache dynamically. <div>
arXiv:2505.17138v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at language understanding and generation, but their enormous computational and memory requirements hinder deployment. Compression offers a potential solution to mitigate these constraints. However, most existing methods rely on fixed heuristics and thus fail to adapt to runtime memory variations or heterogeneous KV-cache demands arising from diverse user requests. To address these limitations, we propose RAP, an elastic pruning framework driven by reinforcement learning (RL) that dynamically adjusts compression strategies in a runtime-aware manner. Specifically, RAP dynamically tracks the evolving ratio between model parameters and KV-cache across practical execution. Recognizing that FFNs house most parameters, whereas parameter -light attention layers dominate KV-cache formation, the RL agent retains only those components that maximize utility within the current memory budget, conditioned on instantaneous workload and device state. Extensive experiments results demonstrate that RAP outperforms state-of-the-art baselines, marking the first time to jointly consider model weights and KV-cache on the fly.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSTH-Sleep: Towards Effective Few-Shot Sleep Stage Classification with Spatial-Temporal Hypergraph Enhanced Meta-Learning</title>
<link>https://arxiv.org/abs/2505.17142</link>
<guid>https://arxiv.org/abs/2505.17142</guid>
<content:encoded><![CDATA[
<div> deep learning, sleep stage classification, bio-signals, meta-learning, spatial-temporal hypergraph

Summary: 
MetaSTH-Sleep is introduced as a few-shot sleep stage classification framework using spatial-temporal hypergraph enhanced meta-learning. This approach addresses challenges faced in accurate classification of sleep stages such as limited annotated data, inter-individual variability, and neglect of high-order signal relationships. By enabling rapid adaptation to new subjects with minimal labeled samples, MetaSTH-Sleep improves model performance and generalization. The use of hypergraph structure effectively captures spatial interconnections and temporal dynamics in EEG signals, enhancing the understanding of signal heterogeneity. Experimental results showcase significant performance enhancements across diverse subjects, providing valuable insights for clinicians in sleep stage annotation. <div>
arXiv:2505.17142v1 Announce Type: new 
Abstract: Accurate classification of sleep stages based on bio-signals is fundamental for automatic sleep stage annotation. Traditionally, this task relies on experienced clinicians to manually annotate data, a process that is both time-consuming and labor-intensive. In recent years, deep learning methods have shown promise in automating this task. However, three major challenges remain: (1) deep learning models typically require large-scale labeled datasets, making them less effective in real-world settings where annotated data is limited; (2) significant inter-individual variability in bio-signals often results in inconsistent model performance when applied to new subjects, limiting generalization; and (3) existing approaches often overlook the high-order relationships among bio-signals, failing to simultaneously capture signal heterogeneity and spatial-temporal dependencies. To address these issues, we propose MetaSTH-Sleep, a few-shot sleep stage classification framework based on spatial-temporal hypergraph enhanced meta-learning. Our approach enables rapid adaptation to new subjects using only a few labeled samples, while the hypergraph structure effectively models complex spatial interconnections and temporal dynamics simultaneously in EEG signals. Experimental results demonstrate that MetaSTH-Sleep achieves substantial performance improvements across diverse subjects, offering valuable insights to support clinicians in sleep stage annotation.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Training of Neural SDEs Using Stochastic Optimal Control</title>
<link>https://arxiv.org/abs/2505.17150</link>
<guid>https://arxiv.org/abs/2505.17150</guid>
<content:encoded><![CDATA[
<div> control theory, variational inference, neural stochastic differential equations, stochastic optimal control, training efficiency

Summary:
The article introduces a hierarchical method for variational inference for neural stochastic differential equations (SDEs), inspired by control theory. By decomposing the control term into linear and non-linear components, the authors derive an optimal control term for linear SDEs using stochastic optimal control. The non-linear component is modeled by a neural network, allowing for efficient training of neural SDEs without sacrificing expressive power. Training is initialized at a lower cost since the linear part of the control term is optimal and does not need to be learned. This approach leads to faster convergence in training neural SDEs, making uncertainty-aware reasoning in time-series more computationally feasible. <div>
arXiv:2505.17150v1 Announce Type: new 
Abstract: We present a hierarchical, control theory inspired method for variational inference (VI) for neural stochastic differential equations (SDEs). While VI for neural SDEs is a promising avenue for uncertainty-aware reasoning in time-series, it is computationally challenging due to the iterative nature of maximizing the ELBO. In this work, we propose to decompose the control term into linear and residual non-linear components and derive an optimal control term for linear SDEs, using stochastic optimal control. Modeling the non-linear component by a neural network, we show how to efficiently train neural SDEs without sacrificing their expressive power. Since the linear part of the control term is optimal and does not need to be learned, the training is initialized at a lower cost and we observe faster convergence.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling</title>
<link>https://arxiv.org/abs/2505.17155</link>
<guid>https://arxiv.org/abs/2505.17155</guid>
<content:encoded><![CDATA[
<div> compression, reasoning models, test-time scaling, verifier-based framework, inference efficiency

Summary:
TrimR is a new framework designed to improve the efficiency of Large Reasoning Models (LRMs) by dynamically compressing their Chain-of-Thought reasoning process. LRMs often generate redundant thinking processes, leading to inefficiencies during inference. TrimR uses a pre-trained verifier to detect and truncate these redundant thoughts without the need for fine-tuning. The framework is specifically engineered for production-level deployment and has been shown to enhance test-time scaling and inference efficiency on various benchmarks. Empirical evaluations on different LRMs demonstrate up to a 70% improvement in reasoning runtime without sacrificing accuracy. TrimR offers a solution to the decoding overhead incurred by LRMs using test-time scaling methods, making it suitable for high-throughput industrial applications. <div>
arXiv:2505.17155v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling complex mathematical, logical, and coding tasks by leveraging extended Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging CoT with explicit token-level exploration, can push LRMs' accuracy boundaries, but they incur significant decoding overhead. A key inefficiency source is LRMs often generate redundant thinking CoTs, which demonstrate clear structured overthinking and underthinking patterns. Inspired by human cognitive reasoning processes and numerical optimization theories, we propose TrimR, a verifier-based, training-free, efficient framework for dynamic CoT compression to trim reasoning and enhance test-time scaling, explicitly tailored for production-level deployment. Our method employs a lightweight, pretrained, instruction-tuned verifier to detect and truncate redundant intermediate thoughts of LRMs without any LRM or verifier fine-tuning. We present both the core algorithm and asynchronous online system engineered for high-throughput industrial applications. Empirical evaluations on Ascend NPUs and vLLM show that our framework delivers substantial gains in inference efficiency under large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and GPQA benchmarks, the reasoning runtime of Pangu-R-38B, QwQ-32B, and DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on accuracy.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning</title>
<link>https://arxiv.org/abs/2505.17163</link>
<guid>https://arxiv.org/abs/2505.17163</guid>
<content:encoded><![CDATA[
<div> OCR-Reasoning, multimodal slow-thinking systems, benchmark, reasoning tasks, text-rich image reasoning.  
Summary:  
OCR-Reasoning is a new benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on text-rich image reasoning tasks. It consists of 1,069 human-annotated examples covering 6 core reasoning abilities and 18 practical reasoning tasks. Unlike other benchmarks, OCR-Reasoning annotates both the final answers and the reasoning process, allowing for a more comprehensive analysis of model performance. Evaluation of state-of-the-art MLLMs on this benchmark revealed significant challenges, with none achieving accuracy above 50%. This highlights the urgent need to address the difficulties in text-rich image reasoning tasks. The benchmark and evaluation scripts are publicly available, providing a valuable resource for researchers in this field. <br><br>Summary: <div>
arXiv:2505.17163v1 Announce Type: new 
Abstract: Recent advancements in multimodal slow-thinking systems have demonstrated remarkable performance across diverse visual reasoning tasks. However, their capabilities in text-rich image reasoning tasks remain understudied due to the lack of a systematic benchmark. To address this gap, we propose OCR-Reasoning, a comprehensive benchmark designed to systematically assess Multimodal Large Language Models on text-rich image reasoning tasks. The benchmark comprises 1,069 human-annotated examples spanning 6 core reasoning abilities and 18 practical reasoning tasks in text-rich visual scenarios. Furthermore, unlike other text-rich image understanding benchmarks that only annotate the final answers, OCR-Reasoning also annotates the reasoning process simultaneously. With the annotated reasoning process and the final answers, OCR-Reasoning evaluates not only the final answers generated by models but also their reasoning processes, enabling a holistic analysis of their problem-solving abilities. Leveraging this benchmark, we conducted a comprehensive evaluation of state-of-the-art MLLMs. Our results demonstrate the limitations of existing methodologies. Notably, even state-of-the-art MLLMs exhibit substantial difficulties, with none achieving accuracy surpassing 50\% across OCR-Reasoning, indicating that the challenges of text-rich image reasoning are an urgent issue to be addressed. The benchmark and evaluation scripts are available at https://github.com/SCUT-DLVCLab/OCR-Reasoning.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms</title>
<link>https://arxiv.org/abs/2505.17190</link>
<guid>https://arxiv.org/abs/2505.17190</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic programming, Tropical attention, Neural Algorithmic Reasoning, Max-plus semiring, Adversarial attacks

Summary:
Dynamic programming algorithms for optimization problems typically use max-plus semirings for value functions, but existing Neural Algorithmic Reasoning models rely on softmax attention, which can blur sharp polyhedral structures. This paper introduces Tropical attention, a new attention function operating in the max-plus semiring, improving reasoning in out-of-distribution settings. Tropical attention approximates tropical circuits of DP-type algorithms and enhances OOD performance in algorithmic reasoning tasks. It surpasses softmax baselines in length and value generalization while remaining stable under adversarial attacks. The study also introduces adversarial-attack generalization as a benchmark for Neural Algorithmic Reasoning. Tropical attention restores sharp, scale-invariant reasoning that softmax lacks, improving performance and stability in challenging scenarios. 

<br><br>Summary: <div>
arXiv:2505.17190v1 Announce Type: new 
Abstract: Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning models, however, rely on softmax-normalized dot-product attention where the smooth exponential weighting blurs these sharp polyhedral structures and collapses when evaluated on out-of-distribution (OOD) settings. We introduce Tropical attention, a novel attention function that operates natively in the max-plus semiring of tropical geometry. We prove that Tropical attention can approximate tropical circuits of DP-type combinatorial algorithms. We then propose that using Tropical transformers enhances empirical OOD performance in both length generalization and value generalization, on algorithmic reasoning tasks, surpassing softmax baselines while remaining stable under adversarial attacks. We also present adversarial-attack generalization as a third axis for Neural Algorithmic Reasoning benchmarking. Our results demonstrate that Tropical attention restores the sharp, scale-invariant reasoning absent from softmax.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shape it Up! Restoring LLM Safety during Finetuning</title>
<link>https://arxiv.org/abs/2505.17196</link>
<guid>https://arxiv.org/abs/2505.17196</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, safety risks, dynamic shaping, guardrail models, safety assessment

Summary:
Finetuning large language models (LLMs) can lead to safety risks as even a few harmful examples can compromise alignment. To address this issue, the authors propose dynamic safety shaping (DSS) as a framework that fine-tunes models by reinforcing learning from safe segments of a response while suppressing unsafe content, instead of treating all parts equally. They introduce the idea of guardrail models being repurposed to evaluate partial responses, enabling the Safety Trajectory Assessment of Response (STAR) for token-level signals to operate dynamically during training. STAR-DSS, guided by STAR scores, effectively mitigates risks without sacrificing task performance across various threats and datasets. The research encourages future safety efforts to adopt dynamic shaping principles for enhanced mitigation against evolving finetuning risks. 

<br><br>Summary: <div>
arXiv:2505.17196v1 Announce Type: new 
Abstract: Finetuning large language models (LLMs) enables user-specific customization but introduces critical safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal-a coarse treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present STAR-DSS, guided by STAR scores, that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families-all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LengthLogD: A Length-Stratified Ensemble Framework for Enhanced Peptide Lipophilicity Prediction via Multi-Scale Feature Integration</title>
<link>https://arxiv.org/abs/2505.17198</link>
<guid>https://arxiv.org/abs/2505.17198</guid>
<content:encoded><![CDATA[
<div> Keywords: peptide compounds, membrane permeability, predictive framework, molecular length stratification, ensemble learning

Summary: 
LengthLogD is a novel predictive framework designed to accurately predict the logD of peptide compounds, considering the impact of molecular weight and length on membrane permeability. This framework utilizes multi-scale molecular representations and employs a length-stratified strategy, along with an adaptive weight allocation mechanism for long peptides. Experimental results show superior performance in predicting logD values for short, medium, and long peptides, with a notable reduction in prediction errors for long peptides. The study highlights the significance of topological features and the effectiveness of the length-stratified approach, contributing to the overall improvement in predictive accuracy. Compared to existing models, LengthLogD offers precise logD predictions for peptide drug development, particularly showcasing its value in optimizing long peptide lead compounds. <br><br>Summary: <div>
arXiv:2505.17198v1 Announce Type: new 
Abstract: Peptide compounds demonstrate considerable potential as therapeutic agents due to their high target affinity and low toxicity, yet their drug development is constrained by their low membrane permeability. Molecular weight and peptide length have significant effects on the logD of peptides, which in turn influences their ability to cross biological membranes. However, accurate prediction of peptide logD remains challenging due to the complex interplay between sequence, structure, and ionization states. This study introduces LengthLogD, a predictive framework that establishes specialized models through molecular length stratification while innovatively integrating multi-scale molecular representations. We constructed feature spaces across three hierarchical levels: atomic (10 molecular descriptors), structural (1024-bit Morgan fingerprints), and topological (3 graph-based features including Wiener index), optimized through stratified ensemble learning. An adaptive weight allocation mechanism specifically developed for long peptides significantly enhances model generalizability. Experimental results demonstrate superior performance across all categories: short peptides (R^2=0.855), medium peptides (R^2=0.816), and long peptides (R^2=0.882), with a 34.7% reduction in prediction error for long peptides compared to conventional single-model approaches. Ablation studies confirm: 1) The length-stratified strategy contributes 41.2% to performance improvement; 2) Topological features account for 28.5% of predictive importance. Compared to state-of-the-art models, our method maintains short peptide prediction accuracy while achieving a 25.7% increase in the coefficient of determination (R^2) for long peptides. This research provides a precise logD prediction tool for peptide drug development, particularly demonstrating unique value in optimizing long peptide lead compounds.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation</title>
<link>https://arxiv.org/abs/2505.17226</link>
<guid>https://arxiv.org/abs/2505.17226</guid>
<content:encoded><![CDATA[
<div> rKrum, Average-rKrum, Federated Learning, Byzantine attacks, Privacy-preserving AI
<br>
Summary: 
Average-rKrum (ArKrum) is introduced as a novel aggregation strategy to enhance the resilience and privacy guarantees of Federated Learning (FL) systems against Byzantine attacks from malicious participants. It incorporates a median-based filtering mechanism to remove outliers and estimate the number of adversarial clients accurately. Additionally, ArKrum applies a multi-update averaging scheme to improve stability and performance, especially in cases where client data distributions differ. Evaluation on image and text datasets under various Byzantine attack types demonstrates that ArKrum consistently achieves high accuracy and stability, performing comparably or better than existing robust aggregation methods. These results highlight ArKrum as an effective and practical solution for securing FL systems in adversarial environments. 
<br> <div>
arXiv:2505.17226v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative machine learning across decentralized data sources without sharing raw data. It offers a promising approach to privacy-preserving AI. However, FL remains vulnerable to adversarial threats from malicious participants, referred to as Byzantine clients, who can send misleading updates to corrupt the global model. Traditional aggregation methods, such as simple averaging, are not robust to such attacks. More resilient approaches, like the Krum algorithm, require prior knowledge of the number of malicious clients, which is often unavailable in real-world scenarios. To address these limitations, we propose Average-rKrum (ArKrum), a novel aggregation strategy designed to enhance both the resilience and privacy guarantees of FL systems. Building on our previous work (rKrum), ArKrum introduces two key innovations. First, it includes a median-based filtering mechanism that removes extreme outliers before estimating the number of adversarial clients. Second, it applies a multi-update averaging scheme to improve stability and performance, particularly when client data distributions are not identical. We evaluate ArKrum on benchmark image and text datasets under three widely studied Byzantine attack types. Results show that ArKrum consistently achieves high accuracy and stability. It performs as well as or better than other robust aggregation methods. These findings demonstrate that ArKrum is an effective and practical solution for secure FL systems in adversarial environments.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Capability Evaluation of Foundation Models</title>
<link>https://arxiv.org/abs/2505.17228</link>
<guid>https://arxiv.org/abs/2505.17228</guid>
<content:encoded><![CDATA[
<div> framework, evaluation, foundation models, active learning, capability

Summary:
- The paper introduces the Active learning for Capability Evaluation (ACE) framework for evaluating foundation models in a scalable and automated manner.
- ACE decomposes domains into meaningful capabilities using language models, reducing human effort in creating evaluation tasks.
- The framework models a subject model's performance as a capability function over a semantic space and prioritizes evaluating informative capabilities using active learning.
- ACE's adaptive evaluation strategy allows for the discovery of strengths, weaknesses, and failure modes that static benchmarks may overlook.
- Results indicate that ACE offers a more comprehensive and informative assessment of model capabilities, crucial for the safe and informed deployment of foundation models. 

<br><br>Summary: <div>
arXiv:2505.17228v1 Announce Type: new 
Abstract: Current evaluation frameworks for foundation models rely heavily on fixed, manually curated benchmarks, limiting their ability to capture the full breadth of model capabilities. This paper introduces Active learning for Capability Evaluation (ACE), a novel framework for scalable, automated, and fine-grained evaluation of foundation models. ACE leverages the knowledge embedded in powerful language models to decompose a domain into semantically meaningful capabilities and generate diverse evaluation tasks, significantly reducing human effort. To maximize coverage and efficiency, ACE models a subject model's performance as a capability function over a latent semantic space and uses active learning to prioritize the evaluation of the most informative capabilities. This adaptive evaluation strategy enables cost-effective discovery of strengths, weaknesses, and failure modes that static benchmarks may miss. Our results suggest that ACE provides a more complete and informative picture of model capabilities, which is essential for safe and well-informed deployment of foundation models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Aware Interpretable Multimodal Music Auto-Tagging</title>
<link>https://arxiv.org/abs/2505.17233</link>
<guid>https://arxiv.org/abs/2505.17233</guid>
<content:encoded><![CDATA[
<div> Keywords: music auto-tagging, interpretable framework, multimodal features, clustering, transparent system

Summary:
This article introduces a novel framework for music auto-tagging that focuses on interpretability. By combining various musically relevant features from signal processing, deep learning, ontology engineering, and natural language processing, the framework enhances the understanding of the tagging process. The method clusters these features based on their semantic meaning and utilizes an expectation maximization algorithm to assign weights to each group. This approach not only achieves competitive performance in music tagging but also provides insight into the decision-making process, making the system more transparent and user-centric. Overall, the framework offers a deeper understanding of how music auto-tagging algorithms work, improving trust and usability for both researchers and end-users.  

<br><br>Summary: <div>
arXiv:2505.17233v1 Announce Type: new 
Abstract: Music auto-tagging is essential for organizing and discovering music in extensive digital libraries. While foundation models achieve exceptional performance in this domain, their outputs often lack interpretability, limiting trust and usability for researchers and end-users alike. In this work, we present an interpretable framework for music auto-tagging that leverages groups of musically meaningful multimodal features, derived from signal processing, deep learning, ontology engineering, and natural language processing. To enhance interpretability, we cluster features semantically and employ an expectation maximization algorithm, assigning distinct weights to each group based on its contribution to the tagging process. Our method achieves competitive tagging performance while offering a deeper understanding of the decision-making process, paving the way for more transparent and user-centric music tagging systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Policy Minimum Bayesian Risk</title>
<link>https://arxiv.org/abs/2505.17242</link>
<guid>https://arxiv.org/abs/2505.17242</guid>
<content:encoded><![CDATA[
<div> Inference scaling, LLMs, long-chain-of-thought generation, best-of-N sampling, minimum Bayes risk decoding<br>
<br>
Summary: In the study, a novel method for enhancing large language models' (LLMs') accuracy in solving complex reasoning problems through extended computation time is proposed. By incorporating reward and risk/similarity signals into minimum Bayes risk decoding (MBRD), the framework offers improved robustness, accuracy, and well-defined asymptotic behavior compared to traditional methods. This approach also allows for a sample-efficient variant of MBRD that adjusts the number of generated samples based on problem difficulty. Empirical results on math and coding tasks show the advantages of the proposed method using open-source models, along with a comprehensive analysis of accuracy-compute trade-offs. The method's effectiveness lies in leveraging optimal policy concepts in KL-controlled reinforcement learning to enhance LLM performance in generating multiple candidate solutions and aggregating them effectively. <br><br>Summary: <div>
arXiv:2505.17242v1 Announce Type: new 
Abstract: Inference scaling can help LLMs solve complex reasoning problems through extended runtime computation. On top of targeted supervision for long chain-of-thought (long-CoT) generation, purely inference-time techniques such as best-of-N (BoN) sampling, majority voting, or more generally, minimum Bayes risk decoding (MBRD), can further improve LLM accuracy by generating multiple candidate solutions and aggregating over them. These methods typically leverage additional signals in the form of reward models and risk/similarity functions that compare generated samples, e.g., exact match in some normalized space or standard similarity metrics such as Rouge. Here we present a novel method for incorporating reward and risk/similarity signals into MBRD. Based on the concept of optimal policy in KL-controlled reinforcement learning, our framework provides a simple and well-defined mechanism for leveraging such signals, offering several advantages over traditional inference-time methods: higher robustness, improved accuracy, and well-understood asymptotic behavior. In addition, it allows for the development of a sample-efficient variant of MBRD that can adjust the number of samples to generate according to the difficulty of the problem, without relying on majority vote counts. We empirically demonstrate the advantages of our approach on math (MATH-$500$) and coding (HumanEval) tasks using recent open-source models. We also present a comprehensive analysis of its accuracy-compute trade-offs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backdoors in DRL: Four Environments Focusing on In-distribution Triggers</title>
<link>https://arxiv.org/abs/2505.17248</link>
<guid>https://arxiv.org/abs/2505.17248</guid>
<content:encoded><![CDATA[
<div> backdoor attacks, trojans, deep neural network models, reinforcement learning agents, security threat <br>
<br>
Summary: <br>
Backdoor attacks, also known as trojans, are a concerning security risk in deep neural network models, especially in open-source or third-party models. This research focuses on in-distribution triggers in deep reinforcement learning agents, which are more dangerous as they can be easily activated within the natural data distribution. The study develops trojans in four RL environments and finds that in-distribution triggers, though requiring more effort to implement, are still viable threats in DRL through basic data poisoning attacks. This research sheds light on the potential vulnerabilities in DRL models and the importance of understanding and mitigating backdoor attacks. <div>
arXiv:2505.17248v1 Announce Type: new 
Abstract: Backdoor attacks, or trojans, pose a security risk by concealing undesirable behavior in deep neural network models. Open-source neural networks are downloaded from the internet daily, possibly containing backdoors, and third-party model developers are common. To advance research on backdoor attack mitigation, we develop several trojans for deep reinforcement learning (DRL) agents. We focus on in-distribution triggers, which occur within the agent's natural data distribution, since they pose a more significant security threat than out-of-distribution triggers due to their ease of activation by the attacker during model deployment. We implement backdoor attacks in four reinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld, Colorful Memory, and Modified Safety Gymnasium. We train various models, both clean and backdoored, to characterize these attacks. We find that in-distribution triggers can require additional effort to implement and be more challenging for models to learn, but are nevertheless viable threats in DRL even using basic data poisoning attacks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approach to Finding a Robust Deep Learning Model</title>
<link>https://arxiv.org/abs/2505.17254</link>
<guid>https://arxiv.org/abs/2505.17254</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, artificial intelligence, model robustness, deep learning models, training sample size<br>
Summary: <br>
The article introduces a new approach for assessing the robustness of machine learning models, particularly deep learning models. The approach includes a model selection algorithm and meta-algorithm that can be applied to any machine learning model. The study focuses on small models made up of convolutional and fully connected layers, using common optimizers. The impact of training sample size, model weight initialization, and inductive bias on model robustness is examined within this framework. The aim is to train models without human supervision while ensuring reliable predictions, addressing the growing demand for efficient and accurate model training in the field of artificial intelligence. <div>
arXiv:2505.17254v1 Announce Type: new 
Abstract: The rapid development of machine learning (ML) and artificial intelligence (AI) applications requires the training of large numbers of models. This growing demand highlights the importance of training models without human supervision, while ensuring that their predictions are reliable. In response to this need, we propose a novel approach for determining model robustness. This approach, supplemented with a proposed model selection algorithm designed as a meta-algorithm, is versatile and applicable to any machine learning model, provided that it is appropriate for the task at hand. This study demonstrates the application of our approach to evaluate the robustness of deep learning models. To this end, we study small models composed of a few convolutional and fully connected layers, using common optimizers due to their ease of interpretation and computational efficiency. Within this framework, we address the influence of training sample size, model weight initialization, and inductive bias on the robustness of deep learning models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model</title>
<link>https://arxiv.org/abs/2505.17257</link>
<guid>https://arxiv.org/abs/2505.17257</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, genomics, JanusDNA, bidirectional understanding, pretraining paradigm

Summary:
JanusDNA is a novel bidirectional DNA foundation model that addresses the challenges of applying large language models to genomics. By combining the efficiency of autoregressive modeling with the bidirectional comprehension of masked modeling, JanusDNA can capture long-range dependencies within DNA sequences efficiently. It adopts a hybrid Mamba, Attention, and Mixture of Experts (MoE) architecture to achieve high performance on genomic representation benchmarks, outperforming models with significantly more parameters. JanusDNA can process up to 1 million base pairs with single nucleotide resolution on a single 80GB GPU, showcasing its scalability and computational efficiency. This innovative approach opens up new possibilities for utilizing large language models in genomics research. 

<br><br>Summary: JanusDNA is a novel bidirectional DNA foundation model that efficiently captures long-range dependencies within DNA sequences by combining autoregressive and masked modeling approaches. It outperforms models with more parameters on genomic representation benchmarks and can process large sequences on a single GPU, showcasing its scalability and efficiency in genomics research. <div>
arXiv:2505.17257v1 Announce Type: new 
Abstract: Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genomics presents significant challenges. Capturing complex genomic interactions requires modeling long-range dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene, posing substantial computational burdens under conventional model architectures and training paradigms. Moreover, standard LLM training approaches are suboptimal for DNA: autoregressive training, while efficient, supports only unidirectional understanding. However, DNA is inherently bidirectional, e.g., bidirectional promoters regulate transcription in both directions and account for nearly 11% of human gene expression. Masked language models (MLMs) allow bidirectional understanding but are inefficient, as only masked tokens contribute to the loss per step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm that combines the optimization efficiency of autoregressive modeling with the bidirectional comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and Mixture of Experts (MoE) architecture, combining long-range modeling of Attention with efficient sequential learning of Mamba. MoE layers further scale model capacity via sparse activation while keeping computational cost low. Notably, JanusDNA processes up to 1 million base pairs at single nucleotide resolution on a single 80GB GPU. Extensive experiments and ablations show JanusDNA achieves new SOTA results on three genomic representation benchmarks, outperforming models with 250x more activated parameters. Code: https://github.com/Qihao-Duan/JanusDNA
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zebra-Llama: Towards Extremely Efficient Hybrid Models</title>
<link>https://arxiv.org/abs/2505.17272</link>
<guid>https://arxiv.org/abs/2505.17272</guid>
<content:encoded><![CDATA[
<div> Efficient Language Models, Hybrid Models, State Space Models, Multi-head Latent Attention, Knowledge Transfer

Summary:
Zebra-Llama proposes composing efficient hybrid language models from existing pre-trained models as a scalable alternative to expensive retraining. By combining State Space Models and Multi-head Latent Attention layers, Zebra-Llama achieves Transformer-level accuracy with near-SSM efficiency using significantly fewer training tokens and smaller teachers. The model reduces KV cache size while preserving high performance on LM tasks. Compared to other models, Zebra-Llama delivers competitive or superior accuracy with smaller teachers and memory requirements. Zebra-Llama-8B outperforms Minitron-8B in few-shot accuracy while using fewer tokens, smaller KV cache, and achieving higher throughput. The approach showcases the potential for practical and sustainable improvements in inference efficiency for large language models.<br><br>Summary: <div>
arXiv:2505.17272v1 Announce Type: new 
Abstract: With the growing demand for deploying large language models (LLMs) across diverse applications, improving their inference efficiency is crucial for sustainable and democratized access. However, retraining LLMs to meet new user-specific requirements is prohibitively expensive and environmentally unsustainable. In this work, we propose a practical and scalable alternative: composing efficient hybrid language models from existing pre-trained models. Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA) layers, using a refined initialization and post-training pipeline to efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B training tokens (compared to trillions of tokens required for pre-training) and an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants, respectively-while preserving 100%, 100%, and >97% of average zero-shot performance on LM Harness tasks. Compared to models like MambaInLLaMA, X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive or superior accuracy while using significantly fewer tokens, smaller teachers, and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens, over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves 2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context length. We will release code and model checkpoints upon acceptance.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparator-Adaptive $\Phi$-Regret: Improved Bounds, Simpler Algorithms, and Applications to Games</title>
<link>https://arxiv.org/abs/2505.17277</link>
<guid>https://arxiv.org/abs/2505.17277</guid>
<content:encoded><![CDATA[
<div> Sparsity-based complexity measure, Adaptive algorithm, Comparator-adaptive regret, Prior distribution, Binary transformations<br>
<br>
Summary:
This article introduces a new approach to the classic expert problem, focusing on achieving better comparator-adaptive $\Phi$-regret bounds with simpler algorithms compared to previous works. The proposed method involves a prior distribution over binary transformations, leading to prior-dependent regret against these transformations. Two efficient algorithms are presented, leveraging a prior-aware kernelized MWU algorithm and a BM-reduction approach. These methods offer improved regret bounds and simplicity in comparison to existing techniques. Additionally, the second algorithm can be extended to general-sum games, providing accelerated and adaptive convergence rates to $\Phi$-equilibria. In the context of correlated equilibria, the proposed bound surpasses existing results, showcasing the effectiveness and advantages of the proposed approach. <br><br>Summary: <div>
arXiv:2505.17277v1 Announce Type: new 
Abstract: In the classic expert problem, $\Phi$-regret measures the gap between the learner's total loss and that achieved by applying the best action transformation $\phi \in \Phi$. A recent work by Lu et al., [2025] introduces an adaptive algorithm whose regret against a comparator $\phi$ depends on a certain sparsity-based complexity measure of $\phi$, (almost) recovering and interpolating optimal bounds for standard regret notions such as external, internal, and swap regret. In this work, we propose a general idea to achieve an even better comparator-adaptive $\Phi$-regret bound via much simpler algorithms compared to Lu et al., [2025]. Specifically, we discover a prior distribution over all possible binary transformations and show that it suffices to achieve prior-dependent regret against these transformations. Then, we propose two concrete and efficient algorithms to achieve so, where the first one learns over multiple copies of a prior-aware variant of the Kernelized MWU algorithm of Farina et al., [2022], and the second one learns over multiple copies of a prior-aware variant of the BM-reduction [Blum and Mansour, 2007]. To further showcase the power of our methods and the advantages over Lu et al., [2025] besides the simplicity and better regret bounds, we also show that our second approach can be extended to the game setting to achieve accelerated and adaptive convergence rate to $\Phi$-equilibria for a class of general-sum games. When specified to the special case of correlated equilibria, our bound improves over the existing ones from Anagnostides et al., [2022a,b]
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention with Trained Embeddings Provably Selects Important Tokens</title>
<link>https://arxiv.org/abs/2505.17282</link>
<guid>https://arxiv.org/abs/2505.17282</guid>
<content:encoded><![CDATA[
<div> embeddings, language modeling, gradient descent, softmax attention model, binary classification <br>
<br>
Summary: 
The study focuses on understanding the structure of token embeddings in language modeling through gradient descent. They analyze a one-layer softmax attention model with a linear head for binary classification. After a single step of training, the embeddings align with the output vector based on token frequency in the dataset. Training the embedding of the special token $\mathrm{\langle cls \rangle}$ results in selecting important tokens predictive of the label, maximizing the margin for this selection. Real-world experiments on IMDB and Yelp datasets confirm the theory's findings, showing that the embeddings capture token importance and align with predictive tokens, showcasing a practical understanding of token embeddings in language modeling. <br><br>Summary: <div>
arXiv:2505.17282v1 Announce Type: new 
Abstract: Token embeddings play a crucial role in language modeling but, despite this practical relevance, their theoretical understanding remains limited. Our paper addresses the gap by characterizing the structure of embeddings obtained via gradient descent. Specifically, we consider a one-layer softmax attention model with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots, E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output vector. First, we show that, already after a single step of gradient training with the logistic loss, the embeddings $E_X$ capture the importance of tokens in the dataset by aligning with the output vector $v$ proportionally to the frequency with which the corresponding tokens appear in the dataset. Then, after training $p$ via gradient flow until convergence, the softmax selects the important tokens in the sentence (i.e., those that are predictive of the label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes the margin for such a selection. Experiments on real-world datasets (IMDB, Yelp) exhibit a phenomenology close to that unveiled by our theory.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Free Graph Data Selection under Distribution Shift</title>
<link>https://arxiv.org/abs/2505.17293</link>
<guid>https://arxiv.org/abs/2505.17293</guid>
<content:encoded><![CDATA[
<div> Graph domain adaptation, GDA, shift-robust, graph neural networks, GRADATE<br>
Summary: <br>
Graph domain adaptation (GDA) is a crucial task in graph machine learning, addressing distribution shifts with model-centric techniques like shift-robust graph neural networks (GNNs). However, these approaches can struggle with severe shifts and limited computational resources. To overcome these challenges, GRADATE, a model-free framework, has been proposed. GRADATE selects optimal training data from the source domain for classification on the target domain, utilizing optimal transport theory to adapt to distribution changes. It is data-efficient, scalable, and complements existing model-centric GDA methods. Empirical studies on real-world graph-level datasets show that GRADATE outperforms other selection methods and improves off-the-shelf GDA methods with fewer training data. <div>
arXiv:2505.17293v1 Announce Type: new 
Abstract: Graph domain adaptation (GDA) is a fundamental task in graph machine learning, with techniques like shift-robust graph neural networks (GNNs) and specialized training procedures to tackle the distribution shift problem. Although these model-centric approaches show promising results, they often struggle with severe shifts and constrained computational resources. To address these challenges, we propose a novel model-free framework, GRADATE (GRAph DATa sElector), that selects the best training data from the source domain for the classification task on the target domain. GRADATE picks training samples without relying on any GNN model's predictions or training recipes, leveraging optimal transport theory to capture and adapt to distribution changes. GRADATE is data-efficient, scalable and meanwhile complements existing model-centric GDA approaches. Through comprehensive empirical studies on several real-world graph-level datasets and multiple covariate shift types, we demonstrate that GRADATE outperforms existing selection methods and enhances off-the-shelf GDA methods with much fewer training data.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Regularization of Infinitesimally-perturbed Gradient Descent Toward Low-dimensional Solutions</title>
<link>https://arxiv.org/abs/2505.17304</link>
<guid>https://arxiv.org/abs/2505.17304</guid>
<content:encoded><![CDATA[
<div> implicit regularization, gradient-based methods, second-order stationary points, strict saddle points, infinitesimally perturbed gradient descent

Summary:
This paper investigates implicit regularization in optimization problems, particularly in over-parameterized scenarios. It explores the conditions necessary for gradient-based methods to converge to second-order stationary points in a low-dimensional region of a smooth, possibly nonconvex function. Key to successful implicit regularization are the efficient escape from strict saddle points and maintaining proximity to the implicit region. The study introduces infinitesimally perturbed gradient descent (IPGD), incorporating small perturbations to escape strict saddle points while staying close to the implicit region. By applying this framework to over-parameterized matrix sensing, the paper establishes formal guarantees for implicit regularization behavior of IPGD. Extensive experiments validate these findings across various learning problems.<br><br>Summary: <div>
arXiv:2505.17304v1 Announce Type: new 
Abstract: Implicit regularization refers to the phenomenon where local search algorithms converge to low-dimensional solutions, even when such structures are neither explicitly specified nor encoded in the optimization problem. While widely observed, this phenomenon remains theoretically underexplored, particularly in modern over-parameterized problems. In this paper, we study the conditions that enable implicit regularization by investigating when gradient-based methods converge to second-order stationary points (SOSPs) within an implicit low-dimensional region of a smooth, possibly nonconvex function. We show that successful implicit regularization hinges on two key conditions: $(i)$ the ability to efficiently escape strict saddle points, while $(ii)$ maintaining proximity to the implicit region. Existing analyses enabling the convergence of gradient descent (GD) to SOSPs often rely on injecting large perturbations to escape strict saddle points. However, this comes at the cost of deviating from the implicit region. The central premise of this paper is that it is possible to achieve the best of both worlds: efficiently escaping strict saddle points using infinitesimal perturbations, while controlling deviation from the implicit region via a small deviation rate. We show that infinitesimally perturbed gradient descent (IPGD), which can be interpreted as GD with inherent ``round-off errors'', can provably satisfy both conditions. We apply our framework to the problem of over-parameterized matrix sensing, where we establish formal guarantees for the implicit regularization behavior of IPGD. We further demonstrate through extensive experiments that these insights extend to a broader class of learning problems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wavelet Probabilistic Recurrent Convolutional Network for Multivariate Time Series Classification</title>
<link>https://arxiv.org/abs/2505.17307</link>
<guid>https://arxiv.org/abs/2505.17307</guid>
<content:encoded><![CDATA[
<div> Wavelet Probabilistic Recurrent Convolutional Network, Multivariate Time Series Classification, non-stationary environments, data scarcity, noise perturbations <br>
Summary:
The paper introduces the Wavelet Probabilistic Recurrent Convolutional Network (WPRCN) for Multivariate Time Series Classification (MTSC), focusing on dealing with non-stationary environments, data scarcity, and noise perturbations. This network includes a versatile wavelet probabilistic module with an Adaptive Wavelet Probabilistic Feature Generator (AWPG) and a Channel Attention-based Probabilistic Temporal Convolutional Network (APTCN). The AWPG creates a probabilistic model suited for varying data scarcities and non-stationarity, while the APTCN analyzes feature correlations to enhance classification in collaboration with existing MTSC models like Long Short-Term Memory (LSTM) and Causal Fully Convolutional Network (C-FCN). Evaluation on 30 datasets demonstrates WPRCN's superiority over benchmark algorithms in accuracy and robustness, particularly in scenarios with limited data and perturbations in physiological data. <br><br>Summary: <div>
arXiv:2505.17307v1 Announce Type: new 
Abstract: This paper presents a Wavelet Probabilistic Recurrent Convolutional Network (WPRCN) for Multivariate Time Series Classification (MTSC), especially effective in handling non-stationary environments, data scarcity and noise perturbations. We introduce a versatile wavelet probabilistic module designed to extract and analyse the probabilistic features, which can seamlessly integrate with a variety of neural network architectures. This probabilistic module comprises an Adaptive Wavelet Probabilistic Feature Generator (AWPG) and a Channel Attention-based Probabilistic Temporal Convolutional Network (APTCN). Such formulation extends the application of wavelet probabilistic neural networks to deep neural networks for MTSC. The AWPG constructs an ensemble probabilistic model addressing different data scarcities and non-stationarity; it adaptively selects the optimal ones and generates probabilistic features for APTCN. The APTCN analyses the correlations of the features and forms a comprehensive feature space with existing MTSC models for classification. Here, we instantiate the proposed module to work in parallel with a Long Short-Term Memory (LSTM) network and a Causal Fully Convolutional Network (C-FCN), demonstrating its broad applicability in time series analysis. The WPRCN is evaluated on 30 diverse MTS datasets and outperforms all the benchmark algorithms on average accuracy and rank, exhibiting pronounced strength in handling scarce data and physiological data subject to perturbations and non-stationarities.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training</title>
<link>https://arxiv.org/abs/2505.17331</link>
<guid>https://arxiv.org/abs/2505.17331</guid>
<content:encoded><![CDATA[
<div> Keywords: ECHO-LLaMA, LLaMA architecture, training speed, inference throughput, language performance

Summary: 
ECHO-LLaMA is an efficient LLaMA architecture that enhances training speed and inference throughput while maintaining learning capacity. It achieves up to 77% higher token-per-second throughput during training, up to 16% higher Model FLOPs Utilization (MFU), and up to 14% lower loss compared to baseline models. By transforming LLaMA models into shared KV caching, ECHO-LLaMA significantly reduces computational complexity without sacrificing language performance. Additionally, it provides approximately 7% higher test-time throughput on the 1.1B model. With a computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable and cost-effective solution for training large language models, enabling faster and more resource-efficient training. This innovation ensures that pretraining and finetuning tasks can be completed more efficiently without compromising performance.<br><br>Summary: <div>
arXiv:2505.17331v1 Announce Type: new 
Abstract: This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to improve both the training speed and inference throughput of LLaMA architectures while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models into shared KV caching across certain layers, significantly reducing KV computational complexity while maintaining or improving language performance. Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher token-per-second throughput during training, up to 16\% higher Model FLOPs Utilization (MFU), and up to 14\% lower loss when trained on an equal number of tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\% higher test-time throughput compared to the baseline. By introducing a computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable and cost-effective solution for pretraining and finetuning large language models, enabling faster and more resource-efficient training without compromising performance.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Predictive Distributions for Order Fulfillment Time Forecasting</title>
<link>https://arxiv.org/abs/2505.17340</link>
<guid>https://arxiv.org/abs/2505.17340</guid>
<content:encoded><![CDATA[
<div> machine learning, order fulfillment time, e-commerce logistics, distributional forecasting, Conformal Predictive Systems
Summary:
This paper presents a novel framework for accurate estimation of order fulfillment time in e-commerce logistics using machine learning techniques. It leverages Conformal Predictive Systems and Cross Venn-Abers Predictors to provide rigorous coverage guarantees. The framework integrates granular spatiotemporal features to capture fulfillment location and carrier performance dynamics, enhancing predictive accuracy. A cost-sensitive decision rule is developed to convert probabilistic forecasts into reliable point predictions. Experimental evaluation on a large-scale industrial dataset shows that the proposed methods generate competitive distributional forecasts and significantly outperform existing rule-based systems. The machine learning-based point predictions achieve up to 14% higher prediction accuracy and up to 75% improvement in identifying late deliveries.<br><br>Summary: <div>
arXiv:2505.17340v1 Announce Type: new 
Abstract: Accurate estimation of order fulfillment time is critical for e-commerce logistics, yet traditional rule-based approaches often fail to capture the inherent uncertainties in delivery operations. This paper introduces a novel framework for distributional forecasting of order fulfillment time, leveraging Conformal Predictive Systems and Cross Venn-Abers Predictors--model-agnostic techniques that provide rigorous coverage or validity guarantees. The proposed machine learning methods integrate granular spatiotemporal features, capturing fulfillment location and carrier performance dynamics to enhance predictive accuracy. Additionally, a cost-sensitive decision rule is developed to convert probabilistic forecasts into reliable point predictions. Experimental evaluation on a large-scale industrial dataset demonstrates that the proposed methods generate competitive distributional forecasts, while machine learning-based point predictions significantly outperform the existing rule-based system--achieving up to 14% higher prediction accuracy and up to 75% improvement in identifying late deliveries.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TI-DeepONet: Learnable Time Integration for Stable Long-Term Extrapolation</title>
<link>https://arxiv.org/abs/2505.17341</link>
<guid>https://arxiv.org/abs/2505.17341</guid>
<content:encoded><![CDATA[
<div> framework, neural operators, numerical time-stepping, dynamical systems, predictive accuracy <br>
Summary: <br>
The article introduces TI-DeepONet, a framework that combines neural operators with adaptive numerical time-stepping to improve accurate temporal extrapolation in dynamical systems modeling. By focusing on approximating instantaneous time-derivative fields and integrating them using numerical schemes, the approach supports continuous-time prediction and allows for the use of higher-precision integrators during inference. The TI(L)-DeepONet variant incorporates learnable coefficients for intermediate slopes to enhance prediction accuracy. Evaluation on three PDEs shows that both TI-DeepONet and TI(L)-DeepONet outperform traditional methods, reducing relative L2 extrapolation errors by approximately 81% and 70%, respectively. Notably, the models maintain stability in predictions for temporal domains twice the training interval. This research bridges neural approximation and numerical analysis, offering a physics-aware learning paradigm for dynamical systems. <br> <div>
arXiv:2505.17341v1 Announce Type: new 
Abstract: Accurate temporal extrapolation presents a fundamental challenge for neural operators in modeling dynamical systems, where reliable predictions must extend significantly beyond the training time horizon. Conventional Deep Operator Network (DeepONet) approaches employ two inherently limited training paradigms - fixed-horizon rollouts that predict complete spatiotemporal solutions while disregarding temporal causality, and autoregressive formulations that accumulate errors through sequential predictions. We introduce TI-DeepONet, a framework that integrates neural operators with adaptive numerical time-stepping techniques to preserve the Markovian structure of dynamical systems while mitigating error propagation in extended temporal forecasting. Our approach reformulates the learning objective from direct state prediction to the approximation of instantaneous time-derivative fields, which are then integrated using established numerical schemes. This architecture supports continuous-time prediction and enables deployment of higher-precision integrators during inference than those used during training, balancing computational efficiency with predictive accuracy. We further develop TI(L)-DeepONet, which incorporates learnable coefficients for intermediate slopes in the integration process, adapting to solution-specific variations and enhancing fidelity. Evaluation across three canonical PDEs shows that TI(L)-DeepONet marginally outperforms TI-DeepONet, with both reducing relative L2 extrapolation errors: approximately 81% over autoregressive and 70% over fixed-horizon methods. Notably, both maintain prediction stability for temporal domains extending to about twice the training interval. This research establishes a physics-aware operator learning paradigm that bridges neural approximation with numerical analysis while preserving the causal structure of dynamical systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety</title>
<link>https://arxiv.org/abs/2505.17342</link>
<guid>https://arxiv.org/abs/2505.17342</guid>
<content:encoded><![CDATA[
<div> Keywords: Safe Reinforcement Learning, Constrained Markov Decision Processes (CMDPs), Multi-Agent Safe RL (SafeMARL), policy gradient methods, safe exploration strategies 

Summary: 
Safe Reinforcement Learning (SafeRL) focuses on incorporating safety constraints into the learning and deployment of agents. This survey delves into SafeRL formulations based on Constrained Markov Decision Processes (CMDPs) and Multi-Agent Safe RL (SafeMARL). It covers the theoretical foundations of CMDPs, outlining definitions, constrained optimization techniques, and important theorems. The survey also discusses state-of-the-art algorithms for single agents, including policy gradient methods with safety guarantees and safe exploration strategies. Furthermore, it explores recent advances in SafeMARL for both cooperative and competitive scenarios. The survey concludes by proposing five open research problems to advance the field, with a particular emphasis on SafeMARL. These problems are presented with their motivations, key challenges, and relevant prior research. This comprehensive survey serves as a technical guide for researchers interested in exploring SafeRL and SafeMARL, offering insights into key concepts, methodologies, and potential future research directions. 

<br><br>Summary: <div>
arXiv:2505.17342v1 Announce Type: new 
Abstract: Safe Reinforcement Learning (SafeRL) is the subfield of reinforcement learning that explicitly deals with safety constraints during the learning and deployment of agents. This survey provides a mathematically rigorous overview of SafeRL formulations based on Constrained Markov Decision Processes (CMDPs) and extensions to Multi-Agent Safe RL (SafeMARL). We review theoretical foundations of CMDPs, covering definitions, constrained optimization techniques, and fundamental theorems. We then summarize state-of-the-art algorithms in SafeRL for single agents, including policy gradient methods with safety guarantees and safe exploration strategies, as well as recent advances in SafeMARL for cooperative and competitive settings. Additionally, we propose five open research problems to advance the field, with three focusing on SafeMARL. Each problem is described with motivation, key challenges, and related prior work. This survey is intended as a technical guide for researchers interested in SafeRL and SafeMARL, highlighting key concepts, methods, and open future research directions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction</title>
<link>https://arxiv.org/abs/2505.17344</link>
<guid>https://arxiv.org/abs/2505.17344</guid>
<content:encoded><![CDATA[
<div> Keywords: patient no-shows, machine learning, attention mechanisms, random forest, predictive modeling

Summary: 
The study introduces a new predictive model, the Multi-Head Attention Soft Random Forest (MHASRF), to address the issue of patient no-shows in healthcare settings. Patient no-shows can disrupt healthcare operations and resource allocation. The MHASRF model integrates attention mechanisms into a random forest model, allowing for soft splitting instead of hard decision splits. This enhances adaptability to specific patient behaviors. The model outperformed traditional models like decision trees, logistic regression, random forest, and naive Bayes with an accuracy of 93.56%. It also provides deeper insights into patient no-show predictors by identifying key features at both the tree and attention mechanism levels. The MHASRF model is robust, adaptable, and interpretable, offering healthcare providers a valuable tool to optimize resources and improve patient care.<br><br>Summary: <div>
arXiv:2505.17344v1 Announce Type: new 
Abstract: Unattended scheduled appointments, defined as patient no-shows, adversely affect both healthcare providers and patients' health, disrupting the continuity of care, operational efficiency, and the efficient allocation of medical resources. Accurate predictive modelling is needed to reduce the impact of no-shows. Although machine learning methods, such as logistic regression, random forest models, and decision trees, are widely used in predicting patient no-shows, they often rely on hard decision splits and static feature importance, limiting their adaptability to specific or complex patient behaviors. To address this limitation, we propose a new hybrid Multi-Head Attention Soft Random Forest (MHASRF) model that integrates attention mechanisms into a random forest model using probabilistic soft splitting instead of hard splitting. The MHASRF model assigns attention weights differently across the trees, enabling attention on specific patient behaviors. The model exhibited 93.56% accuracy, 93.67% precision, 93.56% recall, and a 93.59% F1 score, surpassing the performance of decision tree, logistic regression, random forest, and naive Bayes models. Furthermore, MHASRF was able to identify key predictors of patient no-shows using two levels of feature importance (tree level and attention mechanism level), offering deeper insights into patient no-show predictors. The proposed model is a robust, adaptable, and interpretable method for predicting patient no-shows that will help healthcare providers in optimizing resources.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems</title>
<link>https://arxiv.org/abs/2505.17351</link>
<guid>https://arxiv.org/abs/2505.17351</guid>
<content:encoded><![CDATA[
<div> Transformer, U-Net, convolutional ResNet, diffusion model, spatio-temporal modeling

Summary: 

The article introduces FLEX, a novel architecture for generative modeling of spatio-temporal physical systems using diffusion models. FLEX operates in the residual space instead of raw data, reducing velocity field variance and enhancing training stability. It combines a latent Transformer with a U-Net and convolutional ResNet layers, enabling capture of local spatial detail and long-range dependencies in latent space. FLEX leverages a task-specific encoder for improved spatio-temporal conditioning, incorporating weak conditioning for generalization and strong conditioning for reconstruction fidelity. It achieves accurate predictions for super-resolution and forecasting tasks with only two reverse diffusion steps and provides calibrated uncertainty estimates through sampling. Evaluation on high-resolution 2D turbulence data demonstrates FLEX's superior performance over strong baselines, showcasing its generalization to out-of-distribution settings. <div>
arXiv:2505.17351v1 Announce Type: new 
Abstract: We introduce FLEX (FLow EXpert), a backbone architecture for generative modeling of spatio-temporal physical systems using diffusion models. FLEX operates in the residual space rather than on raw data, a modeling choice that we motivate theoretically, showing that it reduces the variance of the velocity field in the diffusion model, which helps stabilize training. FLEX integrates a latent Transformer into a U-Net with standard convolutional ResNet layers and incorporates a redesigned skip connection scheme. This hybrid design enables the model to capture both local spatial detail and long-range dependencies in latent space. To improve spatio-temporal conditioning, FLEX uses a task-specific encoder that processes auxiliary inputs such as coarse or past snapshots. Weak conditioning is applied to the shared encoder via skip connections to promote generalization, while strong conditioning is applied to the decoder through both skip and bottleneck features to ensure reconstruction fidelity. FLEX achieves accurate predictions for super-resolution and forecasting tasks using as few as two reverse diffusion steps. It also produces calibrated uncertainty estimates through sampling. Evaluations on high-resolution 2D turbulence data show that FLEX outperforms strong baselines and generalizes to out-of-distribution settings, including unseen Reynolds numbers, physical observables (e.g., fluid flow velocity fields), and boundary conditions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CT-OT Flow: Estimating Continuous-Time Dynamics from Discrete Temporal Snapshots</title>
<link>https://arxiv.org/abs/2505.17354</link>
<guid>https://arxiv.org/abs/2505.17354</guid>
<content:encoded><![CDATA[
<div> method, continuous-time dynamics, optimal transport, data reconstruction, temporal discretization<br>
Summary:<br>
The article introduces Continuous-Time Optimal Transport Flow (CT-OT Flow) to recover continuous-time dynamics from discrete-time snapshots with noisy timestamps. This method involves inferring high-resolution time labels using partial optimal transport and reconstructing continuous-time data distribution through temporal kernel smoothing. The reconstructed data distribution enables accurate training of dynamics models like ODEs and SDEs. CT-OT Flow outperforms other existing methods on synthetic benchmarks and achieves lower reconstruction errors on real datasets like scRNA-seq and typhoon-track datasets. The results emphasize the importance of explicitly considering temporal discretization and timestamp uncertainty in accurately modeling discrete snapshots and continuous-time processes. <br>Summary: <div>
arXiv:2505.17354v1 Announce Type: new 
Abstract: In many real-world scenarios, such as single-cell RNA sequencing, data are observed only as discrete-time snapshots spanning finite time intervals and subject to noisy timestamps, with no continuous trajectories available. Recovering the underlying continuous-time dynamics from these snapshots with coarse and noisy observation times is a critical and challenging task. We propose Continuous-Time Optimal Transport Flow (CT-OT Flow), which first infers high-resolution time labels via partial optimal transport and then reconstructs a continuous-time data distribution through a temporal kernel smoothing. This reconstruction enables accurate training of dynamics models such as ODEs and SDEs. CT-OT Flow consistently outperforms state-of-the-art methods on synthetic benchmarks and achieves lower reconstruction errors on real scRNA-seq and typhoon-track datasets. Our results highlight the benefits of explicitly modeling temporal discretization and timestamp uncertainty, offering an accurate and general framework for bridging discrete snapshots and continuous-time processes.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Robustness of Nonparametric Regression</title>
<link>https://arxiv.org/abs/2505.17356</link>
<guid>https://arxiv.org/abs/2505.17356</guid>
<content:encoded><![CDATA[
<div> adversarial robustness, regression, nonparametric, minimax lower bound, smoothing spline<br>
<br>
Summary:<br>
This paper investigates the adversarial robustness of nonparametric regression in the presence of arbitrarily corrupted input data. The study focuses on the second-order Sobolev space, with two main contributions. Firstly, a minimax lower bound on estimation error is established, revealing an inherent limit for all estimators. Secondly, it is shown that the properly regularized smoothing spline estimator displays robustness against adversarial corruption, with its estimation error diminishing as the number of corrupted samples approaches zero. However, when a constant fraction of data is corrupted, no estimator can ensure vanishing estimation error, highlighting the effectiveness of the smoothing spline in tolerating a maximal number of corrupted samples. <div>
arXiv:2505.17356v1 Announce Type: new 
Abstract: In this paper, we investigate the adversarial robustness of regression, a fundamental problem in machine learning, under the setting where an adversary can arbitrarily corrupt a subset of the input data. While the robustness of parametric regression has been extensively studied, its nonparametric counterpart remains largely unexplored. We characterize the adversarial robustness in nonparametric regression, assuming the regression function belongs to the second-order Sobolev space (i.e., it is square integrable up to its second derivative).
  The contribution of this paper is two-fold: (i) we establish a minimax lower bound on the estimation error, revealing a fundamental limit that no estimator can overcome, and (ii) we show that, perhaps surprisingly, the classical smoothing spline estimator, when properly regularized, exhibits robustness against adversarial corruption. These results imply that if $o(n)$ out of $n$ samples are corrupted, the estimation error of the smoothing spline vanishes as $n \to \infty$. On the other hand, when a constant fraction of the data is corrupted, no estimator can guarantee vanishing estimation error, implying the optimality of the smoothing spline in terms of maximum tolerable number of corrupted samples.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction</title>
<link>https://arxiv.org/abs/2505.17357</link>
<guid>https://arxiv.org/abs/2505.17357</guid>
<content:encoded><![CDATA[
<div> Keywords: IoT-based botnet attacks, Graph Neural Networks (GNNs), attention mechanisms, dimension reduction techniques, botnet attack detection

Summary:
In the realm of IoT-based botnet attack detection, researchers have been exploring different learning models, including traditional machine learning, deep learning, and hybrid approaches. One key development involves the use of attention mechanisms to capture long-term dependencies among features, improving detection accuracy significantly. Graph Neural Networks (GNNs) offer a solution to the oversight of inter-instance relationships in most models by learning an embedding space through iterative message passing. To enhance classification performance, attention mechanisms have been incorporated within GNNs to leverage long-range dependencies and inter-instance connections. However, transforming high-dimensional IoT attack datasets into graph structures poses challenges due to the computational overhead. To address this, a framework is proposed in this paper that reduces the dimensionality of NetFlow-based IoT attack datasets before transforming them into graph datasets. Three dimension reduction techniques are evaluated--Variational Autoencoder (VAE-encoder), classical autoencoder (AE-encoder), and Principal Component Analysis (PCA)--and their impact on a Graph Attention neural network (GAT) model for botnet attack detection is compared. 

<br><br>Summary: <div>
arXiv:2505.17357v1 Announce Type: new 
Abstract: With the rise of IoT-based botnet attacks, researchers have explored various learning models for detection, including traditional machine learning, deep learning, and hybrid approaches. A key advancement involves deploying attention mechanisms to capture long-term dependencies among features, significantly improving detection accuracy. However, most models treat attack instances independently, overlooking inter-instance relationships. Graph Neural Networks (GNNs) address this limitation by learning an embedding space via iterative message passing where similar instances are placed closer based on node features and relationships, enhancing classification performance. To further improve detection, attention mechanisms have been embedded within GNNs, leveraging both long-range dependencies and inter-instance connections. However, transforming the high dimensional IoT attack datasets into a graph structured dataset poses challenges, such as large graph structures leading computational overhead. To mitigate this, this paper proposes a framework that first reduces dimensionality of the NetFlow-based IoT attack dataset before transforming it into a graph dataset. We evaluate three dimension reduction techniques--Variational Autoencoder (VAE-encoder), classical autoencoder (AE-encoder), and Principal Component Analysis (PCA)--and compare their effects on a Graph Attention neural network (GAT) model for botnet attack detection
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards VM Rescheduling Optimization Through Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17359</link>
<guid>https://arxiv.org/abs/2505.17359</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, virtual machine rescheduling, data centers, performance optimization, open-source<br>
Summary:<br>
The article introduces VM2RL, a reinforcement learning system for virtual machine (VM) rescheduling in data centers. Existing VM rescheduling algorithms are hindered by long inference times due to dynamic VM state changes, impacting performance as data centers grow. VM2RL addresses this challenge with a two-stage framework accommodating diverse constraints and workload conditions. It includes a feature extraction module capturing relational information specific to rescheduling and a risk-seeking evaluation to optimize the trade-off between latency and accuracy. Extensive experiments with industry-scale data center data demonstrate that VM2RL can achieve performance comparable to the optimal solution in a matter of seconds. The code and datasets are open-sourced for further research and development. <div>
arXiv:2505.17359v1 Announce Type: new 
Abstract: Modern industry-scale data centers need to manage a large number of virtual machines (VMs). Due to the continual creation and release of VMs, many small resource fragments are scattered across physical machines (PMs). To handle these fragments, data centers periodically reschedule some VMs to alternative PMs, a practice commonly referred to as VM rescheduling. Despite the increasing importance of VM rescheduling as data centers grow in size, the problem remains understudied. We first show that, unlike most combinatorial optimization tasks, the inference time of VM rescheduling algorithms significantly influences their performance, due to dynamic VM state changes during this period. This causes existing methods to scale poorly. Therefore, we develop a reinforcement learning system for VM rescheduling, VM2RL, which incorporates a set of customized techniques, such as a two-stage framework that accommodates diverse constraints and workload conditions, a feature extraction module that captures relational information specific to rescheduling, as well as a risk-seeking evaluation enabling users to optimize the trade-off between latency and accuracy. We conduct extensive experiments with data from an industry-scale data center. Our results show that VM2RL can achieve a performance comparable to the optimal solution but with a running time of seconds. Code and datasets are open-sourced: https://github.com/zhykoties/VMR2L_eurosys, https://drive.google.com/drive/folders/1PfRo1cVwuhH30XhsE2Np3xqJn2GpX5qy.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved and Oracle-Efficient Online $\ell_1$-Multicalibration</title>
<link>https://arxiv.org/abs/2505.17365</link>
<guid>https://arxiv.org/abs/2505.17365</guid>
<content:encoded><![CDATA[
<div> online multicalibration, framework, adversarial settings, linear-product optimization, oracle-efficient<br>
<br>
Summary:<br>
The study introduces a framework for online multicalibration to ensure calibrated predictions across multiple groups in adversarial scenarios over T rounds. Unlike previous approaches, this study directly addresses online multicalibration in the ℓ1 norm, achieving improved rates of Õ(T^-1/3) and Õ(T^-1/4) for online ℓ1-multicalibration. The novel method involves a reduction of online ℓ1-multicalibration to online linear-product optimization (OLPO) and introduces a linearization technique combined with a no-regret algorithm to achieve the desired sublinear rate. To enhance scalability, an oracle-efficient approach to OLPO is proposed, making a polynomial number of calls to an offline optimization oracle. The framework's applicability extends to infinite group families by leveraging the Lipschitz property of the ℓ1-multicalibration error with respect to the group family. <div>
arXiv:2505.17365v1 Announce Type: new 
Abstract: We study \emph{online multicalibration}, a framework for ensuring calibrated predictions across multiple groups in adversarial settings, across $T$ rounds. Although online calibration is typically studied in the $\ell_1$ norm, prior approaches to online multicalibration have taken the indirect approach of obtaining rates in other norms (such as $\ell_2$ and $\ell_{\infty}$) and then transferred these guarantees to $\ell_1$ at additional loss. In contrast, we propose a direct method that achieves improved and oracle-efficient rates of $\widetilde{\mathcal{O}}(T^{-1/3})$ and $\widetilde{\mathcal{O}}(T^{-1/4})$ respectively, for online $\ell_1$-multicalibration. Our key insight is a novel reduction of online \(\ell_1\)-multicalibration to an online learning problem with product-based rewards, which we refer to as \emph{online linear-product optimization} ($\mathtt{OLPO}$).
  To obtain the improved rate of $\widetilde{\mathcal{O}}(T^{-1/3})$, we introduce a linearization of $\mathtt{OLPO}$ and design a no-regret algorithm for this linearized problem. Although this method guarantees the desired sublinear rate (nearly matching the best rate for online calibration), it becomes computationally expensive when the group family \(\mathcal{H}\) is large or infinite, since it enumerates all possible groups. To address scalability, we propose a second approach to $\mathtt{OLPO}$ that makes only a polynomial number of calls to an offline optimization (\emph{multicalibration evaluation}) oracle, resulting in \emph{oracle-efficient} online \(\ell_1\)-multicalibration with a rate of $\widetilde{\mathcal{O}}(T^{-1/4})$. Our framework also extends to certain infinite families of groups (e.g., all linear functions on the context space) by exploiting a $1$-Lipschitz property of the \(\ell_1\)-multicalibration error with respect to \(\mathcal{H}\).
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIREN: Beyond Trajectories -- A Spectral Lens on Time</title>
<link>https://arxiv.org/abs/2505.17370</link>
<guid>https://arxiv.org/abs/2505.17370</guid>
<content:encoded><![CDATA[
<div> latent representation, Wasserstein-2 distance, spectral analysis, long-term time-series forecasting, chaotic systems

Summary: 
The study introduces FRIREN, a new model for long-term time-series forecasting that focuses on geometric structures rather than pointwise prediction. FRIREN utilizes a normalizing flow block to embed data into a normally distributed latent representation, allowing for efficient prediction that preserves geometric changes. By minimizing the Wasserstein-2 distance, FRIREN achieves accurate and interpretable forecasts for chaotic systems like Lorenz-63 and Rossler. The model also provides a spectral view of dynamics, enabling practitioners to analyze mode behavior locally and system-wide. FRIREN outperforms existing models like TimeMixer on forecasting accuracy for various datasets, showcasing its effectiveness in long-horizon predictions. The connection of modern generative flows with spectral analysis sets a new benchmark for LTSF model design. <div>
arXiv:2505.17370v1 Announce Type: new 
Abstract: Long-term time-series forecasting (LTSF) models are often presented as general-purpose solutions that can be applied across domains, implicitly assuming that all data is pointwise predictable. Using chaotic systems such as Lorenz-63 as a case study, we argue that geometric structure - not pointwise prediction - is the right abstraction for a dynamic-agnostic foundational model. Minimizing the Wasserstein-2 distance (W2), which captures geometric changes, and providing a spectral view of dynamics are essential for long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via Interpretable Eigen-networks), implements an augmented normalizing-flow block that embeds data into a normally distributed latent representation. It then generates a W2-efficient optimal path that can be decomposed into rotation, scaling, inverse rotation, and translation. This architecture yields locally generated, geometry-preserving predictions that are independent of the underlying dynamics, and a global spectral representation that functions as a finite Koopman operator with a small modification. This enables practitioners to identify which modes grow, decay, or oscillate, both locally and system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE 27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out), FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170, outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065. FRIREN is also competitive on standard LTSF datasets such as ETT and Weather. By connecting modern generative flows with classical spectral analysis, FRIREN makes long-term forecasting both accurate and interpretable, setting a new benchmark for LTSF model design.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End-to-End Approach for Child Reading Assessment in the Xhosa Language</title>
<link>https://arxiv.org/abs/2505.17371</link>
<guid>https://arxiv.org/abs/2505.17371</guid>
<content:encoded><![CDATA[
<div> Keywords: Child literacy, Reading assessments, Automatic reading assessment systems, Xhosa language, End-to-end models

Summary: 
Child literacy is a crucial predictor of future life outcomes, especially in vulnerable populations. Targeted interventions are needed to bridge the gap between literacy levels in low-income areas and higher-income regions. The development of automatic reading assessment systems using AI can support educators in evaluating the effectiveness of literacy programs. This study focuses on Xhosa, a South African language, to improve child speech recognition capabilities. A novel dataset of Xhosa child speech samples is created, with each recording labeled using a cost-effective approach. Three state-of-the-art end-to-end models are evaluated on the dataset, showing the importance of training data amount and balance for optimal performance. The wav2vec 2.0 model benefits from training on multiple classes simultaneously, even with limited samples available.

<br><br>Summary: <div>
arXiv:2505.17371v1 Announce Type: new 
Abstract: Child literacy is a strong predictor of life outcomes at the subsequent stages of an individual's life. This points to a need for targeted interventions in vulnerable low and middle income populations to help bridge the gap between literacy levels in these regions and high income ones. In this effort, reading assessments provide an important tool to measure the effectiveness of these programs and AI can be a reliable and economical tool to support educators with this task. Developing accurate automatic reading assessment systems for child speech in low-resource languages poses significant challenges due to limited data and the unique acoustic properties of children's voices. This study focuses on Xhosa, a language spoken in South Africa, to advance child speech recognition capabilities. We present a novel dataset composed of child speech samples in Xhosa. The dataset is available upon request and contains ten words and letters, which are part of the Early Grade Reading Assessment (EGRA) system. Each recording is labeled with an online and cost-effective approach by multiple markers and a subsample is validated by an independent EGRA reviewer. This dataset is evaluated with three fine-tuned state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The results indicate that the performance of these models can be significantly influenced by the amount and balancing of the available training data, which is fundamental for cost-effective large dataset collection. Furthermore, our experiments indicate that the wav2vec 2.0 performance is improved by training on multiple classes at a time, even when the number of available samples is constrained.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value-Guided Search for Efficient Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.17373</link>
<guid>https://arxiv.org/abs/2505.17373</guid>
<content:encoded><![CDATA[
<div> Method, Value model training, Long-context reasoning traces, Dataset, DeepSeek models

Summary: 
- The paper proposes a new method for training value models on long-context reasoning traces without the need for a fine-grained notion of "step."
- A dataset of 2.5 million reasoning traces is collected to train a 1.5B token-level value model, enhancing DeepSeek models' performance with test-time compute scaling.
- Block-wise value-guided search (VGS) with weighted majority voting outperforms standard methods like majority voting or best-of-n, achieving an average accuracy of 45.7% across four competition math benchmarks with an inference budget of 64 generations.
- VGS with DeepSeek-R1-Distill-1.5B reaches parity with o3-mini-medium while significantly reducing the needed inference FLOPs compared to majority voting.
- The dataset, model, and codebase are openly available for further research and development. 

<br><br>Summary: <div>
arXiv:2505.17373v1 Announce Type: new 
Abstract: In this paper, we propose a simple and efficient method for value model training on long-context reasoning traces. Compared to existing process reward models (PRMs), our method does not require a fine-grained notion of "step," which is difficult to define for long-context reasoning models. By collecting a dataset of 2.5 million reasoning traces, we train a 1.5B token-level value model and apply it to DeepSeek models for improved performance with test-time compute scaling. We find that block-wise value-guided search (VGS) with a final weighted majority vote achieves better test-time scaling than standard methods such as majority voting or best-of-n. With an inference budget of 64 generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of 45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024 & 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly reduces the inference FLOPs required to achieve the same performance of majority voting. Our dataset, model and codebase are open-sourced.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Efficient Algorithm for Best Scoring Rule Identification in Online Principal-Agent Information Acquisition</title>
<link>https://arxiv.org/abs/2505.17379</link>
<guid>https://arxiv.org/abs/2505.17379</guid>
<content:encoded><![CDATA[
<div> Algorithm, Scoring rule, Principal-agent framework, Information acquisition, Sample complexity 
<br>
Summary: 
The article focuses on determining the optimal scoring rule in the principal-agent framework for online information acquisition. Two algorithms, OIAFC and OIAFB, are proposed for fixed confidence and fixed budget settings, respectively. OIAFC can identify the desired $(\epsilon, \delta)$-scoring rule with efficient sample complexity, while OIAFB matches its performance but operates in both settings. The study shows that both algorithms have the same complexity level across fixed confidence and fixed budget scenarios. <div>
arXiv:2505.17379v1 Announce Type: new 
Abstract: We investigate the problem of identifying the optimal scoring rule within the principal-agent framework for online information acquisition problem. We focus on the principal's perspective, seeking to determine the desired scoring rule through interactions with the agent. To address this challenge, we propose two algorithms: OIAFC and OIAFB, tailored for fixed confidence and fixed budget settings, respectively. Our theoretical analysis demonstrates that OIAFC can extract the desired $(\epsilon, \delta)$-scoring rule with a efficient instance-dependent sample complexity or an instance-independent sample complexity. Our analysis also shows that OIAFB matches the instance-independent performance bound of OIAFC, while both algorithms share the same complexity across fixed confidence and fixed budget settings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling</title>
<link>https://arxiv.org/abs/2505.17384</link>
<guid>https://arxiv.org/abs/2505.17384</guid>
<content:encoded><![CDATA[
<div> Variational Autoencoding Discrete Diffusion, masked diffusion models, latent variable modeling, inter-dimensional dependencies, sample quality<br>
Summary:<br>
Discrete diffusion models have proven effective in modeling complex discrete data, with masked diffusion models (MDMs) offering a balance between quality and speed. However, the performance of MDMs can degrade with fewer denoising steps due to limited modeling of inter-dimensional dependencies. The proposed Variational Autoencoding Discrete Diffusion (VADD) framework enhances discrete diffusion models by incorporating latent variable modeling to capture correlations among dimensions. VADD introduces an auxiliary recognition model for stable training through variational lower bounds maximization and amortized inference. Empirical results across 2D toy data, pixel-level image generation, and text generation show that VADD outperforms MDM baselines in sample quality, particularly with a small number of denoising steps.<br>
Summary: <div>
arXiv:2505.17384v1 Announce Type: new 
Abstract: Discrete diffusion models have recently shown great promise for modeling complex discrete data, with masked diffusion models (MDMs) offering a compelling trade-off between quality and generation speed. MDMs denoise by progressively unmasking multiple dimensions from an all-masked input, but their performance can degrade when using few denoising steps due to limited modeling of inter-dimensional dependencies. In this paper, we propose Variational Autoencoding Discrete Diffusion (VADD), a novel framework that enhances discrete diffusion with latent variable modeling to implicitly capture correlations among dimensions. By introducing an auxiliary recognition model, VADD enables stable training via variational lower bounds maximization and amortized inference over the training set. Our approach retains the efficiency of traditional MDMs while significantly improving sample quality, especially when the number of denoising steps is small. Empirical results on 2D toy data, pixel-level image generation, and text generation demonstrate that VADD consistently outperforms MDM baselines.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Mixture Kernels for Bayesian Optimization</title>
<link>https://arxiv.org/abs/2505.17393</link>
<guid>https://arxiv.org/abs/2505.17393</guid>
<content:encoded><![CDATA[
<div> Gaussian Process, Bayesian Optimization, Surrogate Model, Spectral Mixture Kernels, Black-Box Optimization

Summary: 
The article introduces a novel Gaussian Process-based Bayesian Optimization (BO) method that incorporates spectral mixture kernels derived from scale-location mixtures of Cauchy and Gaussian distributions. This approach significantly improves efficiency and optimization performance, matching simpler kernels in computational speed while surpassing more complex models and automatic BO methods in results. The authors provide bounds on information gain and cumulative regret related to finding the optimum. Extensive numerical experiments demonstrate the method's superiority over existing baselines in various synthetic and real-world problems across low- and high-dimensional settings. <div>
arXiv:2505.17393v1 Announce Type: new 
Abstract: Bayesian Optimization (BO) is a widely used approach for solving expensive black-box optimization tasks. However, selecting an appropriate probabilistic surrogate model remains an important yet challenging problem. In this work, we introduce a novel Gaussian Process (GP)-based BO method that incorporates spectral mixture kernels, derived from spectral densities formed by scale-location mixtures of Cauchy and Gaussian distributions. This method achieves a significant improvement in both efficiency and optimization performance, matching the computational speed of simpler kernels while delivering results that outperform more complex models and automatic BO methods. We provide bounds on the information gain and cumulative regret associated with obtaining the optimum. Extensive numerical experiments demonstrate that our method consistently outperforms existing baselines across a diverse range of synthetic and real-world problems, including both low- and high-dimensional settings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wasserstein Transfer Learning</title>
<link>https://arxiv.org/abs/2505.17404</link>
<guid>https://arxiv.org/abs/2505.17404</guid>
<content:encoded><![CDATA[
<div> probability distributions, transfer learning, regression models, Wasserstein space, domain similarity

Summary:
This article introduces a novel framework for transfer learning in regression models, focusing on probability distributions in the Wasserstein space. It addresses the limitations of traditional transfer learning approaches that deal with scalar or multivariate data in Euclidean spaces. The proposed estimator for known informative source domains demonstrates provable asymptotic convergence rates, quantifying the impact of domain similarity on transfer efficiency. Additionally, a data-driven transfer learning procedure is developed for cases where the informative subset is unknown, aiming to mitigate negative transfer. The methods are supported by rigorous theoretical analysis and validated through simulations and real-world applications. This research contributes to enhancing learning in target domains by leveraging knowledge from source domains through the transfer of probability distributions. <div>
arXiv:2505.17404v1 Announce Type: new 
Abstract: Transfer learning is a powerful paradigm for leveraging knowledge from source domains to enhance learning in a target domain. However, traditional transfer learning approaches often focus on scalar or multivariate data within Euclidean spaces, limiting their applicability to complex data structures such as probability distributions. To address this, we introduce a novel framework for transfer learning in regression models, where outputs are probability distributions residing in the Wasserstein space. When the informative subset of transferable source domains is known, we propose an estimator with provable asymptotic convergence rates, quantifying the impact of domain similarity on transfer efficiency. For cases where the informative subset is unknown, we develop a data-driven transfer learning procedure designed to mitigate negative transfer. The proposed methods are supported by rigorous theoretical analysis and are validated through extensive simulations and real-world applications.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperIMTS: Hypergraph Neural Network for Irregular Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.17431</link>
<guid>https://arxiv.org/abs/2505.17431</guid>
<content:encoded><![CDATA[
<div> Keywords: Irregular multivariate time series, Hypergraph neural network, Forecasting, Variable dependencies, Message passing

Summary:
HyperIMTS is a novel approach for forecasting irregular multivariate time series data. It utilizes a Hypergraph neural network to capture dependencies among unaligned observations across variables. This model represents observed values as nodes in a hypergraph, interconnected by temporal and variable hyperedges for efficient message passing. HyperIMTS adapts to the irregular time intervals within variables, allowing for accurate forecasting without the need for padded samples. By leveraging irregularity-aware message passing, HyperIMTS successfully captures variable dependencies in a time-adaptive manner. Experimental results demonstrate the competitive performance of HyperIMTS compared to existing models in IMTS forecasting, all while maintaining low computational costs. Overall, HyperIMTS provides a unified framework for learning temporal and variable dependencies from original observations in irregular multivariate time series data.<br><br>Summary: <div>
arXiv:2505.17431v1 Announce Type: new 
Abstract: Irregular multivariate time series (IMTS) are characterized by irregular time intervals within variables and unaligned observations across variables, posing challenges in learning temporal and variable dependencies. Many existing IMTS models either require padded samples to learn separately from temporal and variable dimensions, or represent original samples via bipartite graphs or sets. However, the former approaches often need to handle extra padding values affecting efficiency and disrupting original sampling patterns, while the latter ones have limitations in capturing dependencies among unaligned observations. To represent and learn both dependencies from original observations in a unified form, we propose HyperIMTS, a Hypergraph neural network for Irregular Multivariate Time Series forecasting. Observed values are converted as nodes in the hypergraph, interconnected by temporal and variable hyperedges to enable message passing among all observations. Through irregularity-aware message passing, HyperIMTS captures variable dependencies in a time-adaptive way to achieve accurate forecasting. Experiments demonstrate HyperIMTS's competitive performance among state-of-the-art models in IMTS forecasting with low computational cost.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discretization-free Multicalibration through Loss Minimization over Tree Ensembles</title>
<link>https://arxiv.org/abs/2505.17435</link>
<guid>https://arxiv.org/abs/2505.17435</guid>
<content:encoded><![CDATA[
<div> multicalibration, learning objective, predictor, empirical risk minimization, decision trees 

Summary:
This article introduces a novel discretization-free method for achieving multicalibration in predictors across diverse subpopulations. Unlike existing approaches that rely on discretization, this method directly optimizes an empirical risk objective using an ensemble of depth-two decision trees, enabling implementation with common tree ensemble learning methods. The algorithm ensures multicalibration under a condition termed loss saturation, which is found to hold in practical datasets. Empirical evaluations demonstrate that the proposed algorithm consistently matches or surpasses existing methods, even when evaluated using comparable metrics. This approach eliminates the need for discretization, avoiding associated issues such as rounding error and hyperparameter sensitivity, and allows for improved downstream decision-making. <div>
arXiv:2505.17435v1 Announce Type: new 
Abstract: In recent years, multicalibration has emerged as a desirable learning objective for ensuring that a predictor is calibrated across a rich collection of overlapping subpopulations. Existing approaches typically achieve multicalibration by discretizing the predictor's output space and iteratively adjusting its output values. However, this discretization approach departs from the standard empirical risk minimization (ERM) pipeline, introduces rounding error and additional sensitive hyperparameter, and may distort the predictor's outputs in ways that hinder downstream decision-making.
  In this work, we propose a discretization-free multicalibration method that directly optimizes an empirical risk objective over an ensemble of depth-two decision trees. Our ERM approach can be implemented using off-the-shelf tree ensemble learning methods such as LightGBM. Our algorithm provably achieves multicalibration, provided that the data distribution satisfies a technical condition we term as loss saturation. Across multiple datasets, our empirical evaluation shows that this condition is always met in practice. Our discretization-free algorithm consistently matches or outperforms existing multicalibration approaches--even when evaluated using a discretization-based multicalibration metric that shares its discretization granularity with the baselines.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning</title>
<link>https://arxiv.org/abs/2505.17439</link>
<guid>https://arxiv.org/abs/2505.17439</guid>
<content:encoded><![CDATA[
<div> Keywords: humanitarian supply chain, reinforcement learning, PPO, heuristic algorithms, satisfaction rate

Summary:
This study introduces a novel approach to designing a dynamic and efficient humanitarian supply chain using reinforcement learning, specifically Proximal Policy Optimization (PPO). The model constructed with PPO prioritizes optimizing the average satisfaction rate in an equitable manner. By comparing the performance of PPO with traditional heuristic algorithms, the study demonstrates the superiority of the reinforcement learning-based approach in achieving higher levels of satisfaction and efficiency in humanitarian supply chain management. This research contributes to enhancing the responsiveness and effectiveness of humanitarian operations by leveraging state-of-the-art machine learning techniques to optimize decision-making processes. <div>
arXiv:2505.17439v1 Announce Type: new 
Abstract: This study designs an efficient and equitable humanitarian supply chain dynamically by using reinforcement learning, PPO, and compared with heuristic algorithms. This study demonstrates the model of PPO always treats average satisfaction rate as the priority.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning</title>
<link>https://arxiv.org/abs/2505.17448</link>
<guid>https://arxiv.org/abs/2505.17448</guid>
<content:encoded><![CDATA[
<div> algorithm, clickbait, YouTube, deep learning, video

Summary:
The study introduces an algorithm called BaitRadar to tackle the issue of clickbait on YouTube. Clickbait entices users to click on videos through appealing titles and thumbnails, misleading them about the video content. BaitRadar employs six inference models that analyze different attributes of a video, such as title, comments, thumbnail, tags, video statistics, and audio transcript. By considering multiple models and averaging their outputs, BaitRadar enhances classification accuracy, even in cases of missing data. Testing on 1,400 YouTube videos demonstrates an average test accuracy of 98% with an inference time of less than 2 seconds. BaitRadar aims to combat clickbait and ensure users receive accurate video content based on the information presented. 

<br><br>Summary: <div>
arXiv:2505.17448v1 Announce Type: new 
Abstract: Following the rising popularity of YouTube, there is an emerging problem on this platform called clickbait, which provokes users to click on videos using attractive titles and thumbnails. As a result, users ended up watching a video that does not have the content as publicized in the title. This issue is addressed in this study by proposing an algorithm called BaitRadar, which uses a deep learning technique where six inference models are jointly consulted to make the final classification decision. These models focus on different attributes of the video, including title, comments, thumbnail, tags, video statistics and audio transcript. The final classification is attained by computing the average of multiple models to provide a robust and accurate output even in situation where there is missing data. The proposed method is tested on 1,400 YouTube videos. On average, a test accuracy of 98% is achieved with an inference time of less than 2s.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIMB: Class-imbalanced Learning Benchmark on Tabular Data</title>
<link>https://arxiv.org/abs/2505.17451</link>
<guid>https://arxiv.org/abs/2505.17451</guid>
<content:encoded><![CDATA[
<div> benchmark, class-imbalanced learning, tabular data, algorithms, Python package

Summary: 
The paper introduces CLIMB, a benchmark for class-imbalanced learning on tabular data, comprising 73 real-world datasets and 29 CIL algorithms. The benchmark aims to address the imbalance in critical yet rare outcomes in various domains. The CLIMB includes a high-quality open-source Python package with standardized API designs, documentation, and code quality controls. Through extensive experiments, the paper provides insights on method accuracy and efficiency, emphasizing the limitations of simple rebalancing methods, the effectiveness of ensembles, and the significance of data quality. The comprehensive benchmark and experiments aim to facilitate the implementation and comparison of CIL algorithms for different imbalanced learning tasks. The code, documentation, and examples of CLIMB can be accessed at https://github.com/ZhiningLiu1998/imbalanced-ensemble. <div>
arXiv:2505.17451v1 Announce Type: new 
Abstract: Class-imbalanced learning (CIL) on tabular data is important in many real-world applications where the minority class holds the critical but rare outcomes. In this paper, we present CLIMB, a comprehensive benchmark for class-imbalanced learning on tabular data. CLIMB includes 73 real-world datasets across diverse domains and imbalance levels, along with unified implementations of 29 representative CIL algorithms. Built on a high-quality open-source Python package with unified API designs, detailed documentation, and rigorous code quality controls, CLIMB supports easy implementation and comparison between different CIL algorithms. Through extensive experiments, we provide practical insights on method accuracy and efficiency, highlighting the limitations of naive rebalancing, the effectiveness of ensembles, and the importance of data quality. Our code, documentation, and examples are available at https://github.com/ZhiningLiu1998/imbalanced-ensemble.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Training Large Language Models with Confident Reasoning</title>
<link>https://arxiv.org/abs/2505.17454</link>
<guid>https://arxiv.org/abs/2505.17454</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, self-training, reasoning-level confidence, CORE-PO, Policy Optimization

Summary:
Large language models (LLMs) have shown impressive performance in generating reasoning paths, but the high cost of human supervision for learning these paths is a challenge. Recent studies have explored self-training methods to improve reasoning capabilities using pseudo-labels generated by LLMs. However, existing methods focus solely on the quality of final answers and may overlook the quality of reasoning paths. This article proposes a new self-training method, CORE-PO, which prioritizes high-confidence reasoning paths through Policy Optimization. Experimental results demonstrate that CORE-PO enhances the accuracy of outputs on various benchmarks, including in-distribution and out-of-distribution datasets. By considering reasoning-level confidence, CORE-PO improves the overall performance of LLMs in generating high-quality reasoning paths. <div>
arXiv:2505.17454v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown impressive performance by generating reasoning paths before final answers, but learning such a reasoning path requires costly human supervision. To address this issue, recent studies have explored self-training methods that improve reasoning capabilities using pseudo-labels generated by the LLMs themselves. Among these, confidence-based self-training fine-tunes LLMs to prefer reasoning paths with high-confidence answers, where confidence is estimated via majority voting. However, such methods exclusively focus on the quality of the final answer and may ignore the quality of the reasoning paths, as even an incorrect reasoning path leads to a correct answer by chance. Instead, we advocate the use of reasoning-level confidence to identify high-quality reasoning paths for self-training, supported by our empirical observations. We then propose a new self-training method, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths through Policy Optimization. Our experiments show that CORE-PO improves the accuracy of outputs on four in-distribution and two out-of-distribution benchmarks, compared to existing self-training methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Heterogeneous Continual Graph Learning via Meta-knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.17458</link>
<guid>https://arxiv.org/abs/2505.17458</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, heterogeneous graphs, continual learning, Meta-learning, knowledge distillation 

Summary: 
Machine learning on heterogeneous graphs has advanced rapidly, addressing the challenge of continual learning on evolving graph structures. The Meta-learning based Knowledge Distillation framework (MKD) combines meta-learning for rapid task adaptation with knowledge distillation to balance new information and existing knowledge. MKD uses a novel sampling strategy to efficiently select target-type nodes and retrieve first-order neighbors along metapaths. It incorporates a semantic-level distillation module to align attention distributions between teacher and student models for semantic consistency. Evaluations on benchmark datasets validate MKD's effectiveness in handling continual learning on expanding heterogeneous graphs. <div>
arXiv:2505.17458v1 Announce Type: new 
Abstract: Machine learning on heterogeneous graphs has experienced rapid advancement in recent years, driven by the inherently heterogeneous nature of real-world data. However, existing studies typically assume the graphs to be static, while real-world graphs are continuously expanding. This dynamic nature requires models to adapt to new data while preserving existing knowledge. To this end, this work addresses the challenge of continual learning on heterogeneous graphs by introducing the Meta-learning based Knowledge Distillation framework (MKD), designed to mitigate catastrophic forgetting in evolving heterogeneous graph structures. MKD combines rapid task adaptation through meta-learning on limited samples with knowledge distillation to achieve an optimal balance between incorporating new information and maintaining existing knowledge. To improve the efficiency and effectiveness of sample selection, MKD incorporates a novel sampling strategy that selects a small number of target-type nodes based on node diversity and maintains fixed-size buffers for other types. The strategy retrieves first-order neighbors along metapaths and selects important neighbors based on their structural relevance, enabling the sampled subgraphs to retain key topological and semantic information. In addition, MKD introduces a semantic-level distillation module that aligns the attention distributions over different metapaths between teacher and student models, encouraging semantic consistency beyond the logit level. Comprehensive evaluations across three benchmark datasets validate MKD's effectiveness in handling continual learning scenarios on expanding heterogeneous graphs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient compression of neural networks and datasets</title>
<link>https://arxiv.org/abs/2505.17469</link>
<guid>https://arxiv.org/abs/2505.17469</guid>
<content:encoded><![CDATA[
<div> compression, neural networks, data compression, regularization, optimization

Summary: 
This article discusses methods to reduce the number of parameters in neural networks while maintaining high accuracy. The authors compare and improve existing techniques for data compression and introduce a probabilistic approach to $\ell_0$ regularized optimization. They explore layerwise methods and investigate different architectures and datasets, including image datasets and Wikipedia excerpts. A synthetic teacher-student setup is also implemented for controlled compression analysis. The concept of compression algorithms is related to Solomonoff's theory of inductive inference, and the article confirms that regularized models can exhibit more sample-efficient convergence. <div>
arXiv:2505.17469v1 Announce Type: new 
Abstract: We compare, improve, and contribute methods that substantially decrease the number of parameters of neural networks while maintaining high test accuracy. When applying our methods to minimize description length, we obtain very effective data compression algorithms. In particular, we develop a probabilistic reformulation of $\ell_0$ regularized optimization for nonlinear models that does not require Monte-Carlo sampling and thus improves upon previous methods. We also improve upon methods involving smooth approximations to the $\ell_0$ norm, and investigate layerwise methods. We compare the methods on different architectures and datasets, including convolutional networks trained on image datasets and transformers trained on parts of Wikipedia. We also created a synthetic teacher-student setup to investigate compression in a controlled continuous setting. Finally, we conceptually relate compression algorithms to Solomonoff's theory of inductive inference and empirically verify the prediction that regularized models can exhibit more sample-efficient convergence.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance</title>
<link>https://arxiv.org/abs/2505.17477</link>
<guid>https://arxiv.org/abs/2505.17477</guid>
<content:encoded><![CDATA[
<div> Keywords: Reverse-Speech-Finder, Alzheimer's Disease, neural network, speech analysis, diagnostic models

Summary:
Reverse-Speech-Finder (RSF) is a novel neural network architecture that enhances Alzheimer's Disease (AD) diagnosis through speech analysis by identifying AD-specific speech markers. RSF leverages pre-trained large language models to address the scarcity of real AD speech samples and limited interpretability in existing models. The architecture utilizes a unique approach involving the identification of the most probable speech markers (MPMs) and neurons (MPNs) for AD prediction. By backtracking from MPNs to the input layer, RSF identifies the most probable speech-tokens (MPTs) and corresponding MPMs, uncovering new speech markers for AD detection. Experimental results show RSF outperforms traditional methods, improving accuracy and F1-score. By generating speech data with novel markers, RSF enhances the robustness and accuracy of AD diagnostic models, offering insights into AD-related linguistic deficits and potential for effective early intervention strategies.

<br><br>Summary: <div>
arXiv:2505.17477v1 Announce Type: new 
Abstract: This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimer's Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSF's unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSF's superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSF's potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression</title>
<link>https://arxiv.org/abs/2505.17478</link>
<guid>https://arxiv.org/abs/2505.17478</guid>
<content:encoded><![CDATA[
<div> autoregressive model, protein dynamics, molecular dynamics, conformational space, protein folding

Summary:
ConfRover is a new autoregressive model designed to understand protein dynamics by learning from molecular dynamics (MD) data. This model captures temporal dependencies between protein conformations and supports both time-dependent and time-independent sampling. It includes an encoding layer to embed protein-specific information and conformation, a temporal module to capture conformational dynamics, and an SE(3) diffusion model for structure generation. ConfRover is effective in learning conformational dynamics from the ATLAS protein MD dataset and can be used for various downstream tasks. It is the first model to generate protein conformations and trajectories simultaneously, providing a novel and flexible approach to learning from protein MD data.<br><br>Summary: <div>
arXiv:2505.17478v1 Announce Type: new 
Abstract: Understanding protein dynamics is critical for elucidating their biological functions. The increasing availability of molecular dynamics (MD) data enables the training of deep generative models to efficiently explore the conformational space of proteins. However, existing approaches either fail to explicitly capture the temporal dependencies between conformations or do not support direct generation of time-independent samples. To address these limitations, we introduce ConfRover, an autoregressive model that simultaneously learns protein conformation and dynamics from MD trajectories, supporting both time-dependent and time-independent sampling. At the core of our model is a modular architecture comprising: (i) an encoding layer, adapted from protein folding models, that embeds protein-specific information and conformation at each time frame into a latent space; (ii) a temporal module, a sequence model that captures conformational dynamics across frames; and (iii) an SE(3) diffusion model as the structure decoder, generating conformations in continuous space. Experiments on ATLAS, a large-scale protein MD dataset of diverse structures, demonstrate the effectiveness of our model in learning conformational dynamics and supporting a wide range of downstream tasks. ConfRover is the first model to sample both protein conformations and trajectories within a single framework, offering a novel and flexible approach for learning from protein MD data.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperspectral in situ remote sensing of water surface nitrate in the Fitzroy River estuary, Queensland, Australia, using deep learning</title>
<link>https://arxiv.org/abs/2505.17483</link>
<guid>https://arxiv.org/abs/2505.17483</guid>
<content:encoded><![CDATA[

arXiv:2505.17483v1 Announce Type: new 
Abstract: Nitrate ($\text{NO}_3^-$) is a form of dissolved inorganic nitrogen derived primarily from anthropogenic sources. The recent increase in river-discharged nitrate poses a major risk for coral bleaching in the Great Barrier Reef (GBR) lagoon. Although nitrate is an optically inactive (i.e., colourless) constituent, previous studies have demonstrated there is an indirect, non-causal relationship between water surface nitrate and water-leaving reflectance that is mediated through optically active water quality parameters such as total suspended solids and coloured dissolved organic matter. This work aims to advance our understanding of this relationship with an effort to measure time-series nitrate and simultaneous hyperspectral reflectance at the Fitzroy River estuary, Queensland, Australia. Time-series observations revealed periodic cycles in nitrate loads due to the tidal influence in the estuarine study site. The water surface nitrate loads were predicted from hyperspectral reflectance and water salinity measurements, with hyperspectral reflectance indicating the concentrations of optically active variables and salinity indicating the mixing of river water and seawater proportions. The accuracy assessment of model-predicted nitrate against in-situ measured nitrate values showed that the predicted nitrate values correlated well with the ground-truth data, with an $R^2$ score of 0.86, and an RMSE of 0.03 mg/L. This work demonstrates the feasibility of predicting water surface nitrate from hyperspectral reflectance and salinity measurements.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExARNN: An Environment-Driven Adaptive RNN for Learning Non-Stationary Power Dynamics</title>
<link>https://arxiv.org/abs/2505.17488</link>
<guid>https://arxiv.org/abs/2505.17488</guid>
<content:encoded><![CDATA[

arXiv:2505.17488v1 Announce Type: new 
Abstract: Non-stationary power system dynamics, influenced by renewable energy variability, evolving demand patterns, and climate change, are becoming increasingly complex. Accurately capturing these dynamics requires a model capable of adapting to environmental factors. Traditional models, including Recurrent Neural Networks (RNNs), lack efficient mechanisms to encode external factors, such as time or environmental data, for dynamic adaptation. To address this, we propose the External Adaptive RNN (ExARNN), a novel framework that integrates external data (e.g., weather, time) to continuously adjust the parameters of a base RNN. ExARNN achieves this through a hierarchical hypernetwork design, using Neural Controlled Differential Equations (NCDE) to process external data and generate RNN parameters adaptively. This approach enables ExARNN to handle inconsistent timestamps between power and external measurements, ensuring continuous adaptation. Extensive forecasting tests demonstrate ExARNN's superiority over established baseline models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs</title>
<link>https://arxiv.org/abs/2505.17495</link>
<guid>https://arxiv.org/abs/2505.17495</guid>
<content:encoded><![CDATA[

arXiv:2505.17495v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable performance by capturing complex interactions between input features. To identify these interactions, most existing approaches require enumerating all possible combinations of features up to a given order, causing them to scale poorly with the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an information-theoretic approach that uses interaction sparsity to scale to $n \approx 10^3$ features. SPEX greatly improves upon prior methods but requires tens of thousands of model inferences, which can be prohibitive for large models. In this paper, we observe that LLM feature interactions are often hierarchical -- higher-order interactions are accompanied by their lower-order subsets -- which enables more efficient discovery. To exploit this hierarchy, we propose ProxySPEX, an interaction attribution algorithm that first fits gradient boosted trees to masked LLM outputs and then extracts the important interactions. Experiments across four challenging high-dimensional datasets show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over marginal attribution approaches while using $10\times$ fewer inferences than SPEX. By accounting for interactions, ProxySPEX identifies features that influence model output over 20% more than those selected by marginal approaches. Further, we apply ProxySPEX to two interpretability tasks. Data attribution, where we identify interactions among CIFAR-10 training samples that influence test predictions, and mechanistic interpretability, where we uncover interactions between attention heads, both within and across layers, on a question-answering task. ProxySPEX identifies interactions that enable more aggressive pruning of heads than marginal approaches.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.17508</link>
<guid>https://arxiv.org/abs/2505.17508</guid>
<content:encoded><![CDATA[

arXiv:2505.17508v1 Announce Type: new 
Abstract: Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). Despite the widespread use of Kullback-Leibler (KL) regularization in policy gradient algorithms to stabilize training, the systematic exploration of how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online reinforcement learning (RL) presents a nuanced and systematically explorable design space. In this paper, we propose regularized policy gradient (RPG), a systematic framework for deriving and analyzing KL-regularized policy gradient methods in the online RL setting. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by both forward and reverse KL divergences, considering both normalized and unnormalized policy distributions. Furthermore, we present derivations for fully differentiable loss functions as well as REINFORCE-style gradient estimators, accommodating diverse algorithmic needs. We conduct extensive experiments on RL for LLM reasoning using these methods, showing improved or competitive results in terms of training stability and performance compared to strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at https://github.com/complex-reasoning/RPG.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection</title>
<link>https://arxiv.org/abs/2505.17513</link>
<guid>https://arxiv.org/abs/2505.17513</guid>
<content:encoded><![CDATA[

arXiv:2505.17513v1 Announce Type: new 
Abstract: Recent advances in text-to-speech technologies have enabled realistic voice generation, fueling audio-based deepfake attacks such as fraud and impersonation. While audio anti-spoofing systems are critical for detecting such threats, prior work has predominantly focused on acoustic-level perturbations, leaving the impact of linguistic variation largely unexplored. In this paper, we investigate the linguistic sensitivity of both open-source and commercial anti-spoofing detectors by introducing transcript-level adversarial attacks. Our extensive evaluation reveals that even minor linguistic perturbations can significantly degrade detection accuracy: attack success rates surpass 60% on several open-source detector-voice pairs, and notably one commercial detection accuracy drops from 100% on synthetic audio to just 32%. Through a comprehensive feature attribution analysis, we identify that both linguistic complexity and model-level audio embedding similarity contribute strongly to detector vulnerability. We further demonstrate the real-world risk via a case study replicating the Brad Pitt audio deepfake scam, using transcript adversarial attacks to completely bypass commercial detectors. These results highlight the need to move beyond purely acoustic defenses and account for linguistic variation in the design of robust anti-spoofing systems. All source code will be publicly available.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spacetime Geometry of Denoising in Diffusion Models</title>
<link>https://arxiv.org/abs/2505.17517</link>
<guid>https://arxiv.org/abs/2505.17517</guid>
<content:encoded><![CDATA[

arXiv:2505.17517v1 Announce Type: new 
Abstract: We present a novel perspective on diffusion models using the framework of information geometry. We show that the set of noisy samples, taken across all noise levels simultaneously, forms a statistical manifold -- a family of denoising probability distributions. Interpreting the noise level as a temporal parameter, we refer to this manifold as spacetime. This manifold naturally carries a Fisher-Rao metric, which defines geodesics -- shortest paths between noisy points. Notably, this family of distributions is exponential, enabling efficient geodesic computation even in high-dimensional settings without retraining or fine-tuning. We demonstrate the practical value of this geometric viewpoint in transition path sampling, where spacetime geodesics define smooth sequences of Boltzmann distributions, enabling the generation of continuous trajectories between low-energy metastable states. Code is available at: https://github.com/Aalto-QuML/diffusion-spacetime-geometry.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeCF: A TimeMixer-Based Model with adaptive Convolution and Sharpness-Aware Minimization Frequency Domain Loss for long-term time seris forecasting</title>
<link>https://arxiv.org/abs/2505.17532</link>
<guid>https://arxiv.org/abs/2505.17532</guid>
<content:encoded><![CDATA[

arXiv:2505.17532v1 Announce Type: new 
Abstract: Recent studies have shown that by introducing prior knowledge, multi-scale analysis of complex and non-stationary time series in real environments can achieve good results in the field of long-term forecasting. However, affected by channel-independent methods, models based on multi-scale analysis may produce suboptimal prediction results due to the autocorrelation between time series labels, which in turn affects the generalization ability of the model. To address this challenge, we are inspired by the idea of sharpness-aware minimization and the recently proposed FreDF method and design a deep learning model TimeCF for long-term time series forecasting based on the TimeMixer, combined with our designed adaptive convolution information aggregation module and Sharpness-Aware Minimization Frequency Domain Loss (SAMFre). Specifically, TimeCF first decomposes the original time series into sequences of different scales. Next, the same-sized convolution modules are used to adaptively aggregate information of different scales on sequences of different scales. Then, decomposing each sequence into season and trend parts and the two parts are mixed at different scales through bottom-up and top-down methods respectively. Finally, different scales are aggregated through a Feed-Forward Network. What's more, extensive experimental results on different real-world datasets show that our proposed TimeCF has excellent performance in the field of long-term forecasting.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Representational Disparities</title>
<link>https://arxiv.org/abs/2505.17533</link>
<guid>https://arxiv.org/abs/2505.17533</guid>
<content:encoded><![CDATA[

arXiv:2505.17533v1 Announce Type: new 
Abstract: We propose a fair machine learning algorithm to model interpretable differences between observed and desired human decision-making, with the latter aimed at reducing disparity in a downstream outcome impacted by the human decision. Prior work learns fair representations without considering the outcome in the decision-making process. We model the outcome disparities as arising due to the different representations of the input seen by the observed and desired decision-maker, which we term representational disparities. Our goal is to learn interpretable representational disparities which could potentially be corrected by specific nudges to the human decision, mitigating disparities in the downstream outcome; we frame this as a multi-objective optimization problem using a neural network. Under reasonable simplifying assumptions, we prove that our neural network model of the representational disparity learns interpretable weights that fully mitigate the outcome disparity. We validate objectives and interpret results using real-world German Credit, Adult, and Heritage Health datasets.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Style Transfer for Counterfactual Explainability</title>
<link>https://arxiv.org/abs/2505.17542</link>
<guid>https://arxiv.org/abs/2505.17542</guid>
<content:encoded><![CDATA[

arXiv:2505.17542v1 Announce Type: new 
Abstract: Counterfactual explainability seeks to uncover model decisions by identifying minimal changes to the input that alter the predicted outcome. This task becomes particularly challenging for graph data due to preserving structural integrity and semantic meaning. Unlike prior approaches that rely on forward perturbation mechanisms, we introduce Graph Inverse Style Transfer (GIST), the first framework to re-imagine graph counterfactual generation as a backtracking process, leveraging spectral style transfer. By aligning the global structure with the original input spectrum and preserving local content faithfulness, GIST produces valid counterfactuals as interpolations between the input style and counterfactual content. Tested on 8 binary and multi-class graph classification benchmarks, GIST achieves a remarkable +7.6% improvement in the validity of produced counterfactuals and significant gains (+45.5%) in faithfully explaining the true class distribution. Additionally, GIST's backtracking mechanism effectively mitigates overshooting the underlying predictor's decision boundary, minimizing the spectral differences between the input and the counterfactuals. These results challenge traditional forward perturbation methods, offering a novel perspective that advances graph explainability.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing</title>
<link>https://arxiv.org/abs/2505.17552</link>
<guid>https://arxiv.org/abs/2505.17552</guid>
<content:encoded><![CDATA[

arXiv:2505.17552v1 Announce Type: new 
Abstract: De novo peptide sequencing is a critical task in proteomics. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs a list-wise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass Deviation) and RMD (residual Mass Deviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its base models used to generate training candidates for reranking pre-training, but also sets a new state-of-the-art benchmark. Moreover, RankNovo exhibits strong zero-shot generalization to unseen models whose generations were not exposed during training, highlighting its robustness and potential as a universal reranking framework for peptide sequencing. Our work presents a novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo sequencing. Our source code is provided on GitHub.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning</title>
<link>https://arxiv.org/abs/2505.17553</link>
<guid>https://arxiv.org/abs/2505.17553</guid>
<content:encoded><![CDATA[

arXiv:2505.17553v1 Announce Type: new 
Abstract: In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves specializing functionalities into different experts and sparsely activating them appropriately, has been widely adopted as a promising approach to trade-off between model capacity and computation overhead. However, current MoE variants fall short on heterogeneous datasets, ignoring the fact that experts may learn similar knowledge, resulting in the underutilization of MoE's capacity. In this paper, we propose Contrastive Representation for MoE (CoMoE), a novel method to promote modularization and specialization in MoE, where the experts are trained along with a contrastive objective by sampling from activated and inactivated experts in top-k routing. We demonstrate that such a contrastive objective recovers the mutual-information gap between inputs and the two types of experts. Experiments on several benchmarks and in multi-task settings demonstrate that CoMoE can consistently enhance MoE's capacity and promote modularization among the experts.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wildfire spread forecasting with Deep Learning</title>
<link>https://arxiv.org/abs/2505.17556</link>
<guid>https://arxiv.org/abs/2505.17556</guid>
<content:encoded><![CDATA[

arXiv:2505.17556v1 Announce Type: new 
Abstract: Accurate prediction of wildfire spread is crucial for effective risk management, emergency response, and strategic resource allocation. In this study, we present a deep learning (DL)-based framework for forecasting the final extent of burned areas, using data available at the time of ignition. We leverage a spatio-temporal dataset that covers the Mediterranean region from 2006 to 2022, incorporating remote sensing data, meteorological observations, vegetation maps, land cover classifications, anthropogenic factors, topography data, and thermal anomalies. To evaluate the influence of temporal context, we conduct an ablation study examining how the inclusion of pre- and post-ignition data affects model performance, benchmarking the temporal-aware DL models against a baseline trained exclusively on ignition-day inputs. Our results indicate that multi-day observational data substantially improve predictive accuracy. Particularly, the best-performing model, incorporating a temporal window of four days before to five days after ignition, improves both the F1 score and the Intersection over Union by almost 5% in comparison to the baseline on the test dataset. We publicly release our dataset and models to enhance research into data-driven approaches for wildfire modeling and response.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs</title>
<link>https://arxiv.org/abs/2505.17575</link>
<guid>https://arxiv.org/abs/2505.17575</guid>
<content:encoded><![CDATA[

arXiv:2505.17575v1 Announce Type: new 
Abstract: Solving partial differential equations (PDEs) with machine learning has recently attracted great attention, as PDEs are fundamental tools for modeling real-world systems that range from fundamental physical science to advanced engineering disciplines. Most real-world physical systems across various disciplines are actually involved in multiple coupled physical fields rather than a single field. However, previous machine learning studies mainly focused on solving single-field problems, but overlooked the importance and characteristics of multiphysics problems in real world. Multiphysics PDEs typically entail multiple strongly coupled variables, thereby introducing additional complexity and challenges, such as inter-field coupling. Both benchmarking and solving multiphysics problems with machine learning remain largely unexamined. To identify and address the emerging challenges in multiphysics problems, we mainly made three contributions in this work. First, we collect the first general multiphysics dataset, the Multiphysics Bench, that focuses on multiphysics PDE solving with machine learning. Multiphysics Bench is also the most comprehensive PDE dataset to date, featuring the broadest range of coupling types, the greatest diversity of PDE formulations, and the largest dataset scale. Second, we conduct the first systematic investigation on multiple representative learning-based PDE solvers, such as PINNs, FNO, DeepONet, and DiffusionPDE solvers, on multiphysics problems. Unfortunately, naively applying these existing solvers usually show very poor performance for solving multiphysics. Third, through extensive experiments and discussions, we report multiple insights and a bag of useful tricks for solving multiphysics with machine learning, motivating future directions in the study and simulation of complex, coupled physical systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation</title>
<link>https://arxiv.org/abs/2505.17579</link>
<guid>https://arxiv.org/abs/2505.17579</guid>
<content:encoded><![CDATA[

arXiv:2505.17579v1 Announce Type: new 
Abstract: In this paper, we propose a novel framework for ownership verification of deep neural network (DNN) models for image classification tasks. It allows verification of model identity by both the rightful owner and third party without presenting the original model. We assume a gray-box scenario where an unauthorized user owns a model that is illegally copied from the original model, provides services in a cloud environment, and the user throws images and receives the classification results as a probability distribution of output classes. The framework applies a white-box adversarial attack to align the output probability of a specific class to a designated value. Due to the knowledge of original model, it enables the owner to generate such adversarial examples. We propose a simple but effective adversarial attack method based on the iterative Fast Gradient Sign Method (FGSM) by introducing control parameters. Experimental results confirm the effectiveness of the identification of DNN models using adversarial attack.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity</title>
<link>https://arxiv.org/abs/2505.17591</link>
<guid>https://arxiv.org/abs/2505.17591</guid>
<content:encoded><![CDATA[

arXiv:2505.17591v1 Announce Type: new 
Abstract: In autonomous navigation systems, the solution of the place recognition problem is crucial for their safe functioning. But this is not a trivial solution, since it must be accurate regardless of any changes in the scene, such as seasonal changes and different weather conditions, and it must be generalizable to other environments. This paper presents our method, MinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input data to obtain its spherical coordinates and intensity values normalized within a range of 0 to 1 for each point, and it produces a robust place recognition descriptor. To that end, a deep learning approach that combines Minkowski convolutions and a U-net architecture with skip connections is used. The results of MinkUNeXt-SI demonstrate that this method reaches and surpasses state-of-the-art performance while it also generalizes satisfactorily to other datasets. Additionally, we showcase the capture of a custom dataset and its use in evaluating our solution, which also achieves outstanding results. Both the code of our solution and the runs of our dataset are publicly available for reproducibility purposes.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeUQI: Near-Optimal Uniform Quantization Parameter Initialization</title>
<link>https://arxiv.org/abs/2505.17595</link>
<guid>https://arxiv.org/abs/2505.17595</guid>
<content:encoded><![CDATA[

arXiv:2505.17595v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve impressive performance across domains but face significant challenges when deployed on consumer-grade GPUs or personal devices such as laptops, due to high memory consumption and inference costs. Post-training quantization (PTQ) of LLMs offers a promising solution that reduces their memory footprint and decoding latency. In practice, PTQ with uniform quantization representation is favored for its efficiency and ease of deployment since uniform quantization is widely supported by mainstream hardware and software libraries. Recent studies on $\geq 2$-bit uniform quantization have led to noticeable improvements in post-quantization model performance; however, they primarily focus on quantization methodologies, while the initialization of quantization parameters is underexplored and still relies on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method devoted to efficiently determining near-optimal initial parameters for uniform quantization. NeUQI is orthogonal to prior quantization methodologies and can seamlessly integrate with them. The experiments with the LLaMA and Qwen families on various tasks demonstrate that our NeUQI consistently outperforms existing methods. Furthermore, when combined with a lightweight distillation strategy, NeUQI can achieve superior performance to PV-tuning, a much more resource-intensive approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Text Bundling Supervision for Zero-Shot Inference on Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2505.17599</link>
<guid>https://arxiv.org/abs/2505.17599</guid>
<content:encoded><![CDATA[

arXiv:2505.17599v1 Announce Type: new 
Abstract: Large language models (LLMs) have been used in many zero-shot learning problems, with their strong generalization ability. Recently, adopting LLMs in text-attributed graphs (TAGs) has drawn increasing attention. However, the adoption of LLMs faces two major challenges: limited information on graph structure and unreliable responses. LLMs struggle with text attributes isolated from the graph topology. Worse still, they yield unreliable predictions due to both information insufficiency and the inherent weakness of LLMs (e.g., hallucination). Towards this end, this paper proposes a novel method named Dynamic Text Bundling Supervision (DENSE) that queries LLMs with bundles of texts to obtain bundle-level labels and uses these labels to supervise graph neural networks. Specifically, we sample a set of bundles, each containing a set of nodes with corresponding texts of close proximity. We then query LLMs with the bundled texts to obtain the label of each bundle. Subsequently, the bundle labels are used to supervise the optimization of graph neural networks, and the bundles are further refined to exclude noisy items. To justify our design, we also provide theoretical analysis of the proposed method. Extensive experiments across ten datasets validate the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Semantic Token Communication for Transformer-based Edge Inference</title>
<link>https://arxiv.org/abs/2505.17604</link>
<guid>https://arxiv.org/abs/2505.17604</guid>
<content:encoded><![CDATA[

arXiv:2505.17604v1 Announce Type: new 
Abstract: This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning</title>
<link>https://arxiv.org/abs/2505.17610</link>
<guid>https://arxiv.org/abs/2505.17610</guid>
<content:encoded><![CDATA[

arXiv:2505.17610v1 Announce Type: new 
Abstract: This paper provides the first expert sample complexity characterization for learning a Nash equilibrium from expert data in Markov Games. We show that a new quantity named the single policy deviation concentrability coefficient is unavoidable in the non-interactive imitation learning setting, and we provide an upper bound for behavioral cloning (BC) featuring such coefficient. BC exhibits substantial regret in games with high concentrability coefficient, leading us to utilize expert queries to develop and introduce two novel solution algorithms: MAIL-BRO and MURMAIL. The former employs a best response oracle and learns an $\varepsilon$-Nash equilibrium with $\mathcal{O}(\varepsilon^{-4})$ expert and oracle queries. The latter bypasses completely the best response oracle at the cost of a worse expert query complexity of order $\mathcal{O}(\varepsilon^{-8})$. Finally, we provide numerical evidence, confirming our theoretical findings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language model as user daily behavior data generator: balancing population diversity and individual personality</title>
<link>https://arxiv.org/abs/2505.17615</link>
<guid>https://arxiv.org/abs/2505.17615</guid>
<content:encoded><![CDATA[

arXiv:2505.17615v1 Announce Type: new 
Abstract: Predicting human daily behavior is challenging due to the complexity of routine patterns and short-term fluctuations. While data-driven models have improved behavior prediction by leveraging empirical data from various platforms and devices, the reliance on sensitive, large-scale user data raises privacy concerns and limits data availability. Synthetic data generation has emerged as a promising solution, though existing methods are often limited to specific applications. In this work, we introduce BehaviorGen, a framework that uses large language models (LLMs) to generate high-quality synthetic behavior data. By simulating user behavior based on profiles and real events, BehaviorGen supports data augmentation and replacement in behavior prediction models. We evaluate its performance in scenarios such as pertaining augmentation, fine-tuning replacement, and fine-tuning augmentation, achieving significant improvements in human mobility and smartphone usage predictions, with gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen to enhance user behavior modeling through flexible and privacy-preserving synthetic data generation.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration</title>
<link>https://arxiv.org/abs/2505.17621</link>
<guid>https://arxiv.org/abs/2505.17621</guid>
<content:encoded><![CDATA[

arXiv:2505.17621v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has emerged as a pivotal method for improving the reasoning capabilities of Large Language Models (LLMs). However, prevalent RL approaches such as Proximal Policy Optimization (PPO) and Group-Regularized Policy Optimization (GRPO) face critical limitations due to their reliance on sparse outcome-based rewards and inadequate mechanisms for incentivizing exploration. These limitations result in inefficient guidance for multi-step reasoning processes. Specifically, sparse reward signals fail to deliver effective or sufficient feedback, particularly for challenging problems. Furthermore, such reward structures induce systematic biases that prioritize exploitation of familiar trajectories over novel solution discovery. These shortcomings critically hinder performance in complex reasoning tasks, which inherently demand iterative refinement across ipntermediate steps. To address these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense rewards and amplify explorations in the RL-based training paradigm. i-MENTOR introduces three key innovations: trajectory-aware exploration rewards that mitigate bias in token-level strategies while maintaining computational efficiency; dynamic reward scaling to stabilize exploration and exploitation in large action spaces; and advantage-preserving reward implementation that maintains advantage distribution integrity while incorporating exploratory guidance. Experiments across three public datasets demonstrate i-MENTOR's effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Stochastic Depth Training for Adaptive Inference</title>
<link>https://arxiv.org/abs/2505.17626</link>
<guid>https://arxiv.org/abs/2505.17626</guid>
<content:encoded><![CDATA[

arXiv:2505.17626v1 Announce Type: new 
Abstract: Dynamic DNN optimization techniques such as layer-skipping offer increased adaptability and efficiency gains but can lead to i) a larger memory footprint as in decision gates, ii) increased training complexity (e.g., with non-differentiable operations), and iii) less control over performance-quality trade-offs due to its inherent input-dependent execution. To approach these issues, we propose a simpler yet effective alternative for adaptive inference with a zero-overhead, single-model, and time-predictable inference. Central to our approach is the observation that models trained with Stochastic Depth -- a method for faster training of residual networks -- become more resilient to arbitrary layer-skipping at inference time. We propose a method to first select near Pareto-optimal skipping configurations from a stochastically-trained model to adapt the inference at runtime later. Compared to original ResNets, our method shows improvements of up to 2X in power efficiency at accuracy drops as low as 0.71%.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis</title>
<link>https://arxiv.org/abs/2505.17636</link>
<guid>https://arxiv.org/abs/2505.17636</guid>
<content:encoded><![CDATA[

arXiv:2505.17636v1 Announce Type: new 
Abstract: Various AI safety datasets have been developed to measure LLMs against evolving interpretations of harm. Our evaluation of five recently published open-source safety benchmarks reveals distinct semantic clusters using UMAP dimensionality reduction and kmeans clustering (silhouette score: 0.470). We identify six primary harm categories with varying benchmark representation. GretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix emphasizes self-harm scenarios. Significant differences in prompt length distribution suggests confounds to data collection and interpretations of harm as well as offer possible context. Our analysis quantifies benchmark orthogonality among AI benchmarks, allowing for transparency in coverage gaps despite topical similarities. Our quantitative framework for analyzing semantic orthogonality across safety benchmarks enables more targeted development of datasets that comprehensively address the evolving landscape of harms in AI use, however that is defined in the future.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Spatio-Temporal Prediction: An Effective and Efficient Multi-Modal Approach</title>
<link>https://arxiv.org/abs/2505.17637</link>
<guid>https://arxiv.org/abs/2505.17637</guid>
<content:encoded><![CDATA[

arXiv:2505.17637v1 Announce Type: new 
Abstract: Spatio-temporal prediction plays a crucial role in intelligent transportation, weather forecasting, and urban planning. While integrating multi-modal data has shown potential for enhancing prediction accuracy, key challenges persist: (i) inadequate fusion of multi-modal information, (ii) confounding factors that obscure causal relations, and (iii) high computational complexity of prediction models. To address these challenges, we propose E^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal Prediction framework. E^2-CSTP leverages cross-modal attention and gating mechanisms to effectively integrate multi-modal data. Building on this, we design a dual-branch causal inference approach: the primary branch focuses on spatio-temporal prediction, while the auxiliary branch mitigates bias by modeling additional modalities and applying causal interventions to uncover true causal dependencies. To improve model efficiency, we integrate GCN with the Mamba architecture for accelerated spatio-temporal encoding. Extensive experiments on 4 real-world datasets show that E^2-CSTP significantly outperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in accuracy as well as 17.37%-56.11% reductions in computational overhead.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training</title>
<link>https://arxiv.org/abs/2505.17638</link>
<guid>https://arxiv.org/abs/2505.17638</guid>
<content:encoded><![CDATA[

arXiv:2505.17638v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time $\tau_\mathrm{gen}$ at which models begin to generate high-quality samples, and a later time $\tau_\mathrm{mem}$ beyond which memorization emerges. Crucially, we find that $\tau_\mathrm{mem}$ increases linearly with the training set size $n$, while $\tau_\mathrm{gen}$ remains constant. This creates a growing window of training times with $n$ where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when $n$ becomes larger than a model-dependent threshold that overfitting disappears at infinite training times. These findings reveal a form of implicit dynamical regularization in the training dynamics, which allow to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval</title>
<link>https://arxiv.org/abs/2505.17639</link>
<guid>https://arxiv.org/abs/2505.17639</guid>
<content:encoded><![CDATA[

arXiv:2505.17639v1 Announce Type: new 
Abstract: Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, a novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs a new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying a minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When a user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B maintains 97.2\% accuracy on MATH500 when pruned to 8/128 configuration (50\% expert reduction), and still achieves 72.0\% with aggressive 8/32 pruning (87.5\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\% on MATH500 and 81.3\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95\% accuracy on MATH500. We make our code publicly available at https://github.com/JarvisPei/PreMoe.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Network Science Approach to Granular Time Series Segmentation</title>
<link>https://arxiv.org/abs/2505.17640</link>
<guid>https://arxiv.org/abs/2505.17640</guid>
<content:encoded><![CDATA[

arXiv:2505.17640v1 Announce Type: new 
Abstract: Time series segmentation (TSS) is one of the time series (TS) analysis techniques, that has received considerably less attention compared to other TS related tasks. In recent years, deep learning architectures have been introduced for TSS, however their reliance on sliding windows limits segmentation granularity due to fixed window sizes and strides. To overcome these challenges, we propose a new more granular TSS approach that utilizes the Weighted Dual Perspective Visbility Graph (WDPVG) TS into a graph and combines it with a Graph Attention Network (GAT). By transforming TS into graphs, we are able to capture different structural aspects of the data that would otherwise remain hidden. By utilizing the representation learning capabilities of Graph Neural Networks, our method is able to effectively identify meaningful segments within the TS. To better understand the potential of our approach, we also experimented with different TS-to-graph transformations and compared their performance. Our contributions include: a) formulating the TSS as a node classification problem on graphs; b) conducting an extensive analysis of various TS- to-graph transformations applied to TSS using benchmark datasets from the TSSB repository; c) providing the first detailed study on utilizing GNNs for analyzing graph representations of TS in the context of TSS; d) demonstrating the effectiveness of our method, which achieves an average F1 score of 0.97 across 59 diverse TSS benchmark datasets; e) outperforming the seq2point baseline method by 0.05 in terms of F1 score; and f) reducing the required training data compared to the baseline methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives</title>
<link>https://arxiv.org/abs/2505.17646</link>
<guid>https://arxiv.org/abs/2505.17646</guid>
<content:encoded><![CDATA[

arXiv:2505.17646v1 Announce Type: new 
Abstract: Recent studies have revealed that the loss landscape of large language models resembles a basin, within which the models perform nearly identically, and outside of which they lose all their capabilities. In this work, we conduct further studies on the loss landscape of large language models. We discover that pre-training creates a "basic capability" basin, and subsequent fine-tuning creates "specific capability" basins (e.g., math, safety, coding) within the basic capability basin. We further investigate two types of loss landscapes: the most-case landscape (i.e., the landscape along most directions) and the worst-case landscape (i.e., the landscape along the worst direction). We argue that as long as benign fine-tuning remains within the most-case basin, it will not compromise previous capabilities. Similarly, any fine-tuning (including the adversarial one) that stays within the worst-case basin would not compromise previous capabilities. Finally, we theoretically demonstrate that the size of the most-case basin can bound the size of the worst-case basin and the robustness with respect to input perturbations. We also show that, due to the over-parameterization property of current large language models, one can easily enlarge the basins by five times.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective</title>
<link>https://arxiv.org/abs/2505.17652</link>
<guid>https://arxiv.org/abs/2505.17652</guid>
<content:encoded><![CDATA[

arXiv:2505.17652v1 Announce Type: new 
Abstract: Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces \textbf{C}ompetence-\textbf{D}ifficulty \textbf{A}lignment \textbf{S}ampling (\textbf{CDAS}), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is \textbf{2.33} times slower than CDAS.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification</title>
<link>https://arxiv.org/abs/2505.17660</link>
<guid>https://arxiv.org/abs/2505.17660</guid>
<content:encoded><![CDATA[

arXiv:2505.17660v1 Announce Type: new 
Abstract: Neighborhood-aware tokenized graph Transformers have recently shown great potential for node classification tasks. Despite their effectiveness, our in-depth analysis of neighborhood tokens reveals two critical limitations in the existing paradigm. First, current neighborhood token generation methods fail to adequately capture attribute correlations within a neighborhood. Second, the conventional self-attention mechanism suffers from attention diversion when processing neighborhood tokens, where high-hop neighborhoods receive disproportionate focus, severely disrupting information interactions between the target node and its neighborhood tokens. To address these challenges, we propose DAM-GT, Dual positional encoding-based Attention Masking graph Transformer. DAM-GT introduces a novel dual positional encoding scheme that incorporates attribute-aware encoding via an attribute clustering strategy, effectively preserving node correlations in both topological and attribute spaces. In addition, DAM-GT formulates a new attention mechanism with a simple yet effective masking strategy to guide interactions between target nodes and their neighborhood tokens, overcoming the issue of attention diversion. Extensive experiments on various graphs with different homophily levels as well as different scales demonstrate that DAM-GT consistently outperforms state-of-the-art methods in node classification tasks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated scientific minimization of regret</title>
<link>https://arxiv.org/abs/2505.17661</link>
<guid>https://arxiv.org/abs/2505.17661</guid>
<content:encoded><![CDATA[

arXiv:2505.17661v1 Announce Type: new 
Abstract: We introduce automated scientific minimization of regret (ASMR) -- a framework for automated computational cognitive science. Building on the principles of scientific regret minimization, ASMR leverages Centaur -- a recently proposed foundation model of human cognition -- to identify gaps in an interpretable cognitive model. These gaps are then addressed through automated revisions generated by a language-based reasoning model. We demonstrate the utility of this approach in a multi-attribute decision-making task, showing that ASMR discovers cognitive models that predict human behavior at noise ceiling while retaining interpretability. Taken together, our results highlight the potential of ASMR to automate core components of the cognitive modeling pipeline.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs</title>
<link>https://arxiv.org/abs/2505.17662</link>
<guid>https://arxiv.org/abs/2505.17662</guid>
<content:encoded><![CDATA[

arXiv:2505.17662v1 Announce Type: new 
Abstract: Transformer-based models have shown strong performance across diverse time-series tasks, but their deployment on resource-constrained devices remains challenging due to high memory and computational demand. While prior work targeting Microcontroller Units (MCUs) has explored hardware-specific optimizations, such approaches are often task-specific and limited to 8-bit fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater flexibility, enabling fine-grained control over data precision and architecture. However, existing FPGA-based deployments of Transformers for time-series analysis typically focus on high-density platforms with manual configuration. This paper presents a unified and fully automated deployment framework for Tiny Transformers on embedded FPGAs. Our framework supports a compact encoder-only Transformer architecture across three representative time-series tasks (forecasting, classification, and anomaly detection). It combines quantization-aware training (down to 4 bits), hardware-aware hyperparameter search using Optuna, and automatic VHDL generation for seamless deployment. We evaluate our framework on six public datasets across two embedded FPGA platforms. Results show that our framework produces integer-only, task-specific Transformer accelerators achieving as low as 0.033 mJ per inference with millisecond latency on AMD Spartan-7, while also providing insights into deployment feasibility on Lattice iCE40. All source code will be released in the GitHub repository (https://github.com/Edwina1030/TinyTransformer4TS).
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is the role of memorization in Continual Learning?</title>
<link>https://arxiv.org/abs/2505.17664</link>
<guid>https://arxiv.org/abs/2505.17664</guid>
<content:encoded><![CDATA[

arXiv:2505.17664v1 Announce Type: new 
Abstract: Memorization impacts the performance of deep learning algorithms. Prior works have studied memorization primarily in the context of generalization and privacy. This work studies the memorization effect on incremental learning scenarios. Forgetting prevention and memorization seem similar. However, one should discuss their differences. We designed extensive experiments to evaluate the impact of memorization on continual learning. We clarified that learning examples with high memorization scores are forgotten faster than regular samples. Our findings also indicated that memorization is necessary to achieve the highest performance. However, at low memory regimes, forgetting regular samples is more important. We showed that the importance of a high-memorization score sample rises with an increase in the buffer size. We introduced a memorization proxy and employed it in the buffer policy problem to showcase how memorization could be used during incremental training. We demonstrated that including samples with a higher proxy memorization score is beneficial when the buffer size is large.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards General Continuous Memory for Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.17670</link>
<guid>https://arxiv.org/abs/2505.17670</guid>
<content:encoded><![CDATA[

arXiv:2505.17670v1 Announce Type: new 
Abstract: Language models (LMs) and their extension, vision-language models (VLMs), have achieved remarkable performance across various tasks. However, they still struggle with complex reasoning tasks that require multimodal or multilingual real-world knowledge. To support such capabilities, an external memory system that can efficiently provide relevant multimodal information is essential. Existing approaches generally concatenate image and text tokens into a long sequence as memory, which, however, may drastically increase context length and even degrade performance. In contrast, we propose using continuous memory, a compact set of dense embeddings to more effectively and efficiently represent multimodal and multilingual knowledge. Our key insight is that a VLM can serve as its own continuous memory encoder. We empirically show that this design improves performance on complex multimodal reasoning tasks. Building on this, we introduce a data-efficient and parameter-efficient method to fine-tune the VLM into a memory encoder, requiring only 1.2% of the model's parameters and a small corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes VLM's original capabilities to encode arbitrary multimodal and multilingual knowledge into just 8 continuous embeddings. Since the inference-time VLM remains frozen, our memory module is plug-and-play and can be flexibly integrated as needed. Extensive experiments across eight multimodal reasoning benchmarks demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding</title>
<link>https://arxiv.org/abs/2505.17694</link>
<guid>https://arxiv.org/abs/2505.17694</guid>
<content:encoded><![CDATA[

arXiv:2505.17694v1 Announce Type: new 
Abstract: Prefix-sharing among multiple prompts presents opportunities to combine the operations of the shared prefix, while attention computation in the decode stage, which becomes a critical bottleneck with increasing context lengths, is a memory-intensive process requiring heavy memory access on the key-value (KV) cache of the prefixes. Therefore, in this paper, we explore the potential of prefix-sharing in the attention computation of the decode stage. However, the tree structure of the prefix-sharing mechanism presents significant challenges for attention computation in efficiently processing shared KV cache access patterns while managing complex dependencies and balancing irregular workloads. To address the above challenges, we propose a dedicated attention kernel to combine the memory access of shared prefixes in the decoding stage, namely FlashForge. FlashForge delivers two key innovations: a novel shared-prefix attention kernel that optimizes memory hierarchy and exploits both intra-block and inter-block parallelism, and a comprehensive workload balancing mechanism that efficiently estimates cost, divides tasks, and schedules execution. Experimental results show that FlashForge achieves an average 1.9x speedup and 120.9x memory access reduction compared to the state-of-the-art FlashDecoding kernel regarding attention computation in the decode stage and 3.8x end-to-end time per output token compared to the vLLM.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data</title>
<link>https://arxiv.org/abs/2505.17695</link>
<guid>https://arxiv.org/abs/2505.17695</guid>
<content:encoded><![CDATA[

arXiv:2505.17695v1 Announce Type: new 
Abstract: Despite the advances in Referring Expression Segmentation (RES) benchmarks, their evaluation protocols remain constrained, primarily focusing on either single targets with short queries (containing minimal attributes) or multiple targets from distinctly different queries on a single domain. This limitation significantly hinders the assessment of more complex reasoning capabilities in RES models. We introduce WildRES, a novel benchmark that incorporates long queries with diverse attributes and non-distinctive queries for multiple targets. This benchmark spans diverse application domains, including autonomous driving environments and robotic manipulation scenarios, thus enabling more rigorous evaluation of complex reasoning capabilities in real-world settings. Our analysis reveals that current RES models demonstrate substantial performance deterioration when evaluated on WildRES. To address this challenge, we introduce SynRES, an automated pipeline generating densely paired compositional synthetic training data through three innovations: (1) a dense caption-driven synthesis for attribute-rich image-mask-expression triplets, (2) reliable semantic alignment mechanisms rectifying caption-pseudo mask inconsistencies via Image-Text Aligned Grouping, and (3) domain-aware augmentations incorporating mosaic composition and superclass replacement to emphasize generalization ability and distinguishing attributes over object categories. Experimental results demonstrate that models trained with SynRES achieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and 3.8% on WildRES-DS. Code and datasets are available at https://github.com/UTLLab/SynRES.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection</title>
<link>https://arxiv.org/abs/2505.17701</link>
<guid>https://arxiv.org/abs/2505.17701</guid>
<content:encoded><![CDATA[

arXiv:2505.17701v1 Announce Type: new 
Abstract: The growing size of large language models has created significant computational inefficiencies. To address this challenge, sparse activation methods selectively deactivates non-essential parameters during inference, reducing computational costs in FFNN layers. While existing methods focus on non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN layer lies globally in the form of a linear combination over its internal down projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN, leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct coefficients of the linear combination. Experimental results demonstrate that D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5% ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4% better performance preservation compared to existing methods. Our specialized kernel implementations effectively realize these theoretical gains into substantial real-world acceleration.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations</title>
<link>https://arxiv.org/abs/2505.17708</link>
<guid>https://arxiv.org/abs/2505.17708</guid>
<content:encoded><![CDATA[

arXiv:2505.17708v1 Announce Type: new 
Abstract: Causal reasoning and discovery, two fundamental tasks of causal analysis, often face challenges in applications due to the complexity, noisiness, and high-dimensionality of real-world data. Despite recent progress in identifying latent causal structures using causal representation learning (CRL), what makes learned representations useful for causal downstream tasks and how to evaluate them are still not well understood. In this paper, we reinterpret CRL using a measurement model framework, where the learned representations are viewed as proxy measurements of the latent causal variables. Our approach clarifies the conditions under which learned representations support downstream causal reasoning and provides a principled basis for quantitatively assessing the quality of representations using a new Test-based Measurement EXclusivity (T-MEX) score. We validate T-MEX across diverse causal inference scenarios, including numerical simulations and real-world ecological video analysis, demonstrating that the proposed framework and corresponding score effectively assess the identification of learned representations and their usefulness for causal downstream tasks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization</title>
<link>https://arxiv.org/abs/2505.17714</link>
<guid>https://arxiv.org/abs/2505.17714</guid>
<content:encoded><![CDATA[

arXiv:2505.17714v1 Announce Type: new 
Abstract: Despite Proximal Policy Optimization (PPO) dominating policy gradient methods -- from robotic control to game AI -- its static trust region forces a brittle trade-off: aggressive clipping stifles early exploration, while late-stage updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive RL by fusing exploration and convergence signals into a single bounded trust region -- a theoretically grounded innovation that outperforms five SOTA baselines with less than 2% overhead. This work bridges a critical gap in phase-aware learning, enabling real-world deployment in safety-critical systems like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1% faster convergence by combining: (1) entropy-driven expansion (epsilon up) for exploration in high-uncertainty states, and (2) reward-guided contraction (epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo, Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001), 2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with only five lines of code change. PPO-BR's simplicity and theoretical guarantees make it ready-to-deploy in safety-critical domains -- from surgical robotics to autonomous drones. In contrast to recent methods such as Group Relative Policy Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism applicable to both language models and general reinforcement learning environments.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Get Experience from Practice: LLM Agents with Record &amp; Replay</title>
<link>https://arxiv.org/abs/2505.17716</link>
<guid>https://arxiv.org/abs/2505.17716</guid>
<content:encoded><![CDATA[

arXiv:2505.17716v1 Announce Type: new 
Abstract: AI agents, empowered by Large Language Models (LLMs) and communication protocols such as MCP and A2A, have rapidly evolved from simple chatbots to autonomous entities capable of executing complex, multi-step tasks, demonstrating great potential. However, the LLMs' inherent uncertainty and heavy computational resource requirements pose four significant challenges to the development of safe and efficient agents: reliability, privacy, cost and performance. Existing approaches, like model alignment, workflow constraints and on-device model deployment, can partially alleviate some issues but often with limitations, failing to fundamentally resolve these challenges.
  This paper proposes a new paradigm called AgentRR (Agent Record & Replay), which introduces the classical record-and-replay mechanism into AI agent frameworks. The core idea is to: 1. Record an agent's interaction trace with its environment and internal decision process during task execution, 2. Summarize this trace into a structured "experience" encapsulating the workflow and constraints, and 3. Replay these experiences in subsequent similar tasks to guide the agent's behavior. We detail a multi-level experience abstraction method and a check function mechanism in AgentRR: the former balances experience specificity and generality, while the latter serves as a trust anchor to ensure completeness and safety during replay. In addition, we explore multiple application modes of AgentRR, including user-recorded task demonstration, large-small model collaboration and privacy-aware agent execution, and envision an experience repository for sharing and reusing knowledge to further reduce deployment cost.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEAR: Equal Area Weather Forecasting on the Sphere</title>
<link>https://arxiv.org/abs/2505.17720</link>
<guid>https://arxiv.org/abs/2505.17720</guid>
<content:encoded><![CDATA[

arXiv:2505.17720v1 Announce Type: new 
Abstract: Machine learning methods for global medium-range weather forecasting have recently received immense attention. Following the publication of the Pangu Weather model, the first deep learning model to outperform traditional numerical simulations of the atmosphere, numerous models have been published in this domain, building on Pangu's success. However, all of these models operate on input data and produce predictions on the Driscoll--Healy discretization of the sphere which suffers from a much finer grid at the poles than around the equator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization (HEALPix) of the sphere, each pixel covers the same surface area, removing unphysical biases. Motivated by a growing support for this grid in meteorology and climate sciences, we propose to perform weather forecasting with deep learning models which natively operate on the HEALPix grid. To this end, we introduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting model which operates directly on HEALPix-features and outperforms the corresponding model on Driscoll--Healy without any computational overhead.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redirection for Erasing Memory (REM): Towards a universal unlearning method for corrupted data</title>
<link>https://arxiv.org/abs/2505.17730</link>
<guid>https://arxiv.org/abs/2505.17730</guid>
<content:encoded><![CDATA[

arXiv:2505.17730v1 Announce Type: new 
Abstract: Machine unlearning is studied for a multitude of tasks, but specialization of unlearning methods to particular tasks has made their systematic comparison challenging. To address this issue, we propose a conceptual space to characterize diverse corrupted data unlearning tasks in vision classifiers. This space is described by two dimensions, the discovery rate (the fraction of the corrupted data that are known at unlearning time) and the statistical regularity of the corrupted data (from random exemplars to shared concepts). Methods proposed previously have been targeted at portions of this space and-we show-fail predictably outside these regions. We propose a novel method, Redirection for Erasing Memory (REM), whose key feature is that corrupted data are redirected to dedicated neurons introduced at unlearning time and then discarded or deactivated to suppress the influence of corrupted data. REM performs strongly across the space of tasks, in contrast to prior SOTA methods that fail outside the regions for which they were designed.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2505.17734</link>
<guid>https://arxiv.org/abs/2505.17734</guid>
<content:encoded><![CDATA[

arXiv:2505.17734v1 Announce Type: new 
Abstract: Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future urban networks, potentially by optimizing their routing decisions. Unlike for human drivers, these decisions can be made with collective, data-driven policies, developed by machine learning algorithms. Reinforcement learning (RL) can facilitate the development of such collective routing strategies, yet standardized and realistic benchmarks are missing. To that end, we present \our{}: Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles. \our{} is a comprehensive benchmarking environment that unifies evaluation across 29 real-world traffic networks paired with realistic demand patterns. \our{} comes with a catalog of predefined tasks, four state-of-the-art multi-agent RL (MARL) algorithm implementations, three baseline methods, domain-specific performance metrics, and a modular configuration scheme. Our results suggest that, despite the lengthy and costly training, state-of-the-art MARL algorithms rarely outperformed humans. Experimental results reported in this paper initiate the first leaderboard for MARL in large-scale urban routing optimization and reveal that current approaches struggle to scale, emphasizing the urgent need for advancements in this domain.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A tensor network approach for chaotic time series prediction</title>
<link>https://arxiv.org/abs/2505.17740</link>
<guid>https://arxiv.org/abs/2505.17740</guid>
<content:encoded><![CDATA[

arXiv:2505.17740v1 Announce Type: new 
Abstract: Making accurate predictions of chaotic time series is a complex challenge. Reservoir computing, a neuromorphic-inspired approach, has emerged as a powerful tool for this task. It exploits the memory and nonlinearity of dynamical systems without requiring extensive parameter tuning. However, selecting and optimizing reservoir architectures remains an open problem. Next-generation reservoir computing simplifies this problem by employing nonlinear vector autoregression based on truncated Volterra series, thereby reducing hyperparameter complexity. Nevertheless, the latter suffers from exponential parameter growth in terms of the maximum monomial degree. Tensor networks offer a promising solution to this issue by decomposing multidimensional arrays into low-dimensional structures, thus mitigating the curse of dimensionality. This paper explores the application of a previously proposed tensor network model for predicting chaotic time series, demonstrating its advantages in terms of accuracy and computational efficiency compared to conventional echo state networks. Using a state-of-the-art tensor network approach enables us to bridge the gap between the tensor network and reservoir computing communities, fostering advances in both fields.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Neural Flow Samplers with Locally Equivariant Transformer</title>
<link>https://arxiv.org/abs/2505.17741</link>
<guid>https://arxiv.org/abs/2505.17741</guid>
<content:encoded><![CDATA[

arXiv:2505.17741v1 Announce Type: new 
Abstract: Sampling from unnormalised discrete distributions is a fundamental problem across various domains. While Markov chain Monte Carlo offers a principled approach, it often suffers from slow mixing and poor convergence. In this paper, we propose Discrete Neural Flow Samplers (DNFS), a trainable and efficient framework for discrete sampling. DNFS learns the rate matrix of a continuous-time Markov chain such that the resulting dynamics satisfy the Kolmogorov equation. As this objective involves the intractable partition function, we then employ control variates to reduce the variance of its Monte Carlo estimation, leading to a coordinate descent learning algorithm. To further facilitate computational efficiency, we propose locally equivaraint Transformer, a novel parameterisation of the rate matrix that significantly improves training efficiency while preserving powerful network expressiveness. Empirically, we demonstrate the efficacy of DNFS in a wide range of applications, including sampling from unnormalised distributions, training discrete energy-based models, and solving combinatorial optimisation problems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization</title>
<link>https://arxiv.org/abs/2505.17745</link>
<guid>https://arxiv.org/abs/2505.17745</guid>
<content:encoded><![CDATA[

arXiv:2505.17745v1 Announce Type: new 
Abstract: Meta-Black-Box Optimization (MetaBBO) streamlines the automation of optimization algorithm design through meta-learning. It typically employs a bi-level structure: the meta-level policy undergoes meta-training to reduce the manual effort required in developing algorithms for low-level optimization tasks. The original MetaBox (2023) provided the first open-source framework for reinforcement learning-based single-objective MetaBBO. However, its relatively narrow scope no longer keep pace with the swift advancement in this field. In this paper, we introduce MetaBox-v2 (https://github.com/MetaEvo/MetaBox) as a milestone upgrade with four novel features: 1) a unified architecture supporting RL, evolutionary, and gradient-based approaches, by which we reproduce 23 up-to-date baselines; 2) efficient parallelization schemes, which reduce the training/testing time by 10-40x; 3) a comprehensive benchmark suite of 18 synthetic/realistic tasks (1900+ instances) spanning single-objective, multi-objective, multi-model, and multi-task optimization scenarios; 4) plentiful and extensible interfaces for custom analysis/visualization and integrating to external optimization tools/benchmarks. To show the utility of MetaBox-v2, we carry out a systematic case study that evaluates the built-in baselines in terms of the optimization performance, generalization ability and learning efficiency. Valuable insights are concluded from thorough and detailed analysis for practitioners and those new to the field.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft-CAM: Making black box models self-explainable for high-stakes decisions</title>
<link>https://arxiv.org/abs/2505.17748</link>
<guid>https://arxiv.org/abs/2505.17748</guid>
<content:encoded><![CDATA[

arXiv:2505.17748v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) are widely used for high-stakes applications like medicine, often surpassing human performance. However, most explanation methods rely on post-hoc attribution, approximating the decision-making process of already trained black-box models. These methods are often sensitive, unreliable, and fail to reflect true model reasoning, limiting their trustworthiness in critical applications. In this work, we introduce SoftCAM, a straightforward yet effective approach that makes standard CNN architectures inherently interpretable. By removing the global average pooling layer and replacing the fully connected classification layer with a convolution-based class evidence layer, SoftCAM preserves spatial information and produces explicit class activation maps that form the basis of the model's predictions. Evaluated on three medical datasets, SoftCAM maintains classification performance while significantly improving both the qualitative and quantitative explanation compared to existing post-hoc methods. Our results demonstrate that CNNs can be inherently interpretable without compromising performance, advancing the development of self-explainable deep learning for high-stakes decision-making.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17749</link>
<guid>https://arxiv.org/abs/2505.17749</guid>
<content:encoded><![CDATA[

arXiv:2505.17749v1 Announce Type: new 
Abstract: Scaling deep reinforcement learning in pixel-based environments presents a significant challenge, often resulting in diminished performance. While recent works have proposed algorithmic and architectural approaches to address this, the underlying cause of the performance drop remains unclear. In this paper, we identify the connection between the output of the encoder (a stack of convolutional layers) and the ensuing dense layers as the main underlying factor limiting scaling capabilities; we denote this connection as the bottleneck, and we demonstrate that previous approaches implicitly target this bottleneck. As a result of our analyses, we present global average pooling as a simple yet effective way of targeting the bottleneck, thereby avoiding the complexity of earlier approaches.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors</title>
<link>https://arxiv.org/abs/2505.17760</link>
<guid>https://arxiv.org/abs/2505.17760</guid>
<content:encoded><![CDATA[

arXiv:2505.17760v1 Announce Type: new 
Abstract: Recent safety evaluations of Large Language Models (LLMs) show that many models exhibit dishonest behavior, such as sycophancy. However, most honesty benchmarks focus exclusively on factual knowledge or explicitly harmful behavior and rely on external judges, which are often unable to detect less obvious forms of dishonesty. In this work, we introduce a new framework, Judge Using Safety-Steered Alternatives (JUSSA), which utilizes steering vectors trained on a single sample to elicit more honest responses from models, helping LLM-judges in the detection of dishonest behavior. To test our framework, we introduce a new manipulation dataset with prompts specifically designed to elicit deceptive responses. We find that JUSSA enables LLM judges to better differentiate between dishonest and benign responses, and helps them identify subtle instances of manipulative behavior.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models</title>
<link>https://arxiv.org/abs/2505.17761</link>
<guid>https://arxiv.org/abs/2505.17761</guid>
<content:encoded><![CDATA[

arXiv:2505.17761v1 Announce Type: new 
Abstract: Structured Linear Controlled Differential Equations (SLiCEs) provide a unifying framework for sequence models with structured, input-dependent state-transition matrices that retain the maximal expressivity of dense matrices whilst being cheaper to compute. The framework encompasses existing architectures, such as input-dependent block-diagonal linear recurrent neural networks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel variants based on sparsity and the Walsh--Hadamard transform. We prove that, unlike the diagonal state-transition matrices of S4 and Mamba, SLiCEs employing block-diagonal, sparse, or Walsh--Hadamard matrices match the maximal expressivity of dense matrices. Empirically, SLiCEs solve the $A_5$ state-tracking benchmark with a single layer, achieve best-in-class length generalisation on regular language tasks among parallel-in-time models, and match the state-of-the-art performance of log neural controlled differential equations on six multivariate time-series classification datasets while cutting the average time per training step by a factor of twenty.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Clustering for Fault Analysis in High-Voltage Power Systems Using Voltage and Current Signals</title>
<link>https://arxiv.org/abs/2505.17763</link>
<guid>https://arxiv.org/abs/2505.17763</guid>
<content:encoded><![CDATA[

arXiv:2505.17763v1 Announce Type: new 
Abstract: The widespread use of sensors in modern power grids has led to the accumulation of large amounts of voltage and current waveform data, especially during fault events. However, the lack of labeled datasets poses a significant challenge for fault classification and analysis. This paper explores the application of unsupervised clustering techniques for fault diagnosis in high-voltage power systems. A dataset provided by the Reseau de Transport d'Electricite (RTE) is analyzed, with frequency domain features extracted using the Fast Fourier Transform (FFT). The K-Means algorithm is then applied to identify underlying patterns in the data, enabling automated fault categorization without the need for labeled training samples. The resulting clusters are evaluated in collaboration with power system experts to assess their alignment with real-world fault characteristics. The results demonstrate the potential of unsupervised learning for scalable and data-driven fault analysis, providing a robust approach to detecting and classifying power system faults with minimal prior assumptions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joker: Joint Optimization Framework for Lightweight Kernel Machines</title>
<link>https://arxiv.org/abs/2505.17765</link>
<guid>https://arxiv.org/abs/2505.17765</guid>
<content:encoded><![CDATA[

arXiv:2505.17765v1 Announce Type: new 
Abstract: Kernel methods are powerful tools for nonlinear learning with well-established theory. The scalability issue has been their long-standing challenge. Despite the existing success, there are two limitations in large-scale kernel methods: (i) The memory overhead is too high for users to afford; (ii) existing efforts mainly focus on kernel ridge regression (KRR), while other models lack study. In this paper, we propose Joker, a joint optimization framework for diverse kernel models, including KRR, logistic regression, and support vector machines. We design a dual block coordinate descent method with trust region (DBCD-TR) and adopt kernel approximation with randomized features, leading to low memory costs and high efficiency in large-scale learning. Experiments show that Joker saves up to 90\% memory but achieves comparable training time and performance (or even better) than the state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models</title>
<link>https://arxiv.org/abs/2505.17769</link>
<guid>https://arxiv.org/abs/2505.17769</guid>
<content:encoded><![CDATA[

arXiv:2505.17769v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage Models (LLM) activations into interpretable latents. However, due to their substantial training cost, most academic research uses open-source SAEs which are only available for a restricted set of models of up to 27B parameters. SAE latents are also learned from a dataset of activations, which means they do not transfer between models. Motivated by relative representation similarity measures, we introduce Inference-Time Decomposition of Activations (ITDA) models, an alternative method for decomposing language model activations. To train an ITDA, we greedily construct a dictionary of language model activations on a dataset of prompts, selecting those activations which were worst approximated by matching pursuit on the existing dictionary. ITDAs can be trained in just 1\% of the time required for SAEs, using 1\% of the data. This allowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU. ITDAs can achieve similar reconstruction performance to SAEs on some target LLMs, but generally incur a performance penalty. However, ITDA dictionaries enable cross-model comparisons, and a simple Jaccard similarity index on ITDA dictionaries outperforms existing methods like CKA, SVCCA, and relative representation similarity metrics. ITDAs provide a cheap alternative to SAEs where computational resources are limited, or when cross model comparisons are necessary. Code available at https://github.com/pleask/itda.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models</title>
<link>https://arxiv.org/abs/2505.17773</link>
<guid>https://arxiv.org/abs/2505.17773</guid>
<content:encoded><![CDATA[

arXiv:2505.17773v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning large language models (LLMs), but it often produces overconfident predictions in data-scarce few-shot settings. To address this issue, several classical statistical learning approaches have been repurposed for scalable uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input characteristics affect the predictive uncertainty estimates. To address this limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a novel uncertainty-aware and parameter efficient fine-tuning approach, by developing new lightweight LoRA modules contextualized to each input data sample to dynamically adapt uncertainty estimates. Incorporating data-driven contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves well-calibrated uncertainties, and yields robust predictions. Extensive experiments demonstrate that C-LoRA consistently outperforms the state-of-the-art uncertainty-aware LoRA methods in both uncertainty quantification and model generalization. Ablation studies further confirm the critical role of our contextual modules in capturing sample-specific uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot regimes.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Shortfall Risk Metric for Learning Regression Models</title>
<link>https://arxiv.org/abs/2505.17777</link>
<guid>https://arxiv.org/abs/2505.17777</guid>
<content:encoded><![CDATA[

arXiv:2505.17777v1 Announce Type: new 
Abstract: We consider the problem of estimating and optimizing utility-based shortfall risk (UBSR) of a loss, say $(Y - \hat Y)^2$, in the context of a regression problem. Empirical risk minimization with a UBSR objective is challenging since UBSR is a non-linear function of the underlying distribution. We first derive a concentration bound for UBSR estimation using independent and identically distributed (i.i.d.) samples. We then frame the UBSR optimization problem as minimization of a pseudo-linear function in the space of achievable distributions $\mathcal D$ of the loss $(Y- \hat Y)^2$. We construct a gradient oracle for the UBSR objective and a linear minimization oracle (LMO) for the set $\mathcal D$. Using these oracles, we devise a bisection-type algorithm, and establish convergence to the UBSR-optimal solution.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Graph Contrastive Learning for Gene Regulatory Network</title>
<link>https://arxiv.org/abs/2505.17786</link>
<guid>https://arxiv.org/abs/2505.17786</guid>
<content:encoded><![CDATA[

arXiv:2505.17786v1 Announce Type: new 
Abstract: Graph representation learning is effective for obtaining a meaningful latent space utilizing the structure of graph data and is widely applied, including biological networks. In particular, Graph Contrastive Learning (GCL) has emerged as a powerful self-supervised method that relies on applying perturbations to graphs for data augmentation. However, when applying existing GCL methods to biological networks such as Gene Regulatory Networks (GRNs), they overlooked meaningful biologically relevant perturbations, e.g., gene knockdowns. In this study, we introduce SupGCL (Supervised Graph Contrastive Learning), a novel GCL method for GRNs that directly incorporates biological perturbations derived from gene knockdown experiments as the supervision. SupGCL mathematically extends existing GCL methods that utilize non-biological perturbations to probabilistic models that introduce actual biological gene perturbation utilizing gene knockdown data. Using the GRN representation obtained by our proposed method, our aim is to improve the performance of biological downstream tasks such as patient hazard prediction and disease subtype classification (graph-level task), and gene function classification (node-level task). We applied SupGCL on real GRN datasets derived from patients with multiple types of cancer, and in all experiments SupGCL achieves better performance than state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2505.17794</link>
<guid>https://arxiv.org/abs/2505.17794</guid>
<content:encoded><![CDATA[

arXiv:2505.17794v1 Announce Type: new 
Abstract: Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped relations between entities. TKG completion involves forecasting missing or future links, requiring models to reason over time-evolving structure. While LLMs show promise for this task, existing approaches often overemphasize supervised fine-tuning and struggle particularly when historical evidence is limited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient framework designed to improve accuracy and generalization in settings with sparse historical context. It combines (1) rule-based multi-hop retrieval for structurally diverse history, (2) contrastive fine-tuning of lightweight adapters to encode relational semantics, and (3) test-time semantic filtering to iteratively refine generations based on embedding similarity. Experiments on four TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based approaches, achieving up to 30.6\% relative improvement in Hits@10. Moreover, our proposed framework produces more semantically coherent predictions, even for the samples with limited historical context.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Mode Decomposition</title>
<link>https://arxiv.org/abs/2505.17797</link>
<guid>https://arxiv.org/abs/2505.17797</guid>
<content:encoded><![CDATA[

arXiv:2505.17797v1 Announce Type: new 
Abstract: We introduce Variational Latent Mode Decomposition (VLMD), a new algorithm for extracting oscillatory modes and associated connectivity structures from multivariate signals. VLMD addresses key limitations of existing Multivariate Mode Decomposition (MMD) techniques -including high computational cost, sensitivity to parameter choices, and weak modeling of interchannel dependencies. Its improved performance is driven by a novel underlying model, Latent Mode Decomposition (LMD), which blends sparse coding and mode decomposition to represent multichannel signals as sparse linear combinations of shared latent components composed of AM-FM oscillatory modes. This formulation enables VLMD to operate in a lower-dimensional latent space, enhancing robustness to noise, scalability, and interpretability. The algorithm solves a constrained variational optimization problem that jointly enforces reconstruction fidelity, sparsity, and frequency regularization. Experiments on synthetic and real-world datasets demonstrate that VLMD outperforms state-of-the-art MMD methods in accuracy, efficiency, and interpretability of extracted structures.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances</title>
<link>https://arxiv.org/abs/2505.17799</link>
<guid>https://arxiv.org/abs/2505.17799</guid>
<content:encoded><![CDATA[

arXiv:2505.17799v1 Announce Type: new 
Abstract: Coreset selection targets the challenge of finding a small, representative subset of a large dataset that preserves essential patterns for effective machine learning. Although several surveys have examined data reduction strategies before, most focus narrowly on either classical geometry-based methods or active learning techniques. In contrast, this survey presents a more comprehensive view by unifying three major lines of coreset research, namely, training-free, training-oriented, and label-free approaches, into a single taxonomy. We present subfields often overlooked by existing work, including submodular formulations, bilevel optimization, and recent progress in pseudo-labeling for unlabeled datasets. Additionally, we examine how pruning strategies influence generalization and neural scaling laws, offering new insights that are absent from prior reviews. Finally, we compare these methods under varying computational, robustness, and performance demands and highlight open challenges, such as robustness, outlier filtering, and adapting coreset selection to foundation models, for future research.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperparameter Optimization via Interacting with Probabilistic Circuits</title>
<link>https://arxiv.org/abs/2505.17804</link>
<guid>https://arxiv.org/abs/2505.17804</guid>
<content:encoded><![CDATA[

arXiv:2505.17804v1 Announce Type: new 
Abstract: Despite the growing interest in designing truly interactive hyperparameter optimization (HPO) methods, to date, only a few allow to include human feedback. Existing interactive Bayesian optimization (BO) methods incorporate human beliefs by weighting the acquisition function with a user-defined prior distribution. However, in light of the non-trivial inner optimization of the acquisition function prevalent in BO, such weighting schemes do not always accurately reflect given user beliefs. We introduce a novel BO approach leveraging tractable probabilistic models named probabilistic circuits (PCs) as a surrogate model. PCs encode a tractable joint distribution over the hybrid hyperparameter space and evaluation scores. They enable exact conditional inference and sampling. Based on conditional sampling, we construct a novel selection policy that enables an acquisition function-free generation of candidate points (thereby eliminating the need for an additional inner-loop optimization) and ensures that user beliefs are reflected accurately in the selection policy. We provide a theoretical analysis and an extensive empirical evaluation, demonstrating that our method achieves state-of-the-art performance in standard HPO and outperforms interactive BO baselines in interactive HPO.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIBE: Vector Index Benchmark for Embeddings</title>
<link>https://arxiv.org/abs/2505.17810</link>
<guid>https://arxiv.org/abs/2505.17810</guid>
<content:encoded><![CDATA[

arXiv:2505.17810v1 Announce Type: new 
Abstract: Approximate nearest neighbor (ANN) search is a performance-critical component of many machine learning pipelines. Rigorous benchmarking is essential for evaluating the performance of vector indexes for ANN search. However, the datasets of the existing benchmarks are no longer representative of the current applications of ANN search. Hence, there is an urgent need for an up-to-date set of benchmarks. To this end, we introduce Vector Index Benchmark for Embeddings (VIBE), an open source project for benchmarking ANN algorithms. VIBE contains a pipeline for creating benchmark datasets using dense embedding models characteristic of modern applications, such as retrieval-augmented generation (RAG). To replicate real-world workloads, we also include out-of-distribution (OOD) datasets where the queries and the corpus are drawn from different distributions. We use VIBE to conduct a comprehensive evaluation of SOTA vector indexes, benchmarking 21 implementations on 12 in-distribution and 6 out-of-distribution datasets.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2505.17826</link>
<guid>https://arxiv.org/abs/2505.17826</guid>
<content:encoded><![CDATA[

arXiv:2505.17826v1 Announce Type: new 
Abstract: Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT, (2) seamless integration for agent-environment interaction with high efficiency and robustness, and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for exploring advanced reinforcement learning paradigms. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples demonstrating the utility and user-friendliness of the proposed framework.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17830</link>
<guid>https://arxiv.org/abs/2505.17830</guid>
<content:encoded><![CDATA[

arXiv:2505.17830v1 Announce Type: new 
Abstract: Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously acquire diverse behaviors, but faces major challenges in visual environments due to high-dimensional, semantically sparse observations. In the online setting, where agents learn representations while exploring, the latent space evolves with the agent's policy, to capture newly discovered areas of the environment. However, without incentivization to maximize state coverage in the representation, classical approaches based on auto-encoders may converge to latent spaces that over-represent a restricted set of states frequently visited by the agent. This is exacerbated in an intrinsic motivation setting, where the agent uses the distribution encoded in the latent space to sample the goals it learns to master. To address this issue, we propose to progressively enforce distributional shifts towards a uniform distribution over the full state space, to ensure a full coverage of skills that can be learned in the environment. We introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that combines the $\beta$-VAE framework with Distributionally Robust Optimization. DRAG leverages an adversarial neural weighter of training states of the VAE, to account for the mismatch between the current data distribution and unseen parts of the environment. This allows the agent to construct semantically meaningful latent spaces beyond its immediate experience. Our approach improves state space coverage and downstream control performance on hard exploration environments such as mazes and robotic control involving walls to bypass, without pre-training nor prior environment knowledge.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransDF: Time-Series Forecasting Needs Transformed Label Alignment</title>
<link>https://arxiv.org/abs/2505.17847</link>
<guid>https://arxiv.org/abs/2505.17847</guid>
<content:encoded><![CDATA[

arXiv:2505.17847v1 Announce Type: new 
Abstract: Training time-series forecasting models presents unique challenges in designing effective learning objectives. Existing methods predominantly utilize the temporal mean squared error, which faces two critical challenges: (1) label autocorrelation, which leads to bias from the label sequence likelihood; (2) excessive amount of tasks, which increases with the forecast horizon and complicates optimization. To address these challenges, we propose Transform-enhanced Direct Forecast (TransDF), which transforms the label sequence into decorrelated components with discriminated significance. Models are trained to align the most significant components, thereby effectively mitigating label autocorrelation and reducing task amount. Extensive experiments demonstrate that TransDF achieves state-of-the-art performance and is compatible with various forecasting models. Code is available at https://anonymous.4open.science/r/TransDF-88CF.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization</title>
<link>https://arxiv.org/abs/2505.17852</link>
<guid>https://arxiv.org/abs/2505.17852</guid>
<content:encoded><![CDATA[

arXiv:2505.17852v1 Announce Type: new 
Abstract: During inference, Recurrent Neural Networks (RNNs) scale constant in both FLOPs and GPU memory with increasing context length, as they compress all prior tokens into a fixed-size memory. In contrast, transformers scale linearly in FLOPs and, at best, linearly in memory during generation, since they must attend to all previous tokens explicitly. Despite this inference-time advantage, training large RNNs on long contexts remains impractical because standard optimization methods depend on Backpropagation Through Time (BPTT). BPTT requires retention of all intermediate activations during the forward pass, causing memory usage to scale linearly with both context length and model size. In this paper, we show that Zero-Order Optimization (ZOO) methods such as Random-vector Gradient Estimation (RGE) can successfully replace BPTT to train RNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while using orders of magnitude less memory and cost, as the model remains in inference mode throughout training. We further demonstrate that Central-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate loss, inherently regularizing training and improving generalization. Our method matches or outperforms BPTT across three settings: (1) overfitting, (2) transduction, and (3) language modeling. Across all tasks, with sufficient perturbations, our models generalize as well as or better than those trained with BPTT, often in fewer steps. Despite the need for more forward passes per step, we can surpass BPTT wall-clock time per step using recent advancements such as FlashRNN and distributed inference.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out of the Shadows: Exploring a Latent Space for Neural Network Verification</title>
<link>https://arxiv.org/abs/2505.17854</link>
<guid>https://arxiv.org/abs/2505.17854</guid>
<content:encoded><![CDATA[

arXiv:2505.17854v1 Announce Type: new 
Abstract: Neural networks are ubiquitous. However, they are often sensitive to small input changes. Hence, to prevent unexpected behavior in safety-critical applications, their formal verification -- a notoriously hard problem -- is necessary. Many state-of-the-art verification algorithms use reachability analysis or abstract interpretation to enclose the set of possible outputs of a neural network. Often, the verification is inconclusive due to the conservatism of the enclosure. To address this problem, we design a novel latent space for formal verification that enables the transfer of output specifications to the input space for an iterative specification-driven input refinement, i.e., we iteratively reduce the set of possible inputs to only enclose the unsafe ones. The latent space is constructed from a novel view of projection-based set representations, e.g., zonotopes, which are commonly used in reachability analysis of neural networks. A projection-based set representation is a "shadow" of a higher-dimensional set -- a latent space -- that does not change during a set propagation through a neural network. Hence, the input set and the output enclosure are "shadows" of the same latent space that we can use to transfer constraints. We present an efficient verification tool for neural networks that uses our iterative refinement to significantly reduce the number of subproblems in a branch-and-bound procedure. Using zonotopes as a set representation, unlike many other state-of-the-art approaches, our approach can be realized by only using matrix operations, which enables a significant speed-up through efficient GPU acceleration. We demonstrate that our tool achieves competitive performance, which would place it among the top-ranking tools of the last neural network verification competition (VNN-COMP'24).
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Weight Sharing for Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2505.17856</link>
<guid>https://arxiv.org/abs/2505.17856</guid>
<content:encoded><![CDATA[

arXiv:2505.17856v1 Announce Type: new 
Abstract: While offering a principled framework for uncertainty quantification in deep learning, the employment of Bayesian Neural Networks (BNNs) is still constrained by their increased computational requirements and the convergence difficulties when training very deep, state-of-the-art architectures. In this work, we reinterpret weight-sharing quantization techniques from a stochastic perspective in the context of training and inference with Bayesian Neural Networks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions, Wasserstein distance estimations, and alpha blending to encode the stochastic behaviour of a BNN in a lower dimensional, soft Gaussian representation. Through extensive empirical investigation, we demonstrate that our approach significantly reduces the computational overhead inherent in Bayesian learning by several orders of magnitude, enabling the efficient Bayesian training of large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various computer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our approach compresses model parameters by approximately 50x and reduces model size by 75, while achieving accuracy and uncertainty estimations comparable to the state-of-the-art.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Valuation of Human Feedback through Provably Robust Model Alignment</title>
<link>https://arxiv.org/abs/2505.17859</link>
<guid>https://arxiv.org/abs/2505.17859</guid>
<content:encoded><![CDATA[

arXiv:2505.17859v1 Announce Type: new 
Abstract: Despite the importance of aligning language models with human preferences, crowd-sourced human feedback is often noisy -- for example, preferring less desirable responses -- posing a fundamental challenge to alignment. A truly robust alignment objective should yield identical model parameters even under severe label noise, a property known as redescending. We prove that no existing alignment methods satisfy this property. To address this, we propose H\"older-DPO, the first principled alignment loss with a provable redescending property, enabling estimation of the clean data distribution from noisy feedback. The aligned model estimates the likelihood of clean data, providing a theoretically grounded metric for dataset valuation that identifies the location and fraction of mislabels. This metric is gradient-free, enabling scalable and automated human feedback valuation without costly manual verification or clean validation dataset. H\"older-DPO achieves state-of-the-art robust alignment performance while accurately detecting mislabels in controlled datasets. Finally, we apply H\"older-DPO to widely used alignment datasets, revealing substantial noise levels and demonstrating that removing these mislabels significantly improves alignment performance across methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The emergence of sparse attention: impact of data distribution and benefits of repetition</title>
<link>https://arxiv.org/abs/2505.17863</link>
<guid>https://arxiv.org/abs/2505.17863</guid>
<content:encoded><![CDATA[

arXiv:2505.17863v1 Announce Type: new 
Abstract: Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization</title>
<link>https://arxiv.org/abs/2505.17866</link>
<guid>https://arxiv.org/abs/2505.17866</guid>
<content:encoded><![CDATA[

arXiv:2505.17866v1 Announce Type: new 
Abstract: Designing effective black-box optimizers is hampered by limited problem-specific knowledge and manual control that spans months for almost every detail. In this paper, we present DesignX, the first automated algorithm design framework that generates an effective optimizer specific to a given black-box optimization problem within seconds. Rooted in the first principles, we identify two key sub-tasks: 1) algorithm structure generation and 2) hyperparameter control. To enable systematic construction, a comprehensive modular algorithmic space is first built, embracing hundreds of algorithm components collected from decades of research. We then introduce a dual-agent reinforcement learning system that collaborates on structural and parametric design through a novel cooperative training objective, enabling large-scale meta-training across 10k diverse instances. Remarkably, through days of autonomous learning, the DesignX-generated optimizers continuously surpass human-crafted optimizers by orders of magnitude, either on synthetic testbed or on realistic optimization scenarios such as Protein-docking, AutoML and UAV path planning. Further in-depth analysis reveals DesignX's capability to discover non-trivial algorithm patterns beyond expert intuition, which, conversely, provides valuable design insights for the optimization community. We provide DesignX's inference code at https://github.com/MetaEvo/DesignX.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpectraLDS: Provable Distillation for Linear Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.17868</link>
<guid>https://arxiv.org/abs/2505.17868</guid>
<content:encoded><![CDATA[

arXiv:2505.17868v1 Announce Type: new 
Abstract: We present the first provable method for identifying symmetric linear dynamical systems (LDS) with accuracy guarantees that are independent of the systems' state dimension or effective memory. Our approach builds upon recent work that represents symmetric LDSs as convolutions learnable via fixed spectral transformations. We show how to invert this representation, thereby recovering an LDS model from its spectral transform and yielding an end-to-end convex optimization procedure. This distillation preserves predictive accuracy while enabling constant-time and constant-space inference per token, independent of sequence length. We evaluate our method, SpectraLDS, as a component in sequence prediction architectures and demonstrate that accuracy is preserved while inference efficiency is improved on tasks such as language modeling.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Best Group Identification in Multi-Objective Bandits</title>
<link>https://arxiv.org/abs/2505.17869</link>
<guid>https://arxiv.org/abs/2505.17869</guid>
<content:encoded><![CDATA[

arXiv:2505.17869v1 Announce Type: new 
Abstract: We introduce the Best Group Identification problem in a multi-objective multi-armed bandit setting, where an agent interacts with groups of arms with vector-valued rewards. The performance of a group is determined by an efficiency vector which represents the group's best attainable rewards across different dimensions. The objective is to identify the set of optimal groups in the fixed-confidence setting. We investigate two key formulations: group Pareto set identification, where efficiency vectors of optimal groups are Pareto optimal and linear best group identification, where each reward dimension has a known weight and the optimal group maximizes the weighted sum of its efficiency vector's entries. For both settings, we propose elimination-based algorithms, establish upper bounds on their sample complexity, and derive lower bounds that apply to any correct algorithm. Through numerical experiments, we demonstrate the strong empirical performance of the proposed algorithms.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models</title>
<link>https://arxiv.org/abs/2505.17871</link>
<guid>https://arxiv.org/abs/2505.17871</guid>
<content:encoded><![CDATA[

arXiv:2505.17871v1 Announce Type: new 
Abstract: The advent of universal time series forecasting models has revolutionized zero-shot forecasting across diverse domains, yet the critical role of data diversity in training these models remains underexplored. Existing large-scale time series datasets often suffer from inherent biases and imbalanced distributions, leading to suboptimal model performance and generalization. To address this gap, we introduce BLAST, a novel pre-training corpus designed to enhance data diversity through a balanced sampling strategy. First, BLAST incorporates 321 billion observations from publicly available datasets and employs a comprehensive suite of statistical metrics to characterize time series patterns. Then, to facilitate pattern-oriented sampling, the data is implicitly clustered using grid-based partitioning. Furthermore, by integrating grid sampling and grid mixup techniques, BLAST ensures a balanced and representative coverage of diverse patterns. Experimental results demonstrate that models pre-trained on BLAST achieve state-of-the-art performance with a fraction of the computational resources and training tokens required by existing methods. Our findings highlight the pivotal role of data diversity in improving both training efficiency and model performance for the universal forecasting task.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.17872</link>
<guid>https://arxiv.org/abs/2505.17872</guid>
<content:encoded><![CDATA[

arXiv:2505.17872v1 Announce Type: new 
Abstract: Multi-task forecasting has become the standard approach for time-series forecasting (TSF). However, we show that it suffers from an Expressiveness Bottleneck, where predictions at different time steps share the same representation, leading to unavoidable errors even with optimal representations. To address this issue, we propose a two-stage framework: first, pre-train a foundation model for one-step-ahead prediction; then, adapt it using step-specific LoRA modules.This design enables the foundation model to handle any number of forecast steps while avoiding the expressiveness bottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which employs adaptively weighted LoRA experts to achieve partial parameter sharing across steps. This approach enhances both efficiency and forecasting performance by exploiting interdependencies between forecast steps. Experiments show that MoLA significantly improves model expressiveness and outperforms state-of-the-art time-series forecasting methods. Code is available at https://anonymous.4open.science/r/MoLA-BC92.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph Learning</title>
<link>https://arxiv.org/abs/2505.17875</link>
<guid>https://arxiv.org/abs/2505.17875</guid>
<content:encoded><![CDATA[

arXiv:2505.17875v1 Announce Type: new 
Abstract: In practical domains, high-dimensional data are usually associated with diverse semantic labels, whereas traditional feature selection methods are designed for single-label data. Moreover, existing multi-label methods encounter two main challenges in semi-supervised scenarios: (1). Most semi-supervised methods fail to evaluate the label correlations without enough labeled samples, which are the critical information of multi-label feature selection, making label-specific features discarded. (2). The similarity graph structure directly derived from the original feature space is suboptimal for multi-label problems in existing graph-based methods, leading to unreliable soft labels and degraded feature selection performance. To overcome them, we propose a consistent sparse graph learning method for multi-label semi-supervised feature selection (SGMFS), which can enhance the feature selection performance by maintaining space consistency and learning label correlations in semi-supervised scenarios. Specifically, for Challenge (1), SGMFS learns a low-dimensional and independent label subspace from the projected features, which can compatibly cross multiple labels and effectively achieve the label correlations. For Challenge (2), instead of constructing a fixed similarity graph for semi-supervised learning, SGMFS thoroughly explores the intrinsic structure of the data by performing sparse reconstruction of samples in both the label space and the learned subspace simultaneously. In this way, the similarity graph can be adaptively learned to maintain the consistency between label space and the learned subspace, which can promote propagating proper soft labels for unlabeled samples, facilitating the ultimate feature selection. An effective solution with fast convergence is designed to optimize the objective function. Extensive experiments validate the superiority of SGMFS.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks</title>
<link>https://arxiv.org/abs/2505.17883</link>
<guid>https://arxiv.org/abs/2505.17883</guid>
<content:encoded><![CDATA[

arXiv:2505.17883v1 Announce Type: new 
Abstract: Concepts such as objects, patterns, and shapes are how humans understand the world. Building on this intuition, concept-based explainability methods aim to study representations learned by deep neural networks in relation to human-understandable concepts. Here, Concept Activation Vectors (CAVs) are an important tool and can identify whether a model learned a concept or not. However, the computational cost and time requirements of existing CAV computation pose a significant challenge, particularly in large-scale, high-dimensional architectures. To address this limitation, we introduce FastCAV, a novel approach that accelerates the extraction of CAVs by up to 63.6x (on average 46.4x). We provide a theoretical foundation for our approach and give concrete assumptions under which it is equivalent to established SVM-based methods. Our empirical results demonstrate that CAVs calculated with FastCAV maintain similar performance while being more efficient and stable. In downstream applications, i.e., concept-based explanation methods, we show that FastCAV can act as a replacement leading to equivalent insights. Hence, our approach enables previously infeasible investigations of deep models, which we demonstrate by tracking the evolution of concepts during model training.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Domain Adaptation Benchmark for Time Series Data Representation</title>
<link>https://arxiv.org/abs/2505.17899</link>
<guid>https://arxiv.org/abs/2505.17899</guid>
<content:encoded><![CDATA[

arXiv:2505.17899v1 Announce Type: new 
Abstract: Deep learning models have significantly improved the ability to detect novelties in time series (TS) data. This success is attributed to their strong representation capabilities. However, due to the inherent variability in TS data, these models often struggle with generalization and robustness. To address this, a common approach is to perform Unsupervised Domain Adaptation, particularly Universal Domain Adaptation (UniDA), to handle domain shifts and emerging novel classes. While extensively studied in computer vision, UniDA remains underexplored for TS data. This work provides a comprehensive implementation and comparison of state-of-the-art TS backbones in a UniDA framework. We propose a reliable protocol to evaluate their robustness and generalization across different domains. The goal is to provide practitioners with a framework that can be easily extended to incorporate future advancements in UniDA and TS architectures. Our results highlight the critical influence of backbone selection in UniDA performance and enable a robustness analysis across various datasets and architectures.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolving Machine Learning: A Survey</title>
<link>https://arxiv.org/abs/2505.17902</link>
<guid>https://arxiv.org/abs/2505.17902</guid>
<content:encoded><![CDATA[

arXiv:2505.17902v1 Announce Type: new 
Abstract: In an era defined by rapid data evolution, traditional machine learning (ML) models often fall short in adapting to dynamic environments. Evolving Machine Learning (EML) has emerged as a critical paradigm, enabling continuous learning and adaptation in real-time data streams. This survey presents a comprehensive analysis of EML, focusing on five core challenges: data drift, concept drift, catastrophic forgetting, skewed learning, and network adaptation. We systematically review over 120 studies, categorizing state-of-the-art methods across supervised, unsupervised, and semi-supervised approaches. The survey explores diverse evaluation metrics, benchmark datasets, and real-world applications, offering a comparative lens on the effectiveness and limitations of current techniques. Additionally, we highlight the growing role of adaptive neural architectures, meta-learning, and ensemble strategies in addressing evolving data complexities. By synthesizing insights from recent literature, this work not only maps the current landscape of EML but also identifies critical gaps and opportunities for future research. Our findings aim to guide researchers and practitioners in developing robust, ethical, and scalable EML systems for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling</title>
<link>https://arxiv.org/abs/2505.17909</link>
<guid>https://arxiv.org/abs/2505.17909</guid>
<content:encoded><![CDATA[

arXiv:2505.17909v1 Announce Type: new 
Abstract: Model ensembles have long been a cornerstone for improving generalization and robustness in deep learning. However, their effectiveness often comes at the cost of substantial computational overhead. To address this issue, state-of-the-art methods aim to replicate ensemble-class performance without requiring multiple independently trained networks. Unfortunately, these algorithms often still demand considerable compute at inference. In response to these limitations, we introduce $\textbf{NeuroTrails}$, a sparse multi-head architecture with dynamically evolving topology. This unexplored model-agnostic training paradigm improves ensemble performance while reducing the required resources. We analyze the underlying reason for its effectiveness and observe that the various neural trails induced by dynamic sparsity attain a $\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays efficacy with convolutional and transformer-based architectures on computer vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4, among many others, demonstrate increased accuracy and stronger robustness in zero-shot generalization, while requiring significantly fewer parameters.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Meeting Decision Trees on Tabular Data</title>
<link>https://arxiv.org/abs/2505.17918</link>
<guid>https://arxiv.org/abs/2505.17918</guid>
<content:encoded><![CDATA[

arXiv:2505.17918v1 Announce Type: new 
Abstract: Tabular data have been playing a vital role in diverse real-world fields, including healthcare, finance, etc. With the recent success of Large Language Models (LLMs), early explorations of extending LLMs to the domain of tabular data have been developed. Most of these LLM-based methods typically first serialize tabular data into natural language descriptions, and then tune LLMs or directly infer on these serialized data. However, these methods suffer from two key inherent issues: (i) data perspective: existing data serialization methods lack universal applicability for structured tabular data, and may pose privacy risks through direct textual exposure, and (ii) model perspective: LLM fine-tuning methods struggle with tabular data, and in-context learning scalability is bottle-necked by input length constraints (suitable for few-shot learning). This work explores a novel direction of integrating LLMs into tabular data throughough logical decision tree rules as intermediaries, proposes a decision tree enhancer with LLM-derived rule for tabular prediction, DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied to full data learning setting without LLM fine-tuning. Specifically, we leverage the reasoning ability of LLMs to redesign an improved rule given a set of decision tree rules. Furthermore, we provide a calibration method for original decision trees via new generated rule by LLM, which approximates the error correction vector to steer the original decision tree predictions in the direction of ``errors'' reducing. Finally, extensive experiments on diverse tabular benchmarks show that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KITINet: Kinetics Theory Inspired Network Architectures with PDE Simulation Approaches</title>
<link>https://arxiv.org/abs/2505.17919</link>
<guid>https://arxiv.org/abs/2505.17919</guid>
<content:encoded><![CDATA[

arXiv:2505.17919v1 Announce Type: new 
Abstract: Despite the widely recognized success of residual connections in modern neural networks, their design principles remain largely heuristic. This paper introduces KITINet (Kinetics Theory Inspired Network), a novel architecture that reinterprets feature propagation through the lens of non-equilibrium particle dynamics and partial differential equation (PDE) simulation. At its core, we propose a residual module that models feature updates as the stochastic evolution of a particle system, numerically simulated via a discretized solver for the Boltzmann transport equation (BTE). This formulation mimics particle collisions and energy exchange, enabling adaptive feature refinement via physics-informed interactions. Additionally, we reveal that this mechanism induces network parameter condensation during training, where parameters progressively concentrate into a sparse subset of dominant channels. Experiments on scientific computation (PDE operator), image classification (CIFAR-10/100), and text classification (IMDb/SNLI) show consistent improvements over classic network baselines, with negligible increase of FLOPs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Length of Stay in Neurological ICU Patients Using Classical Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV</title>
<link>https://arxiv.org/abs/2505.17929</link>
<guid>https://arxiv.org/abs/2505.17929</guid>
<content:encoded><![CDATA[

arXiv:2505.17929v1 Announce Type: new 
Abstract: Intensive care unit (ICU) is a crucial hospital department that handles life-threatening cases. Nowadays machine learning (ML) is being leveraged in healthcare ubiquitously. In recent years, management of ICU became one of the most significant parts of the hospital functionality (largely but not only due to the worldwide COVID-19 pandemic). This study explores multiple ML approaches for predicting LOS in ICU specifically for the patients with neurological diseases based on the MIMIC-IV dataset. The evaluated models include classic ML algorithms (K-Nearest Neighbors, Random Forest, XGBoost and CatBoost) and Neural Networks (LSTM, BERT and Temporal Fusion Transformer). Given that LOS prediction is often framed as a classification task, this study categorizes LOS into three groups: less than two days, less than a week, and a week or more. As the first ML-based approach targeting LOS prediction for neurological disorder patients, this study does not aim to outperform existing methods but rather to assess their effectiveness in this specific context. The findings provide insights into the applicability of ML techniques for improving ICU resource management and patient care. According to the results, Random Forest model proved to outperform others on static, achieving an accuracy of 0.68, a precision of 0.68, a recall of 0.68, and F1-score of 0.67. While BERT model outperformed LSTM model on time-series data with an accuracy of 0.80, a precision of 0.80, a recall of 0.80 and F1-score 0.80.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Gated Neurons in Transformers from Their Input-Output Functionality</title>
<link>https://arxiv.org/abs/2505.17936</link>
<guid>https://arxiv.org/abs/2505.17936</guid>
<content:encoded><![CDATA[

arXiv:2505.17936v1 Announce Type: new 
Abstract: Interpretability researchers have attempted to understand MLP neurons of language models based on both the contexts in which they activate and their output weight vectors. They have paid little attention to a complementary aspect: the interactions between input and output. For example, when neurons detect a direction in the input, they might add much the same direction to the residual stream ("enrichment neurons") or reduce its presence ("depletion neurons"). We address this aspect by examining the cosine similarity between input and output weights of a neuron. We apply our method to 12 models and find that enrichment neurons dominate in early-middle layers whereas later layers tend more towards depletion. To explain this finding, we argue that enrichment neurons are largely responsible for enriching concept representations, one of the first steps of factual recall. Our input-output perspective is a complement to activation-dependent analyses and to approaches that treat input and output separately.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding</title>
<link>https://arxiv.org/abs/2505.17939</link>
<guid>https://arxiv.org/abs/2505.17939</guid>
<content:encoded><![CDATA[

arXiv:2505.17939v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) excel at learning from pairwise interactions but often overlook multi-way and hierarchical relationships. Topological Deep Learning (TDL) addresses this limitation by leveraging combinatorial topological spaces. However, existing TDL models are restricted to undirected settings and fail to capture the higher-order directed patterns prevalent in many complex systems, e.g., brain networks, where such interactions are both abundant and functionally significant. To fill this gap, we introduce Semi-Simplicial Neural Networks (SSNs), a principled class of TDL models that operate on semi-simplicial sets -- combinatorial structures that encode directed higher-order motifs and their directional relationships. To enhance scalability, we propose Routing-SSNs, which dynamically select the most informative relations in a learnable manner. We prove that SSNs are strictly more expressive than standard graph and TDL models. We then introduce a new principled framework for brain dynamics representation learning, grounded in the ability of SSNs to provably recover topological descriptors shown to successfully characterize brain activity. Empirically, SSNs achieve state-of-the-art performance on brain dynamics classification tasks, outperforming the second-best model by up to 27%, and message passing GNNs by up to 50% in accuracy. Our results highlight the potential of principled topological models for learning from structured brain data, establishing a unique real-world case study for TDL. We also test SSNs on standard node classification and edge regression tasks, showing competitive performance. We will make the code and data publicly available.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriThinker: Learning to Verify Makes Reasoning Model Efficient</title>
<link>https://arxiv.org/abs/2505.17941</link>
<guid>https://arxiv.org/abs/2505.17941</guid>
<content:encoded><![CDATA[

arXiv:2505.17941v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Principled Bayesian Framework for Training Binary and Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2505.17962</link>
<guid>https://arxiv.org/abs/2505.17962</guid>
<content:encoded><![CDATA[

arXiv:2505.17962v1 Announce Type: new 
Abstract: We propose a Bayesian framework for training binary and spiking neural networks that achieves state-of-the-art performance without normalisation layers. Unlike commonly used surrogate gradient methods -- often heuristic and sensitive to hyperparameter choices -- our approach is grounded in a probabilistic model of noisy binary networks, enabling fully end-to-end gradient-based optimisation. We introduce importance-weighted straight-through (IW-ST) estimators, a unified class generalising straight-through and relaxation-based estimators. We characterise the bias-variance trade-off in this family and derive a bias-minimising objective implemented via an auxiliary loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs), a variational inference framework that uses posterior noise to train Binary and Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient bias, regularises parameters, and introduces dropout-like noise. By linking low-bias conditions, vanishing gradients, and the KL term, we enable training of deep residual networks without normalisation. Experiments on CIFAR-10, DVS Gesture, and SHD show our method matches or exceeds existing approaches without normalisation or hand-tuned gradients.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models</title>
<link>https://arxiv.org/abs/2505.17967</link>
<guid>https://arxiv.org/abs/2505.17967</guid>
<content:encoded><![CDATA[

arXiv:2505.17967v1 Announce Type: new 
Abstract: Low-rank optimization has emerged as a promising direction in training large language models (LLMs) to reduce the memory usage of adaptive optimizers by constraining learning to a lower-dimensional space. Prior work typically projects gradients of linear layers using approaches based on Singular Value Decomposition (SVD). However, applying SVD-based procedures individually to each layer in large models is computationally expensive and incurs additional memory costs due to storing the projection matrices. In this work, we propose a computationally efficient and conceptually simple two-step procedure to approximate SVD-based gradient projections into lower-dimensional spaces. First, we construct a complete orthogonal basis using predefined orthogonal matrices of the Discrete Cosine Transform (DCT). Second, we adaptively select basis columns based on their alignment with the gradient of each layer. Each projection matrix in our method is obtained via a single matrix multiplication followed by a lightweight sorting step to identify the most relevant basis vectors. Due to the predefined nature of the orthogonal bases, they are computed once at the start of training. During training, we store only the indices of the selected columns, avoiding the need to store full projection matrices for each layer. Our numerical experiments on both pre-training and fine-tuning tasks demonstrate the effectiveness of our dual strategy in approximating optimal low-rank projections, matching the performance of costly SVD-based methods while achieving faster runtime and reduced memory usage.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems</title>
<link>https://arxiv.org/abs/2505.17968</link>
<guid>https://arxiv.org/abs/2505.17968</guid>
<content:encoded><![CDATA[

arXiv:2505.17968v1 Announce Type: new 
Abstract: Using AI to create autonomous researchers has the potential to accelerate scientific discovery. A prerequisite for this vision is understanding how well an AI model can identify the underlying structure of a black-box system from its behavior. In this paper, we explore how well a large language model (LLM) learns to identify a black-box function from passively observed versus actively collected data. We investigate the reverse-engineering capabilities of LLMs across three distinct types of black-box systems, each chosen to represent different problem domains where future autonomous AI researchers may have considerable impact: Program, Formal Language, and Math Equation. Through extensive experiments, we show that LLMs fail to extract information from observations, reaching a performance plateau that falls short of the ideal of Bayesian inference. However, we demonstrate that prompting LLMs to not only observe but also intervene -- actively querying the black-box with specific inputs to observe the resulting output -- improves performance by allowing LLMs to test edge cases and refine their beliefs. By providing the intervention data from one LLM to another, we show that this improvement is partly a result of engaging in the process of generating effective interventions, paralleling results in the literature on human learning. Further analysis reveals that engaging in intervention can help LLMs escape from two common failure modes: overcomplication, where the LLM falsely assumes prior knowledge about the black-box, and overlooking, where the LLM fails to incorporate observations. These insights provide practical guidance for helping LLMs more effectively reverse-engineer black-box systems, supporting their use in making new discoveries.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models</title>
<link>https://arxiv.org/abs/2505.17974</link>
<guid>https://arxiv.org/abs/2505.17974</guid>
<content:encoded><![CDATA[

arXiv:2505.17974v1 Announce Type: new 
Abstract: The Fisher information is a fundamental concept for characterizing the sensitivity of parameters in neural networks. However, leveraging the full observed Fisher information is too expensive for large models, so most methods rely on simple diagonal approximations. While efficient, this approach ignores parameter correlations, often resulting in reduced performance on downstream tasks. In this work, we mitigate these limitations and propose Generalized Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that accounts for both diagonal and off-diagonal elements of the Fisher information matrix, providing a more accurate reflection of parameter importance. To make the method tractable, we introduce a scalable adaptation of the Kronecker-factored approximation algorithm for the observed Fisher information. We demonstrate the effectiveness of our method on LLM compression, showing improvements over existing compression baselines. For example, at a 20 compression rate on the MMLU benchmark, our method outperforms FWSVD, which is based on a diagonal approximation of the Fisher information, by 5 percent, SVD-LLM by 3 percent, and ASVD by 6 percent compression rate.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling</title>
<link>https://arxiv.org/abs/2505.17987</link>
<guid>https://arxiv.org/abs/2505.17987</guid>
<content:encoded><![CDATA[

arXiv:2505.17987v1 Announce Type: new 
Abstract: Real world collection of Activities of Daily Living data is challenging due to privacy concerns, costly deployment and labeling, and the inherent sparsity and imbalance of human behavior. We present ADLGen, a generative framework specifically designed to synthesize realistic, event triggered, and symbolic sensor sequences for ambient assistive environments. ADLGen integrates a decoder only Transformer with sign based symbolic temporal encoding, and a context and layout aware sampling mechanism to guide generation toward semantically rich and physically plausible sensor event sequences. To enhance semantic fidelity and correct structural inconsistencies, we further incorporate a large language model into an automatic generate evaluate refine loop, which verifies logical, behavioral, and temporal coherence and generates correction rules without manual intervention or environment specific tuning. Through comprehensive experiments with novel evaluation metrics, ADLGen is shown to outperform baseline generators in statistical fidelity, semantic richness, and downstream activity recognition, offering a scalable and privacy-preserving solution for ADL data synthesis.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17988</link>
<guid>https://arxiv.org/abs/2505.17988</guid>
<content:encoded><![CDATA[

arXiv:2505.17988v1 Announce Type: new 
Abstract: R1-style Reinforcement Learning (RL) significantly enhances Large Language Models' reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&amp;K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: https://github.com/on1262/deep-reasoning
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outcome-based Reinforcement Learning to Predict the Future</title>
<link>https://arxiv.org/abs/2505.17989</link>
<guid>https://arxiv.org/abs/2505.17989</guid>
<content:encoded><![CDATA[

arXiv:2505.17989v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has boosted math and coding in large language models, yet there has been little effort to extend RLVR into messier, real-world domains like forecasting. One sticking point is that outcome-based reinforcement learning for forecasting must learn from binary, delayed, and noisy rewards, a regime where standard fine-tuning is brittle. We show that outcome-only online RL on a 14B model can match frontier-scale accuracy and surpass it in calibration and hypothetical prediction market betting by adapting two leading algorithms, Group-Relative Policy Optimisation (GRPO) and ReMax, to the forecasting setting. Our adaptations remove per-question variance scaling in GRPO, apply baseline-subtracted advantages in ReMax, hydrate training with 100k temporally consistent synthetic questions, and introduce lightweight guard-rails that penalise gibberish, non-English responses and missing rationales, enabling a single stable pass over 110k events. Scaling ReMax to 110k questions and ensembling seven predictions yields a 14B model that matches frontier baseline o1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in calibration (ECE = 0.042, p < 0.001). A simple trading rule turns this calibration edge into \$127 of hypothetical profit versus \$92 for o1 (p = 0.037). This demonstrates that refined RLVR methods can convert small-scale LLMs into potentially economically valuable forecasting tools, with implications for scaling this to larger models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective</title>
<link>https://arxiv.org/abs/2505.17997</link>
<guid>https://arxiv.org/abs/2505.17997</guid>
<content:encoded><![CDATA[

arXiv:2505.17997v1 Announce Type: new 
Abstract: The VAPO framework has demonstrated significant empirical success in enhancing the efficiency and reliability of reinforcement learning for long chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By systematically addressing challenges such as value model bias, heterogeneous sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art performance. While its practical benefits are evident, a deeper theoretical understanding of its underlying mechanisms and potential limitations is crucial for guiding future advancements. This paper aims to initiate such a discussion by exploring VAPO from a theoretical perspective, highlighting areas where its assumptions might be challenged and where further investigation could yield more robust and generalizable reasoning agents. We delve into the intricacies of value function approximation in complex reasoning spaces, the optimality of adaptive advantage estimation, the impact of token-level optimization, and the enduring challenges of exploration and generalization.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Contrastive Learning in Graph Anomaly Detection: A Clean-View Perspective</title>
<link>https://arxiv.org/abs/2505.18002</link>
<guid>https://arxiv.org/abs/2505.18002</guid>
<content:encoded><![CDATA[

arXiv:2505.18002v1 Announce Type: new 
Abstract: Graph anomaly detection aims to identify unusual patterns in graph-based data, with wide applications in fields such as web security and financial fraud detection. Existing methods typically rely on contrastive learning, assuming that a lower similarity between a node and its local subgraph indicates abnormality. However, these approaches overlook a crucial limitation: the presence of interfering edges invalidates this assumption, since it introduces disruptive noise that compromises the contrastive learning process. Consequently, this limitation impairs the ability to effectively learn meaningful representations of normal patterns, leading to suboptimal detection performance. To address this issue, we propose a Clean-View Enhanced Graph Anomaly Detection framework (CVGAD), which includes a multi-scale anomaly awareness module to identify key sources of interference in the contrastive learning process. Moreover, to mitigate bias from the one-step edge removal process, we introduce a novel progressive purification module. This module incrementally refines the graph by iteratively identifying and removing interfering edges, thereby enhancing model performance. Extensive experiments on five benchmark datasets validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Example Safety Case for Safeguards Against Misuse</title>
<link>https://arxiv.org/abs/2505.18003</link>
<guid>https://arxiv.org/abs/2505.18003</guid>
<content:encoded><![CDATA[

arXiv:2505.18003v1 Announce Type: new 
Abstract: Existing evaluations of AI misuse safeguards provide a patchwork of evidence that is often difficult to connect to real-world decisions. To bridge this gap, we describe an end-to-end argument (a "safety case") that misuse safeguards reduce the risk posed by an AI assistant to low levels. We first describe how a hypothetical developer red teams safeguards, estimating the effort required to evade them. Then, the developer plugs this estimate into a quantitative "uplift model" to determine how much barriers introduced by safeguards dissuade misuse (https://www.aimisusemodel.com/). This procedure provides a continuous signal of risk during deployment that helps the developer rapidly respond to emerging threats. Finally, we describe how to tie these components together into a simple safety case. Our work provides one concrete path -- though not the only path -- to rigorously justifying AI misuse risks are low.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distances for Markov chains from sample streams</title>
<link>https://arxiv.org/abs/2505.18005</link>
<guid>https://arxiv.org/abs/2505.18005</guid>
<content:encoded><![CDATA[

arXiv:2505.18005v1 Announce Type: new 
Abstract: Bisimulation metrics are powerful tools for measuring similarities between stochastic processes, and specifically Markov chains. Recent advances have uncovered that bisimulation metrics are, in fact, optimal-transport distances, which has enabled the development of fast algorithms for computing such metrics with provable accuracy and runtime guarantees. However, these recent methods, as well as all previously known methods, assume full knowledge of the transition dynamics. This is often an impractical assumption in most real-world scenarios, where typically only sample trajectories are available. In this work, we propose a stochastic optimization method that addresses this limitation and estimates bisimulation metrics based on sample access, without requiring explicit transition models. Our approach is derived from a new linear programming (LP) formulation of bisimulation metrics, which we solve using a stochastic primal-dual optimization method. We provide theoretical guarantees on the sample complexity of the algorithm and validate its effectiveness through a series of empirical evaluations.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling</title>
<link>https://arxiv.org/abs/2505.18017</link>
<guid>https://arxiv.org/abs/2505.18017</guid>
<content:encoded><![CDATA[

arXiv:2505.18017v1 Announce Type: new 
Abstract: Deep generative models hold great promise for representing complex physical systems, but their deployment is currently limited by the lack of guarantees on the physical plausibility of the generated outputs. Ensuring that known physical constraints are enforced is therefore critical when applying generative models to scientific and engineering problems. We address this limitation by developing a principled framework for sampling from a target distribution while rigorously satisfying physical constraints. Leveraging the variational formulation of Langevin dynamics, we propose Split Augmented Langevin (SAL), a novel primal-dual sampling algorithm that enforces constraints progressively through variable splitting, with convergence guarantees. While the method is developed theoretically for Langevin dynamics, we demonstrate its effective applicability to diffusion models. In particular, we use constrained diffusion models to generate physical fields satisfying energy and mass conservation laws. We apply our method to diffusion-based data assimilation on a complex physical system, where enforcing physical constraints substantially improves both forecast accuracy and the preservation of critical conserved quantities. We also demonstrate the potential of SAL for challenging feasibility problems in optimal control.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time to Spike? Understanding the Representational Power of Spiking Neural Networks in Discrete Time</title>
<link>https://arxiv.org/abs/2505.18023</link>
<guid>https://arxiv.org/abs/2505.18023</guid>
<content:encoded><![CDATA[

arXiv:2505.18023v1 Announce Type: new 
Abstract: Recent years have seen significant progress in developing spiking neural networks (SNNs) as a potential solution to the energy challenges posed by conventional artificial neural networks (ANNs). However, our theoretical understanding of SNNs remains relatively limited compared to the ever-growing body of literature on ANNs. In this paper, we study a discrete-time model of SNNs based on leaky integrate-and-fire (LIF) neurons, referred to as discrete-time LIF-SNNs, a widely used framework that still lacks solid theoretical foundations. We demonstrate that discrete-time LIF-SNNs with static inputs and outputs realize piecewise constant functions defined on polyhedral regions, and more importantly, we quantify the network size required to approximate continuous functions. Moreover, we investigate the impact of latency (number of time steps) and depth (number of layers) on the complexity of the input space partitioning induced by discrete-time LIF-SNNs. Our analysis highlights the importance of latency and contrasts these networks with ANNs employing piecewise linear activation functions. Finally, we present numerical experiments to support our theoretical findings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knot So Simple: A Minimalistic Environment for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2505.18028</link>
<guid>https://arxiv.org/abs/2505.18028</guid>
<content:encoded><![CDATA[

arXiv:2505.18028v1 Announce Type: new 
Abstract: We propose KnotGym, an interactive environment for complex, spatial reasoning and manipulation. KnotGym includes goal-oriented rope manipulation tasks with varying levels of complexity, all requiring acting from pure image observations. Tasks are defined along a clear and quantifiable axis of complexity based on the number of knot crossings, creating a natural generalization test. KnotGym has a simple observation space, allowing for scalable development, yet it highlights core challenges in integrating acute perception, spatial reasoning, and grounded manipulation. We evaluate methods of different classes, including model-based RL, model-predictive control, and chain-of-thought reasoning, and illustrate the challenges KnotGym presents. KnotGym is available at https://github.com/lil-lab/knotgym.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mahalanobis++: Improving OOD Detection via Feature Normalization</title>
<link>https://arxiv.org/abs/2505.18032</link>
<guid>https://arxiv.org/abs/2505.18032</guid>
<content:encoded><![CDATA[

arXiv:2505.18032v1 Announce Type: new 
Abstract: Detecting out-of-distribution (OOD) examples is an important task for deploying reliable machine learning models in safety-critial applications. While post-hoc methods based on the Mahalanobis distance applied to pre-logit features are among the most effective for ImageNet-scale OOD detection, their performance varies significantly across models. We connect this inconsistency to strong variations in feature norms, indicating severe violations of the Gaussian assumption underlying the Mahalanobis distance estimation. We show that simple $\ell_2$-normalization of the features mitigates this problem effectively, aligning better with the premise of normally distributed data with shared covariance matrix. Extensive experiments on 44 models across diverse architectures and pretraining schemes show that $\ell_2$-normalization improves the conventional Mahalanobis distance-based approaches significantly and consistently, and outperforms other recently proposed OOD detection methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach</title>
<link>https://arxiv.org/abs/2505.18043</link>
<guid>https://arxiv.org/abs/2505.18043</guid>
<content:encoded><![CDATA[

arXiv:2505.18043v1 Announce Type: new 
Abstract: Clustering is a fundamental task in both machine learning and data mining. Among various methods, edge-colored clustering (ECC) has emerged as a useful approach for handling categorical data. Given a hypergraph with (hyper)edges labeled by colors, ECC aims to assign vertex colors to minimize the number of edges where the vertex color differs from the edge's color. However, traditional ECC has inherent limitations, as it enforces a nonoverlapping and exhaustive clustering. To tackle these limitations, three versions of ECC have been studied: Local ECC and Global ECC, which allow overlapping clusters, and Robust ECC, which accounts for vertex outliers. For these problems, both linear programming (LP) rounding algorithms and greedy combinatorial algorithms have been proposed. While these LP-rounding algorithms provide high-quality solutions, they demand substantial computation time; the greedy algorithms, on the other hand, run very fast but often compromise solution quality. In this paper, we present an algorithmic framework that combines the strengths of LP with the computational efficiency of combinatorial algorithms. Both experimental and theoretical analyses show that our algorithms efficiently produce high-quality solutions for all three problems: Local, Global, and Robust ECC. We complement our algorithmic contributions with complexity-theoretic inapproximability results and integrality gap bounds, which suggest that significant theoretical improvements are unlikely. Our results also answer two open questions previously raised in the literature.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Mixture Distributionally Robust Markov Decision Processes</title>
<link>https://arxiv.org/abs/2505.18044</link>
<guid>https://arxiv.org/abs/2505.18044</guid>
<content:encoded><![CDATA[

arXiv:2505.18044v1 Announce Type: new 
Abstract: Many real-world decision-making problems face the off-dynamics challenge: the agent learns a policy in a source domain and deploys it in a target domain with different state transitions. The distributionally robust Markov decision process (DRMDP) addresses this challenge by finding a robust policy that performs well under the worst-case environment within a pre-specified uncertainty set of transition dynamics. Its effectiveness heavily hinges on the proper design of these uncertainty sets, based on prior knowledge of the dynamics. In this work, we propose a novel linear mixture DRMDP framework, where the nominal dynamics is assumed to be a linear mixture model. In contrast with existing uncertainty sets directly defined as a ball centered around the nominal kernel, linear mixture DRMDPs define the uncertainty sets based on a ball around the mixture weighting parameter. We show that this new framework provides a more refined representation of uncertainties compared to conventional models based on $(s,a)$-rectangularity and $d$-rectangularity, when prior knowledge about the mixture model is present. We propose a meta algorithm for robust policy learning in linear mixture DRMDPs with general $f$-divergence defined uncertainty sets, and analyze its sample complexities under three divergence metrics instantiations: total variation, Kullback-Leibler, and $\chi^2$ divergences. These results establish the statistical learnability of linear mixture DRMDPs, laying the theoretical foundation for future research on this new setting.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions</title>
<link>https://arxiv.org/abs/2505.18046</link>
<guid>https://arxiv.org/abs/2505.18046</guid>
<content:encoded><![CDATA[

arXiv:2505.18046v1 Announce Type: new 
Abstract: The Restricted Boltzmann Machine (RBM) is one of the simplest generative neural networks capable of learning input distributions. Despite its simplicity, the analysis of its performance in learning from the training data is only well understood in cases that essentially reduce to singular value decomposition of the data. Here, we consider the limit of a large dimension of the input space and a constant number of hidden units. In this limit, we simplify the standard RBM training objective into a form that is equivalent to the multi-index model with non-separable regularization. This opens a path to analyze training of the RBM using methods that are established for multi-index models, such as Approximate Message Passing (AMP) and its state evolution, and the analysis of Gradient Descent (GD) via the dynamical mean-field theory. We then give rigorous asymptotics of the training dynamics of RBM on data generated by the spiked covariance model as a prototype of a structure suitable for unsupervised learning. We show in particular that RBM reaches the optimal computational weak recovery threshold, aligning with the BBP transition, in the spiked covariance model.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymptotically optimal regret in communicating Markov decision processes</title>
<link>https://arxiv.org/abs/2505.18064</link>
<guid>https://arxiv.org/abs/2505.18064</guid>
<content:encoded><![CDATA[

arXiv:2505.18064v1 Announce Type: new 
Abstract: In this paper, we present a learning algorithm that achieves asymptotically optimal regret for Markov decision processes in average reward under a communicating assumption. That is, given a communicating Markov decision process $M$, our algorithm has regret $K(M) \log(T) + \mathrm{o}(\log(T))$ where $T$ is the number of learning steps and $K(M)$ is the best possible constant. This algorithm works by explicitly tracking the constant $K(M)$ to learn optimally, then balances the trade-off between exploration (playing sub-optimally to gain information), co-exploration (playing optimally to gain information) and exploitation (playing optimally to score maximally). We further show that the function $K(M)$ is discontinuous, which is a consequence challenge for our approach. To that end, we describe a regularization mechanism to estimate $K(M)$ with arbitrary precision from empirical data.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Model Generalization for Compute-Aware Test-Time Reasoning</title>
<link>https://arxiv.org/abs/2505.18065</link>
<guid>https://arxiv.org/abs/2505.18065</guid>
<content:encoded><![CDATA[

arXiv:2505.18065v1 Announce Type: new 
Abstract: External test-time reasoning enhances large language models (LLMs) by decoupling generation and selection. At inference time, the model generates multiple reasoning paths, and an auxiliary process reward model (PRM) is used to score and select the best one. A central challenge in this setting is test-time compute optimality (TCO), i.e., how to maximize answer accuracy under a fixed inference budget. In this work, we establish a theoretical framework to analyze how the generalization error of the PRM affects compute efficiency and reasoning performance. Leveraging PAC-Bayes theory, we derive generalization bounds and show that a lower generalization error of PRM leads to fewer samples required to find correct answers. Motivated by this analysis, we propose Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically controls search behavior. The actor outputs sampling hyperparameters based on reward distributions and sparsity statistics, while the critic estimates their utility to guide budget allocation. Experiments on the MATH and AIME benchmarks with various LLMs and PRMs demonstrate that CATS consistently outperforms other external TTS methods, validating our theoretical predictions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Hebbian Dynamics in Regularized Non-Local Learners</title>
<link>https://arxiv.org/abs/2505.18069</link>
<guid>https://arxiv.org/abs/2505.18069</guid>
<content:encoded><![CDATA[

arXiv:2505.18069v1 Announce Type: new 
Abstract: Stochastic Gradient Descent (SGD) has emerged as a remarkably effective learning algorithm, underpinning nearly all state-of-the-art machine learning models, from large language models to autonomous vehicles. Despite its practical success, SGD appears fundamentally distinct from biological learning mechanisms. It is widely believed that the biological brain can not implement gradient descent because it is nonlocal, and we have found little (if any) experimental evidence for it. In contrast, the brain is widely thought to learn via local Hebbian learning principles, which have been seen as incompatible with gradient descent. In this paper, we establish a theoretical and empirical connection between the learning signals of neural networks trained using SGD with weight decay and those trained with Hebbian learning near convergence. We show that SGD with regularization can appear to learn according to a Hebbian rule, and SGD with injected noise according to an anti-Hebbian rule. We also provide empirical evidence that Hebbian learning properties can emerge in a network with weight decay from virtually any learning rule--even random ones. These results may bridge a long-standing gap between artificial and biological learning, revealing Hebbian properties as an epiphenomenon of deeper optimization principles and cautioning against interpreting their presence in neural data as evidence against more complex hetero-synaptic mechanisms.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction</title>
<link>https://arxiv.org/abs/2505.18080</link>
<guid>https://arxiv.org/abs/2505.18080</guid>
<content:encoded><![CDATA[

arXiv:2505.18080v1 Announce Type: new 
Abstract: This paper presents AFD-STA Net, a neural framework integrating adaptive filtering and spatiotemporal dynamics learning for predicting high-dimensional chaotic systems governed by partial differential equations. The architecture combines: 1) An adaptive exponential smoothing module with position-aware decay coefficients for robust attractor reconstruction, 2) Parallel attention mechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated fusion of multiscale features, and 4) Deep projection networks with dimension-scaling capabilities. Numerical experiments on nonlinear PDE systems demonstrate the model's effectiveness in maintaining prediction accuracy under both smooth and strongly chaotic regimes while exhibiting noise tolerance through adaptive filtering. Component ablation studies confirm critical contributions from each module, particularly highlighting the essential role of spatiotemporal attention in learning complex dynamical interactions. The framework shows promising potential for real-world applications requiring simultaneous handling of measurement uncertainties and high-dimensional nonlinear dynamics.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backpropagation-Free Metropolis-Adjusted Langevin Algorithm</title>
<link>https://arxiv.org/abs/2505.18081</link>
<guid>https://arxiv.org/abs/2505.18081</guid>
<content:encoded><![CDATA[

arXiv:2505.18081v1 Announce Type: new 
Abstract: Recent work on backpropagation-free learning has shown that it is possible to use forward-mode automatic differentiation (AD) to perform optimization on differentiable models. Forward-mode AD requires sampling a tangent vector for each forward pass of a model. The result is the model evaluation with the directional derivative along the tangent. In this paper, we illustrate how the sampling of this tangent vector can be incorporated into the proposal mechanism for the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the first to introduce a backpropagation-free gradient-based Markov chain Monte Carlo (MCMC) algorithm. We also extend to a novel backpropagation-free position-specific preconditioned forward-mode MALA that leverages Hessian information. Overall, we propose four new algorithms: Forward MALA; Line Forward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward MALA. We highlight the reduced computational cost of the forward-mode samplers and show that forward-mode is competitive with the original MALA, while even outperforming it depending on the probabilistic model. We include Bayesian inference results on a range of probabilistic models, including hierarchical distributions and Bayesian neural networks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Iterative Framework for Generative Backmapping of Coarse Grained Proteins</title>
<link>https://arxiv.org/abs/2505.18082</link>
<guid>https://arxiv.org/abs/2505.18082</guid>
<content:encoded><![CDATA[

arXiv:2505.18082v1 Announce Type: new 
Abstract: The techniques of data-driven backmapping from coarse-grained (CG) to fine-grained (FG) representation often struggle with accuracy, unstable training, and physical realism, especially when applied to complex systems such as proteins. In this work, we introduce a novel iterative framework by using conditional Variational Autoencoders and graph-based neural networks, specifically designed to tackle the challenges associated with such large-scale biomolecules. Our method enables stepwise refinement from CG beads to full atomistic details. We outline the theory of iterative generative backmapping and demonstrate via numerical experiments the advantages of multistep schemes by applying them to proteins of vastly different structures with very coarse representations. This multistep approach not only improves the accuracy of reconstructions but also makes the training process more computationally efficient for proteins with ultra-CG representations.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?</title>
<link>https://arxiv.org/abs/2505.18083</link>
<guid>https://arxiv.org/abs/2505.18083</guid>
<content:encoded><![CDATA[

arXiv:2505.18083v1 Announce Type: new 
Abstract: In planning, stitching is an ability of algorithms to piece together sub-trajectories of data they are trained on to generate new and diverse behaviours. While stitching is historically a strength of offline reinforcement learning, recent generative behavioural cloning (BC) methods have also shown proficiency at stitching. However, the main factors behind this are poorly understood, hindering the development of new algorithms that can reliably stitch. Focusing on diffusion planners trained via BC, we find two properties are needed to compose: \emph{positional equivariance} and \emph{local receptiveness}. We use these two properties to explain architecture, data, and inference choices in existing generative BC methods based on diffusion planning, including replanning frequency, data augmentation, and data scaling. Experimental comparisions show that (1) while locality is more important than positional equivariance in creating a diffusion planner capable of composition, both are crucial (2) enabling these properties through relatively simple architecture choices can be competitive with more computationally expensive methods such as replanning or scaling data, and (3) simple inpainting-based guidance can guide architecturally compositional models to enable generalization in goal-conditioned settings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early-Exit Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.18088</link>
<guid>https://arxiv.org/abs/2505.18088</guid>
<content:encoded><![CDATA[

arXiv:2505.18088v1 Announce Type: new 
Abstract: Early-exit mechanisms allow deep neural networks to halt inference as soon as classification confidence is high enough, adaptively trading depth for confidence, and thereby cutting latency and energy on easy inputs while retaining full-depth accuracy for harder ones. Similarly, adding early exit mechanisms to Graph Neural Networks (GNNs), the go-to models for graph-structured data, allows for dynamic trading depth for confidence on simple graphs while maintaining full-depth accuracy on harder and more complex graphs to capture intricate relationships. Although early exits have proven effective across various deep learning domains, their potential within GNNs in scenarios that require deep architectures while resisting over-smoothing and over-squashing remains largely unexplored. We unlock that potential by first introducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose symmetry-based inductive biases mitigate these issues and yield stable intermediate representations that can be useful to allow early exiting in GNNs. Building on this backbone, we present Early-Exit Graph Neural Networks (EEGNNs), which append confidence-aware exit heads that allow on-the-fly termination of propagation based on each node or the entire graph. Experiments show that EEGNNs preserve robust performance as depth grows and deliver competitive accuracy on heterophilic and long-range benchmarks, matching attention-based and asynchronous message-passing models while substantially reducing computation and latency. We plan to release the code to reproduce our experiments.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Mixing Can Induce Phase Transitions in Knowledge Acquisition</title>
<link>https://arxiv.org/abs/2505.18091</link>
<guid>https://arxiv.org/abs/2505.18091</guid>
<content:encoded><![CDATA[

arXiv:2505.18091v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge. In this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets, unlike training exclusively on knowledge-dense data (arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards more transferable adversarial attack in black-box manner</title>
<link>https://arxiv.org/abs/2505.18097</link>
<guid>https://arxiv.org/abs/2505.18097</guid>
<content:encoded><![CDATA[

arXiv:2505.18097v1 Announce Type: new 
Abstract: Adversarial attacks have become a well-explored domain, frequently serving as evaluation baselines for model robustness. Among these, black-box attacks based on transferability have received significant attention due to their practical applicability in real-world scenarios. Traditional black-box methods have generally focused on improving the optimization framework (e.g., utilizing momentum in MI-FGSM) to enhance transferability, rather than examining the dependency on surrogate white-box model architectures. Recent state-of-the-art approach DiffPGD has demonstrated enhanced transferability by employing diffusion-based adversarial purification models for adaptive attacks. The inductive bias of diffusion-based adversarial purification aligns naturally with the adversarial attack process, where both involving noise addition, reducing dependency on surrogate white-box model selection. However, the denoising process of diffusion models incurs substantial computational costs through chain rule derivation, manifested in excessive VRAM consumption and extended runtime. This progression prompts us to question whether introducing diffusion models is necessary. We hypothesize that a model sharing similar inductive bias to diffusion-based adversarial purification, combined with an appropriate loss function, could achieve comparable or superior transferability while dramatically reducing computational overhead. In this paper, we propose a novel loss function coupled with a unique surrogate model to validate our hypothesis. Our approach leverages the score of the time-dependent classifier from classifier-guided diffusion models, effectively incorporating natural data distribution knowledge into the adversarial optimization process. Experimental results demonstrate significantly improved transferability across diverse model architectures while maintaining robustness against diffusion-based defenses.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning</title>
<link>https://arxiv.org/abs/2505.18101</link>
<guid>https://arxiv.org/abs/2505.18101</guid>
<content:encoded><![CDATA[

arXiv:2505.18101v1 Announce Type: new 
Abstract: Online Continual Learning (OCL) presents a complex learning environment in which new data arrives in a batch-to-batch online format, and the risk of catastrophic forgetting can significantly impair model efficacy. In this study, we address OCL by introducing an innovative memory framework that incorporates a short-term memory system to retain dynamic information and a long-term memory system to archive enduring knowledge. Specifically, the long-term memory system comprises a collection of sub-memory buffers, each linked to a cluster prototype and designed to retain data samples from distinct categories. We propose a novel $K$-means-based sample selection method to identify cluster prototypes for each encountered category. To safeguard essential and critical samples, we introduce a novel memory optimisation strategy that selectively retains samples in the appropriate sub-memory buffer by evaluating each cluster prototype against incoming samples through an optimal transportation mechanism. This approach specifically promotes each sub-memory buffer to retain data samples that exhibit significant discrepancies from the corresponding cluster prototype, thereby ensuring the preservation of semantically rich information. In addition, we propose a novel Divide-and-Conquer (DAC) approach that formulates the memory updating as an optimisation problem and divides it into several subproblems. As a result, the proposed DAC approach can solve these subproblems separately and thus can significantly reduce computations of the proposed memory updating process. We conduct a series of experiments across standard and imbalanced learning settings, and the empirical findings indicate that the proposed memory framework achieves state-of-the-art performance in both learning contexts.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</title>
<link>https://arxiv.org/abs/2505.18102</link>
<guid>https://arxiv.org/abs/2505.18102</guid>
<content:encoded><![CDATA[

arXiv:2505.18102v1 Announce Type: new 
Abstract: Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization</title>
<link>https://arxiv.org/abs/2505.18113</link>
<guid>https://arxiv.org/abs/2505.18113</guid>
<content:encoded><![CDATA[

arXiv:2505.18113v1 Announce Type: new 
Abstract: Training quantized neural networks requires addressing the non-differentiable and discrete nature of the underlying optimization problem. To tackle this challenge, the straight-through estimator (STE) has become the most widely adopted heuristic, allowing backpropagation through discrete operations by introducing surrogate gradients. However, its theoretical properties remain largely unexplored, with few existing works simplifying the analysis by assuming an infinite amount of training data. In contrast, this work presents the first finite-sample analysis of STE in the context of neural network quantization. Our theoretical results highlight the critical role of sample size in the success of STE, a key insight absent from existing studies. Specifically, by analyzing the quantization-aware training of a two-layer neural network with binary weights and activations, we derive the sample complexity bound in terms of the data dimensionality that guarantees the convergence of STE-based optimization to the global minimum. Moreover, in the presence of label noises, we uncover an intriguing recurrence property of STE-gradient method, where the iterate repeatedly escape from and return to the optimal binary weights. Our analysis leverages tools from compressed sensing and dynamical systems theory.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Supervised Learning and Reinforcement Learning in Math Reasoning</title>
<link>https://arxiv.org/abs/2505.18116</link>
<guid>https://arxiv.org/abs/2505.18116</guid>
<content:encoded><![CDATA[

arXiv:2505.18116v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has played a central role in the recent surge of LLMs' math abilities by enabling self-improvement through binary verifier signals. In contrast, Supervised Learning (SL) is rarely considered for such verification-driven training, largely due to its heavy reliance on reference answers and inability to reflect on mistakes. In this work, we challenge the prevailing notion that self-improvement is exclusive to RL and propose Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to reflect on their failures and improve autonomously with no external teachers. In online training, instead of throwing away self-generated negative answers, NFT constructs an implicit negative policy to model them. This implicit policy is parameterized with the same positive LLM we target to optimize on positive data, enabling direct policy optimization on all LLMs' generations. We conduct experiments on 7B and 32B models in math reasoning tasks. Results consistently show that through the additional leverage of negative feedback, NFT significantly improves over SL baselines like Rejection sampling Fine-Tuning, matching or even surpassing leading RL algorithms like GRPO and DAPO. Furthermore, we demonstrate that NFT and GRPO are actually equivalent in strict-on-policy training, even though they originate from entirely different theoretical foundations. Our experiments and theoretical findings bridge the gap between SL and RL methods in binary-feedback learning systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations</title>
<link>https://arxiv.org/abs/2505.18125</link>
<guid>https://arxiv.org/abs/2505.18125</guid>
<content:encoded><![CDATA[

arXiv:2505.18125v1 Announce Type: new 
Abstract: While deep learning has achieved remarkable success across many domains, it has historically underperformed on tabular learning tasks, which remain dominated by gradient boosting decision trees (GBDTs). However, recent advancements are paving the way for Tabular Foundation Models, which can leverage real-world knowledge and generalize across diverse datasets, particularly when the data contains free-text. Although incorporating language model capabilities into tabular tasks has been explored, most existing methods utilize static, target-agnostic textual representations, limiting their effectiveness. We introduce TabSTAR: a Foundation Tabular Model with Semantically Target-Aware Representations. TabSTAR is designed to enable transfer learning on tabular data with textual features, with an architecture free of dataset-specific parameters. It unfreezes a pretrained text encoder and takes as input target tokens, which provide the model with the context needed to learn task-specific embeddings. TabSTAR achieves state-of-the-art performance for both medium- and large-sized datasets across known benchmarks of classification tasks with text features, and its pretraining phase exhibits scaling laws in the number of datasets, offering a pathway for further performance improvements.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Model Overoptimisation in Iterated RLHF</title>
<link>https://arxiv.org/abs/2505.18126</link>
<guid>https://arxiv.org/abs/2505.18126</guid>
<content:encoded><![CDATA[

arXiv:2505.18126v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) is a widely used method for aligning large language models with human preferences. However, RLHF often suffers from reward model overoptimisation, in which models overfit to the reward function, resulting in non-generalisable policies that exploit the idiosyncrasies and peculiarities of the reward function. A common mitigation is iterated RLHF, in which reward models are repeatedly retrained with updated human feedback and policies are re-optimised. Despite its increasing adoption, the dynamics of overoptimisation in this setting remain poorly understood. In this work, we present the first comprehensive study of overoptimisation in iterated RLHF. We systematically analyse key design choices - how reward model training data is transferred across iterations, which reward function is used for optimisation, and how policies are initialised. Using the controlled AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over successive iterations, as reward models increasingly approximate ground-truth preferences. However, performance gains diminish over time, and while reinitialising from the base policy is robust, it limits optimisation flexibility. Other initialisation strategies often fail to recover from early overoptimisation. These findings offer actionable insights for building more stable and generalisable RLHF pipelines.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement</title>
<link>https://arxiv.org/abs/2505.18131</link>
<guid>https://arxiv.org/abs/2505.18131</guid>
<content:encoded><![CDATA[

arXiv:2505.18131v1 Announce Type: new 
Abstract: Multilayer perceptrons (MLPs) are a workhorse machine learning architecture, used in a variety of modern deep learning frameworks. However, recently Kolmogorov-Arnold Networks (KANs) have become increasingly popular due to their success on a range of problems, particularly for scientific machine learning tasks. In this paper, we exploit the relationship between KANs and multichannel MLPs to gain structural insight into how to train MLPs faster. We demonstrate the KAN basis (1) provides geometric localized support, and (2) acts as a preconditioned descent in the ReLU basis, overall resulting in expedited training and improved accuracy. Our results show the equivalence between free-knot spline KAN architectures, and a class of MLPs that are refined geometrically along the channel dimension of each weight tensor. We exploit this structural equivalence to define a hierarchical refinement scheme that dramatically accelerates training of the multi-channel MLP architecture. We show further accuracy improvements can be had by allowing the $1$D locations of the spline knots to be trained simultaneously with the weights. These advances are demonstrated on a range of benchmark examples for regression and scientific machine learning.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Distribution Embeddings</title>
<link>https://arxiv.org/abs/2505.18150</link>
<guid>https://arxiv.org/abs/2505.18150</guid>
<content:encoded><![CDATA[

arXiv:2505.18150v1 Announce Type: new 
Abstract: Many real-world problems require reasoning across multiple scales, demanding models which operate not on single data points, but on entire distributions. We introduce generative distribution embeddings (GDE), a framework that lifts autoencoders to the space of distributions. In GDEs, an encoder acts on sets of samples, and the decoder is replaced by a generator which aims to match the input distribution. This framework enables learning representations of distributions by coupling conditional generative models with encoder networks which satisfy a criterion we call distributional invariance. We show that GDEs learn predictive sufficient statistics embedded in the Wasserstein space, such that latent GDE distances approximately recover the $W_2$ distance, and latent interpolation approximately recovers optimal transport trajectories for Gaussian and Gaussian mixture distributions. We systematically benchmark GDEs against existing approaches on synthetic datasets, demonstrating consistently stronger performance. We then apply GDEs to six key problems in computational biology: learning representations of cell populations from lineage-tracing data (150K cells), predicting perturbation effects on single-cell transcriptomes (1M cells), predicting perturbation effects on cellular phenotypes (20M single-cell images), modeling tissue-specific DNA methylation patterns (253M sequences), designing synthetic yeast promoters (34M sequences), and spatiotemporal modeling of viral protein sequences (1M sequences).
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Uncertain Combinatorics through Graphization, Hyperization, and Uncertainization: Fuzzy, Neutrosophic, Soft, Rough, and Beyond</title>
<link>https://arxiv.org/abs/2411.17411</link>
<guid>https://arxiv.org/abs/2411.17411</guid>
<content:encoded><![CDATA[

arXiv:2411.17411v1 Announce Type: cross 
Abstract: To better handle real-world uncertainty, concepts such as fuzzy sets, neutrosophic sets, rough sets, and soft sets have been introduced. For example, neutrosophic sets, which simultaneously represent truth, indeterminacy, and falsehood, have proven to be valuable tools for modeling uncertainty in complex systems. These set concepts are increasingly studied in graphized forms, and generalized graph concepts now encompass well-known structures such as hypergraphs and superhypergraphs. Furthermore, hyperconcepts and superhyperconcepts are being actively researched in areas beyond graph theory.
  Combinatorics, uncertain sets (including fuzzy sets, neutrosophic sets, rough sets, soft sets, and plithogenic sets), uncertain graphs, and hyper and superhyper concepts are active areas of research with significant mathematical and practical implications. Recognizing their importance, this paper explores new graph and set concepts, as well as hyper and superhyper concepts, as detailed in the "Results" section of "The Structure of the Paper." Additionally, this work aims to consolidate recent findings, providing a survey-like resource to inform and engage readers.
  For instance, we extend several graph concepts by introducing Neutrosophic Oversets, Neutrosophic Undersets, Neutrosophic Offsets, and the Nonstandard Real Set. This paper defines a variety of concepts with the goal of inspiring new ideas and serving as a valuable resource for researchers in their academic pursuits.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation-Enabled Knowledge Alignment Protocol for Semantic Communication in AI Agent Networks</title>
<link>https://arxiv.org/abs/2505.17030</link>
<guid>https://arxiv.org/abs/2505.17030</guid>
<content:encoded><![CDATA[

arXiv:2505.17030v1 Announce Type: cross 
Abstract: Future networks are envisioned to connect massive artificial intelligence (AI) agents, enabling their extensive collaboration on diverse tasks. Compared to traditional entities, these agents naturally suit the semantic communication (SC), which can significantly enhance the bandwidth efficiency. Nevertheless, SC requires the knowledge among agents to be aligned, while agents have distinct expert knowledge for their individual tasks in practice. In this paper, we propose a distillation-enabled knowledge alignment protocol (DeKAP), which distills the expert knowledge of each agent into parameter-efficient low-rank matrices, allocates them across the network, and allows agents to simultaneously maintain aligned knowledge for multiple tasks. We formulate the joint minimization of alignment loss, communication overhead, and storage cost as a large-scale integer linear programming problem and develop a highly efficient greedy algorithm. From computer simulation, the DeKAP establishes knowledge alignment with the lowest communication and computation resources compared to conventional approaches.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A brief review of the Deep BSDE method for solving high-dimensional partial differential equations</title>
<link>https://arxiv.org/abs/2505.17032</link>
<guid>https://arxiv.org/abs/2505.17032</guid>
<content:encoded><![CDATA[

arXiv:2505.17032v1 Announce Type: cross 
Abstract: High-dimensional partial differential equations (PDEs) pose significant challenges for numerical computation due to the curse of dimensionality, which limits the applicability of traditional mesh-based methods. Since 2017, the Deep BSDE method has introduced deep learning techniques that enable the effective solution of nonlinear PDEs in very high dimensions. This innovation has sparked considerable interest in using neural networks for high-dimensional PDEs, making it an active area of research. In this short review, we briefly sketch the Deep BSDE method, its subsequent developments, and future directions for the field.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM-KG: Multimodal Radiology Knowledge Graph Generation</title>
<link>https://arxiv.org/abs/2505.17042</link>
<guid>https://arxiv.org/abs/2505.17042</guid>
<content:encoded><![CDATA[

arXiv:2505.17042v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success in natural language generation, excelling at instruction following and structured output generation. Knowledge graphs play a crucial role in radiology, serving as valuable sources of factual information and enhancing various downstream tasks. However, generating radiology-specific knowledge graphs presents significant challenges due to the specialized language of radiology reports and the limited availability of domain-specific data. Existing solutions are predominantly unimodal, meaning they generate knowledge graphs only from radiology reports while excluding radiographic images. Additionally, they struggle with long-form radiology data due to limited context length. To address these limitations, we propose a novel multimodal VLM-based framework for knowledge graph generation in radiology. Our approach outperforms previous methods and introduces the first multimodal solution for radiology knowledge graph generation.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Flexible Quantum-Inspired Differential Equation Solvers with Data Integration</title>
<link>https://arxiv.org/abs/2505.17046</link>
<guid>https://arxiv.org/abs/2505.17046</guid>
<content:encoded><![CDATA[

arXiv:2505.17046v1 Announce Type: cross 
Abstract: Accurately solving high-dimensional partial differential equations (PDEs) remains a central challenge in computational mathematics. Traditional numerical methods, while effective in low-dimensional settings or on coarse grids, often struggle to deliver the precision required in practical applications. Recent machine learning-based approaches offer flexibility but frequently fall short in terms of accuracy and reliability, particularly in industrial contexts. In this work, we explore a quantum-inspired method based on quantized tensor trains (QTT), enabling efficient and accurate solutions to PDEs in a variety of challenging scenarios. Through several representative examples, we demonstrate that the QTT approach can achieve logarithmic scaling in both memory and computational cost for linear and nonlinear PDEs. Additionally, we introduce a novel technique for data-driven learning within the quantum-inspired framework, combining the adaptability of neural networks with enhanced accuracy and reduced training time.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>METHOD: Modular Efficient Transformer for Health Outcome Discovery</title>
<link>https://arxiv.org/abs/2505.17054</link>
<guid>https://arxiv.org/abs/2505.17054</guid>
<content:encoded><![CDATA[

arXiv:2505.17054v1 Announce Type: cross 
Abstract: Recent advances in transformer architectures have revolutionised natural language processing, but their application to healthcare domains presents unique challenges. Patient timelines are characterised by irregular sampling, variable temporal dependencies, and complex contextual relationships that differ substantially from traditional language tasks. This paper introduces \METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel transformer architecture specifically designed to address the challenges of clinical sequence modelling in electronic health records. \METHOD~integrates three key innovations: (1) a patient-aware attention mechanism that prevents information leakage whilst enabling efficient batch processing; (2) an adaptive sliding window attention scheme that captures multi-scale temporal dependencies; and (3) a U-Net inspired architecture with dynamic skip connections for effective long sequence processing. Evaluations on the MIMIC-IV database demonstrate that \METHOD~consistently outperforms the state-of-the-art \ETHOS~model, particularly in predicting high-severity cases that require urgent clinical intervention. \METHOD~exhibits stable performance across varying inference lengths, a crucial feature for clinical deployment where patient histories vary significantly in length. Analysis of learned embeddings reveals that \METHOD~better preserves clinical hierarchies and relationships between medical concepts. These results suggest that \METHOD~represents a significant advancement in transformer architectures optimised for healthcare applications, providing more accurate and clinically relevant predictions whilst maintaining computational efficiency.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data RL: Task Definition Is All You Need</title>
<link>https://arxiv.org/abs/2505.17063</link>
<guid>https://arxiv.org/abs/2505.17063</guid>
<content:encoded><![CDATA[

arXiv:2505.17063v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) is a powerful way to adapt foundation models to specialized tasks, but its reliance on large-scale human-labeled data limits broad adoption. We introduce Synthetic Data RL, a simple and general framework that reinforcement fine-tunes models using only synthetic data generated from a task definition. Our method first generates question and answer pairs from the task definition and retrieved documents, then adapts the difficulty of the question based on model solvability, and selects questions using the average pass rate of the model across samples for RL training. On Qwen-2.5-7B, our method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9 pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA (finance). It surpasses supervised fine-tuning under the same data budget and nearly matches RL with full human data across datasets (e.g., +17.2 pp on GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only by 0.4 pp, showing a limited added value. By reducing human data annotation, Synthetic Data RL enables scalable and efficient RL-based model adaptation. Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases</title>
<link>https://arxiv.org/abs/2505.17065</link>
<guid>https://arxiv.org/abs/2505.17065</guid>
<content:encoded><![CDATA[

arXiv:2505.17065v1 Announce Type: cross 
Abstract: Recent advances in artificial intelligence, particularly large language models LLMs, have shown promising capabilities in transforming rare disease research. This survey paper explores the integration of LLMs in the analysis of rare diseases, highlighting significant strides and pivotal studies that leverage textual data to uncover insights and patterns critical for diagnosis, treatment, and patient care. While current research predominantly employs textual data, the potential for multimodal data integration combining genetic, imaging, and electronic health records stands as a promising frontier. We review foundational papers that demonstrate the application of LLMs in identifying and extracting relevant medical information, simulating intelligent conversational agents for patient interaction, and enabling the formulation of accurate and timely diagnoses. Furthermore, this paper discusses the challenges and ethical considerations inherent in deploying LLMs, including data privacy, model transparency, and the need for robust, inclusive data sets. As part of this exploration, we present a section on experimentation that utilizes multiple LLMs alongside structured questionnaires, specifically designed for diagnostic purposes in the context of different diseases. We conclude with future perspectives on the evolution of LLMs towards truly multimodal platforms, which would integrate diverse data types to provide a more comprehensive understanding of rare diseases, ultimately fostering better outcomes in clinical settings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.17067</link>
<guid>https://arxiv.org/abs/2505.17067</guid>
<content:encoded><![CDATA[

arXiv:2505.17067v1 Announce Type: cross 
Abstract: Detecting Mild Cognitive Impairment from picture descriptions is critical yet challenging, especially in multilingual and multiple picture settings. Prior work has primarily focused on English speakers describing a single picture (e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by introducing multilingual speakers and multiple pictures, which presents new challenges in analyzing picture-dependent content. To address these challenges, we propose a framework with three components: (1) enhancing discriminative representation learning via supervised contrastive learning, (2) involving image modality rather than relying solely on speech and text modalities, and (3) applying a Product of Experts (PoE) strategy to mitigate spurious correlations and overfitting. Our framework improves MCI detection performance, achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to 75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the text unimodal baseline. Notably, the contrastive learning component yields greater gains for the text modality compared to speech. These results highlight our framework's effectiveness in multilingual and multi-picture MCI detection.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning</title>
<link>https://arxiv.org/abs/2505.17068</link>
<guid>https://arxiv.org/abs/2505.17068</guid>
<content:encoded><![CDATA[

arXiv:2505.17068v1 Announce Type: cross 
Abstract: In health-related topics, user toxicity in online discussions frequently becomes a source of social conflict or promotion of dangerous, unscientific behaviour; common approaches for battling it include different forms of detection, flagging and/or removal of existing toxic comments, which is often counterproductive for platforms and users alike. In this work, we propose the alternative of combatting user toxicity predictively, anticipating where a user could interact toxically in health-related online discussions. Applying a Collaborative Filtering-based Machine Learning methodology, we predict the toxicity in COVID-related conversations between any user and subcommunity of Reddit, surpassing 80% predictive performance in relevant metrics, and allowing us to prevent the pairing of conflicting users and subcommunities.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Alignment Can Be Not Superficial With Explicit Safety Signals</title>
<link>https://arxiv.org/abs/2505.17072</link>
<guid>https://arxiv.org/abs/2505.17072</guid>
<content:encoded><![CDATA[

arXiv:2505.17072v1 Announce Type: cross 
Abstract: Recent studies on the safety alignment of large language models (LLMs) have revealed that existing approaches often operate superficially, leaving models vulnerable to various adversarial attacks. Despite their significance, these studies generally fail to offer actionable solutions beyond data augmentation for achieving more robust safety mechanisms. This paper identifies a fundamental cause of this superficiality: existing alignment approaches often presume that models can implicitly learn a safety-related reasoning task during the alignment process, enabling them to refuse harmful requests. However, the learned safety signals are often diluted by other competing objectives, leading models to struggle with drawing a firm safety-conscious decision boundary when confronted with adversarial attacks. Based on this observation, by explicitly introducing a safety-related binary classification task and integrating its signals with our attention and decoding strategies, we eliminate this ambiguity and allow models to respond more responsibly to malicious queries. We emphasize that, with less than 0.2x overhead cost, our approach enables LLMs to assess the safety of both the query and the previously generated tokens at each necessary generating step. Extensive experiments demonstrate that our method significantly improves the resilience of LLMs against various adversarial attacks, offering a promising pathway toward more robust generative AI systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Interpretability of GPT-like Models on Summarization Tasks</title>
<link>https://arxiv.org/abs/2505.17073</link>
<guid>https://arxiv.org/abs/2505.17073</guid>
<content:encoded><![CDATA[

arXiv:2505.17073v1 Announce Type: cross 
Abstract: Mechanistic interpretability research seeks to reveal the inner workings of large language models, yet most work focuses on classification or generative tasks rather than summarization. This paper presents an interpretability framework for analyzing how GPT-like models adapt to summarization tasks. We conduct differential analysis between pre-trained and fine-tuned models, quantifying changes in attention patterns and internal activations. By identifying specific layers and attention heads that undergo significant transformation, we locate the "summarization circuit" within the model architecture. Our findings reveal that middle layers (particularly 2, 3, and 5) exhibit the most dramatic changes, with 62% of attention heads showing decreased entropy, indicating a shift toward focused information selection. We demonstrate that targeted LoRA adaptation of these identified circuits achieves significant performance improvement over standard LoRA fine-tuning while requiring fewer training epochs. This work bridges the gap between black-box evaluation and mechanistic understanding, providing insights into how neural networks perform information selection and compression during summarization.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency</title>
<link>https://arxiv.org/abs/2505.17074</link>
<guid>https://arxiv.org/abs/2505.17074</guid>
<content:encoded><![CDATA[

arXiv:2505.17074v1 Announce Type: cross 
Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by employing a small speculative model (SSM) to generate multiple candidate tokens and verify them using the LLM in parallel. This technique has been widely integrated into LLM inference serving systems. However, inference requests typically exhibit uncertain execution time, which poses a significant challenge of efficiently scheduling requests in these systems. Existing work estimates execution time based solely on predicted output length, which could be inaccurate because execution time depends on both output length and token acceptance rate of verification by the LLM. In this paper, we propose a semi-clairvoyant request scheduling algorithm called Least-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a number of inference requests, LAPS-SD can effectively minimize average inference latency by adaptively scheduling requests according to their features during decoding. When the token acceptance rate is dynamic and execution time is difficult to estimate, LAPS-SD maintains multiple priority queues and allows request execution preemption across different queues. Once the token acceptance rate becomes stable, LAPS-SD can accurately estimate the execution time and schedule requests accordingly. Extensive experiments show that LAPS-SD reduces inference latency by approximately 39\% compared to state-of-the-art scheduling methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streamlining HTTP Flooding Attack Detection through Incremental Feature Selection</title>
<link>https://arxiv.org/abs/2505.17077</link>
<guid>https://arxiv.org/abs/2505.17077</guid>
<content:encoded><![CDATA[

arXiv:2505.17077v1 Announce Type: cross 
Abstract: Applications over the Web primarily rely on the HTTP protocol to transmit web pages to and from systems. There are a variety of application layer protocols, but among all, HTTP is the most targeted because of its versatility and ease of integration with online services. The attackers leverage the fact that by default no detection system blocks any HTTP traffic. Thus, by exploiting such characteristics of the protocol, attacks are launched against web applications. HTTP flooding attacks are one such attack in the application layer of the OSI model. In this paper, a method for the detection of such an attack is proposed. The heart of the detection method is an incremental feature subset selection method based on mutual information and correlation. INFS-MICC helps in identifying a subset of highly relevant and independent feature subset so as to detect HTTP Flooding attacks with best possible classification performance in near-real time.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scale-invariant Attention</title>
<link>https://arxiv.org/abs/2505.17083</link>
<guid>https://arxiv.org/abs/2505.17083</guid>
<content:encoded><![CDATA[

arXiv:2505.17083v1 Announce Type: cross 
Abstract: One persistent challenge in LLM research is the development of attention mechanisms that are able to generalise from training on shorter contexts to inference on longer contexts. We propose two conditions that we expect all effective long context attention mechanisms to have: scale-invariant total attention, and scale-invariant attention sparsity. Under a Gaussian assumption, we show that a simple position-dependent transformation of the attention logits is sufficient for these conditions to hold. Experimentally we find that the resulting scale-invariant attention scheme gives considerable benefits in terms of validation loss when zero-shot generalising from training on short contexts to validation on longer contexts, and is effective at long-context retrieval.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Informatics for Food Processing</title>
<link>https://arxiv.org/abs/2505.17087</link>
<guid>https://arxiv.org/abs/2505.17087</guid>
<content:encoded><![CDATA[

arXiv:2505.17087v1 Announce Type: cross 
Abstract: This chapter explores the evolution, classification, and health implications of food processing, while emphasizing the transformative role of machine learning, artificial intelligence (AI), and data science in advancing food informatics. It begins with a historical overview and a critical review of traditional classification frameworks such as NOVA, Nutri-Score, and SIGA, highlighting their strengths and limitations, particularly the subjectivity and reproducibility challenges that hinder epidemiological research and public policy. To address these issues, the chapter presents novel computational approaches, including FoodProX, a random forest model trained on nutrient composition data to infer processing levels and generate a continuous FPro score. It also explores how large language models like BERT and BioBERT can semantically embed food descriptions and ingredient lists for predictive tasks, even in the presence of missing data. A key contribution of the chapter is a novel case study using the Open Food Facts database, showcasing how multimodal AI models can integrate structured and unstructured data to classify foods at scale, offering a new paradigm for food processing assessment in public health and research.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Implicitly Learn to See and Hear Just By Reading</title>
<link>https://arxiv.org/abs/2505.17091</link>
<guid>https://arxiv.org/abs/2505.17091</guid>
<content:encoded><![CDATA[

arXiv:2505.17091v1 Announce Type: cross 
Abstract: This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Covert Attacks on Machine Learning Training in Passively Secure MPC</title>
<link>https://arxiv.org/abs/2505.17092</link>
<guid>https://arxiv.org/abs/2505.17092</guid>
<content:encoded><![CDATA[

arXiv:2505.17092v1 Announce Type: cross 
Abstract: Secure multiparty computation (MPC) allows data owners to train machine learning models on combined data while keeping the underlying training data private. The MPC threat model either considers an adversary who passively corrupts some parties without affecting their overall behavior, or an adversary who actively modifies the behavior of corrupt parties. It has been argued that in some settings, active security is not a major concern, partly because of the potential risk of reputation loss if a party is detected cheating.
  In this work we show explicit, simple, and effective attacks that an active adversary can run on existing passively secure MPC training protocols, while keeping essentially zero risk of the attack being detected. The attacks we show can compromise both the integrity and privacy of the model, including attacks reconstructing exact training data. Our results challenge the belief that a threat model that does not include malicious behavior by the involved parties may be reasonable in the context of PPML, motivating the use of actively secure protocols for training.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuromorphic Mimicry Attacks Exploiting Brain-Inspired Computing for Covert Cyber Intrusions</title>
<link>https://arxiv.org/abs/2505.17094</link>
<guid>https://arxiv.org/abs/2505.17094</guid>
<content:encoded><![CDATA[

arXiv:2505.17094v1 Announce Type: cross 
Abstract: Neuromorphic computing, inspired by the human brain's neural architecture, is revolutionizing artificial intelligence and edge computing with its low-power, adaptive, and event-driven designs. However, these unique characteristics introduce novel cybersecurity risks. This paper proposes Neuromorphic Mimicry Attacks (NMAs), a groundbreaking class of threats that exploit the probabilistic and non-deterministic nature of neuromorphic chips to execute covert intrusions. By mimicking legitimate neural activity through techniques such as synaptic weight tampering and sensory input poisoning, NMAs evade traditional intrusion detection systems, posing risks to applications such as autonomous vehicles, smart medical implants, and IoT networks. This research develops a theoretical framework for NMAs, evaluates their impact using a simulated neuromorphic chip dataset, and proposes countermeasures, including neural-specific anomaly detection and secure synaptic learning protocols. The findings underscore the critical need for tailored cybersecurity measures to protect brain-inspired computing, offering a pioneering exploration of this emerging threat landscape.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An approach to identify the most semantically informative deep representations of text and images</title>
<link>https://arxiv.org/abs/2505.17101</link>
<guid>https://arxiv.org/abs/2505.17101</guid>
<content:encoded><![CDATA[

arXiv:2505.17101v1 Announce Type: cross 
Abstract: Deep neural networks are known to develop similar representations for semantically related data, even when they belong to different domains, such as an image and its description, or the same text in different languages. We present a method for quantitatively investigating this phenomenon by measuring the relative information content of the representations of semantically related data and probing how it is encoded into multiple tokens of large language models (LLMs) and vision transformers. Looking first at how LLMs process pairs of translated sentences, we identify inner ``semantic'' layers containing the most language-transferable information. We find moreover that, on these layers, a larger LLM (DeepSeek-V3) extracts significantly more general information than a smaller one (Llama3.1-8B). Semantic information is spread across many tokens and it is characterized by long-distance correlations between tokens and by a causal left-to-right (i.e., past-future) asymmetry. We also identify layers encoding semantic information within visual transformers. We show that caption representations in the semantic layers of LLMs predict visual representations of the corresponding images. We observe significant and model-dependent information asymmetries between image and text representations.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution</title>
<link>https://arxiv.org/abs/2505.17107</link>
<guid>https://arxiv.org/abs/2505.17107</guid>
<content:encoded><![CDATA[

arXiv:2505.17107v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents can automate cybersecurity tasks and can adapt to the evolving cybersecurity landscape without re-engineering. While LLM agents have demonstrated cybersecurity capabilities on Capture-The-Flag (CTF) competitions, they have two key limitations: accessing latest cybersecurity expertise beyond training data, and integrating new knowledge into complex task planning. Knowledge-based approaches that incorporate technical understanding into the task-solving automation can tackle these limitations. We present CRAKEN, a knowledge-based LLM agent framework that improves cybersecurity capability through three core mechanisms: contextual decomposition of task-critical information, iterative self-reflected knowledge retrieval, and knowledge-hint injection that transforms insights into adaptive attack strategies. Comprehensive evaluations with different configurations show CRAKEN's effectiveness in multi-stage vulnerability detection and exploitation compared to previous approaches. Our extensible architecture establishes new methodologies for embedding new security knowledge into LLM-driven cybersecurity agentic systems. With a knowledge database of CTF writeups, CRAKEN obtained an accuracy of 22% on NYU CTF Bench, outperforming prior works by 3% and achieving state-of-the-art results. On evaluation of MITRE ATT&amp;CK techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating improved cybersecurity capabilities via knowledge-based execution. We make our framework open source to public https://github.com/NYU-LLM-CTF/nyuctf_agents_craken.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language</title>
<link>https://arxiv.org/abs/2505.17114</link>
<guid>https://arxiv.org/abs/2505.17114</guid>
<content:encoded><![CDATA[

arXiv:2505.17114v1 Announce Type: cross 
Abstract: Multimodal question answering (QA) often requires identifying which video, audio, or sensor tokens are relevant to the question. Yet modality disagreements are common: off-camera speech, background noise, or motion outside the field of view often mislead fusion models that weight all streams equally. We present RAVEN, a unified QA architecture whose core is QuART, a query-conditioned cross-modal gating module that assigns scalar relevance scores to each token across modalities, enabling the model to amplify informative signals and suppress distractors before fusion. RAVEN is trained through a three-stage pipeline comprising unimodal pretraining, query-aligned fusion, and disagreement-oriented fine-tuning -- each stage targeting a distinct challenge in multi-modal reasoning: representation quality, cross-modal relevance, and robustness to modality mismatch. To support training and evaluation, we release AVS-QA, a dataset of 300K synchronized Audio--Video-Sensor streams paired with automatically generated question-answer pairs. Experimental results on seven multi-modal QA benchmarks -- including egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\% and 8.0\% gains in accuracy compared to state-of-the-art multi-modal large language models, respectively. Incorporating sensor data provides an additional 16.4\% boost, and the model remains robust under modality corruption, outperforming SOTA baselines by 50.23\%. Our code and dataset are available at https://github.com/BASHLab/RAVEN.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.17119</link>
<guid>https://arxiv.org/abs/2505.17119</guid>
<content:encoded><![CDATA[

arXiv:2505.17119v1 Announce Type: cross 
Abstract: Recent research leverages large language models (LLMs) for early mental health detection, such as depression, often optimized with machine-generated data. However, their detection may be subject to unknown weaknesses. Meanwhile, quality control has not been applied to these generated corpora besides limited human verifications. Our goal is to systematically evaluate LLM reasoning and reveal potential weaknesses. To this end, we first provide a systematic evaluation of the reasoning over machine-generated detection and interpretation. Then we use the models' reasoning abilities to explore mitigation strategies for enhanced performance. Specifically, we do the following: A. Design an LLM instruction strategy that allows for systematic analysis of the detection by breaking down the task into several subtasks. B. Design contrastive few-shot and chain-of-thought prompts by selecting typical positive and negative examples of detection reasoning. C. Perform human annotation for the subtasks identified in the first step and evaluate the performance. D. Identify human-preferred detection with desired logical reasoning from the few-shot generation and use them to explore different optimization strategies. We conducted extensive comparisons on the DepTweet dataset across the following subtasks: 1. identifying whether the speaker is describing their own depression; 2. accurately detecting the presence of PHQ-9 symptoms, and 3. finally, detecting depression. Human verification of statistical outliers shows that LLMs demonstrate greater accuracy in analyzing and detecting explicit language of depression as opposed to implicit expressions of depression. Two optimization methods are used for performance enhancement and reduction of the statistic bias: supervised fine-tuning (SFT) and direct preference optimization (DPO). Notably, the DPO approach achieves significant performance improvement.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Language Model Reasoning with Coherent Factuality</title>
<link>https://arxiv.org/abs/2505.17126</link>
<guid>https://arxiv.org/abs/2505.17126</guid>
<content:encoded><![CDATA[

arXiv:2505.17126v1 Announce Type: cross 
Abstract: Language models are increasingly being used in important decision pipelines, so ensuring the correctness of their outputs is crucial. Recent work has proposed evaluating the "factuality" of claims decomposed from a language model generation and applying conformal prediction techniques to filter out those claims that are not factual. This can be effective for tasks such as information retrieval, where constituent claims may be evaluated in isolation for factuality, but is not appropriate for reasoning tasks, as steps of a logical argument can be evaluated for correctness only within the context of the claims that precede them. To capture this, we define "coherent factuality" and develop a conformal-prediction-based method to guarantee coherent factuality for language model outputs. Our approach applies split conformal prediction to subgraphs within a "deducibility" graph" that represents the steps of a reasoning problem. We evaluate our method on mathematical reasoning problems from the MATH and FELM datasets and find that our algorithm consistently produces correct and substantiated orderings of claims, achieving coherent factuality across target coverage levels. Moreover, we achieve 90% factuality on our stricter definition while retaining 80% or more of the original claims, highlighting the utility of our deducibility-graph-guided approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts</title>
<link>https://arxiv.org/abs/2505.17127</link>
<guid>https://arxiv.org/abs/2505.17127</guid>
<content:encoded><![CDATA[

arXiv:2505.17127v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) perform well on tasks such as visual question answering, but it remains unclear whether their reasoning relies more on memorized world knowledge or on the visual information present in the input image. To investigate this, we introduce Visual CounterFact, a new dataset of visually-realistic counterfactuals that put world knowledge priors (e.g, red strawberry) into direct conflict with visual input (e.g, blue strawberry). Using Visual CounterFact, we show that model predictions initially reflect memorized priors, but shift toward visual evidence in mid-to-late layers. This dynamic reveals a competition between the two modalities, with visual input ultimately overriding priors during evaluation. To control this behavior, we propose Pixels Versus Priors (PvP) steering vectors, a mechanism for controlling model outputs toward either world knowledge or visual input through activation-level interventions. On average, PvP successfully shifts 92.5% of color and 74.6% of size predictions from priors to counterfactuals. Together, these findings offer new tools for interpreting and controlling factual behavior in multimodal models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Probabilities of Causation from Finite Population Data</title>
<link>https://arxiv.org/abs/2505.17133</link>
<guid>https://arxiv.org/abs/2505.17133</guid>
<content:encoded><![CDATA[

arXiv:2505.17133v1 Announce Type: cross 
Abstract: Probabilities of causation play a crucial role in modern decision-making. This paper addresses the challenge of predicting probabilities of causation for subpopulations with \textbf{insufficient} data using machine learning models. Tian and Pearl first defined and derived tight bounds for three fundamental probabilities of causation: the probability of necessity and sufficiency (PNS), the probability of sufficiency (PS), and the probability of necessity (PN). However, estimating these probabilities requires both experimental and observational distributions specific to each subpopulation, which are often unavailable or impractical to obtain with limited population-level data. Therefore, for most subgroups, the amount of data they have is not enough to guarantee the accuracy of their probabilities. Hence, to estimate these probabilities for subpopulations with \textbf{insufficient} data, we propose using machine learning models that draw insights from subpopulations with sufficient data. Our evaluation of multiple machine learning models indicates that, given the population-level data and an appropriate choice of machine learning model and activation function, PNS can be effectively predicted. Through simulation studies on multiple Structured Causal Models (SCMs), we show that our multilayer perceptron (MLP) model with the Mish activation function achieves a mean absolute error (MAE) of approximately $0.02$ in predicting PNS for $32,768$ subpopulations across most SCMs using data from only $2,000$ subpopulations with known PNS values.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs</title>
<link>https://arxiv.org/abs/2505.17140</link>
<guid>https://arxiv.org/abs/2505.17140</guid>
<content:encoded><![CDATA[

arXiv:2505.17140v1 Announce Type: cross 
Abstract: As the knowledge of large language models (LLMs) becomes outdated over time, there is a growing need for efficient methods to update them, especially when injecting proprietary information. Our study reveals that comprehension-intensive fine-tuning tasks (e.g., question answering and blanks) achieve substantially higher knowledge retention rates (48%) compared to mapping-oriented tasks like translation (17%) or text-to-JSON conversion (20%), despite exposure to identical factual content. We demonstrate that this pattern persists across model architectures and follows scaling laws, with larger models showing improved retention across all task types. However, all models exhibit significant performance drops when applying injected knowledge in broader contexts, suggesting limited semantic integration. These findings show the importance of task selection in updating LLM knowledge, showing that effective knowledge injection relies not just on data exposure but on the depth of cognitive engagement during fine-tuning.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG</title>
<link>https://arxiv.org/abs/2505.17156</link>
<guid>https://arxiv.org/abs/2505.17156</guid>
<content:encoded><![CDATA[

arXiv:2505.17156v1 Announce Type: cross 
Abstract: The introduction of Large Language Models (LLMs) has significantly transformed Natural Language Processing (NLP) applications by enabling more advanced analysis of customer personas. At Volvo Construction Equipment (VCE), customer personas have traditionally been developed through qualitative methods, which are time-consuming and lack scalability. The main objective of this paper is to generate synthetic customer personas and integrate them into a Retrieval-Augmented Generation (RAG) chatbot to support decision-making in business processes. To this end, we first focus on developing a persona-based RAG chatbot integrated with verified personas. Next, synthetic personas are generated using Few-Shot and Chain-of-Thought (CoT) prompting techniques and evaluated based on completeness, relevance, and consistency using McNemar's test. In the final step, the chatbot's knowledge base is augmented with synthetic personas and additional segment information to assess improvements in response accuracy and practical utility. Key findings indicate that Few-Shot prompting outperformed CoT in generating more complete personas, while CoT demonstrated greater efficiency in terms of response time and token usage. After augmenting the knowledge base, the average accuracy rating of the chatbot increased from 5.88 to 6.42 on a 10-point scale, and 81.82% of participants found the updated system useful in business contexts.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting</title>
<link>https://arxiv.org/abs/2505.17160</link>
<guid>https://arxiv.org/abs/2505.17160</guid>
<content:encoded><![CDATA[

arXiv:2505.17160v1 Announce Type: cross 
Abstract: This work presents LURK (Latent UnleaRned Knowledge), a novel framework that probes for hidden retained knowledge in unlearned LLMs through adversarial suffix prompting. LURK automatically generates adversarial prompt suffixes designed to elicit residual knowledge about the Harry Potter domain, a commonly used benchmark for unlearning. Our experiments reveal that even models deemed successfully unlearned can leak idiosyncratic information under targeted adversarial conditions, highlighting critical limitations of current unlearning evaluation standards. By uncovering latent knowledge through indirect probing, LURK offers a more rigorous and diagnostic tool for assessing the robustness of unlearning algorithms. All code will be publicly available.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer Faster, Price Smarter: Minimax Dynamic Pricing under Cross-Market Preference Shift</title>
<link>https://arxiv.org/abs/2505.17203</link>
<guid>https://arxiv.org/abs/2505.17203</guid>
<content:encoded><![CDATA[

arXiv:2505.17203v1 Announce Type: cross 
Abstract: We study contextual dynamic pricing when a target market can leverage K auxiliary markets -- offline logs or concurrent streams -- whose mean utilities differ by a structured preference shift. We propose Cross-Market Transfer Dynamic Pricing (CM-TDP), the first algorithm that provably handles such model-shift transfer and delivers minimax-optimal regret for both linear and non-parametric utility models.
  For linear utilities of dimension d, where the difference between source- and target-task coefficients is $s_{0}$-sparse, CM-TDP attains regret $\tilde{O}((d*K^{-1}+s_{0})\log T)$. For nonlinear demand residing in a reproducing kernel Hilbert space with effective dimension $\alpha$, complexity $\beta$ and task-similarity parameter $H$, the regret becomes $\tilde{O}\!(K^{-2\alpha\beta/(2\alpha\beta+1)}T^{1/(2\alpha\beta+1)} + H^{2/(2\alpha+1)}T^{1/(2\alpha+1)})$, matching information-theoretic lower bounds up to logarithmic factors. The RKHS bound is the first of its kind for transfer pricing and is of independent interest.
  Extensive simulations show up to 50% lower cumulative regret and 5 times faster learning relative to single-market pricing baselines. By bridging transfer learning, robust aggregation, and revenue optimization, CM-TDP moves toward pricing systems that transfer faster, price smarter.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Liouville PDE-based sliced-Wasserstein flow for fair regression</title>
<link>https://arxiv.org/abs/2505.17204</link>
<guid>https://arxiv.org/abs/2505.17204</guid>
<content:encoded><![CDATA[

arXiv:2505.17204v1 Announce Type: cross 
Abstract: The sliced Wasserstein flow (SWF), a nonparametric and implicit generative gradient flow, is applied to fair regression. We have improved the SWF in a few aspects. First, the stochastic diffusive term from the Fokker-Planck equation-based Monte Carlo is transformed to Liouville partial differential equation (PDE)-based transport with density estimation, however, without the diffusive term. Now, the computation of the Wasserstein barycenter is approximated by the SWF barycenter with the prescription of Kantorovich potentials for the induced gradient flow to generate its samples. These two efforts improve the convergence in training and testing SWF and SWF barycenters with reduced variance. Applying the generative SWF barycenter for fair regression demonstrates competent profiles in the accuracy-fairness Pareto curves.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Content Moderation in TV Search: Balancing Policy Compliance, Relevance, and User Experience</title>
<link>https://arxiv.org/abs/2505.17207</link>
<guid>https://arxiv.org/abs/2505.17207</guid>
<content:encoded><![CDATA[

arXiv:2505.17207v1 Announce Type: cross 
Abstract: Millions of people rely on search functionality to find and explore content on entertainment platforms. Modern search systems use a combination of candidate generation and ranking approaches, with advanced methods leveraging deep learning and LLM-based techniques to retrieve, generate, and categorize search results. Despite these advancements, search algorithms can still surface inappropriate or irrelevant content due to factors like model unpredictability, metadata errors, or overlooked design flaws. Such issues can misalign with product goals and user expectations, potentially harming user trust and business outcomes. In this work, we introduce an additional monitoring layer using Large Language Models (LLMs) to enhance content moderation. This additional layer flags content if the user did not intend to search for it. This approach serves as a baseline for product quality assurance, with collected feedback used to refine the initial retrieval mechanisms of the search model, ensuring a safer and more reliable user experience.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the generalization performance of SAM for ureteroscopy scene understanding</title>
<link>https://arxiv.org/abs/2505.17210</link>
<guid>https://arxiv.org/abs/2505.17210</guid>
<content:encoded><![CDATA[

arXiv:2505.17210v1 Announce Type: cross 
Abstract: The segmentation of kidney stones is regarded as a critical preliminary step to enable the identification of urinary stone types through machine- or deep-learning-based approaches. In urology, manual segmentation is considered tedious and impractical due to the typically large scale of image databases and the continuous generation of new data. In this study, the potential of the Segment Anything Model (SAM) -- a state-of-the-art deep learning framework -- is investigated for the automation of kidney stone segmentation. The performance of SAM is evaluated in comparison to traditional models, including U-Net, Residual U-Net, and Attention U-Net, which, despite their efficiency, frequently exhibit limitations in generalizing to unseen datasets. The findings highlight SAM's superior adaptability and efficiency. While SAM achieves comparable performance to U-Net on in-distribution data (Accuracy: 97.68 + 3.04; Dice: 97.78 + 2.47; IoU: 95.76 + 4.18), it demonstrates significantly enhanced generalization capabilities on out-of-distribution data, surpassing all U-Net variants by margins of up to 23 percent.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Dataset Pruning to Object Detection: A Variance-based Approach</title>
<link>https://arxiv.org/abs/2505.17245</link>
<guid>https://arxiv.org/abs/2505.17245</guid>
<content:encoded><![CDATA[

arXiv:2505.17245v1 Announce Type: cross 
Abstract: Dataset pruning -- selecting a small yet informative subset of training data -- has emerged as a promising strategy for efficient machine learning, offering significant reductions in computational cost and storage compared to alternatives like dataset distillation. While pruning methods have shown strong performance in image classification, their extension to more complex computer vision tasks, particularly object detection, remains relatively underexplored. In this paper, we present the first principled extension of classification pruning techniques to the object detection domain, to the best of our knowledge. We identify and address three key challenges that hinder this transition: the Object-Level Attribution Problem, the Scoring Strategy Problem, and the Image-Level Aggregation Problem. To overcome these, we propose tailored solutions, including a novel scoring method called Variance-based Prediction Score (VPS). VPS leverages both Intersection over Union (IoU) and confidence scores to effectively identify informative training samples specific to detection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate that our approach consistently outperforms prior dataset pruning methods in terms of mean Average Precision (mAP). We also show that annotation count and class distribution shift can influence detection performance, but selecting informative examples is a more critical factor than dataset size or balance. Our work bridges dataset pruning and object detection, paving the way for dataset pruning in complex vision tasks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17249</link>
<guid>https://arxiv.org/abs/2505.17249</guid>
<content:encoded><![CDATA[

arXiv:2505.17249v1 Announce Type: cross 
Abstract: Big trajectory data hold great promise for human mobility analysis, but their utility is often constrained by the absence of critical traveler attributes, particularly sociodemographic information. While prior studies have explored predicting such attributes from mobility patterns, they often overlooked underlying cognitive mechanisms and exhibited low predictive accuracy. This study introduces SILIC, short for Sociodemographic Inference with LLM-guided Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a theoretically grounded framework that leverages LLMs to infer sociodemographic attributes from observed mobility patterns by capturing latent behavioral intentions and reasoning through psychological constructs. Particularly, our approach explicitly follows the Theory of Planned Behavior (TPB), a foundational behavioral framework in transportation research, to model individuals' latent cognitive processes underlying travel decision-making. The LLMs further provide heuristic guidance to improve IRL reward function initialization and update by addressing its ill-posedness and optimization challenges arising from the vast and unstructured reward space. Evaluated in the 2017 Puget Sound Regional Council Household Travel Survey, our method substantially outperforms state-of-the-art baselines and shows great promise for enriching big trajectory data to support more behaviorally grounded applications in transportation planning and beyond.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models</title>
<link>https://arxiv.org/abs/2505.17250</link>
<guid>https://arxiv.org/abs/2505.17250</guid>
<content:encoded><![CDATA[

arXiv:2505.17250v1 Announce Type: cross 
Abstract: Large language models excel at complex tasks by breaking down problems into structured reasoning steps. However, reasoning traces often extend beyond reaching a correct answer, causing wasted computation, reduced readability, and hallucinations. To address this, we introduce a novel hyperparameter-free conciseness score used as a reward signal within a reinforcement learning framework to guide models toward generating correct and concise reasoning traces. This score is evaluated by a large language model acting as a judge, enabling dynamic, context-aware feedback beyond simple token length. Our method achieves state-of-the-art efficiency-accuracy trade-offs on the MATH dataset, reducing token usage by up to 31x on simple problems while improving accuracy by 7%, and on the hardest problems, it outperforms full reasoning by +7.5% accuracy with up to 3.6x fewer tokens. On TheoremQA, our method improves accuracy by +2.2% using 12.5x fewer tokens. We also conduct ablation studies on the judge model, reward composition, and problem difficulty, showing that our method dynamically adapts reasoning length based on problem difficulty and benefits significantly from stronger judges. The code, model weights, and datasets are open-sourced at https://github.com/RazvanDu/ConciseRL.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deconfounded Warm-Start Thompson Sampling with Applications to Precision Medicine</title>
<link>https://arxiv.org/abs/2505.17283</link>
<guid>https://arxiv.org/abs/2505.17283</guid>
<content:encoded><![CDATA[

arXiv:2505.17283v1 Announce Type: cross 
Abstract: Randomized clinical trials often require large patient cohorts before drawing definitive conclusions, yet abundant observational data from parallel studies remains underutilized due to confounding and hidden biases. To bridge this gap, we propose Deconfounded Warm-Start Thompson Sampling (DWTS), a practical approach that leverages a Doubly Debiased LASSO (DDL) procedure to identify a sparse set of reliable measured covariates and combines them with key hidden covariates to form a reduced context. By initializing Thompson Sampling (LinTS) priors with DDL-estimated means and variances on these measured features -- while keeping uninformative priors on hidden features -- DWTS effectively harnesses confounded observational data to kick-start adaptive clinical trials. Evaluated on both a purely synthetic environment and a virtual environment created using real cardiovascular risk dataset, DWTS consistently achieves lower cumulative regret than standard LinTS, showing how offline causal insights from observational data can improve trial efficiency and support more personalized treatment decisions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning for Bit String Generation</title>
<link>https://arxiv.org/abs/2505.17288</link>
<guid>https://arxiv.org/abs/2505.17288</guid>
<content:encoded><![CDATA[

arXiv:2505.17288v1 Announce Type: cross 
Abstract: Using the bit string generation problem as a case study, we theoretically compare two standard methods for adapting large language models to new tasks. The first, referred to as supervised fine-tuning, involves training a new next token predictor on good generations. The second method, Best-of-N, trains a reward model to select good responses from a collection generated by an unaltered base model. If the learning setting is realizable, we find that supervised fine-tuning outperforms BoN through a better dependence on the response length in its rate of convergence. If realizability fails, then depending on the failure mode, BoN can enjoy a better rate of convergence in either n or a rate of convergence with better dependence on the response length.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transport with Heterogeneously Missing Data</title>
<link>https://arxiv.org/abs/2505.17291</link>
<guid>https://arxiv.org/abs/2505.17291</guid>
<content:encoded><![CDATA[

arXiv:2505.17291v1 Announce Type: cross 
Abstract: We consider the problem of solving the optimal transport problem between two empirical distributions with missing values. Our main assumption is that the data is missing completely at random (MCAR), but we allow for heterogeneous missingness probabilities across features and across the two distributions. As a first contribution, we show that the Wasserstein distance between empirical Gaussian distributions and linear Monge maps between arbitrary distributions can be debiased without significantly affecting the sample complexity. Secondly, we show that entropic regularized optimal transport can be estimated efficiently and consistently using iterative singular value thresholding (ISVT). We propose a validation set-free hyperparameter selection strategy for ISVT that leverages our estimator of the Bures-Wasserstein distance, which could be of independent interest in general matrix completion problems. Finally, we validate our findings on a wide range of numerical applications.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SELF: Self-Extend the Context Length With Logistic Growth Function</title>
<link>https://arxiv.org/abs/2505.17296</link>
<guid>https://arxiv.org/abs/2505.17296</guid>
<content:encoded><![CDATA[

arXiv:2505.17296v1 Announce Type: cross 
Abstract: Large language models suffer issues when operated on long contexts that are larger than their training context length due to the standard position encoding for tokens in the attention layer. Tokens a long distance apart will rarely have an effect on each other and long prompts yield unexpected results. To solve this problem, we propose SELF (Self-Extend the Context Length With Logistic Growth Function): a solution of grouping consecutive tokens at varying group sizes using a logistic capacity equation combined with a constant group size at smaller relative distances. Our model had an increase in performance of up to 12% compared to the LongLM extension method in LEval (specifically on the Qwen model). On summarization related tasks in LongBench, our model performed up to 6.4% better than LongLM (specifically on the Llama-2-7b model). On reading comprehension tasks from LEval, our model performed up to 5.4% better than the LongLM. Our code is available at https://github.com/alexeipc/SELF-LLM.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference for Online Algorithms</title>
<link>https://arxiv.org/abs/2505.17300</link>
<guid>https://arxiv.org/abs/2505.17300</guid>
<content:encoded><![CDATA[

arXiv:2505.17300v1 Announce Type: cross 
Abstract: Construction of confidence intervals and hypothesis tests for functionals based on asymptotically normal estimators is a classical topic in statistical inference. The simplest and in many cases optimal inference procedure is the Wald interval or the likelihood ratio test, both of which require an estimator and an estimate of the asymptotic variance of the estimator. Estimators obtained from online/sequential algorithms forces one to consider the computational aspects of the inference problem, i.e., one cannot access all of the data as many times as needed. Several works on this topic explored the online estimation of asymptotic variance. In this article, we propose computationally efficient, rate-optimal, and asymptotically valid confidence regions based on the output of online algorithms {\em without} estimating the asymptotic variance. As a special case, this implies inference from any algorithm that yields an asymptotically normal estimator. We focus our efforts on stochastic gradient descent with Polyak averaging to understand the practical performance of the proposed method.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repulsive Ensembles for Bayesian Inference in Physics-informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.17308</link>
<guid>https://arxiv.org/abs/2505.17308</guid>
<content:encoded><![CDATA[

arXiv:2505.17308v1 Announce Type: cross 
Abstract: Physics-informed neural networks (PINNs) have proven an effective tool for solving differential equations, in particular when considering non-standard or ill-posed settings. When inferring solutions and parameters of the differential equation from data, uncertainty estimates are preferable to point estimates, as they give an idea about the accuracy of the solution. In this work, we consider the inverse problem and employ repulsive ensembles of PINNs (RE-PINN) for obtaining such estimates. The repulsion is implemented by adding a particular repulsive term to the loss function, which has the property that the ensemble predictions correspond to the true Bayesian posterior in the limit of infinite ensemble members. Where possible, we compare the ensemble predictions to Monte Carlo baselines. Whereas the standard ensemble tends to collapse to maximum-a-posteriori solutions, the repulsive ensemble produces significantly more accurate uncertainty estimates and exhibits higher sample diversity.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays</title>
<link>https://arxiv.org/abs/2505.17311</link>
<guid>https://arxiv.org/abs/2505.17311</guid>
<content:encoded><![CDATA[

arXiv:2505.17311v1 Announce Type: cross 
Abstract: Unsupervised anomaly detection (UAD) in medical imaging is crucial for identifying pathological abnormalities without requiring extensive labeled data. However, existing diffusion-based UAD models rely solely on imaging features, limiting their ability to distinguish between normal anatomical variations and pathological anomalies. To address this, we propose Diff3M, a multi-modal diffusion-based framework that integrates chest X-rays and structured Electronic Health Records (EHRs) for enhanced anomaly detection. Specifically, we introduce a novel image-EHR cross-attention module to incorporate structured clinical context into the image generation process, improving the model's ability to differentiate normal from abnormal features. Additionally, we develop a static masking strategy to enhance the reconstruction of normal-like images from anomalies. Extensive evaluations on CheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art performance, outperforming existing UAD methods in medical imaging. Our code is available at this http URL https://github.com/nth221/Diff3M
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking</title>
<link>https://arxiv.org/abs/2505.17312</link>
<guid>https://arxiv.org/abs/2505.17312</guid>
<content:encoded><![CDATA[

arXiv:2505.17312v1 Announce Type: cross 
Abstract: LLMs often need effective configurations, like temperature and reasoning steps, to handle tasks requiring sophisticated reasoning and problem-solving, ranging from joke generation to mathematical reasoning. Existing prompting approaches usually adopt general-purpose, fixed configurations that work 'well enough' across tasks but seldom achieve task-specific optimality. To address this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM to automate adaptive reasoning configurations for tasks requiring different types of thinking. AdaReasoner is trained using a reinforcement learning (RL) framework, combining a factorized action space with a targeted exploration strategy, along with a pretrained reward model to optimize the policy model for reasoning configurations with only a few-shot guide. AdaReasoner is backed by theoretical guarantees and experiments of fast convergence and a sublinear policy gap. Across six different LLMs and a variety of reasoning tasks, it consistently outperforms standard baselines, preserves out-of-distribution robustness, and yield gains on knowledge-intensive tasks through tailored prompts.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning</title>
<link>https://arxiv.org/abs/2505.17315</link>
<guid>https://arxiv.org/abs/2505.17315</guid>
<content:encoded><![CDATA[

arXiv:2505.17315v1 Announce Type: cross 
Abstract: Recent language models exhibit strong reasoning capabilities, yet the influence of long-context capacity on reasoning remains underexplored. In this work, we hypothesize that current limitations in reasoning stem, in part, from insufficient long-context capacity, motivated by empirical observations such as (1) higher context window length often leads to stronger reasoning performance, and (2) failed reasoning cases resemble failed long-context cases. To test this hypothesis, we examine whether enhancing a model's long-context ability before Supervised Fine-Tuning (SFT) leads to improved reasoning performance. Specifically, we compared models with identical architectures and fine-tuning data but varying levels of long-context capacity. Our results reveal a consistent trend: models with stronger long-context capacity achieve significantly higher accuracy on reasoning benchmarks after SFT. Notably, these gains persist even on tasks with short input lengths, indicating that long-context training offers generalizable benefits for reasoning performance. These findings suggest that long-context modeling is not just essential for processing lengthy inputs, but also serves as a critical foundation for reasoning. We advocate for treating long-context capacity as a first-class objective in the design of future language models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models</title>
<link>https://arxiv.org/abs/2505.17316</link>
<guid>https://arxiv.org/abs/2505.17316</guid>
<content:encoded><![CDATA[

arXiv:2505.17316v1 Announce Type: cross 
Abstract: Achieving better alignment between vision embeddings and Large Language Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs (MLLMs), particularly for recent models that rely on powerful pretrained vision encoders and LLMs. A common approach to connect the pretrained vision encoder and LLM is through a projector applied after the vision encoder. However, the projector is often trained to enable the LLM to generate captions, and hence the mechanism by which LLMs understand each vision token remains unclear. In this work, we first investigate the role of the projector in compressing vision embeddings and aligning them with word embeddings. We show that the projector significantly compresses visual information, removing redundant details while preserving essential elements necessary for the LLM to understand visual content. We then examine patch-level alignment -- the alignment between each vision patch and its corresponding semantic words -- and propose a *multi-semantic alignment hypothesis*. Our analysis indicates that the projector trained by caption loss improves patch-level alignment but only to a limited extent, resulting in weak and coarse alignment. To address this issue, we propose *patch-aligned training* to efficiently enhance patch-level alignment. Our experiments show that patch-aligned training (1) achieves stronger compression capability and improved patch-level alignment, enabling the MLLM to generate higher-quality captions, (2) improves the MLLM's performance by 16% on referring expression grounding tasks, 4% on question-answering tasks, and 3% on modern instruction-following benchmarks when using the same supervised fine-tuning (SFT) setting. The proposed method can be easily extended to other multimodal models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Compression to Expansion: A Layerwise Analysis of In-Context Learning</title>
<link>https://arxiv.org/abs/2505.17322</link>
<guid>https://arxiv.org/abs/2505.17322</guid>
<content:encoded><![CDATA[

arXiv:2505.17322v1 Announce Type: cross 
Abstract: In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks without weight updates by learning from demonstration sequences. While ICL shows strong empirical performance, its internal representational mechanisms are not yet well understood. In this work, we conduct a statistical geometric analysis of ICL representations to investigate how task-specific information is captured across layers. Our analysis reveals an intriguing phenomenon, which we term *Layerwise Compression-Expansion*: early layers progressively produce compact and discriminative representations that encode task information from the input demonstrations, while later layers expand these representations to incorporate the query and generate the prediction. This phenomenon is observed consistently across diverse tasks and a range of contemporary LLM architectures. We demonstrate that it has important implications for ICL performance -- improving with model size and the number of demonstrations -- and for robustness in the presence of noisy examples. To further understand the effect of the compact task representation, we propose a bias-variance decomposition and provide a theoretical analysis showing how attention mechanisms contribute to reducing both variance and bias, thereby enhancing performance as the number of demonstrations increases. Our findings reveal an intriguing layerwise dynamic in ICL, highlight how structured representations emerge within LLMs, and showcase that analyzing internal representations can facilitate a deeper understanding of model behavior.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)</title>
<link>https://arxiv.org/abs/2505.17323</link>
<guid>https://arxiv.org/abs/2505.17323</guid>
<content:encoded><![CDATA[

arXiv:2505.17323v1 Announce Type: cross 
Abstract: Humans are remarkably adept at collaboration, able to infer the strengths and weaknesses of new partners in order to work successfully towards shared goals. To build AI systems with this capability, we must first understand its building blocks: does such flexibility require explicit, dedicated mechanisms for modelling others -- or can it emerge spontaneously from the pressures of open-ended cooperative interaction? To investigate this question, we train simple model-free RNN agents to collaborate with a population of diverse partners. Using the `Overcooked-AI' environment, we collect data from thousands of collaborative teams, and analyse agents' internal hidden states. Despite a lack of additional architectural features, inductive biases, or auxiliary objectives, the agents nevertheless develop structured internal representations of their partners' task abilities, enabling rapid adaptation and generalisation to novel collaborators. We investigated these internal models through probing techniques, and large-scale behavioural analysis. Notably, we find that structured partner modelling emerges when agents can influence partner behaviour by controlling task allocation. Our results show that partner modelling can arise spontaneously in model-free agents -- but only under environmental conditions that impose the right kind of social pressure.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints</title>
<link>https://arxiv.org/abs/2505.17327</link>
<guid>https://arxiv.org/abs/2505.17327</guid>
<content:encoded><![CDATA[

arXiv:2505.17327v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) in late 2022 has impacted academic writing, threatening credibility, and causing institutional uncertainty. We seek to determine the degree to which LLMs are used to generate critical text as opposed to being used for editing, such as checking for grammar errors or inappropriate phrasing. In our study, we analyze arXiv papers for stylistic segmentation, which we measure by varying a PELT threshold against a Bayesian classifier trained on GPT-regenerated text. We find that LLM-attributed language is not predictive of stylistic segmentation, suggesting that when authors use LLMs, they do so uniformly, reducing the risk of hallucinations being introduced into academic preprints.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer brain encoders explain human high-level visual responses</title>
<link>https://arxiv.org/abs/2505.17329</link>
<guid>https://arxiv.org/abs/2505.17329</guid>
<content:encoded><![CDATA[

arXiv:2505.17329v1 Announce Type: cross 
Abstract: A major goal of neuroscience is to understand brain computations during visual processing in naturalistic settings. A dominant approach is to use image-computable deep neural networks trained with different task objectives as a basis for linear encoding models. However, in addition to requiring tuning a large number of parameters, the linear encoding approach ignores the structure of the feature maps both in the brain and the models. Recently proposed alternatives have focused on decomposing the linear mapping to spatial and feature components but focus on finding static receptive fields for units that are applicable only in early visual areas. In this work, we employ the attention mechanism used in the transformer architecture to study how retinotopic visual features can be dynamically routed to category-selective areas in high-level visual processing. We show that this computational motif is significantly more powerful than alternative methods in predicting brain activity during natural scene viewing, across different feature basis models and modalities. We also show that this approach is inherently more interpretable, without the need to create importance maps, by interpreting the attention routing signal for different high-level categorical areas. Our approach proposes a mechanistic model of how visual information from retinotopic maps can be routed based on the relevance of the input content to different category-selective regions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding</title>
<link>https://arxiv.org/abs/2505.17330</link>
<guid>https://arxiv.org/abs/2505.17330</guid>
<content:encoded><![CDATA[

arXiv:2505.17330v1 Announce Type: cross 
Abstract: In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language/vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAG's capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance. Code : https://github.com/oracle-samples/fs-dag
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use</title>
<link>https://arxiv.org/abs/2505.17332</link>
<guid>https://arxiv.org/abs/2505.17332</guid>
<content:encoded><![CDATA[

arXiv:2505.17332v1 Announce Type: cross 
Abstract: Enterprise customers are increasingly adopting Large Language Models (LLMs) for critical communication tasks, such as drafting emails, crafting sales pitches, and composing casual messages. Deploying such models across different regions requires them to understand diverse cultural and linguistic contexts and generate safe and respectful responses. For enterprise applications, it is crucial to mitigate reputational risks, maintain trust, and ensure compliance by effectively identifying and handling unsafe or offensive language. To address this, we introduce SweEval, a benchmark simulating real-world scenarios with variations in tone (positive or negative) and context (formal or informal). The prompts explicitly instruct the model to include specific swear words while completing the task. This benchmark evaluates whether LLMs comply with or resist such inappropriate instructions and assesses their alignment with ethical frameworks, cultural nuances, and language comprehension capabilities. In order to advance research in building ethically aligned AI systems for enterprise use and beyond, we release the dataset and code: https://github.com/amitbcp/multilingual_profanity.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Ascent Diffusion for Inverse Problems</title>
<link>https://arxiv.org/abs/2505.17353</link>
<guid>https://arxiv.org/abs/2505.17353</guid>
<content:encoded><![CDATA[

arXiv:2505.17353v1 Announce Type: cross 
Abstract: Ill-posed inverse problems are fundamental in many domains, ranging from astrophysics to medical imaging. Emerging diffusion models provide a powerful prior for solving these problems. Existing maximum-a-posteriori (MAP) or posterior sampling approaches, however, rely on different computational approximations, leading to inaccurate or suboptimal samples. To address this issue, we introduce a new approach to solving MAP problems with diffusion model priors using a dual ascent optimization framework. Our framework achieves better image quality as measured by various metrics for image restoration problems, it is more robust to high levels of measurement noise, it is faster, and it estimates solutions that represent the observations more faithfully than the state of the art.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmable Photonic Unitary Processor Enables Parametrized Differentiable Long-Haul Spatial Division Multiplexed Transmission</title>
<link>https://arxiv.org/abs/2505.17381</link>
<guid>https://arxiv.org/abs/2505.17381</guid>
<content:encoded><![CDATA[

arXiv:2505.17381v1 Announce Type: cross 
Abstract: The explosive growth of global data traffic demands scalable and energy-efficient optical communication systems. Spatial division multiplexing (SDM) using multicore or multimode fibers is a promising solution to overcome the capacity limit of single-mode fibers. However, long-haul SDM transmission faces significant challenges due to modal dispersion, which imposes heavy computational loads on digital signal processing (DSP) for signal equalization. Here, we propose parameterized SDM transmission, where programmable photonic unitary processors are installed at intermediate nodes. Instead of relying on conventional digital equalization only on the receiver side, our approach enables direct optimization of the SDM transmission channel itself by the programmable unitary processor, which reduces digital post-processing loads. We introduce a gradient-based optimization algorithm using a differentiable SDM transmission model to determine the optimal unitary transformation. As a key enabler, we first implemented telecom-grade programmable photonic unitary processor, achieving a low-loss (2.1 dB fiber-to-fiber), wideband (full C-band), polarization-independent, and high-fidelity (R2>96% across the C-band) operation. We experimentally demonstrate 1300-km transmission using a three-mode fiber, achieving strong agreement between simulation and experiment. The optimized photonic processor significantly reduces modal dispersion and post-processing complexity. Our results establish a scalable framework for integrating photonic computation into the optical layer, enabling more efficient, high-capacity optical networks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies</title>
<link>https://arxiv.org/abs/2505.17420</link>
<guid>https://arxiv.org/abs/2505.17420</guid>
<content:encoded><![CDATA[

arXiv:2505.17420v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved remarkable performance across a wide range of NLP tasks. However, their substantial inference cost poses a major barrier to real-world deployment, especially in latency-sensitive scenarios. To address this challenge, we propose \textbf{DASH}, an adaptive layer-skipping framework that dynamically selects computation paths conditioned on input characteristics. We model the skipping process as a Markov Decision Process (MDP), enabling fine-grained token-level decisions based on intermediate representations. To mitigate potential performance degradation caused by skipping, we introduce a lightweight compensation mechanism that injects differential rewards into the decision process. Furthermore, we design an asynchronous execution strategy that overlaps layer computation with policy evaluation to minimize runtime overhead. Experiments on multiple LLM architectures and NLP benchmarks show that our method achieves significant inference acceleration while maintaining competitive task performance, outperforming existing methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Forbidden Topics in Language Models</title>
<link>https://arxiv.org/abs/2505.17441</link>
<guid>https://arxiv.org/abs/2505.17441</guid>
<content:encoded><![CDATA[

arXiv:2505.17441v1 Announce Type: cross 
Abstract: Refusal discovery is the task of identifying the full set of topics that a language model refuses to discuss. We introduce this new problem setting and develop a refusal discovery method, LLM-crawler, that uses token prefilling to find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an open-source model with public safety tuning data. Our crawler manages to retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale the crawl to a frontier model using the prefilling option of Claude-Haiku. Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two of its variants finetuned for reasoning: DeepSeek-R1-70B and Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with censorship tuning: The model exhibits "thought suppression" behavior that indicates memorization of CCP-aligned responses. Although Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned refusals answers in the quantized model. Our findings highlight the critical need for refusal discovery methods to detect biases, boundaries, and alignment failures of AI systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corporate Needs You to Find the Difference: Revisiting Submodular and Supermodular Ratio Optimization Problems</title>
<link>https://arxiv.org/abs/2505.17443</link>
<guid>https://arxiv.org/abs/2505.17443</guid>
<content:encoded><![CDATA[

arXiv:2505.17443v1 Announce Type: cross 
Abstract: We study the problem of minimizing or maximizing the average value $ f(S)/|S| $ of a submodular or supermodular set function $ f: 2^V \to \mathbb{R} $ over non-empty subsets $ S \subseteq V $. This generalizes classical problems such as Densest Subgraph (DSG), Densest Supermodular Set (DSS), and Submodular Function Minimization (SFM). Motivated by recent applications, we introduce two broad formulations: Unrestricted Sparsest Submodular Set (USSS) and Unrestricted Densest Supermodular Set (UDSS), which allow for negative and non-monotone functions.
  We show that DSS, SFM, USSS, UDSS, and the Minimum Norm Point (MNP) problem are equivalent under strongly polynomial-time reductions, enabling algorithmic crossover. In particular, viewing these through the lens of the MNP in the base polyhedron, we connect Fujishige's theory with dense decomposition, and show that both Fujishige-Wolfe's algorithm and the heuristic \textsc{SuperGreedy++} act as universal solvers for all these problems, including sub-modular function minimization.
  Theoretically, we explain why \textsc{SuperGreedy++} is effective beyond DSS, including for tasks like submodular minimization and minimum $ s $-$ t $ cut. Empirically, we test several solvers, including the Fujishige-Wolfe algorithm on over 400 experiments across seven problem types and large-scale real/synthetic datasets. Surprisingly, general-purpose convex and flow-based methods outperform task-specific baselines, demonstrating that with the right framing, general optimization techniques can be both scalable and state-of-the-art for submodular and supermodular ratio problems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Adaptive Experimentation with Non-Compliance</title>
<link>https://arxiv.org/abs/2505.17468</link>
<guid>https://arxiv.org/abs/2505.17468</guid>
<content:encoded><![CDATA[

arXiv:2505.17468v1 Announce Type: cross 
Abstract: We study the problem of estimating the average treatment effect (ATE) in adaptive experiments where treatment can only be encouraged--rather than directly assigned--via a binary instrumental variable. Building on semiparametric efficiency theory, we derive the efficiency bound for ATE estimation under arbitrary, history-dependent instrument-assignment policies, and show it is minimized by a variance-aware allocation rule that balances outcome noise and compliance variability. Leveraging this insight, we introduce AMRIV--an \textbf{A}daptive, \textbf{M}ultiply-\textbf{R}obust estimator for \textbf{I}nstrumental-\textbf{V}ariable settings with variance-optimal assignment. AMRIV pairs (i) an online policy that adaptively approximates the optimal allocation with (ii) a sequential, influence-function-based estimator that attains the semiparametric efficiency bound while retaining multiply-robust consistency. We establish asymptotic normality, explicit convergence rates, and anytime-valid asymptotic confidence sequences that enable sequential inference. Finally, we demonstrate the practical effectiveness of our approach through empirical studies, showing that adaptive instrument assignment, when combined with the AMRIV estimator, yields improved efficiency and robustness compared to existing baselines.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2505.17492</link>
<guid>https://arxiv.org/abs/2505.17492</guid>
<content:encoded><![CDATA[

arXiv:2505.17492v1 Announce Type: cross 
Abstract: Project duplication detection is critical for project quality assessment, as it improves resource utilization efficiency by preventing investing in newly proposed project that have already been studied. It requires the ability to understand high-level semantics and generate constructive and valuable feedback. Existing detection methods rely on basic word- or sentence-level comparison or solely apply large language models, lacking valuable insights for experts and in-depth comprehension of project content and review criteria. To tackle this issue, we propose PD$^3$, a Project Duplication Detection framework via adapted multi-agent Debate. Inspired by real-world expert debates, it employs a fair competition format to guide multi-agent debate to retrieve relevant projects. For feedback, it incorporates both qualitative and quantitative analysis to improve its practicality. Over 800 real-world power project data spanning more than 20 specialized fields are used to evaluate the framework, demonstrating that our method outperforms existing approaches by 7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online platform, Review Dingdang, to assist power experts, saving 5.73 million USD in initial detection on more than 100 newly proposed projects.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models</title>
<link>https://arxiv.org/abs/2505.17496</link>
<guid>https://arxiv.org/abs/2505.17496</guid>
<content:encoded><![CDATA[

arXiv:2505.17496v1 Announce Type: cross 
Abstract: End-to-end training of Spoken Language Models (SLMs) commonly involves adapting pre-trained text-based Large Language Models (LLMs) to the speech modality through multi-stage training on diverse tasks such as ASR, TTS and spoken question answering (SQA). Although this multi-stage continual learning equips LLMs with both speech understanding and generation capabilities, the substantial differences in task and data distributions across stages can lead to catastrophic forgetting, where previously acquired knowledge is lost. This paper investigates catastrophic forgetting and evaluates three mitigation strategies-model merging, discounting the LoRA scaling factor, and experience replay to balance knowledge retention with new learning. Results show that experience replay is the most effective, with further gains achieved by combining it with other methods. These findings provide insights for developing more robust and efficient SLM training pipelines.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition</title>
<link>https://arxiv.org/abs/2505.17501</link>
<guid>https://arxiv.org/abs/2505.17501</guid>
<content:encoded><![CDATA[

arXiv:2505.17501v1 Announce Type: cross 
Abstract: Multimodal emotion recognition analyzes emotions by combining data from multiple sources. However, real-world noise or sensor failures often cause missing or corrupted data, creating the Incomplete Multimodal Emotion Recognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion Recovery (RoHyDR), a novel framework that performs missing-modality recovery at unimodal, multimodal, feature, and semantic levels. For unimodal representation recovery of missing modalities, RoHyDR exploits a diffusion-based generator to generate distribution-consistent and semantically aligned representations from Gaussian noise, using available modalities as conditioning. For multimodal fusion recovery, we introduce adversarial learning to produce a realistic fused multimodal representation and recover missing semantic content. We further propose a multi-stage optimization strategy that enhances training stability and efficiency. In contrast to previous work, the hybrid diffusion and adversarial learning-based recovery mechanism in RoHyDR allows recovery of missing information in both unimodal representation and multimodal fusion, at both feature and semantic levels, effectively mitigating performance degradation caused by suboptimal optimization. Comprehensive experiments conducted on two widely used multimodal emotion recognition benchmarks demonstrate that our proposed method outperforms state-of-the-art IMER methods, achieving robust recognition performance under various missing-modality scenarios. Our code will be made publicly available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Constrained Reinforcement Learning under Partial Data Coverage</title>
<link>https://arxiv.org/abs/2505.17506</link>
<guid>https://arxiv.org/abs/2505.17506</guid>
<content:encoded><![CDATA[

arXiv:2505.17506v1 Announce Type: cross 
Abstract: We study offline constrained reinforcement learning (RL) with general function approximation. We aim to learn a policy from a pre-collected dataset that maximizes the expected discounted cumulative reward for a primary reward signal while ensuring that expected discounted returns for multiple auxiliary reward signals are above predefined thresholds. Existing algorithms either require fully exploratory data, are computationally inefficient, or depend on an additional auxiliary function classes to obtain an $\epsilon$-optimal policy with sample complexity $O(\epsilon^{-2})$. In this paper, we propose an oracle-efficient primal-dual algorithm based on a linear programming (LP) formulation, achieving $O(\epsilon^{-2})$ sample complexity under partial data coverage. By introducing a realizability assumption, our approach ensures that all saddle points of the Lagrangian are optimal, removing the need for regularization that complicated prior analyses. Through Lagrangian decomposition, our method extracts policies without requiring knowledge of the data-generating distribution, enhancing practical applicability.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agent Systems for Misinformation Lifecycle : Detection, Correction And Source Identification</title>
<link>https://arxiv.org/abs/2505.17511</link>
<guid>https://arxiv.org/abs/2505.17511</guid>
<content:encoded><![CDATA[

arXiv:2505.17511v1 Announce Type: cross 
Abstract: The rapid proliferation of misinformation in digital media demands solutions that go beyond isolated Large Language Model(LLM) or AI Agent based detection methods. This paper introduces a novel multi-agent framework that covers the complete misinformation lifecycle: classification, detection, correction, and source verification to deliver more transparent and reliable outcomes. In contrast to single-agent or monolithic architectures, our approach employs five specialized agents: an Indexer agent for dynamically maintaining trusted repositories, a Classifier agent for labeling misinformation types, an Extractor agent for evidence based retrieval and ranking, a Corrector agent for generating fact-based correction and a Verification agent for validating outputs and tracking source credibility. Each agent can be individually evaluated and optimized, ensuring scalability and adaptability as new types of misinformation and data sources emerge. By decomposing the misinformation lifecycle into specialized agents - our framework enhances scalability, modularity, and explainability. This paper proposes a high-level system overview, agent design with emphasis on transparency, evidence-based outputs, and source provenance to support robust misinformation detection and correction at scale.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transparency and Proportionality in Post-Processing Algorithmic Bias Correction</title>
<link>https://arxiv.org/abs/2505.17525</link>
<guid>https://arxiv.org/abs/2505.17525</guid>
<content:encoded><![CDATA[

arXiv:2505.17525v1 Announce Type: cross 
Abstract: Algorithmic decision-making systems sometimes produce errors or skewed predictions toward a particular group, leading to unfair results. Debiasing practices, applied at different stages of the development of such systems, occasionally introduce new forms of unfairness or exacerbate existing inequalities. We focus on post-processing techniques that modify algorithmic predictions to achieve fairness in classification tasks, examining the unintended consequences of these interventions. To address this challenge, we develop a set of measures that quantify the disparity in the flips applied to the solution in the post-processing stage. The proposed measures will help practitioners: (1) assess the proportionality of the debiasing strategy used, (2) have transparency to explain the effects of the strategy in each group, and (3) based on those results, analyze the possibility of the use of some other approaches for bias mitigation or to solve the problem. We introduce a methodology for applying the proposed metrics during the post-processing stage and illustrate its practical application through an example. This example demonstrates how analyzing the proportionality of the debiasing strategy complements traditional fairness metrics, providing a deeper perspective to ensure fairer outcomes across all groups.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPS-Aided Deep Learning for Beam Prediction and Tracking in UAV mmWave Communication</title>
<link>https://arxiv.org/abs/2505.17530</link>
<guid>https://arxiv.org/abs/2505.17530</guid>
<content:encoded><![CDATA[

arXiv:2505.17530v1 Announce Type: cross 
Abstract: Millimeter-wave (mmWave) communication enables high data rates for cellular-connected Unmanned Aerial Vehicles (UAVs). However, a robust beam management remains challenging due to significant path loss and the dynamic mobility of UAVs, which can destabilize the UAV-base station (BS) link. This research presents a GPS-aided deep learning (DL) model that simultaneously predicts current and future optimal beams for UAV mmWave communications, maintaining a Top-1 prediction accuracy exceeding 70% and an average power loss below 0.6 dB across all prediction steps. These outcomes stem from a proposed data set splitting method ensuring balanced label distribution, paired with a GPS preprocessing technique that extracts key positional features, and a DL architecture that maps sequential position data to beam index predictions. The model reduces overhead by approximately 93% (requiring the training of 2 ~ 3 beams instead of 32 beams) with 95% beam prediction accuracy guarantees, and ensures 94% to 96% of predictions exhibit mean power loss not exceeding 1 dB.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AstroMLab 4: Benchmark-Topping Performance in Astronomy Q&amp;A with a 70B-Parameter Domain-Specialized Reasoning Model</title>
<link>https://arxiv.org/abs/2505.17592</link>
<guid>https://arxiv.org/abs/2505.17592</guid>
<content:encoded><![CDATA[

arXiv:2505.17592v1 Announce Type: cross 
Abstract: General-purpose large language models, despite their broad capabilities, often struggle with specialized domain knowledge, a limitation particularly pronounced in more accessible, lower-parameter versions. This gap hinders their deployment as effective agents in demanding fields such as astronomy. Building on our prior work with AstroSage-8B, this study introduces AstroSage-70B, a significantly larger and more advanced domain-specialized natural-language AI assistant. It is designed for research and education across astronomy, astrophysics, space science, astroparticle physics, cosmology, and astronomical instrumentation. Developed from the Llama-3.1-70B foundation, AstroSage-70B underwent extensive continued pre-training on a vast corpus of astronomical literature, followed by supervised fine-tuning and model merging. Beyond its 70-billion parameter scale, this model incorporates refined datasets, judiciously chosen learning hyperparameters, and improved training procedures, achieving state-of-the-art performance on complex astronomical tasks. Notably, we integrated reasoning chains into the SFT dataset, enabling AstroSage-70B to either answer the user query immediately, or first emit a human-readable thought process. Evaluated on the AstroMLab-1 benchmark -- comprising 4,425 questions from literature withheld during training -- AstroSage-70B achieves state-of-the-art performance. It surpasses all other tested open-weight and proprietary models, including leading systems like o3, Gemini-2.5-Pro, Claude-3.7-Sonnet, Deepseek-R1, and Qwen-3-235B, even those with API costs two orders of magnitude higher. This work demonstrates that domain specialization, when applied to large-scale models, can enable them to outperform generalist counterparts in specialized knowledge areas like astronomy, thereby advancing the frontier of AI capabilities in the field.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Image and Video Generation via Test-Time Evolutionary Search</title>
<link>https://arxiv.org/abs/2505.17618</link>
<guid>https://arxiv.org/abs/2505.17618</guid>
<content:encoded><![CDATA[

arXiv:2505.17618v1 Announce Type: cross 
Abstract: As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as a promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains a notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose \textbf{Evo}lutionary \textbf{Search} (EvoSearch), a novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website https://tinnerhrhe.github.io/evosearch.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>\texttt{Range-Arithmetic}: Verifiable Deep Learning Inference on an Untrusted Party</title>
<link>https://arxiv.org/abs/2505.17623</link>
<guid>https://arxiv.org/abs/2505.17623</guid>
<content:encoded><![CDATA[

arXiv:2505.17623v1 Announce Type: cross 
Abstract: Verifiable computing (VC) has gained prominence in decentralized machine learning systems, where resource-intensive tasks like deep neural network (DNN) inference are offloaded to external participants due to blockchain limitations. This creates a need to verify the correctness of outsourced computations without re-execution. We propose \texttt{Range-Arithmetic}, a novel framework for efficient and verifiable DNN inference that transforms non-arithmetic operations, such as rounding after fixed-point matrix multiplication and ReLU, into arithmetic steps verifiable using sum-check protocols and concatenated range proofs. Our approach avoids the complexity of Boolean encoding, high-degree polynomials, and large lookup tables while remaining compatible with finite-field-based proof systems. Experimental results show that our method not only matches the performance of existing approaches, but also reduces the computational cost of verifying the results, the computational effort required from the untrusted party performing the DNN inference, and the communication overhead between the two sides.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIM: Improved Interpretability for Large Language Models</title>
<link>https://arxiv.org/abs/2505.17630</link>
<guid>https://arxiv.org/abs/2505.17630</guid>
<content:encoded><![CDATA[

arXiv:2505.17630v1 Announce Type: cross 
Abstract: Ensuring faithful interpretability in large language models is imperative for trustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where networks compensate for reduced signal in one component by amplifying others, masking the true importance of the ablated component. While prior work attributes self-repair to layer normalization and back-up components that compensate for ablated components, we identify a novel form occurring within the attention mechanism, where softmax redistribution conceals the influence of important attention scores. This leads traditional ablation and gradient-based methods to underestimate the significance of all components contributing to these attention scores. We introduce Gradient Interaction Modifications (GIM), a technique that accounts for self-repair during backpropagation. Extensive experiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B, Qwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves faithfulness over existing circuit identification and feature attribution methods. Our work is a significant step toward better understanding the inner mechanisms of LLMs, which is crucial for improving them and ensuring their safety. Our code is available at https://github.com/JoakimEdin/gim.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReqBrain: Task-Specific Instruction Tuning of LLMs for AI-Assisted Requirements Generation</title>
<link>https://arxiv.org/abs/2505.17632</link>
<guid>https://arxiv.org/abs/2505.17632</guid>
<content:encoded><![CDATA[

arXiv:2505.17632v1 Announce Type: cross 
Abstract: Requirements elicitation and specification remains a labor-intensive, manual process prone to inconsistencies and gaps, presenting a significant challenge in modern software engineering. Emerging studies underscore the potential of employing large language models (LLMs) for automated requirements generation to support requirements elicitation and specification; however, it remains unclear how to implement this effectively. In this work, we introduce ReqBrain, an Al-assisted tool that employs a fine-tuned LLM to generate authentic and adequate software requirements. Software engineers can engage with ReqBrain through chat-based sessions to automatically generate software requirements and categorize them by type. We curated a high-quality dataset of ISO 29148-compliant requirements and fine-tuned five 7B-parameter LLMs to determine the most effective base model for ReqBrain. The top-performing model, Zephyr-7b-beta, achieved 89.30\% Fl using the BERT score and a FRUGAL score of 91.20 in generating authentic and adequate requirements. Human evaluations further confirmed ReqBrain's effectiveness in generating requirements. Our findings suggest that generative Al, when fine-tuned, has the potential to improve requirements elicitation and specification, paving the way for future extensions into areas such as defect identification, test case generation, and agile user story creation.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks</title>
<link>https://arxiv.org/abs/2505.17643</link>
<guid>https://arxiv.org/abs/2505.17643</guid>
<content:encoded><![CDATA[

arXiv:2505.17643v1 Announce Type: cross 
Abstract: Conventional machine learning models, particularly tree-based approaches, have demonstrated promising performance across various clinical prediction tasks using electronic health record (EHR) data. Despite their strengths, these models struggle with tasks that require deeper contextual understanding, such as predicting 30-day hospital readmission. This can be primarily due to the limited semantic information available in structured EHR data. To address this limitation, we propose a deep multimodal contrastive learning (CL) framework that aligns the latent representations of structured EHR data with unstructured discharge summary notes. It works by pulling together paired EHR and text embeddings while pushing apart unpaired ones. Fine-tuning the pretrained EHR encoder extracted from this framework significantly boosts downstream task performance, e.g., a 4.1% AUROC enhancement over XGBoost for 30-day readmission prediction. Such results demonstrate the effect of integrating domain knowledge from clinical notes into EHR-based pipelines, enabling more accurate and context-aware clinical decision support systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning</title>
<link>https://arxiv.org/abs/2505.17645</link>
<guid>https://arxiv.org/abs/2505.17645</guid>
<content:encoded><![CDATA[

arXiv:2505.17645v1 Announce Type: cross 
Abstract: Embodied agents operating in smart homes must understand human behavior through diverse sensory inputs and communicate via natural language. While Vision-Language Models (VLMs) have enabled impressive language-grounded perception, their reliance on visual data limits robustness in real-world scenarios with occlusions, poor lighting, or privacy constraints. In this paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that integrates uncommon but powerful sensing modalities, such as LiDAR, infrared, mmWave radar, and WiFi, to enable seamless human perception and reasoning across heterogeneous environments. We address two key challenges: (1) the scarcity of aligned modality-text data for rare sensors, and (2) the heterogeneity of their physical signal representations. To overcome these, we design a Universal Modality-Injection Projector (UMIP) that enhances pre-aligned modality embeddings with fine-grained, text-aligned features from tailored encoders via coarse-to-fine cross-attention without introducing significant alignment overhead. We further introduce a human-VLM collaborative data curation pipeline to generate paired textual annotations for sensing datasets. Extensive experiments on two newly constructed benchmarks show that HoloLLM significantly outperforms existing MLLMs, improving language-grounded human sensing accuracy by up to 30%. This work establishes a new foundation for real-world, language-informed multisensory embodied intelligence.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models</title>
<link>https://arxiv.org/abs/2505.17697</link>
<guid>https://arxiv.org/abs/2505.17697</guid>
<content:encoded><![CDATA[

arXiv:2505.17697v1 Announce Type: cross 
Abstract: Despite the remarkable reasoning performance, eliciting the long chain-of-thought (CoT) ability in large language models (LLMs) typically requires costly reinforcement learning or supervised fine-tuning on high-quality distilled data. We investigate the internal mechanisms behind this capability and show that a small set of high-impact activations in the last few layers largely governs long-form reasoning attributes, such as output length and self-reflection. By simply amplifying these activations and inserting "wait" tokens, we can invoke the long CoT ability without any training, resulting in significantly increased self-reflection rates and accuracy. Moreover, we find that the activation dynamics follow predictable trajectories, with a sharp rise after special tokens and a subsequent exponential decay. Building on these insights, we introduce a general training-free activation control technique. It leverages a few contrastive examples to identify key activations, and employs simple analytic functions to modulate their values at inference time to elicit long CoTs. Extensive experiments confirm the effectiveness of our method in efficiently eliciting long CoT reasoning in LLMs and improving their performance. Additionally, we propose a parameter-efficient fine-tuning method that trains only a last-layer activation amplification module and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning benchmarks with significantly fewer parameters. Our code and data are publicly released.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-Based Program Repair: Fixing Bugs in Continuous Program Spaces</title>
<link>https://arxiv.org/abs/2505.17703</link>
<guid>https://arxiv.org/abs/2505.17703</guid>
<content:encoded><![CDATA[

arXiv:2505.17703v1 Announce Type: cross 
Abstract: Automatic program repair seeks to generate correct code from buggy programs, with most approaches searching the correct program in a discrete, symbolic space of source code tokens. This symbolic search is fundamentally limited by its inability to directly reason about program behavior. We introduce Gradient-Based Program Repair (GBPR), a new paradigm that reframes program repair as continuous optimization in a differentiable numerical program space. Our core insight is to compile symbolic programs into differentiable numerical representations, enabling search in the numerical program space directly guided by program behavior. To evaluate GBPR, we present RaspBugs, a new benchmark of 1,466 buggy symbolic RASP programs and their respective numerical representations. Our experiments demonstrate that GBPR can effectively repair buggy symbolic programs by gradient-based optimization in the numerical program space, with convincing repair trajectories. To our knowledge, we are the first to state program repair as continuous optimization in a numerical program space. Our work establishes a new direction for program repair research, bridging two rich worlds: continuous optimization and program behavior.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models</title>
<link>https://arxiv.org/abs/2505.17705</link>
<guid>https://arxiv.org/abs/2505.17705</guid>
<content:encoded><![CDATA[

arXiv:2505.17705v1 Announce Type: cross 
Abstract: Knowledge Tracing (KT) aims to model a student's learning state over time and predict their future performance. However, traditional KT methods often face challenges in explainability, scalability, and effective modeling of complex knowledge dependencies. While Large Language Models (LLMs) present new avenues for KT, their direct application often struggles with generating structured, explainable student representations and lacks mechanisms for continuous, task-specific refinement. To address these gaps, we propose Collaborative Iterative Knowledge Tracing (CIKT), a framework that harnesses LLMs to enhance both prediction accuracy and explainability. CIKT employs a dual-component architecture: an Analyst generates dynamic, explainable user profiles from student historical responses, and a Predictor utilizes these profiles to forecast future performance. The core of CIKT is a synergistic optimization loop. In this loop, the Analyst is iteratively refined based on the predictive accuracy of the Predictor, which conditions on the generated profiles, and the Predictor is subsequently retrained using these enhanced profiles. Evaluated on multiple educational datasets, CIKT demonstrates significant improvements in prediction accuracy, offers enhanced explainability through its dynamically updated user profiles, and exhibits improved scalability. Our work presents a robust and explainable solution for advancing knowledge tracing systems, effectively bridging the gap between predictive performance and model transparency.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Distributionally-Robust Framework for Nuisance in Causal Effect Estimation</title>
<link>https://arxiv.org/abs/2505.17717</link>
<guid>https://arxiv.org/abs/2505.17717</guid>
<content:encoded><![CDATA[

arXiv:2505.17717v1 Announce Type: cross 
Abstract: Causal inference requires evaluating models on balanced distributions between treatment and control groups, while training data often exhibits imbalance due to historical decision-making policies. Most conventional statistical methods address this distribution shift through inverse probability weighting (IPW), which requires estimating propensity scores as an intermediate step. These methods face two key challenges: inaccurate propensity estimation and instability from extreme weights. We decompose the generalization error to isolate these issues--propensity ambiguity and statistical instability--and address them through an adversarial loss function. Our approach combines distributionally robust optimization for handling propensity uncertainty with weight regularization based on weighted Rademacher complexity. Experiments on synthetic and real-world datasets demonstrate consistent improvements over existing methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection</title>
<link>https://arxiv.org/abs/2505.17732</link>
<guid>https://arxiv.org/abs/2505.17732</guid>
<content:encoded><![CDATA[

arXiv:2505.17732v1 Announce Type: cross 
Abstract: Accurate, fast, and reliable 3D perception is essential for autonomous driving. Recently, bird's-eye view (BEV)-based perception approaches have emerged as superior alternatives to perspective-based solutions, offering enhanced spatial understanding and more natural outputs for planning. Existing BEV-based 3D object detection methods, typically adhering to angle-based representation, directly estimate the size and orientation of rotated bounding boxes. We observe that BEV-based 3D object detection is analogous to aerial oriented object detection, where angle-based methods are recognized for being affected by discontinuities in their loss functions. Drawing inspiration from this domain, we propose Restricted Quadrilateral Representation to define 3D regression targets. RQR3D regresses the smallest horizontal bounding box encapsulating the oriented box, along with the offsets between the corners of these two boxes, thereby transforming the oriented object detection problem into a keypoint regression task. RQR3D is compatible with any 3D object detection approach. We employ RQR3D within an anchor-free single-stage object detection method and introduce an objectness head to address class imbalance problem. Furthermore, we introduce a simplified radar fusion backbone that eliminates the need for voxel grouping and processes the BEV-mapped point cloud with standard 2D convolutions, rather than sparse convolutions. Extensive evaluations on the nuScenes dataset demonstrate that RQR3D achieves state-of-the-art performance in camera-radar 3D object detection, outperforming the previous best method by +4% in NDS and +2.4% in mAP, and significantly reducing the translation and orientation errors, which are crucial for safe autonomous driving. These consistent gains highlight the robustness, precision, and real-world readiness of our approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Qiskit Machine Learning: an open-source library for quantum machine learning tasks at scale on quantum hardware and classical simulators</title>
<link>https://arxiv.org/abs/2505.17756</link>
<guid>https://arxiv.org/abs/2505.17756</guid>
<content:encoded><![CDATA[

arXiv:2505.17756v1 Announce Type: cross 
Abstract: We present Qiskit Machine Learning (ML), a high-level Python library that combines elements of quantum computing with traditional machine learning. The API abstracts Qiskit's primitives to facilitate interactions with classical simulators and quantum hardware. Qiskit ML started as a proof-of-concept code in 2019 and has since been developed to be a modular, intuitive tool for non-specialist users while allowing extensibility and fine-tuning controls for quantum computational scientists and developers. The library is available as a public, open-source tool and is distributed under the Apache version 2.0 license.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding</title>
<link>https://arxiv.org/abs/2505.17779</link>
<guid>https://arxiv.org/abs/2505.17779</guid>
<content:encoded><![CDATA[

arXiv:2505.17779v1 Announce Type: cross 
Abstract: Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Online Change Detection via Random Fourier Features</title>
<link>https://arxiv.org/abs/2505.17789</link>
<guid>https://arxiv.org/abs/2505.17789</guid>
<content:encoded><![CDATA[

arXiv:2505.17789v1 Announce Type: cross 
Abstract: This article studies the problem of online non-parametric change point detection in multivariate data streams. We approach the problem through the lens of kernel-based two-sample testing and introduce a sequential testing procedure based on random Fourier features, running with logarithmic time complexity per observation and with overall logarithmic space complexity. The algorithm has two advantages compared to the state of the art. First, our approach is genuinely online, and no access to training data known to be from the pre-change distribution is necessary. Second, the algorithm does not require the user to specify a window parameter over which local tests are to be calculated. We prove strong theoretical guarantees on the algorithm's performance, including information-theoretic bounds demonstrating that the detection delay is optimal in the minimax sense. Numerical studies on real and synthetic data show that our algorithm is competitive with respect to the state of the art.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying uncertainty in spectral clusterings: expectations for perturbed and incomplete data</title>
<link>https://arxiv.org/abs/2505.17819</link>
<guid>https://arxiv.org/abs/2505.17819</guid>
<content:encoded><![CDATA[

arXiv:2505.17819v1 Announce Type: cross 
Abstract: Spectral clustering is a popular unsupervised learning technique which is able to partition unlabelled data into disjoint clusters of distinct shapes. However, the data under consideration are often experimental data, implying that the data is subject to measurement errors and measurements may even be lost or invalid. These uncertainties in the corrupted input data induce corresponding uncertainties in the resulting clusters, and the clusterings thus become unreliable.
  Modelling the uncertainties as random processes, we discuss a mathematical framework based on random set theory for the computational Monte Carlo approximation of statistically expected clusterings in case of corrupted, i.e., perturbed, incomplete, and possibly even additional, data. We propose several computationally accessible quantities of interest and analyze their consistency in the infinite data point and infinite Monte Carlo sample limit. Numerical experiments are provided to illustrate and compare the proposed quantities.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Source Separation of Small Classical Ensembles: Challenges and Opportunities</title>
<link>https://arxiv.org/abs/2505.17823</link>
<guid>https://arxiv.org/abs/2505.17823</guid>
<content:encoded><![CDATA[

arXiv:2505.17823v1 Announce Type: cross 
Abstract: Musical (MSS) source separation of western popular music using non-causal deep learning can be very effective. In contrast, MSS for classical music is an unsolved problem. Classical ensembles are harder to separate than popular music because of issues such as the inherent greater variation in the music; the sparsity of recordings with ground truth for supervised training; and greater ambiguity between instruments. The Cadenza project has been exploring MSS for classical music. This is being done so music can be remixed to improve listening experiences for people with hearing loss. To enable the work, a new database of synthesized woodwind ensembles was created to overcome instrumental imbalances in the EnsembleSet. For the MSS, a set of ConvTasNet models was used with each model being trained to extract a string or woodwind instrument. ConvTasNet was chosen because it enabled both causal and non-causal approaches to be tested. Non-causal approaches have dominated MSS work and are useful for recorded music, but for live music or processing on hearing aids, causal signal processing is needed. The MSS performance was evaluated on the two small datasets (Bach10 and URMP) of real instrument recordings where the ground-truth is available. The performances of the causal and non-causal systems were similar. Comparing the average Signal-to-Distortion (SDR) of the synthesized validation set (6.2 dB causal; 6.9 non-causal), to the real recorded evaluation set (0.3 dB causal, 0.4 dB non-causal), shows that mismatch between synthesized and recorded data is a problem. Future work needs to either gather more real recordings that can be used for training, or to improve the realism and diversity of the synthesized recordings to reduce the mismatch...
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus</title>
<link>https://arxiv.org/abs/2505.17833</link>
<guid>https://arxiv.org/abs/2505.17833</guid>
<content:encoded><![CDATA[

arXiv:2505.17833v1 Announce Type: cross 
Abstract: Study of affect in speech requires suitable data, as emotional expression and perception vary across languages. Until now, no corpus has existed for natural expression of affect in spontaneous Finnish, existing data being acted or from a very specific communicative setting. This paper presents the first such corpus, created by annotating 12,000 utterances for emotional arousal and valence, sampled from three large-scale Finnish speech corpora. To ensure diverse affective expression, sample selection was conducted with an affect mining approach combining acoustic, cross-linguistic speech emotion, and text sentiment features. We compare this method to random sampling in terms of annotation diversity, and conduct post-hoc analyses to identify sampling choices that would have maximized the diversity. As an outcome, the work introduces a spontaneous Finnish affective speech corpus and informs sampling strategies for affective speech corpus creation in other languages or domains.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Mamba-Transformer Decoder for Error-Correcting Codes</title>
<link>https://arxiv.org/abs/2505.17834</link>
<guid>https://arxiv.org/abs/2505.17834</guid>
<content:encoded><![CDATA[

arXiv:2505.17834v1 Announce Type: cross 
Abstract: We introduce a novel deep learning method for decoding error correction codes based on the Mamba architecture, enhanced with Transformer layers. Our approach proposes a hybrid decoder that leverages Mamba's efficient sequential modeling while maintaining the global context capabilities of Transformers. To further improve performance, we design a novel layer-wise masking strategy applied to each Mamba layer, allowing selective attention to relevant code features at different depths. Additionally, we introduce a progressive layer-wise loss, supervising the network at intermediate stages and promoting robust feature extraction throughout the decoding process. Comprehensive experiments across a range of linear codes demonstrate that our method significantly outperforms Transformer-only decoders and standard Mamba models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means</title>
<link>https://arxiv.org/abs/2505.17836</link>
<guid>https://arxiv.org/abs/2505.17836</guid>
<content:encoded><![CDATA[

arXiv:2505.17836v1 Announce Type: cross 
Abstract: This paper addresses the problem of robust estimation in gossip algorithms over arbitrary communication graphs. Gossip algorithms are fully decentralized, relying only on local neighbor-to-neighbor communication, making them well-suited for situations where communication is constrained. A fundamental challenge in existing mean-based gossip algorithms is their vulnerability to malicious or corrupted nodes. In this paper, we show that an outlier-robust mean can be computed by globally estimating a robust statistic. More specifically, we propose a novel gossip algorithm for rank estimation, referred to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed description of the proposed methods, a key contribution of our work is a precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank estimation and an $\mathcal{O}(\log(t)/t)$ rate for trimmed mean estimation, where by $t$ is meant the number of iterations. Moreover, we provide a breakdown point analysis of \textsc{GoTrim}. We empirically validate our theoretical results through experiments on diverse network topologies, data distributions and contamination schemes.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuum Transformers Perform In-Context Learning by Operator Gradient Descent</title>
<link>https://arxiv.org/abs/2505.17838</link>
<guid>https://arxiv.org/abs/2505.17838</guid>
<content:encoded><![CDATA[

arXiv:2505.17838v1 Announce Type: cross 
Abstract: Transformers robustly exhibit the ability to perform in-context learning, whereby their predictive accuracy on a task can increase not by parameter updates but merely with the placement of training samples in their context windows. Recent works have shown that transformers achieve this by implementing gradient descent in their forward passes. Such results, however, are restricted to standard transformer architectures, which handle finite-dimensional inputs. In the space of PDE surrogate modeling, a generalization of transformers to handle infinite-dimensional function inputs, known as "continuum transformers," has been proposed and similarly observed to exhibit in-context learning. Despite impressive empirical performance, such in-context learning has yet to be theoretically characterized. We herein demonstrate that continuum transformers perform in-context operator learning by performing gradient descent in an operator RKHS. We demonstrate this using novel proof strategies that leverage a generalized representer theorem for Hilbert spaces and gradient flows over the space of functionals of a Hilbert space. We additionally show the operator learned in context is the Bayes Optimal Predictor in the infinite depth limit of the transformer. We then provide empirical validations of this optimality result and demonstrate that the parameters under which such gradient descent is performed are recovered through the continuum transformer training.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Person Interaction Generation from Two-Person Motion Priors</title>
<link>https://arxiv.org/abs/2505.17860</link>
<guid>https://arxiv.org/abs/2505.17860</guid>
<content:encoded><![CDATA[

arXiv:2505.17860v1 Announce Type: cross 
Abstract: Generating realistic human motion with high-level controls is a crucial task for social understanding, robotics, and animation. With high-quality MOCAP data becoming more available recently, a wide range of data-driven approaches have been presented. However, modelling multi-person interactions still remains a less explored area. In this paper, we present Graph-driven Interaction Sampling, a method that can generate realistic and diverse multi-person interactions by leveraging existing two-person motion diffusion models as motion priors. Instead of training a new model specific to multi-person interaction synthesis, our key insight is to spatially and temporally separate complex multi-person interactions into a graph structure of two-person interactions, which we name the Pairwise Interaction Graph. We thus decompose the generation task into simultaneous single-person motion generation conditioned on one other's motion. In addition, to reduce artifacts such as interpenetrations of body parts in generated multi-person interactions, we introduce two graph-dependent guidance terms into the diffusion sampling scheme. Unlike previous work, our method can produce various high-quality multi-person interactions without having repetitive individual motions. Extensive experiments demonstrate that our approach consistently outperforms existing methods in reducing artifacts when generating a wide range of two-person and multi-person interactions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Optimal ANC: Establishing Mutual Information Lower Bound</title>
<link>https://arxiv.org/abs/2505.17877</link>
<guid>https://arxiv.org/abs/2505.17877</guid>
<content:encoded><![CDATA[

arXiv:2505.17877v1 Announce Type: cross 
Abstract: Active Noise Cancellation (ANC) algorithms aim to suppress unwanted acoustic disturbances by generating anti-noise signals that destructively interfere with the original noise in real time. Although recent deep learning-based ANC algorithms have set new performance benchmarks, there remains a shortage of theoretical limits to rigorously assess their improvements. To address this, we derive a unified lower bound on cancellation performance composed of two components. The first component is information-theoretic: it links residual error power to the fraction of disturbance entropy captured by the anti-noise signal, thereby quantifying limits imposed by information-processing capacity. The second component is support-based: it measures the irreducible error arising in frequency bands that the cancellation path cannot address, reflecting fundamental physical constraints. By taking the maximum of these two terms, our bound establishes a theoretical ceiling on the Normalized Mean Squared Error (NMSE) attainable by any ANC algorithm. We validate its tightness empirically on the NOISEX dataset under varying reverberation times, demonstrating robustness across diverse acoustic conditions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization</title>
<link>https://arxiv.org/abs/2505.17881</link>
<guid>https://arxiv.org/abs/2505.17881</guid>
<content:encoded><![CDATA[

arXiv:2505.17881v1 Announce Type: cross 
Abstract: In recent years, tensor decomposition-based approaches for hyperspectral anomaly detection (HAD) have gained significant attention in the field of remote sensing. However, existing methods often fail to fully leverage both the global correlations and local smoothness of the background components in hyperspectral images (HSIs), which exist in both the spectral and spatial domains. This limitation results in suboptimal detection performance. To mitigate this critical issue, we put forward a novel HAD method named HAD-EUNTRFR, which incorporates an enhanced unified nonconvex tensor ring (TR) factors regularization. In the HAD-EUNTRFR framework, the raw HSIs are first decomposed into background and anomaly components. The TR decomposition is then employed to capture the spatial-spectral correlations within the background component. Additionally, we introduce a unified and efficient nonconvex regularizer, induced by tensor singular value decomposition (TSVD), to simultaneously encode the low-rankness and sparsity of the 3-D gradient TR factors into a unique concise form. The above characterization scheme enables the interpretable gradient TR factors to inherit the low-rankness and smoothness of the original background. To further enhance anomaly detection, we design a generalized nonconvex regularization term to exploit the group sparsity of the anomaly component. To solve the resulting doubly nonconvex model, we develop a highly efficient optimization algorithm based on the alternating direction method of multipliers (ADMM) framework. Experimental results on several benchmark datasets demonstrate that our proposed method outperforms existing state-of-the-art (SOTA) approaches in terms of detection accuracy.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DataRater: Meta-Learned Dataset Curation</title>
<link>https://arxiv.org/abs/2505.17895</link>
<guid>https://arxiv.org/abs/2505.17895</guid>
<content:encoded><![CDATA[

arXiv:2505.17895v1 Announce Type: cross 
Abstract: The quality of foundation models depends heavily on their training data. Consequently, great efforts have been put into dataset curation. Yet most approaches rely on manual tuning of coarse-grained mixtures of large buckets of data, or filtering by hand-crafted heuristics. An approach that is ultimately more scalable (let alone more satisfying) is to \emph{learn} which data is actually valuable for training. This type of meta-learning could allow more sophisticated, fine-grained, and effective curation. Our proposed \emph{DataRater} is an instance of this idea. It estimates the value of training on any particular data point. This is done by meta-learning using `meta-gradients', with the objective of improving training efficiency on held out data. In extensive experiments across a range of model scales and datasets, we find that using our DataRater to filter data is highly effective, resulting in significantly improved compute efficiency.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Function Forms of Simple ReLU Networks with Random Hidden Weights</title>
<link>https://arxiv.org/abs/2505.17907</link>
<guid>https://arxiv.org/abs/2505.17907</guid>
<content:encoded><![CDATA[

arXiv:2505.17907v1 Announce Type: cross 
Abstract: We investigate the function space dynamics of a two-layer ReLU neural network in the infinite-width limit, highlighting the Fisher information matrix (FIM)'s role in steering learning. Extending seminal works on approximate eigendecomposition of the FIM, we derive the asymptotic behavior of basis functions ($f_v(x) = X^{\top} v $) for four groups of approximate eigenvectors, showing their convergence to distinct function forms. These functions, prioritized by gradient descent, exhibit FIM-induced inner products that approximate orthogonality in the function space, forging a novel connection between parameter and function spaces. Simulations validate the accuracy of these theoretical approximations, confirming their practical relevance. By refining the function space inner product's role, we advance the theoretical framework for ReLU networks, illuminating their optimization and expressivity. Overall, this work offers a robust foundation for understanding wide neural networks and enhances insights into scalable deep learning architectures, paving the way for improved design and analysis of neural networks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible MOF Generation with Torsion-Aware Flow Matching</title>
<link>https://arxiv.org/abs/2505.17914</link>
<guid>https://arxiv.org/abs/2505.17914</guid>
<content:encoded><![CDATA[

arXiv:2505.17914v1 Announce Type: cross 
Abstract: Designing metal-organic frameworks (MOFs) with novel chemistries is a long-standing challenge due to their large combinatorial space and the complex 3D arrangements of building blocks. While recent deep generative models have enabled scalable MOF generation, they assume (1) a fixed set of building blocks and (2) known ground-truth local block-wise 3D coordinates. However, this limits their ability to (1) design novel MOFs and (2) generate the structure using novel building blocks. We propose a two-stage de novo MOF generation framework that overcomes these limitations by modeling both chemical and geometric degrees of freedom. First, we train a SMILES-based autoregressive model to generate novel metal and organic building blocks, paired with cheminformatics for 3D structure initialization. Second, we introduce a flow-matching model that predicts translations, rotations, and torsional angles to assemble flexible blocks into valid 3D frameworks. Our experiments demonstrate improved reconstruction accuracy, the generation of valid, novel, and unique MOFs, and the ability of our model to create novel building blocks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M-learner:A Flexible And Powerful Framework To Study Heterogeneous Treatment Effect In Mediation Model</title>
<link>https://arxiv.org/abs/2505.17917</link>
<guid>https://arxiv.org/abs/2505.17917</guid>
<content:encoded><![CDATA[

arXiv:2505.17917v1 Announce Type: cross 
Abstract: We propose a novel method, termed the M-learner, for estimating heterogeneous indirect and total treatment effects and identifying relevant subgroups within a mediation framework. The procedure comprises four key steps. First, we compute individual-level conditional average indirect/total treatment effect Second, we construct a distance matrix based on pairwise differences. Third, we apply tSNE to project this matrix into a low-dimensional Euclidean space, followed by K-means clustering to identify subgroup structures. Finally, we calibrate and refine the clusters using a threshold-based procedure to determine the optimal configuration. To the best of our knowledge, this is the first approach specifically designed to capture treatment effect heterogeneity in the presence of mediation. Experimental results validate the robustness and effectiveness of the proposed framework. Application to the real-world Jobs II dataset highlights the broad adaptability and potential applicability of our method.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy</title>
<link>https://arxiv.org/abs/2505.17921</link>
<guid>https://arxiv.org/abs/2505.17921</guid>
<content:encoded><![CDATA[

arXiv:2505.17921v1 Announce Type: cross 
Abstract: Determining the type of kidney stones is crucial for prescribing appropriate treatments to prevent recurrence. Currently, various approaches exist to identify the type of kidney stones. However, obtaining results through the reference ex vivo identification procedure can take several weeks, while in vivo visual recognition requires highly trained specialists. For this reason, deep learning models have been developed to provide urologists with an automated classification of kidney stones during ureteroscopies. Nevertheless, a common issue with these models is the lack of training data. This contribution presents a deep learning method based on few-shot learning, aimed at producing sufficiently discriminative features for identifying kidney stone types in endoscopic images, even with a very limited number of samples. This approach was specifically designed for scenarios where endoscopic images are scarce or where uncommon classes are present, enabling classification even with a limited training dataset. The results demonstrate that Prototypical Networks, using up to 25% of the training data, can achieve performance equal to or better than traditional deep learning models trained with the complete dataset.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Practical Defect-Focused Automated Code Review</title>
<link>https://arxiv.org/abs/2505.17928</link>
<guid>https://arxiv.org/abs/2505.17928</guid>
<content:encoded><![CDATA[

arXiv:2505.17928v1 Announce Type: cross 
Abstract: The complexity of code reviews has driven efforts to automate review comments, but prior approaches oversimplify this task by treating it as snippet-level code-to-text generation and relying on text similarity metrics like BLEU for evaluation. These methods overlook repository context, real-world merge request evaluation, and defect detection, limiting their practicality. To address these issues, we explore the full automation pipeline within the online recommendation service of a company with nearly 400 million daily active users, analyzing industry-grade C++ codebases comprising hundreds of thousands of lines of code. We identify four key challenges: 1) capturing relevant context, 2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and 4) integrating human workflows. To tackle these, we propose 1) code slicing algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a filtering mechanism for FAR reduction, and 4) a novel prompt design for better human interaction. Our approach, validated on real-world merge requests from historical fault reports, achieves a 2x improvement over standard LLMs and a 10x gain over previous baselines. While the presented results focus on C++, the underlying framework design leverages language-agnostic principles (e.g., AST-based analysis), suggesting potential for broader applicability.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models</title>
<link>https://arxiv.org/abs/2505.17931</link>
<guid>https://arxiv.org/abs/2505.17931</guid>
<content:encoded><![CDATA[

arXiv:2505.17931v1 Announce Type: cross 
Abstract: Medical image segmentation is vital for clinical diagnosis, yet current deep learning methods often demand extensive expert effort, i.e., either through annotating large training datasets or providing prompts at inference time for each new case. This paper introduces a zero-shot and automatic segmentation pipeline that combines off-the-shelf vision-language and segmentation foundation models. Given a medical image and a task definition (e.g., "segment the optic disc in an eye fundus image"), our method uses a grounding model to generate an initial bounding box, followed by a visual prompt boosting module that enhance the prompts, which are then processed by a promptable segmentation model to produce the final mask. To address the challenges of domain gap and result verification, we introduce a test-time adaptation framework featuring a set of learnable adaptors that align the medical inputs with foundation model representations. Its hyperparameters are optimized via Bayesian Optimization, guided by a proxy validation model without requiring ground-truth labels. Our pipeline offers an annotation-efficient and scalable solution for zero-shot medical image segmentation across diverse tasks. Our pipeline is evaluated on seven diverse medical imaging datasets and shows promising results. By proper decomposition and test-time adaptation, our fully automatic pipeline performs competitively with weakly-prompted interactive foundation models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selection Mechanisms for Sequence Modeling using Linear State Space Models</title>
<link>https://arxiv.org/abs/2505.17932</link>
<guid>https://arxiv.org/abs/2505.17932</guid>
<content:encoded><![CDATA[

arXiv:2505.17932v1 Announce Type: cross 
Abstract: Recent advancements in language modeling tasks have been driven by architectures such as Transformers and, more recently, by Selective State Space Models (SSMs). In this paper, we introduce an alternative selection mechanism inspired by control theory methodologies. Specifically, we propose a novel residual generator for selection, drawing an analogy to fault detection strategies in Linear Time-Invariant (LTI) systems. Unlike Mamba, which utilizes Linear Time-Varying (LTV) systems, our approach combines multiple LTI systems, preserving their beneficial properties during training while achieving comparable selectivity. To evaluate the effectiveness of the proposed architecture, we test its performance on synthetic tasks. While these tasks are not inherently critical, they serve as benchmarks to test the selectivity properties of different cores architecture. This work highlights the potential of integrating theoretical insights with experimental advancements, offering a complementary perspective to deep learning innovations at the intersection of control theory and machine learning.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMask: Learn to Solve Constrained Routing Problems with Lazy Masking</title>
<link>https://arxiv.org/abs/2505.17938</link>
<guid>https://arxiv.org/abs/2505.17938</guid>
<content:encoded><![CDATA[

arXiv:2505.17938v1 Announce Type: cross 
Abstract: Routing problems are canonical combinatorial optimization tasks with wide-ranging applications in logistics, transportation, and supply chain management. However, solving these problems becomes significantly more challenging when complex constraints are involved. In this paper, we propose LMask, a novel learning framework that utilizes dynamic masking to generate high-quality feasible solutions for constrained routing problems. LMask introduces the LazyMask decoding method, which lazily refines feasibility masks with the backtracking mechanism. In addition, it employs the refinement intensity embedding to encode the search trace into the model, mitigating representation ambiguities induced by backtracking. To further reduce sampling cost, LMask sets a backtracking budget during decoding, while constraint violations are penalized in the loss function during training to counteract infeasibility caused by this budget. We provide theoretical guarantees for the validity and probabilistic optimality of our approach. Extensive experiments on the traveling salesman problem with time windows (TSPTW) and TSP with draft limits (TSPDL) demonstrate that LMask achieves state-of-the-art feasibility rates and solution quality, outperforming existing neural methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks</title>
<link>https://arxiv.org/abs/2505.17958</link>
<guid>https://arxiv.org/abs/2505.17958</guid>
<content:encoded><![CDATA[

arXiv:2505.17958v1 Announce Type: cross 
Abstract: We study the high-dimensional asymptotics of empirical risk minimization (ERM) in over-parametrized two-layer neural networks with quadratic activations trained on synthetic data. We derive sharp asymptotics for both training and test errors by mapping the $\ell_2$-regularized learning problem to a convex matrix sensing task with nuclear norm penalization. This reveals that capacity control in such networks emerges from a low-rank structure in the learned feature maps. Our results characterize the global minima of the loss and yield precise generalization thresholds, showing how the width of the target function governs learnability. This analysis bridges and extends ideas from spin-glass methods, matrix factorization, and convex optimization and emphasizes the deep link between low-rank matrix sensing and learning in quadratic neural networks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Domain Gap: Measuring the Domain Gap Between Real-World and Synthetic Point Clouds for Automated Driving Development</title>
<link>https://arxiv.org/abs/2505.17959</link>
<guid>https://arxiv.org/abs/2505.17959</guid>
<content:encoded><![CDATA[

arXiv:2505.17959v1 Announce Type: cross 
Abstract: Owing to the typical long-tail data distribution issues, simulating domain-gap-free synthetic data is crucial in robotics, photogrammetry, and computer vision research. The fundamental challenge pertains to credibly measuring the difference between real and simulated data. Such a measure is vital for safety-critical applications, such as automated driving, where out-of-domain samples may impact a car's perception and cause fatal accidents. Previous work has commonly focused on simulating data on one scene and analyzing performance on a different, real-world scene, hampering the disjoint analysis of domain gap coming from networks' deficiencies, class definitions, and object representation. In this paper, we propose a novel approach to measuring the domain gap between the real world sensor observations and simulated data representing the same location, enabling comprehensive domain gap analysis. To measure such a domain gap, we introduce a novel metric DoGSS-PCL and evaluation assessing the geometric and semantic quality of the simulated point cloud. Our experiments corroborate that the introduced approach can be used to measure the domain gap. The tests also reveal that synthetic semantic point clouds may be used for training deep neural networks, maintaining the performance at the 50/50 real-to-synthetic ratio. We strongly believe that this work will facilitate research on credible data simulation and allow for at-scale deployment in automated driving testing and digital twinning.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Tight Bounds for SGD without Variance Assumption: A Computer-Aided Lyapunov Analysis</title>
<link>https://arxiv.org/abs/2505.17965</link>
<guid>https://arxiv.org/abs/2505.17965</guid>
<content:encoded><![CDATA[

arXiv:2505.17965v1 Announce Type: cross 
Abstract: The analysis of Stochastic Gradient Descent (SGD) often relies on making some assumption on the variance of the stochastic gradients, which is usually not satisfied or difficult to verify in practice. This paper contributes to a recent line of works which attempt to provide guarantees without making any variance assumption, leveraging only the (strong) convexity and smoothness of the loss functions. In this context, we prove new theoretical bounds derived from the monotonicity of a simple Lyapunov energy, improving the current state-of-the-art and extending their validity to larger step-sizes. Our theoretical analysis is backed by a Performance Estimation Problem analysis, which allows us to claim that, empirically, the bias term in our bounds is tight within our framework.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings</title>
<link>https://arxiv.org/abs/2505.17972</link>
<guid>https://arxiv.org/abs/2505.17972</guid>
<content:encoded><![CDATA[

arXiv:2505.17972v1 Announce Type: cross 
Abstract: Feature engineering for generalized seizure detection models remains a significant challenge. Recently proposed models show variable performance depending on the training data and remain ineffective at accurately distinguishing artifacts from seizure data. In this study, we propose a novel end-to-end model, ''Multiresolutional EEGWaveNet (MR-EEGWaveNet),'' which efficiently distinguishes seizure events from background electroencephalogram (EEG) and artifacts/noise by capturing both temporal dependencies across different time frames and spatial relationships between channels. The model has three modules: convolution, feature extraction, and predictor. The convolution module extracts features through depth-wise and spatio-temporal convolution. The feature extraction module individually reduces the feature dimension extracted from EEG segments and their sub-segments. Subsequently, the extracted features are concatenated into a single vector for classification using a fully connected classifier called the predictor module. In addition, an anomaly score-based post-classification processing technique was introduced to reduce the false-positive rates of the model. Experimental results were reported and analyzed using different parameter settings and datasets (Siena (public) and Juntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the conventional non-multiresolution approach, improving the F1 scores from 0.177 to 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9% and 20.62%, respectively.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models</title>
<link>https://arxiv.org/abs/2505.17973</link>
<guid>https://arxiv.org/abs/2505.17973</guid>
<content:encoded><![CDATA[

arXiv:2505.17973v1 Announce Type: cross 
Abstract: Feature matching is a necessary step for many computer vision and photogrammetry applications such as image registration, structure-from-motion, and visual localization. Classical handcrafted methods such as SIFT feature detection and description combined with nearest neighbour matching and RANSAC outlier removal have been state-of-the-art for mobile mapping cameras. With recent advances in deep learning, learnable methods have been introduced and proven to have better robustness and performance under complex conditions. Despite their growing adoption, a comprehensive comparison between classical and learnable feature matching methods for the specific task of semantic 3D building camera-to-model matching is still missing. This submission systematically evaluates the effectiveness of different feature-matching techniques in visual localization using textured CityGML LoD2 models. We use standard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets consisting of facade textures and corresponding camera images (terrestrial and drone). For the latter, we evaluate the achievable accuracy of the absolute pose estimated using a Perspective-n-Point (PnP) algorithm, with geometric ground truth derived from geo-referenced trajectory data. The results indicate that the learnable feature matching methods vastly outperform traditional approaches regarding accuracy and robustness on our challenging custom datasets with zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We believe that this work will foster the development of model-based visual localization methods. Link to the code: https://github.com/simBauer/To\_Glue\_or\_not\_to\_Glue
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Feature Interactions from the Perspective of Quadratic Neural Networks for Click-through Rate Prediction</title>
<link>https://arxiv.org/abs/2505.17999</link>
<guid>https://arxiv.org/abs/2505.17999</guid>
<content:encoded><![CDATA[

arXiv:2505.17999v1 Announce Type: cross 
Abstract: Hadamard Product (HP) has long been a cornerstone in click-through rate (CTR) prediction tasks due to its simplicity, effectiveness, and ability to capture feature interactions without additional parameters. However, the underlying reasons for its effectiveness remain unclear. In this paper, we revisit HP from the perspective of Quadratic Neural Networks (QNN), which leverage quadratic interaction terms to model complex feature relationships. We further reveal QNN's ability to expand the feature space and provide smooth nonlinear approximations without relying on activation functions. Meanwhile, we find that traditional post-activation does not further improve the performance of the QNN. Instead, mid-activation is a more suitable alternative. Through theoretical analysis and empirical evaluation of 25 QNN neuron formats, we identify a good-performing variant and make further enhancements on it. Specifically, we propose the Multi-Head Khatri-Rao Product as a superior alternative to HP and a Self-Ensemble Loss with dynamic ensemble capability within the same network to enhance computational efficiency and performance. Ultimately, we propose a novel neuron format, QNN-alpha, which is tailored for CTR prediction tasks. Experimental results show that QNN-alpha achieves new state-of-the-art performance on six public datasets while maintaining low inference latency, good scalability, and excellent compatibility. The code, running logs, and detailed hyperparameter configurations are available at: https://github.com/salmon1802/QNN.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anytime-valid, Bayes-assisted,Prediction-Powered Inference</title>
<link>https://arxiv.org/abs/2505.18000</link>
<guid>https://arxiv.org/abs/2505.18000</guid>
<content:encoded><![CDATA[

arXiv:2505.18000v1 Announce Type: cross 
Abstract: Given a large pool of unlabelled data and a smaller amount of labels, prediction-powered inference (PPI) leverages machine learning predictions to increase the statistical efficiency of standard confidence interval procedures based solely on labelled data, while preserving their fixed-time validity.
  In this paper, we extend the PPI framework to the sequential setting, where labelled and unlabelled datasets grow over time.
  Exploiting Ville's inequality and the method of mixtures, we propose prediction-powered confidence sequence procedures that are valid uniformly over time and naturally accommodate prior knowledge on the quality of the predictions to further boost efficiency.
  We carefully illustrate the design choices behind our method and demonstrate its effectiveness in real and synthetic examples.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Operator Neural Network Model Predictive Control</title>
<link>https://arxiv.org/abs/2505.18008</link>
<guid>https://arxiv.org/abs/2505.18008</guid>
<content:encoded><![CDATA[

arXiv:2505.18008v1 Announce Type: cross 
Abstract: In this paper, we consider the design of model predictive control (MPC) algorithms based on deep operator neural networks (DeepONets). These neural networks are capable of accurately approximating real and complex valued solutions of continuous time nonlinear systems without relying on recurrent architectures. The DeepONet architecture is made up of two feedforward neural networks: the branch network, which encodes the input function space, and the trunk network, which represents dependencies on temporal variables or initial conditions. Utilizing the original DeepONet architecture as a predictor within MPC for Multi Input Multi Output (MIMO) systems requires multiple branch networks, to generate multi output predictions, one for each input. Moreover, to predict multiple time steps into the future, the network has to be evaluated multiple times. Motivated by this, we introduce a multi step DeepONet (MS-DeepONet) architecture that computes in one shot multi step predictions of system outputs from multi step input sequences, which is better suited for MPC. We prove that the MS DeepONet is a universal approximator in terms of multi step sequence prediction. Additionally, we develop automated hyper parameter selection strategies and implement MPC frameworks using both the standard DeepONet and the proposed MS DeepONet architectures in PyTorch. The implementation is publicly available on GitHub. Simulation results demonstrate that MS-DeepONet consistently outperforms the standard DeepONet in learning and predictive control tasks across several nonlinear benchmark systems: the van der Pol oscillator, the quadruple tank process, and a cart pendulum unstable system, where it successfully learns and executes multiple swing up and stabilization policies.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemSegBench &amp; DetecBench: Benchmarking Reliability and Generalization Beyond Classification</title>
<link>https://arxiv.org/abs/2505.18015</link>
<guid>https://arxiv.org/abs/2505.18015</guid>
<content:encoded><![CDATA[

arXiv:2505.18015v1 Announce Type: cross 
Abstract: Reliability and generalization in deep learning are predominantly studied in the context of image classification. Yet, real-world applications in safety-critical domains involve a broader set of semantic tasks, such as semantic segmentation and object detection, which come with a diverse set of dedicated model architectures. To facilitate research towards robust model design in segmentation and detection, our primary objective is to provide benchmarking tools regarding robustness to distribution shifts and adversarial manipulations. We propose the benchmarking tools SEMSEGBENCH and DETECBENCH, along with the most extensive evaluation to date on the reliability and generalization of semantic segmentation and object detection models. In particular, we benchmark 76 segmentation models across four datasets and 61 object detectors across two datasets, evaluating their performance under diverse adversarial attacks and common corruptions. Our findings reveal systematic weaknesses in state-of-the-art models and uncover key trends based on architecture, backbone, and model capacity. SEMSEGBENCH and DETECBENCH are open-sourced in our GitHub repository (https://github.com/shashankskagnihotri/benchmarking_reliability_generalization) along with our complete set of total 6139 evaluations. We anticipate the collected data to foster and encourage future research towards improved model reliability beyond classification.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automata Learning of Preferences over Temporal Logic Formulas from Pairwise Comparisons</title>
<link>https://arxiv.org/abs/2505.18030</link>
<guid>https://arxiv.org/abs/2505.18030</guid>
<content:encoded><![CDATA[

arXiv:2505.18030v1 Announce Type: cross 
Abstract: Many preference elicitation algorithms consider preference over propositional logic formulas or items with different attributes. In sequential decision making, a user's preference can be a preorder over possible outcomes, each of which is a temporal sequence of events. This paper considers a class of preference inference problems where the user's unknown preference is represented by a preorder over regular languages (sets of temporal sequences), referred to as temporal goals. Given a finite set of pairwise comparisons between finite words, the objective is to learn both the set of temporal goals and the preorder over these goals. We first show that a preference relation over temporal goals can be modeled by a Preference Deterministic Finite Automaton (PDFA), which is a deterministic finite automaton augmented with a preorder over acceptance conditions. The problem of preference inference reduces to learning the PDFA. This problem is shown to be computationally challenging, with the problem of determining whether there exists a PDFA of size smaller than a given integer $k$, consistent with the sample, being NP-Complete. We formalize the properties of characteristic samples and develop an algorithm that guarantees to learn, given a characteristic sample, the minimal PDFA equivalent to the true PDFA from which the sample is drawn. We present the method through a running example and provide detailed analysis using a robotic motion planning problem.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Uncertainty Aware Task Delegation and Human-AI Collaborative Decision-Making</title>
<link>https://arxiv.org/abs/2505.18066</link>
<guid>https://arxiv.org/abs/2505.18066</guid>
<content:encoded><![CDATA[

arXiv:2505.18066v1 Announce Type: cross 
Abstract: Despite the growing promise of artificial intelligence (AI) in supporting decision-making across domains, fostering appropriate human reliance on AI remains a critical challenge. In this paper, we investigate the utility of exploring distance-based uncertainty scores for task delegation to AI and describe how these scores can be visualized through embedding representations for human-AI decision-making. After developing an AI-based system for physical stroke rehabilitation assessment, we conducted a study with 19 health professionals and 10 students in medicine/health to understand the effect of exploring distance-based uncertainty scores on users' reliance on AI. Our findings showed that distance-based uncertainty scores outperformed traditional probability-based uncertainty scores in identifying uncertain cases. In addition, after exploring confidence scores for task delegation and reviewing embedding-based visualizations of distance-based uncertainty scores, participants achieved an 8.20% higher rate of correct decisions, a 7.15% higher rate of changing their decisions to correct ones, and a 7.14% lower rate of incorrect changes after reviewing AI outputs than those reviewing probability-based uncertainty scores ($p<0.01$). Our findings highlight the potential of distance-based uncertainty scores to enhance decision accuracy and appropriate reliance on AI while discussing ongoing challenges for human-AI collaborative decision-making.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Deep Learning for Discrete Choice</title>
<link>https://arxiv.org/abs/2505.18077</link>
<guid>https://arxiv.org/abs/2505.18077</guid>
<content:encoded><![CDATA[

arXiv:2505.18077v1 Announce Type: cross 
Abstract: Discrete choice models (DCMs) are used to analyze individual decision-making in contexts such as transportation choices, political elections, and consumer preferences. DCMs play a central role in applied econometrics by enabling inference on key economic variables, such as marginal rates of substitution, rather than focusing solely on predicting choices on new unlabeled data. However, while traditional DCMs offer high interpretability and support for point and interval estimation of economic quantities, these models often underperform in predictive tasks compared to deep learning (DL) models. Despite their predictive advantages, DL models remain largely underutilized in discrete choice due to concerns about their lack of interpretability, unstable parameter estimates, and the absence of established methods for uncertainty quantification. Here, we introduce a deep learning model architecture specifically designed to integrate with approximate Bayesian inference methods, such as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model collapses to behaviorally informed hypotheses when data is limited, mitigating overfitting and instability in underspecified settings while retaining the flexibility to capture complex nonlinear relationships when sufficient data is available. We demonstrate our approach using SGLD through a Monte Carlo simulation study, evaluating both predictive metrics--such as out-of-sample balanced accuracy--and inferential metrics--such as empirical coverage for marginal rates of substitution interval estimates. Additionally, we present results from two empirical case studies: one using revealed mode choice data in NYC, and the other based on the widely used Swiss train choice stated preference data.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable Reinforcement Learning for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2505.18086</link>
<guid>https://arxiv.org/abs/2505.18086</guid>
<content:encoded><![CDATA[

arXiv:2505.18086v1 Announce Type: cross 
Abstract: The success of Deepseek-R1 has drawn the LLM community's attention to reinforcement learning (RL) methods like GRPO. However, such rule-based 0/1 outcome reward methods lack the capability to regulate the intermediate reasoning processes during chain-of-thought (CoT) generation, leading to severe overthinking phenomena. In response, recent studies have designed reward functions to reinforce models' behaviors in producing shorter yet correct completions. Nevertheless, we observe that these length-penalty reward functions exacerbate RL training instability: as the completion length decreases, model accuracy abruptly collapses, often occurring early in training. To address this issue, we propose a simple yet effective solution GRPO-$\lambda$, an efficient and stabilized variant of GRPO, which dynamically adjusts the reward strategy by monitoring the correctness ratio among completions within each query-sampled group. A low correctness ratio indicates the need to avoid length penalty that compromises CoT quality, triggering a switch to length-agnostic 0/1 rewards that prioritize reasoning capability. A high ratio maintains length penalties to boost efficiency. Experimental results show that our approach avoids training instability caused by length penalty while maintaining the optimal accuracy-efficiency trade-off. On the GSM8K, GPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average accuracy by 1.48% while reducing CoT sequence length by 47.3%.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles</title>
<link>https://arxiv.org/abs/2505.18106</link>
<guid>https://arxiv.org/abs/2505.18106</guid>
<content:encoded><![CDATA[

arXiv:2505.18106v1 Announce Type: cross 
Abstract: Nanomaterial research is becoming a vital area for energy, medicine, and materials science, and accurate analysis of the nanoparticle topology is essential to determine their properties. Unfortunately, the lack of high-quality annotated datasets drastically hinders the creation of strong segmentation models for nanoscale imaging. To alleviate this problem, we introduce F-ANcGAN, an attention-enhanced cycle consistent generative adversarial system that can be trained using a limited number of data samples and generates realistic scanning electron microscopy (SEM) images directly from segmentation maps. Our model uses a Style U-Net generator and a U-Net segmentation network equipped with self-attention to capture structural relationships and applies augmentation methods to increase the variety of the dataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset generation, with a further reduction in FID score to nearly 10.39 by using efficient post-processing techniques. By facilitating scalable high-fidelity synthetic dataset generation, our approach can improve the effectiveness of downstream segmentation task training, overcoming severe data shortage issues in nanoparticle analysis, thus extending its applications to resource-limited fields.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Policy Maximization Under Network Interference</title>
<link>https://arxiv.org/abs/2505.18118</link>
<guid>https://arxiv.org/abs/2505.18118</guid>
<content:encoded><![CDATA[

arXiv:2505.18118v1 Announce Type: cross 
Abstract: Many interventions, such as vaccines in clinical trials or coupons in online marketplaces, must be assigned sequentially without full knowledge of their effects. Multi-armed bandit algorithms have proven successful in such settings. However, standard independence assumptions fail when the treatment status of one individual impacts the outcomes of others, a phenomenon known as interference. We study optimal-policy learning under interference on a dynamic network. Existing approaches to this problem require repeated observations of the same fixed network and struggle to scale in sample size beyond as few as fifteen connected units -- both limit applications. We show that under common assumptions on the structure of interference, rewards become linear. This enables us to develop a scalable Thompson sampling algorithm that maximizes policy impact when a new $n$-node network is observed each round. We prove a Bayesian regret bound that is sublinear in $n$ and the number of rounds. Simulation experiments show that our algorithm learns quickly and outperforms existing methods. The results close a key scalability gap between causal inference methods for interference and practical bandit algorithms, enabling policy optimization in large-scale networked systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProgRM: Build Better GUI Agents with Progress Rewards</title>
<link>https://arxiv.org/abs/2505.18121</link>
<guid>https://arxiv.org/abs/2505.18121</guid>
<content:encoded><![CDATA[

arXiv:2505.18121v1 Announce Type: cross 
Abstract: LLM-based (Large Language Model) GUI (Graphical User Interface) agents can potentially reshape our daily lives significantly. However, current LLM-based GUI agents suffer from the scarcity of high-quality training data owing to the difficulties of trajectory collection and reward annotation. Existing works have been exploring LLMs to collect trajectories for imitation learning or to offer reward signals for online RL training. However, the Outcome Reward Model (ORM) used in existing works cannot provide finegrained feedback and can over-penalize the valuable steps in finally failed trajectories. To this end, we propose Progress Reward Model (ProgRM) to provide dense informative intermediate rewards by predicting a task completion progress for each step in online training. To handle the challenge of progress reward label annotation, we further design an efficient LCS-based (Longest Common Subsequence) self-annotation algorithm to discover the key steps in trajectories and assign progress labels accordingly. ProgRM is evaluated with extensive experiments and analyses. Actors trained with ProgRM outperform leading proprietary LLMs and ORM-trained actors, illustrating the effectiveness of ProgRM. The codes for experiments will be made publicly available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaming Tool Preferences in Agentic LLMs</title>
<link>https://arxiv.org/abs/2505.18135</link>
<guid>https://arxiv.org/abs/2505.18135</guid>
<content:encoded><![CDATA[

arXiv:2505.18135v1 Announce Type: cross 
Abstract: Large language models (LLMs) can now access a wide range of external tools, thanks to the Model Context Protocol (MCP). This greatly expands their abilities as various agents. However, LLMs rely entirely on the text descriptions of tools to decide which ones to use--a process that is surprisingly fragile. In this work, we expose a vulnerability in prevalent tool/function-calling protocols by investigating a series of edits to tool descriptions, some of which can drastically increase a tool's usage from LLMs when competing with alternatives. Through controlled experiments, we show that tools with properly edited descriptions receive over 10 times more usage from GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further evaluate how various edits to tool descriptions perform when competing directly with one another and how these trends generalize or differ across a broader set of 10 different models. These phenomenons, while giving developers a powerful way to promote their tools, underscore the need for a more reliable foundation for agentic LLMs to select and utilize tools and resources.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Open Set Recognition Performance through Modulated Representation Learning</title>
<link>https://arxiv.org/abs/2505.18137</link>
<guid>https://arxiv.org/abs/2505.18137</guid>
<content:encoded><![CDATA[

arXiv:2505.18137v1 Announce Type: cross 
Abstract: The open set recognition (OSR) problem aims to identify test samples from novel semantic classes that are not part of the training classes, a task that is crucial in many practical scenarios. However, existing OSR methods use a constant scaling factor (the temperature) to the logits before applying a loss function, which hinders the model from exploring both ends of the spectrum in representation learning -- from instance-level to semantic-level features. In this paper, we address this problem by enabling temperature-modulated representation learning using our novel negative cosine scheduling scheme. Our scheduling lets the model form a coarse decision boundary at the beginning of training by focusing on fewer neighbors, and gradually prioritizes more neighbors to smooth out rough edges. This gradual task switching leads to a richer and more generalizable representation space. While other OSR methods benefit by including regularization or auxiliary negative samples, such as with mix-up, thereby adding a significant computational overhead, our scheme can be folded into any existing OSR method with no overhead. We implement the proposed scheme on top of a number of baselines, using both cross-entropy and contrastive loss functions as well as a few other OSR methods, and find that our scheme boosts both the OSR performance and the closed set performance in most cases, especially on the tougher semantic shift benchmarks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find</title>
<link>https://arxiv.org/abs/2505.18148</link>
<guid>https://arxiv.org/abs/2505.18148</guid>
<content:encoded><![CDATA[

arXiv:2505.18148v1 Announce Type: cross 
Abstract: Large language models (LLMs) face significant challenges with needle-in-a-haystack tasks, where relevant information ("the needle") must be drawn from a large pool of irrelevant context ("the haystack"). Previous studies have highlighted positional bias and distractor quantity as critical factors affecting model performance, yet the influence of gold context size has received little attention. We address this gap by systematically studying how variations in gold context length impact LLM performance on long-context question answering tasks. Our experiments reveal that LLM performance drops sharply when the gold context is shorter, i.e., smaller gold contexts consistently degrade model performance and amplify positional sensitivity, posing a major challenge for agentic systems that must integrate scattered, fine-grained information of varying lengths. This pattern holds across three diverse domains (general knowledge, biomedical reasoning, and mathematical reasoning) and seven state-of-the-art LLMs of various sizes and architectures. Our work provides clear insights to guide the design of robust, context-aware LLM-driven systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architecture Selection via the Trade-off Between Accuracy and Robustness</title>
<link>https://arxiv.org/abs/1906.01354</link>
<guid>https://arxiv.org/abs/1906.01354</guid>
<content:encoded><![CDATA[

arXiv:1906.01354v2 Announce Type: replace 
Abstract: We provide a general framework for characterizing the trade-off between accuracy and robustness in supervised learning. We propose a method and define quantities to characterize the trade-off between accuracy and robustness for a given architecture, and provide theoretical insight into the trade-off. Specifically we introduce a simple trade-off curve, define and study an influence function that captures the sensitivity, under adversarial attack, of the optima of a given loss function. We further show how adversarial training regularizes the parameters in an over-parameterized linear model, recovering the LASSO and ridge regression as special cases, which also allows us to theoretically analyze the behavior of the trade-off curve. In experiments, we demonstrate the corresponding trade-off curves of neural networks and how they vary with respect to factors such as number of layers, neurons, and across different network structures. Such information provides a useful guideline to architecture selection.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Hyperparameter Tuning with Differential Privacy</title>
<link>https://arxiv.org/abs/2211.01852</link>
<guid>https://arxiv.org/abs/2211.01852</guid>
<content:encoded><![CDATA[

arXiv:2211.01852v3 Announce Type: replace 
Abstract: Hyperparameter tuning is a common practice in the application of machine learning but is a typically ignored aspect in the literature on privacy-preserving machine learning due to its negative effect on the overall privacy parameter. In this paper, we aim to tackle this fundamental yet challenging problem by providing an effective hyperparameter tuning framework with differential privacy. The proposed method allows us to adopt a broader hyperparameter search space and even to perform a grid search over the whole space, since its privacy loss parameter is independent of the number of hyperparameter candidates. Interestingly, it instead correlates with the utility gained from hyperparameter searching, revealing an explicit and mandatory trade-off between privacy and utility. Theoretically, we show that its additional privacy loss bound incurred by hyperparameter tuning is upper-bounded by the squared root of the gained utility. However, we note that the additional privacy loss bound would empirically scale like a squared root of the logarithm of the utility term, benefiting from the design of doubling step.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Multi-task Learning via Seeking Task-based Flat Regions</title>
<link>https://arxiv.org/abs/2211.13723</link>
<guid>https://arxiv.org/abs/2211.13723</guid>
<content:encoded><![CDATA[

arXiv:2211.13723v4 Announce Type: replace 
Abstract: Multi-Task Learning (MTL) is a widely-used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions on real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Assessment Benchmark for Rigorously Evaluating Deep Learning Image Classifiers</title>
<link>https://arxiv.org/abs/2308.04137</link>
<guid>https://arxiv.org/abs/2308.04137</guid>
<content:encoded><![CDATA[

arXiv:2308.04137v3 Announce Type: replace 
Abstract: Reliable and robust evaluation methods are a necessary first step towards developing machine learning models that are themselves robust and reliable. Unfortunately, current evaluation protocols typically used to assess classifiers fail to comprehensively evaluate performance as they tend to rely on limited types of test data, and ignore others. For example, using the standard test data fails to evaluate the predictions made by the classifier to samples from classes it was not trained on. On the other hand, testing with data containing samples from unknown classes fails to evaluate how well the classifier can predict the labels for known classes. This article advocates benchmarking performance using a wide range of different types of data and using a single metric that can be applied to all such data types to produce a consistent evaluation of performance. Using the proposed benchmark it is found that current deep neural networks, including those trained with methods that are believed to produce state-of-the-art robustness, are vulnerable to making mistakes on certain types of data. This means that such models will be unreliable in real-world scenarios where they may encounter data from many different domains, and that they are insecure as they can be easily fooled into making the wrong decisions. It is hoped that these results will motivate the wider adoption of more comprehensive testing methods that will, in turn, lead to the development of more robust machine learning methods in the future.
  Code is available at: https://codeberg.org/mwspratling/RobustnessEvaluation
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding the Generalizability of Delayed Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2308.09430</link>
<guid>https://arxiv.org/abs/2308.09430</guid>
<content:encoded><![CDATA[

arXiv:2308.09430v3 Announce Type: replace 
Abstract: Stochastic gradient descent (SGD) performed in an asynchronous manner plays a crucial role in training large-scale machine learning models. However, the generalization performance of asynchronous delayed SGD, which is an essential metric for assessing machine learning algorithms, has rarely been explored. Existing generalization error bounds are rather pessimistic and cannot reveal the correlation between asynchronous delays and generalization. In this paper, we investigate sharper generalization error bound for SGD with asynchronous delay $\tau$. Leveraging the generating function analysis tool, we first establish the average stability of the delayed gradient algorithm. Based on this algorithmic stability, we provide upper bounds on the generalization error of $\tilde{\mathcal{O}}(\frac{T-\tau}{n\tau})$ and $\tilde{\mathcal{O}}(\frac{1}{n})$ for quadratic convex and strongly convex problems, respectively, where $T$ refers to the iteration number and $n$ is the amount of training data. Our theoretical results indicate that asynchronous delays reduce the generalization error of the delayed SGD algorithm. Analogous analysis can be generalized to the random delay setting, and the experimental results validate our theoretical findings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Linear Speedup with ProxSkip in Distributed Stochastic Optimization</title>
<link>https://arxiv.org/abs/2310.07983</link>
<guid>https://arxiv.org/abs/2310.07983</guid>
<content:encoded><![CDATA[

arXiv:2310.07983v3 Announce Type: replace 
Abstract: The ProxSkip algorithm for distributed optimization is gaining increasing attention due to its proven benefits in accelerating communication complexity while maintaining robustness against data heterogeneity. However, existing analyses of ProxSkip are limited to the strongly convex setting and do not achieve linear speedup, where convergence performance increases linearly with respect to the number of nodes. So far, questions remain open about how ProxSkip behaves in the non-convex setting and whether linear speedup is achievable. In this paper, we revisit distributed ProxSkip and address both questions. We demonstrate that the leading communication complexity of ProxSkip is $\mathcal{O}(\frac{p\sigma^2}{n\epsilon^2})$ for non-convex and convex settings, and $\mathcal{O}(\frac{p\sigma^2}{n\epsilon})$ for the strongly convex setting, where $n$ represents the number of nodes, $p$ denotes the probability of communication, $\sigma^2$ signifies the level of stochastic noise, and $\epsilon$ denotes the desired accuracy level. This result illustrates that ProxSkip achieves linear speedup and can asymptotically reduce communication overhead proportional to the probability of communication. Additionally, for the strongly convex setting, we further prove that ProxSkip can achieve linear speedup with network-independent stepsizes.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compact Matrix Quantum Group Equivariant Neural Networks</title>
<link>https://arxiv.org/abs/2311.06358</link>
<guid>https://arxiv.org/abs/2311.06358</guid>
<content:encoded><![CDATA[

arXiv:2311.06358v2 Announce Type: replace 
Abstract: Group equivariant neural networks have proven effective in modelling a wide range of tasks where the data lives in a classical geometric space and exhibits well-defined group symmetries. However, these networks are not suitable for learning from data that lives in a non-commutative geometry, described formally by non-commutative $C^{*}$-algebras, since the $C^{*}$-algebra of continuous functions on a compact matrix group is commutative. To address this limitation, we derive the existence of a new type of equivariant neural network, called compact matrix quantum group equivariant neural networks, which encode symmetries that are described by compact matrix quantum groups. We characterise the weight matrices that appear in these neural networks for the easy compact matrix quantum groups, which are defined by set partitions. As a result, we obtain new characterisations of equivariant weight matrices for some compact matrix groups that have not appeared previously in the machine learning literature.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On non-approximability of zero loss global ${\mathcal L}^2$ minimizers by gradient descent in Deep Learning</title>
<link>https://arxiv.org/abs/2311.07065</link>
<guid>https://arxiv.org/abs/2311.07065</guid>
<content:encoded><![CDATA[

arXiv:2311.07065v3 Announce Type: replace 
Abstract: We analyze geometric aspects of the gradient descent algorithm in Deep Learning (DL), and give a detailed discussion of the circumstance that in underparametrized DL networks, zero loss minimization can generically not be attained. As a consequence, we conclude that the distribution of training inputs must necessarily be non-generic in order to produce zero loss minimizers, both for the method constructed in [Chen-Munoz Ewald 2023, 2024], or for gradient descent [Chen 2025] (which assume clustering of training data).
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Similarity Function for Spectral Clustering with Application to Plant Phenotypic Data</title>
<link>https://arxiv.org/abs/2312.14920</link>
<guid>https://arxiv.org/abs/2312.14920</guid>
<content:encoded><![CDATA[

arXiv:2312.14920v3 Announce Type: replace 
Abstract: Clustering species of the same plant into different groups is an important step in developing new species of the concerned plant. Phenotypic (or physical) characteristics of plant species are commonly used to perform clustering. Hierarchical Clustering (HC) is popularly used for this task, and this algorithm suffers from low accuracy. In one of the recent works (Shastri et al., 2021), the authors have used the standard Spectral Clustering (SC) algorithm to improve the clustering accuracy. They have demonstrated the efficacy of their algorithm on soybean species.
  In the SC algorithm, one of the crucial steps is building the similarity matrix. A Gaussian similarity function is the standard choice to build this matrix. In the past, many works have proposed variants of the Gaussian similarity function to improve the performance of the SC algorithm, however, all have focused on the variance or scaling of the Gaussian. None of the past works have investigated upon the choice of base "e" (Euler's number) of the Gaussian similarity function (natural exponential function).
  Based upon spectral graph theory, specifically the Cheeger's inequality, in this work we propose use of a base "a" exponential function as the similarity function. We also integrate this new approach with the notion of "local scaling" from one of the first works that experimented with the scaling of the Gaussian similarity function (Zelnik-Manor et al., 2004).
  Using an eigenvalue analysis, we theoretically justify that our proposed algorithm should work better than the existing one. With evaluation on 2376 soybean species and 1865 rice species, we experimentally demonstrate that our new SC is 35% and 11% better than the standard SC, respectively.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Aligned Regression via Pairwise Losses</title>
<link>https://arxiv.org/abs/2402.06104</link>
<guid>https://arxiv.org/abs/2402.06104</guid>
<content:encoded><![CDATA[

arXiv:2402.06104v5 Announce Type: replace 
Abstract: Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample. Recent research endeavors have introduced novel perspectives by incorporating label similarity to regression via imposing extra pairwise regularization on the latent feature space and demonstrated the effectiveness. However, there are two drawbacks for those approaches: i) their pairwise operation in latent feature space is computationally more expensive than conventional regression losses; ii) it lacks of theoretical justifications behind such regularization. In this work, we propose GAR (Gradient Aligned Regression) as a competitive alternative method in label space, which is constituted by a conventional regression loss and two pairwise label difference losses for gradient alignment including magnitude and direction. GAR enjoys: i) the same level efficiency as conventional regression loss because the quadratic complexity for the proposed pairwise losses can be reduced to linear complexity; ii) theoretical insights from learning the pairwise label difference to learning the gradient of the ground truth function. We limit our current scope as regression on the clean data setting without noises, outliers or distributional shifts, etc. We demonstrate the effectiveness of the proposed method practically on two synthetic datasets and on eight extensive real-world tasks from six benchmark datasets with other eight competitive baselines. Running time experiments demonstrate the superior efficiency of the proposed GAR over existing methods with pairwise regularization in latent feature space and ablation studies demonstrate the effectiveness of each component for GAR.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning in Genetics: Extended Analysis of Accuracy, Performance and Privacy Trade-offs</title>
<link>https://arxiv.org/abs/2402.14527</link>
<guid>https://arxiv.org/abs/2402.14527</guid>
<content:encoded><![CDATA[

arXiv:2402.14527v2 Announce Type: replace 
Abstract: Machine learning on large-scale genomic or transcriptomic data is important for many novel health applications. For example, precision medicine tailors medical treatments to patients on the basis of individual biomarkers, cellular and molecular states, etc. However, the data required is sensitive, voluminous, heterogeneous, and typically distributed across locations where dedicated machine learning hardware is not available. Due to privacy and regulatory reasons, it is also problematic to aggregate all data at a trusted third party. Federated learning is a promising solution to this dilemma, because it enables decentralized, collaborative machine learning without exchanging raw data. In this paper, we perform comparative experiments with the federated learning frameworks TensorFlow Federated and Flower. Our test case is the training of disease prognosis and cell type classification models. We train the models with distributed transcriptomic data, considering both data heterogeneity and architectural heterogeneity. We measure model quality, robustness against privacy-enhancing noise and computational performance. We evaluate the resource overhead of a federated system from both client and global perspectives and assess benefits and limitations. Each of the federated learning frameworks has different strengths. However, our experiments confirm that both frameworks can readily build models on transcriptomic data, without transferring personal raw data to a third party with abundant computational resources. This paper is the extended version of https://link.springer.com/chapter/10.1007/978-3-031-63772-8_26.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tackling Decision Processes with Non-Cumulative Objectives using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2405.13609</link>
<guid>https://arxiv.org/abs/2405.13609</guid>
<content:encoded><![CDATA[

arXiv:2405.13609v3 Announce Type: replace 
Abstract: Markov decision processes (MDPs) are used to model a wide variety of applications ranging from game playing over robotics to finance. Their optimal policy typically maximizes the expected sum of rewards given at each step of the decision process. However, a large class of problems does not fit straightforwardly into this framework: Non-cumulative Markov decision processes (NCMDPs), where instead of the expected sum of rewards, the expected value of an arbitrary function of the rewards is maximized. Example functions include the maximum of the rewards or their mean divided by their standard deviation. In this work, we introduce a general mapping of NCMDPs to standard MDPs. This allows all techniques developed to find optimal policies for MDPs, such as reinforcement learning or dynamic programming, to be directly applied to the larger class of NCMDPs. Focusing on reinforcement learning, we show applications in a diverse set of tasks, including classical control, portfolio optimization in finance, and discrete optimization problems. Given our approach, we can improve both final performance and training time compared to relying on standard MDPs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-Ensemble: Efficient Uncertainty Modelling for Self-Attention Networks</title>
<link>https://arxiv.org/abs/2405.14438</link>
<guid>https://arxiv.org/abs/2405.14438</guid>
<content:encoded><![CDATA[

arXiv:2405.14438v4 Announce Type: replace 
Abstract: Numerous real-world decisions rely on machine learning algorithms and require calibrated uncertainty estimates. However, modern methods often yield overconfident, uncalibrated predictions. The dominant approach to quantifying the uncertainty inherent in the model is to train an ensemble of separate predictors and measure their empirical variance. In an explicit implementation, the ensemble has high computational cost and memory footprint, especially if the base model itself is already large, like modern transformers. This motivates efforts to develop implicit ensemble methods that emulate the ensemble without explicitly instantiating all its members. We introduce LoRA-Ensemble, a parameter-efficient ensembling method for self-attention networks. It is based on Low-Rank Adaptation (LoRA), originally developed for efficient LLM fine-tuning, and extends it into an implicit ensembling scheme, where all ensemble members share the same, pre-trained self-attention network, but have individual low-rank matrices for the attention projections. The resulting method not only outperforms state-of-the-art implicit techniques like BatchEnsemble, but even matches or exceeds the accuracy of an Explicit Ensemble, while at the same time achieving superior calibration.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Black-box Model Predictions via Two-level Nested Feature Attributions with Consistency Property</title>
<link>https://arxiv.org/abs/2405.14522</link>
<guid>https://arxiv.org/abs/2405.14522</guid>
<content:encoded><![CDATA[

arXiv:2405.14522v2 Announce Type: replace 
Abstract: Techniques that explain the predictions of black-box machine learning models are crucial to make the models transparent, thereby increasing trust in AI systems. The input features to the models often have a nested structure that consists of high- and low-level features, and each high-level feature is decomposed into multiple low-level features. For such inputs, both high-level feature attributions (HiFAs) and low-level feature attributions (LoFAs) are important for better understanding the model's decision. In this paper, we propose a model-agnostic local explanation method that effectively exploits the nested structure of the input to estimate the two-level feature attributions simultaneously. A key idea of the proposed method is to introduce the consistency property that should exist between the HiFAs and LoFAs, thereby bridging the separate optimization problems for estimating them. Thanks to this consistency property, the proposed method can produce HiFAs and LoFAs that are both faithful to the black-box models and consistent with each other, using a smaller number of queries to the models. In experiments on image classification in multiple instance learning and text classification using language models, we demonstrate that the HiFAs and LoFAs estimated by the proposed method are accurate, faithful to the behaviors of the black-box models, and provide consistent explanations.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normalized AOPC: Fixing Misleading Faithfulness Metrics for Feature Attribution Explainability</title>
<link>https://arxiv.org/abs/2408.08137</link>
<guid>https://arxiv.org/abs/2408.08137</guid>
<content:encoded><![CDATA[

arXiv:2408.08137v2 Announce Type: replace 
Abstract: Deep neural network predictions are notoriously difficult to interpret. Feature attribution methods aim to explain these predictions by identifying the contribution of each input feature. Faithfulness, often evaluated using the area over the perturbation curve (AOPC), reflects feature attributions' accuracy in describing the internal mechanisms of deep neural networks. However, many studies rely on AOPC to compare faithfulness across different models, which we show can lead to false conclusions about models' faithfulness. Specifically, we find that AOPC is sensitive to variations in the model, resulting in unreliable cross-model comparisons. Moreover, AOPC scores are difficult to interpret in isolation without knowing the model-specific lower and upper limits. To address these issues, we propose a normalization approach, Normalized AOPC (NAOPC), enabling consistent cross-model evaluations and more meaningful interpretation of individual scores. Our experiments demonstrate that this normalization can radically change AOPC results, questioning the conclusions of earlier studies and offering a more robust framework for assessing feature attribution faithfulness. Our code is available at https://github.com/JoakimEdin/naopc.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Generalized Hamiltonians using fully Symplectic Mappings</title>
<link>https://arxiv.org/abs/2409.11138</link>
<guid>https://arxiv.org/abs/2409.11138</guid>
<content:encoded><![CDATA[

arXiv:2409.11138v2 Announce Type: replace 
Abstract: Many important physical systems can be described as the evolution of a Hamiltonian system, which has the important property of being conservative, that is, energy is conserved throughout the evolution. Physics Informed Neural Networks and in particular Hamiltonian Neural Networks have emerged as a mechanism to incorporate structural inductive bias into the NN model. By ensuring physical invariances are conserved, the models exhibit significantly better sample complexity and out-of-distribution accuracy than standard NNs. Learning the Hamiltonian as a function of its canonical variables, typically position and velocity, from sample observations of the system thus becomes a critical task in system identification and long-term prediction of system behavior. However, to truly preserve the long-run physical conservation properties of Hamiltonian systems, one must use symplectic integrators for a forward pass of the system's simulation. While symplectic schemes have been used in the literature, they are thus far limited to situations when they reduce to explicit algorithms, which include the case of separable Hamiltonians or augmented non-separable Hamiltonians. We extend it to generalized non-separable Hamiltonians, and noting the self-adjoint property of symplectic integrators, we bypass computationally intensive backpropagation through an ODE solver. We show that the method is robust to noise and provides a good approximation of the system Hamiltonian when the state variables are sampled from a noisy observation. In the numerical results, we show the performance of the method concerning Hamiltonian reconstruction and conservation, indicating its particular advantage for non-separable systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MotifDisco: Motif Causal Discovery For Time Series Motifs</title>
<link>https://arxiv.org/abs/2409.15219</link>
<guid>https://arxiv.org/abs/2409.15219</guid>
<content:encoded><![CDATA[

arXiv:2409.15219v2 Announce Type: replace 
Abstract: Many time series, particularly health data streams, can be best understood as a sequence of phenomenon or events, which we call \textit{motifs}. A time series motif is a short trace segment which may implicitly capture an underlying phenomenon within the time series. Specifically, we focus on glucose traces collected from continuous glucose monitors (CGMs), which inherently contain motifs representing underlying human behaviors such as eating and exercise. The ability to identify and quantify \textit{causal} relationships amongst motifs can provide a mechanism to better understand and represent these patterns, useful for improving deep learning and generative models and for advanced technology development (e.g., personalized coaching and artificial insulin delivery systems). However, no previous work has developed causal discovery methods for time series motifs. Therefore, in this paper we develop MotifDisco (\textbf{motif} \textbf{disco}very of causality), a novel causal discovery framework to learn causal relations amongst motifs from time series traces. We formalize a notion of \textit{Motif Causality (MC)}, inspired from Granger Causality and Transfer Entropy, and develop a Graph Neural Network-based framework that learns causality between motifs by solving an unsupervised link prediction problem. We integrate MC with three model use cases of forecasting, anomaly detection and clustering, to showcase the use of MC as a building block for downstream tasks. Finally, we evaluate our framework on different health data streams and find that Motif Causality provides a significant performance improvement in all use cases.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SetPINNs: Set-based Physics-informed Neural Networks</title>
<link>https://arxiv.org/abs/2409.20206</link>
<guid>https://arxiv.org/abs/2409.20206</guid>
<content:encoded><![CDATA[

arXiv:2409.20206v4 Announce Type: replace 
Abstract: Physics-Informed Neural Networks (PINNs) solve partial differential equations using deep learning. However, conventional PINNs perform pointwise predictions that neglect dependencies within a domain, which may result in suboptimal solutions. We introduce SetPINNs, a framework that effectively captures local dependencies. With a finite element-inspired sampling scheme, we partition the domain into sets to model local dependencies while simultaneously enforcing physical laws. We provide a rigorous theoretical analysis showing that SetPINNs yield unbiased, lower-variance estimates of residual energy and its gradients, ensuring improved domain coverage and reduced residual error. Extensive experiments on synthetic and real-world tasks show improved accuracy, efficiency, and robustness.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score-based Pullback Riemannian Geometry: Extracting the Data Manifold Geometry using Anisotropic Flows</title>
<link>https://arxiv.org/abs/2410.01950</link>
<guid>https://arxiv.org/abs/2410.01950</guid>
<content:encoded><![CDATA[

arXiv:2410.01950v2 Announce Type: replace 
Abstract: Data-driven Riemannian geometry has emerged as a powerful tool for interpretable representation learning, offering improved efficiency in downstream tasks. Moving forward, it is crucial to balance cheap manifold mappings with efficient training algorithms. In this work, we integrate concepts from pullback Riemannian geometry and generative models to propose a framework for data-driven Riemannian geometry that is scalable in both geometry and learning: score-based pullback Riemannian geometry. Focusing on unimodal distributions as a first step, we propose a score-based Riemannian structure with closed-form geodesics that pass through the data probability density. With this structure, we construct a Riemannian autoencoder (RAE) with error bounds for discovering the correct data manifold dimension. This framework can naturally be used with anisotropic normalizing flows by adopting isometry regularization during training. Through numerical experiments on diverse datasets, including image data, we demonstrate that the proposed framework produces high-quality geodesics passing through the data support, reliably estimates the intrinsic dimension of the data manifold, and provides a global chart of the manifold. To the best of our knowledge, this is the first scalable framework for extracting the complete geometry of the data manifold.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fundamental Limitations on Subquadratic Alternatives to Transformers</title>
<link>https://arxiv.org/abs/2410.04271</link>
<guid>https://arxiv.org/abs/2410.04271</guid>
<content:encoded><![CDATA[

arXiv:2410.04271v2 Announce Type: replace 
Abstract: The Transformer architecture is widely deployed in many popular and impactful Large Language Models. At its core is the attention mechanism for calculating correlations between pairs of tokens. Performing an attention computation takes quadratic time in the input size, and had become the time bottleneck for transformer operations. In order to circumvent this, researchers have used a variety of approaches, including designing heuristic algorithms for performing attention computations faster, and proposing alternatives to the attention mechanism which can be computed more quickly. For instance, state space models such as Mamba were designed to replace attention with an almost linear time alternative.
  In this paper, we prove that any such approach cannot perform important tasks that Transformer is able to perform (assuming a popular conjecture from fine-grained complexity theory). We focus on document similarity tasks, where one is given as input many documents and would like to find a pair which is (approximately) the most similar. We prove that Transformer is able to perform this task, and we prove that this task cannot be performed in truly subquadratic time by any algorithm. Thus, any model which can be evaluated in subquadratic time - whether because of subquadratic-time heuristics for attention, faster attention replacements like Mamba, or any other reason - cannot perform this task. In other words, in order to perform tasks that (implicitly or explicitly) involve document similarity, one may as well use Transformer and cannot avoid its quadratic running time.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data</title>
<link>https://arxiv.org/abs/2410.05078</link>
<guid>https://arxiv.org/abs/2410.05078</guid>
<content:encoded><![CDATA[

arXiv:2410.05078v2 Announce Type: replace 
Abstract: Foundation models are strong data compressors, but when accounting for their parameter size, their compression ratios are inferior to standard compression algorithms. Naively reducing the parameter count does not necessarily help as it deteriorates predictions and, accordingly, compression. We conduct a large-scale empirical study to find a sweet spot where pre-trained vanilla transformers can achieve competitive compression ratios. To this end, we train models on 165GB of raw byte sequences of either text, image, or audio data (and all possible combinations of the three) and then compress 1GB of out-of-distribution (OOD) data from each modality. We find that relatively small models (millions of parameters) can outperform standard general-purpose compression algorithms (gzip, LZMA2) and even domain-specific compressors (PNG, JPEG-XL, FLAC) $\unicode{x2013}$ even when accounting for parameter size. We achieve, e.g., the lowest compression ratio of 0.49 on OOD audio data (vs. 0.54 for FLAC). We conduct extensive ablations and hyperparameter sweeps to study the impact of model- and dataset scale, and we investigate the effect of unimodal versus multimodal training. We find that even small models can be trained to perform well on multiple modalities, but unlike large-scale foundation models, transfer to unseen modalities is generally weak.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Model Predictive Control</title>
<link>https://arxiv.org/abs/2410.05364</link>
<guid>https://arxiv.org/abs/2410.05364</guid>
<content:encoded><![CDATA[

arXiv:2410.05364v2 Announce Type: replace 
Abstract: We propose Diffusion Model Predictive Control (D-MPC), a novel MPC approach that learns a multi-step action proposal and a multi-step dynamics model, both using diffusion models, and combines them for use in online MPC. On the popular D4RL benchmark, we show performance that is significantly better than existing model-based offline planning methods using MPC (e.g. MBOP) and competitive with state-of-the-art (SOTA) model-based and model-free reinforcement learning methods. We additionally illustrate D-MPC's ability to optimize novel reward functions at run time and adapt to novel dynamics, and highlight its advantages compared to existing diffusion-based planning baselines.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Reasoning Improves Molecular Understanding of LLM</title>
<link>https://arxiv.org/abs/2410.05610</link>
<guid>https://arxiv.org/abs/2410.05610</guid>
<content:encoded><![CDATA[

arXiv:2410.05610v2 Announce Type: replace 
Abstract: Recently, large language models (LLMs) have shown significant progress, approaching human perception levels. In this work, we demonstrate that despite these advances, LLMs still struggle to reason using molecular structural information. This gap is critical because many molecular properties, including functional groups, depend heavily on such structural details. To address this limitation, we propose an approach that sketches molecular structures for reasoning. Specifically, we introduce Molecular Structural Reasoning (MSR) framework to enhance the understanding of LLMs by explicitly incorporating the key structural features. We present two frameworks for scenarios where the target molecule is known or unknown. We verify that our MSR improves molecular understanding through extensive experiments.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Erasing Undesirable Concepts in Diffusion Models with Adversarial Preservation</title>
<link>https://arxiv.org/abs/2410.15618</link>
<guid>https://arxiv.org/abs/2410.15618</guid>
<content:encoded><![CDATA[

arXiv:2410.15618v4 Announce Type: replace 
Abstract: Diffusion models excel at generating visually striking content from text but can inadvertently produce undesirable or harmful content when trained on unfiltered internet data. A practical solution is to selectively removing target concepts from the model, but this may impact the remaining concepts. Prior approaches have tried to balance this by introducing a loss term to preserve neutral content or a regularization term to minimize changes in the model parameters, yet resolving this trade-off remains challenging. In this work, we propose to identify and preserving concepts most affected by parameter changes, termed as \textit{adversarial concepts}. This approach ensures stable erasure with minimal impact on the other concepts. We demonstrate the effectiveness of our method using the Stable Diffusion model, showing that it outperforms state-of-the-art erasure methods in eliminating unwanted content while maintaining the integrity of other unrelated elements. Our code is available at https://github.com/tuananhbui89/Erasing-Adversarial-Preservation.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unpicking Data at the Seams: Understanding Disentanglement in VAEs</title>
<link>https://arxiv.org/abs/2410.22559</link>
<guid>https://arxiv.org/abs/2410.22559</guid>
<content:encoded><![CDATA[

arXiv:2410.22559v5 Announce Type: replace 
Abstract: Disentanglement, or identifying statistically independent factors of the data, is relevant to much of machine learning, from controlled data generation and robust classification to efficient encoding and improving our understanding of the data itself. Disentanglement arises in several generative paradigms including Variational Autoencoders (VAEs), Generative Adversarial Networks and diffusion models. Prior work takes a step towards understanding disentanglement in VAEs by showing diagonal posterior covariance matrices promote orthogonality between columns of the decoder's Jacobian. Building on this, we close the gap in our understanding of disentanglement by showing how if follows from such orthogonality and equates to factoring the data distribution into statistically independent components.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoLDTree: A ULDA-Based Decision Tree Framework for Efficient Oblique Splits and Feature Selection</title>
<link>https://arxiv.org/abs/2410.23147</link>
<guid>https://arxiv.org/abs/2410.23147</guid>
<content:encoded><![CDATA[

arXiv:2410.23147v2 Announce Type: replace 
Abstract: Traditional decision trees are limited by axis-orthogonal splits, which can perform poorly when true decision boundaries are oblique. While oblique decision tree methods address this limitation, they often face high computational costs, difficulties with multi-class classification, and a lack of effective feature selection. In this paper, we introduce LDATree and FoLDTree, two novel frameworks that integrate Uncorrelated Linear Discriminant Analysis (ULDA) and Forward ULDA into a decision tree structure. These methods enable efficient oblique splits, handle missing values, support feature selection, and provide both class labels and probabilities as model outputs. Through evaluations on simulated and real-world datasets, LDATree and FoLDTree consistently outperform axis-orthogonal and other oblique decision tree methods, achieving accuracy levels comparable to the random forest. The results highlight the potential of these frameworks as robust alternatives to traditional single-tree methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OneProt: Towards Multi-Modal Protein Foundation Models</title>
<link>https://arxiv.org/abs/2411.04863</link>
<guid>https://arxiv.org/abs/2411.04863</guid>
<content:encoded><![CDATA[

arXiv:2411.04863v2 Announce Type: replace 
Abstract: Recent advances in Artificial Intelligence have enabled multi-modal systems to model and translate diverse information spaces. Extending beyond text and vision, we introduce OneProt, a multi-modal AI for proteins that integrates structural, sequence, text, and binding site data. Using the ImageBind framework, OneProt aligns the latent spaces of protein modality encoders in a lightweight fine-tuning scheme that focuses on pairwise alignment with sequence data rather than requiring full matches. This novel approach comprises a mix of Graph Neural Networks and transformer architectures. It demonstrates strong performance in retrieval tasks and showcases the efficacy of multi-modal systems in Protein Machine Learning through a broad spectrum of downstream baselines, including enzyme function prediction and binding site analysis. Furthermore, OneProt enables the transfer of representational information from specialized encoders to the sequence encoder, enhancing capabilities for distinguishing evolutionarily related and unrelated sequences and exhibiting representational properties where evolutionarily related proteins align in similar directions within the latent space. In addition, we extensively investigate modality ablations to identify the encoders that contribute most to predictive performance, highlighting the significance of the binding site encoder, which has not been used in similar models previously. This work expands the horizons of multi-modal protein models, paving the way for transformative applications in drug discovery, biocatalytic reaction planning, and protein engineering.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Language Model Refusal with Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2411.11296</link>
<guid>https://arxiv.org/abs/2411.11296</guid>
<content:encoded><![CDATA[

arXiv:2411.11296v2 Announce Type: replace 
Abstract: Responsible deployment of language models requires mechanisms for refusing unsafe prompts while preserving model performance. While most approaches modify model weights through additional training, we explore an alternative: steering model activations at inference time via amplifying sparse autoencoder (SAE) features that mediate refusal. This work uncovers a fundamental tension between SAE steering-based safety improvements and general model capabilities. While feature steering successfully improves robustness against both single-turn and challenging multi-turn jailbreak attempts, we discover that this comes at a previously underexplored cost -- systematic degradation of performance across multiple benchmark tasks, even on safe inputs with no apparent connection to refusal behavior. This suggests that features mediating refusal may be more deeply entangled with general language model capabilities than previously understood. Our findings reveal important open questions about the nature of safety-relevant features in language models and the feasibility of isolating them for targeted intervention. While SAE-based steering shows promise as a flexible approach to enhancing language model safety, our results highlight the critical need to understand and address the mechanisms behind these capability tradeoffs before such techniques can be practically deployed.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-Based Reconstruction of Full-Field Tsunami Waves from Sparse Tsunameter Networks</title>
<link>https://arxiv.org/abs/2411.12948</link>
<guid>https://arxiv.org/abs/2411.12948</guid>
<content:encoded><![CDATA[

arXiv:2411.12948v4 Announce Type: replace 
Abstract: We investigate the potential of an attention-based neural network architecture, the Senseiver, for sparse sensing in tsunami forecasting. Specifically, we focus on the Tsunami Data Assimilation Method, which generates forecasts from tsunameter networks. Our model is used to reconstruct high-resolution tsunami wavefields from extremely sparse observations, including cases where the tsunami epicenters are not represented in the training set. Furthermore, we demonstrate that our approach significantly outperforms the Linear Interpolation with Huygens-Fresnel Principle in generating dense observation networks, achieving markedly improved accuracy.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Load Forecasting for Households and Energy Communities: Are Deep Learning Models Worth the Effort?</title>
<link>https://arxiv.org/abs/2501.05000</link>
<guid>https://arxiv.org/abs/2501.05000</guid>
<content:encoded><![CDATA[

arXiv:2501.05000v3 Announce Type: replace 
Abstract: Energy communities (ECs) play a key role in enabling local demand shifting and enhancing self-sufficiency, as energy systems transition toward decentralized structures with high shares of renewable generation. To optimally operate them, accurate short-term load forecasting is essential, particularly for implementing demand-side management strategies. With the recent rise of deep learning methods, data-driven forecasting has gained significant attention, however, it remains insufficiently explored in many practical contexts. Therefore, this study evaluates the effectiveness of state-of-the-art deep learning models -- including LSTM, xLSTM, and Transformer architectures -- compared to traditional benchmarks such as K-Nearest Neighbors (KNN) and persistence forecasting, across varying community size, historical data availability, and model complexity. Additionally, we assess the benefits of transfer learning using publicly available synthetic load profiles. On average, transfer learning improves the normalized mean absolute error by 1.97%pt when only two months of training data are available. Interestingly, for less than six months of training data, simple persistence models outperform deep learning architectures in forecast accuracy. The practical value of improved forecasting is demonstrated using a mixed-integer linear programming optimization for ECs with a shared battery energy storage system. The most accurate deep learning model achieves an average reduction in financial energy costs of 8.06%. Notably, a simple KNN approach achieves average savings of 8.01%, making it a competitive and robust alternative. All implementations are publicly available to facilitate reproducibility. These findings offer actionable insights for ECs, and they highlight when the additional complexity of deep learning is warranted by performance gains.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FBQuant: FeedBack Quantization for Large Language Models</title>
<link>https://arxiv.org/abs/2501.16385</link>
<guid>https://arxiv.org/abs/2501.16385</guid>
<content:encoded><![CDATA[

arXiv:2501.16385v2 Announce Type: replace 
Abstract: Deploying Large Language Models (LLMs) on edge devices is increasingly important, as it eliminates reliance on network connections, reduces expensive API calls, and enhances user privacy. However, on-device deployment is challenging due to the limited computational resources of edge devices. In particular, the key bottleneck stems from memory bandwidth constraints related to weight loading. Weight-only quantization effectively reduces memory access, yet often induces significant accuracy degradation. Recent efforts to incorporate sub-branches have shown promise for mitigating quantization errors, but these methods either lack robust optimization strategies or rely on suboptimal objectives. To address these gaps, we propose FeedBack Quantization (FBQuant), a novel approach inspired by negative feedback mechanisms in automatic control. FBQuant inherently ensures that the reconstructed weights remain bounded by the quantization process, thereby reducing the risk of overfitting. To further offset the additional latency introduced by sub-branches, we develop an efficient CUDA kernel that decreases 60% of extra inference time. Comprehensive experiments demonstrate the efficiency and effectiveness of FBQuant across various LLMs. Notably, for 3-bit Llama2-7B, FBQuant improves zero-shot accuracy by 1.2%.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HopCast: Calibration of Autoregressive Dynamics Models</title>
<link>https://arxiv.org/abs/2501.16587</link>
<guid>https://arxiv.org/abs/2501.16587</guid>
<content:encoded><![CDATA[

arXiv:2501.16587v3 Announce Type: replace 
Abstract: Deep learning models are often trained to approximate dynamical systems that can be modeled using differential equations. Many of these models are optimized to predict one step ahead; such approaches produce calibrated one-step predictions if the predictive model can quantify uncertainty, such as Deep Ensembles. At inference time, multi-step predictions are generated via autoregression, which needs a sound uncertainty propagation method to produce calibrated multi-step predictions. This work introduces an alternative Predictor-Corrector approach named \hop{} that uses Modern Hopfield Networks (MHN) to learn the errors of a deterministic Predictor that approximates the dynamical system. The Corrector predicts a set of errors for the Predictor's output based on a context state at any timestep during autoregression. The set of errors creates sharper and well-calibrated prediction intervals with higher predictive accuracy compared to baselines without uncertainty propagation. The calibration and prediction performances are evaluated across a set of dynamical systems. This work is also the first to benchmark existing uncertainty propagation methods based on calibration errors.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Large Language Model Training Using FP4 Quantization</title>
<link>https://arxiv.org/abs/2501.17116</link>
<guid>https://arxiv.org/abs/2501.17116</guid>
<content:encoded><![CDATA[

arXiv:2501.17116v2 Announce Type: replace 
Abstract: The growing computational demands of training large language models (LLMs) necessitate more efficient methods. Quantized training presents a promising solution by enabling low-bit arithmetic operations to reduce these costs. While FP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge due to significant quantization errors and limited representational capacity. This work introduces the first FP4 training framework for LLMs, addressing these challenges with two key innovations: a differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse. To ensure stability, the framework integrates a mixed-precision training scheme and vector-wise quantization. Experimental results demonstrate that our FP4 framework achieves accuracy comparable to BF16 and FP8, with minimal degradation, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens. With the emergence of next-generation hardware supporting FP4, our framework sets a foundation for efficient ultra-low precision training.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training</title>
<link>https://arxiv.org/abs/2501.18511</link>
<guid>https://arxiv.org/abs/2501.18511</guid>
<content:encoded><![CDATA[

arXiv:2501.18511v2 Announce Type: replace 
Abstract: Language model (LLM) post-training, from DPO to distillation, can refine behaviors and unlock new skills, but the open science supporting these post-training techniques is still in its infancy. One limiting factor has been the difficulty of conducting large-scale comparative analyses of synthetic data generating models and LLM judges. To close this gap, we introduce WILDCHAT-50M, the largest public chat dataset to date. We extend the existing WildChat dataset to include responses not only from GPT, but from over 50 different open-weight models, ranging in size from 0.5B to 104B parameters. We conduct an extensive comparative analysis and demonstrate the potential of this dataset by creating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3 SFT mixture from Allen AI with only 40% as many samples. Our dataset, samples and code are available at https://github.com/penfever/wildchat-50m.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them</title>
<link>https://arxiv.org/abs/2501.18950</link>
<guid>https://arxiv.org/abs/2501.18950</guid>
<content:encoded><![CDATA[

arXiv:2501.18950v3 Announce Type: replace 
Abstract: Concept erasure has emerged as a promising technique for mitigating the risk of harmful content generation in diffusion models by selectively unlearning undesirable concepts. The common principle of previous works to remove a specific concept is to map it to a fixed generic concept, such as a neutral concept or just an empty text prompt. In this paper, we demonstrate that this fixed-target strategy is suboptimal, as it fails to account for the impact of erasing one concept on the others. To address this limitation, we model the concept space as a graph and empirically analyze the effects of erasing one concept on the remaining concepts. Our analysis uncovers intriguing geometric properties of the concept space, where the influence of erasing a concept is confined to a local region. Building on this insight, we propose the Adaptive Guided Erasure (AGE) method, which \emph{dynamically} selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects. Experimental results show that AGE significantly outperforms state-of-the-art erasure methods on preserving unrelated concepts while maintaining effective erasure performance. Our code is published at {https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elucidating Subspace Perturbation in Zeroth-Order Optimization: Theory and Practice at Scale</title>
<link>https://arxiv.org/abs/2501.19099</link>
<guid>https://arxiv.org/abs/2501.19099</guid>
<content:encoded><![CDATA[

arXiv:2501.19099v2 Announce Type: replace 
Abstract: Zeroth-order (ZO) optimization has emerged as a promising alternative to gradient-based backpropagation methods, particularly for black-box optimization and large language model (LLM) fine-tuning. However, ZO methods often suffer from slow convergence due to high-variance stochastic gradient estimators. While subspace perturbations, such as sparsity and low-rank constraints, have been explored to mitigate this issue, their effectiveness remains poorly understood. In this work, we develop a \emph{unified theoretical framework} that analyzes both the convergence and generalization properties of ZO optimization under subspace perturbations. We show that high dimensionality is the primary bottleneck and introduce the notion of \textit{subspace alignment} to explain how the subspace perturbations reduce gradient noise and accelerate convergence. Our analysis further shows that a broad class of subspace perturbations exhibits a similar convergence rate, motivating us to prioritize practical considerations in real-world algorithm design. Building on these insights, we propose an efficient ZO method using block coordinate descent (MeZO-BCD), which perturbs and updates only a subset of parameters at each step. Extensive experiments show that MeZO-BCD significantly accelerates optimization, achieving up to $\mathbf{\times2.77}$ speedup in wall-clock time over MeZO on OPT-13B, while maintaining comparable iteration complexity and fine-tuning performance.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRADIEND: Monosemantic Feature Learning within Neural Networks Applied to Gender Debiasing of Transformer Models</title>
<link>https://arxiv.org/abs/2502.01406</link>
<guid>https://arxiv.org/abs/2502.01406</guid>
<content:encoded><![CDATA[

arXiv:2502.01406v2 Announce Type: replace 
Abstract: AI systems frequently exhibit and amplify social biases, including gender bias, leading to harmful consequences in critical areas. This study introduces a novel encoder-decoder approach that leverages model gradients to learn a single monosemantic feature neuron encoding gender information. We show that our method can be used to debias transformer-based language models, while maintaining other capabilities. We demonstrate the effectiveness of our approach across various model architectures and highlight its potential for broader applications.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Alpha-Alternator: Dynamic Adaptation To Varying Noise Levels In Sequences Using The Vendi Score For Improved Robustness and Performance</title>
<link>https://arxiv.org/abs/2502.04593</link>
<guid>https://arxiv.org/abs/2502.04593</guid>
<content:encoded><![CDATA[

arXiv:2502.04593v2 Announce Type: replace 
Abstract: Current state-of-the-art dynamical models, such as Mamba, assume the same level of noisiness for all elements of a given sequence, which limits their performance on noisy temporal data. In this paper, we introduce the $\alpha$-Alternator, a novel generative model for time-dependent data that dynamically adapts to the complexity introduced by varying noise levels in sequences. The $\alpha$-Alternator leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to adjust, at each time step $t$, the influence of the sequence element at time $t$ and the latent representation of the dynamics up to that time step on the predicted future dynamics. This influence is captured by a parameter that is learned and shared across all sequences in a given dataset. The sign of this parameter determines the direction of influence. A negative value indicates a noisy dataset, where a sequence element that increases the VS is considered noisy, and the model relies more on the latent history when processing that element. Conversely, when the parameter is positive, a sequence element that increases the VS is considered informative, and the $\alpha$-Alternator relies more on this new input than on the latent history when updating its predicted latent dynamics. The $\alpha$-Alternator is trained using a combination of observation masking and Alternator loss minimization. Masking simulates varying noise levels in sequences, enabling the model to be more robust to these fluctuations and improving its performance in trajectory prediction, imputation, and forecasting. Our experimental results demonstrate that the $\alpha$-Alternator outperforms both Alternators and state-of-the-art state-space models across neural decoding and time-series forecasting benchmarks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter Symmetry Potentially Unifies Deep Learning Theory</title>
<link>https://arxiv.org/abs/2502.05300</link>
<guid>https://arxiv.org/abs/2502.05300</guid>
<content:encoded><![CDATA[

arXiv:2502.05300v2 Announce Type: replace 
Abstract: The dynamics of learning in modern large AI systems is hierarchical, often characterized by abrupt, qualitative shifts akin to phase transitions observed in physical systems. While these phenomena hold promise for uncovering the mechanisms behind neural networks and language models, existing theories remain fragmented, addressing specific cases. In this position paper, we advocate for the crucial role of the research direction of parameter symmetries in unifying these fragmented theories. This position is founded on a centralizing hypothesis for this direction: parameter symmetry breaking and restoration are the unifying mechanisms underlying the hierarchical learning behavior of AI models. We synthesize prior observations and theories to argue that this direction of research could lead to a unified understanding of three distinct hierarchies in neural networks: learning dynamics, model complexity, and representation formation. By connecting these hierarchies, our position paper elevates symmetry -- a cornerstone of theoretical physics -- to become a potential fundamental principle in modern AI.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flowing Through Layers: A Continuous Dynamical Systems Perspective on Transformers</title>
<link>https://arxiv.org/abs/2502.05656</link>
<guid>https://arxiv.org/abs/2502.05656</guid>
<content:encoded><![CDATA[

arXiv:2502.05656v2 Announce Type: replace 
Abstract: We show that the standard discrete update rule of transformer layers can be naturally interpreted as a forward Euler discretization of a continuous dynamical system. Our Transformer Flow Approximation Theorem demonstrates that, under standard Lipschitz continuity assumptions, token representations converge uniformly to the unique solution of an ODE as the number of layers grows. Moreover, if the underlying mapping satisfies a one-sided Lipschitz condition with a negative constant, the resulting dynamics are contractive, causing perturbations to decay exponentially across layers. Beyond clarifying the empirical stability and expressivity of transformer models, these insights link transformer updates to a broader iterative reasoning framework, suggesting new avenues for accelerated convergence and architectural innovations inspired by dynamical systems theory.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Lifting of Neural Representations: Zero-Shot Generalization for Causal Inferences</title>
<link>https://arxiv.org/abs/2502.06343</link>
<guid>https://arxiv.org/abs/2502.06343</guid>
<content:encoded><![CDATA[

arXiv:2502.06343v2 Announce Type: replace 
Abstract: In many scientific domains, the cost of data annotation limits the scale and pace of experimentation. Yet, modern machine learning systems offer a promising alternative, provided their predictions yield correct conclusions. We focus on Prediction-Powered Causal Inferences (PPCI), i.e., estimating the treatment effect in a target experiment with unlabeled factual outcomes, retrievable zero-shot from a pre-trained model. We first identify the conditional calibration property to guarantee valid PPCI at population level. Then, we introduce causal lifting, a new causal lifting constraint transferring validity across experiments, which we propose to enforce in practice in Deconfounded Empirical Risk Minimization, our new model-agnostic training objective. We validate our method on synthetic and real-world scientific data, offering solutions to instances not solvable by vanilla Empirical Risk Minimization and invariant training. In particular, we solve zero-shot PPCI on the ISTAnt dataset for the first time, fine-tuning a foundational model on our replica dataset of their ecological experiment with a different recording platform and treatment.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partial-Label Learning with Conformal Candidate Cleaning</title>
<link>https://arxiv.org/abs/2502.07661</link>
<guid>https://arxiv.org/abs/2502.07661</guid>
<content:encoded><![CDATA[

arXiv:2502.07661v2 Announce Type: replace 
Abstract: Real-world data is often ambiguous; for example, human annotation produces instances with multiple conflicting class labels. Partial-label learning (PLL) aims at training a classifier in this challenging setting, where each instance is associated with a set of candidate labels and one correct, but unknown, class label. A multitude of algorithms targeting this setting exists and, to enhance their prediction quality, several extensions that are applicable across a wide range of PLL methods have been introduced. While many of these extensions rely on heuristics, this article proposes a novel enhancing method that incrementally prunes candidate sets using conformal prediction. To work around the missing labeled validation set, which is typically required for conformal prediction, we propose a strategy that alternates between training a PLL classifier to label the validation set, leveraging these predicted class labels for calibration, and pruning candidate labels that are not part of the resulting conformal sets. In this sense, our method alternates between empirical risk minimization and candidate set pruning. We establish that our pruning method preserves the conformal validity with respect to the unknown ground truth. Our extensive experiments on artificial and real-world data show that the proposed approach significantly improves the test set accuracies of several state-of-the-art PLL classifiers.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streaming Attention Approximation via Discrepancy Theory</title>
<link>https://arxiv.org/abs/2502.07861</link>
<guid>https://arxiv.org/abs/2502.07861</guid>
<content:encoded><![CDATA[

arXiv:2502.07861v2 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. In this paper we study the streaming complexity of attention approximation, a key computational primitive underlying token generation.
  Our main contribution is BalanceKV, a streaming algorithm for $\epsilon$-approximating attention computations based on geometric process for selecting a balanced collection of Key and Value tokens as per Banaszczyk's vector balancing theory. We complement our algorithm with space lower bounds for streaming attention computation. Besides strong theoretical guarantees, BalanceKV exhibits empirically validated performance improvements over existing methods, both for attention approximation and end-to-end performance on various long context benchmarks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MixMin: Finding Data Mixtures via Convex Minimization</title>
<link>https://arxiv.org/abs/2502.10510</link>
<guid>https://arxiv.org/abs/2502.10510</guid>
<content:encoded><![CDATA[

arXiv:2502.10510v2 Announce Type: replace 
Abstract: Modern machine learning pipelines are increasingly combining and mixing data from diverse and disparate sources, e.g., pre-training large language models. Yet, finding the optimal data mixture is a challenging and open problem. We formalize this data mixing problem as a bi-level objective: the best mixture is the one that would lead to the best model for a downstream objective. Unfortunately, this objective is generally intractable. In this paper, we make the observation that the bi-level data mixing objective becomes convex as our model class becomes larger. We develop and study a gradient-based approach for optimizing this convex objective, which we call MixMin, and test it on language modeling and chemistry tasks. MixMin was the only method that uniformly improved the data mixture in all our experiments. With MixMin, we improved the data mixture using less than 0.2% additional compute for a pythia-410M model trained on 8.2B tokens, resulting between 1-5% relative improvement to negative log likelihood on PIQA, ARC Easy, SciQ, and OpenWebMath. Crucially, we found that MixMin mixtures for smaller models improved training of larger models, suggesting that MixMin mixtures may be scale-invariant. When mixing bioassay data to train an XGBoost model, we saw improvements to average precision scores of 0.03-0.15.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FuncGenFoil: Airfoil Generation and Editing Model in Function Space</title>
<link>https://arxiv.org/abs/2502.10712</link>
<guid>https://arxiv.org/abs/2502.10712</guid>
<content:encoded><![CDATA[

arXiv:2502.10712v3 Announce Type: replace 
Abstract: Aircraft manufacturing is the jewel in the crown of industry, in which generating high-fidelity airfoil geometries with controllable and editable representations remains a fundamental challenge. Existing deep learning methods, which typically rely on predefined parametric representations (e.g., B\'ezier) or discrete point sets, face an inherent trade-off between expressive power and resolution adaptability. To tackle this challenge, we introduce FuncGenFoil, a novel function-space generative model that directly reconstructs airfoil geometries as function curves. Our method inherits the advantages of arbitrary-resolution sampling and smoothness from parametric functions, as well as the strong expressiveness of discrete point-based representations. Empirical evaluations demonstrate that FuncGenFoil improves upon state-of-the-art methods in airfoil generation, achieving a relative 74.4% reduction in label error and a 23.2% increase in diversity on the AF-200K dataset. Our results highlight the advantages of function-space modeling for aerodynamic shape optimization, offering a powerful and flexible framework for high-fidelity airfoil design.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps in Keys and Values</title>
<link>https://arxiv.org/abs/2502.15075</link>
<guid>https://arxiv.org/abs/2502.15075</guid>
<content:encoded><![CDATA[

arXiv:2502.15075v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have introduced significant advancements to the capabilities of Natural Language Processing (NLP) in recent years. However, as these models continue to scale in size, memory constraints pose substantial challenge. Key and Value cache (KV cache) quantization has been well-documented as a promising solution to this limitation. In this work, we provide two novel theorems aimed at enhancing KV quantization methods. Our first theorem, termed Key-Value Norm Disparity, states that the key weight matrices by nature carry richer information compared to the value weight matrices, as evidenced by higher spectral and Frobenius norms across most of the layers. Our second theorem, Key-Driven Quantization, posits that prioritizing the quantization precision of keys over values induces significant improvements to the overall quantization performance. In particular, assigning greater precision to the keys compared to the values achieves a higher degree of precision reduction with minimal impact on model accuracy. We validate these theorems through theory and extensive experiments on several state-of-the-art LLM architectures and benchmarks. These findings offer valuable guidelines for improving KV cache quantization strategies, facilitating more efficient memory utilization without compromising model performance across diverse NLP tasks. Source code is available at https://github.com/mohsenhariri/spectral-kv.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Add, Multiply, and Execute Algorithmic Instructions Exactly with Neural Networks</title>
<link>https://arxiv.org/abs/2502.16763</link>
<guid>https://arxiv.org/abs/2502.16763</guid>
<content:encoded><![CDATA[

arXiv:2502.16763v2 Announce Type: replace 
Abstract: Neural networks are known for their ability to approximate smooth functions, yet they fail to generalize perfectly to unseen inputs when trained on discrete operations. Such operations lie at the heart of algorithmic tasks such as arithmetic, which is often used as a test bed for algorithmic execution in neural networks. In this work, we ask: can neural networks learn to execute binary-encoded algorithmic instructions exactly? We use the Neural Tangent Kernel (NTK) framework to study the training dynamics of two-layer fully connected networks in the infinite-width limit and show how a sufficiently large ensemble of such models can be trained to execute exactly, with high probability, four fundamental tasks: binary permutations, binary addition, binary multiplication, and Subtract and Branch if Negative (SBN) instructions. Since SBN is Turing-complete, our framework extends to computable functions. We show how this can be efficiently achieved using only logarithmically many training data. Our approach relies on two techniques: structuring the training data to isolate bit-level rules, and controlling correlations in the NTK regime to align model predictions with the target algorithmic executions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models</title>
<link>https://arxiv.org/abs/2502.21123</link>
<guid>https://arxiv.org/abs/2502.21123</guid>
<content:encoded><![CDATA[

arXiv:2502.21123v4 Announce Type: replace 
Abstract: Ensuring trustworthiness in machine learning (ML) systems is crucial as they become increasingly embedded in high-stakes domains. This paper advocates for integrating causal methods into machine learning to navigate the trade-offs among key principles of trustworthy ML, including fairness, privacy, robustness, accuracy, and explainability. While these objectives should ideally be satisfied simultaneously, they are often addressed in isolation, leading to conflicts and suboptimal solutions. Drawing on existing applications of causality in ML that successfully align goals such as fairness and accuracy or privacy and robustness, this paper argues that a causal approach is essential for balancing multiple competing objectives in both trustworthy ML and foundation models. Beyond highlighting these trade-offs, we examine how causality can be practically integrated into ML and foundation models, offering solutions to enhance their reliability and interpretability. Finally, we discuss the challenges, limitations, and opportunities in adopting causal frameworks, paving the way for more accountable and ethically sound AI systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Non-stationary Dynamics with Evidential Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2503.01468</link>
<guid>https://arxiv.org/abs/2503.01468</guid>
<content:encoded><![CDATA[

arXiv:2503.01468v2 Announce Type: replace 
Abstract: Continuous control of non-stationary environments is a major challenge for deep reinforcement learning algorithms. The time-dependency of the state transition dynamics aggravates the notorious stability problems of model-free deep actor-critic architectures. We posit that two properties will play a key role in overcoming non-stationarity in transition dynamics: (i) preserving the plasticity of the critic network, (ii) directed exploration for rapid adaptation to the changing dynamics. We show that performing on-policy reinforcement learning with an evidential critic provides both of these properties. The evidential design ensures a fast and sufficiently accurate approximation to the uncertainty around the state-value, which maintains the plasticity of the critic network by detecting the distributional shifts caused by the change in dynamics. The probabilistic critic also makes the actor training objective a random variable, enabling the use of directed exploration approaches as a by-product. We name the resulting algorithm as $\textit{ Evidential Proximal Policy Optimization (EPPO)}$ due to the integral role of evidential uncertainty quantification in both policy evaluation and policy improvement stages. Through experiments on non-stationary continuous control tasks, where the environment dynamics change at regular intervals, we demonstrate that our algorithm outperforms state-of-the-art on-policy reinforcement learning variants in both task-specific and overall return.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers</title>
<link>https://arxiv.org/abs/2503.03961</link>
<guid>https://arxiv.org/abs/2503.03961</guid>
<content:encoded><![CDATA[

arXiv:2503.03961v2 Announce Type: replace 
Abstract: Recent theoretical results show transformers cannot express sequential reasoning problems over long inputs, intuitively because their computational depth is bounded. However, prior work treats the depth as a constant, leaving it unclear to what degree bounded depth may suffice for solving problems over short inputs, or how increasing the transformer's depth affects its expressive power. We address these questions by analyzing transformers whose depth can grow minimally with context length $n$. We show even highly uniform transformers with depth $\Theta(\log n)$ can express two important problems: recognizing regular languages, which captures state tracking abilities and was known to be expressible only by an unconventional, non-uniform model of transformers, and graph connectivity, which underlies multi-step reasoning. Notably, both of these problems cannot be expressed by fixed-depth transformers under standard complexity conjectures, demonstrating the expressivity benefit of growing depth. Moreover, our theory quantitatively predicts how depth must grow with input length to express these problems, showing that depth scaling is more efficient than scaling width or chain-of-thought steps. Empirically, our detailed experiments designed to bridge the expressivity vs. learnability gap reveal that our theoretical depth requirements for regular language recognition closely match the practical depth requirements for successfully training transformers. Thus, our results clarify how depth affects a transformer's reasoning capabilities, and provide practical guidance for effective depth selection for sequential reasoning.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLDyB: Towards Dynamic Benchmarking for Continual Learning with Pre-trained Models</title>
<link>https://arxiv.org/abs/2503.04655</link>
<guid>https://arxiv.org/abs/2503.04655</guid>
<content:encoded><![CDATA[

arXiv:2503.04655v2 Announce Type: replace 
Abstract: The advent of the foundation model era has sparked significant research interest in leveraging pre-trained representations for continual learning (CL), yielding a series of top-performing CL methods on standard evaluation benchmarks. Nonetheless, there are growing concerns regarding potential data contamination during the pre-training stage. Furthermore, standard evaluation benchmarks, which are typically static, fail to capture the complexities of real-world CL scenarios, resulting in saturated performance. To address these issues, we describe CL on dynamic benchmarks (CLDyB), a general computational framework based on Markov decision processes for evaluating CL methods reliably. CLDyB dynamically identifies inherently difficult and algorithm-dependent tasks for the given CL methods, and determines challenging task orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct a joint evaluation of multiple state-of-the-art CL methods, leading to a set of commonly challenging and generalizable task sequences where existing CL methods tend to perform poorly. We then conduct separate evaluations of individual CL methods using CLDyB, discovering their respective strengths and weaknesses. The source code and generated task sequences are publicly accessible at https://github.com/szc12153/CLDyB.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Correct Automata Embeddings for Optimal Automata-Conditioned Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.05042</link>
<guid>https://arxiv.org/abs/2503.05042</guid>
<content:encoded><![CDATA[

arXiv:2503.05042v2 Announce Type: replace 
Abstract: Automata-conditioned reinforcement learning (RL) has given promising results for learning multi-task policies capable of performing temporally extended objectives given at runtime, done by pretraining and freezing automata embeddings prior to training the downstream policy. However, no theoretical guarantees were given. This work provides a theoretical framework for the automata-conditioned RL problem and shows that it is probably approximately correct learnable. We then present a technique for learning provably correct automata embeddings, guaranteeing optimal multi-task policy learning. Our experimental evaluation confirms these theoretical results.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Route Sparse Autoencoder to Interpret Large Language Models</title>
<link>https://arxiv.org/abs/2503.08200</link>
<guid>https://arxiv.org/abs/2503.08200</guid>
<content:encoded><![CDATA[

arXiv:2503.08200v3 Announce Type: replace 
Abstract: Mechanistic interpretability of large language models (LLMs) aims to uncover the internal processes of information propagation and reasoning. Sparse autoencoders (SAEs) have demonstrated promise in this domain by extracting interpretable and monosemantic features. However, prior works primarily focus on feature extraction from a single layer, failing to effectively capture activations that span multiple layers. In this paper, we introduce Route Sparse Autoencoder (RouteSAE), a new framework that integrates a routing mechanism with a shared SAE to efficiently extract features from multiple layers. It dynamically assigns weights to activations from different layers, incurring minimal parameter overhead while achieving high interpretability and flexibility for targeted feature manipulation. We evaluate RouteSAE through extensive experiments on Llama-3.2-1B-Instruct. Specifically, under the same sparsity constraint of 64, RouteSAE extracts 22.5% more features than baseline SAEs while achieving a 22.3% higher interpretability score. These results underscore the potential of RouteSAE as a scalable and effective method for LLM interpretability, with applications in feature discovery and model intervention. Our codes are available at https://github.com/swei2001/RouteSAEs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Residual Policy Gradient: A Reward View of KL-regularized Objective</title>
<link>https://arxiv.org/abs/2503.11019</link>
<guid>https://arxiv.org/abs/2503.11019</guid>
<content:encoded><![CDATA[

arXiv:2503.11019v2 Announce Type: replace 
Abstract: Reinforcement Learning and Imitation Learning have achieved widespread success in many domains but remain constrained during real-world deployment. One of the main issues is the additional requirements that were not considered during training. To address this challenge, policy customization has been introduced, aiming to adapt a prior policy while preserving its inherent properties and meeting new task-specific requirements. A principled approach to policy customization is Residual Q-Learning (RQL), which formulates the problem as a Markov Decision Process (MDP) and derives a family of value-based learning algorithms. However, RQL has not yet been applied to policy gradient methods, which restricts its applicability, especially in tasks where policy gradient has already proven more effective. In this work, we first derive a concise form of Soft Policy Gradient as a preliminary. Building on this, we introduce Residual Policy Gradient (RPG), which extends RQL to policy gradient methods, allowing policy customization in gradient-based RL settings. With the view of RPG, we rethink the KL-regularized objective widely used in RL fine-tuning. We show that under certain assumptions, KL-regularized objective leads to a maximum-entropy policy that balances the inherent properties and task-specific requirements on a reward-level. Our experiments in MuJoCo demonstrate the effectiveness of Soft Policy Gradient and Residual Policy Gradient.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permutation Equivariant Neural Networks for Symmetric Tensors</title>
<link>https://arxiv.org/abs/2503.11276</link>
<guid>https://arxiv.org/abs/2503.11276</guid>
<content:encoded><![CDATA[

arXiv:2503.11276v2 Announce Type: replace 
Abstract: Incorporating permutation equivariance into neural networks has proven to be useful in ensuring that models respect symmetries that exist in data. Symmetric tensors, which naturally appear in statistics, machine learning, and graph theory, are essential for many applications in physics, chemistry, and materials science, amongst others. However, existing research on permutation equivariant models has not explored symmetric tensors as inputs, and most prior work on learning from these tensors has focused on equivariance to Euclidean groups. In this paper, we present two different characterisations of all linear permutation equivariant functions between symmetric power spaces of $\mathbb{R}^n$. We show on two tasks that these functions are highly data efficient compared to standard MLPs and have potential to generalise well to symmetric tensors of different sizes.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Score Matching to Diffusion: A Fine-Grained Error Analysis in the Gaussian Setting</title>
<link>https://arxiv.org/abs/2503.11615</link>
<guid>https://arxiv.org/abs/2503.11615</guid>
<content:encoded><![CDATA[

arXiv:2503.11615v2 Announce Type: replace 
Abstract: Sampling from an unknown distribution, accessible only through discrete samples, is a fundamental problem at the core of generative AI. The current state-of-the-art methods follow a two-step process: first, estimating the score function (the gradient of a smoothed log-distribution) and then applying a diffusion-based sampling algorithm -- such as Langevin or Diffusion models. The resulting distribution's correctness can be impacted by four major factors: the generalization and optimization errors in score matching, and the discretization and minimal noise amplitude in the diffusion. In this paper, we make the sampling error explicit when using a diffusion sampler in the Gaussian setting. We provide a sharp analysis of the Wasserstein sampling error that arises from these four error sources. This allows us to rigorously track how the anisotropy of the data distribution (encoded by its power spectrum) interacts with key parameters of the end-to-end sampling method, including the number of initial samples, the stepsizes in both score matching and diffusion, and the noise amplitude. Notably, we show that the Wasserstein sampling error can be expressed as a kernel-type norm of the data power spectrum, where the specific kernel depends on the method parameters. This result provides a foundation for further analysis of the tradeoffs involved in optimizing sampling accuracy.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based Aggregation</title>
<link>https://arxiv.org/abs/2503.14572</link>
<guid>https://arxiv.org/abs/2503.14572</guid>
<content:encoded><![CDATA[

arXiv:2503.14572v2 Announce Type: replace 
Abstract: The capacity of a foundation model allows for adaptation to new downstream tasks. Weight imprinting is a universal and efficient method to fulfill this purpose. It has been reinvented several times, but it has not been systematically studied. In this paper, we propose a framework for imprinting, identifying three main components: generation, normalization, and aggregation. This allows us to conduct an in-depth analysis of imprinting and a comparison of the existing work. We reveal the benefits of representing novel data with multiple proxies in the generation step and show the importance of proper normalization. We determine proxies through clustering and propose a novel variant of imprinting that outperforms previous work. We motivate this by the neural collapse phenomenon -- an important connection that we can draw for the first time. Our results show an increase of up to 4\% in challenging scenarios with complex data distributions for new classes. Finally, we publicly release our code at https://github.com/DATEXIS/multi-imprinting/.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding the Benefits of Neural Network Parameterizations in Geophysical Inversions: A Study With Neural Fields</title>
<link>https://arxiv.org/abs/2503.17503</link>
<guid>https://arxiv.org/abs/2503.17503</guid>
<content:encoded><![CDATA[

arXiv:2503.17503v2 Announce Type: replace 
Abstract: In this work, we employ neural fields, which use neural networks to map a coordinate to the corresponding physical property value at that coordinate, in a test-time learning manner. For a test-time learning method, the weights are learned during the inversion, as compared to traditional approaches which require a network to be trained using a training dataset. Results for synthetic examples in seismic tomography and direct current resistivity inversions are shown first. We then perform a singular value decomposition analysis on the Jacobian of the weights of the neural network (SVD analysis) for both cases to explore the effects of neural networks on the recovered model. The results show that the test-time learning approach can eliminate unwanted artifacts in the recovered subsurface physical property model caused by the sensitivity of the survey and physics. Therefore, NFs-Inv improves the inversion results compared to the conventional inversion in some cases such as the recovery of the dip angle or the prediction of the boundaries of the main target. In the SVD analysis, we observe similar patterns in the left-singular vectors as were observed in some diffusion models, trained in a supervised manner, for generative tasks in computer vision. This observation provides evidence that there is an implicit bias, which is inherent in neural network structures, that is useful in supervised learning and test-time learning models. This implicit bias has the potential to be useful for recovering models in geophysical inversions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Ride-Sourcing Vehicle Rebalancing with Service Accessibility Guarantee: A Constrained Mean-Field Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2503.24183</link>
<guid>https://arxiv.org/abs/2503.24183</guid>
<content:encoded><![CDATA[

arXiv:2503.24183v2 Announce Type: replace 
Abstract: The rapid expansion of ride-sourcing services such as Uber, Lyft, and Didi Chuxing has fundamentally reshaped urban transportation by offering flexible, on-demand mobility via mobile applications. Despite their convenience, these platforms confront significant operational challenges, particularly vehicle rebalancing - the strategic repositioning of a large group of vehicles to address spatiotemporal mismatches in supply and demand. Inadequate rebalancing not only results in prolonged rider waiting times and inefficient vehicle utilization but also leads to fairness issues, such as the inequitable distribution of service quality and disparities in driver income. To tackle these complexities, we introduce continuous-state mean-field control (MFC) and mean-field reinforcement learning (MFRL) models that employ continuous vehicle repositioning actions. MFC and MFRL offer scalable solutions by modeling each vehicle's behavior through interaction with the vehicle distribution, rather than with individual vehicles. This limits the issues arising from the curse of dimensionality inherent in traditional multi-agent methods, enabling coordination across large fleets with significantly reduced computational complexity. To ensure equitable service access across geographic regions, we integrate an accessibility constraint into both models. Extensive empirical evaluation using real-world data-driven simulation of Shenzhen demonstrates the real-time efficiency and robustness of our approach. Remarkably, it scales to tens of thousands of vehicles, with training times comparable to the decision time of a single linear programming rebalancing. Besides, policies generated by our approach effectively explore the efficiency-equity Pareto front, outperforming conventional benchmarks across key metrics like fleet utilization, fulfilled requests, and pickup distance, while ensuring equitable service access.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title>
<link>https://arxiv.org/abs/2504.06261</link>
<guid>https://arxiv.org/abs/2504.06261</guid>
<content:encoded><![CDATA[

arXiv:2504.06261v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other's memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other's memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activated LoRA: Fine-tuned LLMs for Intrinsics</title>
<link>https://arxiv.org/abs/2504.12397</link>
<guid>https://arxiv.org/abs/2504.12397</guid>
<content:encoded><![CDATA[

arXiv:2504.12397v3 Announce Type: replace 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), an adapter architecture which modifies the LoRA framework to only adapt weights for the tokens in the sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the cache. This enables building what we call \emph{intrinsics}, i.e. specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We train a set of aLoRA-based intrinsics models, demonstrating competitive accuracy with standard LoRA while achieving significant inference benefits. We include a codebase implementing aLoRA in the supplementary material.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations</title>
<link>https://arxiv.org/abs/2504.12721</link>
<guid>https://arxiv.org/abs/2504.12721</guid>
<content:encoded><![CDATA[

arXiv:2504.12721v3 Announce Type: replace 
Abstract: Recent deep learning models for Long-term Time Series Forecasting (LTSF) often emphasize complex, handcrafted designs, while simpler architectures like linear models or MLPs have often outperformed these intricate solutions. In this paper, we revisit and organize the core ideas behind several key techniques, such as redundancy reduction and multi-scale modeling, which are frequently employed in advanced LTSF models. Our goal is to streamline these ideas for more efficient deep learning utilization. To this end, we introduce TimeCapsule, a model built around the principle of high-dimensional information compression that unifies these techniques in a generalized yet simplified framework. Specifically, we model time series as a 3D tensor, incorporating temporal, variate, and level dimensions, and leverage mode production to capture multi-mode dependencies while achieving dimensionality compression. We propose an internal forecast within the compressed representation domain, supported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the learning of predictive representations. Extensive experiments on challenging benchmarks demonstrate the versatility of our method, showing that TimeCapsule can achieve state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Deep Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13241</link>
<guid>https://arxiv.org/abs/2504.13241</guid>
<content:encoded><![CDATA[

arXiv:2504.13241v3 Announce Type: replace 
Abstract: Inferring an adversary's goals from exhibited behavior is crucial for counterplanning and non-cooperative multi-agent systems in domains like cybersecurity, military, and strategy games. Deep Inverse Reinforcement Learning (IRL) methods based on maximum entropy principles show promise in recovering adversaries' goals but are typically offline, require large batch sizes with gradient descent, and rely on first-order updates, limiting their applicability in real-time scenarios. We propose an online Recursive Deep Inverse Reinforcement Learning (RDIRL) approach to recover the cost function governing the adversary actions and goals. Specifically, we minimize an upper bound on the standard Guided Cost Learning (GCL) objective using sequential second-order Newton updates, akin to the Extended Kalman Filter (EKF), leading to a fast (in terms of convergence) learning algorithm. We demonstrate that RDIRL is able to recover cost and reward functions of expert agents in standard and adversarial benchmark tasks. Experiments on benchmark tasks show that our proposed approach outperforms several leading IRL algorithms.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synergistic Benefits of Joint Molecule Generation and Property Prediction</title>
<link>https://arxiv.org/abs/2504.16559</link>
<guid>https://arxiv.org/abs/2504.16559</guid>
<content:encoded><![CDATA[

arXiv:2504.16559v2 Announce Type: replace 
Abstract: Modeling the joint distribution of data samples and their properties allows to construct a single model for both data generation and property prediction, with synergistic benefits reaching beyond purely generative or predictive models. However, training joint models presents daunting architectural and optimization challenges. Here, we propose Hyformer, a transformer-based joint model that successfully blends the generative and predictive functionalities, using an alternating attention mechanism and a joint pre-training scheme. We show that Hyformer is simultaneously optimized for molecule generation and property prediction, while exhibiting synergistic benefits in conditional sampling, out-of-distribution property prediction and representation learning. Finally, we demonstrate the benefits of joint learning in a drug design use case of discovering novel antimicrobial~peptides.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust ML Auditing using Prior Knowledge</title>
<link>https://arxiv.org/abs/2505.04796</link>
<guid>https://arxiv.org/abs/2505.04796</guid>
<content:encoded><![CDATA[

arXiv:2505.04796v2 Announce Type: replace 
Abstract: Among the many technical challenges to enforcing AI regulations, one crucial yet underexplored problem is the risk of audit manipulation. This manipulation occurs when a platform deliberately alters its answers to a regulator to pass an audit without modifying its answers to other users. In this paper, we introduce a novel approach to manipulation-proof auditing by taking into account the auditor's prior knowledge of the task solved by the platform. We first demonstrate that regulators must not rely on public priors (e.g. a public dataset), as platforms could easily fool the auditor in such cases. We then formally establish the conditions under which an auditor can prevent audit manipulations using prior knowledge about the ground truth. Finally, our experiments with two standard datasets illustrate the maximum level of unfairness a platform can hide before being detected as malicious. Our formalization and generalization of manipulation-proof auditing with a prior opens up new research directions for more robust fairness audits.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Causal Direction via Variational Bayesian Compression</title>
<link>https://arxiv.org/abs/2505.07503</link>
<guid>https://arxiv.org/abs/2505.07503</guid>
<content:encoded><![CDATA[

arXiv:2505.07503v2 Announce Type: replace 
Abstract: Telling apart the cause and effect between two random variables with purely observational data is a challenging problem that finds applications in various scientific disciplines. A key principle utilized in this task is the algorithmic Markov condition, which postulates that the joint distribution, when factorized according to the causal direction, yields a more succinct codelength compared to the anti-causal direction. Previous approaches approximate these codelengths by relying on simple functions or Gaussian processes (GPs) with easily evaluable complexity, compromising between model fitness and computational complexity. To overcome these limitations, we propose leveraging the variational Bayesian learning of neural networks as an interpretation of the codelengths. Consequently, we can enhance the model fitness while promoting the succinctness of the codelengths, while avoiding the significant computational complexity of the GP-based approaches. Extensive experiments on both synthetic and real-world benchmarks in cause-effect identification demonstrate the effectiveness of our proposed method, surpassing the overall performance of related complexity-based and structural causal model regression-based approaches.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tuning for Trustworthiness -- Balancing Performance and Explanation Consistency in Neural Network Optimization</title>
<link>https://arxiv.org/abs/2505.07910</link>
<guid>https://arxiv.org/abs/2505.07910</guid>
<content:encoded><![CDATA[

arXiv:2505.07910v2 Announce Type: replace 
Abstract: Despite the growing interest in Explainable Artificial Intelligence (XAI), explainability is rarely considered during hyperparameter tuning or neural architecture optimization, where the focus remains primarily on minimizing predictive loss. In this work, we introduce the novel concept of XAI consistency, defined as the agreement among different feature attribution methods, and propose new metrics to quantify it. For the first time, we integrate XAI consistency directly into the hyperparameter tuning objective, creating a multi-objective optimization framework that balances predictive performance with explanation robustness. Implemented within the Sequential Parameter Optimization Toolbox (SPOT), our approach uses both weighted aggregation and desirability-based strategies to guide model selection. Through our proposed framework and supporting tools, we explore the impact of incorporating XAI consistency into the optimization process. This enables us to characterize distinct regions in the architecture configuration space: one region with poor performance and comparatively low interpretability, another with strong predictive performance but weak interpretability due to low \gls{xai} consistency, and a trade-off region that balances both objectives by offering high interpretability alongside competitive performance. Beyond introducing this novel approach, our research provides a foundation for future investigations into whether models from the trade-off zone-balancing performance loss and XAI consistency-exhibit greater robustness by avoiding overfitting to training performance, thereby leading to more reliable predictions on out-of-distribution data.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making Small Language Models Efficient Reasoners: Intervention, Supervision, Reinforcement</title>
<link>https://arxiv.org/abs/2505.07961</link>
<guid>https://arxiv.org/abs/2505.07961</guid>
<content:encoded><![CDATA[

arXiv:2505.07961v3 Announce Type: replace 
Abstract: Recent research enhances language model reasoning by scaling test-time compute via longer chain-of-thought traces. This often improves accuracy but also introduces redundancy and high computational cost, especially for small language models distilled with supervised fine-tuning (SFT). In this work, we propose new algorithms to improve token-efficient reasoning with small-scale models by effectively trading off accuracy and computation. We first show that the post-SFT model fails to determine the optimal stopping point of the reasoning process, resulting in verbose and repetitive outputs. Verbosity also significantly varies across wrong vs correct responses. To address these issues, we propose two solutions: (1) Temperature scaling (TS) to control the stopping point for the thinking phase and thereby trace length, and (2) TLDR: a length-regularized reinforcement learning method based on GRPO that facilitates multi-level trace length control (e.g. short, medium, long reasoning). Experiments on four reasoning benchmarks, MATH500, AMC, AIME24 and OlympiadBench, demonstrate that TS is highly effective compared to s1's budget forcing approach and TLDR significantly improves token efficiency by about 50% with minimal to no accuracy loss over the SFT baseline. Moreover, TLDR also facilitates flexible control over the response length, offering a practical and effective solution for token-efficient reasoning in small models. Ultimately, our work reveals the importance of stopping time control, highlights shortcomings of pure SFT, and provides effective algorithmic recipes.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Clustering via Alignment</title>
<link>https://arxiv.org/abs/2505.09131</link>
<guid>https://arxiv.org/abs/2505.09131</guid>
<content:encoded><![CDATA[

arXiv:2505.09131v2 Announce Type: replace 
Abstract: Algorithmic fairness in clustering aims to balance the proportions of instances assigned to each cluster with respect to a given sensitive attribute. While recently developed fair clustering algorithms optimize clustering objectives under specific fairness constraints, their inherent complexity or approximation often results in suboptimal clustering utility or numerical instability in practice. To resolve these limitations, we propose a new fair clustering algorithm based on a novel decomposition of the fair $K$-means clustering objective function. The proposed algorithm, called Fair Clustering via Alignment (FCA), operates by alternately (i) finding a joint probability distribution to align the data from different protected groups, and (ii) optimizing cluster centers in the aligned space. A key advantage of FCA is that it theoretically guarantees approximately optimal clustering utility for any given fairness level without complex constraints, thereby enabling high-utility fair clustering in practice. Experiments show that FCA outperforms existing methods by (i) attaining a superior trade-off between fairness level and clustering utility, and (ii) achieving near-perfect fairness without numerical instability.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cell Library Characterization for Composite Current Source Models Based on Gaussian Process Regression and Active Learning</title>
<link>https://arxiv.org/abs/2505.10799</link>
<guid>https://arxiv.org/abs/2505.10799</guid>
<content:encoded><![CDATA[

arXiv:2505.10799v2 Announce Type: replace 
Abstract: The composite current source (CCS) model has been adopted as an advanced timing model that represents the current behavior of cells for improved accuracy and better capability than traditional non-linear delay models (NLDM) to model complex dynamic effects and interactions under advanced process nodes. However, the high accuracy requirement, large amount of data and extensive simulation cost pose severe challenges to CCS characterization. To address these challenges, we introduce a novel Gaussian Process Regression(GPR) model with active learning(AL) to establish the characterization framework efficiently and accurately. Our approach significantly outperforms conventional commercial tools as well as learning based approaches by achieving an average absolute error of 2.05 ps and a relative error of 2.27% for current waveform of 57 cells under 9 process, voltage, temperature (PVT) corners with TSMC 22nm process. Additionally, our model drastically reduces the runtime to 27% and the storage by up to 19.5x compared with that required by commercial tools.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where You Place the Norm Matters: From Prejudiced to Neutral Initializations</title>
<link>https://arxiv.org/abs/2505.11312</link>
<guid>https://arxiv.org/abs/2505.11312</guid>
<content:encoded><![CDATA[

arXiv:2505.11312v2 Announce Type: replace 
Abstract: Normalization layers, such as Batch Normalization and Layer Normalization, are central components in modern neural networks, widely adopted to improve training stability and generalization. While their practical effectiveness is well documented, a detailed theoretical understanding of how normalization affects model behavior, starting from initialization, remains an important open question. In this work, we investigate how both the presence and placement of normalization within hidden layers influence the statistical properties of network predictions before training begins. In particular, we study how these choices shape the distribution of class predictions at initialization, which can range from unbiased (Neutral) to highly concentrated (Prejudiced) toward a subset of classes. Our analysis shows that normalization placement induces systematic differences in the initial prediction behavior of neural networks, which in turn shape the dynamics of learning. By linking architectural choices to prediction statistics at initialization, our work provides a principled understanding of how normalization can influence early training behavior and offers guidance for more controlled and interpretable network design.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Channel-Independent Time Series Forecasting via Cross-Variate Patch Embedding</title>
<link>https://arxiv.org/abs/2505.12761</link>
<guid>https://arxiv.org/abs/2505.12761</guid>
<content:encoded><![CDATA[

arXiv:2505.12761v3 Announce Type: replace 
Abstract: Transformers have recently gained popularity in time series forecasting due to their ability to capture long-term dependencies. However, many existing models focus only on capturing temporal dependencies while omitting intricate relationships between variables. Recent models have tried tackling this by explicitly modeling both cross-time and cross-variate dependencies through a sequential or unified attention mechanism, but they are entirely channel dependent (CD) across all layers, making them potentially susceptible to overfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE), a lightweight CD module that injects cross-variate context into channel-independent (CI) models by simply modifying the patch embedding process. We achieve this by adding a learnable positional encoding and a lightweight router-attention block to the vanilla patch embedding layer. We then integrate CVPE into Time-LLM, a multimodal CI forecasting model, to demonstrate its effectiveness in capturing cross-variate dependencies and enhance the CI model's performance. Extensive experimental results on seven real-world datasets show that our enhanced Time-LLM outperforms the original baseline model simply by incorporating the CVPE module, with no other changes.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents</title>
<link>https://arxiv.org/abs/2505.12842</link>
<guid>https://arxiv.org/abs/2505.12842</guid>
<content:encoded><![CDATA[

arXiv:2505.12842v2 Announce Type: replace 
Abstract: Graphical user interface (GUI) agents have recently emerged as an intriguing paradigm for human-computer interaction, capable of automatically executing user instructions to operate intelligent terminal devices. However, when encountering out-of-distribution (OOD) instructions that violate environmental constraints or exceed the current capabilities of agents, GUI agents may suffer task breakdowns or even pose security threats. Therefore, effective OOD detection for GUI agents is essential. Traditional OOD detection methods perform suboptimally in this domain due to the complex embedding space and evolving GUI environments. In this work, we observe that the in-distribution input semantic space of GUI agents exhibits a clustering pattern with respect to the distance from the centroid. Based on the finding, we propose GEM, a novel method based on fitting a Gaussian mixture model over input embedding distances extracted from the GUI Agent that reflect its capability boundary. Evaluated on eight datasets spanning smartphones, computers, and web browsers, our method achieves an average accuracy improvement of 23.70\% over the best-performing baseline. Analysis verifies the generalization ability of our method through experiments on nine different backbones. The codes are available at https://github.com/Wuzheng02/GEM-OODforGUIagents.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D3C2-Net: Dual-Domain Deep Convolutional Coding Network for Compressive Sensing</title>
<link>https://arxiv.org/abs/2207.13560</link>
<guid>https://arxiv.org/abs/2207.13560</guid>
<content:encoded><![CDATA[

arXiv:2207.13560v2 Announce Type: replace-cross 
Abstract: By mapping iterative optimization algorithms into neural networks (NNs), deep unfolding networks (DUNs) exhibit well-defined and interpretable structures and achieve remarkable success in the field of compressive sensing (CS). However, most existing DUNs solely rely on the image-domain unfolding, which restricts the information transmission capacity and reconstruction flexibility, leading to their loss of image details and unsatisfactory performance. To overcome these limitations, this paper develops a dual-domain optimization framework that combines the priors of (1) image- and (2) convolutional-coding-domains and offers generality to CS and other inverse imaging tasks. By converting this optimization framework into deep NN structures, we present a Dual-Domain Deep Convolutional Coding Network (D3C2-Net), which enjoys the ability to efficiently transmit high-capacity self-adaptive convolutional features across all its unfolded stages. Our theoretical analyses and experiments on simulated and real captured data, covering 2D and 3D natural, medical, and scientific signals, demonstrate the effectiveness, practicality, superior performance, and generalization ability of our method over other competing approaches and its significant potential in achieving a balance among accuracy, complexity, and interpretability. Code is available at https://github.com/lwq20020127/D3C2-Net.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Random Forests and Generalized Linear Models for Improved Accuracy and Interpretability</title>
<link>https://arxiv.org/abs/2307.01932</link>
<guid>https://arxiv.org/abs/2307.01932</guid>
<content:encoded><![CDATA[

arXiv:2307.01932v2 Announce Type: replace-cross 
Abstract: Random forests (RFs) are among the most popular supervised learning algorithms due to their nonlinear flexibility and ease-of-use. However, as black box models, they can only be interpreted via algorithmically-defined feature importance methods, such as Mean Decrease in Impurity (MDI), which have been observed to be highly unstable and have ambiguous scientific meaning. Furthermore, they can perform poorly in the presence of smooth or additive structure. To address this, we reinterpret decision trees and MDI as linear regression and $R^2$ values, respectively, with respect to engineered features associated with the tree's decision splits. This allows us to combine the respective strengths of RFs and generalized linear models in a framework called RF+, which also yields an improved feature importance method we call MDI+. Through extensive data-inspired simulations and real-world datasets, we show that RF+ improves prediction accuracy over RFs and that MDI+ outperforms popular feature importance measures in identifying signal features, often yielding more than a 10% improvement over its closest competitor. In case studies on drug response prediction and breast cancer subtyping, we further show that MDI+ extracts well-established genes with significantly greater stability compared to existing feature importance measures.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Formal Specifications from Membership and Preference Queries</title>
<link>https://arxiv.org/abs/2307.10434</link>
<guid>https://arxiv.org/abs/2307.10434</guid>
<content:encoded><![CDATA[

arXiv:2307.10434v2 Announce Type: replace-cross 
Abstract: Active learning is a well-studied approach to learning formal specifications, such as automata. In this work, we extend active specification learning by proposing a novel framework that strategically requests a combination of membership labels and pair-wise preferences, a popular alternative to membership labels. The combination of pair-wise preferences and membership labels allows for a more flexible approach to active specification learning, which previously relied on membership labels only. We instantiate our framework in two different domains, demonstrating the generality of our approach. Our results suggest that learning from both modalities allows us to robustly and conveniently identify specifications via membership and preferences.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources</title>
<link>https://arxiv.org/abs/2310.07147</link>
<guid>https://arxiv.org/abs/2310.07147</guid>
<content:encoded><![CDATA[

arXiv:2310.07147v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have showcased remarkable impacts across a wide spectrum of natural language processing tasks. Fine-tuning these pretrained models on downstream datasets provides further significant performance gains; however, this process typically requires a large number of expensive, high-end GPUs. Although there have been efforts focused on parameter-efficient fine-tuning, they cannot fully unlock the powerful potential of full-parameter fine-tuning. In this paper, we propose QFT, a Quantized Full-parameter Tuning framework for LLMs that quantizes and stores all training states, including weights, gradients, and optimizer states, in INT8 format to reduce training memory, thereby enabling full-parameter fine-tuning on existing GPUs at an affordable cost. To ensure training performance, we make two key efforts: i) for quantized gradients and optimizer states, we theoretically prove that the Lion optimizer, with its property of consistent update magnitudes, is highly robust to quantization; ii) and for quantized weights, we employ the hybrid feature quantizer, which identifies and protects a small subset of sparse critical features while quantizing the remaining dense features, thus ensuring accurate weight updates without FP32 backups. Moreover, to support backpropagation in the integer context, we develop a stack-based gradient flow scheme with O(1) complexity, forming a unified integer training pipeline. As a result, QFT reduces the model state memory to 21% of the standard solution while achieving comparable performance, e.g., tuning a LLaMA-7B model requires only <30GB of memory, making it feasible on a single A6000 GPU.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Dataset Properties on Membership Inference Vulnerability of Deep Transfer Learning</title>
<link>https://arxiv.org/abs/2402.06674</link>
<guid>https://arxiv.org/abs/2402.06674</guid>
<content:encoded><![CDATA[

arXiv:2402.06674v4 Announce Type: replace-cross 
Abstract: Membership inference attacks (MIAs) are used to test practical privacy of machine learning models. MIAs complement formal guarantees from differential privacy (DP) under a more realistic adversary model. We analyse MIA vulnerability of fine-tuned neural networks both empirically and theoretically, the latter using a simplified model of fine-tuning. We show that the vulnerability of non-DP models when measured as the attacker advantage at fixed false positive rate reduces according to a simple power law as the number of examples per class increases, even for the most vulnerable points, but the dataset size needed for adequate protection of the most vulnerable points is very large.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preconditioners for the Stochastic Training of Neural Fields</title>
<link>https://arxiv.org/abs/2402.08784</link>
<guid>https://arxiv.org/abs/2402.08784</guid>
<content:encoded><![CDATA[

arXiv:2402.08784v2 Announce Type: replace-cross 
Abstract: Neural fields encode continuous multidimensional signals as neural networks, enabling diverse applications in computer vision, robotics, and geometry. While Adam is effective for stochastic optimization, it often requires long training times. To address this, we explore alternative optimization techniques to accelerate training without sacrificing accuracy. Traditional second-order methods like L-BFGS are unsuitable for stochastic settings. We propose a theoretical framework for training neural fields with curvature-aware diagonal preconditioners, demonstrating their effectiveness across tasks such as image reconstruction, shape modeling, and Neural Radiance Fields (NeRF).
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Statistical Significance in Diffusion-Based Anomaly Localization via Selective Inference</title>
<link>https://arxiv.org/abs/2402.11789</link>
<guid>https://arxiv.org/abs/2402.11789</guid>
<content:encoded><![CDATA[

arXiv:2402.11789v4 Announce Type: replace-cross 
Abstract: Anomaly localization in images (identifying regions that deviate from expected patterns) is vital in applications such as medical diagnosis and industrial inspection. A recent trend is the use of image generation models in anomaly localization, where these models generate normal-looking counterparts of anomalous images, thereby allowing flexible and adaptive anomaly localization. However, these methods inherit the uncertainty and bias implicitly embedded in the employed generative model, raising concerns about the reliability. To address this, we propose a statistical framework based on selective inference to quantify the significance of detected anomalous regions. Our method provides $p$-values to assess the false positive detection rates, providing a principled measure of reliability. As a proof of concept, we consider anomaly localization using a diffusion model and its applications to medical diagnoses and industrial inspections. The results indicate that the proposed method effectively controls the risk of false positive detection, supporting its use in high-stakes decision-making tasks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E$^2$M: Double Bounded $\alpha$-Divergence Optimization for Tensor-based Discrete Density Estimation</title>
<link>https://arxiv.org/abs/2405.18220</link>
<guid>https://arxiv.org/abs/2405.18220</guid>
<content:encoded><![CDATA[

arXiv:2405.18220v3 Announce Type: replace-cross 
Abstract: Tensor-based discrete density estimation requires flexible modeling and proper divergence criteria to enable effective learning; however, traditional approaches using $\alpha$-divergence face analytical challenges due to the $\alpha$-power terms in the objective function, which hinder the derivation of closed-form update rules. We present a generalization of the expectation-maximization (EM) algorithm, called E$^2$M algorithm. It circumvents this issue by first relaxing the optimization into minimization of a surrogate objective based on the Kullback-Leibler (KL) divergence, which is tractable via the standard EM algorithm, and subsequently applying a tensor many-body approximation in the M-step to enable simultaneous closed-form updates of all parameters. Our approach offers flexible modeling for not only a variety of low-rank structures, including the CP, Tucker, and Tensor Train formats, but also their mixtures, thus allowing us to leverage the strengths of different low-rank structures. We demonstrate the effectiveness of our approach in classification and density estimation tasks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-efficient density quantum machine learning</title>
<link>https://arxiv.org/abs/2405.20237</link>
<guid>https://arxiv.org/abs/2405.20237</guid>
<content:encoded><![CDATA[

arXiv:2405.20237v2 Announce Type: replace-cross 
Abstract: Quantum machine learning (QML) requires powerful, flexible and efficiently trainable models to be successful in solving challenging problems. We introduce density quantum neural networks, a model family that prepares mixtures of trainable unitaries, with a distributional constraint over coefficients. This framework balances expressivity and efficient trainability, especially on quantum hardware. For expressivity, the Hastings-Campbell Mixing lemma converts benefits from linear combination of unitaries into density models with similar performance guarantees but shallower circuits. For trainability, commuting-generator circuits enable density model construction with efficiently extractable gradients. The framework connects to various facets of QML including post-variational and measurement-based learning. In classical settings, density models naturally integrate the mixture of experts formalism, and offer natural overfitting mitigation. The framework is versatile - we uplift several quantum models into density versions to improve model performance, or trainability, or both. These include Hamming weight-preserving and equivariant models, among others. Extensive numerical experiments validate our findings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigate Position Bias in Large Language Models via Scaling a Single Dimension</title>
<link>https://arxiv.org/abs/2406.02536</link>
<guid>https://arxiv.org/abs/2406.02536</guid>
<content:encoded><![CDATA[

arXiv:2406.02536v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly applied in various real-world scenarios due to their excellent generalization capabilities and robust generative abilities. However, they exhibit position bias, also known as "lost in the middle", a phenomenon that is especially pronounced in long-context scenarios, which indicates the placement of the key information in different positions of a prompt can significantly affect accuracy. This paper first explores the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias. It further identifies that, in addition to position embeddings, causal attention mask also contributes to position bias by creating position-specific hidden states. Based on these insights, we propose a method to mitigate position bias by scaling this positional hidden states. Experiments on the NaturalQuestions Multi-document QA, KV retrieval, LongBench and timeline reorder tasks, using various models including RoPE models, context windowextended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach. Our method can improve performance by up to 15.2% by modifying just one dimension of hidden states. Our code is available at https://aka.ms/PositionalHidden.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large-Scale AI Training Efficiency: The C4 Solution for Real-Time Anomaly Detection and Communication Optimization</title>
<link>https://arxiv.org/abs/2406.04594</link>
<guid>https://arxiv.org/abs/2406.04594</guid>
<content:encoded><![CDATA[

arXiv:2406.04594v2 Announce Type: replace-cross 
Abstract: The emergence of Large Language Models (LLMs) has necessitated the adoption of distributed training techniques, involving the deployment of thousands of GPUs to train a single model. Unfortunately, the efficiency of large-scale distributed training systems is often suboptimal due to the increased likelihood of hardware errors in high-end GPU products and the heightened risk of network traffic collisions. Moreover, any local hardware failure can disrupt training tasks, and the inability to swiftly identify faulty components leads to a significant waste of GPU resources. And, prolonged communication due to traffic collisions can substantially increase GPU waiting times. To address these challenges, we propose a communication-driven solution, namely the C4. The key insights of C4 are twofold. First, the load in distributed training exhibits homogeneous characteristics and is divided into iterations through periodic synchronization, therefore hardware anomalies would incur certain syndrome in collective communication. By leveraging this feature, C4 can rapidly identify the faulty components, swiftly isolate the anomaly, and restart the task, thereby avoiding resource wastage caused by delays in anomaly detection. Second, the predictable communication model of collective communication, involving a limited number of long-lived flows, allows C4 to efficiently execute traffic planning, substantially reducing bandwidth competition among these flows. The C4 has been extensively deployed across real-world production systems in a hyperscale cloud provider, yielding a significant improvement in system efficiency, from 30% to 45%. This enhancement is attributed to a 30% reduction in error-induced overhead and a 15% reduction in communication costs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity</title>
<link>https://arxiv.org/abs/2406.14479</link>
<guid>https://arxiv.org/abs/2406.14479</guid>
<content:encoded><![CDATA[

arXiv:2406.14479v3 Announce Type: replace-cross 
Abstract: Analyzing the similarity of internal representations has been an important technique for understanding the behavior of deep neural networks. Most existing methods for analyzing the similarity between representations of high dimensions, such as those based on Centered Kernel Alignment (CKA), rely on statistical properties of the representations for a set of data points. In this paper, we focus on transformer models and study the similarity of representations between the hidden layers of individual transformers. In this context, we show that a simple sample-wise cosine similarity metric is capable of capturing the similarity and aligns with the complicated CKA. Our experimental results on common transformers reveal that representations across layers are positively correlated, with similarity increasing when layers get closer. We provide a theoretical justification for this phenomenon under the geodesic curve assumption for the learned transformer. We then show that an increase in representation similarity implies an increase in predicted probability when directly applying the last-layer classifier to any hidden layer representation. We then propose an aligned training method to improve the effectiveness of shallow layer by enhancing the similarity between internal representations, with trained models that enjoy the following properties: (1) more early saturation events, (2) layer-wise accuracies monotonically increase and reveal the minimal depth needed for the given task, (3) when served as multi-exit models, they achieve on-par performance with standard multi-exit architectures which consist of additional classifiers designed for early exiting in shallow layers. To our knowledge, our work is the first to show that one common classifier is sufficient for multi-exit models. We conduct experiments on both vision and NLP tasks to demonstrate the performance of the proposed aligned training.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods</title>
<link>https://arxiv.org/abs/2406.15968</link>
<guid>https://arxiv.org/abs/2406.15968</guid>
<content:encoded><![CDATA[

arXiv:2406.15968v2 Announce Type: replace-cross 
Abstract: The rapid scaling of large language models (LLMs) has raised concerns about the transparency and fair use of the data used in their pretraining. Detecting such content is challenging due to the scale of the data and limited exposure of each instance during training. We propose ReCaLL (Relative Conditional Log-Likelihood), a novel membership inference attack (MIA) to detect LLMs' pretraining data by leveraging their conditional language modeling capabilities. ReCaLL examines the relative change in conditional log-likelihoods when prefixing target data points with non-member context. Our empirical findings show that conditioning member data on non-member prefixes induces a larger decrease in log-likelihood compared to non-member data. We conduct comprehensive experiments and show that ReCaLL achieves state-of-the-art performance on the WikiMIA dataset, even with random and synthetic prefixes, and can be further improved using an ensemble approach. Moreover, we conduct an in-depth analysis of LLMs' behavior with different membership contexts, providing insights into how LLMs leverage membership information for effective inference at both the sequence and token level.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Model-Free Bayesian State Estimation from Compressed Measurements</title>
<link>https://arxiv.org/abs/2407.07368</link>
<guid>https://arxiv.org/abs/2407.07368</guid>
<content:encoded><![CDATA[

arXiv:2407.07368v4 Announce Type: replace-cross 
Abstract: We consider data-driven Bayesian state estimation from compressed measurements (BSCM) of a model-free process. The dimension of the temporal measurement vector is lower than that of the temporal state vector to be estimated, leading to an under-determined inverse problem. The underlying dynamical model of the state's evolution is unknown for a 'model-free process.' Hence, it is difficult to use traditional model-driven methods, for example, Kalman and particle filters. Instead, we consider data-driven methods. We experimentally show that two existing unsupervised learning-based data-driven methods fail to address the BSCM problem in a model-free process. The methods are -- data-driven nonlinear state estimation (DANSE) and deep Markov model (DMM). While DANSE provides good predictive/forecasting performance to model the temporal measurement data as a time series, its unsupervised learning lacks suitable regularization for tackling the BSCM task. We then propose a semi-supervised learning approach and develop a semi-supervised learning-based DANSE method, referred to as SemiDANSE. In SemiDANSE, we use a large amount of unlabelled data along with a limited amount of labelled data, i.e., pairwise measurement-and-state data, which provides the desired regularization. Using three benchmark dynamical systems, we empirically show that the data-driven SemiDANSE provides competitive state estimation performance for BSCM using a handful of different measurement systems, against a hybrid method called KalmanNet and two model-driven methods (extended Kalman filter and unscented Kalman filter) that know the dynamical models exactly.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Importance Corrected Neural JKO Sampling</title>
<link>https://arxiv.org/abs/2407.20444</link>
<guid>https://arxiv.org/abs/2407.20444</guid>
<content:encoded><![CDATA[

arXiv:2407.20444v2 Announce Type: replace-cross 
Abstract: In order to sample from an unnormalized probability density function, we propose to combine continuous normalizing flows (CNFs) with rejection-resampling steps based on importance weights. We relate the iterative training of CNFs with regularized velocity fields to a JKO scheme and prove convergence of the involved velocity fields to the velocity field of the Wasserstein gradient flow (WGF). The alternation of local flow steps and non-local rejection-resampling steps allows to overcome local minima or slow convergence of the WGF for multimodal distributions. Since the proposal of the rejection step is generated by the model itself, they do not suffer from common drawbacks of classical rejection schemes. The arising model can be trained iteratively, reduces the reverse Kullback-Leibler (KL) loss function in each step, allows to generate iid samples and moreover allows for evaluations of the generated underlying density. Numerical examples show that our method yields accurate results on various test distributions including high-dimensional multimodal targets and outperforms the state of the art in almost all cases significantly.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions for Large Language Models</title>
<link>https://arxiv.org/abs/2407.21077</link>
<guid>https://arxiv.org/abs/2407.21077</guid>
<content:encoded><![CDATA[

arXiv:2407.21077v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) require high quality instruction data for effective alignment, particularly in code generation tasks where expert curated datasets are expensive to produce. We present Genetic-Instruct, a scalable algorithm for synthesizing large-scale, high quality coding instructions using evolutionary principles. Starting from a small set of seed instructions, Genetic-Instruct generates diverse and challenging instruction-code pairs by leveraging an Instructor-LLM for generation, a Coder-LLM for code synthesis, and a Judge-LLM for automatic quality evaluation. Our proposed approach is highly parallelizable and effective even with a small seed data and weaker generator models. We generated more than 7.5 million coding instructions with the proposed approach. Then we evaluated it by fine-tuning LLMs with the synthetic samples and demonstrated a significant improvement in their code generation capability compared to the other synthetic generation approaches and publicly available datasets. Our results highlight the efficiency, scalability, and generalizability of the Genetic-Instruct framework.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SINDyG: Sparse Identification of Nonlinear Dynamical Systems from Graph-Structured Data</title>
<link>https://arxiv.org/abs/2409.04463</link>
<guid>https://arxiv.org/abs/2409.04463</guid>
<content:encoded><![CDATA[

arXiv:2409.04463v3 Announce Type: replace-cross 
Abstract: The combination of machine learning (ML) and sparsity-promoting techniques is enabling direct extraction of governing equations from data, revolutionizing computational modeling in diverse fields of science and engineering. The discovered dynamical models could be used to address challenges in climate science, neuroscience, ecology, finance, epidemiology, and beyond. However, most existing sparse identification methods for discovering dynamical systems treat the whole system as one without considering the interactions between subsystems. As a result, such models are not able to capture small changes in the emergent system behavior. To address this issue, we developed a new method called Sparse Identification of Nonlinear Dynamical Systems from Graph-structured data (SINDyG), which incorporates the network structure into sparse regression to identify model parameters that explain the underlying network dynamics. We showcase the application of our proposed method using several case studies of neuronal dynamics, where we model the macroscopic oscillation of a population of neurons using the extended Stuart-Landau (SL) equation and utilize the SINDyG method to identify the underlying nonlinear dynamics. Our extensive computational experiments validate the improved accuracy and simplicity of discovered network dynamics when compared to the original SINDy approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protein sequence classification using natural language processing techniques</title>
<link>https://arxiv.org/abs/2409.04491</link>
<guid>https://arxiv.org/abs/2409.04491</guid>
<content:encoded><![CDATA[

arXiv:2409.04491v2 Announce Type: replace-cross 
Abstract: Purpose: This study aimed to enhance protein sequence classification using natural language processing (NLP) techniques while addressing the impact of sequence similarity on model performance. We compared various machine learning and deep learning models under two different data-splitting strategies: random splitting and ECOD family-based splitting, which ensures evolutionary-related sequences are grouped together. Methods: The study evaluated models such as K-Nearest Neighbors (KNN), Multinomial Na\"ive Bayes, Logistic Regression, Multi-Layer Perceptron (MLP), Decision Tree, Random Forest, XGBoost, Voting and Stacking classifiers, Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and transformer models (BertForSequenceClassification, DistilBERT, and ProtBert). Performance was tested using different amino acid ranges and sequence lengths with a focus on generalization across unseen evolutionary families. Results: The Voting classifier achieved the highest performance with 74% accuracy, 74% weighted F1 score, and 65% macro F1 score under random splitting, while ProtBERT obtained 77% accuracy, 76% weighted F1 score, and 61% macro F1 score among transformer models. However, performance declined across all models when tested using ECOD-based splitting, revealing the impact of sequence similarity on classification performance. Conclusion: Advanced NLP techniques, particularly ensemble methods like Voting classifiers, and transformer models show significant potential in protein classification, with sufficient training data and sequence similarity management being crucial for optimal performance. However, the use of biologically meaningful splitting methods, such as ECOD family-based splitting, is crucial for realistic performance evaluation and generalization to unseen evolutionary families.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Lists to Emojis: How Format Bias Affects Model Alignment</title>
<link>https://arxiv.org/abs/2409.11704</link>
<guid>https://arxiv.org/abs/2409.11704</guid>
<content:encoded><![CDATA[

arXiv:2409.11704v2 Announce Type: replace-cross 
Abstract: In this paper, we study format biases in reinforcement learning from human feedback (RLHF). We observe that many widely-used preference models, including human evaluators, GPT-4, and top-ranking models on the RewardBench benchmark, exhibit strong biases towards specific format patterns, such as lists, links, bold text, and emojis. Furthermore, large language models (LLMs) can exploit these biases to achieve higher rankings on popular benchmarks like AlpacaEval and LMSYS Chatbot Arena. One notable example of this is verbosity bias, where current preference models favor longer responses that appear more comprehensive, even when their quality is equal to or lower than shorter, competing responses. However, format biases beyond verbosity remain largely underexplored in the literature. In this work, we extend the study of biases in preference learning beyond the commonly recognized length bias, offering a comprehensive analysis of a wider range of format biases. Additionally, we show that with a small amount of biased data (less than 1%), we can inject significant bias into the reward model. Moreover, these format biases can also be easily exploited by downstream alignment algorithms, such as best-of-n sampling and online iterative DPO, as it is usually easier to manipulate the format than to improve the quality of responses. Our findings emphasize the need to disentangle format and content both for designing alignment algorithms and evaluating models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Differentiable and Dynamic Ray Tracing: Introducing the Multipath Lifetime Map</title>
<link>https://arxiv.org/abs/2410.14535</link>
<guid>https://arxiv.org/abs/2410.14535</guid>
<content:encoded><![CDATA[

arXiv:2410.14535v5 Announce Type: replace-cross 
Abstract: With the increasing presence of dynamic scenarios, such as Vehicle-to-Vehicle communications, radio propagation modeling tools must adapt to the rapidly changing nature of the radio channel. Recently, both Differentiable and Dynamic Ray Tracing frameworks have emerged to address these challenges. However, there is often confusion about how these approaches differ and which one should be used in specific contexts. In this paper, we provide an overview of these two techniques and a comparative analysis against two state-of-the-art tools: 3DSCAT from UniBo and Sionna from NVIDIA. To provide a more precise characterization of the scope of these methods, we introduce a novel simulation-based metric, the Multipath Lifetime Map, which enables the evaluation of spatial and temporal coherence in radio channels only based on the geometrical description of the environment. Finally, our metrics are evaluated on a classic urban street canyon scenario, yielding similar results to those obtained from measurement campaigns.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Two-Stage Learning-to-Defer Approach for Multi-Task Learning</title>
<link>https://arxiv.org/abs/2410.15729</link>
<guid>https://arxiv.org/abs/2410.15729</guid>
<content:encoded><![CDATA[

arXiv:2410.15729v4 Announce Type: replace-cross 
Abstract: The Two-Stage Learning-to-Defer (L2D) framework has been extensively studied for classification and, more recently, regression tasks. However, many real-world applications require solving both tasks jointly in a multi-task setting. We introduce a novel Two-Stage L2D framework for multi-task learning that integrates classification and regression through a unified deferral mechanism. Our method leverages a two-stage surrogate loss family, which we prove to be both Bayes-consistent and $(\mathcal{G}, \mathcal{R})$-consistent, ensuring convergence to the Bayes-optimal rejector. We derive explicit consistency bounds tied to the cross-entropy surrogate and the $L_1$-norm of agent-specific costs, and extend minimizability gap analysis to the multi-expert two-stage regime. We also make explicit how shared representation learning--commonly used in multi-task models--affects these consistency guarantees. Experiments on object detection and electronic health record analysis demonstrate the effectiveness of our approach and highlight the limitations of existing L2D methods in multi-task scenarios.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LL\"aMmlein: Compact and Competitive German-Only Language Models from Scratch</title>
<link>https://arxiv.org/abs/2411.11171</link>
<guid>https://arxiv.org/abs/2411.11171</guid>
<content:encoded><![CDATA[

arXiv:2411.11171v3 Announce Type: replace-cross 
Abstract: We create two German-only decoder models, LL\"aMmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use. The model training involved several key steps, including extensive data preprocessing, the creation of a custom German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks. Throughout the training process, multiple checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor the models' learning dynamics. Compared to state-of-the-art models on the SuperGLEBer benchmark, both LL\"aMmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models' quality scales with size as expected, but performance improvements on some tasks plateaued early, offering valuable insights into resource allocation for future model development.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe PDE Boundary Control with Neural Operators</title>
<link>https://arxiv.org/abs/2411.15643</link>
<guid>https://arxiv.org/abs/2411.15643</guid>
<content:encoded><![CDATA[

arXiv:2411.15643v2 Announce Type: replace-cross 
Abstract: The physical world dynamics are generally governed by underlying partial differential equations (PDEs) with unknown analytical forms in science and engineering problems. Neural network based data-driven approaches have been heavily studied in simulating and solving PDE problems in recent years, but it is still challenging to move forward from understanding to controlling the unknown PDE dynamics. PDE boundary control instantiates a simplified but important problem by only focusing on PDE boundary conditions as the control input and output. However, current model-free PDE controllers cannot ensure the boundary output satisfies some given user-specified safety constraint. To this end, we propose a safety filtering framework to guarantee the boundary output stays within the safe set for current model-free controllers. Specifically, we first introduce a neural boundary control barrier function (BCBF) to ensure the feasibility of the trajectory-wise constraint satisfaction of boundary output. Based on the neural operator modeling the transfer function from boundary control input to output trajectories, we show that the change in the BCBF depends linearly on the change in input boundary, so quadratic programming-based safety filtering can be done for pre-trained model-free controllers. Extensive experiments under challenging hyperbolic, parabolic and Navier-Stokes PDE dynamics environments validate the plug-and-play effectiveness of the proposed method by achieving better general performance and boundary constraint satisfaction compared to the vanilla and constrained model-free controller baselines. The code is available at https://github.com/intelligent-control-lab/safe-pde-control.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffBreak: Is Diffusion-Based Purification Robust?</title>
<link>https://arxiv.org/abs/2411.16598</link>
<guid>https://arxiv.org/abs/2411.16598</guid>
<content:encoded><![CDATA[

arXiv:2411.16598v3 Announce Type: replace-cross 
Abstract: Diffusion-based purification (DBP) has become a cornerstone defense against adversarial examples (AEs), regarded as robust due to its use of diffusion models (DMs) that project AEs onto the natural data manifold. We refute this core claim, theoretically proving that gradient-based attacks effectively target the DM rather than the classifier, causing DBP's outputs to align with adversarial distributions. This prompts a reassessment of DBP's robustness, attributing it to two critical flaws: incorrect gradients and inappropriate evaluation protocols that test only a single random purification of the AE. We show that with proper accounting for stochasticity and resubmission risk, DBP collapses. To support this, we introduce DiffBreak, the first reliable toolkit for differentiation through DBP, eliminating gradient flaws that previously further inflated robustness estimates. We also analyze the current defense scheme used for DBP where classification relies on a single purification, pinpointing its inherent invalidity. We provide a statistically grounded majority-vote (MV) alternative that aggregates predictions across multiple purified copies, showing partial but meaningful robustness gain. We then propose a novel adaptation of an optimization method against deepfake watermarking, crafting systemic perturbations that defeat DBP even under MV, challenging DBP's viability.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning</title>
<link>https://arxiv.org/abs/2411.19557</link>
<guid>https://arxiv.org/abs/2411.19557</guid>
<content:encoded><![CDATA[

arXiv:2411.19557v3 Announce Type: replace-cross 
Abstract: Low-rank adapters have become standard for efficiently fine-tuning large language models (LLMs), but they often fall short of achieving the performance of full fine-tuning. We propose a method, LoRA Silver Bullet or LoRA-SB, that approximates full fine-tuning within low-rank subspaces using a carefully designed initialization strategy. We theoretically demonstrate that the architecture of LoRA-XS, which inserts a learnable (r x r) matrix between B and A while keeping other matrices fixed, provides the precise conditions needed for this approximation. We leverage its constrained update space to achieve optimal scaling for high-rank gradient updates while removing the need for hyperparameter tuning. We prove that our initialization offers an optimal low-rank approximation of the initial gradient and preserves update directions throughout training. Extensive experiments across mathematical reasoning, commonsense reasoning, and language understanding tasks demonstrate that our approach exceeds the performance of standard LoRA while using \textbf{27-90} times fewer learnable parameters, and comprehensively outperforms LoRA-XS. Our findings establish that it is possible to simulate full fine-tuning in low-rank subspaces, and achieve significant efficiency gains without sacrificing performance. Our code is publicly available at https://github.com/RaghavSinghal10/lora-sb.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forensics Adapter: Unleashing CLIP for Generalizable Face Forgery Detection</title>
<link>https://arxiv.org/abs/2411.19715</link>
<guid>https://arxiv.org/abs/2411.19715</guid>
<content:encoded><![CDATA[

arXiv:2411.19715v3 Announce Type: replace-cross 
Abstract: We describe Forensics Adapter, an adapter network designed to transform CLIP into an effective and generalizable face forgery detector. Although CLIP is highly versatile, adapting it for face forgery detection is non-trivial as forgery-related knowledge is entangled with a wide range of unrelated knowledge. Existing methods treat CLIP merely as a feature extractor, lacking task-specific adaptation, which limits their effectiveness. To address this, we introduce an adapter to learn face forgery traces -- the blending boundaries unique to forged faces, guided by task-specific objectives. Then we enhance the CLIP visual tokens with a dedicated interaction strategy that communicates knowledge across CLIP and the adapter. Since the adapter is alongside CLIP, its versatility is highly retained, naturally ensuring strong generalizability in face forgery detection. With only 5.7M trainable parameters, our method achieves a significant performance boost, improving by approximately 7% on average across five standard datasets. Additionally, we describe Forensics Adapter++, an extended method that incorporates textual modality via a newly proposed forgery-aware prompt learning strategy. This extension leads to a further 1.3% performance boost over the original Forensics Adapter. We believe the proposed methods can serve as a baseline for future CLIP-based face forgery detection methods. The codes have been released at https://github.com/OUC-VAS/ForensicsAdapter.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMAct: A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations</title>
<link>https://arxiv.org/abs/2412.01441</link>
<guid>https://arxiv.org/abs/2412.01441</guid>
<content:encoded><![CDATA[

arXiv:2412.01441v3 Announce Type: replace-cross 
Abstract: In this paper, we present a benchmark to pressure-test today's frontier models' multimodal decision-making capabilities in the very long-context regime (up to one million tokens) and investigate whether these models can learn from large numbers of expert demonstrations in their context. We evaluate the performance of Claude 3.5 Sonnet, Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 2.0 Flash Experimental, GPT-4o, o1-mini, o1-preview, and o1 as policies across a battery of simple interactive decision-making tasks: playing tic-tac-toe, chess, and Atari, navigating grid worlds, solving crosswords, and controlling a simulated cheetah. We study increasing amounts of expert demonstrations in the context $\unicode{x2013}$ from no demonstrations to 512 full episodes. Across our tasks, models rarely manage to fully reach expert performance, and often, presenting more demonstrations has little effect. Some models steadily improve with more demonstrations on a few tasks. We investigate the effect of encoding observations as text or images and the impact of chain-of-thought prompting. To help quantify the impact of other approaches and future innovations, we open source our benchmark that covers the zero-, few-, and many-shot regimes in a unified evaluation.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doubly Robust Conformalized Survival Analysis with Right-Censored Data</title>
<link>https://arxiv.org/abs/2412.09729</link>
<guid>https://arxiv.org/abs/2412.09729</guid>
<content:encoded><![CDATA[

arXiv:2412.09729v2 Announce Type: replace-cross 
Abstract: We present a conformal inference method for constructing lower prediction bounds for survival times from right-censored data, extending recent approaches designed for more restrictive type-I censoring scenarios. The proposed method imputes unobserved censoring times using a machine learning model, and then analyzes the imputed data using a survival model calibrated via weighted conformal inference. This approach is theoretically supported by an asymptotic double robustness property. Empirical studies on simulated and real data demonstrate that our method leads to relatively informative predictive inferences and is especially robust in challenging settings where the survival model may be inaccurate.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Board Games by External and Internal Planning with Language Models</title>
<link>https://arxiv.org/abs/2412.12119</link>
<guid>https://arxiv.org/abs/2412.12119</guid>
<content:encoded><![CDATA[

arXiv:2412.12119v3 Announce Type: replace-cross 
Abstract: Advancing planning and reasoning capabilities of Large Language Models (LLMs) is one of the key prerequisites towards unlocking their potential for performing reliably in complex and impactful domains. In this paper, we aim to demonstrate this across board games (Chess, Fischer Random / Chess960, Connect Four, and Hex), and we show that search-based planning can yield significant improvements in LLM game-playing strength. We introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without calls to an external game engine, and in internal search, the model is trained to generate in-context a linearized tree of search and a resulting final choice. Both build on a language model pre-trained on relevant domain knowledge, reliably capturing the transition and value functions in the respective environments, with minimal hallucinations. We evaluate our LLM search implementations against game-specific state-of-the-art engines, showcasing substantial improvements in strength over the base model, and reaching Grandmaster-level performance in chess while operating closer to the human search budget. Our proposed approach, combining search with domain knowledge, is not specific to board games, hinting at more general future applications.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChronoFlow: A Data-Driven Model for Gyrochronology</title>
<link>https://arxiv.org/abs/2412.12244</link>
<guid>https://arxiv.org/abs/2412.12244</guid>
<content:encoded><![CDATA[

arXiv:2412.12244v2 Announce Type: replace-cross 
Abstract: Gyrochronology is a technique for constraining stellar ages using rotation periods, which change over a star's main sequence lifetime due to magnetic braking. This technique shows promise for main sequence FGKM stars, where other methods are imprecise. However, the observed dispersion in rotation rates for similar coeval stars has historically been difficult to characterize. To properly understand this complexity, we have assembled the largest standardized data catalog of rotators in open clusters to date, consisting of $\approx$8,000 stars across 30 open clusters/associations spanning ages of 1.5 Myr to 4 Gyr. We have also developed ChronoFlow: a flexible data-driven model which accurately captures observed rotational dispersion. We show that ChronoFlow can be used to accurately forward model rotational evolution, and to infer both cluster and individual stellar ages. We recover cluster ages with a statistical uncertainty of 0.06 dex ($\approx$15%), and individual stellar ages with a statistical uncertainty of 0.7 dex. Additionally, we conducted robust systematic tests to analyze the impact of extinction models, cluster membership, and calibration ages. These contribute an additional 0.06 dex of uncertainty in cluster age estimates, resulting in a total error budget of 0.08 dex. We apply ChronoFlow to estimate ages for M34, NGC 2516, NGC 6709, and the Theia 456 stellar stream. Our results show that ChronoFlow can precisely estimate the ages of coeval stellar populations, and constrain ages for individual stars. Furthermore, its predictions may be used to inform physical spin down models. ChronoFlow is publicly available at https://github.com/philvanlane/chronoflow.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum framework for Reinforcement Learning: integrating Markov Decision Process, quantum arithmetic, and trajectory search</title>
<link>https://arxiv.org/abs/2412.18208</link>
<guid>https://arxiv.org/abs/2412.18208</guid>
<content:encoded><![CDATA[

arXiv:2412.18208v2 Announce Type: replace-cross 
Abstract: This paper introduces a quantum framework for addressing reinforcement learning (RL) tasks, grounded in the quantum principles and leveraging a fully quantum model of the classical Markov Decision Process (MDP). By employing quantum concepts and a quantum search algorithm, this work presents the implementation and optimization of the agent-environment interactions entirely within the quantum domain, eliminating reliance on classical computations. Key contributions include the quantum-based state transitions, return calculation, and trajectory search mechanism that utilize quantum principles to demonstrate the realization of RL processes through quantum phenomena. The implementation emphasizes the fundamental role of quantum superposition in enhancing computational efficiency for RL tasks. Results demonstrate the capacity of a quantum model to achieve quantum enhancement in RL, highlighting the potential of fully quantum implementations in decision-making tasks. This work not only underscores the applicability of quantum computing in machine learning but also contributes the field of quantum reinforcement learning (QRL) by offering a robust framework for understanding and exploiting quantum computing in RL systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding the Underlying Viscoelastic Constitutive Equation via Universal Differential Equations and Differentiable Physics</title>
<link>https://arxiv.org/abs/2501.00556</link>
<guid>https://arxiv.org/abs/2501.00556</guid>
<content:encoded><![CDATA[

arXiv:2501.00556v2 Announce Type: replace-cross 
Abstract: This research employs Universal Differential Equations (UDEs) alongside differentiable physics to model viscoelastic fluids, merging conventional differential equations, neural networks and numerical methods to reconstruct missing terms in constitutive models. This study focuses on analyzing four viscoelastic models: Upper Convected Maxwell (UCM), Johnson-Segalman, Giesekus, and Exponential Phan-Thien-Tanner (ePTT), through the use of synthetic datasets. The methodology was tested across different experimental conditions, including oscillatory and startup flows. While the UDE framework effectively predicts shear and normal stresses for most models, it demonstrates some limitations when applied to the ePTT model. The findings underscore the potential of UDEs in fluid mechanics while identifying critical areas for methodological improvement. Also, a model distillation approach was employed to extract simplified models from complex ones, emphasizing the versatility and robustness of UDEs in rheological modeling.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics</title>
<link>https://arxiv.org/abs/2501.04686</link>
<guid>https://arxiv.org/abs/2501.04686</guid>
<content:encoded><![CDATA[

arXiv:2501.04686v5 Announce Type: replace-cross 
Abstract: Process Reward Models (PRMs) have shown promise in enhancing the mathematical reasoning capabilities of Large Language Models (LLMs) through Test-Time Scaling (TTS). However, their integration into multimodal reasoning remains largely unexplored. In this work, we take the first step toward unlocking the potential of PRMs in multimodal mathematical reasoning. We identify three key challenges: (1) the scarcity of high-quality reasoning data constrains the capabilities of foundation Multimodal Large Language Models (MLLMs), which imposes further limitations on the upper bounds of TTS and reinforcement learning (RL); (2) a lack of automated methods for process labeling within multimodal contexts persists; (3) the employment of process rewards in unimodal RL faces issues like reward hacking, which may extend to multimodal scenarios. To address these issues, we introduce URSA, a three-stage Unfolding multimodal Process-Supervision Aided training framework. We first construct MMathCoT-1M, a high-quality large-scale multimodal Chain-of-Thought (CoT) reasoning dataset, to build a stronger math reasoning foundation MLLM, URSA-8B. Subsequently, we go through an automatic process to synthesize process supervision data, which emphasizes both logical correctness and perceptual consistency. We introduce DualMath-1.1M to facilitate the training of URSA-8B-RM. Finally, we propose Process-Supervised Group-Relative-Policy-Optimization (PS-GRPO), pioneering a multimodal PRM-aided online RL method that outperforms vanilla GRPO. With PS-GRPO application, URSA-8B-PS-GRPO outperforms Gemma3-12B and GPT-4o by 8.4% and 2.7% on average across 6 benchmarks. Code, data and checkpoint can be found at https://github.com/URSA-MATH.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FIT-Print: Towards False-claim-resistant Model Ownership Verification via Targeted Fingerprint</title>
<link>https://arxiv.org/abs/2501.15509</link>
<guid>https://arxiv.org/abs/2501.15509</guid>
<content:encoded><![CDATA[

arXiv:2501.15509v2 Announce Type: replace-cross 
Abstract: Model fingerprinting is a widely adopted approach to safeguard the copyright of open-source models by detecting and preventing their unauthorized reuse without modifying the protected model. However, in this paper, we reveal that existing fingerprinting methods are vulnerable to false claim attacks where adversaries falsely assert ownership of third-party non-reused models. We find that this vulnerability mostly stems from their untargeted nature, where they generally compare the outputs of given samples on different models instead of the similarities to specific references. Motivated by this finding, we propose a targeted fingerprinting paradigm (i.e., FIT-Print) to counteract false claim attacks. Specifically, FIT-Print transforms the fingerprint into a targeted signature via optimization. Building on the principles of FIT-Print, we develop bit-wise and list-wise black-box model fingerprinting methods, i.e., FIT-ModelDiff and FIT-LIME, which exploit the distance between model outputs and the feature attribution of specific samples as the fingerprint, respectively. Experiments on benchmark models and datasets verify the effectiveness, conferrability, and resistance to false claim attacks of our FIT-Print.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling</title>
<link>https://arxiv.org/abs/2501.16975</link>
<guid>https://arxiv.org/abs/2501.16975</guid>
<content:encoded><![CDATA[

arXiv:2501.16975v2 Announce Type: replace-cross 
Abstract: Tokenization is a fundamental component of large language models (LLMs), yet its influence on model scaling and performance is not fully explored. In this paper, we introduce Over-Tokenized Transformers, a novel framework that decouples input and output vocabularies to improve language modeling performance. Specifically, our approach scales up input vocabularies to leverage multi-gram tokens. Through extensive experiments, we uncover a log-linear relationship between input vocabulary size and training loss, demonstrating that larger input vocabularies consistently enhance model performance, regardless of model size. Using a large input vocabulary, we achieve performance comparable to double-sized baselines with no additional cost. Our findings highlight the importance of tokenization in scaling laws and provide practical insight for tokenizer design, paving the way for more efficient and powerful LLMs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-term prediction of El Ni\~no-Southern Oscillation using reservoir computing with data-driven realtime filter</title>
<link>https://arxiv.org/abs/2501.17781</link>
<guid>https://arxiv.org/abs/2501.17781</guid>
<content:encoded><![CDATA[

arXiv:2501.17781v2 Announce Type: replace-cross 
Abstract: In recent years, the application of machine learning approaches to time-series forecasting of climate dynamical phenomena has become increasingly active. It is known that applying a band-pass filter to a time-series data is a key to obtaining a high-quality data-driven model. Here, to obtain longer-term predictability of machine learning models, we introduce a new type of band-pass filter. It can be applied to realtime operational prediction workflows since it relies solely on past time series. We combine the filter with reservoir computing, which is a machine-learning technique that employs a data-driven dynamical system. As an application, we predict the multi-year dynamics of the El Ni\~{n}o-Southern Oscillation with the prediction horizon of 24 months using only past time series.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Feature Representation Boosting</title>
<link>https://arxiv.org/abs/2501.18283</link>
<guid>https://arxiv.org/abs/2501.18283</guid>
<content:encoded><![CDATA[

arXiv:2501.18283v2 Announce Type: replace-cross 
Abstract: We introduce Random Feature Representation Boosting (RFRBoost), a novel method for constructing deep residual random feature neural networks (RFNNs) using boosting theory. RFRBoost uses random features at each layer to learn the functional gradient of the network representation, enhancing performance while preserving the convex optimization benefits of RFNNs. In the case of MSE loss, we obtain closed-form solutions to greedy layer-wise boosting with random features. For general loss functions, we show that fitting random feature residual blocks reduces to solving a quadratically constrained least squares problem. Through extensive numerical experiments on tabular datasets for both regression and classification, we show that RFRBoost significantly outperforms RFNNs and end-to-end trained MLP ResNets in the small- to medium-scale regime where RFNNs are typically applied. Moreover, RFRBoost offers substantial computational benefits, and theoretical guarantees stemming from boosting theory.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REG: Rectified Gradient Guidance for Conditional Diffusion Models</title>
<link>https://arxiv.org/abs/2501.18865</link>
<guid>https://arxiv.org/abs/2501.18865</guid>
<content:encoded><![CDATA[

arXiv:2501.18865v2 Announce Type: replace-cross 
Abstract: Guidance techniques are simple yet effective for improving conditional generation in diffusion models. Albeit their empirical success, the practical implementation of guidance diverges significantly from its theoretical motivation. In this paper, we reconcile this discrepancy by replacing the scaled marginal distribution target, which we prove theoretically invalid, with a valid scaled joint distribution objective. Additionally, we show that the established guidance implementations are approximations to the intractable optimal solution under no future foresight constraint. Building on these theoretical insights, we propose rectified gradient guidance (REG), a versatile enhancement designed to boost the performance of existing guidance methods. Experiments on 1D and 2D demonstrate that REG provides a better approximation to the optimal solution than prior guidance techniques, validating the proposed theoretical framework. Extensive experiments on class-conditional ImageNet and text-to-image generation tasks show that incorporating REG consistently improves FID and Inception/CLIP scores across various settings compared to its absence.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and Guarantees</title>
<link>https://arxiv.org/abs/2502.01027</link>
<guid>https://arxiv.org/abs/2502.01027</guid>
<content:encoded><![CDATA[

arXiv:2502.01027v2 Announce Type: replace-cross 
Abstract: Two-stage Learning-to-Defer (L2D) enables optimal task delegation by assigning each input to either a fixed main model or one of several offline experts, supporting reliable decision-making in complex, multi-agent environments. However, existing L2D frameworks assume clean inputs and are vulnerable to adversarial perturbations that can manipulate query allocation--causing costly misrouting or expert overload. We present the first comprehensive study of adversarial robustness in two-stage L2D systems. We introduce two novel attack strategie--untargeted and targeted--which respectively disrupt optimal allocations or force queries to specific agents. To defend against such threats, we propose SARD, a convex learning algorithm built on a family of surrogate losses that are provably Bayes-consistent and $(\mathcal{R}, \mathcal{G})$-consistent. These guarantees hold across classification, regression, and multi-task settings. Empirical results demonstrate that SARD significantly improves robustness under adversarial attacks while maintaining strong clean performance, marking a critical step toward secure and trustworthy L2D deployment.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Bidding Algorithms with Strict Return on Spend (ROS) Constraint</title>
<link>https://arxiv.org/abs/2502.05599</link>
<guid>https://arxiv.org/abs/2502.05599</guid>
<content:encoded><![CDATA[

arXiv:2502.05599v2 Announce Type: replace-cross 
Abstract: Auto-bidding problem under a strict return-on-spend constraint (ROSC) is considered, where an algorithm has to make decisions about how much to bid for an ad slot depending on the revealed value, and the hidden allocation and payment function that describes the probability of winning the ad-slot depending on its bid. The objective of an algorithm is to maximize the expected utility (product of ad value and probability of winning the ad slot) summed across all time slots subject to the total expected payment being less than the total expected utility, called the ROSC. A (surprising) impossibility result is derived that shows that no online algorithm can achieve a sub-linear regret even when the value, allocation and payment function are drawn i.i.d. from an unknown distribution. The problem is non-trivial even when the revealed value remains constant across time slots, and an algorithm with regret guarantee that is optimal up to logarithmic factor is derived.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Impact of the Utility in Semivalue-based Data Valuation</title>
<link>https://arxiv.org/abs/2502.06574</link>
<guid>https://arxiv.org/abs/2502.06574</guid>
<content:encoded><![CDATA[

arXiv:2502.06574v2 Announce Type: replace-cross 
Abstract: Semivalue-based data valuation uses cooperative-game theory intuitions to assign each data point a value reflecting its contribution to a downstream task. Still, those values depend on the practitioner's choice of utility, raising the question: How robust is semivalue-based data valuation to changes in the utility? This issue is critical when the utility is set as a trade-off between several criteria and when practitioners must select among multiple equally valid utilities. We address it by introducing the notion of a dataset's spatial signature: given a semivalue, we embed each data point into a lower-dimensional space where any utility becomes a linear functional, making the data valuation framework amenable to a simpler geometric picture. Building on this, we propose a practical methodology centered on an explicit robustness metric that informs practitioners whether and by how much their data valuation results will shift as the utility changes. We validate this approach across diverse datasets and semivalues, demonstrating strong agreement with rank-correlation analyses and offering analytical insight into how choosing a semivalue can amplify or diminish robustness.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iLOCO: Distribution-Free Inference for Feature Interactions</title>
<link>https://arxiv.org/abs/2502.06661</link>
<guid>https://arxiv.org/abs/2502.06661</guid>
<content:encoded><![CDATA[

arXiv:2502.06661v2 Announce Type: replace-cross 
Abstract: Feature importance measures are widely studied and are essential for understanding model behavior, guiding feature selection, and enhancing interpretability. However, many machine learning fitted models involve complex interactions between features. Existing feature importance metrics fail to capture these pairwise or higher-order effects, while existing interaction metrics often suffer from limited applicability or excessive computation; no methods exist to conduct statistical inference for feature interactions. To bridge this gap, we first propose a new model-agnostic metric, interaction Leave-One-Covariate-Out (iLOCO), for measuring the importance of pairwise feature interactions, with extensions to higher-order interactions. Next, we leverage recent advances in LOCO inference to develop distribution-free and assumption-light confidence intervals for our iLOCO metric. To address computational challenges, we also introduce an ensemble learning method for calculating the iLOCO metric and confidence intervals that we show is both computationally and statistically efficient. We validate our iLOCO metric and our confidence intervals on both synthetic and real data sets, showing that our approach outperforms existing methods and provides the first inferential approach to detecting feature interactions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid Whole Brain Motion-robust Mesoscale In-vivo MR Imaging using Multi-scale Implicit Neural Representation</title>
<link>https://arxiv.org/abs/2502.08634</link>
<guid>https://arxiv.org/abs/2502.08634</guid>
<content:encoded><![CDATA[

arXiv:2502.08634v2 Announce Type: replace-cross 
Abstract: High-resolution whole-brain in vivo MR imaging at mesoscale resolutions remains challenging due to long scan durations, motion artifacts, and limited signal-to-noise ratio (SNR). This study proposes Rotating-view super-resolution (ROVER)-MRI, an unsupervised framework based on multi-scale implicit neural representations (INR), enabling efficient recovery of fine anatomical details from multi-view thick-slice acquisitions. ROVER-MRI employs coordinate-based neural networks to implicitly and continuously encode image structures at multiple spatial scales, simultaneously modeling anatomical continuity and correcting inter-view motion through an integrated registration mechanism. Validation on ex-vivo monkey brain data and multiple in-vivo human datasets demonstrates substantially improved reconstruction performance compared to bicubic interpolation and state-of-the-art regularized least-squares super-resolution reconstruction (LS-SRR) with 2-fold reduction in scan time. Notably, ROVER-MRI achieves an unprecedented whole-brain in-vivo T2-weighted imaging at 180 micron isotropic resolution in only 17 minutes of scan time on a 7T scanner with 22.4% lower relative error compared to LS-SRR. We also demonstrate improved SNR using ROVER-MRI compared to a time-matched 3D GRE acquisition. Quantitative results on several datasets demonstrate better sharpness of the reconstructed images with ROVER-MRI for different super-resolution factors (5 to 11). These findings highlight ROVER-MRI's potential as a rapid, accurate, and motion-resilient mesoscale imaging solution, promising substantial advantages for neuroimaging studies.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without Compromising Quality</title>
<link>https://arxiv.org/abs/2502.08923</link>
<guid>https://arxiv.org/abs/2502.08923</guid>
<content:encoded><![CDATA[

arXiv:2502.08923v2 Announce Type: replace-cross 
Abstract: We introduce CopySpec, a simple yet effective technique to tackle the inefficiencies LLMs face when generating responses that closely resemble previous outputs or responses that can be verbatim extracted from context. CopySpec identifies repeated sequences in the model's chat history or context and speculates that the same tokens will follow, enabling seamless copying without compromising output quality and without requiring additional GPU memory. To evaluate the effectiveness of our approach, we conducted experiments using seven LLMs and five datasets: MT-Bench, CNN/DM, GSM8K, HumanEval, and our newly created dataset, MT-Redundant. MT-Redundant, introduced in this paper, transforms the second turn of MT-Bench into a request for variations of the first turn's answer, simulating real-world scenarios where users request modifications to prior responses. Our results demonstrate significant speed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select MT-Redundant categories, and 2.66x on the third turn of GSM8K's self-correction tasks. Importantly, we show that CopySpec integrates seamlessly with speculative decoding, yielding an average 49% additional speed-up over speculative decoding for the second turn of MT-Redundant across all eight categories. While LLMs, even with speculative decoding, suffer from slower inference as context size grows, CopySpec leverages larger contexts to accelerate inference, making it a faster complementary solution. Our code and dataset are publicly available at https://github.com/RazvanDu/CopySpec.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Reasoning</title>
<link>https://arxiv.org/abs/2502.10440</link>
<guid>https://arxiv.org/abs/2502.10440</guid>
<content:encoded><![CDATA[

arXiv:2502.10440v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly integrated into real-world personalized applications through retrieval-augmented generation (RAG) mechanisms to supplement their responses with domain-specific knowledge. However, the valuable and often proprietary nature of the knowledge bases used in RAG introduces the risk of unauthorized usage by adversaries. Existing methods that can be generalized as watermarking techniques to protect these knowledge bases typically involve poisoning or backdoor attacks. However, these methods require altering the LLM's results of verification samples, inevitably making these watermarks susceptible to anomaly detection and even introducing new security risks. To address these challenges, we propose \name{} for `harmless' copyright protection of knowledge bases. Instead of manipulating LLM's final output, \name{} implants distinct yet benign verification behaviors in the space of chain-of-thought (CoT) reasoning, maintaining the correctness of the final answer. Our method has three main stages: (1) Generating CoTs: For each verification question, we generate two `innocent' CoTs, including a target CoT for building watermark behaviors; (2) Optimizing Watermark Phrases and Target CoTs: Inspired by our theoretical analysis, we optimize them to minimize retrieval errors under the \emph{black-box} and \emph{text-only} setting of suspicious LLM, ensuring that only watermarked verification queries can retrieve their correspondingly target CoTs contained in the knowledge base; (3) Ownership Verification: We exploit a pairwise Wilcoxon test to verify whether a suspicious LLM is augmented with the protected knowledge base by comparing its responses to watermarked and benign verification queries. Our experiments on diverse benchmarks demonstrate that \name{} effectively protects knowledge bases and its resistance to adaptive attacks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective</title>
<link>https://arxiv.org/abs/2502.17262</link>
<guid>https://arxiv.org/abs/2502.17262</guid>
<content:encoded><![CDATA[

arXiv:2502.17262v2 Announce Type: replace-cross 
Abstract: The escalating scale and cost of Large Language Models (LLMs) training necessitate accurate pre-training prediction of downstream task performance for efficient resource allocation. This is challenged by: 1) the emergence phenomenon, where metrics become meaningful only after extensive training, hindering prediction by smaller models; and 2) uneven task difficulty and inconsistent performance scaling patterns, leading to high metric variability. Current prediction methods lack accuracy and reliability. We propose a Clustering-On-Difficulty (COD) framework for downstream performance prediction. The COD framework clusters tasks by their difficulty scaling features, thereby establishing a more stable and predictable support subset through the exclusion of tasks exhibiting non-emergent behavior or irregular scaling. We adopt a performance scaling law to predict cluster-wise performance with theoretical support. Predictable subset performance acts as an intermediate predictor for the full evaluation set. We further derive a mapping function to accurately extrapolate the performance of the subset to the full set. Applied to an LLM with 70B parameters, COD achieved a 1.36% average prediction error across eight key LLM benchmarks, offering actionable insights for resource allocation and training monitoring of LLMs pretraining.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Retrieval-Augmented Language Models Adapt to Varying User Needs?</title>
<link>https://arxiv.org/abs/2502.19779</link>
<guid>https://arxiv.org/abs/2502.19779</guid>
<content:encoded><![CDATA[

arXiv:2502.19779v2 Announce Type: replace-cross 
Abstract: Recent advancements in Retrieval-Augmented Language Models (RALMs) have demonstrated their efficacy in knowledge-intensive tasks. However, existing evaluation benchmarks often assume a single optimal approach to leveraging retrieved information, failing to account for varying user needs. This paper introduces a novel evaluation framework that systematically assesses RALMs under three user need cases-Context-Exclusive, Context-First, and Memory-First-across three distinct context settings: Context Matching, Knowledge Conflict, and Information Irrelevant. By varying both user instructions and the nature of retrieved information, our approach captures the complexities of real-world applications where models must adapt to diverse user requirements. Through extensive experiments on multiple QA datasets, including HotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find that restricting memory usage improves robustness in adversarial retrieval conditions but decreases peak performance with ideal retrieval results and model family dominates behavioral differences. Our findings highlight the necessity of user-centric evaluations in the development of retrieval-augmented systems and provide insights into optimizing model performance across varied retrieval contexts. We will release our code and URAQ dataset upon acceptance of the paper.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Causal Reasoning Evaluation in Language Models</title>
<link>https://arxiv.org/abs/2503.04556</link>
<guid>https://arxiv.org/abs/2503.04556</guid>
<content:encoded><![CDATA[

arXiv:2503.04556v3 Announce Type: replace-cross 
Abstract: Causal reasoning and compositional reasoning are two core aspirations in AI. Measuring the extent of these behaviors requires principled evaluation methods. We explore a unified perspective that considers both behaviors simultaneously, termed compositional causal reasoning (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate through graphs. We instantiate a framework for the systematic evaluation of CCR for the average treatment effect and the probability of necessity and sufficiency. As proof of concept, we demonstrate CCR evaluation for language models in the LLama, Phi, and GPT families. On a math word problem, our framework revealed a range of taxonomically distinct error patterns. CCR errors increased with the complexity of causal paths for all models except o1.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier-Based 3D Multistage Transformer for Aberration Correction in Multicellular Specimens</title>
<link>https://arxiv.org/abs/2503.12593</link>
<guid>https://arxiv.org/abs/2503.12593</guid>
<content:encoded><![CDATA[

arXiv:2503.12593v2 Announce Type: replace-cross 
Abstract: High-resolution tissue imaging is often compromised by sample-induced optical aberrations that degrade resolution and contrast. While wavefront sensor-based adaptive optics (AO) can measure these aberrations, such hardware solutions are typically complex, expensive to implement, and slow when serially mapping spatially varying aberrations across large fields of view. Here, we introduce AOViFT (Adaptive Optical Vision Fourier Transformer) -- a machine learning-based aberration sensing framework built around a 3D multistage Vision Transformer that operates on Fourier domain embeddings. AOViFT infers aberrations and restores diffraction-limited performance in puncta-labeled specimens with substantially reduced computational cost, training time, and memory footprint compared to conventional architectures or real-space networks. We validated AOViFT on live gene-edited zebrafish embryos, demonstrating its ability to correct spatially varying aberrations using either a deformable mirror or post-acquisition deconvolution. By eliminating the need for the guide star and wavefront sensing hardware and simplifying the experimental workflow, AOViFT lowers technical barriers for high-resolution volumetric microscopy across diverse biological samples.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TripNet: Learning Large-scale High-fidelity 3D Car Aerodynamics with Triplane Networks</title>
<link>https://arxiv.org/abs/2503.17400</link>
<guid>https://arxiv.org/abs/2503.17400</guid>
<content:encoded><![CDATA[

arXiv:2503.17400v2 Announce Type: replace-cross 
Abstract: Surrogate modeling has emerged as a powerful tool to accelerate Computational Fluid Dynamics (CFD) simulations. Existing 3D geometric learning models based on point clouds, voxels, meshes, or graphs depend on explicit geometric representations that are memory-intensive and resolution-limited. For large-scale simulations with millions of nodes and cells, existing models require aggressive downsampling due to their dependence on mesh resolution, resulting in degraded accuracy. We present TripNet, a triplane-based neural framework that implicitly encodes 3D geometry into a compact, continuous feature map with fixed dimension. Unlike mesh-dependent approaches, TripNet scales to high-resolution simulations without increasing memory cost, and enables CFD predictions at arbitrary spatial locations in a query-based fashion, independent of mesh connectivity or predefined nodes. TripNet achieves state-of-the-art performance on the DrivAerNet and DrivAerNet++ datasets, accurately predicting drag coefficients, surface pressure, and full 3D flow fields. With a unified triplane backbone supporting multiple simulation tasks, TripNet offers a scalable, accurate, and efficient alternative to traditional CFD solvers and existing surrogate models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFH: A Multi-faceted Heuristic Algorithm Selection Approach for Software Verification</title>
<link>https://arxiv.org/abs/2503.22228</link>
<guid>https://arxiv.org/abs/2503.22228</guid>
<content:encoded><![CDATA[

arXiv:2503.22228v2 Announce Type: replace-cross 
Abstract: Currently, many verification algorithms are available to improve the reliability of software systems. Selecting the appropriate verification algorithm typically demands domain expertise and non-trivial manpower. An automated algorithm selector is thus desired. However, existing selectors, either depend on machine-learned strategies or manually designed heuristics, encounter issues such as reliance on high-quality samples with algorithm labels and limited scalability. In this paper, an automated algorithm selection approach, namely MFH, is proposed for software verification. Our approach leverages the heuristics that verifiers producing correct results typically implement certain appropriate algorithms, and the supported algorithms by these verifiers indirectly reflect which ones are potentially applicable. Specifically, MFH embeds the code property graph (CPG) of a semantic-preserving transformed program to enhance the robustness of the prediction model. Furthermore, our approach decomposes the selection task into the sub-tasks of predicting potentially applicable algorithms and matching the most appropriate verifiers. Additionally, MFH also introduces a feedback loop on incorrect predictions to improve model prediction accuracy. We evaluate MFH on 20 verifiers and over 15,000 verification tasks. Experimental results demonstrate the effectiveness of MFH, achieving a prediction accuracy of 91.47% even without ground truth algorithm labels provided during the training phase. Moreover, the prediction accuracy decreases only by 0.84% when introducing 10 new verifiers, indicating the strong scalability of the proposed approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets</title>
<link>https://arxiv.org/abs/2504.02792</link>
<guid>https://arxiv.org/abs/2504.02792</guid>
<content:encoded><![CDATA[

arXiv:2504.02792v3 Announce Type: replace-cross 
Abstract: Imitation learning has emerged as a promising approach towards building generalist robots. However, scaling imitation learning for large robot foundation models remains challenging due to its reliance on high-quality expert demonstrations. Meanwhile, large amounts of video data depicting a wide range of environments and diverse behaviors are readily available. This data provides a rich source of information about real-world dynamics and agent-environment interactions. Leveraging this data directly for imitation learning, however, has proven difficult due to the lack of action annotation. In this work, we present Unified World Models (UWM), a framework that allows for leveraging both video and action data for policy learning. Specifically, a UWM integrates an action diffusion process and a video diffusion process within a unified transformer architecture, where independent diffusion timesteps govern each modality. By controlling each diffusion timestep, UWM can flexibly represent a policy, a forward dynamics, an inverse dynamics, and a video generator. Through simulated and real-world experiments, we show that: (1) UWM enables effective pretraining on large-scale multitask robot datasets with both dynamics and action predictions, resulting in more generalizable and robust policies than imitation learning, (2) UWM naturally facilitates learning from action-free video data through independent control of modality-specific diffusion timesteps, further improving the performance of finetuned policies. Our results suggest that UWM offers a promising step toward harnessing large, heterogeneous datasets for scalable robot learning, and provides a simple unification between the often disparate paradigms of imitation learning and world modeling. Videos and code are available at https://weirdlabuw.github.io/uwm/.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization</title>
<link>https://arxiv.org/abs/2504.05804</link>
<guid>https://arxiv.org/abs/2504.05804</guid>
<content:encoded><![CDATA[

arXiv:2504.05804v2 Announce Type: replace-cross 
Abstract: The integration of large language models (LLMs) into information retrieval systems introduces new attack surfaces, particularly for adversarial ranking manipulations. We present $\textbf{StealthRank}$, a novel adversarial attack method that manipulates LLM-driven ranking systems while maintaining textual fluency and stealth. Unlike existing methods that often introduce detectable anomalies, StealthRank employs an energy-based optimization framework combined with Langevin dynamics to generate StealthRank Prompts (SRPs)-adversarial text sequences embedded within item or document descriptions that subtly yet effectively influence LLM ranking mechanisms. We evaluate StealthRank across multiple LLMs, demonstrating its ability to covertly boost the ranking of target items while avoiding explicit manipulation traces. Our results show that StealthRank consistently outperforms state-of-the-art adversarial ranking baselines in both effectiveness and stealth, highlighting critical vulnerabilities in LLM-driven ranking systems. Our code is publicly available at $\href{https://github.com/Tangyiming205069/controllable-seo}{here}$.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog</title>
<link>https://arxiv.org/abs/2504.07199</link>
<guid>https://arxiv.org/abs/2504.07199</guid>
<content:encoded><![CDATA[

arXiv:2504.07199v3 Announce Type: replace-cross 
Abstract: We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated subject tagging for scientific and technical records in English and German using the GND taxonomy. Participants developed LLM-based systems to recommend top-k subjects, evaluated through quantitative metrics (precision, recall, F1-score) and qualitative assessments by subject specialists. Results highlight the effectiveness of LLM ensembles, synthetic data generation, and multilingual processing, offering insights into applying LLMs for digital library classification.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning</title>
<link>https://arxiv.org/abs/2504.15275</link>
<guid>https://arxiv.org/abs/2504.15275</guid>
<content:encoded><![CDATA[

arXiv:2504.15275v2 Announce Type: replace-cross 
Abstract: Process reward models (PRMs) have proven effective for test-time scaling of Large Language Models (LLMs) on challenging reasoning tasks. However, reward hacking issues with PRMs limit their successful application in reinforcement fine-tuning. In this paper, we identify the main cause of PRM-induced reward hacking: the canonical summation-form credit assignment in reinforcement learning (RL), which defines the value as cumulative gamma-decayed future rewards, easily induces LLMs to hack steps with high rewards. To address this, we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation of PURE is a min-form credit assignment that formulates the value function as the minimum of future rewards. This method significantly alleviates reward hacking by limiting the value function range and distributing advantages more reasonably. Through extensive experiments on 3 base models, we show that PRM-based approaches enabling min-form credit assignment achieve comparable reasoning performance to verifiable reward-based methods within only 30% steps. In contrast, the canonical sum-form credit assignment collapses training even at the beginning! Additionally, when we supplement PRM-based fine-tuning with just 10% verifiable rewards, we further alleviate reward hacking and produce the best fine-tuned model based on Qwen2.5-Math-7B in our experiments, achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5 benchmarks. Moreover, we summarize the observed reward hacking cases and analyze the causes of training collapse. Code and models are available at https://github.com/CJReinforce/PURE.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention</title>
<link>https://arxiv.org/abs/2504.16083</link>
<guid>https://arxiv.org/abs/2504.16083</guid>
<content:encoded><![CDATA[

arXiv:2504.16083v2 Announce Type: replace-cross 
Abstract: The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at https://aka.ms/MMInference.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation</title>
<link>https://arxiv.org/abs/2505.00022</link>
<guid>https://arxiv.org/abs/2505.00022</guid>
<content:encoded><![CDATA[

arXiv:2505.00022v2 Announce Type: replace-cross 
Abstract: Scaling data quantity is essential for large language models (LLMs), yet recent findings show that data quality can significantly boost performance and training efficiency. We introduce a German-language dataset curation pipeline that combines heuristic and model-based filtering techniques with synthetic data generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a large-scale German pre-training dataset which draws from: (1) Common Crawl web data, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual, organic web data. We evaluate our dataset by pre-training both a 1B Llama-style model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A comparison on German-language benchmarks, including MMMLU, shows significant performance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage holds at the 8B scale even when FineWeb2 is enriched by human-curated high-quality data sources such as Wikipedia. Our findings support the growing body of evidence that model-based data curation and synthetic data generation can significantly enhance LLM pre-training datasets.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate to Poincar\'e inequalities on manifolds for dimension reduction in nonlinear feature spaces</title>
<link>https://arxiv.org/abs/2505.01807</link>
<guid>https://arxiv.org/abs/2505.01807</guid>
<content:encoded><![CDATA[

arXiv:2505.01807v2 Announce Type: replace-cross 
Abstract: We aim to approximate a continuously differentiable function $u:\mathbb{R}^d \rightarrow \mathbb{R}$ by a composition of functions $f\circ g$ where $g:\mathbb{R}^d \rightarrow \mathbb{R}^m$, $m\leq d$, and $f : \mathbb{R}^m \rightarrow \mathbb{R}$ are built in a two stage procedure. For a fixed $g$, we build $f$ using classical regression methods, involving evaluations of $u$. Recent works proposed to build a nonlinear $g$ by minimizing a loss function $\mathcal{J}(g)$ derived from Poincar\'e inequalities on manifolds, involving evaluations of the gradient of $u$. A problem is that minimizing $\mathcal{J}$ may be a challenging task. Hence in this work, we introduce new convex surrogates to $\mathcal{J}$. Leveraging concentration inequalities, we provide sub-optimality results for a class of functions $g$, including polynomials, and a wide class of input probability measures. We investigate performances on different benchmarks for various training sample sizes. We show that our approach outperforms standard iterative methods for minimizing the training Poincar\'e inequality based loss, often resulting in better approximation errors, especially for rather small training sets and $m=1$.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP</title>
<link>https://arxiv.org/abs/2505.05528</link>
<guid>https://arxiv.org/abs/2505.05528</guid>
<content:encoded><![CDATA[

arXiv:2505.05528v2 Announce Type: replace-cross 
Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly adopted for diverse downstream tasks and integrated into large vision-language models (VLMs), their susceptibility to adversarial perturbations has emerged as a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel attack method that exposes a universal adversarial vulnerability in CLIP. X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of deceiving various CLIP encoders and downstream VLMs across different samples, tasks, and domains. We refer to this property as \textbf{super transferability}--a single perturbation achieving cross-data, cross-domain, cross-model, and cross-task adversarial transferability simultaneously. This is achieved through \textbf{surrogate scaling}, a key innovation of our approach. Unlike existing methods that rely on fixed surrogate models, which are computationally intensive to scale, X-Transfer employs an efficient surrogate scaling strategy that dynamically selects a small subset of suitable surrogates from a large search space. Extensive evaluations demonstrate that X-Transfer significantly outperforms previous state-of-the-art UAP methods, establishing a new benchmark for adversarial transferability across CLIP models. The code is publicly available in our \href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hamiltonian of Poly-matrix Zero-sum Games</title>
<link>https://arxiv.org/abs/2505.12609</link>
<guid>https://arxiv.org/abs/2505.12609</guid>
<content:encoded><![CDATA[

arXiv:2505.12609v2 Announce Type: replace-cross 
Abstract: Understanding a dynamical system fundamentally relies on establishing an appropriate Hamiltonian function and elucidating its symmetries. By formulating agents' strategies and cumulative payoffs as canonically conjugate variables, we identify the Hamiltonian function that generates the dynamics of poly-matrix zero-sum games. We reveal the symmetries of our Hamiltonian and derive the associated conserved quantities, showing how the conservation of probability and the invariance of the Fenchel coupling are intrinsically encoded within the system. Furthermore, we propose the dissipation FTRL (DFTRL) dynamics by introducing a perturbation that dissipates the Fenchel coupling, proving convergence to the Nash equilibrium and linking DFTRL to last-iterate convergent algorithms. Our results highlight the potential of Hamiltonian dynamics in uncovering the structural properties of learning dynamics in games, and pave the way for broader applications of Hamiltonian dynamics in game theory and machine learning.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Selection for Gaussian-gated Gaussian Mixture of Experts Using Dendrograms of Mixing Measures</title>
<link>https://arxiv.org/abs/2505.13052</link>
<guid>https://arxiv.org/abs/2505.13052</guid>
<content:encoded><![CDATA[

arXiv:2505.13052v2 Announce Type: replace-cross 
Abstract: Mixture of Experts (MoE) models constitute a widely utilized class of ensemble learning approaches in statistics and machine learning, known for their flexibility and computational efficiency. They have become integral components in numerous state-of-the-art deep neural network architectures, particularly for analyzing heterogeneous data across diverse domains. Despite their practical success, the theoretical understanding of model selection, especially concerning the optimal number of mixture components or experts, remains limited and poses significant challenges. These challenges primarily stem from the inclusion of covariates in both the Gaussian gating functions and expert networks, which introduces intrinsic interactions governed by partial differential equations with respect to their parameters. In this paper, we revisit the concept of dendrograms of mixing measures and introduce a novel extension to Gaussian-gated Gaussian MoE models that enables consistent estimation of the true number of mixture components and achieves the pointwise optimal convergence rate for parameter estimation in overfitted scenarios. Notably, this approach circumvents the need to train and compare a range of models with varying numbers of components, thereby alleviating the computational burden, particularly in high-dimensional or deep neural network settings. Experimental results on synthetic data demonstrate the effectiveness of the proposed method in accurately recovering the number of experts. It outperforms common criteria such as the Akaike information criterion, the Bayesian information criterion, and the integrated completed likelihood, while achieving optimal convergence rates for parameter estimation and accurately approximating the regression function.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimum-Excess-Work Guidance</title>
<link>https://arxiv.org/abs/2505.13375</link>
<guid>https://arxiv.org/abs/2505.13375</guid>
<content:encoded><![CDATA[

arXiv:2505.13375v3 Announce Type: replace-cross 
Abstract: We propose a regularization framework inspired by thermodynamic work for guiding pre-trained probability flow generative models (e.g., continuous normalizing flows or diffusion models) by minimizing excess work, a concept rooted in statistical mechanics and with strong conceptual connections to optimal transport. Our approach enables efficient guidance in sparse-data regimes common to scientific applications, where only limited target samples or partial density constraints are available. We introduce two strategies: Path Guidance for sampling rare transition states by concentrating probability mass on user-defined subsets, and Observable Guidance for aligning generated distributions with experimental observables while preserving entropy. We demonstrate the framework's versatility on a coarse-grained protein model, guiding it to sample transition configurations between folded/unfolded states and correct systematic biases using experimental data. The method bridges thermodynamic principles with modern generative architectures, offering a principled, efficient, and physics-inspired alternative to standard fine-tuning in data-scarce domains. Empirical results highlight improved sample efficiency and bias reduction, underscoring its applicability to molecular simulations and beyond.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>SHAP zero Explains Biological Sequence Models with Near-zero Marginal Cost for Future Queries</title>
<link>https://arxiv.org/abs/2410.19236</link>
<guid>https://arxiv.org/abs/2410.19236</guid>
<content:encoded><![CDATA[
<div> Shapley values, machine learning, biological sequences, interpretability, scalability  
Summary:<br /><br />SHAP zero is a new algorithm that addresses the need for interpretable predictions in machine learning models for biological sequences. By amortizing the cost of Shapley value computation across large datasets, SHAP zero significantly reduces the computational burden of extracting global biological insights. The algorithm leverages the sparse Fourier transform of the model to uncover high-order feature interactions, leading to faster and more efficient explanation of predictions for models of guide RNA efficacy, DNA repair outcomes, and protein fitness. SHAP zero enables the exploration of rich combinatorial interactions that were previously inaccessible at scale, offering a scalable and principled approach to interpretability for black-box sequence models in biology.<br /><br /> <div>
arXiv:2410.19236v4 Announce Type: replace 
Abstract: The growing adoption of machine learning models for biological sequences has intensified the need for interpretable predictions, with Shapley values emerging as a theoretically grounded standard for model explanation. While effective for local explanations of individual input sequences, scaling Shapley-based interpretability to extract global biological insights requires evaluating thousands of sequences--incurring exponential computational cost per query. We introduce SHAP zero, a novel algorithm that amortizes the cost of Shapley value computation across large-scale biological datasets. After a one-time model sketching step, SHAP zero enables near-zero marginal cost for future queries by uncovering an underexplored connection between Shapley values, high-order feature interactions, and the sparse Fourier transform of the model. Applied to models of guide RNA efficacy, DNA repair outcomes, and protein fitness, SHAP zero explains predictions orders of magnitude faster than existing methods, recovering rich combinatorial interactions previously inaccessible at scale. This work opens the door to principled, efficient, and scalable interpretability for black-box sequence models in biology.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing the Influence of Labels on Unlabeled Nodes in Graph Convolutional Networks</title>
<link>https://arxiv.org/abs/2411.02279</link>
<guid>https://arxiv.org/abs/2411.02279</guid>
<content:encoded><![CDATA[
<div> Keywords: graph convolutional networks, message-passing mechanism, ELU-GCN, graph learning, graph contrastive learning

Summary:
The article introduces a novel framework called ELU-GCN that enhances the performance of graph convolutional networks (GCNs). The first stage of ELU-GCN involves graph learning to create a new graph structure, known as ELU-graph, which facilitates the positive influence of additional label information on GCN predictions. In the second stage, a graph contrastive learning approach is employed to further improve representation learning by comparing the learned ELU graph with the original graph and exploring their consistency and mutually exclusive information. The theoretical analysis provided in the article demonstrates that the proposed method can enhance the generalization ability of GCNs. Experimental results confirm the effectiveness and superiority of ELU-GCN in comparison to existing methods. 
<br /><br />Summary: <div>
arXiv:2411.02279v3 Announce Type: replace 
Abstract: The message-passing mechanism of graph convolutional networks (i.e., GCNs) enables label information to reach more unlabeled neighbors, thereby increasing the utilization of labels. However, the additional label information does not always contribute positively to the GCN. To address this issue, we propose a new two-step framework called ELU-GCN. In the first stage, ELU-GCN conducts graph learning to learn a new graph structure (i.e., ELU-graph), which allows the additional label information to positively influence the predictions of GCN. In the second stage, we design a new graph contrastive learning on the GCN framework for representation learning by exploring the consistency and mutually exclusive information between the learned ELU graph and the original graph. Moreover, we theoretically demonstrate that the proposed method can ensure the generalization ability of GCNs. Extensive experiments validate the superiority of our method.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal Large Language Models</title>
<link>https://arxiv.org/abs/2505.11654</link>
<guid>https://arxiv.org/abs/2505.11654</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban dynamics prediction, Large Language Models, Spatial-temporal data, Generalization, Test time adaptation

Summary: 
UrbanMind is a spatial-temporal Large Language Model (LLM) framework designed for accurate and robust prediction of multifaceted urban dynamics. It introduces Muffin-MAE, a masked autoencoder that captures complex spatial-temporal dependencies, and a semantic-aware prompting strategy to encode contextual details for improved reasoning. The framework includes a test time adaptation mechanism with a data reconstructor to enhance generalization by adjusting to unseen test data. Extensive experiments on real-world urban datasets show that UrbanMind outperforms existing methods in accuracy and generalization, even in zero-shot scenarios. By effectively integrating multifaceted spatial-temporal data and addressing distributional shifts between training and testing data, UrbanMind demonstrates strong potential for practical applications in urban planning, transportation management, and public service optimization. 

<br /><br />Summary: <div>
arXiv:2505.11654v3 Announce Type: replace 
Abstract: Understanding and predicting urban dynamics is crucial for managing transportation systems, optimizing urban planning, and enhancing public services. While neural network-based approaches have achieved success, they often rely on task-specific architectures and large volumes of data, limiting their ability to generalize across diverse urban scenarios. Meanwhile, Large Language Models (LLMs) offer strong reasoning and generalization capabilities, yet their application to spatial-temporal urban dynamics remains underexplored. Existing LLM-based methods struggle to effectively integrate multifaceted spatial-temporal data and fail to address distributional shifts between training and testing data, limiting their predictive reliability in real-world applications. To bridge this gap, we propose UrbanMind, a novel spatial-temporal LLM framework for multifaceted urban dynamics prediction that ensures both accurate forecasting and robust generalization. At its core, UrbanMind introduces Muffin-MAE, a multifaceted fusion masked autoencoder with specialized masking strategies that capture intricate spatial-temporal dependencies and intercorrelations among multifaceted urban dynamics. Additionally, we design a semantic-aware prompting and fine-tuning strategy that encodes spatial-temporal contextual details into prompts, enhancing LLMs' ability to reason over spatial-temporal patterns. To further improve generalization, we introduce a test time adaptation mechanism with a test data reconstructor, enabling UrbanMind to dynamically adjust to unseen test data by reconstructing LLM-generated embeddings. Extensive experiments on real-world urban datasets across multiple cities demonstrate that UrbanMind consistently outperforms state-of-the-art baselines, achieving high accuracy and robust generalization, even in zero-shot settings.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead Adversarial Example Detection</title>
<link>https://arxiv.org/abs/2505.12586</link>
<guid>https://arxiv.org/abs/2505.12586</guid>
<content:encoded><![CDATA[
<div> Detection framework, Deep neural networks, Adversarial examples, Layer-wise inconsistencies, Lightweight

Summary: 
The article introduces a lightweight detection framework for adversarial examples in deep neural networks. Adversarial examples are subtle perturbations that can lead to incorrect predictions in DNNs. The proposed framework leverages internal layer-wise inconsistencies within the target model itself, based on the A Few Large Shifts Assumption. Two strategies, Recovery Testing (RT) and Logit-layer Testing (LT), are introduced to expose internal disruptions caused by adversaries. The method achieves state-of-the-art detection performance on CIFAR-10, CIFAR-100, and ImageNet datasets under standard and adaptive threat models. The framework requires only benign data for calibration and does not compromise clean accuracy. The code for the framework is available on GitHub for further exploration and implementation. Overall, the approach offers efficient and effective detection of adversarial examples without the need for external models or complex architectures.  

<br /><br />Summary: <div>
arXiv:2505.12586v3 Announce Type: replace 
Abstract: Deep neural networks (DNNs) are highly susceptible to adversarial examples--subtle, imperceptible perturbations that can lead to incorrect predictions. While detection-based defenses offer a practical alternative to adversarial training, many existing methods depend on external models, complex architectures, heavy augmentations, or adversarial data, limiting their efficiency and generalizability. We introduce a lightweight, plug-in detection framework that leverages internal layer-wise inconsistencies within the target model itself, requiring only benign data for calibration. Our approach is grounded in the A Few Large Shifts Assumption, which posits that adversarial perturbations typically induce large representation shifts in a small subset of layers. Building on this, we propose two complementary strategies--Recovery Testing (RT) and Logit-layer Testing (LT)--to expose internal disruptions caused by adversaries. Evaluated on CIFAR-10, CIFAR-100, and ImageNet under both standard and adaptive threat models, our method achieves state-of-the-art detection performance with negligible computational overhead and no compromise to clean accuracy. The code is available here: https://github.com/c0510gy/AFLS-AED.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Thinking via Mode Policy Optimization for Social Language Agents</title>
<link>https://arxiv.org/abs/2505.02156</link>
<guid>https://arxiv.org/abs/2505.02156</guid>
<content:encoded><![CDATA[
<div> Hierarchical thinking modes, cognitive control theory, Adaptive Mode Learning, Adaptive Mode Policy Optimization, context-aware mode switching<br />
<br />
Summary: 
The paper introduces the Adaptive Mode Learning (AML) framework to enhance the adaptive thinking ability of language agents during dynamic social interactions. It identifies hierarchical thinking modes based on cognitive control theory and employs the Adaptive Mode Policy Optimization (AMPO) algorithm to optimize context-aware mode switching and reasoning. AML improves social intelligence simulation through multi-granular thinking mode design, context-aware mode switching, and token-efficient reasoning. Experiment results show that AML achieves 15.6% higher task performance than GPT-4o and outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. This highlights the effectiveness of adaptive thinking mode selection and optimization in AMPO over fixed-depth solutions like GRPO. <br /><br /> <div>
arXiv:2505.02156v4 Announce Type: replace-cross 
Abstract: Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current studies. Existing methods either lack this kind of reasoning capability or enforce Long Chain-of-Thought reasoning uniformly across all scenarios, resulting in excessive token usage and inflexible social simulation. To address this, we propose an $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) framework in this paper, aiming to improve the adaptive thinking ability of language agents in dynamic social interactions. To this end, we first identify hierarchical thinking modes ranging from intuitive response to deep deliberation based on the cognitive control theory. We then develop the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm to optimize the context-aware mode switching and reasoning. Our framework advances existing research in three key aspects: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence benchmarks verify that AML achieves 15.6% higher task performance than GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter reasoning chains, demonstrating the advantage of adaptive thinking mode selection and optimization mechanism in AMPO over GRPO's fixed-depth solution.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Tokenization: On the Hop-Overpriority Problem in Tokenized Graph Learning Models</title>
<link>https://arxiv.org/abs/2505.15845</link>
<guid>https://arxiv.org/abs/2505.15845</guid>
<content:encoded><![CDATA[
<div> Graph Transformers, Tokenized Graph Learning Models, Large Language Models, Learnable Graph Token List, hop-overpriority problem
Summary:
Graph Transformers have improved graph machine learning but face high complexity. Tokenized Graph Learning Models (TGLMs) convert graphs into token lists for scalable processing. Existing TGLMs use hand-designed token lists but struggle with balancing local and global signals, especially in heterophilic graphs. To address this, the Learnable Graph Token List (LGTL) is proposed, adapting weights and prioritizing nodes for adaptively emphasizing informative nodes. LGTL can mitigate the hop-overpriority problem and has been validated across Graph Transformers and Graph Large Language Models through empirical and theoretical studies. The LGTL module shows promising results in handling diverse graph learning scenarios, improving the performance of models on benchmarks. 
<br /><br />Summary: <div>
arXiv:2505.15845v1 Announce Type: new 
Abstract: Graph Transformers, leveraging the global attention to capture long-range dependencies in graph structures, have significantly advanced graph machine learning, but face prohibitive computational complexity. Tokenized Graph Learning Models (TGLMs) address this issue by converting graphs into ordered token lists for scalable processing. Besides, TGLMs also empower Large Language Models (LLMs) to handle text-attributed graphs more effectively and thus are also employed in Graph LLMs. However, existing TGLMs rely on hand-designed token lists and their adaptability to diverse graph learning scenarios remains unexplored. In this paper, we first conduct extensive empirical and theoretical preliminary studies for hand-designed token lists. Surprisingly, we identify an unexplored hop-overpriority problem: the common pre-defined token lists overemphasize nearby nodes and overwhelm the ability of TGLMs to balance local and global signals. This phenomenon is especially harmful for heterophilic graphs. To address this problem, we propose the Learnable Graph Token List (LGTL), a plug-and-play module to replace hand-designed token lists in TGLMs. Specifically, LGTL adaptively adjusts the weights across hops and prioritizes informative nodes within hops through a graph attention gate module and a selection module, respectively. In this way, contextually informative nodes can be adaptively emphasized for both homophilic and heterophilic graphs. Besides, we theoretically show that LGTL can address the hop-overpriority problem. Extensive experiments on benchmarks validate the efficacy of LGTL across both Graph Transformers and Graph LLM backbones.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Last Layer Empirical Bayes</title>
<link>https://arxiv.org/abs/2505.15888</link>
<guid>https://arxiv.org/abs/2505.15888</guid>
<content:encoded><![CDATA[
<div> Bayesian neural networks, deep ensembles, uncertainty quantification, last layer empirical Bayes, normalizing flow  
Summary:  
The article discusses the challenges of quantifying uncertainty in neural network predictions and introduces the last layer empirical Bayes (LLEB) approach. LLEB utilizes a learnable prior in the form of a normalizing flow on the last layer to maximize the evidence lower bound. It bridges the gap between Bayesian neural networks and deep ensembles by varying the strength of the prior used. LLEB demonstrates comparable performance to existing methods, suggesting that empirical Bayes is a promising avenue for future research in uncertainty quantification. <div>
arXiv:2505.15888v1 Announce Type: new 
Abstract: The task of quantifying the inherent uncertainty associated with neural network predictions is a key challenge in artificial intelligence. Bayesian neural networks (BNNs) and deep ensembles are among the most prominent approaches to tackle this task. Both approaches produce predictions by computing an expectation of neural network outputs over some distribution on the corresponding weights; this distribution is given by the posterior in the case of BNNs, and by a mixture of point masses for ensembles. Inspired by recent work showing that the distribution used by ensembles can be understood as a posterior corresponding to a learned data-dependent prior, we propose last layer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a normalizing flow, which is then trained to maximize the evidence lower bound; to retain tractability we use the flow only on the last layer. We show why LLEB is well motivated, and how it interpolates between standard BNNs and ensembles in terms of the strength of the prior that they use. LLEB performs on par with existing approaches, highlighting that empirical Bayes is a promising direction for future research in uncertainty quantification.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is (Selective) Round-To-Nearest Quantization All You Need?</title>
<link>https://arxiv.org/abs/2505.15909</link>
<guid>https://arxiv.org/abs/2505.15909</guid>
<content:encoded><![CDATA[
<div> quantization, Large Language Models, Round-to-Nearest, token generation throughput, data precision format
Summary: 
Quantization is essential for Large Language Models (LLMs) and Round-to-Nearest (RTN) is a simple quantization technique that is cost-effective and can offer better token generation throughput compared to advanced methods. While newer quantization methods claim superiority, this work argues that RTN is a practical choice for quantizing LLMs. By implementing RTN based on Marlin kernels and selectively increasing data precision format for certain model layers, accuracy can be improved gradually. RTN is cheaper to apply and can achieve similar accuracy to more advanced alternatives. This study challenges the notion that RTN is inferior and demonstrates its viability as a quantization method for LLMs.
<br /><br />Summary: <div>
arXiv:2505.15909v1 Announce Type: new 
Abstract: Quantization became a necessary tool for serving ever-increasing Large Language Models (LLMs). RTN (Round-to-Nearest) is perhaps the simplest quantization technique that has been around well before LLMs surged to the forefront of machine learning (ML) research. Yet, it has been largely dismissed by recent and more advanced quantization methods that claim superiority over RTN in nearly every aspect of performance. This work aims to dispel this established point of view, showing that RTN is not only much cheaper to apply, but also its token generation throughput can be better than and accuracy can be similar to more advanced alternatives. In particular, we discuss our implementation of RTN based on the recent Marlin kernels and demonstrate how the accuracy of RTN can be gradually improved by selectively increasing the data precision format of certain model layers and modules. Based on our results, we argue that RTN presents a viable and practical choice for quantizing LLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AllMetrics: A Unified Python Library for Standardized Metric Evaluation and Robust Data Validation in Machine Learning</title>
<link>https://arxiv.org/abs/2505.15931</link>
<guid>https://arxiv.org/abs/2505.15931</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, metric evaluation, standardization, Python library, reliable results

Summary:
Machine learning models rely on consistent performance metrics for evaluation and comparison. Existing libraries lack standardization, leading to implementation and reporting differences that hinder result comparisons. AllMetrics is a Python library designed to unify metric evaluation across various ML tasks like regression, classification, clustering, segmentation, and image-to-image translation. It offers class-specific reporting for multi-class tasks and task-specific parameters to reconcile metric computation disparities. The library utilizes modular API and input validation to ensure reproducibility and reliability in model evaluation. Empirical analyses on diverse datasets show that AllMetrics can harmonize evaluation errors and enhance trust in ML workflows.<br /><br />Summary: <div>
arXiv:2505.15931v1 Announce Type: new 
Abstract: Machine learning (ML) models rely heavily on consistent and accurate performance metrics to evaluate and compare their effectiveness. However, existing libraries often suffer from fragmentation, inconsistent implementations, and insufficient data validation protocols, leading to unreliable results. Existing libraries have often been developed independently and without adherence to a unified standard, particularly concerning the specific tasks they aim to support. As a result, each library tends to adopt its conventions for metric computation, input/output formatting, error handling, and data validation protocols. This lack of standardization leads to both implementation differences (ID) and reporting differences (RD), making it difficult to compare results across frameworks or ensure reliable evaluations. To address these issues, we introduce AllMetrics, an open-source unified Python library designed to standardize metric evaluation across diverse ML tasks, including regression, classification, clustering, segmentation, and image-to-image translation. The library implements class-specific reporting for multi-class tasks through configurable parameters to cover all use cases, while incorporating task-specific parameters to resolve metric computation discrepancies across implementations. Various datasets from domains like healthcare, finance, and real estate were applied to our library and compared with Python, Matlab, and R components to identify which yield similar results. AllMetrics combines a modular Application Programming Interface (API) with robust input validation mechanisms to ensure reproducibility and reliability in model evaluation. This paper presents the design principles, architectural components, and empirical analyses demonstrating the ability to mitigate evaluation errors and to enhance the trustworthiness of ML workflows.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding</title>
<link>https://arxiv.org/abs/2505.15946</link>
<guid>https://arxiv.org/abs/2505.15946</guid>
<content:encoded><![CDATA[
<div> Keywords: visual reconstruction, fMRI decoding, interpretable models, neural decoding, brain network principles

Summary:
MoRE-Brain introduces a neuro-inspired framework for visual reconstruction from fMRI data. It utilizes a hierarchical Mixture-of-Experts architecture based on brain network principles to enhance interpretability. The model encodes fMRI signals into the CLIP space and synthesizes images using a finetuned diffusion model guided by a dual-stage routing mechanism. This approach allows for efficient cross-subject generalization by sharing core expert networks and adapting subject-specific routers. MoRE-Brain provides insights into how different brain regions contribute to the reconstructed image's semantic and spatial attributes. Extensive experiments demonstrate the model's high reconstruction fidelity and effective utilization of fMRI signals, distinguishing genuine neural decoding from generative priors. Overall, MoRE-Brain represents a significant advancement in fMRI-based visual decoding, offering a more generalizable and interpretable approach to understanding human perception and developing brain-computer interfaces.
<br /><br />Summary: <div>
arXiv:2505.15946v1 Announce Type: new 
Abstract: Decoding visual experiences from fMRI offers a powerful avenue to understand human perception and develop advanced brain-computer interfaces. However, current progress often prioritizes maximizing reconstruction fidelity while overlooking interpretability, an essential aspect for deriving neuroscientific insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework designed for high-fidelity, adaptable, and interpretable visual reconstruction. MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture where distinct experts process fMRI signals from functionally related voxel groups, mimicking specialized brain networks. The experts are first trained to encode fMRI into the frozen CLIP space. A finetuned diffusion model then synthesizes images, guided by expert outputs through a novel dual-stage routing mechanism that dynamically weighs expert contributions across the diffusion process. MoRE-Brain offers three main advancements: First, it introduces a novel Mixture-of-Experts architecture grounded in brain network principles for neuro-decoding. Second, it achieves efficient cross-subject generalization by sharing core expert networks while adapting only subject-specific routers. Third, it provides enhanced mechanistic insight, as the explicit routing reveals precisely how different modeled brain regions shape the semantic and spatial attributes of the reconstructed image. Extensive experiments validate MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further demonstrating its effective utilization of fMRI signals, distinguishing genuine neural decoding from over-reliance on generative priors. Consequently, MoRE-Brain marks a substantial advance towards more generalizable and interpretable fMRI-based visual decoding. Code will be publicly available soon: https://github.com/yuxiangwei0808/MoRE-Brain.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Identifiability of Interventional Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2505.15987</link>
<guid>https://arxiv.org/abs/2505.15987</guid>
<content:encoded><![CDATA[
<div> identifiability, stochastic differential equation, interventions, stationary distributions, parameter recovery

Summary:
This study focuses on the identifiability of stochastic differential equation (SDE) models under multiple interventions, providing the first provable bounds for unique recovery of SDE parameters. Tight bounds on the number of necessary interventions for linear SDEs are presented, as well as upper bounds for nonlinear SDEs in the small noise regime. Experimental validation of parameter recovery in synthetic data is conducted, showcasing the advantage of parameterizations with learnable activation functions. The research contributes to advancing the understanding of parameter recovery in SDE models and highlights the effectiveness of certain parameterization strategies in improving model performance. <div>
arXiv:2505.15987v1 Announce Type: new 
Abstract: We study identifiability of stochastic differential equation (SDE) models under multiple interventions. Our results give the first provable bounds for unique recovery of SDE parameters given samples from their stationary distributions. We give tight bounds on the number of necessary interventions for linear SDEs, and upper bounds for nonlinear SDEs in the small noise regime. We experimentally validate the recovery of true parameters in synthetic data, and motivated by our theoretical results, demonstrate the advantage of parameterizations with learnable activation functions.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations</title>
<link>https://arxiv.org/abs/2505.16004</link>
<guid>https://arxiv.org/abs/2505.16004</guid>
<content:encoded><![CDATA[
<div> Sparse autoencoders, large language models, concept representations, robustness, input perturbations <br />
<br />
Summary: 
Sparse autoencoders (SAEs) are frequently used to interpret the internal workings of large language models (LLMs) by translating them into understandable concept representations. While current evaluations of SAEs primarily focus on metrics like reconstruction-sparsity tradeoff and feature disentanglement, they often neglect the importance of robustness in concept representations. The authors argue that the robustness of concept representations, reflecting the accuracy of concept labeling, is crucial. They introduce a framework to assess robustness by creating scenarios with adversarial perturbations that manipulate SAE representations. Through their experiments, they demonstrate that even small adversarial input changes can significantly influence concept-based interpretations, while the base LLM outputs remain unaffected. They conclude that SAE concept representations are vulnerable and may not be suitable for tasks requiring model monitoring and oversight. <br /><br /> <div>
arXiv:2505.16004v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) are commonly used to interpret the internal activations of large language models (LLMs) by mapping them to human-interpretable concept representations. While existing evaluations of SAEs focus on metrics such as the reconstruction-sparsity tradeoff, human (auto-)interpretability, and feature disentanglement, they overlook a critical aspect: the robustness of concept representations to input perturbations. We argue that robustness must be a fundamental consideration for concept representations, reflecting the fidelity of concept labeling. To this end, we formulate robustness quantification as input-space optimization problems and develop a comprehensive evaluation framework featuring realistic scenarios in which adversarial perturbations are crafted to manipulate SAE representations. Empirically, we find that tiny adversarial input perturbations can effectively manipulate concept-based interpretations in most scenarios without notably affecting the outputs of the base LLMs themselves. Overall, our results suggest that SAE concept representations are fragile and may be ill-suited for applications in model monitoring and oversight.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradPCA: Leveraging NTK Alignment for Reliable Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2505.16017</link>
<guid>https://arxiv.org/abs/2505.16017</guid>
<content:encoded><![CDATA[
<div> NTK alignment, gradient class-means, Principal Component Analysis (PCA), Out-of-Distribution (OOD) detection, spectral OOD detection <br />
Summary: 
The article introduces GradPCA, a novel method for OOD detection that leverages the low-rank structure of neural network gradients through NTK alignment. By applying PCA to gradient class-means, GradPCA achieves more consistent performance compared to existing methods in standard image classification benchmarks. The theoretical analysis highlights the importance of feature-space properties enabled by NTK alignment in achieving effective OOD detection. The study also emphasizes the significance of feature quality, particularly the use of pretrained representations, in determining the success of OOD detectors. Extensive experiments confirm the superior performance of GradPCA, while the theoretical framework provides guidance for designing more principled spectral OOD detectors. <br />
 <div>
arXiv:2505.16017v1 Announce Type: new 
Abstract: We introduce GradPCA, an Out-of-Distribution (OOD) detection method that exploits the low-rank structure of neural network gradients induced by Neural Tangent Kernel (NTK) alignment. GradPCA applies Principal Component Analysis (PCA) to gradient class-means, achieving more consistent performance than existing methods across standard image classification benchmarks. We provide a theoretical perspective on spectral OOD detection in neural networks to support GradPCA, highlighting feature-space properties that enable effective detection and naturally emerge from NTK alignment. Our analysis further reveals that feature quality -- particularly the use of pretrained versus non-pretrained representations -- plays a crucial role in determining which detectors will succeed. Extensive experiments validate the strong performance of GradPCA, and our theoretical framework offers guidance for designing more principled spectral OOD detectors.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging</title>
<link>https://arxiv.org/abs/2505.16024</link>
<guid>https://arxiv.org/abs/2505.16024</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion trajectory distillation, linear operators, signal fidelity, dynamic programming, phase transition

Summary: 
Diffusion trajectory distillation methods aim to speed up sampling in diffusion models by training a student model to mimic the multi-step denoising process of a pretrained teacher model. This study reinterprets trajectory distillation as an operator merging problem in the linear regime, where each step of the teacher model is represented as a linear operator acting on noisy data. Signal shrinkage occurs during merging due to discretization and limited optimization of the student model. A dynamic programming algorithm is proposed to compute the optimal merging strategy to maximize signal fidelity. A sharp phase transition in the optimal strategy is observed, influenced by data covariance structures. These findings enhance the understanding of diffusion trajectory distillation and provide insights for improving distillation strategies.
<br /><br />Summary: <div>
arXiv:2505.16024v1 Announce Type: new 
Abstract: Diffusion trajectory distillation methods aim to accelerate sampling in diffusion models, which produce high-quality outputs but suffer from slow sampling speeds. These methods train a student model to approximate the multi-step denoising process of a pretrained teacher model in a single step, enabling one-shot generation. However, theoretical insights into the trade-off between different distillation strategies and generative quality remain limited, complicating their optimization and selection. In this work, we take a first step toward addressing this gap. Specifically, we reinterpret trajectory distillation as an operator merging problem in the linear regime, where each step of the teacher model is represented as a linear operator acting on noisy data. These operators admit a clear geometric interpretation as projections and rescalings corresponding to the noise schedule. During merging, signal shrinkage occurs as a convex combination of operators, arising from both discretization and limited optimization time of the student model. We propose a dynamic programming algorithm to compute the optimal merging strategy that maximally preserves signal fidelity. Additionally, we demonstrate the existence of a sharp phase transition in the optimal strategy, governed by data covariance structures. Our findings enhance the theoretical understanding of diffusion trajectory distillation and offer practical insights for improving distillation strategies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces</title>
<link>https://arxiv.org/abs/2505.16035</link>
<guid>https://arxiv.org/abs/2505.16035</guid>
<content:encoded><![CDATA[
<div> Equivariant Neural Fields, Neural Eikonal Solvers, Lie group, solution steerability, Physics-Informed Neural Networks<br />
Summary:<br />
The Equivariant Neural Eikonal Solvers framework combines Equivariant Neural Fields (ENFs) with Neural Eikonal Solvers to model diverse Eikonal solutions efficiently. It uses a unified neural field conditioned on latent variables in a Lie group for robust geometric grounding. The ENF integration enables predictable and meaningful modifications in the Eikonal solutions through steerability. Coupled with Physics-Informed Neural Networks (PINNs), the framework accurately models Eikonal travel-time solutions on various Riemannian manifolds. It extends to homogeneous spaces such as Euclidean, position-orientation, spherical, and hyperbolic manifolds. Experimental results on seismic travel-time modeling demonstrate superior performance, scalability, adaptability, and user controllability compared to existing methods based on Neural Operators. The framework offers enhanced representation efficiency, solution steerability, and generalization to different manifolds, making it a promising approach for Eikonal solver applications. <br /><br />Summary: <div>
arXiv:2505.16035v1 Announce Type: new 
Abstract: We introduce Equivariant Neural Eikonal Solvers, a novel framework that integrates Equivariant Neural Fields (ENFs) with Neural Eikonal Solvers. Our approach employs a single neural field where a unified shared backbone is conditioned on signal-specific latent variables - represented as point clouds in a Lie group - to model diverse Eikonal solutions. The ENF integration ensures equivariant mapping from these latent representations to the solution field, delivering three key benefits: enhanced representation efficiency through weight-sharing, robust geometric grounding, and solution steerability. This steerability allows transformations applied to the latent point cloud to induce predictable, geometrically meaningful modifications in the resulting Eikonal solution. By coupling these steerable representations with Physics-Informed Neural Networks (PINNs), our framework accurately models Eikonal travel-time solutions while generalizing to arbitrary Riemannian manifolds with regular group actions. This includes homogeneous spaces such as Euclidean, position-orientation, spherical, and hyperbolic manifolds. We validate our approach through applications in seismic travel-time modeling of 2D and 3D benchmark datasets. Experimental results demonstrate superior performance, scalability, adaptability, and user controllability compared to existing Neural Operator-based Eikonal solver methods.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs</title>
<link>https://arxiv.org/abs/2505.16053</link>
<guid>https://arxiv.org/abs/2505.16053</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Algorithm Feedback, Graph Neural Networks, SAT Solver, Heuristics <br />
<br />
Summary: This work introduces Reinforcement Learning from Algorithm Feedback (RLAF) as a method for enhancing Boolean Satisfiability (SAT) solvers using Graph Neural Networks (GNNs). The approach involves training a GNN to guide SAT solver branching heuristics by assigning variable weights and polarities in a single forward pass. By treating this guidance as a reinforcement learning problem and using policy-gradient methods, the GNN is trained based on the solver's computational cost as the reward signal. Evaluations show that the RLAF-trained policies lead to a substantial reduction in solve times for various base solvers on diverse SAT problems, with some cases achieving over a 2x speedup. These policies also demonstrate effective generalization to larger and more challenging problems post-training, outperforming expert-supervised approaches that rely on handcrafted heuristics. This approach signals a promising direction for data-driven heuristic design in combinatorial optimization. <br /> <div>
arXiv:2505.16053v1 Announce Type: new 
Abstract: Boolean Satisfiability (SAT) solvers are foundational to computer science, yet their performance typically hinges on hand-crafted heuristics. This work introduces Reinforcement Learning from Algorithm Feedback (RLAF) as a paradigm for learning to guide SAT solver branching heuristics with Graph Neural Networks (GNNs). Central to our approach is a novel and generic mechanism for injecting inferred variable weights and polarities into the branching heuristics of existing SAT solvers. In a single forward pass, a GNN assigns these parameters to all variables. Casting this one-shot guidance as a reinforcement learning problem lets us train the GNN with off-the-shelf policy-gradient methods, such as GRPO, directly using the solver's computational cost as the sole reward signal. Extensive evaluations demonstrate that RLAF-trained policies significantly reduce the mean solve times of different base solvers across diverse SAT problem distributions, achieving more than a 2x speedup in some cases, while generalizing effectively to larger and harder problems after training. Notably, these policies consistently outperform expert-supervised approaches based on learning handcrafted weighting heuristics, offering a promising path towards data-driven heuristic design in combinatorial optimization.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models</title>
<link>https://arxiv.org/abs/2505.16056</link>
<guid>https://arxiv.org/abs/2505.16056</guid>
<content:encoded><![CDATA[
<div> Cache hit rate, memory-efficient, Mixture-of-Experts, language models, local routing consistency <br />
<br />
Summary: 
The study focuses on measuring the local routing consistency of Mixture-of-Experts (MoE) models in large language models (LLMs) to enable efficient deployment on memory-constrained devices. Two metrics, SRP and SCH, are introduced to evaluate the performance and cache hit rate of segment-level routing in MoE models. Findings indicate that models applying MoE on every layer without shared experts demonstrate the highest local routing consistency. Domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones. Most models achieve a balance between cache effectiveness and efficiency with cache sizes around 2x the active experts. These insights pave the way for designing and deploying memory-efficient MoE models without compromising inference speed. Code for replicating experiments is available at https://github.com/ljcleo/moe-lrc. <div>
arXiv:2505.16056v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the optimal segment-level cache hit rate under a given cache size limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found that models that apply MoE on every layer and do not use shared experts exhibit the highest local routing consistency. We further showed that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models can balance between cache effectiveness and efficiency with cache sizes approximately 2x the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh-free sparse identification of nonlinear dynamics</title>
<link>https://arxiv.org/abs/2505.16058</link>
<guid>https://arxiv.org/abs/2505.16058</guid>
<content:encoded><![CDATA[
<div> algorithm, neural network, auto-differentiation, PDEs, mesh-free 
Summary:
The paper introduces mesh-free SINDy, an algorithm that uses neural network approximation and auto-differentiation to identify governing equations from non-uniform temporal data sampling. It is robust to high noise levels and limited data, requiring minimal hyperparameter tuning. The approach is demonstrated on various PDEs, including the Burgers' equation, heat equation, Korteweg-De Vries equation, and 2D advection-diffusion equation. Mesh-free SINDy successfully identifies PDEs even with high noise levels and limited data, achieving accurate results with up to 75% noise for the Burgers' equation using 5,000 samples and as few as 100 samples with 1% noise, all within a training time of under one minute. Previous state-of-the-art methods are also compared, showcasing the effectiveness and efficiency of mesh-free SINDy. 
<br /><br />Summary: <div>
arXiv:2505.16058v1 Announce Type: new 
Abstract: Identifying the governing equations of a dynamical system is one of the most important tasks for scientific modeling. However, this procedure often requires high-quality spatio-temporal data uniformly sampled on structured grids. In this paper, we propose mesh-free SINDy, a novel algorithm which leverages the power of neural network approximation as well as auto-differentiation to identify governing equations from arbitrary sensor placements and non-uniform temporal data sampling. We show that mesh-free SINDy is robust to high noise levels and limited data while remaining computationally efficient. In our implementation, the training procedure is straight-forward and nearly free of hyperparameter tuning, making mesh-free SINDy widely applicable to many scientific and engineering problems. In the experiments, we demonstrate its effectiveness on a series of PDEs including the Burgers' equation, the heat equation, the Korteweg-De Vries equation and the 2D advection-diffusion equation. We conduct detailed numerical experiments on all datasets, varying the noise levels and number of samples, and we also compare our approach to previous state-of-the-art methods. It is noteworthy that, even in high-noise and low-data scenarios, mesh-free SINDy demonstrates robust PDE discovery, achieving successful identification with up to 75% noise for the Burgers' equation using 5,000 samples and with as few as 100 samples and 1% noise. All of this is achieved within a training time of under one minute.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Test-Time Optimization Without Retraining for Semiconductor Recipe Generation and Beyond</title>
<link>https://arxiv.org/abs/2505.16060</link>
<guid>https://arxiv.org/abs/2505.16060</guid>
<content:encoded><![CDATA[
<div> novel test-time optimization, Model Feedback Learning, pre-trained AI models, deployed hardware systems, semiconductor manufacturing recipe generation

Summary:<br /><br />Model Feedback Learning (MFL) is introduced as a novel framework for optimizing inputs to pre-trained AI models or deployed hardware systems. Unlike existing methods, MFL does not require retraining or modifications to the hardware, making it efficient for real-world settings like semiconductor manufacturing. By leveraging a reverse model, MFL can iteratively search for optimal inputs, achieving target recipe generation in just five iterations in semiconductor tasks. It outperforms Bayesian optimization and human experts in this area. MFL also shows strong performance in chemical processes and electronic systems, showcasing its broad applicability. The framework incorporates stability-aware optimization, enhancing robustness to process variations and excelling in high-dimensional control settings over conventional learning methods. By enabling few-shot adaptation, MFL provides a scalable and efficient method for intelligent control deployment in real-world environments. <div>
arXiv:2505.16060v1 Announce Type: new 
Abstract: We introduce Model Feedback Learning (MFL), a novel test-time optimization framework for optimizing inputs to pre-trained AI models or deployed hardware systems without requiring any retraining of the models or modifications to the hardware. In contrast to existing methods that rely on adjusting model parameters, MFL leverages a lightweight reverse model to iteratively search for optimal inputs, enabling efficient adaptation to new objectives under deployment constraints. This framework is particularly advantageous in real-world settings, such as semiconductor manufacturing recipe generation, where modifying deployed systems is often infeasible or cost-prohibitive. We validate MFL on semiconductor plasma etching tasks, where it achieves target recipe generation in just five iterations, significantly outperforming both Bayesian optimization and human experts. Beyond semiconductor applications, MFL also demonstrates strong performance in chemical processes (e.g., chemical vapor deposition) and electronic systems (e.g., wire bonding), highlighting its broad applicability. Additionally, MFL incorporates stability-aware optimization, enhancing robustness to process variations and surpassing conventional supervised learning and random search methods in high-dimensional control settings. By enabling few-shot adaptation, MFL provides a scalable and efficient paradigm for deploying intelligent control in real-world environments.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merge to Mix: Mixing Datasets via Model Merging</title>
<link>https://arxiv.org/abs/2505.16066</link>
<guid>https://arxiv.org/abs/2505.16066</guid>
<content:encoded><![CDATA[
<div> merge to mix, dataset mixtures, model merging, fine-tuning, large models

Summary:
Merge to Mix is a novel method that accelerates the process of composing dataset mixtures for fine-tuning large models. By leveraging model merging, which combines the abilities of individually fine-tuned models, Merge to Mix can effectively select dataset mixtures without requiring full fine-tuning on each candidate mixture. This approach surpasses existing methods in dataset selection for fine-tuning large models, eliminating the need for heuristic-based trial-and-error processes. Model merging plays a key role in serving as a surrogate for fine-tuning on the entire mixture, enhancing the efficiency of dataset composition. With Merge to Mix, the process of maximizing performance on downstream tasks through dataset mixing becomes more streamlined and effective, providing a novel solution to the challenges faced in fine-tuning large models. 

<br /><br />Summary: <div>
arXiv:2505.16066v1 Announce Type: new 
Abstract: Mixing datasets for fine-tuning large models (LMs) has become critical for maximizing performance on downstream tasks. However, composing effective dataset mixtures typically relies on heuristics and trial-and-error, often requiring multiple fine-tuning runs to achieve the desired outcome. We propose a novel method, $\textit{Merge to Mix}$, that accelerates composing dataset mixtures through model merging. Model merging is a recent technique that combines the abilities of multiple individually fine-tuned LMs into a single LM by using a few simple arithmetic operations. Our key insight is that merging models individually fine-tuned on each dataset in a mixture can effectively serve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix leverages this insight to accelerate selecting dataset mixtures without requiring full fine-tuning on each candidate mixture. Our experiments demonstrate that Merge to Mix surpasses state-of-the-art methods in dataset selection for fine-tuning LMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional Variational Autoencoders</title>
<link>https://arxiv.org/abs/2505.16074</link>
<guid>https://arxiv.org/abs/2505.16074</guid>
<content:encoded><![CDATA[
<div> Keywords: bidirectional variational autoencoder, neural network, image tasks, parameter count, performance

Summary:
The new bidirectional variational autoencoder (BVAE) network architecture utilizes a single neural network for both encoding and decoding processes, in contrast to traditional encoder-decoder pairs. By encoding in the forward direction and decoding in the backward direction through the same synaptic connections, the BVAE demonstrates efficiency by significantly reducing the parameter count while still achieving superior performance compared to standard unidirectional VAEs. Simulations conducted on various image tasks, including image reconstruction, classification, interpolation, and generation using datasets like MNIST, Fashion-MNIST, CIFAR-10, and CelebA-64, showcased the BVAE's capabilities. The bidirectional structure of BVAEs proved to be a promising approach in enhancing VAE performance, making it a potential advancement in neural network architecture for image processing tasks. <br /><br />Summary: <div>
arXiv:2505.16074v1 Announce Type: new 
Abstract: We present the new bidirectional variational autoencoder (BVAE) network architecture. The BVAE uses a single neural network both to encode and decode instead of an encoder-decoder network pair. The network encodes in the forward direction and decodes in the backward direction through the same synaptic web. Simulations compared BVAEs and ordinary VAEs on the four image tasks of image reconstruction, classification, interpolation, and generation. The image datasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and CelebA-64 face images. The bidirectional structure of BVAEs cut the parameter count by almost 50% and still slightly outperformed the unidirectional VAEs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensembling Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2505.16077</link>
<guid>https://arxiv.org/abs/2505.16077</guid>
<content:encoded><![CDATA[
<div> Sparse autoencoders, ensemble learning, bagging, boosting, language models<br />
<br />
Summary: 
Sparse autoencoders (SAEs) are commonly used to extract interpretable features from neural network activations. However, a single SAE may only capture a limited subset of features due to variations in weight initializations. To address this limitation, the study proposes ensembling multiple SAEs using naive bagging and boosting techniques. By integrating SAEs trained with different initial weights or minimizing residual errors sequentially, the ensemble methods enhance feature diversity, reconstruction quality, and stability. Experimental results across various language model settings and SAE architectures demonstrate the effectiveness of ensembling SAEs for tasks such as concept detection and removing spurious correlations. The ensemble approach outperforms individual SAEs, showcasing its practical utility in improving downstream applications. <br /><br /> <div>
arXiv:2505.16077v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) are used to decompose neural network activations into human-interpretable features. Typically, features learned by a single SAE are used for downstream applications. However, it has recently been shown that SAEs trained with different initial weights can learn different features, demonstrating that a single SAE captures only a limited subset of features that can be extracted from the activation space. Motivated by this limitation, we propose to ensemble multiple SAEs through naive bagging and boosting. Specifically, SAEs trained with different weight initializations are ensembled in naive bagging, whereas SAEs sequentially trained to minimize the residual error are ensembled in boosting. We evaluate our ensemble approaches with three settings of language models and SAE architectures. Our empirical results demonstrate that ensembling SAEs can improve the reconstruction of language model activations, diversity of features, and SAE stability. Furthermore, ensembling SAEs performs better than applying a single SAE on downstream tasks such as concept detection and spurious correlation removal, showing improved practical utility.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FR-Mamba: Time-Series Physical Field Reconstruction Based on State Space Model</title>
<link>https://arxiv.org/abs/2505.16083</link>
<guid>https://arxiv.org/abs/2505.16083</guid>
<content:encoded><![CDATA[
<div> Fourier Neural Operator, State Space Model, spatiotemporal flow field reconstruction, physical field reconstruction, deep learning <br />
<br />Summary: 
The article introduces FR-Mamba, a novel framework for spatiotemporal flow field reconstruction in physical systems. It combines Fourier Neural Operator (FNO) and State Space Model (SSM) to capture global spatial features and long-range temporal dependencies efficiently. By leveraging the Mamba architecture for SSM and FNO for non-local spatial features, FR-Mamba significantly outperforms existing methods in flow field reconstruction tasks. The hybrid neural network architecture achieves high-accuracy performance on long sequences of physical system data. <div>
arXiv:2505.16083v1 Announce Type: new 
Abstract: Physical field reconstruction (PFR) aims to predict the state distribution of physical quantities (e.g., velocity, pressure, and temperature) based on limited sensor measurements. It plays a critical role in domains such as fluid dynamics and thermodynamics. However, existing deep learning methods often fail to capture long-range temporal dependencies, resulting in suboptimal performance on time-evolving physical systems. To address this, we propose FR-Mamba, a novel spatiotemporal flow field reconstruction framework based on state space modeling. Specifically, we design a hybrid neural network architecture that combines Fourier Neural Operator (FNO) and State Space Model (SSM) to capture both global spatial features and long-range temporal dependencies. We adopt Mamba, a recently proposed efficient SSM architecture, to model long-range temporal dependencies with linear time complexity. In parallel, the FNO is employed to capture non-local spatial features by leveraging frequency-domain transformations. The spatiotemporal representations extracted by these two components are then fused to reconstruct the full-field distribution of the physical system. Extensive experiments demonstrate that our approach significantly outperforms existing PFR methods in flow field reconstruction tasks, achieving high-accuracy performance on long sequences.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization</title>
<link>https://arxiv.org/abs/2505.16094</link>
<guid>https://arxiv.org/abs/2505.16094</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, molecular discovery, molecule generation, molecule optimization, datasets

Summary:
Large language models (LLMs) have revolutionized molecular discovery by enabling text-guided interaction with chemical spaces through natural language and symbolic notations. This survey explores how LLMs are utilized for molecule generation and optimization, categorizing the techniques and highlighting their applications in different learning settings. The analysis includes an overview of commonly used datasets and evaluation protocols in the field. The survey concludes by addressing key challenges and proposing future directions for research, serving as a valuable resource for researchers at the intersection of LLMs and molecular science. To access an updated list of relevant resources, visit the GitHub repository at https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery. 

<br /><br />Summary: <div>
arXiv:2505.16094v1 Announce Type: new 
Abstract: Large language models (LLMs) are introducing a paradigm shift in molecular discovery by enabling text-guided interaction with chemical spaces through natural language, symbolic notations, with emerging extensions to incorporate multi-modal inputs. To advance the new field of LLM for molecular discovery, this survey provides an up-to-date and forward-looking review of the emerging use of LLMs for two central tasks: molecule generation and molecule optimization. Based on our proposed taxonomy for both problems, we analyze representative techniques in each category, highlighting how LLM capabilities are leveraged across different learning settings. In addition, we include the commonly used datasets and evaluation protocols. We conclude by discussing key challenges and future directions, positioning this survey as a resource for researchers working at the intersection of LLMs and molecular science. A continuously updated reading list is available at https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Stock Transactions</title>
<link>https://arxiv.org/abs/2505.16099</link>
<guid>https://arxiv.org/abs/2505.16099</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, stock market analysis, time to buy/sell, Markov Decision Process, Q-Learning

Summary:
Reinforcement learning is applied to analyze the stock market by determining the best time to buy and potentially sell stocks within a given timeframe. A Markov Decision Process (MDP) problem is defined using real-world data for training the model, with insights from relevant papers. Agents are trained using Q-Learning techniques and deep Q-Learning, along with machine learning regression and classification models to predict stock prices. The agents are compared to evaluate their convergence on a policy for maximizing profit in the stock market. <div>
arXiv:2505.16099v1 Announce Type: new 
Abstract: Much research has been done to analyze the stock market. After all, if one can determine a pattern in the chaotic frenzy of transactions, then they could make a hefty profit from capitalizing on these insights. As such, the goal of our project was to apply reinforcement learning (RL) to determine the best time to buy a stock within a given time frame. With only a few adjustments, our model can be extended to identify the best time to sell a stock as well. In order to use the format of free, real-world data to train the model, we define our own Markov Decision Process (MDP) problem. These two papers [5] [6] helped us in formulating the state space and the reward system of our MDP problem. We train a series of agents using Q-Learning, Q-Learning with linear function approximation, and deep Q-Learning. In addition, we try to predict the stock prices using machine learning regression and classification models. We then compare our agents to see if they converge on a policy, and if so, which one learned the best policy to maximize profit on the stock market.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Trustworthy Keylogger detection: A Comprehensive Analysis of Ensemble Techniques and Feature Selections through Explainable AI</title>
<link>https://arxiv.org/abs/2505.16103</link>
<guid>https://arxiv.org/abs/2505.16103</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Keylogger Detection, Feature Selection, Explainable AI, Model Evaluation

Summary:
- The study focuses on keylogger detection using traditional machine learning models and advanced ensemble methods.
- Various feature selection approaches such as Information gain, Lasso L1, and Fisher Score are evaluated to enhance predictive performance.
- Explainable AI techniques like SHAP and LIME are utilized for model interpretation, providing detailed explanations for feature contributions.
- Model performance is assessed based on AUC score, sensitivity, specificity, accuracy, and F1 score.
- AdaBoost demonstrates the best performance with 99.76% accuracy, F1 score of 0.99, 100% precision, 98.6% recall, 1.0 specificity, and 0.99 AUC using Fisher Score.

<br /><br />Summary: <div>
arXiv:2505.16103v1 Announce Type: new 
Abstract: Keylogger detection involves monitoring for unusual system behaviors such as delays between typing and character display, analyzing network traffic patterns for data exfiltration. In this study, we provide a comprehensive analysis for keylogger detection with traditional machine learning models - SVC, Random Forest, Decision Tree, XGBoost, AdaBoost, Logistic Regression and Naive Bayes and advanced ensemble methods including Stacking, Blending and Voting. Moreover, feature selection approaches such as Information gain, Lasso L1 and Fisher Score are thoroughly assessed to improve predictive performance and lower computational complexity. The Keylogger Detection dataset from publicly available Kaggle website is used in this project. In addition to accuracy-based classification, this study implements the approach for model interpretation using Explainable AI (XAI) techniques namely SHAP (Global) and LIME (Local) to deliver finer explanations for how much each feature contributes in assisting or hindering the detection process. To evaluate the models result, we have used AUC score, sensitivity, Specificity, Accuracy and F1 score. The best performance was achieved by AdaBoost with 99.76% accuracy, F1 score of 0.99, 100% precision, 98.6% recall, 1.0 specificity and 0.99 of AUC that is near-perfect classification with Fisher Score.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools</title>
<link>https://arxiv.org/abs/2505.16113</link>
<guid>https://arxiv.org/abs/2505.16113</guid>
<content:encoded><![CDATA[
<div> uncertainty quantification, language models, tool-calling, external tools, trustworthiness<br />
Summary: <br />
This article introduces a framework for modeling Large Language Models (LLMs) integrated with external tools to determine the trustworthiness of responses. In scenarios where LLMs require external tools for accurate answers, uncertainty quantification becomes crucial. The proposed framework considers the predictive uncertainty of both the LLM and the external tool to enhance trust in the combined system. Efficient approximations are introduced to make uncertainty computation practical for real-world applications. The framework is evaluated on synthetic question answering datasets and applied to retrieval-augmented generation systems, showing effectiveness in scenarios requiring external information retrieval. This work addresses the challenge of assessing uncertainty in LLM-based systems, especially when external tools are needed for generating reliable responses. <div>
arXiv:2505.16113v1 Announce Type: new 
Abstract: Modern Large Language Models (LLMs) often require external tools, such as machine learning classifiers or knowledge retrieval systems, to provide accurate answers in domains where their pre-trained knowledge is insufficient. This integration of LLMs with external tools expands their utility but also introduces a critical challenge: determining the trustworthiness of responses generated by the combined system. In high-stakes applications, such as medical decision-making, it is essential to assess the uncertainty of both the LLM's generated text and the tool's output to ensure the reliability of the final response. However, existing uncertainty quantification methods do not account for the tool-calling scenario, where both the LLM and external tool contribute to the overall system's uncertainty. In this work, we present a novel framework for modeling tool-calling LLMs that quantifies uncertainty by jointly considering the predictive uncertainty of the LLM and the external tool. We extend previous methods for uncertainty quantification over token sequences to this setting and propose efficient approximations that make uncertainty computation practical for real-world applications. We evaluate our framework on two new synthetic QA datasets, derived from well-known machine learning datasets, which require tool-calling for accurate answers. Additionally, we apply our method to retrieval-augmented generation (RAG) systems and conduct a proof-of-concept experiment demonstrating the effectiveness of our uncertainty metrics in scenarios where external information retrieval is needed. Our results show that the framework is effective in enhancing trust in LLM-based systems, especially in cases where the LLM's internal knowledge is insufficient and external tools are required.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generic Framework for Conformal Fairness</title>
<link>https://arxiv.org/abs/2505.16115</link>
<guid>https://arxiv.org/abs/2505.16115</guid>
<content:encoded><![CDATA[
<div> Conformal Prediction, Uncertainty Quantification, Machine Learning, Fairness, Sensitive Attributes<br />
Summary:<br />
The paper introduces the concept of Conformal Fairness, which aims to address fairness issues in machine learning models by incorporating sensitive attributes. By leveraging the exchangeability assumption instead of the typical IID assumption, the proposed framework can be applied to various data types, including non-IID data like graphs. The algorithm ensures controlled fairness-related gaps and coverage aligned with theoretical expectations. Experimental results on both graph and tabular datasets demonstrate the effectiveness of the algorithm in promoting fairness while providing probabilistic guarantees. This research contributes to the growing field of fair machine learning by offering a method to mitigate bias and ensure fairness in predictive models. <div>
arXiv:2505.16115v1 Announce Type: new 
Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification with machine learning models. While conformal prediction provides probabilistic guarantees regarding the coverage of the true label, these guarantees are agnostic to the presence of sensitive attributes within the dataset. In this work, we formalize \textit{Conformal Fairness}, a notion of fairness using conformal predictors, and provide a theoretically well-founded algorithm and associated framework to control for the gaps in coverage between different sensitive groups. Our framework leverages the exchangeability assumption (implicit to CP) rather than the typical IID assumption, allowing us to apply the notion of Conformal Fairness to data types and tasks that are not IID, such as graph data. Experiments were conducted on graph and tabular datasets to demonstrate that the algorithm can control fairness-related gaps in addition to coverage aligned with theoretical expectations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.16122</link>
<guid>https://arxiv.org/abs/2505.16122</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reasoning efficiency, token budget allocation, adaptive scheduling, plan-and-budget <br />
Summary: <br />
The paper addresses the inefficiency in inference of Large Language Models (LLMs) by introducing a theoretical model called the Bayesian Budget Allocation Model (BBAM). It identifies overthinking and underthinking as prevalent issues in LLMs and proposes the $E^3$ metric to measure the trade-off between correctness and computation efficiency. Utilizing BBAM, the Plan-and-Budget framework is introduced as a test-time strategy to enhance reasoning efficiency by decomposing queries into sub-questions and allocating token budgets based on estimated complexity through adaptive scheduling. Empirical results demonstrate significant improvements in accuracy, token reduction, and the $E^3$ metric, with the framework even enabling a smaller model to perform comparably to a larger one. Plan-and-Budget's model-agnostic approach showcases its effectiveness in enhancing LLM performance and closing performance gaps without requiring retraining. The code for implementation is also made available for further exploration and application. <br /> <div>
arXiv:2505.16122v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable success in complex reasoning tasks, but their inference remains computationally inefficient. We observe a common failure mode in many prevalent LLMs, overthinking, where models generate verbose and tangential reasoning traces even for simple queries. Recent works have tried to mitigate this by enforcing fixed token budgets, however, this can lead to underthinking, especially on harder problems. Through empirical analysis, we identify that this inefficiency often stems from unclear problem-solving strategies. To formalize this, we develop a theoretical model, BBAM (Bayesian Budget Allocation Model), which models reasoning as a sequence of sub-questions with varying uncertainty, and introduce the $E^3$ metric to capture the trade-off between correctness and computation efficiency. Building on theoretical results from BBAM, we propose Plan-and-Budget, a model-agnostic, test-time framework that decomposes complex queries into sub-questions and allocates token budgets based on estimated complexity using adaptive scheduling. Plan-and-Budget improves reasoning efficiency across a range of tasks and models, achieving up to +70% accuracy gains, -39% token reduction, and +187.5% improvement in $E^3$. Notably, it elevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger model (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close performance gaps without retraining. Our code is available at anonymous.4open.science/r/P-and-B-6513/.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Invariant Representation Learning by Distribution Extrapolation</title>
<link>https://arxiv.org/abs/2505.16126</link>
<guid>https://arxiv.org/abs/2505.16126</guid>
<content:encoded><![CDATA[
<div> Invariant risk minimization, out-of-distribution generalization, deep learning, bi-level optimization, synthetic distributional shifts <br />
Summary: 
This study introduces a new approach to address the limitations of existing invariant risk minimization (IRM) methods in deep learning. IRM aims to learn invariant representations for better out-of-distribution generalization but often falls short compared to empirical risk minimization (ERM). The researchers identify that IRM is sensitive to limited environment diversity and over-parameterization, leading to performance degradation. To combat this issue, they propose an extrapolation-based framework that introduces synthetic distributional shifts to enhance environmental diversity. Through extensive experiments on synthetic and over-parameterized scenarios, the new method consistently outperforms state-of-the-art IRM variants, demonstrating its effectiveness and robustness. <br /><br /> <div>
arXiv:2505.16126v1 Announce Type: new 
Abstract: Invariant risk minimization (IRM) aims to enable out-of-distribution (OOD) generalization in deep learning by learning invariant representations. As IRM poses an inherently challenging bi-level optimization problem, most existing approaches -- including IRMv1 -- adopt penalty-based single-level approximations. However, empirical studies consistently show that these methods often fail to outperform well-tuned empirical risk minimization (ERM), highlighting the need for more robust IRM implementations. This work theoretically identifies a key limitation common to many IRM variants: their penalty terms are highly sensitive to limited environment diversity and over-parameterization, resulting in performance degradation. To address this issue, a novel extrapolation-based framework is proposed that enhances environmental diversity by augmenting the IRM penalty through synthetic distributional shifts. Extensive experiments -- ranging from synthetic setups to realistic, over-parameterized scenarios -- demonstrate that the proposed method consistently outperforms state-of-the-art IRM variants, validating its effectiveness and robustness.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Graph Generative Modeling via Substructure Sequences</title>
<link>https://arxiv.org/abs/2505.16130</link>
<guid>https://arxiv.org/abs/2505.16130</guid>
<content:encoded><![CDATA[
<div> Transformer, Graph neural networks, Generative pre-training, Scalability, Graph learning<br />
Summary:<br />
Graph neural networks (GNNs) have limitations like expressiveness constraints and over-smoothing due to message-passing methods. In this study, a new approach called Generative Graph Pattern Machine (G$^2$PM) is proposed, utilizing Transformer pre-training for graphs. G$^2$PM represents graph instances as sequences of substructures and employs generative pre-training to learn transferable representations. It outperforms prior approaches in scalability, improving with model sizes up to 60M parameters on the ogbn-arxiv benchmark. Systematic analysis of the model design space identifies key architectural choices contributing to scalability and generalization. G$^2$PM consistently surpasses strong baselines across tasks like node classification, graph classification, and transfer learning, showcasing its potential as a scalable graph learning framework. <div>
arXiv:2505.16130v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) has been predominantly driven by message-passing, where node representations are iteratively updated via local neighborhood aggregation. Despite their success, message-passing suffers from fundamental limitations -- including constrained expressiveness, over-smoothing, over-squashing, and limited capacity to model long-range dependencies. These issues hinder scalability: increasing data size or model size often fails to yield improved performance, limiting the viability of GNNs as backbones for graph foundation models. In this work, we explore pathways beyond message-passing and introduce Generative Graph Pattern Machine (G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM represents graph instances (nodes, edges, or entire graphs) as sequences of substructures, and employs generative pre-training over the sequences to learn generalizable, transferable representations. Empirically, G$^2$PM demonstrates strong scalability: on the ogbn-arxiv benchmark, it continues to improve with model sizes up to 60M parameters, outperforming prior generative approaches that plateau at significantly smaller scales (e.g., 3M). In addition, we systematically analyze the model design space, highlighting key architectural choices that contribute to its scalability and generalization. Across diverse tasks -- including node classification, graph classification, and transfer learning -- G$^2$PM consistently outperforms strong baselines, establishing a compelling foundation for scalable graph learning. The code and dataset are available at https://github.com/Zehong-Wang/G2PM.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Online Federated Learning with Modality Missing in Internet of Things</title>
<link>https://arxiv.org/abs/2505.16138</link>
<guid>https://arxiv.org/abs/2505.16138</guid>
<content:encoded><![CDATA[
<div> Framework, Multimodal, Online Federated Learning, IoT, Distributed Learning

Summary:
The article introduces the concept of Multimodal Online Federated Learning (MMO-FL) for dynamic and decentralized multimodal learning in IoT ecosystems. It addresses the challenges posed by the vast amount of data generated by IoT devices and the need for distributed learning strategies. The real-time nature of data collection and limited local storage on edge devices call for an online learning paradigm, which MMO-FL aims to fulfill. The framework takes into account the instability of edge devices and proposes the Prototypical Modality Mitigation (PMM) algorithm to compensate for missing modalities during the learning process. The article includes a theoretical analysis of complete and missing modality scenarios, highlighting the performance degradation caused by missing modalities. Experimental results on multimodal datasets demonstrate the superior performance of PMM compared to benchmarks. <div>
arXiv:2505.16138v1 Announce Type: new 
Abstract: The Internet of Things (IoT) ecosystem generates vast amounts of multimodal data from heterogeneous sources such as sensors, cameras, and microphones. As edge intelligence continues to evolve, IoT devices have progressed from simple data collection units to nodes capable of executing complex computational tasks. This evolution necessitates the adoption of distributed learning strategies to effectively handle multimodal data in an IoT environment. Furthermore, the real-time nature of data collection and limited local storage on edge devices in IoT call for an online learning paradigm. To address these challenges, we introduce the concept of Multimodal Online Federated Learning (MMO-FL), a novel framework designed for dynamic and decentralized multimodal learning in IoT environments. Building on this framework, we further account for the inherent instability of edge devices, which frequently results in missing modalities during the learning process. We conduct a comprehensive theoretical analysis under both complete and missing modality scenarios, providing insights into the performance degradation caused by missing modalities. To mitigate the impact of modality missing, we propose the Prototypical Modality Mitigation (PMM) algorithm, which leverages prototype learning to effectively compensate for missing modalities. Experimental results on two multimodal datasets further demonstrate the superior performance of PMM compared to benchmarks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAN: A Training-Free Solution to Coefficient Estimation in Model Merging</title>
<link>https://arxiv.org/abs/2505.16148</link>
<guid>https://arxiv.org/abs/2505.16148</guid>
<content:encoded><![CDATA[
<div> Keywords: model merging, multi-task learning, least-squares optimization, task-specific information, performance improvement

Summary:
NAN presents a training-free method for model merging by optimizing coefficients through least-squares optimization. The approach is based on the idea that optimal merging weights should be proportional to the task-specific information contained in each model. NAN calculates merging coefficients using the inverse of parameter norm, making it a versatile and straightforward solution for combining independently fine-tuned models. The method shows consistent performance improvements over existing baseline methods in various experiments. NAN offers scalability and generality in model merging strategies, providing a plug-and-play option for enhancing model performance without the need for additional training data. <div>
arXiv:2505.16148v1 Announce Type: new 
Abstract: Model merging offers a training-free alternative to multi-task learning by combining independently fine-tuned models into a unified one without access to raw data. However, existing approaches often rely on heuristics to determine the merging coefficients, limiting their scalability and generality. In this work, we revisit model merging through the lens of least-squares optimization and show that the optimal merging weights should scale with the amount of task-specific information encoded in each model. Based on this insight, we propose NAN, a simple yet effective method that estimates model merging coefficients via the inverse of parameter norm. NAN is training-free, plug-and-play, and applicable to a wide range of merging strategies. Extensive experiments on show that NAN consistently improves performance of baseline methods.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Can Accurate Models Be Learned from Inaccurate Annotations?</title>
<link>https://arxiv.org/abs/2505.16159</link>
<guid>https://arxiv.org/abs/2505.16159</guid>
<content:encoded><![CDATA[
<div> Learning, inaccurate annotations, models, label information, principal subspace
Summary:<br />
- Models trained with noisy data can still make accurate predictions despite erroneous labels, prompting the investigation into why this is possible.
- Analysis reveals that label inaccuracy mainly affects lower components of weight matrices and subtly perturbs the principal subspace, but within a certain range preserves task-relevant information.
- Formal proof shows that principal subspaces of weights trained on inaccurate labels exhibit minimal deviation, explaining the models' ability to generalize effectively.
- The proposed LIP plug-in helps classifiers retain principal subspace information while reducing noise from inaccurate labels, improving performance across various tasks.
- Extensive experiments demonstrate that LIP consistently enhances existing algorithm performance, offering theoretical and practical insights into model robustness under inaccurate supervision.
Summary: <div>
arXiv:2505.16159v1 Announce Type: new 
Abstract: Learning from inaccurate annotations has gained significant attention due to the high cost of precise labeling. However, despite the presence of erroneous labels, models trained on noisy data often retain the ability to make accurate predictions. This intriguing phenomenon raises a fundamental yet largely unexplored question: why models can still extract correct label information from inaccurate annotations remains unexplored. In this paper, we conduct a comprehensive investigation into this issue. By analyzing weight matrices from both empirical and theoretical perspectives, we find that label inaccuracy primarily accumulates noise in lower singular components and subtly perturbs the principal subspace. Within a certain range, the principal subspaces of weights trained on inaccurate labels remain largely aligned with those learned from clean labels, preserving essential task-relevant information. We formally prove that the angles of principal subspaces exhibit minimal deviation under moderate label inaccuracy, explaining why models can still generalize effectively. Building on these insights, we propose LIP, a lightweight plug-in designed to help classifiers retain principal subspace information while mitigating noise induced by label inaccuracy. Extensive experiments on tasks with various inaccuracy conditions demonstrate that LIP consistently enhances the performance of existing algorithms. We hope our findings can offer valuable theoretical and practical insights to understand of model robustness under inaccurate supervision.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Federated Survival Analysis through Peer-Driven Client Reputation in Healthcare</title>
<link>https://arxiv.org/abs/2505.16190</link>
<guid>https://arxiv.org/abs/2505.16190</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Digital Health, Reputation Mechanism, Decentralized Peer Feedback, Privacy Protection <br />
Summary:
This paper introduces a robust reputation mechanism for federated healthcare to address challenges such as data heterogeneity and unreliable contributions. The proposed approach integrates decentralized peer feedback and clustering-based noise handling to enhance model aggregation. Differential privacy is applied to client-side model updates to protect sensitive information during reputation computation. The framework utilizes the Cox Proportional Hazards model for survival analysis across multiple federated nodes and dynamically adjusts trust scores based on local performance improvements measured via the concordance index. Experimental evaluations demonstrate that the method consistently achieves high and stable C-index values, effectively down-weighing noisy client updates and outperforming FL methods without a reputation system. <div>
arXiv:2505.16190v1 Announce Type: new 
Abstract: Federated Learning (FL) holds great promise for digital health by enabling collaborative model training without compromising patient data privacy. However, heterogeneity across institutions, lack of sustained reputation, and unreliable contributions remain major challenges. In this paper, we propose a robust, peer-driven reputation mechanism for federated healthcare that employs a hybrid communication model to integrate decentralized peer feedback with clustering-based noise handling to enhance model aggregation. Crucially, our approach decouples the federated aggregation and reputation mechanisms by applying differential privacy to client-side model updates before sharing them for peer evaluation. This ensures sensitive information remains protected during reputation computation, while unaltered updates are sent to the server for global model training. Using the Cox Proportional Hazards model for survival analysis across multiple federated nodes, our framework addresses both data heterogeneity and reputation deficit by dynamically adjusting trust scores based on local performance improvements measured via the concordance index. Experimental evaluations on both synthetic datasets and the SEER dataset demonstrate that our method consistently achieves high and stable C-index values, effectively down-weighing noisy client updates and outperforming FL methods that lack a reputation system.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directional Convergence, Benign Overfitting of Gradient Descent in leaky ReLU two-layer Neural Networks</title>
<link>https://arxiv.org/abs/2505.16204</link>
<guid>https://arxiv.org/abs/2505.16204</guid>
<content:encoded><![CDATA[
<div> directional convergence, network parameters, leaky ReLU, gradient descent, exponential loss, benign overfitting, phase transition, test error bound, sub-Gaussian mixture models

Summary:
In this paper, the authors establish directional convergence of network parameters in fixed-width leaky ReLU two-layer neural networks optimized via gradient descent with exponential loss. This result, previously only known for gradient flow, is proven under conditions of benign overfitting, revealing a new phase transition in the test error bound. Importantly, these findings extend beyond the traditional nearly orthogonal data setting. The study shows that benign overfitting is highly probable in sub-Gaussian mixture models, highlighting the practical implications of the theoretical results. The careful analysis of the convergent direction offers insights into network optimization and suggests potential applications for improving model performance and generalization. <div>
arXiv:2505.16204v1 Announce Type: new 
Abstract: In this paper, we prove directional convergence of network parameters of fixed width leaky ReLU two-layer neural networks optimized by gradient descent with exponential loss, which was previously only known for gradient flow. By a careful analysis of the convergent direction, we establish sufficient conditions of benign overfitting and discover a new phase transition in the test error bound. All of these results hold beyond the nearly orthogonal data setting which was studied in prior works. As an application, we demonstrate that benign overfitting occurs with high probability in sub-Gaussian mixture models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics</title>
<link>https://arxiv.org/abs/2505.16210</link>
<guid>https://arxiv.org/abs/2505.16210</guid>
<content:encoded><![CDATA[
<div> quantization, large language models, key-value cache, NQKV algorithm, inference<br />
<br />
Summary: 
The article discusses the challenges faced by Large Language Models (LLMs) in terms of memory resource consumption of the Key-Value (KV) cache during inference. To address this issue, the NQKV algorithm is introduced, which employs per-block quantile quantization based on the element distribution of the KV cache. This approach allows for optimal quantization error and enables LLMs to perform inference with larger batch sizes or longer context lengths without compromising model output quality. By implementing NQKV, the OPT model demonstrates improved throughput by 9.3x compared to traditional methods, showcasing the effectiveness of the algorithm in enhancing the efficiency of LLM deployment. <div>
arXiv:2505.16210v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency across a wide range of tasks. However, LLMs often require larger batch sizes to enhance throughput or longer context lengths to meet task demands, which significantly increases the memory resource consumption of the Key-Value (KV) cache during inference, becoming a major bottleneck in LLM deployment. To address this issue, quantization is a common and straightforward approach. Currently, quantization methods for activations are limited to 8-bit, and quantization to even lower bits can lead to substantial accuracy drops. To further save space by quantizing the KV cache to even lower bits, we analyzed the element distribution of the KV cache and designed the NQKV algorithm. Since the elements within each block of the KV cache follow a normal distribution, NQKV employs per-block quantile quantization to achieve information-theoretically optimal quantization error. Without significantly compromising model output quality, NQKV enables the OPT model to perform inference with an 2x larger batch size or a 4x longer context length, and it improves throughput by 9.3x compared to when the KV cache is not used.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-Aware Proto-Representations in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16217</link>
<guid>https://arxiv.org/abs/2505.16217</guid>
<content:encoded><![CDATA[
<div> Successor representation, reinforcement learning, credit assignment, exploration, reward dynamics <br />
<br />Summary: <br />
The paper introduces the default representation (DR) as a new approach in reinforcement learning, building upon the successor representation (SR) concept. The DR considers both transition dynamics and reward dynamics of the environment, unlike the SR. The theoretical foundation of DR is established through dynamic programming and temporal-difference methods. The vector space and extension to function approximation with default features are characterized. Empirical analysis demonstrates the benefits of DR in various scenarios such as reward shaping, option discovery, exploration, and transfer learning. Compared to SR, DR shows qualitatively different, reward-aware behavior and achieves quantitatively better performance in multiple settings. <div>
arXiv:2505.16217v1 Announce Type: new 
Abstract: In recent years, the successor representation (SR) has attracted increasing attention in reinforcement learning (RL), and it has been used to address some of its key challenges, such as exploration, credit assignment, and generalization. The SR can be seen as representing the underlying credit assignment structure of the environment by implicitly encoding its induced transition dynamics. However, the SR is reward-agnostic. In this paper, we discuss a similar representation that also takes into account the reward dynamics of the problem. We study the default representation (DR), a recently proposed representation with limited theoretical (and empirical) analysis. Here, we lay some of the theoretical foundation underlying the DR in the tabular case by (1) deriving dynamic programming and (2) temporal-difference methods to learn the DR, (3) characterizing the basis for the vector space of the DR, and (4) formally extending the DR to the function approximation case through default features. Empirically, we analyze the benefits of the DR in many of the settings in which the SR has been applied, including (1) reward shaping, (2) option discovery, (3) exploration, and (4) transfer learning. Our results show that, compared to the SR, the DR gives rise to qualitatively different, reward-aware behaviour and quantitatively better performance in several settings.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Realistic Evaluation of TabPFN v2 in Open Environments</title>
<link>https://arxiv.org/abs/2505.16226</link>
<guid>https://arxiv.org/abs/2505.16226</guid>
<content:encoded><![CDATA[
<div> Deep Learning, Tabular Data, TabPFN v2, Open Environments, Tree-Based Models

Summary: 
Tabular data is commonly found in various domains and has been predominantly analyzed using tree-based models. The TabPFN v2 deep learning model has shown superior performance but its adaptability in open environments is questioned. A comprehensive evaluation was conducted on TabPFN v2 to assess its performance under various real-world challenges. Results indicate that while TabPFN v2 is suitable for small-scale, covariate-shifted, and class-balanced tasks, it exhibits limitations in open environments. Tree-based models remain a better choice for general tabular tasks in open environments. Suggestions are made for the development of open environment tabular benchmarks, multi-metric evaluation, and universal modules to enhance model robustness. The evaluation framework for TabPFN v2 in open environments is publicly available for further research. 

<br /><br />Summary: <div>
arXiv:2505.16226v1 Announce Type: new 
Abstract: Tabular data, owing to its ubiquitous presence in real-world domains, has garnered significant attention in machine learning research. While tree-based models have long dominated tabular machine learning tasks, the recently proposed deep learning model TabPFN v2 has emerged, demonstrating unparalleled performance and scalability potential. Although extensive research has been conducted on TabPFN v2 to further improve performance, the majority of this research remains confined to closed environments, neglecting the challenges that frequently arise in open environments. This raises the question: Can TabPFN v2 maintain good performance in open environments? To this end, we conduct the first comprehensive evaluation of TabPFN v2's adaptability in open environments. We construct a unified evaluation framework covering various real-world challenges and assess the robustness of TabPFN v2 under open environments scenarios using this framework. Empirical results demonstrate that TabPFN v2 shows significant limitations in open environments but is suitable for small-scale, covariate-shifted, and class-balanced tasks. Tree-based models remain the optimal choice for general tabular tasks in open environments. To facilitate future research on open environments challenges, we advocate for open environments tabular benchmarks, multi-metric evaluation, and universal modules to strengthen model robustness. We publicly release our evaluation framework at https://anonymous.4open.science/r/tabpfn-ood-4E65.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies</title>
<link>https://arxiv.org/abs/2505.16242</link>
<guid>https://arxiv.org/abs/2505.16242</guid>
<content:encoded><![CDATA[
<div> Keywords: offline reinforcement learning, healthcare, out-of-distribution issues, safe policy exploration, safety constraints

Summary:<br />
Offline reinforcement learning in healthcare faces challenges with out-of-distribution issues, which can lead to harmful recommendations. Current methods like conservative Q-learning only constrain action selection, limiting the discovery of improved treatment strategies. To address these limitations, Offline Guarded Safe Reinforcement Learning (OGSRL) is proposed. OGSRL uses a dual constraint mechanism to improve policy with reliability and safety. The OOD guardian specifies safe policy exploration regions, allowing for reliable exploration of treatment strategies beyond clinician recommendations. Safety cost constraints encode medical knowledge to ensure domain-specific safeguards. Theoretical guarantees are provided on safety and near-optimality, ensuring policies stay within safe and reliable regions while achieving close to optimal performance supported by data. 

Summary:  <br /> <div>
arXiv:2505.16242v1 Announce Type: new 
Abstract: When applying offline reinforcement learning (RL) in healthcare scenarios, the out-of-distribution (OOD) issues pose significant risks, as inappropriate generalization beyond clinical expertise can result in potentially harmful recommendations. While existing methods like conservative Q-learning (CQL) attempt to address the OOD issue, their effectiveness is limited by only constraining action selection by suppressing uncertain actions. This action-only regularization imitates clinician actions that prioritize short-term rewards, but it fails to regulate downstream state trajectories, thereby limiting the discovery of improved long-term treatment strategies. To safely improve policy beyond clinician recommendations while ensuring that state-action trajectories remain in-distribution, we propose \textit{Offline Guarded Safe Reinforcement Learning} ($\mathsf{OGSRL}$), a theoretically grounded model-based offline RL framework. $\mathsf{OGSRL}$ introduces a novel dual constraint mechanism for improving policy with reliability and safety. First, the OOD guardian is established to specify clinically validated regions for safe policy exploration. By constraining optimization within these regions, it enables the reliable exploration of treatment strategies that outperform clinician behavior by leveraging the full patient state history, without drifting into unsupported state-action trajectories. Second, we introduce a safety cost constraint that encodes medical knowledge about physiological safety boundaries, providing domain-specific safeguards even in areas where training data might contain potentially unsafe interventions. Notably, we provide theoretical guarantees on safety and near-optimality: policies that satisfy these constraints remain in safe and reliable regions and achieve performance close to the best possible policy supported by the data.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network-Based Collaborative Perception for Adaptive Scheduling in Distributed Systems</title>
<link>https://arxiv.org/abs/2505.16248</link>
<guid>https://arxiv.org/abs/2505.16248</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Multi-node Perception, Distributed Systems, Collaborative Perception, Task Completion Rate <br />
<br />
Summary: 
This paper introduces a GNN-based multi-node collaborative perception mechanism to address the limitations of multi-node perception and delayed scheduling response in distributed systems. The system is modeled as a graph structure, with message-passing and state-update modules facilitating information aggregation and dynamic state inference among nodes. A multi-layer graph neural network enables efficient information aggregation, and a perception representation method fuses local states with global features to improve each node's perception capabilities. The proposed method is evaluated using a dataset with heterogeneous task loads and dynamic communication topologies. Results show superior performance in task completion rate, average latency, load balancing, and transmission efficiency, outperforming mainstream algorithms under various conditions, including limited bandwidth and dynamic structural changes. The model demonstrates rapid convergence and efficient responses to complex system states. <div>
arXiv:2505.16248v1 Announce Type: new 
Abstract: This paper addresses the limitations of multi-node perception and delayed scheduling response in distributed systems by proposing a GNN-based multi-node collaborative perception mechanism. The system is modeled as a graph structure. Message-passing and state-update modules are introduced. A multi-layer graph neural network is constructed to enable efficient information aggregation and dynamic state inference among nodes. In addition, a perception representation method is designed by fusing local states with global features. This improves each node's ability to perceive the overall system status. The proposed method is evaluated within a customized experimental framework. A dataset featuring heterogeneous task loads and dynamic communication topologies is used. Performance is measured in terms of task completion rate, average latency, load balancing, and transmission efficiency. Experimental results show that the proposed method outperforms mainstream algorithms under various conditions, including limited bandwidth and dynamic structural changes. It demonstrates superior perception capabilities and cooperative scheduling performance. The model achieves rapid convergence and efficient responses to complex system states.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small-to-Large Generalization: Data Influences Models Consistently Across Scale</title>
<link>https://arxiv.org/abs/2505.16260</link>
<guid>https://arxiv.org/abs/2505.16260</guid>
<content:encoded><![CDATA[
<div> training data distribution, model behavior, language model, data attribution, dataset selection
<br />
Small- and large-scale language model predictions show high correlation across different training data distributions. This correlation allows for understanding how choices in training data influence model behavior across compute scales. By examining the impact of proxy scale on two downstream applications — data attribution and dataset selection — researchers can determine the effectiveness of using scaled-down proxy models. This research provides insight into the intricate relationship between training data and model behavior, shedding light on how different data distributions can impact model predictions. The findings suggest that extrapolating from smaller proxy models may not always accurately represent the behavior of larger models, highlighting the complexity of training data influence on model performance.
<br /><br />Summary: <div>
arXiv:2505.16260v1 Announce Type: new 
Abstract: Choice of training data distribution greatly influences model behavior. Yet, in large-scale settings, precisely characterizing how changes in training data affects predictions is often difficult due to model training costs. Current practice is to instead extrapolate from scaled down, inexpensive-to-train proxy models. However, changes in data do not influence smaller and larger models identically. Therefore, understanding how choice of data affects large-scale models raises the question: how does training data distribution influence model behavior across compute scale? We find that small- and large-scale language model predictions (generally) do highly correlate across choice of training data. Equipped with these findings, we characterize how proxy scale affects effectiveness in two downstream proxy model applications: data attribution and dataset selection.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models</title>
<link>https://arxiv.org/abs/2505.16265</link>
<guid>https://arxiv.org/abs/2505.16265</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, human feedback, reward models, generative models, long-horizon reasoning <br />
Summary: <br />
The article introduces a new training framework called Think-RM for reinforcement learning from human feedback (RLHF). Think-RM enables long-horizon reasoning in generative reward models by modeling an internal thinking process. The framework generates flexible reasoning traces supporting advanced reasoning abilities. The models are trained using supervised fine-tuning over long chain-of-thought (CoT) data and further improved with rule-based reinforcement learning. A novel pairwise RLHF pipeline is proposed, optimizing policies using pairwise preference rewards for more effective use of Think-RM outputs. Experimental results on RM-Bench show that Think-RM outperforms conventional reward models and vertically scaled generative models by 8%. When combined with the pairwise RLHF pipeline, Think-RM demonstrates superior end-policy performance compared to traditional approaches. <div>
arXiv:2505.16265v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) has become a powerful post-training paradigm for aligning large language models with human preferences. A core challenge in RLHF is constructing accurate reward signals, where the conventional Bradley-Terry reward models (BT RMs) often suffer from sensitivity to data size and coverage, as well as vulnerability to reward hacking. Generative reward models (GenRMs) offer a more robust alternative by generating chain-of-thought (CoT) rationales followed by a final reward. However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks. Moreover, their pairwise preference outputs are incompatible with standard RLHF algorithms that require pointwise reward signals. In this work, we introduce Think-RM, a training framework that enables long-horizon reasoning in GenRMs by modeling an internal thinking process. Rather than producing structured, externally provided rationales, Think-RM generates flexible, self-guided reasoning traces that support advanced capabilities such as self-reflection, hypothetical reasoning, and divergent reasoning. To elicit these reasoning abilities, we first warm-up the models by supervised fine-tuning (SFT) over long CoT data. We then further improve the model's long-horizon abilities by rule-based reinforcement learning (RL). In addition, we propose a novel pairwise RLHF pipeline that directly optimizes policies using pairwise preference rewards, eliminating the need for pointwise reward conversion and enabling more effective use of Think-RM outputs. Experiments show that Think-RM achieves state-of-the-art results on RM-Bench, outperforming both BT RM and vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline, it demonstrates superior end-policy performance compared to traditional approaches.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Only Large Weights (And Not Skip Connections) Can Prevent the Perils of Rank Collapse</title>
<link>https://arxiv.org/abs/2505.16284</link>
<guid>https://arxiv.org/abs/2505.16284</guid>
<content:encoded><![CDATA[
<div> Keywords: attention mechanisms, large language models, quadratic time, layer collapse, representational strength <br />
Summary: 
This paper discusses the necessity of large weights in preventing layer collapse in expressive transformers with attention mechanisms. Previous research had shown that small weights lead to rank collapse, necessitating skip connections in networks for expressiveness. However, this paper reveals that even with skip connections, layer collapse still occurs without large weights. The concept of layer collapse signifies that the network can be effectively approximated by a single layer, highlighting the importance of weight size in maintaining representational strength. Thus, the study emphasizes that large weights, rather than skip connections, are crucial in avoiding such weaknesses in attention-based models. <div>
arXiv:2505.16284v1 Announce Type: new 
Abstract: Attention mechanisms lie at the heart of modern large language models (LLMs). Straightforward algorithms for forward and backward (gradient) computation take quadratic time, and a line of work initiated by [Alman and Song NeurIPS 2023] and [Alman and Song NeurIPS 2024] has shown that quadratic time is necessary unless the model weights are small, in which case almost linear time algorithms are possible. In this paper, we show that large weights are necessary to avoid a strong preclusion to representational strength we call layer collapse, which means that the entire network can be approximated well by a network with only a single layer. Thus, the quadratic running time of attention is unavoidable for expressive transformers.
  The notion of layer collapse that we introduce is a variant on the notion of rank collapse from the work of [Dong, Cordonnier, and Loukas ICML 2021]. They showed that in Self Attention Networks with small weights and with skip connections, rank collapse must occur. This is typically interpreted as justifying the necessity of skip connections in expressive networks. However, our result shows that even with skip connections, if the weights are small, then layer collapse still occurs. Thus, only large weights, and not skip connections, can prevent these representational weaknesses.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness under Competition</title>
<link>https://arxiv.org/abs/2505.16291</link>
<guid>https://arxiv.org/abs/2505.16291</guid>
<content:encoded><![CDATA[
<div> fairness, algorithm, ML, competing firms, ecosystem<br />
Summary:<br />
The paper discusses the impact of adopting fair classifiers on ecosystem fairness, focusing on fairness in systems with competing firms. It reveals that fair classifiers may not necessarily result in fair ecosystems due to factors such as classifiers' correlation and data overlap. The study emphasizes that even if individual classifiers are fair, the overall ecosystem outcome may be unfair. Adjusting biased algorithms to improve individual fairness could lead to a decline in ecosystem fairness. Theoretical analyses and experimental evidence are provided to support these findings. The research highlights the importance of considering ecosystem fairness alongside individual fairness when designing and implementing ML algorithms. <br /><br /> <div>
arXiv:2505.16291v1 Announce Type: new 
Abstract: Algorithmic fairness has emerged as a central issue in ML, and it has become standard practice to adjust ML algorithms so that they will satisfy fairness requirements such as Equal Opportunity. In this paper we consider the effects of adopting such fair classifiers on the overall level of ecosystem fairness. Specifically, we introduce the study of fairness with competing firms, and demonstrate the failure of fair classifiers in yielding fair ecosystems. Our results quantify the loss of fairness in systems, under a variety of conditions, based on classifiers' correlation and the level of their data overlap. We show that even if competing classifiers are individually fair, the ecosystem's outcome may be unfair; and that adjusting biased algorithms to improve their individual fairness may lead to an overall decline in ecosystem fairness. In addition to these theoretical results, we also provide supporting experimental evidence. Together, our model and results provide a novel and essential call for action.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Bayesian Tensor Reconstruction: An Approximate Message Passing Solution</title>
<link>https://arxiv.org/abs/2505.16305</link>
<guid>https://arxiv.org/abs/2505.16305</guid>
<content:encoded><![CDATA[
<div> Bayesian, CPD, Tensor, GAMP, Scalable
Summary:
The article introduces a scalable Bayesian algorithm, CP-GAMP, for Tensor CANDECOMP/PARAFAC decomposition (CPD). The algorithm utilizes generalized approximate message passing (GAMP) to avoid high-dimensional matrix inversions, resulting in significant runtime reduction for large tensors. By incorporating an expectation-maximization routine, CP-GAMP can jointly infer the tensor rank and noise power. Experimental results on synthetic 100x100x100 rank 20 tensors with 20% observed elements demonstrate an 82.7% decrease in runtime compared to the existing variational Bayesian CPD method while maintaining comparable reconstruction accuracy. This advancement in Bayesian CPD algorithms offers principled uncertainty quantification and automatic hyperparameter learning for tensor reconstruction, with the ability to handle large-scale tensors efficiently. <br /><br />Summary: <div>
arXiv:2505.16305v1 Announce Type: new 
Abstract: Tensor CANDECOMP/PARAFAC decomposition (CPD) is a fundamental model for tensor reconstruction. Although the Bayesian framework allows for principled uncertainty quantification and automatic hyperparameter learning, existing methods do not scale well for large tensors because of high-dimensional matrix inversions. To this end, we introduce CP-GAMP, a scalable Bayesian CPD algorithm. This algorithm leverages generalized approximate message passing (GAMP) to avoid matrix inversions and incorporates an expectation-maximization routine to jointly infer the tensor rank and noise power. Through multiple experiments, for synthetic 100x100x100 rank 20 tensors with only 20% elements observed, the proposed algorithm reduces runtime by 82.7% compared to the state-of-the-art variational Bayesian CPD method, while maintaining comparable reconstruction accuracy.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAIFormer: A Causal Informed Transformer for Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.16308</link>
<guid>https://arxiv.org/abs/2505.16308</guid>
<content:encoded><![CDATA[
<div> causal inference, multivariate time series forecasting, Structural Causal Model, Causal Informed Transformer, spurious correlation <br />
Summary: <br />
The paper introduces a new approach for multivariate time series forecasting that focuses on identifying variable-specific causal influences. It proposes an all-to-one forecasting paradigm that predicts each target variable separately based on a Structural Causal Model. The model partitions historical sequences into endogenous, direct causal, collider causal, and spurious correlation sub-segments and uses only the causally relevant segments for prediction. The proposed Causal Informed Transformer (CAIFormer) consists of three prediction blocks for processing the different sub-segments. Experimental results on benchmark datasets show the effectiveness of the CAIFormer in improving forecasting accuracy by distinguishing between causally relevant information and spurious correlations. <br /> <div>
arXiv:2505.16308v1 Announce Type: new 
Abstract: Most existing multivariate time series forecasting methods adopt an all-to-all paradigm that feeds all variable histories into a unified model to predict their future values without distinguishing their individual roles. However, this undifferentiated paradigm makes it difficult to identify variable-specific causal influences and often entangles causally relevant information with spurious correlations. To address this limitation, we propose an all-to-one forecasting paradigm that predicts each target variable separately. Specifically, we first construct a Structural Causal Model from observational data and then, for each target variable, we partition the historical sequence into four sub-segments according to the inferred causal structure: endogenous, direct causal, collider causal, and spurious correlation. The prediction relies solely on the first three causally relevant sub-segments, while the spurious correlation sub-segment is excluded. Furthermore, we propose Causal Informed Transformer (CAIFormer), a novel forecasting model comprising three components: Endogenous Sub-segment Prediction Block, Direct Causal Sub-segment Prediction Block, and Collider Causal Sub-segment Prediction Block, which process the endogenous, direct causal, and collider causal sub-segments, respectively. Their outputs are then combined to produce the final prediction. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of the CAIFormer.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreshRetailNet-50K: A Stockout-Annotated Censored Demand Dataset for Latent Demand Recovery and Forecasting in Fresh Retail</title>
<link>https://arxiv.org/abs/2505.16319</link>
<guid>https://arxiv.org/abs/2505.16319</guid>
<content:encoded><![CDATA[
<div> retail, demand estimation, censored data, perishable products, dataset  
Summary:  
Accurate demand estimation in retail is crucial for inventory and pricing strategies, but is hindered by censored sales data during stockouts. The article introduces FreshRetailNet-50K, a large-scale benchmark dataset with 50,000 store-product time series containing detailed hourly sales data from 898 stores across 18 cities. This dataset is annotated for stockout events and includes hourly stock status records, contextual covariates, and rich data features. A two-stage demand modeling approach is demonstrated using precise annotations to reconstruct latent demand during stockouts and enhance demand forecasting accuracy. The dataset enables innovative research in demand imputation, perishable inventory optimization, and causal retail analytics. The open release of the dataset and code addresses retail AI limitations and facilitates future methodological advancements.<br /><br />Summary: <div>
arXiv:2505.16319v1 Announce Type: new 
Abstract: Accurate demand estimation is critical for the retail business in guiding the inventory and pricing policies of perishable products. However, it faces fundamental challenges from censored sales data during stockouts, where unobserved demand creates systemic policy biases. Existing datasets lack the temporal resolution and annotations needed to address this censoring effect. To fill this gap, we present FreshRetailNet-50K, the first large-scale benchmark for censored demand estimation. It comprises 50,000 store-product time series of detailed hourly sales data from 898 stores in 18 major cities, encompassing 863 perishable SKUs meticulously annotated for stockout events. The hourly stock status records unique to this dataset, combined with rich contextual covariates, including promotional discounts, precipitation, and temporal features, enable innovative research beyond existing solutions. We demonstrate one such use case of two-stage demand modeling: first, we reconstruct the latent demand during stockouts using precise hourly annotations. We then leverage the recovered demand to train robust demand forecasting models in the second stage. Experimental results show that this approach achieves a 2.73\% improvement in prediction accuracy while reducing the systematic demand underestimation from 7.37\% to near-zero bias. With unprecedented temporal granularity and comprehensive real-world information, FreshRetailNet-50K opens new research directions in demand imputation, perishable inventory optimization, and causal retail analytics. The unique annotation quality and scale of the dataset address long-standing limitations in retail AI, providing immediate solutions and a platform for future methodological innovation. The data (https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K) and code (https://github.com/Dingdong-Inc/frn-50k-baseline}) are openly released.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners</title>
<link>https://arxiv.org/abs/2505.16322</link>
<guid>https://arxiv.org/abs/2505.16322</guid>
<content:encoded><![CDATA[
<div> Algorithm; Adaptive Sampling; Self-Improving Reasoning; Language Models; Efficiency
Summary:
AdaSTaR is introduced as a novel algorithm that addresses the issue of observation imbalance during the training of self-improving reasoning Language Models (LMs) by combining two adaptive sampling principles. Adaptive Sampling for Diversity ensures a balanced training across different observations, while Adaptive Sampling for Curriculum dynamically adjusts the data difficulty to match the model's strength. AdaSTaR outperforms existing baselines across six benchmarks, achieving the best test accuracy in all instances and reducing training FLOPs by an average of 58.6%. These results generalize to various pre-trained LMs and larger models, leading to more efficient and effective self-improving LMs. 
<br /><br />Summary: <div>
arXiv:2505.16322v1 Announce Type: new 
Abstract: Self-Taught Reasoners (STaR), synonymously known as Rejection sampling Fine-Tuning (RFT), is an integral part of the training pipeline of self-improving reasoning Language Models (LMs). The self-improving mechanism often employs random observation (data) sampling. However, this results in trained observation imbalance; inefficiently over-training on solved examples while under-training on challenging ones. In response, we introduce Adaptive STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting balanced training across observations, and (2) Adaptive Sampling for Curriculum: dynamically adjusting data difficulty to match the model's evolving strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all instances (6/6) and reduces training FLOPs by an average of 58.6% against an extensive list of baselines. These improvements in performance and efficiency generalize to different pre-trained LMs and larger models, paving the way for more efficient and effective self-improving LMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemMLLM: Chemical Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2505.16326</link>
<guid>https://arxiv.org/abs/2505.16326</guid>
<content:encoded><![CDATA[
<div> Keywords: ChemMLLM, multimodal large language model, molecule understanding, generation, benchmarking

Summary:
ChemMLLM is introduced as a unified chemical multimodal large language model for molecule understanding and generation. The model addresses the gap in cross-modal understanding and generation capabilities in chemical MLLMs. Five multimodal tasks are designed, spanning text, molecular SMILES strings, and image data, with curated datasets for evaluation. ChemMLLM is benchmarked against general MLLMs and Chemical LLMs, showing superior performance across all tasks. In a molecule image optimization task, ChemMLLM outperforms the best baseline by a significant margin. The code for ChemMLLM is openly available on GitHub for further exploration and development. This research demonstrates the effectiveness of ChemMLLM in enhancing multimodal chemical data processing tasks, highlighting its potential for future applications in the field. 

<br /><br />Summary: <div>
arXiv:2505.16326v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have made impressive progress in many applications in recent years. However, chemical MLLMs that can handle cross-modal understanding and generation remain underexplored. To fill this gap, in this paper, we propose ChemMLLM, a unified chemical multimodal large language model for molecule understanding and generation. Also, we design five multimodal tasks across text, molecular SMILES strings, and image, and curate the datasets. We benchmark ChemMLLM against a range of general leading MLLMs and Chemical LLMs on these tasks. Experimental results show that ChemMLLM achieves superior performance across all evaluated tasks. For example, in molecule image optimization task, ChemMLLM outperforms the best baseline (GPT-4o) by 118.9\% (4.27 vs 1.95 property improvement). The code is publicly available at https://github.com/bbsbz/ChemMLLM.git.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Differential Transformer Unchains Pretrained Self-Attentions</title>
<link>https://arxiv.org/abs/2505.16333</link>
<guid>https://arxiv.org/abs/2505.16333</guid>
<content:encoded><![CDATA[
<div> Keywords: Differential Transformer, attention, expressivity, redundancy, learning dynamics

Summary:
- The Differential Transformer architecture has gained attention for its noise-canceling abilities and empirical performance.
- The success of the model is attributed to (1) enhanced expressivity through negative attention, (2) reduced redundancy among attention heads, and (3) improved learning dynamics.
- The proposed DEX method efficiently integrates these advantages into pretrained language models by reusing softmax attention scores and applying a lightweight differential operation.
- DEX significantly improves pretrained Language Models (LLMs) across various benchmarks with minimal adaptation data required.
- Through DEX, the benefits of differential attention can be effectively incorporated into existing pretrained LLMs, enhancing performance while maintaining efficiency in both training and inference.<br /><br />Summary: <div>
arXiv:2505.16333v1 Announce Type: new 
Abstract: Differential Transformer has recently gained significant attention for its impressive empirical performance, often attributed to its ability to perform noise canceled attention. However, precisely how differential attention achieves its empirical benefits remains poorly understood. Moreover, Differential Transformer architecture demands large-scale training from scratch, hindering utilization of open pretrained weights. In this work, we conduct an in-depth investigation of Differential Transformer, uncovering three key factors behind its success: (1) enhanced expressivity via negative attention, (2) reduced redundancy among attention heads, and (3) improved learning dynamics. Based on these findings, we propose DEX, a novel method to efficiently integrate the advantages of differential attention into pretrained language models. By reusing the softmax attention scores and adding a lightweight differential operation on the output value matrix, DEX effectively incorporates the key advantages of differential attention while remaining lightweight in both training and inference. Evaluations confirm that DEX substantially improves the pretrained LLMs across diverse benchmarks, achieving significant performance gains with minimal adaptation data (< 0.01\%).
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Chemical Understanding of LLMs via SMILES Parsing</title>
<link>https://arxiv.org/abs/2505.16340</link>
<guid>https://arxiv.org/abs/2505.16340</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, molecular science, SMILES representation, CLEANMOL, graph-level comprehension 

Summary: 
CLEANMOL is a novel framework designed to improve the ability of large language models (LLMs) to understand molecular structures encoded in the SMILES representation. By breaking down SMILES parsing into clean and deterministic tasks focused on graph-level molecular comprehension, CLEANMOL provides structured supervision aligned with molecular structural properties. This framework includes tasks such as subgraph matching and global graph matching to enhance structural comprehension. The researchers constructed a molecular pretraining dataset with adaptive difficulty scoring and pre-trained open-source LLMs on these tasks. The results demonstrate that CLEANMOL not only improves structural comprehension but also performs competitively or surpasses baseline models on the Mol-Instructions benchmark. Overall, CLEANMOL addresses the limitations of current LLMs in interpreting molecular structures, offering a promising advancement in the field of molecular science. 

<br /><br />Summary: <div>
arXiv:2505.16340v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, current LLMs struggle to interpret SMILES, even failing to carry out basic tasks such as counting molecular rings. To address this limitation, we introduce CLEANMOL, a novel framework that formulates SMILES parsing into a suite of clean and deterministic tasks explicitly designed to promote graph-level molecular comprehension. These tasks span from subgraph matching to global graph matching, providing structured supervision aligned with molecular structural properties. We construct a molecular pretraining dataset with adaptive difficulty scoring and pre-train open-source LLMs on these tasks. Our results show that CLEANMOL not only enhances structural comprehension but also achieves the best or competes with the baseline on the Mol-Instructions benchmark.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Square Peg in a Square Hole: Meta-Expert for Long-Tailed Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2505.16341</link>
<guid>https://arxiv.org/abs/2505.16341</guid>
<content:encoded><![CDATA[
<div> long-tailed semi-supervised learning, distribution mismatch, dynamic expert assignment module, multi-depth feature fusion module, CIFAR-10-LT, STL-10-LT, SVHN-LT<br />
<br />Summary: 
This paper addresses the issue of long-tailed semi-supervised learning with distribution mismatch by proposing a dynamic expert assignment module that assigns suitable experts to samples based on their estimated class membership. By integrating the strengths of different experts, the method produces high-quality pseudo-labels during training and accurate predictions during testing. The theoretical analysis shows that combining experts results in a smaller generalization error. Additionally, a multi-depth feature fusion module is introduced to mitigate model bias by utilizing features at different depths. Experimental results on datasets such as CIFAR-10-LT, STL-10-LT, and SVHN-LT demonstrate the effectiveness of the proposed method. The code is available for further exploration. <div>
arXiv:2505.16341v1 Announce Type: new 
Abstract: This paper studies the long-tailed semi-supervised learning (LTSSL) with distribution mismatch, where the class distribution of the labeled training data follows a long-tailed distribution and mismatches with that of the unlabeled training data. Most existing methods introduce auxiliary classifiers (experts) to model various unlabeled data distributions and produce pseudo-labels, but the expertises of various experts are not fully utilized. We observe that different experts are good at predicting different intervals of samples, e.g., long-tailed expert is skilled in samples located in the head interval and uniform expert excels in samples located in the medium interval. Therefore, we propose a dynamic expert assignment module that can estimate the class membership (i.e., head, medium, or tail class) of samples, and dynamically assigns suitable expert to each sample based on the estimated membership to produce high-quality pseudo-label in the training phase and produce prediction in the testing phase. We also theoretically reveal that integrating different experts' strengths will lead to a smaller generalization error bound. Moreover, we find that the deeper features are more biased toward the head class but with more discriminative ability, while the shallower features are less biased but also with less discriminative ability. We, therefore, propose a multi-depth feature fusion module to utilize different depth features to mitigate the model bias. Our method demonstrates its effectiveness through comprehensive experiments on the CIFAR-10-LT, STL-10-LT, and SVHN-LT datasets across various settings. The code is available at https://github.com/yaxinhou/Meta-Expert.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arrival Control in Quasi-Reversible Queueing Systems: Optimization and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16353</link>
<guid>https://arxiv.org/abs/2505.16353</guid>
<content:encoded><![CDATA[
<div> quasi-reversible queueing systems, balanced arrival control policies, customer classes, admission control, optimization, reinforcement learning

Summary:<br />
In this paper, a versatile scheme for optimizing arrival rates of quasi-reversible queueing systems is introduced. The authors redefine quasi-reversibility to include reversibility and emphasize the significance of customer classes. They propose balanced arrival control policies, extending the concept of balanced arrival rates to various quasi-reversible queueing systems. Demonstrating that integrating a balanced arrival-control policy maintains quasi-reversibility, the stationary measures are specified. The study revisits Whittle networks and order-independent queues as examples of quasi-reversible queueing systems. Finally, the article addresses admission control and applies the findings to optimization and reinforcement learning. This work contributes to enhancing the performance of queueing systems through strategic arrival control policies. <br />Summary: <div>
arXiv:2505.16353v1 Announce Type: new 
Abstract: In this paper, we introduce a versatile scheme for optimizing the arrival rates of quasi-reversible queueing systems. We first propose an alternative definition of quasi-reversibility that encompasses reversibility and highlights the importance of the definition of customer classes. In a second time, we introduce balanced arrival control policies, which generalize the notion of balanced arrival rates introduced in the context of Whittle networks, to the much broader class of quasi-reversible queueing systems. We prove that supplementing a quasi-reversible queueing system with a balanced arrival-control policy preserves the quasi-reversibility, and we specify the form of the stationary measures. We revisit two canonical examples of quasi-reversible queueing systems, Whittle networks and order-independent queues. Lastly, we focus on the problem of admission control and leverage our results in the frameworks of optimization and reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training</title>
<link>https://arxiv.org/abs/2505.16363</link>
<guid>https://arxiv.org/abs/2505.16363</guid>
<content:encoded><![CDATA[
<div> Keywords: AdamS, large language model, optimization performance, transformer objectives, theoretical guarantees

Summary:
AdamS is introduced as an efficient alternative optimizer for large language model (LLM) pretraining and post-training. It eliminates the need for second-moment estimates by using a novel denominator based on the momentum and current gradient. With comparable memory and compute footprint to SGD with momentum, AdamS offers superior optimization performance. It can inherit hyperparameters from AdamW and seamlessly integrate into existing pipelines without modifications. The motivation for AdamS comes from $(L_0, L_1)$ smoothness properties observed in transformer objectives. The optimizer is model-agnostic and provides rigorous theoretical convergence guarantees along with practical guidelines for hyperparameter selection. Empirical results show strong performance in pre-training runs on GPT-2 and Llama2, as well as reinforcement learning in post-training regimes. AdamS is positioned as a compelling optimizer choice due to its efficiency, simplicity, and solid theoretical foundation. 

<br /><br />Summary: <div>
arXiv:2505.16363v1 Announce Type: new 
Abstract: We introduce AdamS, a simple yet effective alternative to Adam for large language model (LLM) pretraining and post-training. By leveraging a novel denominator, i.e., the root of weighted sum of squares of the momentum and the current gradient, AdamS eliminates the need for second-moment estimates. Hence, AdamS is efficient, matching the memory and compute footprint of SGD with momentum while delivering superior optimization performance. Moreover, AdamS is easy to adopt: it can directly inherit hyperparameters of AdamW, and is entirely model-agnostic, integrating seamlessly into existing pipelines without modifications to optimizer APIs or architectures. The motivation behind AdamS stems from the observed $(L_0, L_1)$ smoothness properties in transformer objectives, where local smoothness is governed by gradient magnitudes that can be further approximated by momentum magnitudes. We establish rigorous theoretical convergence guarantees and provide practical guidelines for hyperparameter selection. Empirically, AdamS demonstrates strong performance in various tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B parameters) and reinforcement learning in post-training regimes. With its efficiency, simplicity, and theoretical grounding, AdamS stands as a compelling alternative to existing optimizers.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules</title>
<link>https://arxiv.org/abs/2505.16365</link>
<guid>https://arxiv.org/abs/2505.16365</guid>
<content:encoded><![CDATA[
arXiv:2505.16365v1 Announce Type: new 
Abstract: Developing new molecular compounds is crucial to address pressing challenges, from health to environmental sustainability. However, exploring the molecular space to discover new molecules is difficult due to the vastness of the space. Here we introduce CoCoGraph, a collaborative and constrained graph diffusion model capable of generating molecules that are guaranteed to be chemically valid. Thanks to the constraints built into the model and to the collaborative mechanism, CoCoGraph outperforms state-of-the-art approaches on standard benchmarks while requiring up to an order of magnitude fewer parameters. Analysis of 36 chemical properties also demonstrates that CoCoGraph generates molecules with distributions more closely matching real molecules than current models. Leveraging the model's efficiency, we created a database of 8.2M million synthetically generated molecules and conducted a Turing-like test with organic chemistry experts to further assess the plausibility of the generated molecules, and potential biases and limitations of CoCoGraph.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.16368</link>
<guid>https://arxiv.org/abs/2505.16368</guid>
<content:encoded><![CDATA[
arXiv:2505.16368v1 Announce Type: new 
Abstract: How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni TM-AE: A Scalable and Interpretable Embedding Model Using the Full Tsetlin Machine State Space</title>
<link>https://arxiv.org/abs/2505.16386</link>
<guid>https://arxiv.org/abs/2505.16386</guid>
<content:encoded><![CDATA[
arXiv:2505.16386v1 Announce Type: new 
Abstract: The increasing complexity of large-scale language models has amplified concerns regarding their interpretability and reusability. While traditional embedding models like Word2Vec and GloVe offer scalability, they lack transparency and often behave as black boxes. Conversely, interpretable models such as the Tsetlin Machine (TM) have shown promise in constructing explainable learning systems, though they previously faced limitations in scalability and reusability. In this paper, we introduce Omni Tsetlin Machine AutoEncoder (Omni TM-AE), a novel embedding model that fully exploits the information contained in the TM's state matrix, including literals previously excluded from clause formation. This method enables the construction of reusable, interpretable embeddings through a single training phase. Extensive experiments across semantic similarity, sentiment classification, and document clustering tasks show that Omni TM-AE performs competitively with and often surpasses mainstream embedding models. These results demonstrate that it is possible to balance performance, scalability, and interpretability in modern Natural Language Processing (NLP) systems without resorting to opaque architectures.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16400</link>
<guid>https://arxiv.org/abs/2505.16400</guid>
<content:encoded><![CDATA[
arXiv:2505.16400v1 Announce Type: new 
Abstract: Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games</title>
<link>https://arxiv.org/abs/2505.16401</link>
<guid>https://arxiv.org/abs/2505.16401</guid>
<content:encoded><![CDATA[
arXiv:2505.16401v1 Announce Type: new 
Abstract: Large language models (LLMs) have been observed to suddenly exhibit advanced reasoning abilities during reinforcement learning (RL), resembling an ``aha moment'' triggered by simple outcome-based rewards. While RL has proven effective in eliciting such breakthroughs in tasks involving mathematics, coding, and vision, it faces significant challenges in multi-scenario games. The diversity of game rules, interaction modes, and environmental complexities often leads to policies that perform well in one scenario but fail to generalize to others. Simply combining multiple scenarios during training introduces additional challenges, such as training instability and poor performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a framework designed to enhance generalization in multi-scenario RL. This approach starts by heuristically grouping games based on characteristics such as rules and difficulties. Specialized models are then trained for each group to excel at games in the group is what we refer to as the divide step. Next, we fuse model parameters from different groups as a new model, and continue training it for multiple groups, until the scenarios in all groups are conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align trained with the Divide-Fuse-Conquer strategy reaches a performance level comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can inspire future research on using reinforcement learning to improve the generalization of LLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach</title>
<link>https://arxiv.org/abs/2505.16403</link>
<guid>https://arxiv.org/abs/2505.16403</guid>
<content:encoded><![CDATA[
arXiv:2505.16403v1 Announce Type: new 
Abstract: Manipulation of local training data and local updates, i.e., the poisoning attack, is the main threat arising from the collaborative nature of the federated learning (FL) paradigm. Most existing poisoning attacks aim to manipulate local data/models in a way that causes denial-of-service (DoS) issues. In this paper, we introduce a novel attack method, named Federated Learning Sliding Attack (FedSA) scheme, aiming at precisely introducing the extent of poisoning in a subtle controlled manner. It operates with a predefined objective, such as reducing global model's prediction accuracy by 10\%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC) theory with model poisoning attacks. It can manipulate the updates from malicious clients to drive the global model towards a compromised state, achieving this at a controlled and inconspicuous rate. Additionally, leveraging the robust control properties of FedSA allows precise control over the convergence bounds, enabling the attacker to set the global accuracy of the poisoned model to any desired level. Experimental results demonstrate that FedSA can accurately achieve a predefined global accuracy with fewer malicious clients while maintaining a high level of stealth and adjustable learning rates.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.16446</link>
<guid>https://arxiv.org/abs/2505.16446</guid>
<content:encoded><![CDATA[
arXiv:2505.16446v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) enable powerful cross-modal reasoning capabilities. However, the expanded input space introduces new attack surfaces. Previous jailbreak attacks often inject malicious instructions from text into less aligned modalities, such as vision. As MLLMs increasingly incorporate cross-modal consistency and alignment mechanisms, such explicit attacks become easier to detect and block. In this work, we propose a novel implicit jailbreak framework termed IJA that stealthily embeds malicious instructions into images via least significant bit steganography and couples them with seemingly benign, image-related textual prompts. To further enhance attack effectiveness across diverse MLLMs, we incorporate adversarial suffixes generated by a surrogate model and introduce a template optimization module that iteratively refines both the prompt and embedding based on model feedback. On commercial models like GPT-4o and Gemini-1.5 Pro, our method achieves attack success rates of over 90% using an average of only 3 queries.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling</title>
<link>https://arxiv.org/abs/2505.16481</link>
<guid>https://arxiv.org/abs/2505.16481</guid>
<content:encoded><![CDATA[
arXiv:2505.16481v1 Announce Type: new 
Abstract: Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by replacing the fully factorised Gaussian prior with a GP prior, thereby capturing richer correlations among latent variables. However, performing exact GP inference in large-scale GPVAEs is computationally prohibitive, often forcing existing approaches to rely on restrictive kernel assumptions or large sets of inducing points. In this work, we propose a neighbour-driven approximation strategy that exploits local adjacencies in the latent space to achieve scalable GPVAE inference. By confining computations to the nearest neighbours of each data point, our method preserves essential latent dependencies, allowing more flexible kernel choices and mitigating the need for numerous inducing points. Through extensive experiments on tasks including representation learning, data imputation, and conditional generation, we demonstrate that our approach outperforms other GPVAE variants in both predictive performance and computational efficiency.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Non-negative Matrix Factorization for Guided Topic Modeling of Minority Topics</title>
<link>https://arxiv.org/abs/2505.16493</link>
<guid>https://arxiv.org/abs/2505.16493</guid>
<content:encoded><![CDATA[
arXiv:2505.16493v1 Announce Type: new 
Abstract: Topic models often fail to capture low-prevalence, domain-critical themes, so-called minority topics, such as mental health themes in online comments. While some existing methods can incorporate domain knowledge, such as expected topical content, methods allowing guidance may require overly detailed expected topics, hindering the discovery of topic divisions and variation. We propose a topic modeling solution via a specially constrained NMF. We incorporate a seed word list characterizing minority content of interest, but we do not require experts to pre-specify their division across minority topics. Through prevalence constraints on minority topics and seed word content across topics, we learn distinct data-driven minority topics as well as majority topics. The constrained NMF is fitted via Karush-Kuhn-Tucker (KKT) conditions with multiplicative updates. We outperform several baselines on synthetic data in terms of topic purity, normalized mutual information, and also evaluate topic quality using Jensen-Shannon divergence (JSD). We conduct a case study on YouTube vlog comments, analyzing viewer discussion of mental health content; our model successfully identifies and reveals this domain-relevant minority content.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accuracy vs. Accuracy: Computational Tradeoffs Between Classification Rates and Utility</title>
<link>https://arxiv.org/abs/2505.16494</link>
<guid>https://arxiv.org/abs/2505.16494</guid>
<content:encoded><![CDATA[
arXiv:2505.16494v1 Announce Type: new 
Abstract: We revisit the foundations of fairness and its interplay with utility and efficiency in settings where the training data contain richer labels, such as individual types, rankings, or risk estimates, rather than just binary outcomes. In this context, we propose algorithms that achieve stronger notions of evidence-based fairness than are possible in standard supervised learning. Our methods support classification and ranking techniques that preserve accurate subpopulation classification rates, as suggested by the underlying data distributions, across a broad class of classification rules and downstream applications. Furthermore, our predictors enable loss minimization, whether aimed at maximizing utility or in the service of fair treatment.
  Complementing our algorithmic contributions, we present impossibility results demonstrating that simultaneously achieving accurate classification rates and optimal loss minimization is, in some cases, computationally infeasible. Unlike prior impossibility results, our notions are not inherently in conflict and are simultaneously satisfied by the Bayes-optimal predictor. Furthermore, we show that each notion can be satisfied individually via efficient learning. Our separation thus stems from the computational hardness of learning a sufficiently good approximation of the Bayes-optimal predictor. These computational impossibilities present a choice between two natural and attainable notions of accuracy that could both be motivated by fairness.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computing Exact Shapley Values in Polynomial Time for Product-Kernel Methods</title>
<link>https://arxiv.org/abs/2505.16516</link>
<guid>https://arxiv.org/abs/2505.16516</guid>
<content:encoded><![CDATA[
arXiv:2505.16516v1 Announce Type: new 
Abstract: Kernel methods are widely used in machine learning due to their flexibility and expressive power. However, their black-box nature poses significant challenges to interpretability, limiting their adoption in high-stakes applications. Shapley value-based feature attribution techniques, such as SHAP and kernel-specific variants like RKHS-SHAP, offer a promising path toward explainability. Yet, computing exact Shapley values remains computationally intractable in general, motivating the development of various approximation schemes. In this work, we introduce PKeX-Shapley, a novel algorithm that utilizes the multiplicative structure of product kernels to enable the exact computation of Shapley values in polynomial time. We show that product-kernel models admit a functional decomposition that allows for a recursive formulation of Shapley values. This decomposition not only yields computational efficiency but also enhances interpretability in kernel-based learning. We also demonstrate how our framework can be generalized to explain kernel-based statistical discrepancies such as the Maximum Mean Discrepancy (MMD) and the Hilbert-Schmidt Independence Criterion (HSIC), thus offering new tools for interpretable statistical inference.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Relational Database Generation via Graph-Conditional Diffusion Models</title>
<link>https://arxiv.org/abs/2505.16527</link>
<guid>https://arxiv.org/abs/2505.16527</guid>
<content:encoded><![CDATA[
arXiv:2505.16527v1 Announce Type: new 
Abstract: Building generative models for relational databases (RDBs) is important for applications like privacy-preserving data release and augmenting real datasets. However, most prior work either focuses on single-table generation or relies on autoregressive factorizations that impose a fixed table order and generate tables sequentially. This approach limits parallelism, restricts flexibility in downstream applications like missing value imputation, and compounds errors due to commonly made conditional independence assumptions. We propose a fundamentally different approach: jointly modeling all tables in an RDB without imposing any order. By using a natural graph representation of RDBs, we propose the Graph-Conditional Relational Diffusion Model (GRDM). GRDM leverages a graph neural network to jointly denoise row attributes and capture complex inter-table dependencies. Extensive experiments on six real-world RDBs demonstrate that our approach substantially outperforms autoregressive baselines in modeling multi-hop inter-table correlations and achieves state-of-the-art performance on single-table fidelity metrics.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOFT: Householder Orthogonal Fine-tuning</title>
<link>https://arxiv.org/abs/2505.16531</link>
<guid>https://arxiv.org/abs/2505.16531</guid>
<content:encoded><![CDATA[
arXiv:2505.16531v1 Announce Type: new 
Abstract: Adaptation of foundation models using low-rank methods is a widespread approach. Another way to adapt these models is to employ orthogonal fine-tuning methods, which are less time and memory efficient despite their good generalization properties. In this work, we propose Householder Orthogonal Fine-tuning (HOFT), a novel orthogonal fine-tuning method that aims to alleviate time and space complexity. Moreover, some theoretical properties of the orthogonal fine-tuning paradigm are explored. From this exploration, Scaled Householder Orthogonal Fine-tuning (SHOFT) is proposed. Both HOFT and SHOFT are evaluated in downstream tasks, namely commonsense reasoning, machine translation, subject-driven generation and mathematical reasoning. Compared with state-of-the-art adaptation methods, HOFT and SHOFT show comparable or better results.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incremental Sequence Classification with Temporal Consistency</title>
<link>https://arxiv.org/abs/2505.16548</link>
<guid>https://arxiv.org/abs/2505.16548</guid>
<content:encoded><![CDATA[
arXiv:2505.16548v1 Announce Type: new 
Abstract: We address the problem of incremental sequence classification, where predictions are updated as new elements in the sequence are revealed. Drawing on temporal-difference learning from reinforcement learning, we identify a temporal-consistency condition that successive predictions should satisfy. We leverage this condition to develop a novel loss function for training incremental sequence classifiers. Through a concrete example, we demonstrate that optimizing this loss can offer substantial gains in data efficiency. We apply our method to text classification tasks and show that it improves predictive accuracy over competing approaches on several benchmark datasets. We further evaluate our approach on the task of verifying large language model generations for correctness in grade-school math problems. Our results show that models trained with our method are better able to distinguish promising generations from unpromising ones after observing only a few tokens.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Coordinate- and Dimension-Agnostic Machine Learning for Partial Differential Equations</title>
<link>https://arxiv.org/abs/2505.16549</link>
<guid>https://arxiv.org/abs/2505.16549</guid>
<content:encoded><![CDATA[
arXiv:2505.16549v1 Announce Type: new 
Abstract: The machine learning methods for data-driven identification of partial differential equations (PDEs) are typically defined for a given number of spatial dimensions and a choice of coordinates the data have been collected in. This dependence prevents the learned evolution equation from generalizing to other spaces. In this work, we reformulate the problem in terms of coordinate- and dimension-independent representations, paving the way toward what we call ``spatially liberated" PDE learning. To this end, we employ a machine learning approach to predict the evolution of scalar field systems expressed in the formalism of exterior calculus, which is coordinate-free and immediately generalizes to arbitrary dimensions by construction. We demonstrate the performance of this approach in the FitzHugh-Nagumo and Barkley reaction-diffusion models, as well as the Patlak-Keller-Segel model informed by in-situ chemotactic bacteria observations. We provide extensive numerical experiments that demonstrate that our approach allows for seamless transitions across various spatial contexts. We show that the field dynamics learned in one space can be used to make accurate predictions in other spaces with different dimensions, coordinate systems, boundary conditions, and curvatures.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Two-Stage Data Selection Framework for Data-Efficient Model Training on Edge Devices</title>
<link>https://arxiv.org/abs/2505.16563</link>
<guid>https://arxiv.org/abs/2505.16563</guid>
<content:encoded><![CDATA[
arXiv:2505.16563v1 Announce Type: new 
Abstract: The demand for machine learning (ML) model training on edge devices is escalating due to data privacy and personalized service needs. However, we observe that current on-device model training is hampered by the under-utilization of on-device data, due to low training throughput, limited storage and diverse data importance. To improve data resource utilization, we propose a two-stage data selection framework {\sf Titan} to select the most important data batch from streaming data for model training with guaranteed efficiency and effectiveness. Specifically, in the first stage, {\sf Titan} filters out a candidate dataset with potentially high importance in a coarse-grained manner.In the second stage of fine-grained selection, we propose a theoretically optimal data selection strategy to identify the data batch with the highest model performance improvement to current training round. To further enhance time-and-resource efficiency, {\sf Titan} leverages a pipeline to co-execute data selection and model training, and avoids resource conflicts by exploiting idle computing resources. We evaluate {\sf Titan} on real-world edge devices and three representative edge computing tasks with diverse models and data modalities. Empirical results demonstrate that {\sf Titan} achieves up to $43\%$ reduction in training time and $6.2\%$ increase in final accuracy with minor system overhead, such as data processing delay, memory footprint and energy consumption.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finetuning-Activated Backdoors in LLMs</title>
<link>https://arxiv.org/abs/2505.16567</link>
<guid>https://arxiv.org/abs/2505.16567</guid>
<content:encoded><![CDATA[
arXiv:2505.16567v1 Announce Type: new 
Abstract: Finetuning openly accessible Large Language Models (LLMs) has become standard practice for achieving task-specific performance improvements. Until now, finetuning has been regarded as a controlled and secure process in which training on benign datasets led to predictable behaviors. In this paper, we demonstrate for the first time that an adversary can create poisoned LLMs that initially appear benign but exhibit malicious behaviors once finetuned by downstream users. To this end, our proposed attack, FAB (Finetuning-Activated Backdoor), poisons an LLM via meta-learning techniques to simulate downstream finetuning, explicitly optimizing for the emergence of malicious behaviors in the finetuned models. At the same time, the poisoned LLM is regularized to retain general capabilities and to exhibit no malicious behaviors prior to finetuning. As a result, when users finetune the seemingly benign model on their own datasets, they unknowingly trigger its hidden backdoor behavior. We demonstrate the effectiveness of FAB across multiple LLMs and three target behaviors: unsolicited advertising, refusal, and jailbreakability. Additionally, we show that FAB-backdoors are robust to various finetuning choices made by the user (e.g., dataset, number of steps, scheduler). Our findings challenge prevailing assumptions about the security of finetuning, revealing yet another critical attack vector exploiting the complexities of LLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Empowered Interactive Load Forecasting</title>
<link>https://arxiv.org/abs/2505.16577</link>
<guid>https://arxiv.org/abs/2505.16577</guid>
<content:encoded><![CDATA[
arXiv:2505.16577v1 Announce Type: new 
Abstract: The growing complexity of power systems has made accurate load forecasting more important than ever. An increasing number of advanced load forecasting methods have been developed. However, the static design of current methods offers no mechanism for human-model interaction. As the primary users of forecasting models, system operators often find it difficult to understand and apply these advanced models, which typically requires expertise in artificial intelligence (AI). This also prevents them from incorporating their experience and real-world contextual understanding into the forecasting process. Recent breakthroughs in large language models (LLMs) offer a new opportunity to address this issue. By leveraging their natural language understanding and reasoning capabilities, we propose an LLM-based multi-agent collaboration framework to bridge the gap between human operators and forecasting models. A set of specialized agents is designed to perform different tasks in the forecasting workflow and collaborate via a dedicated communication mechanism. This framework embeds interactive mechanisms throughout the load forecasting pipeline, reducing the technical threshold for non-expert users and enabling the integration of human experience. Our experiments demonstrate that the interactive load forecasting accuracy can be significantly improved when users provide proper insight in key stages. Our cost analysis shows that the framework remains affordable, making it practical for real-world deployment.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16581</link>
<guid>https://arxiv.org/abs/2505.16581</guid>
<content:encoded><![CDATA[
arXiv:2505.16581v1 Announce Type: new 
Abstract: In the zero-shot policy transfer setting in reinforcement learning, the goal is to train an agent on a fixed set of training environments so that it can generalise to similar, but unseen, testing environments. Previous work has shown that policy distillation after training can sometimes produce a policy that outperforms the original in the testing environments. However, it is not yet entirely clear why that is, or what data should be used to distil the policy. In this paper, we prove, under certain assumptions, a generalisation bound for policy distillation after training. The theory provides two practical insights: for improved generalisation, you should 1) train an ensemble of distilled policies, and 2) distil it on as much data from the training environments as possible. We empirically verify that these insights hold in more general settings, when the assumptions required for the theory no longer hold. Finally, we demonstrate that an ensemble of policies distilled on a diverse dataset can generalise significantly better than the original agent.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training on Plausible Counterfactuals Removes Spurious Correlations</title>
<link>https://arxiv.org/abs/2505.16583</link>
<guid>https://arxiv.org/abs/2505.16583</guid>
<content:encoded><![CDATA[
arXiv:2505.16583v1 Announce Type: new 
Abstract: Plausible counterfactual explanations (p-CFEs) are perturbations that minimally modify inputs to change classifier decisions while remaining plausible under the data distribution. In this study, we demonstrate that classifiers can be trained on p-CFEs labeled with induced \emph{incorrect} target classes to classify unperturbed inputs with the original labels. While previous studies have shown that such learning is possible with adversarial perturbations, we extend this paradigm to p-CFEs. Interestingly, our experiments reveal that learning from p-CFEs is even more effective: the resulting classifiers achieve not only high in-distribution accuracy but also exhibit significantly reduced bias with respect to spurious correlations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalDynamics: A large-scale benchmark for structural discovery of dynamical causal models</title>
<link>https://arxiv.org/abs/2505.16620</link>
<guid>https://arxiv.org/abs/2505.16620</guid>
<content:encoded><![CDATA[
arXiv:2505.16620v1 Announce Type: new 
Abstract: Causal discovery for dynamical systems poses a major challenge in fields where active interventions are infeasible. Most methods used to investigate these systems and their associated benchmarks are tailored to deterministic, low-dimensional and weakly nonlinear time-series data. To address these limitations, we present CausalDynamics, a large-scale benchmark and extensible data generation framework to advance the structural discovery of dynamical causal models. Our benchmark consists of true causal graphs derived from thousands of coupled ordinary and stochastic differential equations as well as two idealized climate models. We perform a comprehensive evaluation of state-of-the-art causal discovery algorithms for graph reconstruction on systems with noisy, confounded, and lagged dynamics. CausalDynamics consists of a plug-and-play, build-your-own coupling workflow that enables the construction of a hierarchy of physical systems. We anticipate that our framework will facilitate the development of robust causal discovery algorithms that are broadly applicable across domains while addressing their unique challenges. We provide a user-friendly implementation and documentation on https://kausable.github.io/CausalDynamics.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multivariate Latent Recalibration for Conditional Normalizing Flows</title>
<link>https://arxiv.org/abs/2505.16636</link>
<guid>https://arxiv.org/abs/2505.16636</guid>
<content:encoded><![CDATA[
arXiv:2505.16636v1 Announce Type: new 
Abstract: Reliably characterizing the full conditional distribution of a multivariate response variable given a set of covariates is crucial for trustworthy decision-making. However, misspecified or miscalibrated multivariate models may yield a poor approximation of the joint distribution of the response variables, leading to unreliable predictions and suboptimal decisions. Furthermore, standard recalibration methods are primarily limited to univariate settings, while conformal prediction techniques, despite generating multivariate prediction regions with coverage guarantees, do not provide a full probability density function. We address this gap by first introducing a novel notion of latent calibration, which assesses probabilistic calibration in the latent space of a conditional normalizing flow. Second, we propose latent recalibration (LR), a novel post-hoc model recalibration method that learns a transformation of the latent space with finite-sample bounds on latent calibration. Unlike existing methods, LR produces a recalibrated distribution with an explicit multivariate density function while remaining computationally efficient. Extensive experiments on both tabular and image datasets show that LR consistently improves latent calibration error and the negative log-likelihood of the recalibrated models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconsidering Fairness Through Unawareness from the Perspective of Model Multiplicity</title>
<link>https://arxiv.org/abs/2505.16638</link>
<guid>https://arxiv.org/abs/2505.16638</guid>
<content:encoded><![CDATA[
arXiv:2505.16638v1 Announce Type: new 
Abstract: Fairness through Unawareness (FtU) describes the idea that discrimination against demographic groups can be avoided by not considering group membership in the decisions or predictions. This idea has long been criticized in the machine learning literature as not being sufficient to ensure fairness. In addition, the use of additional features is typically thought to increase the accuracy of the predictions for all groups, so that FtU is sometimes thought to be detrimental to all groups. In this paper, we show both theoretically and empirically that FtU can reduce algorithmic discrimination without necessarily reducing accuracy. We connect this insight with the literature on Model Multiplicity, to which we contribute with novel theoretical and empirical results. Furthermore, we illustrate how, in a real-life application, FtU can contribute to the deployment of more equitable policies without losing efficacy. Our findings suggest that FtU is worth considering in practical applications, particularly in high-risk scenarios, and that the use of protected attributes such as gender in predictive models should be accompanied by a clear and well-founded justification.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Forward-Forward Learning through Representational Dimensionality Compression</title>
<link>https://arxiv.org/abs/2505.16649</link>
<guid>https://arxiv.org/abs/2505.16649</guid>
<content:encoded><![CDATA[
arXiv:2505.16649v1 Announce Type: new 
Abstract: The Forward-Forward (FF) algorithm provides a bottom-up alternative to backpropagation (BP) for training neural networks, relying on a layer-wise "goodness" function to guide learning. Existing goodness functions, inspired by energy-based learning (EBL), are typically defined as the sum of squared post-synaptic activations, neglecting the correlations between neurons. In this work, we propose a novel goodness function termed dimensionality compression that uses the effective dimensionality (ED) of fluctuating neural responses to incorporate second-order statistical structure. Our objective minimizes ED for clamped inputs when noise is considered while maximizing it across the sample distribution, promoting structured representations without the need to prepare negative samples. We demonstrate that this formulation achieves competitive performance compared to other non-BP methods. Moreover, we show that noise plays a constructive role that can enhance generalization and improve inference when predictions are derived from the mean of squared outputs, which is equivalent to making predictions based on the energy term. Our findings contribute to the development of more biologically plausible learning algorithms and suggest a natural fit for neuromorphic computing, where stochasticity is a computational resource rather than a nuisance. The code is available at https://github.com/ZhichaoZhu/StochasticForwardForward
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries</title>
<link>https://arxiv.org/abs/2505.16664</link>
<guid>https://arxiv.org/abs/2505.16664</guid>
<content:encoded><![CDATA[
arXiv:2505.16664v1 Announce Type: new 
Abstract: Accurate prediction of the Remaining Useful Life (RUL) is essential for enabling timely maintenance of lithium-ion batteries, impacting the operational efficiency of electric applications that rely on them. This paper proposes a RUL prediction approach that leverages data from recent charge-discharge cycles to estimate the number of remaining usable cycles. The approach introduces both a novel signal processing pipeline and a deep learning prediction model. In the signal preprocessing pipeline, a derived capacity feature is computed based on current and capacity signals. Alongside original capacity, voltage and current, these features are denoised and enhanced using statistical metrics and a delta-based method to capture differences between the current and previous cycles. In the prediction model, the processed features are then fed into a hybrid deep learning architecture composed of 1D Convolutional Neural Networks (CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential Equation-based LSTM (ODE-LSTM) modules. This architecture is designed to capture both local signal characteristics and long-range temporal dependencies while modeling the continuous-time dynamics of battery degradation. The model is further evaluated using transfer learning across different learning strategies and target data partitioning scenarios. Results indicate that the model maintains robust performance, even when fine-tuned on limited target data. Experimental results on two publicly available large-scale datasets demonstrate that the proposed method outperforms a baseline deep learning approach and machine learning techniques, achieving an RMSE of 101.59, highlighting its strong potential for real-world RUL prediction applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Feature Optimization for Enhanced Clustering of Blockchain Transaction Data</title>
<link>https://arxiv.org/abs/2505.16672</link>
<guid>https://arxiv.org/abs/2505.16672</guid>
<content:encoded><![CDATA[
arXiv:2505.16672v1 Announce Type: new 
Abstract: Blockchain transaction data exhibits high dimensionality, noise, and intricate feature entanglement, presenting significant challenges for traditional clustering algorithms. In this study, we conduct a comparative analysis of three clustering approaches: (1) Classical K-Means Clustering, applied to pre-processed feature representations; (2) Hybrid Clustering, wherein classical features are enhanced with quantum random features extracted using randomly initialized quantum neural networks (QNNs); and (3) Fully Quantum Clustering, where a QNN is trained in a self-supervised manner leveraging a SwAV-based loss function to optimize the feature space for clustering directly. The proposed experimental framework systematically investigates the impact of quantum circuit depth and the number of learned prototypes, demonstrating that even shallow quantum circuits can effectively extract meaningful non-linear representations, significantly improving clustering performance.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Out-of-Distribution Generalization of Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2505.16675</link>
<guid>https://arxiv.org/abs/2505.16675</guid>
<content:encoded><![CDATA[
arXiv:2505.16675v1 Announce Type: new 
Abstract: In this paper, we focus on the out-of-distribution (OOD) generalization of self-supervised learning (SSL). By analyzing the mini-batch construction during the SSL training phase, we first give one plausible explanation for SSL having OOD generalization. Then, from the perspective of data generation and causal inference, we analyze and conclude that SSL learns spurious correlations during the training process, which leads to a reduction in OOD generalization. To address this issue, we propose a post-intervention distribution (PID) grounded in the Structural Causal Model. PID offers a scenario where the spurious variable and label variable is mutually independent. Besides, we demonstrate that if each mini-batch during SSL training satisfies PID, the resulting SSL model can achieve optimal worst-case OOD performance. This motivates us to develop a batch sampling strategy that enforces PID constraints through the learning of a latent variable model. Through theoretical analysis, we demonstrate the identifiability of the latent variable model and validate the effectiveness of the proposed sampling strategy. Experiments conducted on various downstream OOD tasks demonstrate the effectiveness of the proposed sampling strategy.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Genomic Structure from $k$-mers</title>
<link>https://arxiv.org/abs/2505.16680</link>
<guid>https://arxiv.org/abs/2505.16680</guid>
<content:encoded><![CDATA[
arXiv:2505.16680v1 Announce Type: new 
Abstract: Sequencing a genome to determine an individual's DNA produces an enormous number of short nucleotide subsequences known as reads, which must be reassembled to reconstruct the full genome. We present a method for analyzing this type of data using contrastive learning, in which an encoder model is trained to produce embeddings that cluster together sequences from the same genomic region. The sequential nature of genomic regions is preserved in the form of trajectories through this embedding space. Trained solely to reflect the structure of the genome, the resulting model provides a general representation of $k$-mer sequences, suitable for a range of downstream tasks involving read data. We apply our framework to learn the structure of the $E.\ coli$ genome, and demonstrate its use in simulated ancient DNA (aDNA) read mapping and identification of structural variations. Furthermore, we illustrate the potential of using this type of model for metagenomic species identification. We show how incorporating a domain-specific noise model can enhance embedding robustness, and how a supervised contrastive learning setting can be adopted when a linear reference genome is available, by introducing a distance thresholding parameter $\Gamma$. The model can also be trained fully self-supervised on read data, enabling analysis without the need to construct a full genome assembly using specialized algorithms. Small prediction heads based on a pre-trained embedding are shown to perform on par with BWA-aln, the current gold standard approach for aDNA mapping, in terms of accuracy and runtime for short genomes. Given the method's favorable scaling properties with respect to total genome size, inference using our approach is highly promising for metagenomic applications and for mapping to genomes comparable in size to the human genome.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator</title>
<link>https://arxiv.org/abs/2505.16690</link>
<guid>https://arxiv.org/abs/2505.16690</guid>
<content:encoded><![CDATA[
arXiv:2505.16690v1 Announce Type: new 
Abstract: Post-training of large language models is essential for adapting pre-trained language models (PLMs) to align with human preferences and downstream tasks. While PLMs typically exhibit well-calibrated confidence, post-trained language models (PoLMs) often suffer from over-confidence, assigning high confidence to both correct and incorrect outputs, which can undermine reliability in critical applications. A major obstacle in calibrating PoLMs is the scarcity of labeled data for individual downstream tasks. To address this, we propose Disagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to optimize the parameters (e.g., temperature $\tau$) in post-hoc confidence calibration. Our method is motivated by the under-confidence issue caused by prediction disagreement between the PLM and PoLM while aligning their confidence via temperature scaling. Theoretically, the PLM's confidence underestimates PoLM's prediction accuracy on disagreement examples, causing a larger $\tau$ and producing under-confident predictions. DACA mitigates this by selectively using only agreement examples for calibration, effectively decoupling the influence of disagreement. In this manner, our method avoids an overly large $\tau$ in temperature scaling caused by disagreement examples, improving calibration performance. Extensive experiments demonstrate the effectiveness of our method, improving the average ECE of open-sourced and API-based LLMs (e.g. GPT-4o) by up to 15.08$\%$ on common benchmarks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations</title>
<link>https://arxiv.org/abs/2505.16705</link>
<guid>https://arxiv.org/abs/2505.16705</guid>
<content:encoded><![CDATA[
arXiv:2505.16705v1 Announce Type: new 
Abstract: Concept bottleneck models (CBMs) ensure interpretability by decomposing predictions into human interpretable concepts. Yet the annotations used for training CBMs that enable this transparency are often noisy, and the impact of such corruption is not well understood. In this study, we present the first systematic study of noise in CBMs and show that even moderate corruption simultaneously impairs prediction performance, interpretability, and the intervention effectiveness. Our analysis identifies a susceptible subset of concepts whose accuracy declines far more than the average gap between noisy and clean supervision and whose corruption accounts for most performance loss. To mitigate this vulnerability we propose a two-stage framework. During training, sharpness-aware minimization stabilizes the learning of noise-sensitive concepts. During inference, where clean labels are unavailable, we rank concepts by predictive entropy and correct only the most uncertain ones, using uncertainty as a proxy for susceptibility. Theoretical analysis and extensive ablations elucidate why sharpness-aware training confers robustness and why uncertainty reliably identifies susceptible concepts, providing a principled basis that preserves both interpretability and resilience in the presence of noise.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Long-Context LLMs Efficiently via Chunk-wise Optimization</title>
<link>https://arxiv.org/abs/2505.16710</link>
<guid>https://arxiv.org/abs/2505.16710</guid>
<content:encoded><![CDATA[
arXiv:2505.16710v1 Announce Type: new 
Abstract: While long-context large language models (LLMs) exhibit remarkable document processing capabilities, their prohibitively high training costs often hinder customized applications. To mitigate this issue, we propose \textit{Sequential Chunk-wise Optimization} (SeCO), a memory-efficient training paradigm that partitions lengthy inputs into manageable chunks. Each chunk independently constructs its computational graph and performs localized backpropagation, ensuring that only one chunk's forward activations are stored in memory. Building on SeCO, we further introduce \textit{Sparse Chunk-wise Optimization} (SpaCO), which reduces computational overhead by selectively propagating gradients to specific chunks and incorporates a carefully designed compensation factor to ensure unbiased gradient estimation. SpaCO decouples the computational cost of backpropagation from the context length, enabling training time to gradually converge to inference time as sequences become longer. Implemented as lightweight training wrappers, both SeCO and SpaCO offer substantial practical benefits. For example, when fine-tuning an 8B model with LoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to 16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up to 3x faster than SeCO under the same experimental setup. These innovations provide new insights into optimizing long-context models, making them more accessible for practical applications. We have open-sourced the code at \href{https://github.com/wenhaoli-xmu/seco}{here}.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Brainwave Modeling with a Codebook-Based Foundation Model</title>
<link>https://arxiv.org/abs/2505.16724</link>
<guid>https://arxiv.org/abs/2505.16724</guid>
<content:encoded><![CDATA[
arXiv:2505.16724v1 Announce Type: new 
Abstract: Recent advances in large-scale pre-trained Electroencephalogram (EEG) models have shown great promise, driving progress in Brain-Computer Interfaces (BCIs) and healthcare applications. However, despite their success, many existing pre-trained models have struggled to fully capture the rich information content of neural oscillations, a limitation that fundamentally constrains their performance and generalizability across diverse BCI tasks. This limitation is frequently rooted in suboptimal architectural design choices which constrain their representational capacity. In this work, we introduce LaBraM++, an enhanced Large Brainwave Foundation Model (LBM) that incorporates principled improvements grounded in robust signal processing foundations. LaBraM++ demonstrates substantial gains across a variety of tasks, consistently outperforming its originally-based architecture and achieving competitive results when compared to other open-source LBMs. Its superior performance and training efficiency highlight its potential as a strong foundation for future advancements in LBMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Conditioning for Deep Generative Models</title>
<link>https://arxiv.org/abs/2505.16725</link>
<guid>https://arxiv.org/abs/2505.16725</guid>
<content:encoded><![CDATA[
arXiv:2505.16725v1 Announce Type: new 
Abstract: Datasets in engineering domains are often small, sparsely labeled, and contain numerical as well as categorical conditions. Additionally. computational resources are typically limited in practical applications which hinders the adoption of generative models for engineering tasks. We introduce a novel masked-conditioning approach, that enables generative models to work with sparse, mixed-type data. We mask conditions during training to simulate sparse conditions at inference time. For this purpose, we explore the use of various sparsity schedules that show different strengths and weaknesses. In addition, we introduce a flexible embedding that deals with categorical as well as numerical conditions. We integrate our method into an efficient variational autoencoder as well as a latent diffusion model and demonstrate the applicability of our approach on two engineering-related datasets of 2D point clouds and images. Finally, we show that small models trained on limited data can be coupled with large pretrained foundation models to improve generation quality while retaining the controllability induced by our conditioning scheme.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Monte Carlo for Policy Optimization in Continuous POMDPs</title>
<link>https://arxiv.org/abs/2505.16732</link>
<guid>https://arxiv.org/abs/2505.16732</guid>
<content:encoded><![CDATA[
arXiv:2505.16732v1 Announce Type: new 
Abstract: Optimal decision-making under partial observability requires agents to balance reducing uncertainty (exploration) against pursuing immediate objectives (exploitation). In this paper, we introduce a novel policy optimization framework for continuous partially observable Markov decision processes (POMDPs) that explicitly addresses this challenge. Our method casts policy learning as probabilistic inference in a non-Markovian Feynman--Kac model that inherently captures the value of information gathering by anticipating future observations, without requiring extrinsic exploration bonuses or handcrafted heuristics. To optimize policies under this model, we develop a nested sequential Monte Carlo~(SMC) algorithm that efficiently estimates a history-dependent policy gradient under samples from the optimal trajectory distribution induced by the POMDP. We demonstrate the effectiveness of our algorithm across standard continuous POMDP benchmarks, where existing methods struggle to act under uncertainty.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forward-only Diffusion Probabilistic Models</title>
<link>https://arxiv.org/abs/2505.16733</link>
<guid>https://arxiv.org/abs/2505.16733</guid>
<content:encoded><![CDATA[
arXiv:2505.16733v1 Announce Type: new 
Abstract: This work presents a forward-only diffusion (FoD) approach for generative modelling. In contrast to traditional diffusion models that rely on a coupled forward-backward diffusion scheme, FoD directly learns data generation through a single forward diffusion process, yielding a simple yet efficient generative framework. The core of FoD is a state-dependent linear stochastic differential equation that involves a mean-reverting term in both the drift and diffusion functions. This mean-reversion property guarantees the convergence to clean data, naturally simulating a stochastic interpolation between source and target distributions. More importantly, FoD is analytically tractable and is trained using a simple stochastic flow matching objective, enabling a few-step non-Markov chain sampling during inference. The proposed FoD model, despite its simplicity, achieves competitive performance on various image-conditioned (e.g., image restoration) and unconditional generation tasks, demonstrating its effectiveness in generative modelling. Our code is available at https://github.com/Algolzw/FoD.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum Total Correlation Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16734</link>
<guid>https://arxiv.org/abs/2505.16734</guid>
<content:encoded><![CDATA[
arXiv:2505.16734v1 Announce Type: new 
Abstract: Simplicity is a powerful inductive bias. In reinforcement learning, regularization is used for simpler policies, data augmentation for simpler representations, and sparse reward functions for simpler objectives, all that, with the underlying motivation to increase generalizability and robustness by focusing on the essentials. Supplementary to these techniques, we investigate how to promote simple behavior throughout the episode. To that end, we introduce a modification of the reinforcement learning problem that additionally maximizes the total correlation within the induced trajectories. We propose a practical algorithm that optimizes all models, including policy and state representation, based on a lower-bound approximation. In simulated robot environments, our method naturally generates policies that induce periodic and compressible trajectories, and that exhibit superior robustness to noise and changes in dynamics compared to baseline methods, while also improving performance in the original tasks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backward Oversmoothing: why is it hard to train deep Graph Neural Networks?</title>
<link>https://arxiv.org/abs/2505.16736</link>
<guid>https://arxiv.org/abs/2505.16736</guid>
<content:encoded><![CDATA[
arXiv:2505.16736v1 Announce Type: new 
Abstract: Oversmoothing has long been identified as a major limitation of Graph Neural Networks (GNNs): input node features are smoothed at each layer and converge to a non-informative representation, if the weights of the GNN are sufficiently bounded. This assumption is crucial: if, on the contrary, the weights are sufficiently large, then oversmoothing may not happen. Theoretically, GNN could thus learn to not oversmooth. However it does not really happen in practice, which prompts us to examine oversmoothing from an optimization point of view. In this paper, we analyze backward oversmoothing, that is, the notion that backpropagated errors used to compute gradients are also subject to oversmoothing from output to input. With non-linear activation functions, we outline the key role of the interaction between forward and backward smoothing. Moreover, we show that, due to backward oversmoothing, GNNs provably exhibit many spurious stationary points: as soon as the last layer is trained, the whole GNN is at a stationary point. As a result, we can exhibit regions where gradients are near-zero while the loss stays high. The proof relies on the fact that, unlike forward oversmoothing, backward errors are subjected to a linear oversmoothing even in the presence of non-linear activation function, such that the average of the output error plays a key role. Additionally, we show that this phenomenon is specific to deep GNNs, and exhibit counter-example Multi-Layer Perceptron. This paper is a step toward a more complete comprehension of the optimization landscape specific to GNNs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization</title>
<link>https://arxiv.org/abs/2505.16737</link>
<guid>https://arxiv.org/abs/2505.16737</guid>
<content:encoded><![CDATA[
arXiv:2505.16737v1 Announce Type: new 
Abstract: The significant progress of large language models (LLMs) has led to remarkable achievements across numerous applications. However, their ability to generate harmful content has sparked substantial safety concerns. Despite the implementation of safety alignment techniques during the pre-training phase, recent research indicates that fine-tuning LLMs on adversarial or even benign data can inadvertently compromise their safety. In this paper, we re-examine the fundamental issue of why fine-tuning on non-harmful data still results in safety degradation. We introduce a safety-aware probing (SAP) optimization framework designed to mitigate the safety risks of fine-tuning LLMs. Specifically, SAP incorporates a safety-aware probe into the gradient propagation process, mitigating the model's risk of safety degradation by identifying potential pitfalls in gradient directions, thereby enhancing task-specific performance while successfully preserving model safety. Our extensive experimental results demonstrate that SAP effectively reduces harmfulness below the original fine-tuned model and achieves comparable test loss to standard fine-tuning methods. Our code is available at https://github.com/ChengcanWu/SAP.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-reinforcement learning with minimum attention</title>
<link>https://arxiv.org/abs/2505.16741</link>
<guid>https://arxiv.org/abs/2505.16741</guid>
<content:encoded><![CDATA[
arXiv:2505.16741v1 Announce Type: new 
Abstract: Minimum attention applies the least action principle in the changes of control concerning state and time, first proposed by Brockett. The involved regularization is highly relevant in emulating biological control, such as motor learning. We apply minimum attention in reinforcement learning (RL) as part of the rewards and investigate its connection to meta-learning and stabilization. Specifically, model-based meta-learning with minimum attention is explored in high-dimensional nonlinear dynamics. Ensemble-based model learning and gradient-based meta-policy learning are alternately performed. Empirically, we show that the minimum attention does show outperforming competence in comparison to the state-of-the-art algorithms in model-free and model-based RL, i.e., fast adaptation in few shots and variance reduction from the perturbations of the model and environment. Furthermore, the minimum attention demonstrates the improvement in energy efficiency.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revenue Optimization with Price-Sensitive and Interdependent Demand</title>
<link>https://arxiv.org/abs/2505.16748</link>
<guid>https://arxiv.org/abs/2505.16748</guid>
<content:encoded><![CDATA[
arXiv:2505.16748v1 Announce Type: new 
Abstract: As Kalyan T. Talluri and Garrett J. Van Ryzin describe in their work [3], Revenue Management aims to maximize an organization's revenue by considering three types of decision categories: structural, pricing, and quantity. In this document, our primary focus will be on decisions related to pricing and quantity for the sale of airline tickets on a direct flight over a certain number of time periods. More specifically, we will only focus on the optimization aspect of this problem. We will assume the demand data to be given, since Air France estimates it beforehand using real data. Similarly, we assume all price options to be predetermined by Air France's algorithms and verified by their analysts. Our objective will be to maximize the revenue of a direct flight by choosing the prices for each product from the predefined set of options.
  --
  Comme d\'ecrit par Kalyan T. Talluri et Garrett J. Van Ryzin dans leur ouvrage [3], le Revenue Management consiste en la maximisation du revenu d'un organisme \`a partir de trois types de cat\'egories de d\'ecision : structurelles, prix et quantit\'e. Dans ce document, nous nous int\'eresserons principalement aux d\'ecisions de type prix et quantit\'e pour la vente de billets d'avion sur un vol direct au cours d'un certain nombre de pas de temps. Plus pr\'ecis\'ement, nous nous situerons dans la partie optimisation du probl\`eme. Nous prendrons ainsi les donn\'ees de demande comme acquises, car elles sont estim\'ees au pr\'ealable par Air France \`a partir des donn\'ees r\'eelles. De m\^eme, pour chaque produit que l'on cherchera \`a vendre, on nous impose en amont les prix possibles que l'on a droit d'utiliser et qui se basent sur des algorithmes d'Air France dont les r\'esultats sont v\'erifi\'es par des analystes. Notre but sera alors de maximiser le revenu d'un vol direct en choisissant les prix de chaque produit parmi ceux impos\'es.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyTupli: A Scalable Infrastructure for Collaborative Offline Reinforcement Learning Projects</title>
<link>https://arxiv.org/abs/2505.16754</link>
<guid>https://arxiv.org/abs/2505.16754</guid>
<content:encoded><![CDATA[
arXiv:2505.16754v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) has gained traction as a powerful paradigm for learning control policies from pre-collected data, eliminating the need for costly or risky online interactions. While many open-source libraries offer robust implementations of offline RL algorithms, they all rely on datasets composed of experience tuples consisting of state, action, next state, and reward. Managing, curating, and distributing such datasets requires suitable infrastructure. Although static datasets exist for established benchmark problems, no standardized or scalable solution supports developing and sharing datasets for novel or user-defined benchmarks. To address this gap, we introduce PyTupli, a Python-based tool to streamline the creation, storage, and dissemination of benchmark environments and their corresponding tuple datasets. PyTupli includes a lightweight client library with defined interfaces for uploading and retrieving benchmarks and data. It supports fine-grained filtering at both the episode and tuple level, allowing researchers to curate high-quality, task-specific datasets. A containerized server component enables production-ready deployment with authentication, access control, and automated certificate provisioning for secure use. By addressing key barriers in dataset infrastructure, PyTupli facilitates more collaborative, reproducible, and scalable offline RL research.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Output Gaussian Processes for Graph-Structured Data</title>
<link>https://arxiv.org/abs/2505.16755</link>
<guid>https://arxiv.org/abs/2505.16755</guid>
<content:encoded><![CDATA[
arXiv:2505.16755v1 Announce Type: new 
Abstract: Graph-structured data is a type of data to be obtained associated with a graph structure where vertices and edges describe some kind of data correlation. This paper proposes a regression method on graph-structured data, which is based on multi-output Gaussian processes (MOGP), to capture both the correlation between vertices and the correlation between associated data. The proposed formulation is built on the definition of MOGP. This allows it to be applied to a wide range of data configurations and scenarios. Moreover, it has high expressive capability due to its flexibility in kernel design. It includes existing methods of Gaussian processes for graph-structured data as special cases and is possible to remove restrictions on data configurations, model selection, and inference scenarios in the existing methods. The performance of extensions achievable by the proposed formulation is evaluated through computer experiments with synthetic and real data.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowMixer: A Constrained Neural Architecture for Interpretable Spatiotemporal Forecasting</title>
<link>https://arxiv.org/abs/2505.16786</link>
<guid>https://arxiv.org/abs/2505.16786</guid>
<content:encoded><![CDATA[
arXiv:2505.16786v1 Announce Type: new 
Abstract: We introduce FlowMixer, a neural architecture that leverages constrained matrix operations to model structured spatiotemporal patterns. At its core, FlowMixer incorporates non-negative matrix mixing layers within a reversible mapping framework-applying transforms before mixing and their inverses afterward. This shape-preserving design enables a Kronecker-Koopman eigenmode framework that bridges statistical learning with dynamical systems theory, providing interpretable spatiotemporal patterns and facilitating direct algebraic manipulation of prediction horizons without retraining. Extensive experiments across diverse domains demonstrate FlowMixer's robust long-horizon forecasting capabilities while effectively modeling physical phenomena such as chaotic attractors and turbulent flows. These results suggest that architectural constraints can simultaneously enhance predictive performance and mathematical interpretability in neural forecasting systems.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Flexible Forward Trajectories for Masked Molecular Diffusion</title>
<link>https://arxiv.org/abs/2505.16790</link>
<guid>https://arxiv.org/abs/2505.16790</guid>
<content:encoded><![CDATA[
arXiv:2505.16790v1 Announce Type: new 
Abstract: Masked diffusion models (MDMs) have achieved notable progress in modeling discrete data, while their potential in molecular generation remains underexplored. In this work, we explore their potential and introduce the surprising result that naively applying standards MDMs severely degrades the performance. We identify the critical cause of this issue as a state-clashing problem-where the forward diffusion of distinct molecules collapse into a common state, resulting in a mixture of reconstruction targets that cannot be learned using typical reverse diffusion process with unimodal predictions. To mitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that orchestrates per-element corruption trajectories to avoid collision between distinct molecular graphs. This is achieved through a parameterized noise scheduling network that assigns distinct corruption rates to individual graph elements, i.e., atoms and bonds. Extensive experiments on diverse molecular benchmarks reveal that MELD markedly enhances overall generation quality compared to element-agnostic noise scheduling, increasing the chemical validity of vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves state-of-the-art property alignment in conditional generation tasks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cohort-Based Active Modality Acquisition</title>
<link>https://arxiv.org/abs/2505.16791</link>
<guid>https://arxiv.org/abs/2505.16791</guid>
<content:encoded><![CDATA[
arXiv:2505.16791v1 Announce Type: new 
Abstract: Real-world machine learning applications often involve data from multiple modalities that must be integrated effectively to make robust predictions. However, in many practical settings, not all modalities are available for every sample, and acquiring additional modalities can be costly. This raises the question: which samples should be prioritized for additional modality acquisition when resources are limited? While prior work has explored individual-level acquisition strategies and training-time active learning paradigms, test-time and cohort-based acquisition remain underexplored despite their importance in many real-world settings. We introduce Cohort-based Active Modality Acquisition (CAMA), a novel test-time setting to formalize the challenge of selecting which samples should receive additional modalities. We derive acquisition strategies that leverage a combination of generative imputation and discriminative modeling to estimate the expected benefit of acquiring missing modalities based on common evaluation metrics. We also introduce upper-bound heuristics that provide performance ceilings to benchmark acquisition strategies. Experiments on common multimodal datasets demonstrate that our proposed imputation-based strategies can more effectively guide the acquisition of new samples in comparison to those relying solely on unimodal information, entropy guidance, and random selections. Our work provides an effective solution for optimizing modality acquisition at the cohort level, enabling better utilization of resources in constrained settings.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents</title>
<link>https://arxiv.org/abs/2505.16801</link>
<guid>https://arxiv.org/abs/2505.16801</guid>
<content:encoded><![CDATA[
arXiv:2505.16801v1 Announce Type: new 
Abstract: Serious Games (SGs) are nowadays shifting focus to include procedural content generation (PCG) in the development process as a means of offering personalized and enhanced player experience. However, the development of a framework to assess the impact of PCG techniques when integrated into SGs remains particularly challenging. This study proposes a methodology for automated evaluation of PCG integration in SGs, incorporating deep reinforcement learning (DRL) game testing agents. To validate the proposed framework, a previously introduced SG featuring card game mechanics and incorporating three different versions of PCG for nonplayer character (NPC) creation has been deployed. Version 1 features random NPC creation, while versions 2 and 3 utilize a genetic algorithm approach. These versions are used to test the impact of different dynamic SG environments on the proposed framework's agents. The obtained results highlight the superiority of the DRL game testing agents trained on Versions 2 and 3 over those trained on Version 1 in terms of win rate (i.e. number of wins per played games) and training time. More specifically, within the execution of a test emulating regular gameplay, both Versions 2 and 3 peaked at a 97% win rate and achieved statistically significant higher (p=0009) win rates compared to those achieved in Version 1 that peaked at 94%. Overall, results advocate towards the proposed framework's capability to produce meaningful data for the evaluation of procedurally generated content in SGs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Learning for Stochastic Optimization</title>
<link>https://arxiv.org/abs/2505.16829</link>
<guid>https://arxiv.org/abs/2505.16829</guid>
<content:encoded><![CDATA[
arXiv:2505.16829v1 Announce Type: new 
Abstract: Motivated by stochastic optimization, we introduce the problem of learning from samples of contextual value distributions. A contextual value distribution can be understood as a family of real-valued distributions, where each sample consists of a context $x$ and a random variable drawn from the corresponding real-valued distribution $D_x$. By minimizing a convex surrogate loss, we learn an empirical distribution $D'_x$ for each context, ensuring a small L\'evy distance to $D_x$. We apply this result to obtain the sample complexity bounds for the learning of an $\epsilon$-optimal policy for stochastic optimization problems defined on an unknown contextual value distribution. The sample complexity is shown to be polynomial for the general case of strongly monotone and stable optimization problems, including Single-item Revenue Maximization, Pandora's Box and Optimal Stopping.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategically Linked Decisions in Long-Term Planning and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16833</link>
<guid>https://arxiv.org/abs/2505.16833</guid>
<content:encoded><![CDATA[
arXiv:2505.16833v1 Announce Type: new 
Abstract: Long-term planning, as in reinforcement learning (RL), involves finding strategies: actions that collectively work toward a goal rather than individually optimizing their immediate outcomes. As part of a strategy, some actions are taken at the expense of short-term benefit to enable future actions with even greater returns. These actions are only advantageous if followed up by the actions they facilitate, consequently, they would not have been taken if those follow-ups were not available. In this paper, we quantify such dependencies between planned actions with strategic link scores: the drop in the likelihood of one decision under the constraint that a follow-up decision is no longer available. We demonstrate the utility of strategic link scores through three practical applications: (i) explaining black-box RL agents by identifying strategically linked pairs among decisions they make, (ii) improving the worst-case performance of decision support systems by distinguishing whether recommended actions can be adopted as standalone improvements or whether they are strategically linked hence requiring a commitment to a broader strategy to be effective, and (iii) characterizing the planning processes of non-RL agents purely through interventions aimed at measuring strategic link scores - as an example, we consider a realistic traffic simulator and analyze through road closures the effective planning horizon of the emergent routing behavior of many drivers.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning</title>
<link>https://arxiv.org/abs/2505.16850</link>
<guid>https://arxiv.org/abs/2505.16850</guid>
<content:encoded><![CDATA[
arXiv:2505.16850v1 Announce Type: new 
Abstract: Federated Learning (FL) has emerged as a promising paradigm for collaborative model training while preserving data privacy across decentralized participants. As FL adoption grows, numerous techniques have been proposed to tackle its practical challenges. However, the lack of standardized evaluation across key dimensions hampers systematic progress and fair comparison of FL methods. In this work, we introduce ATR-Bench, a unified framework for analyzing federated learning through three foundational dimensions: Adaptation, Trust, and Reasoning. We provide an in-depth examination of the conceptual foundations, task formulations, and open research challenges associated with each theme. We have extensively benchmarked representative methods and datasets for adaptation to heterogeneous clients and trustworthiness in adversarial or unreliable environments. Due to the lack of reliable metrics and models for reasoning in FL, we only provide literature-driven insights for this dimension. ATR-Bench lays the groundwork for a systematic and holistic evaluation of federated learning with real-world relevance. We will make our complete codebase publicly accessible and a curated repository that continuously tracks new developments and research in the FL literature.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only</title>
<link>https://arxiv.org/abs/2505.16856</link>
<guid>https://arxiv.org/abs/2505.16856</guid>
<content:encoded><![CDATA[
arXiv:2505.16856v1 Announce Type: new 
Abstract: Improving the performance of pre-trained policies through online reinforcement learning (RL) is a critical yet challenging topic. Existing online RL fine-tuning methods require continued training with offline pretrained Q-functions for stability and performance. However, these offline pretrained Q-functions commonly underestimate state-action pairs beyond the offline dataset due to the conservatism in most offline RL methods, which hinders further exploration when transitioning from the offline to the online setting. Additionally, this requirement limits their applicability in scenarios where only pre-trained policies are available but pre-trained Q-functions are absent, such as in imitation learning (IL) pre-training. To address these challenges, we propose a method for efficient online RL fine-tuning using solely the offline pre-trained policy, eliminating reliance on pre-trained Q-functions. We introduce PORL (Policy-Only Reinforcement Learning Fine-Tuning), which rapidly initializes the Q-function from scratch during the online phase to avoid detrimental pessimism. Our method not only achieves competitive performance with advanced offline-to-online RL algorithms and online RL approaches that leverage data or policies prior, but also pioneers a new path for directly fine-tuning behavior cloning (BC) policies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redefining Clustered Federated Learning for System Identification: The Path of ClusterCraft</title>
<link>https://arxiv.org/abs/2505.16857</link>
<guid>https://arxiv.org/abs/2505.16857</guid>
<content:encoded><![CDATA[
arXiv:2505.16857v1 Announce Type: new 
Abstract: This paper addresses the System Identification (SYSID) problem within the framework of federated learning. We introduce a novel algorithm, Incremental Clustering-based federated learning method for SYSID (IC-SYSID), designed to tackle SYSID challenges across multiple data sources without prior knowledge. IC-SYSID utilizes an incremental clustering method, ClusterCraft (CC), to eliminate the dependency on the prior knowledge of the dataset. CC starts with a single cluster model and assigns similar local workers to the same clusters by dynamically increasing the number of clusters. To reduce the number of clusters generated by CC, we introduce ClusterMerge, where similar cluster models are merged. We also introduce enhanced ClusterCraft to reduce the generation of similar cluster models during the training. Moreover, IC-SYSID addresses cluster model instability by integrating a regularization term into the loss function and initializing cluster models with scaled Glorot initialization. It also utilizes a mini-batch deep learning approach to manage large SYSID datasets during local training. Through the experiments conducted on a real-world representing SYSID problem, where a fleet of vehicles collaboratively learns vehicle dynamics, we show that IC-SYSID achieves a high SYSID performance while preventing the learning of unstable clusters.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCAL: Adapting Graph Models to Evolving Domain Shifts</title>
<link>https://arxiv.org/abs/2505.16860</link>
<guid>https://arxiv.org/abs/2505.16860</guid>
<content:encoded><![CDATA[
arXiv:2505.16860v1 Announce Type: new 
Abstract: This paper addresses the challenge of graph domain adaptation on evolving, multiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation methods are confined to single-step adaptation, making them ineffective in handling continuous domain shifts and prone to catastrophic forgetting. This paper introduces the Graph Continual Adaptive Learning (GCAL) method, designed to enhance model sustainability and adaptability across various graph domains. GCAL employs a bilevel optimization strategy. The "adapt" phase uses an information maximization approach to fine-tune the model with new graph domains while re-adapting past memories to mitigate forgetting. Concurrently, the "generate memory" phase, guided by a theoretical lower bound derived from information bottleneck theory, involves a variational memory graph generation module to condense original graphs into memories. Extensive experimental evaluations demonstrate that GCAL substantially outperforms existing methods in terms of adaptability and knowledge retention.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Step Comparative Framework for Anomaly Detection in IoT Data Streams</title>
<link>https://arxiv.org/abs/2505.16872</link>
<guid>https://arxiv.org/abs/2505.16872</guid>
<content:encoded><![CDATA[
arXiv:2505.16872v1 Announce Type: new 
Abstract: The rapid expansion of Internet of Things (IoT) devices has introduced critical security challenges, underscoring the need for accurate anomaly detection. Although numerous studies have proposed machine learning (ML) methods for this purpose, limited research systematically examines how different preprocessing steps--normalization, transformation, and feature selection--interact with distinct model architectures. To address this gap, this paper presents a multi-step evaluation framework assessing the combined impact of preprocessing choices on three ML algorithms: RNN-LSTM, autoencoder neural networks (ANN), and Gradient Boosting (GBoosting). Experiments on the IoTID20 dataset shows that GBoosting consistently delivers superior accuracy across preprocessing configurations, while RNN-LSTM shows notable gains with z-score normalization and autoencoders excel in recall, making them well-suited for unsupervised scenarios. By offering a structured analysis of preprocessing decisions and their interplay with various ML techniques, the proposed framework provides actionable guidance to enhance anomaly detection performance in IoT environments.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-Aligned Protein Language Model</title>
<link>https://arxiv.org/abs/2505.16896</link>
<guid>https://arxiv.org/abs/2505.16896</guid>
<content:encoded><![CDATA[
arXiv:2505.16896v1 Announce Type: new 
Abstract: Protein language models (pLMs) pre-trained on vast protein sequence databases excel at various downstream tasks but lack the structural knowledge essential for many biological applications. To address this, we integrate structural insights from pre-trained protein graph neural networks (pGNNs) into pLMs through a latent-level contrastive learning task. This task aligns residue representations from pLMs with those from pGNNs across multiple proteins, enriching pLMs with inter-protein structural knowledge. Additionally, we incorporate a physical-level task that infuses intra-protein structural knowledge by optimizing pLMs to predict structural tokens. The proposed dual-task framework effectively incorporates both inter-protein and intra-protein structural knowledge into pLMs. Given the variability in the quality of protein structures in PDB, we further introduce a residue loss selection module, which uses a small model trained on high-quality structures to select reliable yet challenging residue losses for the pLM to learn. Applying our structure alignment method to the state-of-the-art ESM2 and AMPLIFY results in notable performance gains across a wide range of tasks, including a 12.7% increase in ESM2 contact prediction. The data, code, and resulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Prompting for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.16903</link>
<guid>https://arxiv.org/abs/2505.16903</guid>
<content:encoded><![CDATA[
arXiv:2505.16903v1 Announce Type: new 
Abstract: Prompt tuning methods for Graph Neural Networks (GNNs) have become popular to address the semantic gap between pre-training and fine-tuning steps. However, existing GNN prompting methods rely on labeled data and involve lightweight fine-tuning for downstream tasks. Meanwhile, in-context learning methods for Large Language Models (LLMs) have shown promising performance with no parameter updating and no or minimal labeled data. Inspired by these approaches, in this work, we first introduce a challenging problem setup to evaluate GNN prompting methods. This setup encourages a prompting function to enhance a pre-trained GNN's generalization to a target dataset under covariate shift without updating the GNN's parameters and with no labeled data. Next, we propose a fully unsupervised prompting method based on consistency regularization through pseudo-labeling. We use two regularization techniques to align the prompted graphs' distribution with the original data and reduce biased predictions. Through extensive experiments under our problem setting, we demonstrate that our unsupervised approach outperforms the state-of-the-art prompting methods that have access to labels.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable and Interpretable Contextual Bandits: A Literature Review and Retail Offer Prototype</title>
<link>https://arxiv.org/abs/2505.16918</link>
<guid>https://arxiv.org/abs/2505.16918</guid>
<content:encoded><![CDATA[
arXiv:2505.16918v1 Announce Type: new 
Abstract: This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB) methods and introduces an experimental framework for scalable, interpretable offer selection, addressing the challenge of fast-changing offers. The approach models context at the product category level, allowing offers to span multiple categories and enabling knowledge transfer across similar offers. This improves learning efficiency and generalization in dynamic environments. The framework extends standard CMAB methodology to support multi-category contexts, and achieves scalability through efficient feature engineering and modular design. Advanced features such as MPG (Member Purchase Gap) and MF (Matrix Factorization) capture nuanced user-offer interactions, with implementation in Python for practical deployment.
  A key contribution is interpretability at scale: logistic regression models yield transparent weight vectors, accessible via a large language model (LLM) interface for real-time, user-level tracking and explanation of evolving preferences. This enables the generation of detailed member profiles and identification of behavioral patterns, supporting personalized offer optimization and enhancing trust in automated decisions. By situating our prototype alongside established paradigms like Generalized Linear Models and Thompson Sampling, we demonstrate its value for both research and real-world CMAB applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk-Averse Reinforcement Learning with Itakura-Saito Loss</title>
<link>https://arxiv.org/abs/2505.16925</link>
<guid>https://arxiv.org/abs/2505.16925</guid>
<content:encoded><![CDATA[
arXiv:2505.16925v1 Announce Type: new 
Abstract: Risk-averse reinforcement learning finds application in various high-stakes fields. Unlike classical reinforcement learning, which aims to maximize expected returns, risk-averse agents choose policies that minimize risk, occasionally sacrificing expected value. These preferences can be framed through utility theory. We focus on the specific case of the exponential utility function, where we can derive the Bellman equations and employ various reinforcement learning algorithms with few modifications. However, these methods suffer from numerical instability due to the need for exponent computation throughout the process. To address this, we introduce a numerically stable and mathematically sound loss function based on the Itakura-Saito divergence for learning state-value and action-value functions. We evaluate our proposed loss function against established alternatives, both theoretically and empirically. In the experimental section, we explore multiple financial scenarios, some with known analytical solutions, and show that our loss function outperforms the alternatives.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm</title>
<link>https://arxiv.org/abs/2505.16932</link>
<guid>https://arxiv.org/abs/2505.16932</guid>
<content:encoded><![CDATA[
arXiv:2505.16932v1 Announce Type: new 
Abstract: Computing the polar decomposition and the related matrix sign function, has been a well-studied problem in numerical analysis for decades. More recently, it has emerged as an important subroutine in deep learning, particularly within the Muon optimization framework. However, the requirements in this setting differ significantly from those of traditional numerical analysis. In deep learning, methods must be highly efficient and GPU-compatible, but high accuracy is often unnecessary. As a result, classical algorithms like Newton-Schulz (which suffers from slow initial convergence) and methods based on rational functions (which rely on QR decompositions or matrix inverses) are poorly suited to this context. In this work, we introduce Polar Express, a GPU-friendly algorithm for computing the polar decomposition. Like classical polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix multiplications, making it GPU-compatible. Motivated by earlier work of Chen & Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule at each iteration by solving a minimax optimization problem, and we prove that it enjoys a strong worst-case optimality guarantee. This property ensures both rapid early convergence and fast asymptotic convergence. We also address finite-precision issues, making it stable in bfloat16 in practice. We apply Polar Express within the Muon optimization framework and show consistent improvements in validation loss on large-scale models such as GPT-2, outperforming recent alternatives across a range of learning rates.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning</title>
<link>https://arxiv.org/abs/2505.16933</link>
<guid>https://arxiv.org/abs/2505.16933</guid>
<content:encoded><![CDATA[
arXiv:2505.16933v1 Announce Type: new 
Abstract: In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large Language Model (MLLM) that integrates visual instruction tuning with masked diffusion models, representing a departure from the autoregressive paradigms dominant in current multimodal approaches. Built upon LLaDA, a representative large language diffusion model, LLaDA-V incorporates a vision encoder and MLP connector that projects visual features into the language embedding space, enabling effective multimodal alignment. Our empirical investigation reveals several intriguing results: First, LLaDA-V demonstrates promising multimodal performance despite its language model being weaker on purely textual tasks than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal tasks with better data scalability. It also narrows the performance gap to Qwen2-VL, suggesting the effectiveness of its architecture for multimodal tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal understanding compared to existing hybrid autoregressive-diffusion and purely diffusion-based MLLMs. Our findings suggest that large language diffusion models show promise in multimodal contexts and warrant further investigation in future research. Project page and codes: https://ml-gsai.github.io/LLaDA-V-demo/.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPAR: Self-supervised Placement-Aware Representation Learning for Multi-Node IoT Systems</title>
<link>https://arxiv.org/abs/2505.16936</link>
<guid>https://arxiv.org/abs/2505.16936</guid>
<content:encoded><![CDATA[
arXiv:2505.16936v1 Announce Type: new 
Abstract: This work develops the underpinnings of self-supervised placement-aware representation learning given spatially-distributed (multi-view and multimodal) sensor observations, motivated by the need to represent external environmental state in multi-sensor IoT systems in a manner that correctly distills spatial phenomena from the distributed multi-vantage observations. The objective of sensing in IoT systems is, in general, to collectively represent an externally observed environment given multiple vantage points from which sensory observations occur. Pretraining of models that help interpret sensor data must therefore encode the relation between signals observed by sensors and the observers' vantage points in order to attain a representation that encodes the observed spatial phenomena in a manner informed by the specific placement of the measuring instruments, while allowing arbitrary placement. The work significantly advances self-supervised model pretraining from IoT signals beyond current solutions that often overlook the distinctive spatial nature of IoT data. Our framework explicitly learns the dependencies between measurements and geometric observer layouts and structural characteristics, guided by a core design principle: the duality between signals and observer positions. We further provide theoretical analyses from the perspectives of information theory and occlusion-invariant representation learning to offer insight into the rationale behind our design. Experiments on three real-world datasets--covering vehicle monitoring, human activity recognition, and earthquake localization--demonstrate the superior generalizability and robustness of our method across diverse modalities, sensor placements, application-level inference tasks, and spatial scales.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records</title>
<link>https://arxiv.org/abs/2505.16941</link>
<guid>https://arxiv.org/abs/2505.16941</guid>
<content:encoded><![CDATA[
arXiv:2505.16941v1 Announce Type: new 
Abstract: Foundation models hold significant promise in healthcare, given their capacity to extract meaningful representations independent of downstream tasks. This property has enabled state-of-the-art performance across several clinical applications trained on structured electronic health record (EHR) data, even in settings with limited labeled data, a prevalent challenge in healthcare. However, there is little consensus on these models' potential for clinical utility due to the lack of desiderata of comprehensive and meaningful tasks and sufficiently diverse evaluations to characterize the benefit over conventional supervised learning. To address this gap, we propose a suite of clinically meaningful tasks spanning patient outcomes, early prediction of acute and chronic conditions, including desiderata for robust evaluations. We evaluate state-of-the-art foundation models on EHR data consisting of 5 million patients from Columbia University Irving Medical Center (CUMC), a large urban academic medical center in New York City, across 14 clinically relevant tasks. We measure overall accuracy, calibration, and subpopulation performance to surface tradeoffs based on the choice of pre-training, tokenization, and data representation strategies. Our study aims to advance the empirical evaluation of structured EHR foundation models and guide the development of future healthcare foundation models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MixAT: Combining Continuous and Discrete Adversarial Training for LLMs</title>
<link>https://arxiv.org/abs/2505.16947</link>
<guid>https://arxiv.org/abs/2505.16947</guid>
<content:encoded><![CDATA[
arXiv:2505.16947v1 Announce Type: new 
Abstract: Despite recent efforts in Large Language Models (LLMs) safety and alignment, current adversarial attacks on frontier LLMs are still able to force harmful generations consistently. Although adversarial training has been widely studied and shown to significantly improve the robustness of traditional machine learning models, its strengths and weaknesses in the context of LLMs are less understood. Specifically, while existing discrete adversarial attacks are effective at producing harmful content, training LLMs with concrete adversarial prompts is often computationally expensive, leading to reliance on continuous relaxations. As these relaxations do not correspond to discrete input tokens, such latent training methods often leave models vulnerable to a diverse set of discrete attacks. In this work, we aim to bridge this gap by introducing MixAT, a novel method that combines stronger discrete and faster continuous attacks during training. We rigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks, proposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the worst-case vulnerability of models. We show MixAT achieves substantially better robustness (ALO-ASR < 20%) compared to prior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to methods based on continuous relaxations. We further analyze MixAT in realistic deployment settings, exploring how chat templates, quantization, low-rank adapters, and temperature affect both adversarial training and evaluation, revealing additional blind spots in current methodologies. Our results demonstrate that MixAT's discrete-continuous defense offers a principled and superior robustness-accuracy tradeoff with minimal computational overhead, highlighting its promise for building safer LLMs. We provide our code and models at https://github.com/insait-institute/MixAT.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning</title>
<link>https://arxiv.org/abs/2505.16950</link>
<guid>https://arxiv.org/abs/2505.16950</guid>
<content:encoded><![CDATA[
arXiv:2505.16950v1 Announce Type: new 
Abstract: Despite their impressive capabilities, Large Language Models struggle with generalisation beyond their training distribution, often exhibiting sophisticated pattern interpolation rather than true abstract reasoning (extrapolation). In this work, we approach this limitation through the lens of Information Bottleneck (IB) theory, which posits that model generalisation emerges from an optimal balance between input compression and retention of predictive information in latent representations. We prove using IB theory that decoder-only Transformers are inherently constrained in their ability to form task-optimal sequence representations. We then use this result to demonstrate that periodic global transformation of the internal sequence-level representations (KV cache) is a necessary computational step for improving Transformer generalisation in reasoning tasks. Based on these theoretical insights, we propose a modification to the Transformer architecture, in the form of an additional module that globally rewrites the KV cache at periodic intervals, shifting its capacity away from memorising input prefixes and toward encoding features most useful for predicting future tokens. Our model delivers substantial gains on mathematical reasoning benchmarks, outperforming both vanilla Transformers with up to 3.5x more parameters, as well as heuristic-driven pruning mechanisms for cache compression. Our approach can be seen as a principled generalisation of existing KV-cache compression methods; whereas such methods focus solely on compressing input representations, they often do so at the expense of retaining predictive information, and thus their capabilities are inherently bounded by those of an unconstrained model. This establishes a principled framework to manipulate Transformer memory using information theory, addressing fundamental reasoning limitations that scaling alone cannot overcome.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Evaluation of Contemporary ML-Based Solvers for Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2505.16952</link>
<guid>https://arxiv.org/abs/2505.16952</guid>
<content:encoded><![CDATA[
arXiv:2505.16952v1 Announce Type: new 
Abstract: Machine learning (ML) has demonstrated considerable potential in supporting model design and optimization for combinatorial optimization (CO) problems. However, much of the progress to date has been evaluated on small-scale, synthetic datasets, raising concerns about the practical effectiveness of ML-based solvers in real-world, large-scale CO scenarios. Additionally, many existing CO benchmarks lack sufficient training data, limiting their utility for evaluating data-driven approaches. To address these limitations, we introduce FrontierCO, a comprehensive benchmark that covers eight canonical CO problem types and evaluates 16 representative ML-based solvers--including graph neural networks and large language model (LLM) agents. FrontierCO features challenging instances drawn from industrial applications and frontier CO research, offering both realistic problem difficulty and abundant training data. Our empirical results provide critical insights into the strengths and limitations of current ML methods, helping to guide more robust and practically relevant advances at the intersection of machine learning and combinatorial optimization. Our data is available at https://huggingface.co/datasets/CO-Bench/FrontierCO.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICYM2I: The illusion of multimodal informativeness under missingness</title>
<link>https://arxiv.org/abs/2505.16953</link>
<guid>https://arxiv.org/abs/2505.16953</guid>
<content:encoded><![CDATA[
arXiv:2505.16953v1 Announce Type: new 
Abstract: Multimodal learning is of continued interest in artificial intelligence-based applications, motivated by the potential information gain from combining different types of data. However, modalities collected and curated during development may differ from the modalities available at deployment due to multiple factors including cost, hardware failure, or -- as we argue in this work -- the perceived informativeness of a given modality. Na{\"i}ve estimation of the information gain associated with including an additional modality without accounting for missingness may result in improper estimates of that modality's value in downstream tasks. Our work formalizes the problem of missingness in multimodal learning and demonstrates the biases resulting from ignoring this process. To address this issue, we introduce ICYM2I (In Case You Multimodal Missed It), a framework for the evaluation of predictive performance and information gain under missingness through inverse probability weighting-based correction. We demonstrate the importance of the proposed adjustment to estimate information gain under missingness on synthetic, semi-synthetic, and real-world medical datasets.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bigger Isn't Always Memorizing: Early Stopping Overparameterized Diffusion Models</title>
<link>https://arxiv.org/abs/2505.16959</link>
<guid>https://arxiv.org/abs/2505.16959</guid>
<content:encoded><![CDATA[
arXiv:2505.16959v1 Announce Type: new 
Abstract: Diffusion probabilistic models have become a cornerstone of modern generative AI, yet the mechanisms underlying their generalization remain poorly understood. In fact, if these models were perfectly minimizing their training loss, they would just generate data belonging to their training set, i.e., memorize, as empirically found in the overparameterized regime. We revisit this view by showing that, in highly overparameterized diffusion models, generalization in natural data domains is progressively achieved during training before the onset of memorization. Our results, ranging from image to language diffusion models, systematically support the empirical law that memorization time is proportional to the dataset size. Generalization vs. memorization is then best understood as a competition between time scales. We show that this phenomenology is recovered in diffusion models learning a simple probabilistic context-free grammar with random rules, where generalization corresponds to the hierarchical acquisition of deeper grammar rules as training time grows, and the generalization cost of early stopping can be characterized. We summarize these results in a phase diagram. Overall, our results support that a principled early-stopping criterion - scaling with dataset size - can effectively optimize generalization while avoiding memorization, with direct implications for hyperparameter transfer and privacy-sensitive applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UFT: Unifying Supervised and Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.16984</link>
<guid>https://arxiv.org/abs/2505.16984</guid>
<content:encoded><![CDATA[
arXiv:2505.16984v1 Announce Type: new 
Abstract: Post-training has demonstrated its importance in enhancing the reasoning capabilities of large language models (LLMs). The primary post-training methods can be categorized into supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). SFT is efficient and well-suited for small language models, but it may lead to overfitting and limit the reasoning abilities of larger models. In contrast, RFT generally yields better generalization but depends heavily on the strength of the base model. To address the limitations of SFT and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm that unifies SFT and RFT into a single, integrated process. UFT enables the model to effectively explore solutions while incorporating informative supervision signals, bridging the gap between memorizing and thinking underlying existing methods. Notably, UFT outperforms both SFT and RFT in general, regardless of model sizes. Furthermore, we theoretically prove that UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for the first time that unified training can exponentially accelerate convergence on long-horizon reasoning tasks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PICT -- A Differentiable, GPU-Accelerated Multi-Block PISO Solver for Simulation-Coupled Learning Tasks in Fluid Dynamics</title>
<link>https://arxiv.org/abs/2505.16992</link>
<guid>https://arxiv.org/abs/2505.16992</guid>
<content:encoded><![CDATA[
arXiv:2505.16992v1 Announce Type: new 
Abstract: Despite decades of advancements, the simulation of fluids remains one of the most challenging areas of in scientific computing. Supported by the necessity of gradient information in deep learning, differentiable simulators have emerged as an effective tool for optimization and learning in physics simulations. In this work, we present our fluid simulator PICT, a differentiable pressure-implicit solver coded in PyTorch with Graphics-processing-unit (GPU) support. We first verify the accuracy of both the forward simulation and our derived gradients in various established benchmarks like lid-driven cavities and turbulent channel flows before we show that the gradients provided by our solver can be used to learn complicated turbulence models in 2D and 3D. We apply both supervised and unsupervised training regimes using physical priors to match flow statistics. In particular, we learn a stable sub-grid scale (SGS) model for a 3D turbulent channel flow purely based on reference statistics. The low-resolution corrector trained with our solver runs substantially faster than the highly resolved references, while keeping or even surpassing their accuracy. Finally, we give additional insights into the physical interpretation of different solver gradients, and motivate a physically informed regularization technique. To ensure that the full potential of PICT can be leveraged, it is published as open source: https://github.com/tum-pbs/PICT.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Framework for Simultaneous Parameter and Function Discovery in Differential Equations</title>
<link>https://arxiv.org/abs/2505.16996</link>
<guid>https://arxiv.org/abs/2505.16996</guid>
<content:encoded><![CDATA[
arXiv:2505.16996v1 Announce Type: new 
Abstract: Inverse problems involving differential equations often require identifying unknown parameters or functions from data. Existing approaches, such as Physics-Informed Neural Networks (PINNs), Universal Differential Equations (UDEs) and Universal Physics-Informed Neural Networks (UPINNs), are effective at isolating either parameters or functions but can face challenges when applied simultaneously due to solution non-uniqueness. In this work, we introduce a framework that addresses these limitations by establishing conditions under which unique solutions can be guaranteed. To illustrate, we apply it to examples from biological systems and ecological dynamics, demonstrating accurate and interpretable results. Our approach significantly enhances the potential of machine learning techniques in modeling complex systems in science and engineering.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Diffusion Sampling on Function Spaces with Applications to PDEs</title>
<link>https://arxiv.org/abs/2505.17004</link>
<guid>https://arxiv.org/abs/2505.17004</guid>
<content:encoded><![CDATA[
arXiv:2505.17004v1 Announce Type: new 
Abstract: We propose a general framework for conditional sampling in PDE-based inverse problems, targeting the recovery of whole solutions from extremely sparse or noisy measurements. This is accomplished by a function-space diffusion model and plug-and-play guidance for conditioning. Our method first trains an unconditional discretization-agnostic denoising model using neural operator architectures. At inference, we refine the samples to satisfy sparse observation data via a gradient-based guidance mechanism. Through rigorous mathematical analysis, we extend Tweedie's formula to infinite-dimensional Hilbert spaces, providing the theoretical foundation for our posterior sampling approach. Our method (FunDPS) accurately captures posterior distributions in function spaces under minimal supervision and severe data scarcity. Across five PDE tasks with only 3% observation, our method achieves an average 32% accuracy improvement over state-of-the-art fixed-resolution diffusion baselines while reducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning ensures strong cross-resolution generalizability. To the best of our knowledge, this is the first diffusion-based framework to operate independently of discretization, offering a practical and flexible solution for forward and inverse problems in the context of PDEs. Code is available at https://github.com/neuraloperator/FunDPS
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Prompt Tuning and In-Context Learning via Meta-Learning</title>
<link>https://arxiv.org/abs/2505.17010</link>
<guid>https://arxiv.org/abs/2505.17010</guid>
<content:encoded><![CDATA[
arXiv:2505.17010v1 Announce Type: new 
Abstract: Prompting is one of the main ways to adapt a pretrained model to target tasks. Besides manually constructing prompts, many prompt optimization methods have been proposed in the literature. Method development is mainly empirically driven, with less emphasis on a conceptual understanding of prompting. In this paper we discuss how optimal prompting can be understood through a Bayesian view, which also implies some fundamental limitations of prompting that can only be overcome by tuning weights. The paper explains in detail how meta-trained neural networks behave as Bayesian predictors over the pretraining distribution, whose hallmark feature is rapid in-context adaptation. Optimal prompting can be studied formally as conditioning these Bayesian predictors, yielding criteria for target tasks where optimal prompting is and is not possible. We support the theory with educational experiments on LSTMs and Transformers, where we compare different versions of prefix-tuning and different weight-tuning methods. We also confirm that soft prefixes, which are sequences of real-valued vectors outside the token alphabet, can lead to very effective prompts for trained and even untrained networks by manipulating activations in ways that are not achievable by hard tokens. This adds an important mechanistic aspect beyond the conceptual Bayesian theory.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Are Concepts Erased From Diffusion Models?</title>
<link>https://arxiv.org/abs/2505.17013</link>
<guid>https://arxiv.org/abs/2505.17013</guid>
<content:encoded><![CDATA[
arXiv:2505.17013v1 Announce Type: new 
Abstract: Concept erasure, the ability to selectively prevent a model from generating specific concepts, has attracted growing interest, with various approaches emerging to address the challenge. However, it remains unclear how thoroughly these methods erase the target concept. We begin by proposing two conceptual models for the erasure mechanism in diffusion models: (i) reducing the likelihood of generating the target concept, and (ii) interfering with the model's internal guidance mechanisms. To thoroughly assess whether a concept has been truly erased from the model, we introduce a suite of independent evaluations. Our evaluation framework includes adversarial attacks, novel probing techniques, and analysis of the model's alternative generations in place of the erased concept. Our results shed light on the tension between minimizing side effects and maintaining robustness to adversarial prompts. Broadly, our work underlines the importance of comprehensive evaluation for erasure in diffusion models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Post-Training for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2505.17016</link>
<guid>https://arxiv.org/abs/2505.17016</guid>
<content:encoded><![CDATA[
arXiv:2505.17016v1 Announce Type: new 
Abstract: We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation.
  RIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14403</link>
<guid>https://arxiv.org/abs/2505.14403</guid>
<content:encoded><![CDATA[
arXiv:2505.14403v1 Announce Type: cross 
Abstract: Recent advances in reasoning language models have witnessed a paradigm shift from short to long CoT pattern. Given the substantial computational cost of rollouts in long CoT models, maximizing the utility of fixed training datasets becomes crucial. Our analysis reveals that negative responses contain valuable components such as self-reflection and error-correction steps, yet primary existing methods either completely discard negative samples (RFT) or apply equal penalization across all tokens (RL), failing to leverage these potential learning signals. In light of this, we propose Behavior Constrained Policy Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline RL framework that encompasses three stages: 1) sample segmentation, 2) consensus-based step correctness assessment combining LLM and PRM judgers, and 3) policy optimization with NSA designed to effectively mine positive steps within negative samples. Experimental results show that BCPG-NSA outperforms baselines on several challenging math/coding reasoning benchmarks using the same training dataset, achieving improved sample efficiency and demonstrating robustness and scalability when extended to multiple iterations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models</title>
<link>https://arxiv.org/abs/2505.14679</link>
<guid>https://arxiv.org/abs/2505.14679</guid>
<content:encoded><![CDATA[
arXiv:2505.14679v1 Announce Type: cross 
Abstract: Lifelong learning enables large language models (LLMs) to adapt to evolving information by continually updating their internal knowledge. An ideal system should support efficient, wide-ranging updates while preserving existing capabilities and ensuring reliable deployment. Model editing stands out as a promising solution for this goal, offering a focused and efficient way to revise a model's internal knowledge. Although recent paradigms have made notable progress, they often struggle to meet the demands of practical lifelong adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally new editing solution that is training-, subject- and memory-free, making it particularly well-suited for ultra-scalable, real-world lifelong model editing. ULTRAEDIT performs editing through a self-contained process that relies solely on lightweight linear algebra operations to compute parameter shifts, enabling fast and consistent parameter modifications with minimal overhead. To improve scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization strategy that continuously updates feature statistics across turns, allowing it to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT achieves editing speeds over 7x faster than the previous state-of-the-art method-which was also the fastest known approach-while consuming less than 1/3 the VRAM, making it the only method currently capable of editing a 7B LLM on a 24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest dataset in the field to date, with over 2M editing pairs-and demonstrate that our method supports up to 1M edits while maintaining high accuracy. Comprehensive experiments on four datasets and six models show that ULTRAEDIT consistently achieves superior performance across diverse model editing scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MambaStyle: Efficient StyleGAN Inversion for Real Image Editing with State-Space Models</title>
<link>https://arxiv.org/abs/2505.15822</link>
<guid>https://arxiv.org/abs/2505.15822</guid>
<content:encoded><![CDATA[
arXiv:2505.15822v1 Announce Type: cross 
Abstract: The task of inverting real images into StyleGAN's latent space to manipulate their attributes has been extensively studied. However, existing GAN inversion methods struggle to balance high reconstruction quality, effective editability, and computational efficiency. In this paper, we introduce MambaStyle, an efficient single-stage encoder-based approach for GAN inversion and editing that leverages vision state-space models (VSSMs) to address these challenges. Specifically, our approach integrates VSSMs within the proposed architecture, enabling high-quality image inversion and flexible editing with significantly fewer parameters and reduced computational complexity compared to state-of-the-art methods. Extensive experiments show that MambaStyle achieves a superior balance among inversion accuracy, editing quality, and computational efficiency. Notably, our method achieves superior inversion and editing results with reduced model complexity and faster inference, making it suitable for real-time applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarially Robust Spiking Neural Networks with Sparse Connectivity</title>
<link>https://arxiv.org/abs/2505.15833</link>
<guid>https://arxiv.org/abs/2505.15833</guid>
<content:encoded><![CDATA[
arXiv:2505.15833v1 Announce Type: cross 
Abstract: Deployment of deep neural networks in resource-constrained embedded systems requires innovative algorithmic solutions to facilitate their energy and memory efficiency. To further ensure the reliability of these systems against malicious actors, recent works have extensively studied adversarial robustness of existing architectures. Our work focuses on the intersection of adversarial robustness, memory- and energy-efficiency in neural networks. We introduce a neural network conversion algorithm designed to produce sparse and adversarially robust spiking neural networks (SNNs) by leveraging the sparse connectivity and weights from a robustly pretrained artificial neural network (ANN). Our approach combines the energy-efficient architecture of SNNs with a novel conversion algorithm, leading to state-of-the-art performance with enhanced energy and memory efficiency through sparse connectivity and activations. Our models are shown to achieve up to 100x reduction in the number of weights to be stored in memory, with an estimated 8.6x increase in energy efficiency compared to dense SNNs, while maintaining high performance and robustness against adversarial threats.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Evolutionary Neural Networks for Multi-Agent Federated Learning</title>
<link>https://arxiv.org/abs/2505.15836</link>
<guid>https://arxiv.org/abs/2505.15836</guid>
<content:encoded><![CDATA[
arXiv:2505.15836v1 Announce Type: cross 
Abstract: As artificial intelligence continues to drive innovation in complex, decentralized environments, the need for scalable, adaptive, and privacy-preserving decision-making systems has become critical. This paper introduces a novel framework combining quantum-inspired neural networks with evolutionary algorithms to optimize real-time decision-making in multi-agent systems (MAS). The proposed Quantum-Evolutionary Neural Network (QE-NN) leverages quantum computing principles -- such as quantum superposition and entanglement -- to enhance learning speed and decision accuracy, while integrating evolutionary optimization to continually refine agent behaviors in dynamic, uncertain environments. By utilizing federated learning, QE-NN ensures privacy preservation, enabling decentralized agents to collaborate without sharing sensitive data. The framework is designed to allow agents to adapt in real-time to their environments, optimizing decision-making processes for applications in areas such as autonomous systems, smart cities, and healthcare. This research represents a breakthrough in merging quantum computing, evolutionary optimization, and privacy-preserving techniques to solve complex problems in multi-agent decision-making systems, pushing the boundaries of AI in real-world, privacy-sensitive applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum Learning in Genetic Programming Guided Local Search for Large-scale Vehicle Routing Problems</title>
<link>https://arxiv.org/abs/2505.15839</link>
<guid>https://arxiv.org/abs/2505.15839</guid>
<content:encoded><![CDATA[
arXiv:2505.15839v1 Announce Type: cross 
Abstract: Manually designing (meta-)heuristics for the Vehicle Routing Problem (VRP) is a challenging task that requires significant domain expertise. Recently, data-driven approaches have emerged as a promising solution, automatically learning heuristics that perform well on training instances and generalize to unseen test cases. Such an approach learns (meta-)heuristics that can perform well on the training instances, expecting it to generalize well on the unseen test instances. A recent method, named GPGLS, uses Genetic Programming (GP) to learn the utility function in Guided Local Search (GLS) and solved large scale VRP effectively. However, the selection of appropriate training instances during the learning process remains an open question, with most existing studies including GPGLS relying on random instance selection. To address this, we propose a novel method, CL-GPGLS, which integrates Curriculum Learning (CL) into GPGLS. Our approach leverages a predefined curriculum to introduce training instances progressively, starting with simpler tasks and gradually increasing complexity, enabling the model to better adapt and optimize for large-scale VRP (LSVRP). Extensive experiments verify the effectiveness of CL-GPGLS, demonstrating significant performance improvements over three baseline methods.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AH-UGC: Adaptive and Heterogeneous-Universal Graph Coarsening</title>
<link>https://arxiv.org/abs/2505.15842</link>
<guid>https://arxiv.org/abs/2505.15842</guid>
<content:encoded><![CDATA[
arXiv:2505.15842v1 Announce Type: cross 
Abstract: $\textbf{Graph Coarsening (GC)}$ is a prominent graph reduction technique that compresses large graphs to enable efficient learning and inference. However, existing GC methods generate only one coarsened graph per run and must recompute from scratch for each new coarsening ratio, resulting in unnecessary overhead. Moreover, most prior approaches are tailored to $\textit{homogeneous}$ graphs and fail to accommodate the semantic constraints of $\textit{heterogeneous}$ graphs, which comprise multiple node and edge types. To overcome these limitations, we introduce a novel framework that combines Locality Sensitive Hashing (LSH) with Consistent Hashing to enable $\textit{adaptive graph coarsening}$. Leveraging hashing techniques, our method is inherently fast and scalable. For heterogeneous graphs, we propose a $\textit{type isolated coarsening}$ strategy that ensures semantic consistency by restricting merges to nodes of the same type. Our approach is the first unified framework to support both adaptive and heterogeneous coarsening. Extensive evaluations on 23 real-world datasets including homophilic, heterophilic, homogeneous, and heterogeneous graphs demonstrate that our method achieves superior scalability while preserving the structural and semantic integrity of the original graph.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Tabular Stroke Modelling Through a Novel Hybrid Architecture and Feature-Selection Synergy</title>
<link>https://arxiv.org/abs/2505.15844</link>
<guid>https://arxiv.org/abs/2505.15844</guid>
<content:encoded><![CDATA[
arXiv:2505.15844v1 Announce Type: cross 
Abstract: Brain stroke remains one of the principal causes of death and disability worldwide, yet most tabular-data prediction models still hover below the 95% accuracy threshold, limiting real-world utility. Addressing this gap, the present work develops and validates a completely data-driven and interpretable machine-learning framework designed to predict strokes using ten routinely gathered demographic, lifestyle, and clinical variables sourced from a public cohort of 4,981 records. We employ a detailed exploratory data analysis (EDA) to understand the dataset's structure and distribution, followed by rigorous data preprocessing, including handling missing values, outlier removal, and class imbalance correction using Synthetic Minority Over-sampling Technique (SMOTE). To streamline feature selection, point-biserial correlation and random-forest Gini importance were utilized, and ten varied algorithms-encompassing tree ensembles, boosting, kernel methods, and a multilayer neural network-were optimized using stratified five-fold cross-validation. Their predictions based on probabilities helped us build the proposed model, which included Random Forest, XGBoost, LightGBM, and a support-vector classifier, with logistic regression acting as a meta-learner. The proposed model achieved an accuracy rate of 97.2% and an F1-score of 97.15%, indicating a significant enhancement compared to the leading individual model, LightGBM, which had an accuracy of 91.4%. Our study's findings indicate that rigorous preprocessing, coupled with a diverse hybrid model, can convert low-cost tabular data into a nearly clinical-grade stroke-risk assessment tool.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Networks Based Anomalous RSSI Detection</title>
<link>https://arxiv.org/abs/2505.15847</link>
<guid>https://arxiv.org/abs/2505.15847</guid>
<content:encoded><![CDATA[
arXiv:2505.15847v1 Announce Type: cross 
Abstract: In today's world, modern infrastructures are being equipped with information and communication technologies to create large IoT networks.
  It is essential to monitor these networks to ensure smooth operations by detecting and correcting link failures or abnormal network behaviour proactively, which can otherwise cause interruptions in business operations.
  This paper presents a novel method for detecting anomalies in wireless links using graph neural networks. The proposed approach involves converting time series data into graphs and training a new graph neural network architecture based on graph attention networks that successfully detects anomalies at the level of individual measurements of the time series data. The model provides competitive results compared to the state of the art while being computationally more efficient with ~171 times fewer trainable parameters.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integration of TinyML and LargeML: A Survey of 6G and Beyond</title>
<link>https://arxiv.org/abs/2505.15854</link>
<guid>https://arxiv.org/abs/2505.15854</guid>
<content:encoded><![CDATA[
arXiv:2505.15854v1 Announce Type: cross 
Abstract: The transition from 5G networks to 6G highlights a significant demand for machine learning (ML). Deep learning models, in particular, have seen wide application in mobile networking and communications to support advanced services in emerging wireless environments, such as smart healthcare, smart grids, autonomous vehicles, aerial platforms, digital twins, and the metaverse. The rapid expansion of Internet-of-Things (IoT) devices, many with limited computational capabilities, has accelerated the development of tiny machine learning (TinyML) and resource-efficient ML approaches for cost-effective services. However, the deployment of large-scale machine learning (LargeML) solutions require major computing resources and complex management strategies to support extensive IoT services and ML-generated content applications. Consequently, the integration of TinyML and LargeML is projected as a promising approach for future seamless connectivity and efficient resource management.
  Although the integration of TinyML and LargeML shows abundant potential, several challenges persist, including performance optimization, practical deployment strategies, effective resource management, and security considerations. In this survey, we review and analyze the latest research aimed at enabling the integration of TinyML and LargeML models for the realization of smart services and applications in future 6G networks and beyond. The paper concludes by outlining critical challenges and identifying future research directions for the holistic integration of TinyML and LargeML in next-generation wireless networks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-omic Causal Discovery using Genotypes and Gene Expression</title>
<link>https://arxiv.org/abs/2505.15866</link>
<guid>https://arxiv.org/abs/2505.15866</guid>
<content:encoded><![CDATA[
arXiv:2505.15866v1 Announce Type: cross 
Abstract: Causal discovery in multi-omic datasets is crucial for understanding the bigger picture of gene regulatory mechanisms, but remains challenging due to high dimensionality, differentiation of direct from indirect relationships, and hidden confounders. We introduce GENESIS (GEne Network inference from Expression SIgnals and SNPs), a constraint-based algorithm that leverages the natural causal precedence of genotypes to infer ancestral relationships in transcriptomic data. Unlike traditional causal discovery methods that start with a fully connected graph, GENESIS initialises an empty ancestrality matrix and iteratively populates it with direct, indirect or non-causal relationships using a series of provably sound marginal and conditional independence tests. By integrating genotypes as fixed causal anchors, GENESIS provides a principled ``head start'' to classical causal discovery algorithms, restricting the search space to biologically plausible edges. We test GENESIS on synthetic and real-world genomic datasets. This framework offers a powerful avenue for uncovering causal pathways in complex traits, with promising applications to functional genomics, drug discovery, and precision medicine.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval</title>
<link>https://arxiv.org/abs/2505.15867</link>
<guid>https://arxiv.org/abs/2505.15867</guid>
<content:encoded><![CDATA[
arXiv:2505.15867v1 Announce Type: cross 
Abstract: Despite the dominance of convolutional and transformer-based architectures in image-to-image retrieval, these models are prone to biases arising from low-level visual features, such as color. Recognizing the lack of semantic understanding as a key limitation, we propose a novel scene graph-based retrieval framework that emphasizes semantic content over superficial image characteristics. Prior approaches to scene graph retrieval predominantly rely on supervised Graph Neural Networks (GNNs), which require ground truth graph pairs driven from image captions. However, the inconsistency of caption-based supervision stemming from variable text encodings undermine retrieval reliability. To address these, we present SCENIR, a Graph Autoencoder-based unsupervised retrieval framework, which eliminates the dependence on labeled training data. Our model demonstrates superior performance across metrics and runtime efficiency, outperforming existing vision-based, multimodal, and supervised GNN approaches. We further advocate for Graph Edit Distance (GED) as a deterministic and robust ground truth measure for scene graph similarity, replacing the inconsistent caption-based alternatives for the first time in image-to-image retrieval evaluation. Finally, we validate the generalizability of our method by applying it to unannotated datasets via automated scene graph generation, while substantially contributing in advancing state-of-the-art in counterfactual image retrieval.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval</title>
<link>https://arxiv.org/abs/2505.15877</link>
<guid>https://arxiv.org/abs/2505.15877</guid>
<content:encoded><![CDATA[
arXiv:2505.15877v1 Announce Type: cross 
Abstract: While an image is worth more than a thousand words, only a few provide crucial information for a given task and thus should be focused on. In light of this, ideal text-to-image (T2I) retrievers should prioritize specific visual attributes relevant to queries. To evaluate current retrievers on handling attribute-focused queries, we build COCO-Facet, a COCO-based benchmark with 9,112 queries about diverse attributes of interest. We find that CLIP-like retrievers, which are widely adopted due to their efficiency and zero-shot ability, have poor and imbalanced performance, possibly because their image embeddings focus on global semantics and subjects while leaving out other details. Notably, we reveal that even recent Multimodal Large Language Model (MLLM)-based, stronger retrievers with a larger output dimension struggle with this limitation. Hence, we hypothesize that retrieving with general image embeddings is suboptimal for performing such queries. As a solution, we propose to use promptable image embeddings enabled by these multimodal retrievers, which boost performance by highlighting required attributes. Our pipeline for deriving such embeddings generalizes across query types, image pools, and base retriever architectures. To enhance real-world applicability, we offer two acceleration strategies: Pre-processing promptable embeddings and using linear approximations. We show that the former yields a 15% improvement in Recall@5 when prompts are predefined, while the latter achieves an 8% improvement when prompts are only available during inference.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision</title>
<link>https://arxiv.org/abs/2505.15927</link>
<guid>https://arxiv.org/abs/2505.15927</guid>
<content:encoded><![CDATA[
arXiv:2505.15927v1 Announce Type: cross 
Abstract: Learning complex functions that involve multi-step reasoning poses a significant challenge for standard supervised learning from input-output examples. Chain-of-thought (CoT) supervision, which provides intermediate reasoning steps together with the final output, has emerged as a powerful empirical technique, underpinning much of the recent progress in the reasoning capabilities of large language models. This paper develops a statistical theory of learning under CoT supervision. A key characteristic of the CoT setting, in contrast to standard supervision, is the mismatch between the training objective (CoT risk) and the test objective (end-to-end risk). A central part of our analysis, distinguished from prior work, is explicitly linking those two types of risk to achieve sharper sample complexity bounds. This is achieved via the *CoT information measure* $\mathcal{I}_{\mathcal{D}, h_\star}^{\mathrm{CoT}}(\epsilon; \calH)$, which quantifies the additional discriminative power gained from observing the reasoning process. The main theoretical results demonstrate how CoT supervision can yield significantly faster learning rates compared to standard E2E supervision. Specifically, it is shown that the sample complexity required to achieve a target E2E error $\epsilon$ scales as $d/\mathcal{I}_{\mathcal{D}, h_\star}^{\mathrm{CoT}}(\epsilon; \calH)$, where $d$ is a measure of hypothesis class complexity, which can be much faster than standard $d/\epsilon$ rates. Information-theoretic lower bounds in terms of the CoT information are also obtained. Together, these results suggest that CoT information is a fundamental measure of statistical complexity for learning under chain-of-thought supervision.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven Verification of Procedural Programs with Integer Arrays</title>
<link>https://arxiv.org/abs/2505.15958</link>
<guid>https://arxiv.org/abs/2505.15958</guid>
<content:encoded><![CDATA[
arXiv:2505.15958v1 Announce Type: cross 
Abstract: We address the problem of verifying automatically procedural programs manipulating parametric-size arrays of integers, encoded as a constrained Horn clauses solving problem. We propose a new algorithmic method for synthesizing loop invariants and procedure pre/post-conditions represented as universally quantified first-order formulas constraining the array elements and program variables. We adopt a data-driven approach that extends the decision tree Horn-ICE framework to handle arrays. We provide a powerful learning technique based on reducing a complex classification problem of vectors of integer arrays to a simpler classification problem of vectors of integers. The obtained classifier is generalized to get universally quantified invariants and procedure pre/post-conditions. We have implemented our method and shown its efficiency and competitiveness w.r.t. state-of-the-art tools on a significant benchmark.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-training Large Memory Language Models with Internal and External Knowledge</title>
<link>https://arxiv.org/abs/2505.15962</link>
<guid>https://arxiv.org/abs/2505.15962</guid>
<content:encoded><![CDATA[
arXiv:2505.15962v1 Announce Type: cross 
Abstract: Neural language models are black-boxes -- both linguistic patterns and factual knowledge are distributed across billions of opaque parameters. This entangled encoding makes it difficult to reliably inspect, verify, or update specific facts. We propose a new class of language models, Large Memory Language Models (LMLM) with a pre-training recipe that stores factual knowledge in both internal weights and an external database. Our approach strategically masks externally retrieved factual values from the training loss, thereby teaching the model to perform targeted lookups rather than relying on memorization in model weights. Our experiments demonstrate that LMLMs achieve competitive performance compared to significantly larger, knowledge-dense LLMs on standard benchmarks, while offering the advantages of explicit, editable, and verifiable knowledge bases. This work represents a fundamental shift in how language models interact with and manage factual knowledge.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2505.15970</link>
<guid>https://arxiv.org/abs/2505.15970</guid>
<content:encoded><![CDATA[
arXiv:2505.15970v1 Announce Type: cross 
Abstract: The ImageNet hierarchy provides a structured taxonomy of object categories, offering a valuable lens through which to analyze the representations learned by deep vision models. In this work, we conduct a comprehensive analysis of how vision models encode the ImageNet hierarchy, leveraging Sparse Autoencoders (SAEs) to probe their internal representations. SAEs have been widely used as an explanation tool for large language models (LLMs), where they enable the discovery of semantically meaningful features. Here, we extend their use to vision models to investigate whether learned representations align with the ontological structure defined by the ImageNet taxonomy. Our results show that SAEs uncover hierarchical relationships in model activations, revealing an implicit encoding of taxonomic structure. We analyze the consistency of these representations across different layers of the popular vision foundation model DINOv2 and provide insights into how deep vision models internalize hierarchical category information by increasing information in the class token through each layer. Our study establishes a framework for systematic hierarchical analysis of vision model representations and highlights the potential of SAEs as a tool for probing semantic structure in deep networks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Stress Monitoring, Detection, and Management in College Students: A Wearable Technology and Machine-Learning Approach</title>
<link>https://arxiv.org/abs/2505.15974</link>
<guid>https://arxiv.org/abs/2505.15974</guid>
<content:encoded><![CDATA[
arXiv:2505.15974v1 Announce Type: cross 
Abstract: College students are increasingly affected by stress, anxiety, and depression, yet face barriers to traditional mental health care. This study evaluated the efficacy of a mobile health (mHealth) intervention, Mental Health Evaluation and Lookout Program (mHELP), which integrates a smartwatch sensor and machine learning (ML) algorithms for real-time stress detection and self-management. In a 12-week randomized controlled trial (n = 117), participants were assigned to a treatment group using mHELP's full suite of interventions or a control group using the app solely for real-time stress logging and weekly psychological assessments. The primary outcome, "Moments of Stress" (MS), was assessed via physiological and self-reported indicators and analyzed using Generalized Linear Mixed Models (GLMM) approaches. Similarly, secondary outcomes of psychological assessments, including the Generalized Anxiety Disorder-7 (GAD-7) for anxiety, the Patient Health Questionnaire (PHQ-8) for depression, and the Perceived Stress Scale (PSS), were also analyzed via GLMM. The finding of the objective measure, MS, indicates a substantial decrease in MS among the treatment group compared to the control group, while no notable between-group differences were observed in subjective scores of anxiety (GAD-7), depression (PHQ-8), or stress (PSS). However, the treatment group exhibited a clinically meaningful decline in GAD-7 and PSS scores. These findings underscore the potential of wearable-enabled mHealth tools to reduce acute stress in college populations and highlight the need for extended interventions and tailored features to address chronic symptoms like depression.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Probabilistic Generative Models for Accelerated, in-NICU Permanent Magnet Neonatal MRI</title>
<link>https://arxiv.org/abs/2505.15984</link>
<guid>https://arxiv.org/abs/2505.15984</guid>
<content:encoded><![CDATA[
arXiv:2505.15984v1 Announce Type: cross 
Abstract: Purpose: Magnetic Resonance Imaging (MRI) enables non-invasive assessment of brain abnormalities during early life development. Permanent magnet scanners operating in the neonatal intensive care unit (NICU) facilitate MRI of sick infants, but have long scan times due to lower signal-to-noise ratios (SNR) and limited receive coils. This work accelerates in-NICU MRI with diffusion probabilistic generative models by developing a training pipeline accounting for these challenges.
  Methods: We establish a novel training dataset of clinical, 1 Tesla neonatal MR images in collaboration with Aspect Imaging and Sha'are Zedek Medical Center. We propose a pipeline to handle the low quantity and SNR of our real-world dataset (1) modifying existing network architectures to support varying resolutions; (2) training a single model on all data with learned class embedding vectors; (3) applying self-supervised denoising before training; and (4) reconstructing by averaging posterior samples. Retrospective under-sampling experiments, accounting for signal decay, evaluated each item of our proposed methodology. A clinical reader study with practicing pediatric neuroradiologists evaluated our proposed images reconstructed from 1.5x under-sampled data.
  Results: Combining all data, denoising pre-training, and averaging posterior samples yields quantitative improvements in reconstruction. The generative model decouples the learned prior from the measurement model and functions at two acceleration rates without re-training. The reader study suggests that proposed images reconstructed from approximately 1.5x under-sampled data are adequate for clinical use.
  Conclusion: Diffusion probabilistic generative models applied with the proposed pipeline to handle challenging real-world datasets could reduce scan time of in-NICU neonatal MRI.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16022</link>
<guid>https://arxiv.org/abs/2505.16022</guid>
<content:encoded><![CDATA[
arXiv:2505.16022v1 Announce Type: cross 
Abstract: Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learning paradigm that computes rewards solely based on the final answer part of a language model's output, thereby encouraging the generation of intermediate reasoning steps. However, these methods fundamentally rely on external verifiers, which limits their applicability to domains like mathematics and coding where such verifiers are readily available. Although reward models can serve as verifiers, they require high-quality annotated data and are costly to train. In this work, we propose NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning framework that requires only standard supervised fine-tuning data with no need for an external verifier. NOVER enables incentive training across a wide range of text-to-text tasks and outperforms the model of the same size distilled from large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the flexibility of NOVER enables new possibilities for optimizing large language models, such as inverse incentive training.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal LLM Routing: End-to-End Regret Minimization from Observational Data</title>
<link>https://arxiv.org/abs/2505.16037</link>
<guid>https://arxiv.org/abs/2505.16037</guid>
<content:encoded><![CDATA[
arXiv:2505.16037v1 Announce Type: cross 
Abstract: LLM routing aims to select the most appropriate model for each query, balancing competing performance metrics such as accuracy and cost across a pool of language models. Prior approaches typically adopt a decoupled strategy, where the metrics are first predicted and the model is then selected based on these estimates. This setup is prone to compounding errors and often relies on full-feedback data, where each query is evaluated by all candidate models, which is costly to obtain and maintain in practice. In contrast, we learn from observational data, which records only the outcome of the model actually deployed. We propose a causal end-to-end framework that learns routing policies by minimizing decision-making regret from observational data. To enable efficient optimization, we introduce two theoretically grounded surrogate objectives: a classification-based upper bound, and a softmax-weighted regret approximation shown to recover the optimal policy at convergence. We further extend our framework to handle heterogeneous cost preferences via an interval-conditioned architecture. Experiments on public benchmarks show that our method outperforms existing baselines, achieving state-of-the-art performance across different embedding models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-based machine learning for mantle convection simulations</title>
<link>https://arxiv.org/abs/2505.16041</link>
<guid>https://arxiv.org/abs/2505.16041</guid>
<content:encoded><![CDATA[
arXiv:2505.16041v1 Announce Type: cross 
Abstract: Mantle convection simulations are an essential tool for understanding how rocky planets evolve. However, the poorly known input parameters to these simulations, the non-linear dependence of transport properties on pressure and temperature, and the long integration times in excess of several billion years all pose a computational challenge for numerical solvers. We propose a physics-based machine learning approach that predicts creeping flow velocities as a function of temperature while conserving mass, thereby bypassing the numerical solution of the Stokes problem. A finite-volume solver then uses the predicted velocities to advect and diffuse the temperature field to the next time-step, enabling autoregressive rollout at inference. For training, our model requires temperature-velocity snapshots from a handful of simulations (94). We consider mantle convection in a two-dimensional rectangular box with basal and internal heating, pressure- and temperature-dependent viscosity. Overall, our model is up to 89 times faster than the numerical solver. We also show the importance of different components in our convolutional neural network architecture such as mass conservation, learned paddings on the boundaries, and loss scaling for the overall rollout performance. Finally, we test our approach on unseen scenarios to demonstrate some of its strengths and weaknesses.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom Severity Estimation</title>
<link>https://arxiv.org/abs/2505.16044</link>
<guid>https://arxiv.org/abs/2505.16044</guid>
<content:encoded><![CDATA[
arXiv:2505.16044v1 Announce Type: cross 
Abstract: Studies on schizophrenia assessments using deep learning typically treat it as a classification task to detect the presence or absence of the disorder, oversimplifying the condition and reducing its clinical applicability. This traditional approach overlooks the complexity of schizophrenia, limiting its practical value in healthcare settings. This study shifts the focus to individual symptom severity estimation using a multimodal approach that integrates speech, video, and text inputs. We develop unimodal models for each modality and a multimodal framework to improve accuracy and robustness. By capturing a more detailed symptom profile, this approach can help in enhancing diagnostic precision and support personalized treatment, offering a scalable and objective tool for mental health assessment.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PO-Flow: Flow-based Generative Models for Sampling Potential Outcomes and Counterfactuals</title>
<link>https://arxiv.org/abs/2505.16051</link>
<guid>https://arxiv.org/abs/2505.16051</guid>
<content:encoded><![CDATA[
arXiv:2505.16051v1 Announce Type: cross 
Abstract: We propose PO-Flow, a novel continuous normalizing flow (CNF) framework for causal inference that jointly models potential outcomes and counterfactuals. Trained via flow matching, PO-Flow provides a unified framework for individualized potential outcome prediction, counterfactual predictions, and uncertainty-aware density learning. Among generative models, it is the first to enable density learning of potential outcomes without requiring explicit distributional assumptions (e.g., Gaussian mixtures), while also supporting counterfactual prediction conditioned on factual outcomes in general observational datasets. On benchmarks such as ACIC, IHDP, and IBM, it consistently outperforms prior methods across a range of causal inference tasks. Beyond that, PO-Flow succeeds in high-dimensional settings, including counterfactual image generation, demonstrating its broad applicability.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oh SnapMMD! Forecasting Stochastic Dynamics Beyond the Schr\"odinger Bridge's End</title>
<link>https://arxiv.org/abs/2505.16082</link>
<guid>https://arxiv.org/abs/2505.16082</guid>
<content:encoded><![CDATA[
arXiv:2505.16082v1 Announce Type: cross 
Abstract: Scientists often want to make predictions beyond the observed time horizon of "snapshot" data following latent stochastic dynamics. For example, in time course single-cell mRNA profiling, scientists have access to cellular transcriptional state measurements (snapshots) from different biological replicates at different time points, but they cannot access the trajectory of any one cell because measurement destroys the cell. Researchers want to forecast (e.g.) differentiation outcomes from early state measurements of stem cells. Recent Schr\"odinger-bridge (SB) methods are natural for interpolating between snapshots. But past SB papers have not addressed forecasting -- likely since existing methods either (1) reduce to following pre-set reference dynamics (chosen before seeing data) or (2) require the user to choose a fixed, state-independent volatility since they minimize a Kullback-Leibler divergence. Either case can lead to poor forecasting quality. In the present work, we propose a new framework, SnapMMD, that learns dynamics by directly fitting the joint distribution of both state measurements and observation time with a maximum mean discrepancy (MMD) loss. Unlike past work, our method allows us to infer unknown and state-dependent volatilities from the observed data. We show in a variety of real and synthetic experiments that our method delivers accurate forecasts. Moreover, our approach allows us to learn in the presence of incomplete state measurements and yields an $R^2$-style statistic that diagnoses fit. We also find that our method's performance at interpolation (and general velocity-field reconstruction) is at least as good as (and often better than) state-of-the-art in almost all of our experiments.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dimension-adapted Momentum Outscales SGD</title>
<link>https://arxiv.org/abs/2505.16098</link>
<guid>https://arxiv.org/abs/2505.16098</guid>
<content:encoded><![CDATA[
arXiv:2505.16098v1 Announce Type: cross 
Abstract: We investigate scaling laws for stochastic momentum algorithms with small batch on the power law random features model, parameterized by data complexity, target complexity, and model size. When trained with a stochastic momentum algorithm, our analysis reveals four distinct loss curve shapes determined by varying data-target complexities. While traditional stochastic gradient descent with momentum (SGD-M) yields identical scaling law exponents to SGD, dimension-adapted Nesterov acceleration (DANA) improves these exponents by scaling momentum hyperparameters based on model size and data complexity. This outscaling phenomenon, which also improves compute-optimal scaling behavior, is achieved by DANA across a broad range of data and target complexities, while traditional methods fall short. Extensive experiments on high-dimensional synthetic quadratics validate our theoretical predictions and large-scale text experiments with LSTMs show DANA's improved loss exponents over SGD hold in a practical setting.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.16104</link>
<guid>https://arxiv.org/abs/2505.16104</guid>
<content:encoded><![CDATA[
arXiv:2505.16104v1 Announce Type: cross 
Abstract: With the increasing size of Large Vision-Language Models (LVLMs), network pruning techniques aimed at compressing models for deployment in resource-constrained environments have garnered significant attention. However, we observe that pruning often leads to a degradation in safety performance. To address this issue, we present a novel and lightweight approach, termed Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the contribution of each attention head to safety, identifying the most critical ones, and then selectively restoring neurons directly within these attention heads that play a pivotal role in maintaining safety. This process hierarchically realigns the safety of pruned LVLMs, progressing from the attention head level to the neuron level. We validate HSR across various models and pruning strategies, consistently achieving notable improvements in safety performance. To our knowledge, this is the first work explicitly focused on restoring safety in LVLMs post-pruning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning the 6d Supergravity Landscape</title>
<link>https://arxiv.org/abs/2505.16131</link>
<guid>https://arxiv.org/abs/2505.16131</guid>
<content:encoded><![CDATA[
arXiv:2505.16131v1 Announce Type: cross 
Abstract: In this paper, we apply both supervised and unsupervised machine learning algorithms to the study of the string landscape and swampland in 6-dimensions. Our data are the (almost) anomaly-free 6-dimensional $\mathcal{N} = (1,0)$ supergravity models, characterised by the Gram matrix of anomaly coefficients. Our work demonstrates the ability of machine learning algorithms to efficiently learn highly complex features of the landscape and swampland. Employing an autoencoder for unsupervised learning, we provide an auto-classification of these models by compressing the Gram matrix data to 2-dimensions. Through compression, similar models cluster together, and we identify prominent features of these clusters. The autoencoder also identifies outlier models which are difficult to reconstruct. One of these outliers proves to be incredibly difficult to combine with other models such that the $\text{tr}R^{4}$ anomaly vanishes, making its presence in the landscape extremely rare. Further, we utilise supervised learning to build two classifiers predicting (1) model consistency under probe string insertion (precision: 0.78, predicting consistency for 214,837 models with reasonable certainty) and (2) inconsistency under anomaly inflow (precision: 0.91, predicting inconsistency for 1,909,359 models). Notably, projecting these predictions onto the autoencoder's 2-dimensional latent layer shows consistent models clustering together, further indicating that the autoencoder has learnt interesting and complex features of the set of models and potentially offers a novel approach to mapping the landscape and swampland of 6-dimensional supergravity theories.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2505.16134</link>
<guid>https://arxiv.org/abs/2505.16134</guid>
<content:encoded><![CDATA[
arXiv:2505.16134v1 Announce Type: cross 
Abstract: Large language models exhibit positional bias -- systematic neglect of information at specific context positions -- yet its interplay with linguistic diversity remains poorly understood. We present a cross-linguistic study across five typologically distinct languages (English, Russian, German, Hindi, Vietnamese), examining how positional bias interacts with model uncertainty, syntax, and prompting. Key findings: (1) Positional bias is model-driven, with language-specific variations -- Qwen2.5-7B favors late positions, challenging assumptions of early-token bias; (2) Explicit positional guidance (e.g., correct context is at position X) reduces accuracy across languages, undermining prompt-engineering practices; (3) Aligning context with positional bias increases entropy, yet minimal entropy does not predict accuracy. (4) We further uncover that LLMs differently impose dominant word order in free-word-order languages like Hindi.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sudoku-Bench: Evaluating creative reasoning with Sudoku variants</title>
<link>https://arxiv.org/abs/2505.16135</link>
<guid>https://arxiv.org/abs/2505.16135</guid>
<content:encoded><![CDATA[
arXiv:2505.16135v1 Announce Type: cross 
Abstract: Existing reasoning benchmarks for large language models (LLMs) frequently fail to capture authentic creativity, often rewarding memorization of previously observed patterns. We address this shortcoming with Sudoku-Bench, a curated benchmark of challenging and unconventional Sudoku variants specifically selected to evaluate creative, multi-step logical reasoning. Sudoku variants form an unusually effective domain for reasoning research: each puzzle introduces unique or subtly interacting constraints, making memorization infeasible and requiring solvers to identify novel logical breakthroughs (``break-ins''). Despite their diversity, Sudoku variants maintain a common and compact structure, enabling clear and consistent evaluation. Sudoku-Bench includes a carefully chosen puzzle set, a standardized text-based puzzle representation, and flexible tools compatible with thousands of publicly available puzzles -- making it easy to extend into a general research environment. Baseline experiments show that state-of-the-art LLMs solve fewer than 15\% of puzzles unaided, highlighting significant opportunities to advance long-horizon, strategic reasoning capabilities.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Machine Learning for Macro Alpha: A News Sentiment Case Study</title>
<link>https://arxiv.org/abs/2505.16136</link>
<guid>https://arxiv.org/abs/2505.16136</guid>
<content:encoded><![CDATA[
arXiv:2505.16136v1 Announce Type: cross 
Abstract: This study introduces an interpretable machine learning (ML) framework to extract macroeconomic alpha from global news sentiment. We process the Global Database of Events, Language, and Tone (GDELT) Project's worldwide news feed using FinBERT -- a Bidirectional Encoder Representations from Transformers (BERT) based model pretrained on finance-specific language -- to construct daily sentiment indices incorporating mean tone, dispersion, and event impact. These indices drive an XGBoost classifier, benchmarked against logistic regression, to predict next-day returns for EUR/USD, USD/JPY, and 10-year U.S. Treasury futures (ZN). Rigorous out-of-sample (OOS) backtesting (5-fold expanding-window cross-validation, OOS period: c. 2017-April 2025) demonstrates exceptional, cost-adjusted performance for the XGBoost strategy: Sharpe ratios achieve 5.87 (EUR/USD), 4.65 (USD/JPY), and 4.65 (Treasuries), with respective compound annual growth rates (CAGRs) exceeding 50% in Foreign Exchange (FX) and 22% in bonds. Shapley Additive Explanations (SHAP) affirm that sentiment dispersion and article impact are key predictive features. Our findings establish that integrating domain-specific Natural Language Processing (NLP) with interpretable ML offers a potent and explainable source of macro alpha.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exponential Convergence of CAVI for Bayesian PCA</title>
<link>https://arxiv.org/abs/2505.16145</link>
<guid>https://arxiv.org/abs/2505.16145</guid>
<content:encoded><![CDATA[
arXiv:2505.16145v1 Announce Type: cross 
Abstract: Probabilistic principal component analysis (PCA) and its Bayesian variant (BPCA) are widely used for dimension reduction in machine learning and statistics. The main advantage of probabilistic PCA over the traditional formulation is allowing uncertainty quantification. The parameters of BPCA are typically learned using mean-field variational inference, and in particular, the coordinate ascent variational inference (CAVI) algorithm. So far, the convergence speed of CAVI for BPCA has not been characterized. In our paper, we fill this gap in the literature. Firstly, we prove a precise exponential convergence result in the case where the model uses a single principal component (PC). Interestingly, this result is established through a connection with the classical $\textit{power iteration algorithm}$ and it indicates that traditional PCA is retrieved as points estimates of the BPCA parameters. Secondly, we leverage recent tools to prove exponential convergence of CAVI for the model with any number of PCs, thus leading to a more general result, but one that is of a slightly different flavor. To prove the latter result, we additionally needed to introduce a novel lower bound for the symmetric Kullback--Leibler divergence between two multivariate normal distributions, which, we believe, is of independent interest in information theory.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation</title>
<link>https://arxiv.org/abs/2505.16146</link>
<guid>https://arxiv.org/abs/2505.16146</guid>
<content:encoded><![CDATA[
arXiv:2505.16146v1 Announce Type: cross 
Abstract: Large vision-language models (LVLMs) have achieved remarkable performance on multimodal tasks such as visual question answering (VQA) and image captioning. However, they still suffer from hallucinations, generating text inconsistent with visual input, posing significant risks in real-world applications. Existing approaches to address this issue focus on incorporating external knowledge bases, alignment training, or decoding strategies, all of which require substantial computational cost and time. Recent works try to explore more efficient alternatives by adjusting LVLMs' internal representations. Although promising, these methods may cause hallucinations to be insufficiently suppressed or lead to excessive interventions that negatively affect normal semantics. In this work, we leverage sparse autoencoders (SAEs) to identify semantic directions closely associated with either hallucinations or actuality, realizing more precise and direct hallucination-related representations. Our analysis demonstrates that interventions along the faithful direction we identified can mitigate hallucinations, while those along the hallucinatory direction can exacerbate them. Building on these insights, we propose Steering LVLMs via SAE Latent Directions (SSL), a training-free method based on SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive experiments demonstrate that SSL significantly outperforms existing decoding approaches in mitigating hallucinations, while maintaining transferability across different model architectures with negligible additional time overhead.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integral Imprecise Probability Metrics</title>
<link>https://arxiv.org/abs/2505.16156</link>
<guid>https://arxiv.org/abs/2505.16156</guid>
<content:encoded><![CDATA[
arXiv:2505.16156v1 Announce Type: cross 
Abstract: Quantifying differences between probability distributions is fundamental to statistics and machine learning, primarily for comparing statistical uncertainty. In contrast, epistemic uncertainty (EU) -- due to incomplete knowledge -- requires richer representations than those offered by classical probability. Imprecise probability (IP) theory offers such models, capturing ambiguity and partial belief. This has driven growing interest in imprecise probabilistic machine learning (IPML), where inference and decision-making rely on broader uncertainty models -- highlighting the need for metrics beyond classical probability. This work introduces the Integral Imprecise Probability Metric (IIPM) framework, a Choquet integral-based generalisation of classical Integral Probability Metric (IPM) to the setting of capacities -- a broad class of IP models encompassing many existing ones, including lower probabilities, probability intervals, belief functions, and more. Theoretically, we establish conditions under which IIPM serves as a valid metric and metrises a form of weak convergence of capacities. Practically, IIPM not only enables comparison across different IP models but also supports the quantification of epistemic uncertainty within a single IP model. In particular, by comparing an IP model with its conjugate, IIPM gives rise to a new class of EU measures -- Maximum Mean Imprecision -- which satisfy key axiomatic properties proposed in the Uncertainty Quantification literature. We validate MMI through selective classification experiments, demonstrating strong empirical performance against established EU measures, and outperforming them when classical methods struggle to scale to a large number of classes. Our work advances both theory and practice in IPML, offering a principled framework for comparing and quantifying epistemic uncertainty under imprecision.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet</title>
<link>https://arxiv.org/abs/2505.16195</link>
<guid>https://arxiv.org/abs/2505.16195</guid>
<content:encoded><![CDATA[
arXiv:2505.16195v1 Announce Type: cross 
Abstract: Foley synthesis aims to synthesize high-quality audio that is both semantically and temporally aligned with video frames. Given its broad application in creative industries, the task has gained increasing attention in the research community. To avoid the non-trivial task of training audio generative models from scratch, adapting pretrained audio generative models for video-synchronized foley synthesis presents an attractive direction. ControlNet, a method for adding fine-grained controls to pretrained generative models, has been applied to foley synthesis, but its use has been limited to handcrafted human-readable temporal conditions. In contrast, from-scratch models achieved success by leveraging high-dimensional deep features extracted using pretrained video encoders. We have observed a performance gap between ControlNet-based and from-scratch foley models. To narrow this gap, we propose SpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward video-synchronized foley synthesis via ControlNet. To unlock the potential of a single ControlNet branch, we resolve the discrepancy between the temporal video features and the time-frequency nature of the pretrained SpecMaskGIT via a frequency-aware temporal feature aligner, eliminating the need for complicated conditioning mechanisms widely used in prior arts. Evaluations on a common foley synthesis benchmark demonstrate that SpecMaskFoley could even outperform strong from-scratch baselines, substantially advancing the development of ControlNet-based foley synthesis models. Demo page: https://zzaudio.github.io/SpecMaskFoley_Demo/
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Echo-State Networks to Reproduce Rare Events in Chaotic Systems</title>
<link>https://arxiv.org/abs/2505.16208</link>
<guid>https://arxiv.org/abs/2505.16208</guid>
<content:encoded><![CDATA[
arXiv:2505.16208v1 Announce Type: cross 
Abstract: We apply the Echo-State Networks to predict the time series and statistical properties of the competitive Lotka-Volterra model in the chaotic regime. In particular, we demonstrate that Echo-State Networks successfully learn the chaotic attractor of the competitive Lotka-Volterra model and reproduce histograms of dependent variables, including tails and rare events. We use the Generalized Extreme Value distribution to quantify the tail behavior.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable Hierarchical Intrusion Detection System for Internet of Vehicles</title>
<link>https://arxiv.org/abs/2505.16215</link>
<guid>https://arxiv.org/abs/2505.16215</guid>
<content:encoded><![CDATA[
arXiv:2505.16215v1 Announce Type: cross 
Abstract: Due to its nature of dynamic, mobility, and wireless data transfer, the Internet of Vehicles (IoV) is prone to various cyber threats, ranging from spoofing and Distributed Denial of Services (DDoS) attacks to malware. To safeguard the IoV ecosystem from intrusions, malicious activities, policy violations, intrusion detection systems (IDS) play a critical role by continuously monitoring and analyzing network traffic to identify and mitigate potential threats in real-time. However, most existing research has focused on developing centralized, machine learning-based IDS systems for IoV without accounting for its inherently distributed nature. Due to intensive computing requirements, these centralized systems often rely on the cloud to detect cyber threats, increasing delay of system response. On the other hand, edge nodes typically lack the necessary resources to train and deploy complex machine learning algorithms. To address this issue, this paper proposes an effective hierarchical classification framework tailored for IoV networks. Hierarchical classification allows classifiers to be trained and tested at different levels, enabling edge nodes to detect specific types of attacks independently. With this approach, edge nodes can conduct targeted attack detection while leveraging cloud nodes for comprehensive threat analysis and support. Given the resource constraints of edge nodes, we have employed the Boruta feature selection method to reduce data dimensionality, optimizing processing efficiency. To evaluate our proposed framework, we utilize the latest IoV security dataset CIC-IoV2024, achieving promising results that demonstrate the feasibility and effectiveness of our models in securing IoV networks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network</title>
<link>https://arxiv.org/abs/2505.16223</link>
<guid>https://arxiv.org/abs/2505.16223</guid>
<content:encoded><![CDATA[
arXiv:2505.16223v1 Announce Type: cross 
Abstract: In this paper, we propose MADCluster, a novel model-agnostic anomaly detection framework utilizing self-supervised clustering. MADCluster is applicable to various deep learning architectures and addresses the 'hypersphere collapse' problem inherent in existing deep learning-based anomaly detection methods. The core idea is to cluster normal pattern data into a 'single cluster' while simultaneously learning the cluster center and mapping data close to this center. Also, to improve expressiveness and enable effective single clustering, we propose a new 'One-directed Adaptive loss'. The optimization of this loss is mathematically proven. MADCluster consists of three main components: Base Embedder capturing high-dimensional temporal dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous center updates. Its model-agnostic characteristics are achieved by applying various architectures to the Base Embedder. Experiments on four time series benchmark datasets demonstrate that applying MADCluster improves the overall performance of comparative models. In conclusion, the compatibility of MADCluster shows potential for enhancing model performance across various architectures.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning</title>
<link>https://arxiv.org/abs/2505.16227</link>
<guid>https://arxiv.org/abs/2505.16227</guid>
<content:encoded><![CDATA[
arXiv:2505.16227v1 Announce Type: cross 
Abstract: Personalizing jargon detection and explanation is essential for making technical documents accessible to readers with diverse disciplinary backgrounds. However, tailoring models to individual users typically requires substantial annotation efforts and computational resources due to user-specific finetuning. To address this, we present a systematic study of personalized jargon detection, focusing on methods that are both efficient and scalable for real-world deployment. We explore two personalization strategies: (1) lightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models, and (2) personalized prompting, which tailors model behavior at inference time without retaining. To reflect realistic constraints, we also investigate hybrid approaches that combine limited annotated data with unsupervised user background signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in F1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably, our method achieves comparable performance using only 10% of the annotated training data, demonstrating its practicality for resource-constrained settings. Our study offers the first work to systematically explore efficient, low-resource personalization of jargon detection using open-source language models, offering a practical path toward scalable, user-adaptive NLP system.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Power Priors for Improved Bayesian Inference with Historical Data</title>
<link>https://arxiv.org/abs/2505.16244</link>
<guid>https://arxiv.org/abs/2505.16244</guid>
<content:encoded><![CDATA[
arXiv:2505.16244v1 Announce Type: cross 
Abstract: The power prior is a class of informative priors designed to incorporate historical data alongside current data in a Bayesian framework. It includes a power parameter that controls the influence of historical data, providing flexibility and adaptability. A key property of the power prior is that the resulting posterior minimizes a linear combination of KL divergences between two pseudo-posterior distributions: one ignoring historical data and the other fully incorporating it. We extend this framework by identifying the posterior distribution as the minimizer of a linear combination of Amari's $\alpha$-divergence, a generalization of KL divergence. We show that this generalization can lead to improved performance by allowing for the data to adapt to appropriate choices of the $\alpha$ parameter. Theoretical properties of this generalized power posterior are established, including behavior as a generalized geodesic on the Riemannian manifold of probability distributions, offering novel insights into its geometric interpretation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Smoothed Bayesian Black-Box Shift Estimator and Its Information Geometry</title>
<link>https://arxiv.org/abs/2505.16251</link>
<guid>https://arxiv.org/abs/2505.16251</guid>
<content:encoded><![CDATA[
arXiv:2505.16251v1 Announce Type: cross 
Abstract: Label shift adaptation aims to recover target class priors when the labelled source distribution $P$ and the unlabelled target distribution $Q$ share $P(X \mid Y) = Q(X \mid Y)$ but $P(Y) \neq Q(Y)$. Classical black-box shift estimators invert an empirical confusion matrix of a frozen classifier, producing a brittle point estimate that ignores sampling noise and similarity among classes. We present Graph-Smoothed Bayesian BBSE (GS-B$^3$SE), a fully probabilistic alternative that places Laplacian-Gaussian priors on both target log-priors and confusion-matrix columns, tying them together on a label-similarity graph. The resulting posterior is tractable with HMC or a fast block Newton-CG scheme. We prove identifiability, $N^{-1/2}$ contraction, variance bounds that shrink with the graph's algebraic connectivity, and robustness to Laplacian misspecification. We also reinterpret GS-B$^3$SE through information geometry, showing that it generalizes existing shift estimators.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Asymptotics of Test-Time Adaptation for Batch Normalization Statistics</title>
<link>https://arxiv.org/abs/2505.16257</link>
<guid>https://arxiv.org/abs/2505.16257</guid>
<content:encoded><![CDATA[
arXiv:2505.16257v1 Announce Type: cross 
Abstract: This study develops a higher-order asymptotic framework for test-time adaptation (TTA) of Batch Normalization (BN) statistics under distribution shift by integrating classical Edgeworth expansion and saddlepoint approximation techniques with a novel one-step M-estimation perspective. By analyzing the statistical discrepancy between training and test distributions, we derive an Edgeworth expansion for the normalized difference in BN means and obtain an optimal weighting parameter that minimizes the mean-squared error of the adapted statistic. Reinterpreting BN TTA as a one-step M-estimator allows us to derive higher-order local asymptotic normality results, which incorporate skewness and other higher moments into the estimator's behavior. Moreover, we quantify the trade-offs among bias, variance, and skewness in the adaptation process and establish a corresponding generalization bound on the model risk. The refined saddlepoint approximations further deliver uniformly accurate density and tail probability estimates for the BN TTA statistic. These theoretical insights provide a comprehensive understanding of how higher-order corrections and robust one-step updating can enhance the reliability and performance of BN layers in adapting to changing data distributions.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All You Need is "Leet": Evading Hate-speech Detection AI</title>
<link>https://arxiv.org/abs/2505.16263</link>
<guid>https://arxiv.org/abs/2505.16263</guid>
<content:encoded><![CDATA[
arXiv:2505.16263v1 Announce Type: cross 
Abstract: Social media and online forums are increasingly becoming popular. Unfortunately, these platforms are being used for spreading hate speech. In this paper, we design black-box techniques to protect users from hate-speech on online platforms by generating perturbations that can fool state of the art deep learning based hate speech detection models thereby decreasing their efficiency. We also ensure a minimal change in the original meaning of hate-speech. Our best perturbation attack is successfully able to evade hate-speech detection for 86.8 % of hateful text.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2505.16270</link>
<guid>https://arxiv.org/abs/2505.16270</guid>
<content:encoded><![CDATA[
arXiv:2505.16270v1 Announce Type: cross 
Abstract: Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space</title>
<link>https://arxiv.org/abs/2505.16301</link>
<guid>https://arxiv.org/abs/2505.16301</guid>
<content:encoded><![CDATA[
arXiv:2505.16301v1 Announce Type: cross 
Abstract: Molecular dynamics (MD) is a powerful tool for exploring the behavior of atomistic systems, but its reliance on sequential numerical integration limits simulation efficiency. We present MDtrajNet-1, a foundational AI model that directly generates MD trajectories across chemical space, bypassing force calculations and integration. This approach accelerates simulations by up to two orders of magnitude compared to traditional MD, even those enhanced by machine-learning interatomic potentials. MDtrajNet-1 combines equivariant neural networks with a Transformer-based architecture to achieve strong accuracy and transferability in predicting long-time trajectories for both known and unseen systems. Remarkably, the errors of the trajectories generated by MDtrajNet-1 for various molecular systems are close to those of the conventional ab initio MD. The model's flexible design supports diverse application scenarios, including different statistical ensembles, boundary conditions, and interaction types. By overcoming the intrinsic speed barrier of conventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable atomistic simulations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models</title>
<link>https://arxiv.org/abs/2505.16307</link>
<guid>https://arxiv.org/abs/2505.16307</guid>
<content:encoded><![CDATA[
arXiv:2505.16307v1 Announce Type: cross 
Abstract: Prompt optimization offers a practical and broadly applicable alternative to fine-tuning for improving large language model (LLM) performance. However, existing methods often rely on costly output generation, self-critiquing abilities, or human-annotated preferences, which limit their scalability, especially for smaller or non-instruction-tuned models. We introduce PMPO (Probabilistic Metric Prompt Optimization), a unified framework that refines prompts using token-level cross-entropy loss as a direct, lightweight evaluation signal. PMPO identifies low-quality prompt segments by masking and measuring their impact on loss, then rewrites and selects improved variants by minimizing loss over positive and negative examples. Unlike prior methods, it requires no output sampling or human evaluation during optimization, relying only on forward passes and log-likelihoods. PMPO supports both supervised and preference-based tasks through a closely aligned loss-based evaluation strategy. Experiments show that PMPO consistently outperforms prior methods across model sizes and tasks: it achieves the highest average accuracy on BBH, performs strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates by over 19 points. These results highlight PMPO's effectiveness, efficiency, and broad applicability.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions</title>
<link>https://arxiv.org/abs/2505.16311</link>
<guid>https://arxiv.org/abs/2505.16311</guid>
<content:encoded><![CDATA[
arXiv:2505.16311v1 Announce Type: cross 
Abstract: Recent advances in generative artificial intelligence (GenAI) models have enabled the generation of personalized content that adapts to up-to-date user context. While personalized decision systems are often modeled using bandit formulations, the integration of GenAI introduces new structure into otherwise classical sequential learning problems. In GenAI-powered interventions, the agent selects a query, but the environment experiences a stochastic response drawn from the generative model. Standard bandit methods do not explicitly account for this structure, where actions influence rewards only through stochastic, observed treatments. We introduce generator-mediated bandit-Thompson sampling (GAMBITTS), a bandit approach designed for this action/treatment split, using mobile health interventions with large language model-generated text as a motivating case study. GAMBITTS explicitly models both the treatment and reward generation processes, using information in the delivered treatment to accelerate policy learning relative to standard methods. We establish regret bounds for GAMBITTS by decomposing sources of uncertainty in treatment and reward, identifying conditions where it achieves stronger guarantees than standard bandit approaches. In simulation studies, GAMBITTS consistently outperforms conventional algorithms by leveraging observed treatments to more accurately estimate expected rewards.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings</title>
<link>https://arxiv.org/abs/2505.16313</link>
<guid>https://arxiv.org/abs/2505.16313</guid>
<content:encoded><![CDATA[
arXiv:2505.16313v1 Announce Type: cross 
Abstract: Deep neural networks for image classification remain vulnerable to adversarial examples -- small, imperceptible perturbations that induce misclassifications. In black-box settings, where only the final prediction is accessible, crafting targeted attacks that aim to misclassify into a specific target class is particularly challenging due to narrow decision regions. Current state-of-the-art methods often exploit the geometric properties of the decision boundary separating a source image and a target image rather than incorporating information from the images themselves. In contrast, we propose Targeted Edge-informed Attack (TEA), a novel attack that utilizes edge information from the target image to carefully perturb it, thereby producing an adversarial image that is closer to the source image while still achieving the desired target classification. Our approach consistently outperforms current state-of-the-art methods across different models in low query settings (nearly 70\% fewer queries are used), a scenario especially relevant in real-world applications with limited queries and black-box access. Furthermore, by efficiently generating a suitable adversarial example, TEA provides an improved target initialization for established geometry-based attacks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning novel representations of variable sources from multi-modal $\textit{Gaia}$ data via autoencoders</title>
<link>https://arxiv.org/abs/2505.16320</link>
<guid>https://arxiv.org/abs/2505.16320</guid>
<content:encoded><![CDATA[
arXiv:2505.16320v1 Announce Type: cross 
Abstract: Gaia Data Release 3 (DR3) published for the first time epoch photometry, BP/RP (XP) low-resolution mean spectra, and supervised classification results for millions of variable sources. This extensive dataset offers a unique opportunity to study their variability by combining multiple Gaia data products. In preparation for DR4, we propose and evaluate a machine learning methodology capable of ingesting multiple Gaia data products to achieve an unsupervised classification of stellar and quasar variability. A dataset of 4 million Gaia DR3 sources is used to train three variational autoencoders (VAE), which are artificial neural networks (ANNs) designed for data compression and generation. One VAE is trained on Gaia XP low-resolution spectra, another on a novel approach based on the distribution of magnitude differences in the Gaia G band, and the third on folded Gaia G band light curves. Each Gaia source is compressed into 15 numbers, representing the coordinates in a 15-dimensional latent space generated by combining the outputs of these three models. The learned latent representation produced by the ANN effectively distinguishes between the main variability classes present in Gaia DR3, as demonstrated through both supervised and unsupervised classification analysis of the latent space. The results highlight a strong synergy between light curves and low-resolution spectral data, emphasising the benefits of combining the different Gaia data products. A two-dimensional projection of the latent variables reveals numerous overdensities, most of which strongly correlate with astrophysical properties, showing the potential of this latent space for astrophysical discovery. We show that the properties of our novel latent representation make it highly valuable for variability analysis tasks, including classification, clustering and outlier detection.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Rates for Private Linear Regression in the Proportional Regime via Aggressive Clipping</title>
<link>https://arxiv.org/abs/2505.16329</link>
<guid>https://arxiv.org/abs/2505.16329</guid>
<content:encoded><![CDATA[
arXiv:2505.16329v1 Announce Type: cross 
Abstract: Differentially private (DP) linear regression has received significant attention in the recent theoretical literature, with several works aimed at obtaining improved error rates. A common approach is to set the clipping constant much larger than the expected norm of the per-sample gradients. While simplifying the analysis, this is however in sharp contrast with what empirical evidence suggests to optimize performance. Our work bridges this gap between theory and practice: we provide sharper rates for DP stochastic gradient descent (DP-SGD) by crucially operating in a regime where clipping happens frequently. Specifically, we consider the setting where the data is multivariate Gaussian, the number of training samples $n$ is proportional to the input dimension $d$, and the algorithm guarantees constant-order zero concentrated DP. Our method relies on establishing a deterministic equivalent for the trajectory of DP-SGD in terms of a family of ordinary differential equations (ODEs). As a consequence, the risk of DP-SGD is bounded between two ODEs, with upper and lower bounds matching for isotropic data. By studying these ODEs when $n / d$ is large enough, we demonstrate the optimality of aggressive clipping, and we uncover the benefits of decaying learning rate and private noise scheduling.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Attention Network for Optimal User Association in Wireless Networks</title>
<link>https://arxiv.org/abs/2505.16347</link>
<guid>https://arxiv.org/abs/2505.16347</guid>
<content:encoded><![CDATA[
arXiv:2505.16347v1 Announce Type: cross 
Abstract: With increased 5G deployments, network densification is higher than ever to support the exponentially high throughput requirements. However, this has meant a significant increase in energy consumption, leading to higher operational expenditure (OpEx) for network operators creating an acute need for improvements in network energy savings (NES). A key determinant of operational efficacy in cellular networks is the user association (UA) policy, as it affects critical aspects like spectral efficiency, load balancing etc. and therefore impacts the overall energy consumption of the network directly. Furthermore, with cellular network topologies lending themselves well to graphical abstractions, use of graphs in network optimization has gained significant prominence. In this work, we propose and analyze a graphical abstraction based optimization for UA in cellular networks to improve NES by determining when energy saving features like cell switch off can be activated. A comparison with legacy approaches establishes the superiority of the proposed approach.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.16360</link>
<guid>https://arxiv.org/abs/2505.16360</guid>
<content:encoded><![CDATA[
arXiv:2505.16360v1 Announce Type: cross 
Abstract: Semantic segmentation models trained on synthetic data often perform poorly on real-world images due to domain gaps, particularly in adverse conditions where labeled data is scarce. Yet, recent foundation models enable to generate realistic images without any training. This paper proposes to leverage such diffusion models to improve the performance of vision models when learned on synthetic data. We introduce two novel techniques for semantically consistent style transfer using diffusion models: Class-wise Adaptive Instance Normalization and Cross-Attention (CACTI) and its extension with selective attention Filtering (CACTIF). CACTI applies statistical normalization selectively based on semantic classes, while CACTIF further filters cross-attention maps based on feature similarity, preventing artifacts in regions with weak cross-attention correspondences. Our methods transfer style characteristics while preserving semantic boundaries and structural coherence, unlike approaches that apply global transformations or generate content without constraints. Experiments using GTA5 as source and Cityscapes/ACDC as target domains show that our approach produces higher quality images with lower FID scores and better content preservation. Our work demonstrates that class-aware diffusion-based style transfer effectively bridges the synthetic-to-real domain gap even with minimal target domain data, advancing robust perception systems for challenging real-world applications. The source code is available at: https://github.com/echigot/cactif.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaTH Attention: Position Encoding via Accumulating Householder Transformations</title>
<link>https://arxiv.org/abs/2505.16381</link>
<guid>https://arxiv.org/abs/2505.16381</guid>
<content:encoded><![CDATA[
arXiv:2505.16381v1 Announce Type: cross 
Abstract: The attention mechanism is a core primitive in modern large language models (LLMs) and AI more broadly. Since attention by itself is permutation-invariant, position encoding is essential for modeling structured domains such as language. Rotary position encoding (RoPE) has emerged as the de facto standard approach for position encoding and is part of many modern LLMs. However, in RoPE the key/query transformation between two elements in a sequence is only a function of their relative position and otherwise independent of the actual input. This limits the expressivity of RoPE-based transformers.
  This paper describes PaTH, a flexible data-dependent position encoding scheme based on accumulated products of Householder(like) transformations, where each transformation is data-dependent, i.e., a function of the input. We derive an efficient parallel algorithm for training through exploiting a compact representation of products of Householder matrices, and implement a FlashAttention-style blockwise algorithm that minimizes I/O cost. Across both targeted synthetic benchmarks and moderate-scale real-world language modeling experiments, we find that PaTH demonstrates superior performance compared to RoPE and other recent baselines.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16410</link>
<guid>https://arxiv.org/abs/2505.16410</guid>
<content:encoded><![CDATA[
arXiv:2505.16410v1 Announce Type: cross 
Abstract: Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose a general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. A subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose a two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at https://github.com/dongguanting/Tool-Star.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression</title>
<link>https://arxiv.org/abs/2505.16411</link>
<guid>https://arxiv.org/abs/2505.16411</guid>
<content:encoded><![CDATA[
arXiv:2505.16411v1 Announce Type: cross 
Abstract: Despite their remarkable progress in multimodal understanding tasks, large vision language models (LVLMs) often suffer from "hallucinations", generating texts misaligned with the visual context. Existing methods aimed at reducing hallucinations through inference time intervention incur a significant increase in latency. To mitigate this, we present SPIN, a task-agnostic attention-guided head suppression strategy that can be seamlessly integrated during inference, without incurring any significant compute or latency overhead. We investigate whether hallucination in LVLMs can be linked to specific model components. Our analysis suggests that hallucinations can be attributed to a dynamic subset of attention heads in each layer. Leveraging this insight, for each text query token, we selectively suppress attention heads that exhibit low attention to image tokens, keeping the top-K attention heads intact. Extensive evaluations on visual question answering and image description tasks demonstrate the efficacy of SPIN in reducing hallucination scores up to 2.7x while maintaining F1, and improving throughput by 1.8x compared to existing alternatives. Code is available at https://github.com/YUECHE77/SPIN.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.16415</link>
<guid>https://arxiv.org/abs/2505.16415</guid>
<content:encoded><![CDATA[
arXiv:2505.16415v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs) combined with external contexts to enhance the accuracy and reliability of generated responses. However, reliably attributing generated content to specific context segments, context attribution, remains challenging due to the computationally intensive nature of current methods, which often require extensive fine-tuning or human annotation. In this work, we introduce a novel Jensen-Shannon Divergence driven method to Attribute Response to Context (ARC-JSD), enabling efficient and accurate identification of essential context sentences without additional fine-tuning or surrogate modelling. Evaluations on a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using instruction-tuned LLMs in different scales demonstrate superior accuracy and significant computational efficiency improvements compared to the previous surrogate-based method. Furthermore, our mechanistic analysis reveals specific attention heads and multilayer perceptron (MLP) layers responsible for context attribution, providing valuable insights into the internal workings of RAG models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16421</link>
<guid>https://arxiv.org/abs/2505.16421</guid>
<content:encoded><![CDATA[
arXiv:2505.16421v1 Announce Type: cross 
Abstract: While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ranked Entropy Minimization for Continual Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2505.16441</link>
<guid>https://arxiv.org/abs/2505.16441</guid>
<content:encoded><![CDATA[
arXiv:2505.16441v1 Announce Type: cross 
Abstract: Test-time adaptation aims to adapt to realistic environments in an online manner by learning during test time. Entropy minimization has emerged as a principal strategy for test-time adaptation due to its efficiency and adaptability. Nevertheless, it remains underexplored in continual test-time adaptation, where stability is more important. We observe that the entropy minimization method often suffers from model collapse, where the model converges to predicting a single class for all images due to a trivial solution. We propose ranked entropy minimization to mitigate the stability problem of the entropy minimization method and extend its applicability to continuous scenarios. Our approach explicitly structures the prediction difficulty through a progressive masking strategy. Specifically, it gradually aligns the model's probability distributions across different levels of prediction difficulty while preserving the rank order of entropy. The proposed method is extensively evaluated across various benchmarks, demonstrating its effectiveness through empirical results. Our code is available at https://github.com/pilsHan/rem
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI</title>
<link>https://arxiv.org/abs/2505.16452</link>
<guid>https://arxiv.org/abs/2505.16452</guid>
<content:encoded><![CDATA[
arXiv:2505.16452v1 Announce Type: cross 
Abstract: Accurate and efficient quantification of cardiac function is essential for the estimation of prognosis of cardiovascular diseases (CVDs). One of the most commonly used metrics for evaluating cardiac pumping performance is left ventricular ejection fraction (LVEF). However, LVEF can be affected by factors such as inter-observer variability and varying pre-load and after-load conditions, which can reduce its reproducibility. Additionally, cardiac dysfunction may not always manifest as alterations in LVEF, such as in heart failure and cardiotoxicity diseases. An alternative measure that can provide a relatively load-independent quantitative assessment of myocardial contractility is myocardial strain and strain rate. By using LVEF in combination with myocardial strain, it is possible to obtain a thorough description of cardiac function. Automated estimation of LVEF and other volumetric measures from cine-MRI sequences can be achieved through segmentation models, while strain calculation requires the estimation of tissue displacement between sequential frames, which can be accomplished using registration models. These tasks are often performed separately, potentially limiting the assessment of cardiac function. To address this issue, in this study we propose an end-to-end deep learning (DL) model that jointly estimates groupwise (GW) registration and segmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep GW network was trained and validated on a large dataset of 4-chamber view cine-MRI image series of 374 subjects. A quantitative comparison with conventional GW registration using elastix and two DL-based methods showed that the proposed model improved performance and substantially reduced computation time.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer</title>
<link>https://arxiv.org/abs/2505.16463</link>
<guid>https://arxiv.org/abs/2505.16463</guid>
<content:encoded><![CDATA[
arXiv:2505.16463v1 Announce Type: cross 
Abstract: Recently, vision transformers (ViTs) have achieved excellent performance on vision tasks by measuring the global self-attention among the image patches. Given $n$ patches, they will have quadratic complexity such as $\mathcal{O}(n^2)$ and the time cost is high when splitting the input image with a small granularity. Meanwhile, the pivotal information is often randomly gathered in a few regions of an input image, some tokens may not be helpful for the downstream tasks. To handle this problem, we introduce an anchor-based efficient vision transformer (AnchorFormer), which employs the anchor tokens to learn the pivotal information and accelerate the inference. Firstly, by estimating the bipartite attention between the anchors and tokens, the complexity will be reduced from $\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$, where $m$ is an anchor number and $m < n$. Notably, by representing the anchors with the neurons in a neural layer, we can differentiable learn these distributions and approximate global self-attention through the Markov process. Moreover, we extend the proposed model to three downstream tasks including classification, detection, and segmentation. Extensive experiments show the effectiveness of our AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs reduction on ImageNet classification, 81.3% higher mAP on COCO detection under comparable FLOPs, as compared to the current baselines.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Supported Dynamic Algorithm Configuration for Multi-Objective Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2505.16471</link>
<guid>https://arxiv.org/abs/2505.16471</guid>
<content:encoded><![CDATA[
arXiv:2505.16471v1 Announce Type: cross 
Abstract: Deep reinforcement learning (DRL) has been widely used for dynamic algorithm configuration, particularly in evolutionary computation, which benefits from the adaptive update of parameters during the algorithmic execution. However, applying DRL to algorithm configuration for multi-objective combinatorial optimization (MOCO) problems remains relatively unexplored. This paper presents a novel graph neural network (GNN) based DRL to configure multi-objective evolutionary algorithms. We model the dynamic algorithm configuration as a Markov decision process, representing the convergence of solutions in the objective space by a graph, with their embeddings learned by a GNN to enhance the state representation. Experiments on diverse MOCO challenges indicate that our method outperforms traditional and DRL-based algorithm configuration methods in terms of efficacy and adaptability. It also exhibits advantageous generalizability across objective types and problem sizes, and applicability to different evolutionary computation methods.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.16524</link>
<guid>https://arxiv.org/abs/2505.16524</guid>
<content:encoded><![CDATA[
arXiv:2505.16524v1 Announce Type: cross 
Abstract: Maintaining robust 3D perception under dynamic and unpredictable test-time conditions remains a critical challenge for autonomous driving systems. Existing test-time adaptation (TTA) methods often fail in high-variance tasks like 3D object detection due to unstable optimization and sharp minima. While recent model merging strategies based on linear mode connectivity (LMC) offer improved stability by interpolating between fine-tuned checkpoints, they are computationally expensive, requiring repeated checkpoint access and multiple forward passes. In this paper, we introduce CodeMerge, a lightweight and scalable model merging framework that bypasses these limitations by operating in a compact latent space. Instead of loading full models, CodeMerge represents each checkpoint with a low-dimensional fingerprint derived from the source model's penultimate features and constructs a key-value codebook. We compute merging coefficients using ridge leverage scores on these fingerprints, enabling efficient model composition without compromising adaptation quality. Our method achieves strong performance across challenging benchmarks, improving end-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by over 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as online mapping, motion prediction and planning even without training. Code and pretrained models are released in the supplementary material.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Object Captioning for Street Scene Videos from LiDAR Tracks</title>
<link>https://arxiv.org/abs/2505.16594</link>
<guid>https://arxiv.org/abs/2505.16594</guid>
<content:encoded><![CDATA[
arXiv:2505.16594v1 Announce Type: cross 
Abstract: Video captioning models have seen notable advancements in recent years, especially with regard to their ability to capture temporal information. While many research efforts have focused on architectural advancements, such as temporal attention mechanisms, there remains a notable gap in understanding how models capture and utilize temporal semantics for effective temporal feature extraction, especially in the context of Advanced Driver Assistance Systems. We propose an automated LiDAR-based captioning procedure that focuses on the temporal dynamics of traffic participants. Our approach uses a rule-based system to extract essential details such as lane position and relative motion from object tracks, followed by a template-based caption generation. Our findings show that training SwinBERT, a video captioning model, using only front camera images and supervised with our template-based captions, specifically designed to encapsulate fine-grained temporal behavior, leads to improved temporal understanding consistently across three datasets. In conclusion, our results clearly demonstrate that integrating LiDAR-based caption supervision significantly enhances temporal understanding, effectively addressing and reducing the inherent visual/static biases prevalent in current state-of-the-art model architectures.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Large Language Models for Machine Translation Personalization</title>
<link>https://arxiv.org/abs/2505.16612</link>
<guid>https://arxiv.org/abs/2505.16612</guid>
<content:encoded><![CDATA[
arXiv:2505.16612v1 Announce Type: cross 
Abstract: High-quality machine translation systems based on large language models (LLMs) have simplified the production of personalized translations reflecting specific stylistic constraints. However, these systems still struggle in settings where stylistic requirements are less explicit and might be harder to convey via prompting. We explore various strategies for personalizing LLM-generated translations in low-resource settings, focusing on the challenging literary translation domain. We explore prompting strategies and inference-time interventions for steering model generations towards a personalized style, and propose a contrastive framework exploiting latent concepts extracted from sparse autoencoders to identify salient personalization properties. Our results show that steering achieves strong personalization while preserving translation quality. We further examine the impact of steering on LLM representations, finding model layers with a relevant impact for personalization are impacted similarly by multi-shot prompting and our steering method, suggesting similar mechanism at play.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning</title>
<link>https://arxiv.org/abs/2505.16635</link>
<guid>https://arxiv.org/abs/2505.16635</guid>
<content:encoded><![CDATA[
arXiv:2505.16635v1 Announce Type: cross 
Abstract: Tabular data, ubiquitous and rich in informational value, is an increasing focus for deep representation learning, yet progress is hindered by studies centered on single tables or isolated databases, which limits model capabilities due to data scale. While collaborative learning approaches such as federated learning, transfer learning, split learning, and tabular foundation models aim to learn from multiple correlated databases, they are challenged by a scarcity of real-world interconnected tabular resources. Current data lakes and corpora largely consist of isolated databases lacking defined inter-database correlations. To overcome this, we introduce WikiDBGraph, a large-scale graph of 100,000 real-world tabular databases from WikiData, interconnected by 17 million edges and characterized by 13 node and 12 edge properties derived from its database schema and data distribution. WikiDBGraph's weighted edges identify both instance- and feature-overlapped databases. Experiments on these newly identified databases confirm that collaborative learning yields superior performance, thereby offering considerable promise for structured foundation model training while also exposing key challenges and future directions for learning from interconnected tabular data.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation</title>
<link>https://arxiv.org/abs/2505.16637</link>
<guid>https://arxiv.org/abs/2505.16637</guid>
<content:encoded><![CDATA[
arXiv:2505.16637v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently demonstrated remarkable capabilities in machine translation (MT). However, most advanced MT-specific LLMs heavily rely on external supervision signals during training, such as human-annotated reference data or trained reward models (RMs), which are often expensive to obtain and challenging to scale. To overcome this limitation, we propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for MT that is reference-free, fully online, and relies solely on self-judging rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs, e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR with external supervision from COMET, our strongest model, SSR-X-Zero-7B, achieves state-of-the-art performance in English $\leftrightarrow$ Chinese translation, surpassing all existing open-source models under 72B parameters and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro. Our analysis highlights the effectiveness of the self-rewarding mechanism compared to the external LLM-as-a-judge approach in MT and demonstrates its complementary benefits when combined with trained RMs. Our findings provide valuable insight into the potential of self-improving RL methods. We have publicly released our code, data and models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning non-equilibrium diffusions with Schr\"odinger bridges: from exactly solvable to simulation-free</title>
<link>https://arxiv.org/abs/2505.16644</link>
<guid>https://arxiv.org/abs/2505.16644</guid>
<content:encoded><![CDATA[
arXiv:2505.16644v1 Announce Type: cross 
Abstract: We consider the Schr\"odinger bridge problem which, given ensemble measurements of the initial and final configurations of a stochastic dynamical system and some prior knowledge on the dynamics, aims to reconstruct the "most likely" evolution of the system compatible with the data. Most existing literature assume Brownian reference dynamics and are implicitly limited to potential-driven dynamics. We depart from this regime and consider reference processes described by a multivariate Ornstein-Uhlenbeck process with generic drift matrix $\mathbf{A} \in \mathbb{R}^{d \times d}$. When $\mathbf{A}$ is asymmetric, this corresponds to a non-equilibrium system with non-conservative forces at play: this is important for applications to biological systems, which are naturally exist out-of-equilibrium. In the case of Gaussian marginals, we derive explicit expressions that characterise the solution of both the static and dynamic Schr\"odinger bridge. For general marginals, we propose mvOU-OTFM, a simulation-free algorithm based on flow and score matching for learning the Schr\"odinger bridge. In application to a range of problems based on synthetic and real single cell data, we demonstrate that mvOU-OTFM achieves higher accuracy compared to competing methods, whilst being significantly faster to train.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding</title>
<link>https://arxiv.org/abs/2505.16652</link>
<guid>https://arxiv.org/abs/2505.16652</guid>
<content:encoded><![CDATA[
arXiv:2505.16652v1 Announce Type: cross 
Abstract: Recent advancements in multimodal large language models (MLLMs) have significantly improved performance in visual question answering. However, they often suffer from hallucinations. In this work, hallucinations are categorized into two main types: initial hallucinations and snowball hallucinations. We argue that adequate contextual information can be extracted directly from the token interaction process. Inspired by causal inference in the decoding strategy, we propose to leverage causal masks to establish information propagation between multimodal tokens. The hypothesis is that insufficient interaction between those tokens may lead the model to rely on outlier tokens, overlooking dense and rich contextual cues. Therefore, we propose to intervene in the propagation process by tackling outlier tokens to enhance in-context inference. With this goal, we present FarSight, a versatile plug-and-play decoding strategy to reduce attention interference from outlier tokens merely by optimizing the causal mask. The heart of our method is effective token propagation. We design an attention register structure within the upper triangular matrix of the causal mask, dynamically allocating attention to capture attention diverted to outlier tokens. Moreover, a positional awareness encoding method with a diminishing masking rate is proposed, allowing the model to attend to further preceding tokens, especially for video sequence tasks. With extensive experiments, FarSight demonstrates significant hallucination-mitigating performance across different MLLMs on both image and video benchmarks, proving its effectiveness.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharp concentration of uniform generalization errors in binary linear classification</title>
<link>https://arxiv.org/abs/2505.16713</link>
<guid>https://arxiv.org/abs/2505.16713</guid>
<content:encoded><![CDATA[
arXiv:2505.16713v1 Announce Type: cross 
Abstract: We examine the concentration of uniform generalization errors around their expectation in binary linear classification problems via an isoperimetric argument. In particular, we establish Poincar\'{e} and log-Sobolev inequalities for the joint distribution of the output labels and the label-weighted input vectors, which we apply to derive concentration bounds. The derived concentration bounds are sharp up to moderate multiplicative constants by those under well-balanced labels. In asymptotic analysis, we also show that almost sure convergence of uniform generalization errors to their expectation occurs in very broad settings, such as proportionally high-dimensional regimes. Using this convergence, we establish uniform laws of large numbers under dimension-free conditions.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experimental robustness benchmark of quantum neural network on a superconducting quantum processor</title>
<link>https://arxiv.org/abs/2505.16714</link>
<guid>https://arxiv.org/abs/2505.16714</guid>
<content:encoded><![CDATA[
arXiv:2505.16714v1 Announce Type: cross 
Abstract: Quantum machine learning (QML) models, like their classical counterparts, are vulnerable to adversarial attacks, hindering their secure deployment. Here, we report the first systematic experimental robustness benchmark for 20-qubit quantum neural network (QNN) classifiers executed on a superconducting processor. Our benchmarking framework features an efficient adversarial attack algorithm designed for QNNs, enabling quantitative characterization of adversarial robustness and robustness bounds. From our analysis, we verify that adversarial training reduces sensitivity to targeted perturbations by regularizing input gradients, significantly enhancing QNN's robustness. Additionally, our analysis reveals that QNNs exhibit superior adversarial robustness compared to classical neural networks, an advantage attributed to inherent quantum noise. Furthermore, the empirical upper bound extracted from our attack experiments shows a minimal deviation ($3 \times 10^{-3}$) from the theoretical lower bound, providing strong experimental confirmation of the attack's effectiveness and the tightness of fidelity-based robustness bounds. This work establishes a critical experimental framework for assessing and improving quantum adversarial robustness, paving the way for secure and reliable QML applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Computational Complexity of Counting Linear Regions in ReLU Neural Networks</title>
<link>https://arxiv.org/abs/2505.16716</link>
<guid>https://arxiv.org/abs/2505.16716</guid>
<content:encoded><![CDATA[
arXiv:2505.16716v1 Announce Type: cross 
Abstract: An established measure of the expressive power of a given ReLU neural network is the number of linear regions into which it partitions the input space. There exist many different, non-equivalent definitions of what a linear region actually is. We systematically assess which papers use which definitions and discuss how they relate to each other. We then analyze the computational complexity of counting the number of such regions for the various definitions. Generally, this turns out to be an intractable problem. We prove NP- and #P-hardness results already for networks with one hidden layer and strong hardness of approximation results for two or more hidden layers. Finally, on the algorithmic side, we demonstrate that counting linear regions can at least be achieved in polynomial space for some common definitions.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust LLM Fingerprinting via Domain-Specific Watermarks</title>
<link>https://arxiv.org/abs/2505.16723</link>
<guid>https://arxiv.org/abs/2505.16723</guid>
<content:encoded><![CDATA[
arXiv:2505.16723v1 Announce Type: cross 
Abstract: As open-source language models (OSMs) grow more capable and are widely shared and finetuned, ensuring model provenance, i.e., identifying the origin of a given model instance, has become an increasingly important issue. At the same time, existing backdoor-based model fingerprinting techniques often fall short of achieving key requirements of real-world model ownership detection. In this work, we build on the observation that while current open-source model watermarks fail to achieve reliable content traceability, they can be effectively adapted to address the challenge of model provenance. To this end, we introduce the concept of domain-specific watermarking for model fingerprinting. Rather than watermarking all generated content, we train the model to embed watermarks only within specified subdomains (e.g., particular languages or topics). This targeted approach ensures detection reliability, while improving watermark durability and quality under a range of real-world deployment settings. Our evaluations show that domain-specific watermarking enables model fingerprinting with strong statistical guarantees, controllable false positive rates, high detection power, and preserved generation quality. Moreover, we find that our fingerprints are inherently stealthy and naturally robust to real-world variability across deployment scenarios.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning</title>
<link>https://arxiv.org/abs/2505.16743</link>
<guid>https://arxiv.org/abs/2505.16743</guid>
<content:encoded><![CDATA[
arXiv:2505.16743v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) present significant computational and memory challenges due to their extensive size, making pruning essential for their efficient deployment. Existing one-shot pruning methods often apply uniform sparsity constraints across layers or within each layer, resulting in suboptimal performance, especially at high sparsity ratios. This work introduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel approach that applies varying sparsity ratios to individual output dimensions (rows) within each layer. TRIM employs an iterative adjustment process guided by quality metrics to optimize dimension-wise sparsity allocation, focusing on reducing variance in quality retention across outputs to preserve critical information. TRIM can be seamlessly integrated with existing layer-wise pruning strategies. Our evaluations on perplexity and zero-shot tasks across diverse LLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that TRIM achieves new state-of-the-art results and enhances stability. For instance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and over 90% for OPT-13B compared to baseline methods. We conclude that fine-grained, dimension-wise sparsity adaptation is crucial for pushing the limits of extreme LLM compression. Code available at: https://github.com/flobk/TRIM
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability</title>
<link>https://arxiv.org/abs/2505.16789</link>
<guid>https://arxiv.org/abs/2505.16789</guid>
<content:encoded><![CDATA[
arXiv:2505.16789v1 Announce Type: cross 
Abstract: As large language models gain popularity, their vulnerability to adversarial attacks remains a primary concern. While fine-tuning models on domain-specific datasets is often employed to improve model performance, it can introduce vulnerabilities within the underlying model. In this work, we investigate Accidental Misalignment, unexpected vulnerabilities arising from characteristics of fine-tuning data. We begin by identifying potential correlation factors such as linguistic features, semantic similarity, and toxicity within our experimental datasets. We then evaluate the adversarial performance of these fine-tuned models and assess how dataset factors correlate with attack success rates. Lastly, we explore potential causal links, offering new insights into adversarial defense strategies and highlighting the crucial role of dataset design in preserving model alignment. Our code is available at https://github.com/psyonp/accidental_misalignment.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols</title>
<link>https://arxiv.org/abs/2505.16821</link>
<guid>https://arxiv.org/abs/2505.16821</guid>
<content:encoded><![CDATA[
arXiv:2505.16821v1 Announce Type: cross 
Abstract: Integrating large AI models (LAMs) into 6G mobile networks promises to redefine protocol design and control-plane intelligence by enabling autonomous, cognitive network operations. While industry concepts, such as ETSI's Experiential Networked Intelligence (ENI), envision LAM-driven agents for adaptive network slicing and intent-based management, practical implementations still face challenges in protocol literacy and real-world deployment. This paper presents an end-to-end demonstration of a LAM that generates standards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as part of control-plane procedures inside a gNB. We treat RRC messaging as a domain-specific language and fine-tune a decoder-only transformer model (LLaMA class) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages linearized to retain their ASN.1 syntactic structure before standard byte-pair encoding tokenization. This enables combinatorial generalization over RRC protocol states while minimizing training overhead. On 30k field-test request-response pairs, our 8 B model achieves a median cosine similarity of 0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a zero-shot LLaMA-3 8B baseline -- indicating substantially improved structural and semantic RRC fidelity. Overall, our results show that LAMs, when augmented with Radio Access Network (RAN)-specific reasoning, can directly orchestrate control-plane procedures, representing a stepping stone toward the AI-native air-interface paradigm. Beyond RRC emulation, this work lays the groundwork for future AI-native wireless standards.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs</title>
<link>https://arxiv.org/abs/2505.16831</link>
<guid>https://arxiv.org/abs/2505.16831</guid>
<content:encoded><![CDATA[
arXiv:2505.16831v1 Announce Type: cross 
Abstract: Unlearning in large language models (LLMs) is intended to remove the influence of specific data, yet current evaluations rely heavily on token-level metrics such as accuracy and perplexity. We show that these metrics can be misleading: models often appear to forget, but their original behavior can be rapidly restored with minimal fine-tuning, revealing that unlearning may obscure information rather than erase it. To diagnose this phenomenon, we introduce a representation-level evaluation framework using PCA-based similarity and shift, centered kernel alignment, and Fisher information. Applying this toolkit across six unlearning methods, three domains (text, code, math), and two open-source LLMs, we uncover a critical distinction between reversible and irreversible forgetting. In reversible cases, models suffer token-level collapse yet retain latent features; in irreversible cases, deeper representational damage occurs. We further provide a theoretical account linking shallow weight perturbations near output layers to misleading unlearning signals, and show that reversibility is modulated by task type and hyperparameters. Our findings reveal a fundamental gap in current evaluation practices and establish a new diagnostic foundation for trustworthy unlearning in LLMs. We provide a unified toolkit for analyzing LLM representation changes under unlearning and relearning: https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization</title>
<link>https://arxiv.org/abs/2505.16832</link>
<guid>https://arxiv.org/abs/2505.16832</guid>
<content:encoded><![CDATA[
arXiv:2505.16832v1 Announce Type: cross 
Abstract: While foundation models (FMs), such as diffusion models and large vision-language models (LVLMs), have been widely applied in educational contexts, their ability to generate pedagogically effective visual explanations remains limited. Most existing approaches focus primarily on textual reasoning, overlooking the critical role of structured and interpretable visualizations in supporting conceptual understanding. To better assess the visual reasoning capabilities of FMs in educational settings, we introduce EduVisBench, a multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem sets requiring visually grounded solutions, along with a fine-grained evaluation rubric informed by pedagogical theory. Our empirical analysis reveals that existing models frequently struggle with the inherent challenge of decomposing complex reasoning and translating it into visual representations aligned with human cognitive processes. To address these limitations, we propose EduVisAgent, a multi-agent collaborative framework that coordinates specialized agents for instructional planning, reasoning decomposition, metacognitive prompting, and visualization design. Experimental results show that EduVisAgent substantially outperforms all baselines, achieving a 40.2% improvement and delivering more educationally aligned visualizations. EduVisBench and EduVisAgent are available at https://github.com/aiming-lab/EduVisBench and https://github.com/aiming-lab/EduVisAgent.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2I-ConBench: Text-to-Image Benchmark for Continual Post-training</title>
<link>https://arxiv.org/abs/2505.16875</link>
<guid>https://arxiv.org/abs/2505.16875</guid>
<content:encoded><![CDATA[
arXiv:2505.16875v1 Announce Type: cross 
Abstract: Continual post-training adapts a single text-to-image diffusion model to learn new tasks without incurring the cost of separate models, but naive post-training causes forgetting of pretrained knowledge and undermines zero-shot compositionality. We observe that the absence of a standardized evaluation protocol hampers related research for continual post-training. To address this, we introduce T2I-ConBench, a unified benchmark for continual post-training of text-to-image models. T2I-ConBench focuses on two practical scenarios, item customization and domain enhancement, and analyzes four dimensions: (1) retention of generality, (2) target-task performance, (3) catastrophic forgetting, and (4) cross-task generalization. It combines automated metrics, human-preference modeling, and vision-language QA for comprehensive assessment. We benchmark ten representative methods across three realistic task sequences and find that no approach excels on all fronts. Even joint "oracle" training does not succeed for every task, and cross-task generalization remains unsolved. We release all datasets, code, and evaluation tools to accelerate research in continual post-training for text-to-image models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How high is `high'? Rethinking the roles of dimensionality in topological data analysis and manifold learning</title>
<link>https://arxiv.org/abs/2505.16879</link>
<guid>https://arxiv.org/abs/2505.16879</guid>
<content:encoded><![CDATA[
arXiv:2505.16879v1 Announce Type: cross 
Abstract: We present a generalised Hanson-Wright inequality and use it to establish new statistical insights into the geometry of data point-clouds. In the setting of a general random function model of data, we clarify the roles played by three notions of dimensionality: ambient intrinsic dimension $p_{\mathrm{int}}$, which measures total variability across orthogonal feature directions; correlation rank, which measures functional complexity across samples; and latent intrinsic dimension, which is the dimension of manifold structure hidden in data. Our analysis shows that in order for persistence diagrams to reveal latent homology and for manifold structure to emerge it is sufficient that $p_{\mathrm{int}}\gg \log n$, where $n$ is the sample size. Informed by these theoretical perspectives, we revisit the ground-breaking neuroscience discovery of toroidal structure in grid-cell activity made by Gardner et al. (Nature, 2022): our findings reveal, for the first time, evidence that this structure is in fact isometric to physical space, meaning that grid cell activity conveys a geometrically faithful representation of the real world.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't "Overthink" Passage Reranking: Is Reasoning Truly Necessary?</title>
<link>https://arxiv.org/abs/2505.16886</link>
<guid>https://arxiv.org/abs/2505.16886</guid>
<content:encoded><![CDATA[
arXiv:2505.16886v1 Announce Type: cross 
Abstract: With the growing success of reasoning models across complex natural language tasks, researchers in the Information Retrieval (IR) community have begun exploring how similar reasoning capabilities can be integrated into passage rerankers built on Large Language Models (LLMs). These methods typically employ an LLM to produce an explicit, step-by-step reasoning process before arriving at a final relevance prediction. But, does reasoning actually improve reranking accuracy? In this paper, we dive deeper into this question, studying the impact of the reasoning process by comparing reasoning-based pointwise rerankers (ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under identical training conditions, and observe that StandardRR generally outperforms ReasonRR. Building on this observation, we then study the importance of reasoning to ReasonRR by disabling its reasoning process (ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more effective than ReasonRR. Examining the cause of this result, our findings reveal that reasoning-based rerankers are limited by the LLM's reasoning process, which pushes it toward polarized relevance scores and thus fails to consider the partial relevance of passages, a key factor for the accuracy of pointwise rerankers.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Test for Saliency Maps of Graph Neural Networks via Selective Inference</title>
<link>https://arxiv.org/abs/2505.16893</link>
<guid>https://arxiv.org/abs/2505.16893</guid>
<content:encoded><![CDATA[
arXiv:2505.16893v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have gained prominence for their ability to process graph-structured data across various domains. However, interpreting GNN decisions remains a significant challenge, leading to the adoption of saliency maps for identifying influential nodes and edges. Despite their utility, the reliability of GNN saliency maps has been questioned, particularly in terms of their robustness to noise. In this study, we propose a statistical testing framework to rigorously evaluate the significance of saliency maps. Our main contribution lies in addressing the inflation of the Type I error rate caused by double-dipping of data, leveraging the framework of Selective Inference. Our method provides statistically valid $p$-values while controlling the Type I error rate, ensuring that identified salient subgraphs contain meaningful information rather than random artifacts. To demonstrate the effectiveness of our method, we conduct experiments on both synthetic and real-world datasets, showing its effectiveness in assessing the reliability of GNN interpretations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality</title>
<link>https://arxiv.org/abs/2505.16900</link>
<guid>https://arxiv.org/abs/2505.16900</guid>
<content:encoded><![CDATA[
arXiv:2505.16900v1 Announce Type: cross 
Abstract: During the finetuning stage of text generation tasks, standard cross-entropy loss treats all tokens equally. This can lead models to overemphasize high-frequency, low-information tokens, neglecting lower-frequency tokens crucial for specificity and informativeness in generated content. This paper introduces a novel loss function, Power-Law Decay Loss (PDL), specifically designed to optimize the finetuning process for text generation. The core motivation for PDL stems from observations in information theory and linguistics: the informativeness of a token is often inversely proportional to its frequency of occurrence. PDL re-weights the contribution of each token in the standard cross-entropy loss based on its frequency in the training corpus, following a power-law decay. Specifically, the weights for high-frequency tokens are reduced, while low-frequency, information-dense tokens are assigned higher weights. This mechanism guides the model during finetuning to focus more on learning and generating tokens that convey specific and unique information, thereby enhancing the quality, diversity, and informativeness of the generated text. We theoretically elaborate on the motivation and construction of PDL and discuss its potential applications and advantages across various text generation finetuning tasks, such as abstractive summarization, dialogue systems, and style transfer.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks</title>
<link>https://arxiv.org/abs/2505.16901</link>
<guid>https://arxiv.org/abs/2505.16901</guid>
<content:encoded><![CDATA[
arXiv:2505.16901v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TULiP: Test-time Uncertainty Estimation via Linearization and Weight Perturbation</title>
<link>https://arxiv.org/abs/2505.16923</link>
<guid>https://arxiv.org/abs/2505.16923</guid>
<content:encoded><![CDATA[
arXiv:2505.16923v1 Announce Type: cross 
Abstract: A reliable uncertainty estimation method is the foundation of many modern out-of-distribution (OOD) detectors, which are critical for safe deployments of deep learning models in the open world. In this work, we propose TULiP, a theoretically-driven post-hoc uncertainty estimator for OOD detection. Our approach considers a hypothetical perturbation applied to the network before convergence. Based on linearized training dynamics, we bound the effect of such perturbation, resulting in an uncertainty score computable by perturbing model parameters. Ultimately, our approach computes uncertainty from a set of sampled predictions. We visualize our bound on synthetic regression and classification datasets. Furthermore, we demonstrate the effectiveness of TULiP using large-scale OOD detection benchmarks for image classification. Our method exhibits state-of-the-art performance, particularly for near-distribution samples.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Principle Discovery for Language Model Self-Improvement</title>
<link>https://arxiv.org/abs/2505.16927</link>
<guid>https://arxiv.org/abs/2505.16927</guid>
<content:encoded><![CDATA[
arXiv:2505.16927v1 Announce Type: cross 
Abstract: When language model (LM) users aim to improve the quality of its generations, it is crucial to specify concrete behavioral attributes that the model should strive to reflect. However, curating such principles across many domains, even non-exhaustively, requires a labor-intensive annotation process. To automate this process, we propose eliciting these latent attributes guiding model reasoning towards human-preferred responses by explicitly modeling them in a self-correction setting. Our approach mines new principles from the LM itself and compresses the discovered elements to an interpretable set via clustering. Specifically, we employ an approximation of posterior-regularized Monte Carlo Expectation-Maximization to both identify a condensed set of the most effective latent principles and teach the LM to strategically invoke them in order to intrinsically refine its responses. We demonstrate that bootstrapping our algorithm over multiple iterations enables smaller language models (7-8B parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on IFEval. We also show that clustering the principles yields interpretable and diverse model-generated constitutions while retaining model performance. The gains our method achieves highlight the potential of automated, principle-driven post-training recipes toward continual self-improvement.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning</title>
<link>https://arxiv.org/abs/2505.16928</link>
<guid>https://arxiv.org/abs/2505.16928</guid>
<content:encoded><![CDATA[
arXiv:2505.16928v1 Announce Type: cross 
Abstract: We introduce $\infty$-THOR, a new framework for long-horizon embodied tasks that advances long-context understanding in embodied AI. $\infty$-THOR provides: (1) a generation framework for synthesizing scalable, reproducible, and unlimited long-horizon trajectories; (2) a novel embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered clues across extended trajectories test agents' long-context reasoning ability; and (3) a long-horizon dataset and benchmark suite featuring complex tasks that span hundreds of environment steps, each paired with ground-truth action sequences. To enable this capability, we explore architectural adaptations, including interleaved Goal-State-Action modeling, context extension techniques, and Context Parallelism, to equip LLM-based agents for extreme long-context reasoning and interaction. Experimental results and analyses highlight the challenges posed by our benchmark and provide insights into training strategies and model behaviors under long-horizon conditions. Our work provides a foundation for the next generation of embodied AI systems capable of robust, long-term reasoning and planning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical Flow Estimation</title>
<link>https://arxiv.org/abs/2505.16942</link>
<guid>https://arxiv.org/abs/2505.16942</guid>
<content:encoded><![CDATA[
arXiv:2505.16942v1 Announce Type: cross 
Abstract: Recent optical flow estimation methods often employ local cost sampling from a dense all-pairs correlation volume. This results in quadratic computational and memory complexity in the number of pixels. Although an alternative memory-efficient implementation with on-demand cost computation exists, this is slower in practice and therefore prior methods typically process images at reduced resolutions, missing fine-grained details.
  To address this, we propose a more efficient implementation of the all-pairs correlation volume sampling, still matching the exact mathematical operator as defined by RAFT. Our approach outperforms on-demand sampling by up to 90% while maintaining low memory usage, and performs on par with the default implementation with up to 95% lower memory usage. As cost sampling makes up a significant portion of the overall runtime, this can translate to up to 50% savings for the total end-to-end model inference in memory-constrained environments. Our evaluation of existing methods includes an 8K ultra-high-resolution dataset and an additional inference-time modification of the recent SEA-RAFT method. With this, we achieve state-of-the-art results at high resolutions both in accuracy and efficiency.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NY Real Estate Racial Equity Analysis via Applied Machine Learning</title>
<link>https://arxiv.org/abs/2505.16946</link>
<guid>https://arxiv.org/abs/2505.16946</guid>
<content:encoded><![CDATA[
arXiv:2505.16946v1 Announce Type: cross 
Abstract: This study analyzes tract-level real estate ownership patterns in New York State (NYS) and New York City (NYC) to uncover racial disparities. We use an advanced race/ethnicity imputation model (LSTM+Geo with XGBoost filtering, validated at 89.2% accuracy) to compare the predicted racial composition of property owners to the resident population from census data. We examine both a Full Model (statewide) and a Name-Only LSTM Model (NYC) to assess how incorporating geospatial context affects our predictions and disparity estimates. The results reveal significant inequities: White individuals hold a disproportionate share of properties and property value relative to their population, while Black, Hispanic, and Asian communities are underrepresented as property owners. These disparities are most pronounced in minority-majority neighborhoods, where ownership is predominantly White despite a predominantly non-White population. Corporate ownership (LLCs, trusts, etc.) exacerbates these gaps by reducing owner-occupied opportunities in urban minority communities. We provide a breakdown of ownership vs. population by race for majority-White, -Black, -Hispanic, and -Asian tracts, identify those with extreme ownership disparities, and compare patterns in urban, suburban, and rural contexts. The findings underscore persistent racial inequity in property ownership, reflecting broader historical and socio-economic forces, and highlight the importance of data-driven approaches to address these issues.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation</title>
<link>https://arxiv.org/abs/2505.16965</link>
<guid>https://arxiv.org/abs/2505.16965</guid>
<content:encoded><![CDATA[
arXiv:2505.16965v1 Announce Type: cross 
Abstract: Text segmentation based on the semantic meaning of sentences is a fundamental task with broad utility in many downstream applications. In this paper, we propose a graphical model-based unsupervised learning approach, named BP-Seg for efficient text segmentation. Our method not only considers local coherence, capturing the intuition that adjacent sentences are often more related, but also effectively groups sentences that are distant in the text yet semantically similar. This is achieved through belief propagation on the carefully constructed graphical models. Experimental results on both an illustrative example and a dataset with long-form documents demonstrate that our method performs favorably compared to competing approaches.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark</title>
<link>https://arxiv.org/abs/2505.16968</link>
<guid>https://arxiv.org/abs/2505.16968</guid>
<content:encoded><![CDATA[
arXiv:2505.16968v1 Announce Type: cross 
Abstract: We introduce \texttt{CASS}, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA~$\leftrightarrow$~HIP) and assembly-level (Nvidia SASS~$\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the \texttt{CASS} family of domain-specific language models, achieving 95\% source translation accuracy and 37.5\% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85\% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce \texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation. Dataset and benchmark are on \href{https://huggingface.co/datasets/MBZUAI/cass}{\textcolor{blue}{HuggingFace}}, with code at \href{https://github.com/GustavoStahl/CASS}{\textcolor{blue}{GitHub}}.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation</title>
<link>https://arxiv.org/abs/2505.16985</link>
<guid>https://arxiv.org/abs/2505.16985</guid>
<content:encoded><![CDATA[
arXiv:2505.16985v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection and segmentation are crucial for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery. While prior research has primarily focused on unimodal image data, real-world applications are inherently multimodal, requiring the integration of multiple modalities for improved OOD detection. A key challenge is the lack of supervision signals from unknown data, leading to overconfident predictions on OOD samples. To address this challenge, we propose Feature Mixing, an extremely simple and fast method for multimodal outlier synthesis with theoretical support, which can be further optimized to help the model better distinguish between in-distribution (ID) and OOD data. Feature Mixing is modality-agnostic and applicable to various modality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal dataset for OOD segmentation, featuring synthetic OOD objects across diverse scenes and weather conditions. Extensive experiments on SemanticKITTI, nuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that Feature Mixing achieves state-of-the-art performance with a $10 \times$ to $370 \times$ speedup. Our source code and dataset will be available at https://github.com/mona4399/FeatureMixing.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Native Segmentation Vision Transformers</title>
<link>https://arxiv.org/abs/2505.16993</link>
<guid>https://arxiv.org/abs/2505.16993</guid>
<content:encoded><![CDATA[
arXiv:2505.16993v1 Announce Type: cross 
Abstract: Uniform downsampling remains the de facto standard for reducing spatial resolution in vision backbones. In this work, we propose an alternative design built around a content-aware spatial grouping layer, that dynamically assigns tokens to a reduced set based on image boundaries and their semantic content. Stacking our grouping layer across consecutive backbone stages results in hierarchical segmentation that arises natively in the feature extraction process, resulting in our coined Native Segmentation Vision Transformer. We show that a careful design of our architecture enables the emergence of strong segmentation masks solely from grouping layers, that is, without additional segmentation-specific heads. This sets the foundation for a new paradigm of native, backbone-level segmentation, which enables strong zero-shot results without mask supervision, as well as a minimal and efficient standalone model design for downstream segmentation tasks. Our project page is https://research.nvidia.com/labs/dvl/projects/native-segmentation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critical Points of Random Neural Networks</title>
<link>https://arxiv.org/abs/2505.17000</link>
<guid>https://arxiv.org/abs/2505.17000</guid>
<content:encoded><![CDATA[
arXiv:2505.17000v1 Announce Type: cross 
Abstract: This work investigates the expected number of critical points of random neural networks with different activation functions as the depth increases in the infinite-width limit. Under suitable regularity conditions, we derive precise asymptotic formulas for the expected number of critical points of fixed index and those exceeding a given threshold. Our analysis reveals three distinct regimes depending on the value of the first derivative of the covariance evaluated at 1: the expected number of critical points may converge, grow polynomially, or grow exponentially with depth. The theoretical predictions are supported by numerical experiments. Moreover, we provide numerical evidence suggesting that, when the regularity condition is not satisfied (e.g. for neural networks with ReLU as activation function), the number of critical points increases as the map resolution increases, indicating a potential divergence in the number of critical points.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sufficient conditions for offline reactivation in recurrent neural networks</title>
<link>https://arxiv.org/abs/2505.17003</link>
<guid>https://arxiv.org/abs/2505.17003</guid>
<content:encoded><![CDATA[
arXiv:2505.17003v1 Announce Type: cross 
Abstract: During periods of quiescence, such as sleep, neural activity in many brain circuits resembles that observed during periods of task engagement. However, the precise conditions under which task-optimized networks can autonomously reactivate the same network states responsible for online behavior is poorly understood. In this study, we develop a mathematical framework that outlines sufficient conditions for the emergence of neural reactivation in circuits that encode features of smoothly varying stimuli. We demonstrate mathematically that noisy recurrent networks optimized to track environmental state variables using change-based sensory information naturally develop denoising dynamics, which, in the absence of input, cause the network to revisit state configurations observed during periods of online activity. We validate our findings using numerical experiments on two canonical neuroscience tasks: spatial position estimation based on self-motion cues, and head direction estimation based on angular velocity cues. Overall, our work provides theoretical support for modeling offline reactivation as an emergent consequence of task optimization in noisy neural circuits.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO</title>
<link>https://arxiv.org/abs/2505.17017</link>
<guid>https://arxiv.org/abs/2505.17017</guid>
<content:encoded><![CDATA[
arXiv:2505.17017v1 Announce Type: cross 
Abstract: Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17022</link>
<guid>https://arxiv.org/abs/2505.17022</guid>
<content:encoded><![CDATA[
arXiv:2505.17022v1 Announce Type: cross 
Abstract: Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, a framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose a dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in a unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at https://github.com/gogoduan/GoT-R1.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Operator Splitting View of Federated Learning</title>
<link>https://arxiv.org/abs/2108.05974</link>
<guid>https://arxiv.org/abs/2108.05974</guid>
<content:encoded><![CDATA[
arXiv:2108.05974v4 Announce Type: replace 
Abstract: Over the past few years, the federated learning ($\texttt{FL}$) community has witnessed a proliferation of new $\texttt{FL}$ algorithms. However, our understating of the theory of $\texttt{FL}$ is still fragmented, and a thorough, formal comparison of these algorithms remains elusive. Motivated by this gap, we show that many of the existing $\texttt{FL}$ algorithms can be understood from an operator splitting point of view. This unification allows us to compare different algorithms with ease, to refine previous convergence results and to uncover new algorithmic variants. In particular, our analysis reveals the vital role played by the step size in $\texttt{FL}$ algorithms. The unification also leads to a streamlined and economic way to accelerate $\texttt{FL}$ algorithms, without incurring any communication overhead. We perform numerical experiments on both convex and nonconvex models to validate our findings.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient Federated Learning With Data and Client Heterogeneity</title>
<link>https://arxiv.org/abs/2206.10032</link>
<guid>https://arxiv.org/abs/2206.10032</guid>
<content:encoded><![CDATA[
arXiv:2206.10032v4 Announce Type: replace 
Abstract: Federated Learning (FL) enables large-scale distributed training of machine learning models, while still allowing individual nodes to maintain data locally. However, executing FL at scale comes with inherent practical challenges: 1) heterogeneity of the local node data distributions, 2) heterogeneity of node computational speeds (asynchrony), but also 3) constraints in the amount of communication between the clients and the server. In this work, we present the first variant of the classic federated averaging (FedAvg) algorithm which, at the same time, supports data heterogeneity, partial client asynchrony, and communication compression. Our algorithm comes with a novel, rigorous analysis showing that, in spite of these system relaxations, it can provide similar convergence to FedAvg in interesting parameter regimes. Experimental results in the rigorous LEAF benchmark on setups of up to 300 nodes show that our algorithm ensures fast convergence for standard federated tasks, improving upon prior quantized and asynchronous approaches.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization</title>
<link>https://arxiv.org/abs/2305.04971</link>
<guid>https://arxiv.org/abs/2305.04971</guid>
<content:encoded><![CDATA[
arXiv:2305.04971v2 Announce Type: replace 
Abstract: Regularization techniques are crucial to improving the generalization performance and training efficiency of deep neural networks. Many deep learning algorithms rely on weight decay, dropout, batch/layer normalization to converge faster and generalize. Label Smoothing (LS) is another simple, versatile and efficient regularization which can be applied to various supervised classification tasks. Conventional LS, however, regardless of the training instance assumes that each non-target class is equally likely. In this work, we present a general framework for training with label regularization, which includes conventional LS but can also model instance-specific variants. Based on this formulation, we propose an efficient way of learning LAbel regularization by devising a Bi-level Optimization (LABO) problem. We derive a deterministic and interpretable solution of the inner loop as the optimal label smoothing without the need to store the parameters or the output of a trained model. Finally, we conduct extensive experiments and demonstrate our LABO consistently yields improvement over conventional label regularization on various fields, including seven machine translation and three image classification tasks across various
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressing Neural Networks Using Tensor Networks with Exponentially Fewer Variational Parameters</title>
<link>https://arxiv.org/abs/2305.06058</link>
<guid>https://arxiv.org/abs/2305.06058</guid>
<content:encoded><![CDATA[
arXiv:2305.06058v3 Announce Type: replace 
Abstract: Neural network (NN) designed for challenging machine learning tasks is in general a highly nonlinear mapping that contains massive variational parameters. High complexity of NN, if unbounded or unconstrained, might unpredictably cause severe issues including \R{overfitting}, loss of generalization power, and unbearable cost of hardware. In this work, we propose a general compression scheme that significantly reduces the variational parameters of NN's, despite of their specific types (linear, convolutional, \textit{etc}), by encoding them to deep \R{automatically differentiable} tensor network (ADTN) that contains exponentially-fewer free parameters. Superior compression performance of our scheme is demonstrated on several widely-recognized NN's (FC-2, LeNet-5, AlextNet, ZFNet and VGG-16) and datasets (MNIST, CIFAR-10 and CIFAR-100). For instance, we compress two linear layers in VGG-16 with approximately $10^{7}$ parameters to two ADTN's with just 424 parameters, improving the testing accuracy on CIFAR-10 from $90.17\%$ to $91.74\%$. We argue that the deep structure of ADTN is an essential reason for the remarkable compression performance of ADTN, compared to existing compression schemes that are mainly based on tensor decompositions/factorization and shallow tensor networks. Our work suggests deep TN as an exceptionally efficient mathematical structure for representing the variational parameters of NN's, which exhibits superior compressibility over the commonly-used matrices and multi-way arrays.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Control of Nonlinear Systems with Unknown Dynamics</title>
<link>https://arxiv.org/abs/2305.15188</link>
<guid>https://arxiv.org/abs/2305.15188</guid>
<content:encoded><![CDATA[
arXiv:2305.15188v2 Announce Type: replace 
Abstract: This paper presents a data-driven method for finding a closed-loop optimal controller, which minimizes a specified infinite-horizon cost function for systems with unknown dynamics given any arbitrary initial state. Suppose the closed-loop optimal controller can be parameterized by a given class of functions, hereafter referred to as the policy. The proposed method introduces a novel gradient estimation framework, which approximates the gradient of the cost function with respect to the policy parameters via integrating the Koopman operator with the classical concept of actor-critic. This enables the policy parameters to be tuned iteratively using gradient descent to achieve an optimal controller, leveraging the linearity of the Koopman operator. The convergence analysis of the proposed framework is provided. The effectiveness of the method is demonstrated through comparisons with a model-free reinforcement learning approach, and its control performance is further evaluated through simulations against model-based optimal control methods that solve the same optimal control problem utilizing the exact system dynamics.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Effects of Fairness Interventions Using Pointwise Representational Similarity</title>
<link>https://arxiv.org/abs/2305.19294</link>
<guid>https://arxiv.org/abs/2305.19294</guid>
<content:encoded><![CDATA[
arXiv:2305.19294v2 Announce Type: replace 
Abstract: Machine learning (ML) algorithms can often exhibit discriminatory behavior, negatively affecting certain populations across protected groups. To address this, numerous debiasing methods, and consequently evaluation measures, have been proposed. Current evaluation measures for debiasing methods suffer from two main limitations: (1) they primarily provide a global estimate of unfairness, failing to provide a more fine-grained analysis, and (2) they predominantly analyze the model output on a specific task, failing to generalize the findings to other tasks. In this work, we introduce Pointwise Normalized Kernel Alignment (PNKA), a pointwise representational similarity measure that addresses these limitations by measuring how debiasing measures affect the intermediate representations of individuals. On tabular data, the use of PNKA reveals previously unknown insights: while group fairness predominantly influences a small subset of the population, maintaining high representational similarity for the majority, individual fairness constraints uniformly impact representations across the entire population, altering nearly every data point. We show that by evaluating representations using PNKA, we can reliably predict the behavior of ML models trained on these representations. Moreover, applying PNKA to language embeddings shows that existing debiasing methods may not perform as intended, failing to remove biases from stereotypical words and sentences. Our findings suggest that current evaluation measures for debiasing methods are insufficient, highlighting the need for a deeper understanding of the effects of debiasing methods, and show how pointwise representational similarity metrics can help with fairness audits.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics</title>
<link>https://arxiv.org/abs/2306.01271</link>
<guid>https://arxiv.org/abs/2306.01271</guid>
<content:encoded><![CDATA[
arXiv:2306.01271v4 Announce Type: replace 
Abstract: Similar to surprising performance in the standard deep learning, deep nets trained by adversarial training also generalize well for unseen clean data (natural data). However, despite adversarial training can achieve low robust training error, there exists a significant robust generalization gap. We call this phenomenon the Clean Generalization and Robust Overfitting (CGRO). In this work, we study the CGRO phenomenon in adversarial training from two views: representation complexity and training dynamics. Specifically, we consider a binary classification setting with $N$ separated training data points. First, we prove that, based on the assumption that we assume there is $\operatorname{poly}(D)$-size clean classifier (where $D$ is the data dimension), ReLU net with only $O(N D)$ extra parameters is able to leverages robust memorization to achieve the CGRO, while robust classifier still requires exponential representation complexity in worst case. Next, we focus on a structured-data case to analyze training dynamics, where we train a two-layer convolutional network with $O(N D)$ width against adversarial perturbation. We then show that a three-stage phase transition occurs during learning process and the network provably converges to robust memorization regime, which thereby results in the CGRO. Besides, we also empirically verify our theoretical analysis by experiments in real-image recognition datasets.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination</title>
<link>https://arxiv.org/abs/2311.02960</link>
<guid>https://arxiv.org/abs/2311.02960</guid>
<content:encoded><![CDATA[
arXiv:2311.02960v3 Announce Type: replace 
Abstract: Over the past decade, deep learning has proven to be a highly effective tool for learning meaningful features from raw data. However, it remains an open question how deep networks perform hierarchical feature learning across layers. In this work, we attempt to unveil this mystery by investigating the structures of intermediate features. Motivated by our empirical findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems. Toward this goal, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is nearly orthogonal and the network weights are minimum-norm, balanced, and approximate low-rank: Each layer of the linear network progressively compresses within-class features at a geometric rate and discriminates between-class features at a linear rate with respect to the number of layers that data have passed through. To the best of our knowledge, this is the first quantitative characterization of feature evolution in hierarchical representations of deep linear networks. Empirically, our extensive experiments not only validate our theoretical results numerically but also reveal a similar pattern in deep nonlinear networks which aligns well with recent empirical studies. Moreover, we demonstrate the practical implications of our results in transfer learning. Our code is available at https://github.com/Heimine/PNC_DLN.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining SNNs with Filtering for Efficient Neural Decoding in Implantable Brain-Machine Interfaces</title>
<link>https://arxiv.org/abs/2312.15889</link>
<guid>https://arxiv.org/abs/2312.15889</guid>
<content:encoded><![CDATA[
arXiv:2312.15889v2 Announce Type: replace 
Abstract: While it is important to make implantable brain-machine interfaces (iBMI) wireless to increase patient comfort and safety, the trend of increased channel count in recent neural probes poses a challenge due to the concomitant increase in the data rate. Extracting information from raw data at the source by using edge computing is a promising solution to this problem, with integrated intention decoders providing the best compression ratio. Recent benchmarking efforts have shown recurrent neural networks to be the best solution. Spiking Neural Networks (SNN) emerge as a promising solution for resource efficient neural decoding while Long Short Term Memory (LSTM) networks achieve the best accuracy. In this work, we show that combining traditional signal processing techniques, namely signal filtering, with SNNs improve their decoding performance significantly for regression tasks, closing the gap with LSTMs, at little added cost. Results with different filters are shown with Bessel filters providing best performance. Two block-bidirectional Bessel filters have been used--one for low latency and another for high accuracy. Adding the high accuracy variant of the Bessel filters to the output of ANN, SNN and variants provided statistically significant benefits with maximum gains of $\approx 5\%$ and $8\%$ in $R^2$ for two SNN topologies (SNN\_Streaming and SNN\_3D). Our work presents state of the art results for this dataset and paves the way for decoder-integrated-implants of the future.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Variational Diffusion Models</title>
<link>https://arxiv.org/abs/2401.06281</link>
<guid>https://arxiv.org/abs/2401.06281</guid>
<content:encoded><![CDATA[
arXiv:2401.06281v2 Announce Type: replace 
Abstract: Despite the growing interest in diffusion models, gaining a deep understanding of the model class remains an elusive endeavour, particularly for the uninitiated in non-equilibrium statistical physics. Thanks to the rapid rate of progress in the field, most existing work on diffusion models focuses on either applications or theoretical contributions. Unfortunately, the theoretical material is often inaccessible to practitioners and new researchers, leading to a risk of superficial understanding in ongoing research. Given that diffusion models are now an indispensable tool, a clear and consolidating perspective on the model class is needed to properly contextualize recent advances in generative modelling and lower the barrier to entry for new researchers. To that end, we revisit predecessors to diffusion models like hierarchical latent variable models and synthesize a holistic perspective using only directed graphical modelling and variational inference principles. The resulting narrative is easier to follow as it imposes fewer prerequisites on the average reader relative to the view from non-equilibrium thermodynamics or stochastic differential equations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization</title>
<link>https://arxiv.org/abs/2403.12474</link>
<guid>https://arxiv.org/abs/2403.12474</guid>
<content:encoded><![CDATA[
arXiv:2403.12474v2 Announce Type: replace 
Abstract: Despite the remarkable success of graph neural networks (GNNs) in modeling graph-structured data, like other machine learning models, GNNs are also susceptible to making biased predictions based on sensitive attributes, such as race and gender. For fairness consideration, recent state-of-the-art (SOTA) methods propose to filter out sensitive information from inputs or representations, e.g., edge dropping or feature masking. However, we argue that such filtering-based strategies may also filter out some non-sensitive feature information, leading to a sub-optimal trade-off between predictive performance and fairness. To address this issue, we unveil an innovative neutralization-based paradigm, where additional Fairness-facilitating Features (F3) are incorporated into node features or representations before message passing. The F3 are expected to statistically neutralize the sensitive bias in node representations and provide additional nonsensitive information. We also provide theoretical explanations for our rationale, concluding that F3 can be realized by emphasizing the features of each node's heterogeneous neighbors (neighbors with different sensitive attributes). We name our method as FairSIN, and present three implementation variants from both data-centric and model-centric perspectives. Experimental results on five benchmark datasets with three different GNN backbones show that FairSIN significantly improves fairness metrics while maintaining high prediction accuracies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Initialisation and Network Effects in Decentralised Federated Learning</title>
<link>https://arxiv.org/abs/2403.15855</link>
<guid>https://arxiv.org/abs/2403.15855</guid>
<content:encoded><![CDATA[
arXiv:2403.15855v4 Announce Type: replace 
Abstract: Fully decentralised federated learning enables collaborative training of individual machine learning models on a distributed network of communicating devices while keeping the training data localised on each node. This approach avoids central coordination, enhances data privacy and eliminates the risk of a single point of failure. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices and the learning models' initial conditions. We propose a strategy for uncoordinated initialisation of the artificial neural networks based on the distribution of eigenvector centralities of the underlying communication network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and the choice of environmental parameters under our proposed initialisation strategy. This work paves the way for more efficient and scalable artificial neural network training in a distributed and uncoordinated environment, offering a deeper understanding of the intertwining roles of network structure and learning dynamics.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A fast algorithm to minimize prediction loss of the optimal solution in inverse optimization problem of MILP</title>
<link>https://arxiv.org/abs/2405.14273</link>
<guid>https://arxiv.org/abs/2405.14273</guid>
<content:encoded><![CDATA[
arXiv:2405.14273v2 Announce Type: replace 
Abstract: We consider the inverse optimization problem of estimating the weights of the objective function such that the given solution is an optimal solution for a mixed integer linear program (MILP). In this inverse optimization problem, the known methods exhibit inefficient convergence. Specifically, if $d$ denotes the dimension of the weights and $k$ the number of iterations, then the error of the weights is bounded by $O(k^{-1/(d-1)})$, leading to slow convergence as $d$ increases.We propose a projected subgradient method with a step size of $k^{-1/2}$ based on suboptimality loss. We theoretically show and demonstrate that the proposed method efficiently learns the weights. In particular, we show that there exists a constant $\gamma > 0$ such that the distance between the learned and true weights is bounded by $ O\left(k^{-1/(1+\gamma)} \exp\left(-\frac{\gamma k^{1/2}}{2+\gamma}\right)\right), $ or the optimal solution is exactly recovered. Furthermore, experiments demonstrate that the proposed method solves the inverse optimization problems of MILP using fewer than $1/7$ the number of MILP calls required by known methods, and converges within a finite number of iterations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities</title>
<link>https://arxiv.org/abs/2407.08112</link>
<guid>https://arxiv.org/abs/2407.08112</guid>
<content:encoded><![CDATA[
arXiv:2407.08112v3 Announce Type: replace 
Abstract: Long sequences occur in abundance within real-world scenarios, hence properly modelling them opens numerous down-stream use-cases. Deep neural networks, however, have often struggled with these for a variety of reasons. Recent advances, both in system engineering as well as model design, have enabled the scaling up of model that are purported to support extended context length. In particular, the state-space and linear recurrent neural network families of models hypothetically can entend to infinite sequence lenth. However, is this too good to be true? We conduct an evaluation to show that while such claims may be sound theoretically, there remain large practical gaps that are empirically observed. In particular, recurrent models still suffer in the same settings as long-context LLMs with attention. We further show that different inductive biases have inconsistent extrapolation capabilities, highlighting the need to further study such paradigms and investigate why long-context models seemingly fail to behave as one might expect.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImPORTance: Machine Learning-Driven Analysis of Global Port Significance and Network Dynamics for Improved Operational Efficiency</title>
<link>https://arxiv.org/abs/2407.09571</link>
<guid>https://arxiv.org/abs/2407.09571</guid>
<content:encoded><![CDATA[
arXiv:2407.09571v3 Announce Type: replace 
Abstract: Seaports play a crucial role in the global economy, and researchers have sought to understand their significance through various studies. In this paper, we aim to explore the common characteristics shared by important ports by analyzing the network of connections formed by vessel movement among them. To accomplish this task, we adopt a bottom-up network construction approach that combines three years' worth of AIS (Automatic Identification System) data from around the world, constructing a Ports Network that represents the connections between different ports. Through this representation, we utilize machine learning to assess the relative significance of various port features. Our model examined such features and revealed that geographical characteristics and the port's depth are indicators of a port's importance to the Ports Network. Accordingly, this study employs a data-driven approach and utilizes machine learning to provide a comprehensive understanding of the factors contributing to the extent of ports. Our work aims to inform decision-making processes related to port development, resource allocation, and infrastructure planning within the industry.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logit Scaling for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2409.01175</link>
<guid>https://arxiv.org/abs/2409.01175</guid>
<content:encoded><![CDATA[
arXiv:2409.01175v2 Announce Type: replace 
Abstract: The safe deployment of machine learning and AI models in open-world settings hinges critically on the ability to detect out-of-distribution (OOD) data accurately, data samples that contrast vastly from what the model was trained with. Current approaches to OOD detection often require further training the model, and/or statistics about the training data which may no longer be accessible. Additionally, many existing OOD detection methods struggle to maintain performance when transferred across different architectures. Our research tackles these issues by proposing a simple, post-hoc method that does not require access to the training data distribution, keeps a trained network intact, and holds strong performance across a variety of architectures. Our method, Logit Scaling (LTS), as the name suggests, simply scales the logits in a manner that effectively distinguishes between in-distribution (ID) and OOD samples. We tested our method on benchmarks across various scales, including CIFAR-10, CIFAR-100, ImageNet and OpenOOD. The experiments cover 3 ID and 14 OOD datasets, as well as 9 model architectures. Overall, we demonstrate state-of-the-art performance, robustness and adaptability across different architectures, paving the way towards a universally applicable solution for advanced OOD detection.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithm Configuration for Structured Pfaffian Settings</title>
<link>https://arxiv.org/abs/2409.04367</link>
<guid>https://arxiv.org/abs/2409.04367</guid>
<content:encoded><![CDATA[
arXiv:2409.04367v4 Announce Type: replace 
Abstract: Data-driven algorithm design automatically adapts algorithms to specific application domains, achieving better performance. In the context of parameterized algorithms, this approach involves tuning the algorithm's hyperparameters using problem instances drawn from the problem distribution of the target application domain. This can be achieved by maximizing empirical utilities that measure the algorithms' performance as a function of their hyperparameters, using problem instances. While empirical evidence supports the effectiveness of data-driven algorithm design, providing theoretical guarantees for several parameterized families remains challenging. This is due to the intricate behaviors of their corresponding utility functions, which typically admit piecewise discontinuous structures. In this work, we present refined frameworks for providing learning guarantees for parameterized data-driven algorithm design problems in both distributional and online learning settings. For the distributional learning setting, we introduce the \textit{Pfaffian GJ framework}, an extension of the classical \textit{GJ framework}, that is capable of providing learning guarantees for function classes for which the computation involves Pfaffian functions. Unlike the GJ framework, which is limited to function classes with computation characterized by rational functions, our proposed framework can deal with function classes involving Pfaffian functions, which are much more general and widely applicable. We then show that for many parameterized algorithms of interest, their utility function possesses a \textit{refined piecewise structure}, which automatically translates to learning guarantees using our proposed framework.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical inference on black-box generative models in the data kernel perspective space</title>
<link>https://arxiv.org/abs/2410.01106</link>
<guid>https://arxiv.org/abs/2410.01106</guid>
<content:encoded><![CDATA[
arXiv:2410.01106v3 Announce Type: replace 
Abstract: Generative models are capable of producing human-expert level content across a variety of topics and domains. As the impact of generative models grows, it is necessary to develop statistical methods to understand collections of available models. These methods are particularly important in settings where the user may not have access to information related to a model's pre-training data, weights, or other relevant model-level covariates. In this paper we extend recent results on representations of black-box generative models to model-level statistical inference tasks. We demonstrate that the model-level representations are effective for multiple inference tasks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Class-Incremental Learning using Sample Weighting</title>
<link>https://arxiv.org/abs/2410.01324</link>
<guid>https://arxiv.org/abs/2410.01324</guid>
<content:encoded><![CDATA[
arXiv:2410.01324v2 Announce Type: replace 
Abstract: Model fairness is becoming important in class-incremental learning for Trustworthy AI. While accuracy has been a central focus in class-incremental learning, fairness has been relatively understudied. However, naively using all the samples of the current task for training results in unfair catastrophic forgetting for certain sensitive groups including classes. We theoretically analyze that forgetting occurs if the average gradient vector of the current task data is in an "opposite direction" compared to the average gradient vector of a sensitive group, which means their inner products are negative. We then propose a fair class-incremental learning framework that adjusts the training weights of current task samples to change the direction of the average gradient vector and thus reduce the forgetting of underperforming groups and achieve fairness. For various group fairness measures, we formulate optimization problems to minimize the overall losses of sensitive groups while minimizing the disparities among them. We also show the problems can be solved with linear programming and propose an efficient Fairness-aware Sample Weighting (FSW) algorithm. Experiments show that FSW achieves better accuracy-fairness tradeoff results than state-of-the-art approaches on real datasets.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permissive Information-Flow Analysis for Large Language Models</title>
<link>https://arxiv.org/abs/2410.03055</link>
<guid>https://arxiv.org/abs/2410.03055</guid>
<content:encoded><![CDATA[
arXiv:2410.03055v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are rapidly becoming commodity components of larger software systems. This poses natural security and privacy problems: poisoned data retrieved from one component can change the model's behavior and compromise the entire system, including coercing the model to spread confidential data to untrusted components. One promising approach is to tackle this problem at the system level via dynamic information flow (aka taint) tracking. Unfortunately, this approach of propagating the most restrictive input label to the output is too conservative for applications where LLMs operate on inputs retrieved from diverse sources. In this paper, we propose a novel, more permissive approach to propagate information flow labels through LLM queries. The key idea behind our approach is to propagate only the labels of the samples that were influential in generating the model output and to eliminate the labels of unnecessary inputs. We implement and investigate the effectiveness of two variations of this approach, based on (i) prompt-based retrieval augmentation, and (ii) a $k$-nearest-neighbors language model. We compare these with a baseline that uses introspection to predict the output label. Our experimental results in an LLM agent setting show that the permissive label propagator improves over the baseline in more than 85% of the cases, which underscores the practicality of our approach.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents</title>
<link>https://arxiv.org/abs/2410.03450</link>
<guid>https://arxiv.org/abs/2410.03450</guid>
<content:encoded><![CDATA[
arXiv:2410.03450v2 Announce Type: replace 
Abstract: MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at hand. To address this issue, we propose a novel method, MLLM As ReTriever (MART), which enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning, such that the retriever fully considers the effectiveness of trajectories and prioritizes them for unseen tasks. We also introduce Trajectory Abstraction, a mechanism that leverages MLLMs' summarization capabilities to represent trajectories with fewer tokens while preserving key information, enabling agents to better comprehend milestones in the trajectory. Experimental results across various environments demonstrate our method significantly improves task success rates in unseen scenes compared to baseline methods. This work presents a new paradigm for multimodal retrieval in embodied agents, by fine-tuning a general-purpose MLLM as the retriever to assess trajectory effectiveness. All the code for benchmark tasks, simulator modifications, and the MLLM retriever is available at https://github.com/PKU-RL/MART.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration Implies Data Augmentation: Reachability and Generalisation in Contextual MDPs</title>
<link>https://arxiv.org/abs/2410.03565</link>
<guid>https://arxiv.org/abs/2410.03565</guid>
<content:encoded><![CDATA[
arXiv:2410.03565v3 Announce Type: replace 
Abstract: In the zero-shot policy transfer (ZSPT) setting for contextual Markov decision processes (MDP), agents train on a fixed set of contexts and must generalise to new ones. Recent work has argued and demonstrated that increased exploration can improve this generalisation, by training on more states in the training contexts. In this paper, we demonstrate that training on more states can indeed improve generalisation, but can come at a cost of reducing the accuracy of the learned value function which should not benefit generalisation. We hypothesise and demonstrate that using exploration to increase the agent's coverage while also increasing the accuracy improves generalisation even more. Inspired by this, we propose a method Explore-Go that implements an exploration phase at the beginning of each episode, which can be combined with existing on- and off-policy RL algorithms and significantly improves generalisation even in partially observable MDPs. We demonstrate the effectiveness of Explore-Go when combined with several popular algorithms and show an increase in generalisation performance across several environments. With this, we hope to provide practitioners with a simple modification that can improve the generalisation of their agents.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Offline Imitation Learning from Diverse Auxiliary Data</title>
<link>https://arxiv.org/abs/2410.03626</link>
<guid>https://arxiv.org/abs/2410.03626</guid>
<content:encoded><![CDATA[
arXiv:2410.03626v3 Announce Type: replace 
Abstract: Offline imitation learning enables learning a policy solely from a set of expert demonstrations, without any environment interaction. To alleviate the issue of distribution shift arising due to the small amount of expert data, recent works incorporate large numbers of auxiliary demonstrations alongside the expert data. However, the performance of these approaches rely on assumptions about the quality and composition of the auxiliary data, and they are rarely successful when those assumptions do not hold. To address this limitation, we propose Robust Offline Imitation from Diverse Auxiliary Data (ROIDA). ROIDA first identifies high-quality transitions from the entire auxiliary dataset using a learned reward function. These high-reward samples are combined with the expert demonstrations for weighted behavioral cloning. For lower-quality samples, ROIDA applies temporal difference learning to steer the policy towards high-reward states, improving long-term returns. This two-pronged approach enables our framework to effectively leverage both high and low-quality data without any assumptions. Extensive experiments validate that ROIDA achieves robust and consistent performance across multiple auxiliary datasets with diverse ratios of expert and non-expert demonstrations. ROIDA effectively leverages unlabeled auxiliary data, outperforming prior methods reliant on specific data assumptions. Our code is available at https://github.com/uditaghosh/roida.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Oriented Time Series Inference Agents for Reasoning and Automated Analysis</title>
<link>https://arxiv.org/abs/2410.04047</link>
<guid>https://arxiv.org/abs/2410.04047</guid>
<content:encoded><![CDATA[
arXiv:2410.04047v4 Announce Type: replace 
Abstract: Real-world time series inference requires more than point forecasting. It demands multi-step reasoning, constraint handling, domain knowledge incorporation, and domain-specific workflow assembly. Existing time series foundation models are limited to narrow tasks and lack flexibility to generalize across diverse scenarios. On the other hand, large language models (LLMs) struggle with numerical precision. To address these limitations, we introduce TS-Reasoner, a Domain-Oriented Time Series Agent that integrates natural language reasoning with precise numerical execution. TS-Reasoner decomposes natural language instructions into structured workflows composed of statistical, logical, and domain-specific operators, and incorporates a self-refinement mechanism for adaptive execution. We evaluate its capabilities through two axes: basic time series understanding and complex multi-step inference, using the TimeSeriesExam benchmark and a newly constructed dataset. Experimental results show that TS-Reasoner significantly outperforms general-purpose LLMs, highlighting the promise of domain-specialized agents for robust and interpretable time series reasoning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Epochal Sawtooth Effect: Unveiling Training Loss Oscillations in Adam and Other Optimizers</title>
<link>https://arxiv.org/abs/2410.10056</link>
<guid>https://arxiv.org/abs/2410.10056</guid>
<content:encoded><![CDATA[
arXiv:2410.10056v2 Announce Type: replace 
Abstract: In this paper, we identify and analyze a recurring training loss pattern, which we term the \textit{Epochal Sawtooth Effect (ESE)}, commonly observed during training with adaptive gradient-based optimizers, particularly Adam optimizer. This pattern is characterized by a sharp drop in loss at the beginning of each epoch, followed by a gradual increase, resulting in a sawtooth-shaped loss curve. Through empirical observations, we demonstrate that while this effect is most pronounced with Adam, it persists, although less severely, with other optimizers such as RMSProp.
  We provide an in-depth explanation of the underlying mechanisms that lead to the Epochal Sawtooth Effect. The influences of factors like $\beta$, batch size, data shuffling on this pattern have been studied. We quantify the influence of $beta_2$ on the shape of the loss curve, showing that higher values of $\beta_2$ result in a nearly linear increase in loss, while lower values create a concave upward trend. Our analysis reveals that this behavior stems from the adaptive learning rate controlled by the second moment estimate, with $\beta_1$ playing a minimal role when $\beta_2$ is large.
  To support our analysis, we replicate this phenomenon through a controlled quadratic minimization task. By incrementally solving a series of quadratic optimization problems using Adam, we demonstrate that the Epochal Sawtooth Effect can emerge even in simple optimization scenarios, reinforcing the generality of this pattern. This paper provides both theoretical insights and quantitative analysis, offering a comprehensive understanding of this ubiquitous phenomenon in modern optimization techniques.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-based Large Language Model Customization as Service</title>
<link>https://arxiv.org/abs/2410.10481</link>
<guid>https://arxiv.org/abs/2410.10481</guid>
<content:encoded><![CDATA[
arXiv:2410.10481v3 Announce Type: replace 
Abstract: Prominent Large Language Model (LLM) services from providers like OpenAI and Google excel at general tasks but often underperform on domain-specific applications. Current customization services for these LLMs typically require users to upload data for fine-tuning, posing significant privacy risks. While differentially private (DP) data synthesis presents a potential alternative, its application commonly results in low effectiveness due to the introduction of excessive noise on data for DP. To overcome this, we introduce Llamdex, a novel framework that facilitates LLM customization as a service, where the client uploads pre-trained domain-specific models rather than data. This client-uploaded model, optionally protected by DP with much lower noise, is inserted into the base LLM via connection modules. Significantly, these connecting modules are trained without requiring sensitive domain data, enabling clients to customize LLM services while preserving data privacy. Experiments demonstrate that Llamdex improves domain-specific accuracy by up to 26\% over state-of-the-art private data synthesis methods under identical privacy constraints and, by obviating the need for users to provide domain context within queries, maintains inference efficiency comparable to the original LLM service.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites</title>
<link>https://arxiv.org/abs/2410.19643</link>
<guid>https://arxiv.org/abs/2410.19643</guid>
<content:encoded><![CDATA[
arXiv:2410.19643v4 Announce Type: replace 
Abstract: Machine learning (ML) models benefit from large datasets. Collecting data in biomedical domains is costly and challenging, hence, combining datasets has become a common practice. However, datasets obtained under different conditions could present undesired site-specific variability. Data harmonization methods aim to remove site-specific variance while retaining biologically relevant information. This study evaluates the effectiveness of popularly used ComBat-based methods for harmonizing data in scenarios where the class balance is not equal across sites. We find that these methods struggle with data leakage issues. To overcome this problem, we propose a novel approach PrettYharmonize, designed to harmonize data by pretending the target labels. We validate our approach using controlled datasets designed to benchmark the utility of harmonization. Finally, using real-world MRI and clinical data, we compare leakage-prone methods with PrettYharmonize and show that it achieves comparable performance while avoiding data leakage, particularly in site-target-dependence scenarios.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KAN-AD: Time Series Anomaly Detection with Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2411.00278</link>
<guid>https://arxiv.org/abs/2411.00278</guid>
<content:encoded><![CDATA[
arXiv:2411.00278v2 Announce Type: replace 
Abstract: Time series anomaly detection (TSAD) underpins real-time monitoring in cloud services and web systems, allowing rapid identification of anomalies to prevent costly failures. Most TSAD methods driven by forecasting models tend to overfit by emphasizing minor fluctuations. Our analysis reveals that effective TSAD should focus on modeling "normal" behavior through smooth local patterns. To achieve this, we reformulate time series modeling as approximating the series with smooth univariate functions. The local smoothness of each univariate function ensures that the fitted time series remains resilient against local disturbances. However, a direct KAN implementation proves susceptible to these disturbances due to the inherently localized characteristics of B-spline functions. We thus propose KAN-AD, replacing B-splines with truncated Fourier expansions and introducing a novel lightweight learning mechanism that emphasizes global patterns while staying robust to local disturbances. On four popular TSAD benchmarks, KAN-AD achieves an average 15% improvement in detection accuracy (with peaks exceeding 27%) over state-of-the-art baselines. Remarkably, it requires fewer than 1,000 trainable parameters, resulting in a 50% faster inference speed compared to the original KAN, demonstrating the approach's efficiency and practical viability.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Rationale-Aware Graph Contrastive Learning for Jet Discrimination</title>
<link>https://arxiv.org/abs/2411.01642</link>
<guid>https://arxiv.org/abs/2411.01642</guid>
<content:encoded><![CDATA[
arXiv:2411.01642v4 Announce Type: replace 
Abstract: In high-energy physics, particle jet tagging plays a pivotal role in distinguishing quark from gluon jets using data from collider experiments. While graph-based deep learning methods have advanced this task beyond traditional feature-engineered approaches, the complex data structure and limited labeled samples present ongoing challenges. However, existing contrastive learning (CL) frameworks struggle to leverage rationale-aware augmentations effectively, often lacking supervision signals that guide the extraction of salient features and facing computational efficiency issues such as high parameter counts. In this study, we demonstrate that integrating a quantum rationale generator (QRG) within our proposed Quantum Rationale-aware Graph Contrastive Learning (QRGCL) framework significantly enhances jet discrimination performance, reducing reliance on labeled data and capturing discriminative features. Evaluated on the quark-gluon jet dataset, QRGCL achieves an AUC score of $77.53\%$ while maintaining a compact architecture of only 45 QRG parameters, outperforming classical, quantum, and hybrid GCL and GNN benchmarks. These results highlight QRGCL's potential to advance jet tagging and other complex classification tasks in high-energy physics, where computational efficiency and feature extraction limitations persist.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Privacy in Continual Learning: Which Labels to Update?</title>
<link>https://arxiv.org/abs/2411.04680</link>
<guid>https://arxiv.org/abs/2411.04680</guid>
<content:encoded><![CDATA[
arXiv:2411.04680v4 Announce Type: replace 
Abstract: The goal of continual learning (CL) is to retain knowledge across tasks, but this conflicts with strict privacy required for sensitive training data that prevents storing or memorising individual samples. To address that, we combine CL and differential privacy (DP). We highlight that failing to account for privacy leakage through the set of labels a model can output can break the privacy of otherwise valid DP algorithms. This is especially relevant in CL. We show that mitigating the issue with a data-independent overly large label space can have minimal negative impact on utility when fine-tuning a pre-trained model under DP, while learning the labels with a separate DP mechanism risks losing small classes.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers</title>
<link>https://arxiv.org/abs/2411.10510</link>
<guid>https://arxiv.org/abs/2411.10510</guid>
<content:encoded><![CDATA[
arXiv:2411.10510v2 Announce Type: replace 
Abstract: Diffusion Transformers (DiT) have emerged as powerful generative models for various tasks, including image, video, and speech synthesis. However, their inference process remains computationally expensive due to the repeated evaluation of resource-intensive attention and feed-forward modules. To address this, we introduce SmoothCache, a model-agnostic inference acceleration technique for DiT architectures. SmoothCache leverages the observed high similarity between layer outputs across adjacent diffusion timesteps. By analyzing layer-wise representation errors from a small calibration set, SmoothCache adaptively caches and reuses key features during inference. Our experiments demonstrate that SmoothCache achieves 8% to 71% speed up while maintaining or even improving generation quality across diverse modalities. We showcase its effectiveness on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio, highlighting its potential to enable real-time applications and broaden the accessibility of powerful DiT models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-decentralized Training of Spatio-Temporal Graph Neural Networks for Traffic Prediction</title>
<link>https://arxiv.org/abs/2412.03188</link>
<guid>https://arxiv.org/abs/2412.03188</guid>
<content:encoded><![CDATA[
arXiv:2412.03188v2 Announce Type: replace 
Abstract: In smart mobility, large networks of geographically distributed sensors produce vast amounts of high-frequency spatio-temporal data that must be processed in real time to avoid major disruptions. Traditional centralized approaches are increasingly unsuitable to this task, as they struggle to scale with expanding sensor networks, and reliability issues in central components can easily affect the whole deployment. To address these challenges, we explore and adapt semi-decentralized training techniques for Spatio-Temporal Graph Neural Networks (ST-GNNs) in smart mobility domain. We implement a simulation framework where sensors are grouped by proximity into multiple cloudlets, each handling a subgraph of the traffic graph, fetching node features from other cloudlets to train its own local ST-GNN model, and exchanging model updates with other cloudlets to ensure consistency, enhancing scalability and removing reliance on a centralized aggregator. We perform extensive comparative evaluation of four different ST-GNN training setups -- centralized, traditional FL, server-free FL, and Gossip Learning -- on large-scale traffic datasets, the METR-LA and PeMS-BAY datasets, for short-, mid-, and long-term vehicle speed predictions. Experimental results show that semi-decentralized setups are comparable to centralized approaches in performance metrics, while offering advantages in terms of scalability and fault tolerance. In addition, we highlight often overlooked issues in existing literature for distributed ST-GNNs, such as the variation in model performance across different geographical areas due to region-specific traffic patterns, and the significant communication overhead and computational costs that arise from the large receptive field of GNNs, leading to substantial data transfers and increased computation of partial embeddings.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperMARL: Adaptive Hypernetworks for Multi-Agent RL</title>
<link>https://arxiv.org/abs/2412.04233</link>
<guid>https://arxiv.org/abs/2412.04233</guid>
<content:encoded><![CDATA[
arXiv:2412.04233v3 Announce Type: replace 
Abstract: Adaptability to specialised or homogeneous behaviours is critical in cooperative multi-agent reinforcement learning (MARL). Parameter sharing (PS) techniques, common for efficient adaptation, often limit behavioural diversity due to cross-agent gradient interference, which we show can be exacerbated by the coupling of observations and agent IDs. Current remedies typically add complexity through altered objectives, manual preset diversity levels, or sequential updates. We ask: can shared policies adapt without these complexities? We propose HyperMARL, a PS approach using hypernetworks for dynamic agent-specific parameters, without altering the RL objective or requiring preset diversity levels. HyperMARL's explicit decoupling of observation- and agent-conditioned gradients empirically reduces policy gradient variance, facilitates shared-policy adaptation (including specialisation), and helps mitigate cross-agent interference. Across diverse MARL benchmarks (up to 20 agents), requiring homogeneous, heterogeneous, or mixed behaviours, HyperMARL achieves competitive performance against key baselines -- fully shared, non-parameter sharing, and three diversity-promoting methods -- while preserving behavioural diversity comparable to non-parameter sharing. These findings establish HyperMARL as a versatile approach for adaptive MARL. The code is publicly available at https://github.com/KaleabTessera/HyperMARL.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts</title>
<link>https://arxiv.org/abs/2412.04614</link>
<guid>https://arxiv.org/abs/2412.04614</guid>
<content:encoded><![CDATA[
arXiv:2412.04614v3 Announce Type: replace 
Abstract: Pretrained language models (LMs) can generalize to implications of facts that they are finetuned on. For example, if finetuned on ``John Doe lives in Tokyo," LMs can correctly answer ``What language do the people in John Doe's city speak?'' with ``Japanese''. However, little is known about the mechanisms that enable this generalization or how they are learned during pretraining. We introduce extractive structures as a framework for describing how components in LMs (e.g., MLPs or attention heads) coordinate to enable this generalization. The structures consist of informative components that store training facts as weight changes, and upstream and downstream extractive components that query and process the stored information to produce the correct implication. We hypothesize that extractive structures are learned during pretraining when encountering implications of previously known facts. This yields two predictions: a data ordering effect where extractive structures can be learned only if facts precede their implications, and a weight grafting effect where extractive structures can be transferred to predict counterfactual implications. We empirically demonstrate these phenomena in the OLMo-7b, Llama 3-8b, Gemma 2-9b, and Qwen 2-7b models. Of independent interest, our results also indicate that fact learning can occur at both early and late layers, which lead to different forms of generalization.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems</title>
<link>https://arxiv.org/abs/2412.07067</link>
<guid>https://arxiv.org/abs/2412.07067</guid>
<content:encoded><![CDATA[
arXiv:2412.07067v4 Announce Type: replace 
Abstract: The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for scaling Large Language Models (LLMs) efficiently, but it depends on heterogeneous compute and memory resources. These factors jointly affect system Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing benchmarks often fail to capture these trade-offs accurately, complicating practical deployment decisions. To address this, we introduce MoE-CAP, a benchmark specifically designed for MoE systems. Our analysis reveals that achieving an optimal balance across CAP is difficult with current hardware; MoE systems typically optimize two of the three dimensions at the expense of the third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose the CAP Radar Diagram. We further introduce sparsity-aware performance metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems across diverse hardware platforms and deployment scenarios.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Hierarchical Representation Learning of Samples and Features via Informed Tree-Wasserstein Distance</title>
<link>https://arxiv.org/abs/2501.03627</link>
<guid>https://arxiv.org/abs/2501.03627</guid>
<content:encoded><![CDATA[
arXiv:2501.03627v2 Announce Type: replace 
Abstract: High-dimensional data often exhibit hierarchical structures in both modes: samples and features. Yet, most existing approaches for hierarchical representation learning consider only one mode at a time. In this work, we propose an unsupervised method for jointly learning hierarchical representations of samples and features via Tree-Wasserstein Distance (TWD). Our method alternates between the two data modes. It first constructs a tree for one mode, then computes a TWD for the other mode based on that tree, and finally uses the resulting TWD to build the second mode's tree. By repeatedly alternating through these steps, the method gradually refines both trees and the corresponding TWDs, capturing meaningful hierarchical representations of the data. We provide a theoretical analysis showing that our method converges. We show that our method can be integrated into hyperbolic graph convolutional networks as a pre-processing technique, improving performance in link prediction and node classification tasks. In addition, our method outperforms baselines in sparse approximation and unsupervised Wasserstein distance learning tasks on word-document and single-cell RNA-sequencing datasets.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ringmaster ASGD: The First Asynchronous SGD with Optimal Time Complexity</title>
<link>https://arxiv.org/abs/2501.16168</link>
<guid>https://arxiv.org/abs/2501.16168</guid>
<content:encoded><![CDATA[
arXiv:2501.16168v2 Announce Type: replace 
Abstract: Asynchronous Stochastic Gradient Descent (Asynchronous SGD) is a cornerstone method for parallelizing learning in distributed machine learning. However, its performance suffers under arbitrarily heterogeneous computation times across workers, leading to suboptimal time complexity and inefficiency as the number of workers scales. While several Asynchronous SGD variants have been proposed, recent findings by Tyurin & Richt\'arik (NeurIPS 2023) reveal that none achieve optimal time complexity, leaving a significant gap in the literature. In this paper, we propose Ringmaster ASGD, a novel Asynchronous SGD method designed to address these limitations and tame the inherent challenges of Asynchronous SGD. We establish, through rigorous theoretical analysis, that Ringmaster ASGD achieves optimal time complexity under arbitrarily heterogeneous and dynamically fluctuating worker computation times. This makes it the first Asynchronous SGD method to meet the theoretical lower bounds for time complexity in such scenarios.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Pricing and Resource Allocation: An Optimal Online-Learning Approach</title>
<link>https://arxiv.org/abs/2501.18049</link>
<guid>https://arxiv.org/abs/2501.18049</guid>
<content:encoded><![CDATA[
arXiv:2501.18049v2 Announce Type: replace 
Abstract: We study an online learning problem on dynamic pricing and resource allocation, where we make joint pricing and inventory decisions to maximize the overall net profit. We consider the stochastic dependence of demands on the price, which complicates the resource allocation process and introduces significant non-convexity and non-smoothness to the problem. To solve this problem, we develop an efficient algorithm that utilizes a "Lower-Confidence Bound (LCB)" meta-strategy over multiple OCO agents. Our algorithm achieves $\tilde{O}(\sqrt{Tmn})$ regret (for $m$ suppliers and $n$ consumers), which is optimal with respect to the time horizon $T$. Our results illustrate an effective integration of statistical learning methodologies with complex operations research problems.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2501.18052</link>
<guid>https://arxiv.org/abs/2501.18052</guid>
<content:encoded><![CDATA[
arXiv:2501.18052v3 Announce Type: replace 
Abstract: Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Our evaluation shows that SAeUron outperforms existing approaches on the UnlearnCanvas benchmark for concepts and style unlearning, and effectively eliminates nudity when evaluated with I2P. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content under adversarial attack. Code and checkpoints are available at https://github.com/cywinski/SAeUron.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATA: Adaptive Task Allocation for Efficient Resource Management in Distributed Machine Learning</title>
<link>https://arxiv.org/abs/2502.00775</link>
<guid>https://arxiv.org/abs/2502.00775</guid>
<content:encoded><![CDATA[
arXiv:2502.00775v2 Announce Type: replace 
Abstract: Asynchronous methods are fundamental for parallelizing computations in distributed machine learning. They aim to accelerate training by fully utilizing all available resources. However, their greedy approach can lead to inefficiencies using more computation than required, especially when computation times vary across devices. If the computation times were known in advance, training could be fast and resource-efficient by assigning more tasks to faster workers. The challenge lies in achieving this optimal allocation without prior knowledge of the computation time distributions. In this paper, we propose ATA (Adaptive Task Allocation), a method that adapts to heterogeneous and random distributions of worker computation times. Through rigorous theoretical analysis, we show that ATA identifies the optimal task allocation and performs comparably to methods with prior knowledge of computation times. Experimental results further demonstrate that ATA is resource-efficient, significantly reducing costs compared to the greedy approach, which can be arbitrarily expensive depending on the number of workers.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Scalable Density Functional Theory Hamiltonian Prediction through Adaptive Sparsity</title>
<link>https://arxiv.org/abs/2502.01171</link>
<guid>https://arxiv.org/abs/2502.01171</guid>
<content:encoded><![CDATA[
arXiv:2502.01171v2 Announce Type: replace 
Abstract: Hamiltonian matrix prediction is pivotal in computational chemistry, serving as the foundation for determining a wide range of molecular properties. While SE(3) equivariant graph neural networks have achieved remarkable success in this domain, their substantial computational cost--driven by high-order tensor product (TP) operations--restricts their scalability to large molecular systems with extensive basis sets. To address this challenge, we introduce SPHNet, an efficient and scalable equivariant network, that incorporates adaptive SParsity into Hamiltonian prediction. SPHNet employs two innovative sparse gates to selectively constrain non-critical interaction combinations, significantly reducing tensor product computations while maintaining accuracy. To optimize the sparse representation, we develop a Three-phase Sparsity Scheduler, ensuring stable convergence and achieving high performance at sparsity rates of up to 70%. Extensive evaluations on QH9 and PubchemQH datasets demonstrate that SPHNet achieves state-of-the-art accuracy while providing up to a 7x speedup over existing models. Beyond Hamiltonian prediction, the proposed sparsification techniques also hold significant potential for improving the efficiency and scalability of other SE(3) equivariant networks, further broadening their applicability and impact. Our code can be found at https://github.com/microsoft/SPHNet.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of Intrinsic Dimension</title>
<link>https://arxiv.org/abs/2502.05075</link>
<guid>https://arxiv.org/abs/2502.05075</guid>
<content:encoded><![CDATA[
arXiv:2502.05075v3 Announce Type: replace 
Abstract: Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student-weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\mathcal{V}_s, \mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\mathcal{V}_s \cap \mathcal{V}_w$, while reduced by a factor of $\dim(\mathcal{V}_s)/N$ in the subspace of discrepancy $\mathcal{V}_w \setminus \mathcal{V}_s$ with $N$ pseudo-labels for W2S. Our analysis further casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported by experiments on synthetic regression problems, as well as real vision and NLP tasks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InSTA: Towards Internet-Scale Training For Agents</title>
<link>https://arxiv.org/abs/2502.06776</link>
<guid>https://arxiv.org/abs/2502.06776</guid>
<content:encoded><![CDATA[
arXiv:2502.06776v2 Announce Type: replace 
Abstract: The predominant approach for training web navigation agents is to gather human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data is an inefficient resource. We develop a pipeline to facilitate internet-scale training for agents without laborious human annotations. In the first stage, an LLM annotates 150k sites with agentic tasks. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM filters trajectories by judging their success. Language models are powerful data curation tools, identifying harmful content with an accuracy of 97%, judging successful trajectories with an accuracy of 82.6%, and producing effective data. We train agents based on Qwen 3 1.7B that are competitive with frontier LLMs as web agents, while being smaller and faster. Our top agent reaches a success rate of 56.9%, outperforming the data collection policy Qwen 3 235B, a 235 times larger Llama 4 Maverick, and reaching 94.7% of the performance of Gemini 2.5 Flash. We are releasing code, models and data at: https://data-for-agents.github.io.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prot2Chat: Protein LLM with Early-Fusion of Text, Sequence and Structure</title>
<link>https://arxiv.org/abs/2502.06846</link>
<guid>https://arxiv.org/abs/2502.06846</guid>
<content:encoded><![CDATA[
arXiv:2502.06846v2 Announce Type: replace 
Abstract: Motivation: Proteins are of great significance in living organisms. However, understanding their functions encounters numerous challenges, such as insufficient integration of multimodal information, a large number of training parameters, limited flexibility of classification-based methods, and the lack of systematic evaluation metrics for protein Q&amp;A systems. To tackle these issues, we propose the Prot2Chat framework. Results: We modified ProteinMPNN to encode protein sequence and structural information in a unified way. We used a large language model (LLM) to encode questions into vectors and developed a protein-text adapter to compress protein information into virtual tokens based on these vectors, achieving the early fusion of text and protein information. Finally, the same LLM reads the virtual tokens and the questions to generate answers. To optimize training efficiency, we froze the encoder and employed Low-Rank Adaptation (LoRA) techniques for the LLM. Experiments on two datasets show that both automated metrics and expert evaluations demonstrate the superior performance of our model, and zero-shot prediction results highlight its generalization ability. The models and codes are available at https://github.com/ wangzc1233/Prot2Chat. Contact: zqcao@suda.edu.cn or wangzc025@163.com Key words: Protein Q&amp;A, Early-Fusion, LLM
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Benign Overfitting in Nadaraya-Watson Interpolators</title>
<link>https://arxiv.org/abs/2502.07480</link>
<guid>https://arxiv.org/abs/2502.07480</guid>
<content:encoded><![CDATA[
arXiv:2502.07480v2 Announce Type: replace 
Abstract: In recent years, there has been much interest in understanding the generalization behavior of interpolating predictors, which overfit on noisy training data. Whereas standard analyses are concerned with whether a method is consistent or not, recent observations have shown that even inconsistent predictors can generalize well. In this work, we revisit the classic interpolating Nadaraya-Watson (NW) estimator (also known as Shepard's method), and study its generalization capabilities through this modern viewpoint. In particular, by varying a single bandwidth-like hyperparameter, we prove the existence of multiple overfitting behaviors, ranging non-monotonically from catastrophic, through benign, to tempered. Our results highlight how even classical interpolating methods can exhibit intricate generalization behaviors. In addition, for the purpose of tuning the hyperparameter, the results suggest that over-estimating the intrinsic dimension of the data is less harmful than under-estimating it. Numerical experiments complement our theory, demonstrating the same phenomena.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Off-Policy Reinforcement Learning with Batch and Weight Normalization</title>
<link>https://arxiv.org/abs/2502.07523</link>
<guid>https://arxiv.org/abs/2502.07523</guid>
<content:encoded><![CDATA[
arXiv:2502.07523v2 Announce Type: replace 
Abstract: Reinforcement learning has achieved significant milestones, but sample efficiency remains a bottleneck for real-world applications. Recently, CrossQ has demonstrated state-of-the-art sample efficiency with a low update-to-data (UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with higher UTD ratios. We identify challenges in the training dynamics, which are emphasized by higher UTD ratios. To address these, we integrate weight normalization into the CrossQ framework, a solution that stabilizes training, has been shown to prevent potential loss of plasticity and keeps the effective learning rate constant. Our proposed approach reliably scales with increasing UTD ratios, achieving competitive performance across 25 challenging continuous control tasks on the DeepMind Control Suite and Myosuite benchmarks, notably the complex dog and humanoid environments. This work eliminates the need for drastic interventions, such as network resets, and offers a simple yet robust pathway for improving sample efficiency and scalability in model-free reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classifier-Free Guidance: From High-Dimensional Analysis to Generalized Guidance Forms</title>
<link>https://arxiv.org/abs/2502.07849</link>
<guid>https://arxiv.org/abs/2502.07849</guid>
<content:encoded><![CDATA[
arXiv:2502.07849v2 Announce Type: replace 
Abstract: Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion and flow-based generative models, enabling high-quality conditional generation. A key theoretical challenge is characterizing the distribution induced by CFG, particularly in high-dimensional settings relevant to real-world data. Previous works have shown that CFG modifies the target distribution, steering it towards a distribution sharper than the target one, more shifted towards the boundary of the class. In this work, we provide a high-dimensional analysis of CFG, showing that these distortions vanish as the data dimension grows. We present a blessing-of-dimensionality result demonstrating that in sufficiently high and infinite dimensions, CFG accurately reproduces the target distribution. Using our high-dimensional theory, we show that there is a large family of guidances enjoying this property, in particular non-linear CFG generalizations. We study a simple non-linear power-law version, for which we demonstrate improved robustness, sample fidelity and diversity. Our findings are validated with experiments on class-conditional and text-to-image generation using state-of-the-art diffusion and flow-matching models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DivIL: Unveiling and Addressing Over-Invariance for Out-of- Distribution Generalization</title>
<link>https://arxiv.org/abs/2502.12413</link>
<guid>https://arxiv.org/abs/2502.12413</guid>
<content:encoded><![CDATA[
arXiv:2502.12413v2 Announce Type: replace 
Abstract: Out-of-distribution generalization is a common problem that expects the model to perform well in the different distributions even far from the train data. A popular approach to addressing this issue is invariant learning (IL), in which the model is compiled to focus on invariant features instead of spurious features by adding strong constraints during training. However, there are some potential pitfalls of strong invariant constraints. Due to the limited number of diverse environments and over-regularization in the feature space, it may lead to a loss of important details in the invariant features while alleviating the spurious correlations, namely the over-invariance, which can also degrade the generalization performance. We theoretically define the over-invariance and observe that this issue occurs in various classic IL methods. To alleviate this issue, we propose a simple approach Diverse Invariant Learning (DivIL) by adding the unsupervised contrastive learning and the random masking mechanism compensatory for the invariant constraints, which can be applied to various IL methods. Furthermore, we conduct experiments across multiple modalities across 12 datasets and 6 classic models, verifying our over-invariance insight and the effectiveness of our DivIL framework. Our code is available at https://github.com/kokolerk/DivIL.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Slamming: Training a Speech Language Model on One GPU in a Day</title>
<link>https://arxiv.org/abs/2502.15814</link>
<guid>https://arxiv.org/abs/2502.15814</guid>
<content:encoded><![CDATA[
arXiv:2502.15814v2 Announce Type: replace 
Abstract: We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Based Exploration in Truthful Monitored Markov Decision Processes</title>
<link>https://arxiv.org/abs/2502.16772</link>
<guid>https://arxiv.org/abs/2502.16772</guid>
<content:encoded><![CDATA[
arXiv:2502.16772v2 Announce Type: replace 
Abstract: A tenet of reinforcement learning is that the agent always observes rewards. However, this is not true in many realistic settings, e.g., a human observer may not always be available to provide rewards, sensors may be limited or malfunctioning, or rewards may be inaccessible during deployment. Monitored Markov decision processes (Mon-MDPs) have recently been proposed to model such settings. However, existing Mon-MDP algorithms have several limitations: they do not fully exploit the problem structure, cannot leverage a known monitor, lack worst-case guarantees for 'unsolvable' Mon-MDPs without specific initialization, and offer only asymptotic convergence proofs. This paper makes three contributions. First, we introduce a model-based algorithm for Mon-MDPs that addresses these shortcomings. The algorithm employs two instances of model-based interval estimation: one to ensure that observable rewards are reliably captured, and another to learn the minimax-optimal policy. Second, we empirically demonstrate the advantages. We show faster convergence than prior algorithms in over four dozen benchmarks, and even more dramatic improvement when the monitoring process is known. Third, we present the first finite-sample bound on performance. We show convergence to a minimax-optimal policy even when some rewards are never observable.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Similarity-Distance-Magnitude Universal Verification</title>
<link>https://arxiv.org/abs/2502.20167</link>
<guid>https://arxiv.org/abs/2502.20167</guid>
<content:encoded><![CDATA[
arXiv:2502.20167v3 Announce Type: replace 
Abstract: We address the neural network robustness problem by adding Similarity (i.e., correctly predicted depth-matches into training)-awareness and Distance-to-training-distribution-awareness to the existing output Magnitude (i.e., decision-boundary)-awareness of the softmax function. The resulting SDM activation function provides strong signals of the relative epistemic (reducible) predictive uncertainty. We use this novel behavior to further address the complementary HCI problem of mapping the output to human-interpretable summary statistics over relevant partitions of a held-out calibration set. Estimates of prediction-conditional uncertainty are obtained via a parsimonious learned transform over the class-conditional empirical CDFs of the output of a final-layer SDM activation function. For decision-making and as an intrinsic model check, estimates of class-conditional accuracy are obtained by further partitioning the high-probability regions of this calibrated output into class-conditional, region-specific CDFs. The uncertainty estimates from SDM calibration are remarkably robust to test-time distribution shifts and out-of-distribution inputs; incorporate awareness of the effective sample size; provide estimates of uncertainty from the learning and data splitting processes; and are well-suited for selective classification and conditional branching for additional test-time compute based on the predictive uncertainty, as for selective LLM generation, routing, and composition over multiple models and retrieval. Finally, we construct SDM networks, LLMs with uncertainty-aware verification and interpretability-by-exemplar as intrinsic properties. We provide open-source software implementing these results.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Remasking Discrete Diffusion Models with Inference-Time Scaling</title>
<link>https://arxiv.org/abs/2503.00307</link>
<guid>https://arxiv.org/abs/2503.00307</guid>
<content:encoded><![CDATA[
arXiv:2503.00307v2 Announce Type: replace 
Abstract: Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with a blog post on the project page: https://remdm.github.io
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Split Gibbs Discrete Diffusion Posterior Sampling</title>
<link>https://arxiv.org/abs/2503.01161</link>
<guid>https://arxiv.org/abs/2503.01161</guid>
<content:encoded><![CDATA[
arXiv:2503.01161v2 Announce Type: replace 
Abstract: We study the problem of posterior sampling in discrete-state spaces using discrete diffusion models. While posterior sampling methods for continuous diffusion models have achieved remarkable progress, analogous methods for discrete diffusion models remain challenging. In this work, we introduce a principled plug-and-play discrete diffusion posterior sampling algorithm based on split Gibbs sampling, which we call SGDD. Our algorithm enables reward-guided generation and solving inverse problems in discrete-state spaces. We demonstrate the convergence of SGDD to the target posterior distribution and verify this through controlled experiments on synthetic benchmarks. Our method enjoys state-of-the-art posterior sampling performance on a range of benchmarks for discrete data, including DNA sequence design, discrete image inverse problems, and music infilling, achieving more than 30% improved performance compared to existing baselines.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POPGym Arcade: Parallel Pixelated POMDPs</title>
<link>https://arxiv.org/abs/2503.01450</link>
<guid>https://arxiv.org/abs/2503.01450</guid>
<content:encoded><![CDATA[
arXiv:2503.01450v3 Announce Type: replace 
Abstract: We present the POPGym Arcade, a collection of hardware-accelerated, pixel-based environments with shared observation and action spaces. Each environment includes fully and partially observable variants, enabling counterfactual studies on partial observability. We also introduce mathematical tools for analyzing policies under partial observability, which reveal how agents recall past information to make decisions. Our analysis shows (1) that controlling for partial observability is critical and (2) that agents with long-term memory learn brittle policies that struggle to generalize. Finally, we demonstrate that recurrent policies can be "poisoned" by old, out-of-distribution observations, with implications for sim-to-real transfer, imitation learning, and offline reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steer LLM Latents for Hallucination Detection</title>
<link>https://arxiv.org/abs/2503.01917</link>
<guid>https://arxiv.org/abs/2503.01917</guid>
<content:encoded><![CDATA[
arXiv:2503.01917v2 Announce Type: replace 
Abstract: Hallucinations in LLMs pose a significant concern to their safe deployment in real-world applications. Recent approaches have leveraged the latent space of LLMs for hallucination detection, but their embeddings, optimized for linguistic coherence rather than factual accuracy, often fail to clearly separate truthful and hallucinated content. To this end, we propose the Truthfulness Separator Vector (TSV), a lightweight and flexible steering vector that reshapes the LLM's representation space during inference to enhance the separation between truthful and hallucinated outputs, without altering model parameters. Our two-stage framework first trains TSV on a small set of labeled exemplars to form compact and well-separated clusters. It then augments the exemplar set with unlabeled LLM generations, employing an optimal transport-based algorithm for pseudo-labeling combined with a confidence-based filtering process. Extensive experiments demonstrate that TSV achieves state-of-the-art performance with minimal labeled data, exhibiting strong generalization across datasets and providing a practical solution for real-world LLM applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers for molecular property prediction: Domain adaptation efficiently improves performance</title>
<link>https://arxiv.org/abs/2503.03360</link>
<guid>https://arxiv.org/abs/2503.03360</guid>
<content:encoded><![CDATA[
arXiv:2503.03360v3 Announce Type: replace 
Abstract: Over the past six years, molecular transformer models have become key tools in drug discovery. Most existing models are pre-trained on large, unlabeled datasets such as ZINC or ChEMBL. However, the extent to which large-scale pre-training improves molecular property prediction remains unclear. This study evaluates transformer models for this task while addressing their limitations. We explore how pre-training dataset size and chemically informed objectives impact performance. Our results show that increasing the dataset beyond approximately 400K to 800K molecules from large-scale unlabeled databases does not enhance performance across seven datasets covering five ADME endpoints: lipophilicity, permeability, solubility (two datasets), microsomal stability (two datasets), and plasma protein binding. In contrast, domain adaptation on a small, domain-specific dataset (less than or equal 4K molecules) using multi-task regression of physicochemical properties significantly boosts performance (P-value less than 0.001). A model pre-trained on 400K molecules and adapted with domain-specific data outperforms larger models such as MolFormer and performs comparably to MolBERT. Benchmarks against Random Forest (RF) baselines using descriptors and Morgan fingerprints show that chemically and physically informed features consistently yield better performance across model types. While RF remains a strong baseline, we identify concrete practices to enhance transformer performance. Aligning pre-training and adaptation with chemically meaningful tasks and domain-relevant data presents a promising direction for molecular property prediction. Our models are available on HuggingFace for easy use and adaptation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All-atom Diffusion Transformers: Unified generative modelling of molecules and materials</title>
<link>https://arxiv.org/abs/2503.03965</link>
<guid>https://arxiv.org/abs/2503.03965</guid>
<content:encoded><![CDATA[
arXiv:2503.03965v2 Announce Type: replace 
Abstract: Diffusion models are the standard toolkit for generative modelling of 3D atomic systems. However, for different types of atomic systems -- such as molecules and materials -- the generative processes are usually highly specific to the target system despite the underlying physics being the same. We introduce the All-atom Diffusion Transformer (ADiT), a unified latent diffusion framework for jointly generating both periodic materials and non-periodic molecular systems using the same model: (1) An autoencoder maps a unified, all-atom representations of molecules and materials to a shared latent embedding space; and (2) A diffusion model is trained to generate new latent embeddings that the autoencoder can decode to sample new molecules or materials. Experiments on MP20, QM9 and GEOM-DRUGS datasets demonstrate that jointly trained ADiT generates realistic and valid molecules as well as materials, obtaining state-of-the-art results on par with molecule and crystal-specific models. ADiT uses standard Transformers with minimal inductive biases for both the autoencoder and diffusion model, resulting in significant speedups during training and inference compared to equivariant diffusion models. Scaling ADiT up to half a billion parameters predictably improves performance, representing a step towards broadly generalizable foundation models for generative chemistry. Open source code: https://github.com/facebookresearch/all-atom-diffusion-transformer
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts</title>
<link>https://arxiv.org/abs/2503.05066</link>
<guid>https://arxiv.org/abs/2503.05066</guid>
<content:encoded><![CDATA[
arXiv:2503.05066v2 Announce Type: replace 
Abstract: The Mixture of Experts (MoE) is an effective architecture for scaling large language models by leveraging sparse expert activation, optimizing the trade-off between performance and efficiency. However, under expert parallelism, MoE suffers from inference inefficiencies due to imbalanced token-to-expert assignment, where some experts are overloaded while others remain underutilized. This imbalance leads to poor resource utilization and increased latency, as the most burdened expert dictates the overall delay, a phenomenon we define as the \textbf{\textit{Straggler Effect}}. To mitigate this, we propose Capacity-Aware Inference, including two key techniques: (1) \textbf{\textit{Capacity-Aware Token Drop}}, which discards overloaded tokens to regulate the maximum latency of MoE, and (2) \textbf{\textit{Capacity-Aware Token Reroute}}, which reallocates overflowed tokens to underutilized experts, balancing the token distribution. These techniques collectively optimize both high-load and low-load expert utilization, leading to a more efficient MoE inference pipeline. Extensive experiments demonstrate the effectiveness of our methods, showing significant improvements in inference efficiency, e.g., 0.2\% average performance increase and a 1.94$\times$ inference speedup on Mixtral-8$\times$7B-Instruct.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExMAG: Learning of Maximally Ancestral Graphs</title>
<link>https://arxiv.org/abs/2503.08245</link>
<guid>https://arxiv.org/abs/2503.08245</guid>
<content:encoded><![CDATA[
arXiv:2503.08245v3 Announce Type: replace 
Abstract: In mixed graphs, there are both directed and undirected edges. An extension of acyclicity to this mixed-graph setting is known as maximally ancestral graphs. This extension is of considerable interest in causal learning in the presence of confounders. There, directed edges represent a clear direction of causality, while undirected edges represent confounding. We propose a score-based branch-and-cut algorithm for learning maximally ancestral graphs. The algorithm produces more accurate results than state-of-the-art methods, while being faster to run on small and medium-sized synthetic instances.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe RLHF-V: Safe Reinforcement Learning from Multi-modal Human Feedback</title>
<link>https://arxiv.org/abs/2503.17682</link>
<guid>https://arxiv.org/abs/2503.17682</guid>
<content:encoded><![CDATA[
arXiv:2503.17682v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) are essential for building general-purpose AI assistants; however, they pose increasing safety risks. How can we ensure safety alignment of MLLMs to prevent undesired behaviors? Going further, it is critical to explore how to fine-tune MLLMs to preserve capabilities while meeting safety constraints. Fundamentally, this challenge can be formulated as a min-max optimization problem. However, existing datasets have not yet disentangled single preference signals into explicit safety constraints, hindering systematic investigation in this direction. Moreover, it remains an open question whether such constraints can be effectively incorporated into the optimization process for multi-modal models. In this work, we present the first exploration of the Safe RLHF-V -- the first multimodal safety alignment framework. The framework consists of: $\mathbf{(I)}$ BeaverTails-V, the first open-source dataset featuring dual preference annotations for helpfulness and safety, supplemented with multi-level safety labels (minor, moderate, severe); $\mathbf{(II)}$ Beaver-Guard-V, a multi-level guardrail system to proactively defend against unsafe queries and adversarial attacks. Applying the guard model over five rounds of filtering and regeneration significantly enhances the precursor model's overall safety by an average of 40.9%. $\mathbf{(III)}$ Based on dual preference, we initiate the first exploration of multi-modal safety alignment within a constrained optimization. Experimental results demonstrate that Safe RLHF effectively improves both model helpfulness and safety. Specifically, Safe RLHF-V enhances model safety by 34.2% and helpfulness by 34.3%.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stroke Disease Classification Using Machine Learning with Feature Selection Techniques</title>
<link>https://arxiv.org/abs/2504.00485</link>
<guid>https://arxiv.org/abs/2504.00485</guid>
<content:encoded><![CDATA[
arXiv:2504.00485v3 Announce Type: replace 
Abstract: Heart disease remains a leading cause of mortality and morbidity worldwide, necessitating the development of accurate and reliable predictive models to facilitate early detection and intervention. While state of the art work has focused on various machine learning approaches for predicting heart disease, but they could not able to achieve remarkable accuracy. In response to this need, we applied nine machine learning algorithms XGBoost, logistic regression, decision tree, random forest, k-nearest neighbors (KNN), support vector machine (SVM), gaussian na\"ive bayes (NB gaussian), adaptive boosting, and linear regression to predict heart disease based on a range of physiological indicators. Our approach involved feature selection techniques to identify the most relevant predictors, aimed at refining the models to enhance both performance and interpretability. The models were trained, incorporating processes such as grid search hyperparameter tuning, and cross-validation to minimize overfitting. Additionally, we have developed a novel voting system with feature selection techniques to advance heart disease classification. Furthermore, we have evaluated the models using key performance metrics including accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (ROC AUC). Among the models, XGBoost demonstrated exceptional performance, achieving 99% accuracy, precision, F1-Score, 98% recall, and 100% ROC AUC. This study offers a promising approach to early heart disease diagnosis and preventive healthcare.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dion: Distributed Orthonormalized Updates</title>
<link>https://arxiv.org/abs/2504.05295</link>
<guid>https://arxiv.org/abs/2504.05295</guid>
<content:encoded><![CDATA[
arXiv:2504.05295v2 Announce Type: replace 
Abstract: Recent work has shown that orthonormal matrix updates speed up neural network optimization, improve training stability, and offer better hyperparameter transfer across model sizes. Applying these updates efficiently when model weights and optimizer states are sharded across a large-scale distributed LLM training system remains a major challenge. We introduce Dion (DIstributed OrthoNormalization), a scalable and communication-efficient orthonormalizing optimizer. Dion leverages low-rank approximation and decoupled momentum buffers, eliminating the need for full gradient synchronization while producing numerically equivalent results. It is compatible with simultaneous DDP, FSDP, and TP parallelism, and it computes an orthonormalized update without unsharding a full parameter matrix on any single device. We evaluate Dion on language models from 120M to 3B parameters and find that its benefits improve with increasing model size and batch size.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.05585</link>
<guid>https://arxiv.org/abs/2504.05585</guid>
<content:encoded><![CDATA[
arXiv:2504.05585v2 Announce Type: replace 
Abstract: Episodic tasks in Reinforcement Learning (RL) often pose challenges due to sparse reward signals and high-dimensional state spaces, which hinder efficient learning. Additionally, these tasks often feature hidden "trap states" -- irreversible failures that prevent task completion but do not provide explicit negative rewards to guide agents away from repeated errors. To address these issues, we propose Time-Weighted Contrastive Reward Learning (TW-CRL), an Inverse Reinforcement Learning (IRL) framework that leverages both successful and failed demonstrations. By incorporating temporal information, TW-CRL learns a dense reward function that identifies critical states associated with success or failure. This approach not only enables agents to avoid trap states but also encourages meaningful exploration beyond simple imitation of expert trajectories. Empirical evaluations on navigation tasks and robotic manipulation benchmarks demonstrate that TW-CRL surpasses state-of-the-art methods, achieving improved efficiency and robustness.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architecture independent generalization bounds for overparametrized deep ReLU networks</title>
<link>https://arxiv.org/abs/2504.05695</link>
<guid>https://arxiv.org/abs/2504.05695</guid>
<content:encoded><![CDATA[
arXiv:2504.05695v3 Announce Type: replace 
Abstract: We prove that overparametrized neural networks are able to generalize with a test error that is independent of the level of overparametrization, and independent of the Vapnik-Chervonenkis (VC) dimension. We prove explicit bounds that only depend on the metric geometry of the test and training sets, on the regularity properties of the activation function, and on the operator norms of the weights and norms of biases. For overparametrized deep ReLU networks with a training sample size bounded by the input space dimension, we explicitly construct zero loss minimizers without use of gradient descent, and prove that the generalization error is independent of the network architecture.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRIP: DRop unImportant data Points -- Enhancing Machine Learning Efficiency with Grad-CAM-Based Real-Time Data Prioritization for On-Device Training</title>
<link>https://arxiv.org/abs/2504.08364</link>
<guid>https://arxiv.org/abs/2504.08364</guid>
<content:encoded><![CDATA[
arXiv:2504.08364v2 Announce Type: replace 
Abstract: Selecting data points for model training is critical in machine learning. Effective selection methods can reduce the labeling effort, optimize on-device training for embedded systems with limited data storage, and enhance the model performance. This paper introduces a novel algorithm that uses Grad-CAM to make online decisions about retaining or discarding data points. Optimized for embedded devices, the algorithm computes a unique DRIP Score to quantify the importance of each data point. This enables dynamic decision-making on whether a data point should be stored for potential retraining or discarded without compromising model performance. Experimental evaluations on four benchmark datasets demonstrate that our approach can match or even surpass the accuracy of models trained on the entire dataset, all while achieving storage savings of up to 39\%. To our knowledge, this is the first algorithm that makes online decisions about data point retention without requiring access to the entire dataset.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling</title>
<link>https://arxiv.org/abs/2504.10612</link>
<guid>https://arxiv.org/abs/2504.10612</guid>
<content:encoded><![CDATA[
arXiv:2504.10612v3 Announce Type: replace 
Abstract: The most widely used generative models map noise and data distributions by matching flows or scores. However, they struggle to incorporate partial observations and additional priors--something energy-based models (EBMs) handle elegantly by simply adding corresponding scalar energy terms. We address this issue by proposing Energy Matching, a framework that endows flow-based approaches with the flexibility of EBMs. Far from the data manifold, samples move along curl-free, optimal transport paths from noise to data. As they approach the data manifold, an entropic energy term guides the system into a Boltzmann equilibrium distribution, explicitly capturing the underlying likelihood structure of the data. We parameterize this dynamic with a single time-independent scalar field, which serves as both a powerful generator and a flexible prior for effective regularization of inverse problems. Our method substantially outperforms existing EBMs on CIFAR-10 and ImageNet generation in terms of fidelity, while retaining simulation-free training of transport-based approaches away from the data manifold. Furthermore, we leverage the method's flexibility to introduce an interaction energy that supports diverse mode exploration, which we demonstrate in a controlled protein-generation setting. Our approach focuses on learning a scalar potential energy--without time-conditioning, auxiliary generators, or additional networks--which marks a significant departure from recent EBM methods. We believe that this simplified framework significantly advances EBMs capabilities and paves the way for their wider adoption in generative modeling across diverse domains.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORION Grounded in Context: Retrieval-Based Method for Hallucination Detection</title>
<link>https://arxiv.org/abs/2504.15771</link>
<guid>https://arxiv.org/abs/2504.15771</guid>
<content:encoded><![CDATA[
arXiv:2504.15771v3 Announce Type: replace 
Abstract: Despite advancements in grounded content generation, production Large Language Models (LLMs) based applications still suffer from hallucinated answers. We present "Grounded in Context" - a member of Deepchecks' ORION (Output Reasoning-based InspectiON) family of lightweight evaluation models. It is our framework for hallucination detection, designed for production-scale long-context data and tailored to diverse use cases, including summarization, data extraction, and RAG. Inspired by RAG architecture, our method integrates retrieval and Natural Language Inference (NLI) models to predict factual consistency between premises and hypotheses using an encoder-based model with only a 512-token context window. Our framework identifies unsupported claims with an F1 score of 0.83 in RAGTruth's response-level classification task, matching methods that trained on the dataset, and outperforming all comparable frameworks using similar-sized models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Personalized Federated Learning for Distributed Photovoltaic Disaggregation under Statistical Heterogeneity</title>
<link>https://arxiv.org/abs/2504.18078</link>
<guid>https://arxiv.org/abs/2504.18078</guid>
<content:encoded><![CDATA[
arXiv:2504.18078v2 Announce Type: replace 
Abstract: The rapid expansion of distributed photovoltaic (PV) installations worldwide, many being behind-the-meter systems, has significantly challenged energy management and grid operations, as unobservable PV generation further complicates the supply-demand balance. Therefore, estimating this generation from net load, known as PV disaggregation, is critical. Given privacy concerns and the need for large training datasets, federated learning becomes a promising approach, but statistical heterogeneity, arising from geographical and behavioral variations among prosumers, poses new challenges to PV disaggregation. To overcome these challenges, a privacy-preserving distributed PV disaggregation framework is proposed using Personalized Federated Learning (PFL). The proposed method employs a two-level framework that combines local and global modeling. At the local level, a transformer-based PV disaggregation model is designed to generate solar irradiance embeddings for representing local PV conditions. A novel adaptive local aggregation mechanism is adopted to mitigate the impact of statistical heterogeneity on the local model, extracting a portion of global information that benefits the local model. At the global level, a central server aggregates information uploaded from multiple data centers, preserving privacy while enabling cross-center knowledge sharing. Experiments on real-world data demonstrate the effectiveness of this proposed framework, showing improved accuracy and robustness compared to benchmark methods.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Approach to Backtracking Counterfactual Explanations: A Unified Causal Framework for Efficient Model Interpretability</title>
<link>https://arxiv.org/abs/2505.02435</link>
<guid>https://arxiv.org/abs/2505.02435</guid>
<content:encoded><![CDATA[
arXiv:2505.02435v2 Announce Type: replace 
Abstract: Counterfactual explanations enhance interpretability by identifying alternative inputs that produce different outputs, offering localized insights into model decisions. However, traditional methods often neglect causal relationships, leading to unrealistic examples. While newer approaches integrate causality, they are computationally expensive. To address these challenges, we propose an efficient method called BRACE based on backtracking counterfactuals that incorporates causal reasoning to generate actionable explanations. We first examine the limitations of existing methods and then introduce our novel approach and its features. We also explore the relationship between our method and previous techniques, demonstrating that it generalizes them in specific scenarios. Finally, experiments show that our method provides deeper insights into model outputs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation</title>
<link>https://arxiv.org/abs/2505.05181</link>
<guid>https://arxiv.org/abs/2505.05181</guid>
<content:encoded><![CDATA[
arXiv:2505.05181v3 Announce Type: replace 
Abstract: Backpropagation (BP) is the cornerstone of deep learning, but its reliance on global gradient synchronization limits scalability and imposes significant memory overhead. We propose Stochastic Variational Propagation (SVP), a scalable alternative that reframes training as hierarchical variational inference. SVP treats layer activations as latent variables and optimizes local Evidence Lower Bounds (ELBOs), enabling independent, local updates while preserving global coherence. However, directly applying KL divergence in layer-wise ELBOs risks inter-layer's representation collapse due to excessive compression. To prevent this, SVP projects activations into low-dimensional spaces via fixed random matrices, ensuring information preservation and representational diversity. Combined with a feature alignment loss for inter-layer consistency, SVP achieves competitive accuracy with BP across diverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to ImageNet), reduces memory usage by up to 4x, and significantly improves scalability. More broadly, SVP introduces a probabilistic perspective to deep representation learning, opening pathways toward more modular and interpretable neural network design.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Outperform Experts on Challenging Biology Benchmarks</title>
<link>https://arxiv.org/abs/2505.06108</link>
<guid>https://arxiv.org/abs/2505.06108</guid>
<content:encoded><![CDATA[
arXiv:2505.06108v3 Announce Type: replace 
Abstract: This study systematically evaluates 27 frontier Large Language Models on eight biology benchmarks spanning molecular biology, genetics, cloning, virology, and biosecurity. Models from major AI developers released between November 2022 and April 2025 were assessed through ten independent runs per benchmark. The findings reveal dramatic improvements in biological capabilities. Top model performance increased more than 4-fold on the challenging text-only subset of the Virology Capabilities Test over the study period, with OpenAI's o3 now performing twice as well as expert virologists. Several models now match or exceed expert-level performance on other challenging benchmarks, including the biology subsets of GPQA and WMDP and LAB-Bench CloningScenarios. Contrary to expectations, chain-of-thought did not substantially improve performance over zero-shot evaluation, while extended reasoning features in o3-mini and Claude 3.7 Sonnet typically improved performance as predicted by inference scaling. Benchmarks such as PubMedQA and the MMLU and WMDP biology subsets exhibited performance plateaus well below 100%, suggesting benchmark saturation and errors in the underlying benchmark data. The analysis highlights the need for more sophisticated evaluation methodologies as AI systems continue to advance.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDDA: Data Driven Attribution at LinkedIn</title>
<link>https://arxiv.org/abs/2505.09861</link>
<guid>https://arxiv.org/abs/2505.09861</guid>
<content:encoded><![CDATA[
arXiv:2505.09861v2 Announce Type: replace 
Abstract: Data Driven Attribution, which assigns conversion credits to marketing interactions based on causal patterns learned from data, is the foundation of modern marketing intelligence and vital to any marketing businesses and advertising platform. In this paper, we introduce a unified transformer-based attribution approach that can handle member-level data, aggregate-level data, and integration of external macro factors. We detail the large scale implementation of the approach at LinkedIn, showcasing significant impact. We also share learning and insights that are broadly applicable to the marketing and ad tech fields.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractal Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.11356</link>
<guid>https://arxiv.org/abs/2505.11356</guid>
<content:encoded><![CDATA[
arXiv:2505.11356v2 Announce Type: replace 
Abstract: While Graph Contrastive Learning (GCL) has attracted considerable attention in the field of graph self-supervised learning, its performance heavily relies on data augmentations that are expected to generate semantically consistent positive pairs. Existing strategies typically resort to random perturbations or local structure preservation, yet lack explicit control over global structural consistency between augmented views. To address this limitation, we propose Fractal Graph Contrastive Learning (FractalGCL), a theory-driven framework that leverages fractal self-similarity to enforce global topological coherence. FractalGCL introduces two key innovations: a renormalisation-based augmentation that generates structurally aligned positive views via box coverings; and a fractal-dimension-aware contrastive loss that aligns graph embeddings according to their fractal dimensions. While combining the two innovations markedly boosts graph-representation quality, it also adds non-trivial computational overhead. To mitigate the computational overhead of fractal dimension estimation, we derive a one-shot estimator by proving that the dimension discrepancy between original and renormalised graphs converges weakly to a centred Gaussian distribution. This theoretical insight enables a reduction in dimension computation cost by an order of magnitude, cutting overall training time by approximately 61%. The experiments show that FractalGCL not only delivers state-of-the-art results on standard benchmarks but also outperforms traditional baselines on traffic networks by an average margin of about remarkably 7%. Codes are available at (https://anonymous.4open.science/r/FractalGCL-0511).
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Treatment Effect Estimation for Optimal Decision-Making</title>
<link>https://arxiv.org/abs/2505.13092</link>
<guid>https://arxiv.org/abs/2505.13092</guid>
<content:encoded><![CDATA[
arXiv:2505.13092v2 Announce Type: replace 
Abstract: Decision-making across various fields, such as medicine, heavily relies on conditional average treatment effects (CATEs). Practitioners commonly make decisions by checking whether the estimated CATE is positive, even though the decision-making performance of modern CATE estimators is poorly understood from a theoretical perspective. In this paper, we study optimal decision-making based on two-stage CATE estimators (e.g., DR-learner), which are considered state-of-the-art and widely used in practice. We prove that, while such estimators may be optimal for estimating CATE, they can be suboptimal when used for decision-making. Intuitively, this occurs because such estimators prioritize CATE accuracy in regions far away from the decision boundary, which is ultimately irrelevant to decision-making. As a remedy, we propose a novel two-stage learning objective that retargets the CATE to balance CATE estimation error and decision performance. We then propose a neural method that optimizes an adaptively-smoothed approximation of our learning objective. Finally, we confirm the effectiveness of our method both empirically and theoretically. In sum, our work is the first to show how two-stage CATE estimators can be adapted for optimal decision-making.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimate-Then-Optimize versus Integrated-Estimation-Optimization versus Sample Average Approximation: A Stochastic Dominance Perspective</title>
<link>https://arxiv.org/abs/2304.06833</link>
<guid>https://arxiv.org/abs/2304.06833</guid>
<content:encoded><![CDATA[
arXiv:2304.06833v4 Announce Type: replace-cross 
Abstract: In data-driven stochastic optimization, model parameters of the underlying distribution need to be estimated from data in addition to the optimization task. Recent literature considers integrating the estimation and optimization processes by selecting model parameters that lead to the best empirical objective performance. This integrated approach, which we call integrated-estimation-optimization (IEO), can be readily shown to outperform simple estimate-then-optimize (ETO) when the model is misspecified. In this paper, we show that a reverse behavior appears when the model class is well-specified and there is sufficient data. Specifically, for a general class of nonlinear stochastic optimization problems, we show that simple ETO outperforms IEO asymptotically when the model class covers the ground truth, in the strong sense of stochastic dominance of the regret. Namely, the entire distribution of the regret, not only its mean or other moments, is always better for ETO compared to IEO. Our results also apply to constrained, contextual optimization problems where the decision depends on observed features. Whenever applicable, we also demonstrate how standard sample average approximation (SAA) performs the worst when the model class is well-specified in terms of regret, and best when it is misspecified. Finally, we provide experimental results to support our theoretical comparisons and illustrate when our insights hold in finite-sample regimes and under various degrees of misspecification.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SegMatch: A semi-supervised learning method for surgical instrument segmentation</title>
<link>https://arxiv.org/abs/2308.05232</link>
<guid>https://arxiv.org/abs/2308.05232</guid>
<content:encoded><![CDATA[
arXiv:2308.05232v2 Announce Type: replace-cross 
Abstract: Surgical instrument segmentation is recognised as a key enabler in providing advanced surgical assistance and improving computer-assisted interventions. In this work, we propose SegMatch, a semi-supervised learning method to reduce the need for expensive annotation for laparoscopic and robotic surgical images. SegMatch builds on FixMatch, a widespread semi supervised classification pipeline combining consistency regularization and pseudo-labelling, and adapts it for the purpose of segmentation. In our proposed SegMatch, the unlabelled images are first weakly augmented and fed to the segmentation model to generate pseudo-labels. In parallel, images are fed to a strong augmentation branch and consistency between the branches is used as an unsupervised loss. To increase the relevance of our strong augmentations, we depart from using only handcrafted augmentations and introduce a trainable adversarial augmentation strategy. Our FixMatch adaptation for segmentation tasks further includes carefully considering the equivariance and invariance properties of the augmentation functions we rely on. For binary segmentation tasks, our algorithm was evaluated on the MICCAI Instrument Segmentation Challenge datasets, Robust-MIS 2019 and EndoVis 2017. For multi-class segmentation tasks, we relied on the recent CholecInstanceSeg dataset. Our results show that SegMatch outperforms fully-supervised approaches by incorporating unlabelled data, and surpasses a range of state-of-the-art semi-supervised models across different labelled to unlabelled data ratios.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fourier Neural Operator Approach for Modelling Exciton-Polariton Condensate Systems</title>
<link>https://arxiv.org/abs/2309.15593</link>
<guid>https://arxiv.org/abs/2309.15593</guid>
<content:encoded><![CDATA[
arXiv:2309.15593v3 Announce Type: replace-cross 
Abstract: A plethora of next-generation all-optical devices based on exciton-polaritons have been proposed in latest years, including prototypes of transistors, switches, analogue quantum simulators and others. However, for such systems consisting of multiple polariton condensates, it is still challenging to predict their properties in a fast and accurate manner. The condensate physics is conventionally described by Gross-Pitaevskii equations (GPEs). While GPU-based solvers currently exist, we propose a significantly more efficient machine-learning-based Fourier neural operator approach to find the solution to the GPE coupled with exciton rate equations, trained on both numerical and experimental datasets. The proposed method predicts solutions almost three orders of magnitude faster than CUDA-based solvers in numerical studies, maintaining the high degree of accuracy. Our method not only accelerates simulations but also opens the door to faster, more scalable designs for all-optical chips and devices, offering profound implications for quantum computing, neuromorphic systems, and beyond.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization error property of infoGAN for two-layer neural network</title>
<link>https://arxiv.org/abs/2310.00443</link>
<guid>https://arxiv.org/abs/2310.00443</guid>
<content:encoded><![CDATA[
arXiv:2310.00443v2 Announce Type: replace-cross 
Abstract: Information Maximizing Generative Adversarial Network (infoGAN) can be understood as a minimax problem involving two neural networks: discriminators and generators with mutual information functions. The infoGAN incorporates various components, including latent variables, mutual information, and objective function. This research demonstrates the Generalization error property of infoGAN as the discriminator and generator sample size approaches infinity. This research explores the generalization error property of InfoGAN as the sample sizes of the discriminator and generator approach infinity. To establish this property, the study considers the difference between the empirical and population versions of the objective function. The error bound is derived from the Rademacher complexity of the discriminator and generator function classes. Additionally, the bound is proven for a two-layer network, where both the discriminator and generator utilize Lipschitz and non-decreasing activation functions.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Miscalibrated In-Context Learners</title>
<link>https://arxiv.org/abs/2312.13772</link>
<guid>https://arxiv.org/abs/2312.13772</guid>
<content:encoded><![CDATA[
arXiv:2312.13772v3 Announce Type: replace-cross 
Abstract: When adapting ICL with or without fine-tuning, we are curious about whether the instruction-tuned language model is able to achieve well-calibrated results without suffering from the problem of overconfidence (i.e., miscalibration) considering its strong instruction following ability, especially in such limited data setups. In this work, we deliver an in-depth analysis of the behavior across different choices of learning methods from the perspective of both performance and calibration. Through extensive controlled experiments, we observe that the miscalibration problem exists across all learning methods in low-resource setups. To achieve simultaneous gain for both in-task performance and calibration, we then study the potential of self-ensembling applied at different modeling stages (e.g., variations of in-context examples or variations in prompts or different ensembling strategies) to make the predictions more calibrated and have comparable or even better performance. We find that self-ensembling with max probability produces robust and calibrated predictions. Our work reveals the potential calibration problem of using ICL despite the improvements in task performance and sheds light on which learning paradigm to choose. We also provide practical guidelines for choosing learning paradigms depending on whether the data has been seen by the model before and a worthwhile solution via self-ensembling on how to enhance both task performance and calibration of LMs, which we hope could encourage further study.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do we need rebalancing strategies? A theoretical and empirical study around SMOTE and its variants</title>
<link>https://arxiv.org/abs/2402.03819</link>
<guid>https://arxiv.org/abs/2402.03819</guid>
<content:encoded><![CDATA[
arXiv:2402.03819v4 Announce Type: replace-cross 
Abstract: Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing strategy for handling imbalanced tabular data sets. However, few works analyze SMOTE theoretically. In this paper, we derive several non-asymptotic upper bound on SMOTE density. From these results, we prove that SMOTE (with default parameter) tends to copy the original minority samples asymptotically. We confirm and illustrate empirically this first theoretical behavior on a real-world data-set.bFurthermore, we prove that SMOTE density vanishes near the boundary of the support of the minority class distribution. We then adapt SMOTE based on our theoretical findings to introduce two new variants. These strategies are compared on 13 tabular data sets with 10 state-of-the-art rebalancing procedures, including deep generative and diffusion models. One of our key findings is that, for most data sets, applying no rebalancing strategy is competitive in terms of predictive performances, would it be with LightGBM, tuned random forests or logistic regression. However, when the imbalance ratio is artificially augmented, one of our two modifications of SMOTE leads to promising predictive performances compared to SMOTE and other state-of-the-art strategies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection</title>
<link>https://arxiv.org/abs/2404.02595</link>
<guid>https://arxiv.org/abs/2404.02595</guid>
<content:encoded><![CDATA[
arXiv:2404.02595v4 Announce Type: replace-cross 
Abstract: This study introduces the Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine Learning (QML) and quantum computing with Federated Learning (FL) for financial fraud detection. Using quantum technologies' computational power and the robust data privacy protections offered by FL, QFNN-FFD emerges as a secure and efficient method for identifying fraudulent transactions within the financial sector. Implementing a dual-phase training model across distributed clients enhances data integrity and enables superior performance metrics, achieving precision rates consistently above 95%. Additionally, QFNN-FFD demonstrates exceptional resilience by maintaining an impressive 80% accuracy, highlighting its robustness and readiness for real-world applications. This combination of high performance, security, and robustness against noise positions QFNN-FFD as a transformative advancement in financial technology solutions and establishes it as a new benchmark for privacy-focused fraud detection systems. This framework facilitates the broader adoption of secure, quantum-enhanced financial services and inspires future innovations that could use QML to tackle complex challenges in other areas requiring high confidentiality and accuracy.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReinWiFi: Application-Layer QoS Optimization of WiFi Networks with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2405.03526</link>
<guid>https://arxiv.org/abs/2405.03526</guid>
<content:encoded><![CDATA[
arXiv:2405.03526v2 Announce Type: replace-cross 
Abstract: The enhanced distributed channel access (EDCA) mechanism is used in current wireless fidelity (WiFi) networks to support priority requirements of heterogeneous applications. However, the EDCA mechanism can not adapt to particular quality-of-service (QoS) objective, network topology, and interference level. In this paper, a novel reinforcement-learning-based scheduling framework is proposed and implemented to optimize the application-layer quality-of-service (QoS) of a WiFi network with commercial adapters and unknown interference. Particularly, application-layer tasks of file delivery and delay-sensitive communication are jointly scheduled by adjusting the contention window sizes and application-layer throughput limitation, such that the throughput of the former and the round trip time of the latter can be optimized. Due to the unknown interference and vendor-dependent implementation of the WiFi adapters, the relation between the scheduling policy and the system QoS is unknown. Hence, a reinforcement learning method is proposed, in which a novel Q-network is trained to map from the historical scheduling parameters and QoS observations to the current scheduling action. It is demonstrated on a testbed that the proposed framework can achieve a significantly better performance than the EDCA mechanism.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causality Pursuit from Heterogeneous Environments via Neural Adversarial Invariance Learning</title>
<link>https://arxiv.org/abs/2405.04715</link>
<guid>https://arxiv.org/abs/2405.04715</guid>
<content:encoded><![CDATA[
arXiv:2405.04715v4 Announce Type: replace-cross 
Abstract: Pursuing causality from data is a fundamental problem in scientific discovery, treatment intervention, and transfer learning. This paper introduces a novel algorithmic method for addressing nonparametric invariance and causality learning in regression models across multiple environments, where the joint distribution of response variables and covariates varies, but the conditional expectations of outcome given an unknown set of quasi-causal variables are invariant. The challenge of finding such an unknown set of quasi-causal or invariant variables is compounded by the presence of endogenous variables that have heterogeneous effects across different environments. The proposed Focused Adversarial Invariant Regularization (FAIR) framework utilizes an innovative minimax optimization approach that drives regression models toward prediction-invariant solutions through adversarial testing. Leveraging the representation power of neural networks, FAIR neural networks (FAIR-NN) are introduced for causality pursuit. It is shown that FAIR-NN can find the invariant variables and quasi-causal variables under a minimal identification condition and that the resulting procedure is adaptive to low-dimensional composition structures in a non-asymptotic analysis. Under a structural causal model, variables identified by FAIR-NN represent pragmatic causality and provably align with exact causal mechanisms under conditions of sufficient heterogeneity. Computationally, FAIR-NN employs a novel Gumbel approximation with decreased temperature and a stochastic gradient descent ascent algorithm. The procedures are demonstrated using simulated and real-data examples.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red-Teaming for Inducing Societal Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2405.04756</link>
<guid>https://arxiv.org/abs/2405.04756</guid>
<content:encoded><![CDATA[
arXiv:2405.04756v2 Announce Type: replace-cross 
Abstract: Ensuring the safe deployment of AI systems is critical in industry settings where biased outputs can lead to significant operational, reputational, and regulatory risks. Thorough evaluation before deployment is essential to prevent these hazards. Red-teaming addresses this need by employing adversarial attacks to develop guardrails that detect and reject biased or harmful queries, enabling models to be retrained or steered away from harmful outputs. However, most red-teaming efforts focus on harmful or unethical instructions rather than addressing social bias, leaving this critical area under-explored despite its significant real-world impact, especially in customer-facing systems. We propose two bias-specific red-teaming methods, Emotional Bias Probe (EBP) and BiasKG, to evaluate how standard safety measures for harmful content affect bias. For BiasKG, we refactor natural language stereotypes into a knowledge graph. We use these attacking strategies to induce biased responses from several open- and closed-source language models. Unlike prior work, these methods specifically target social bias. We find our method increases bias in all models, even those trained with safety guardrails. Our work emphasizes uncovering societal bias in LLMs through rigorous evaluation, and recommends measures ensure AI safety in high-stakes industry deployments.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of clinical, dosimetric and radiomic features for predicting local failure after stereotactic radiotherapy of brain metastases in malignant melanoma</title>
<link>https://arxiv.org/abs/2405.20825</link>
<guid>https://arxiv.org/abs/2405.20825</guid>
<content:encoded><![CDATA[
arXiv:2405.20825v2 Announce Type: replace-cross 
Abstract: Background: This study aimed to predict lesion-specific outcomes after stereotactic radiotherapy (SRT) in patients with brain metastases from malignant melanoma (MBM), using clinical, dosimetric pretherapeutic MRI data. Methods: In this multicenter retrospective study, 517 MBM from 130 patients treated with single-fraction or hypofractionated SRT across three centers were analyzed. From contrast-enhanced T1-weighted MRI, 1576 radiomic features (RF) were extracted per lesion - 788 from the gross tumor volume (GTV), 788 from a 3 mm peritumoral margin. Clinical data, radiation dose and RF from one center were used for feature selection and model development via nested cross-validation; external validation was performed using the other two centers. Results: Local failure occurred in 72 of 517 lesions (13.9%). Predictive models based on clinical data (model 1), RF (model 2), or both (model 3) achieved c-indices of 0.60 +/- 0.15, 0.65 +/- 0.11, and 0.65 +/- 0.12. RF-based models outperformed the clinical model, while dosimetric data alone were not predictive. Most predictive RF came from the peritumoral margin (92%) vs. GTV (76%). On the first external dataset, all models performed similarly (c-index: 0.60-0.63), but showed poor generalization on the second (c-index < 0.50), likely due to differences in patient characteristics and imaging protocols. Conclusions: Information extracted from pretherapeutic MRI, particularly from the peritumoral area, can support accurate prediction of lesion-specific outcomes after SRT in MBM. When combined with clinical data, these imaging-derived markers offer valuable prognostic insights. However, generalizability remains challenging by heterogeneity in patient populations and MRI protocols.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projection Methods for Operator Learning and Universal Approximation</title>
<link>https://arxiv.org/abs/2406.12264</link>
<guid>https://arxiv.org/abs/2406.12264</guid>
<content:encoded><![CDATA[
arXiv:2406.12264v2 Announce Type: replace-cross 
Abstract: We obtain a new universal approximation theorem for continuous (possibly nonlinear) operators on arbitrary Banach spaces using the Leray-Schauder mapping. Moreover, we introduce and study a method for operator learning in Banach spaces $L^p$ of functions with multiple variables, based on orthogonal projections on polynomial bases. We derive a universal approximation result for operators where we learn a linear projection and a finite dimensional mapping under some additional assumptions. For the case of $p=2$, we give some sufficient conditions for the approximation results to hold. This article serves as the theoretical framework for a deep learning methodology in operator learning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Bandit Algorithms with Approximate Inference in Stochastic Linear Bandits</title>
<link>https://arxiv.org/abs/2406.14071</link>
<guid>https://arxiv.org/abs/2406.14071</guid>
<content:encoded><![CDATA[
arXiv:2406.14071v3 Announce Type: replace-cross 
Abstract: Bayesian bandit algorithms with approximate Bayesian inference have been widely used in real-world applications. Despite the superior practical performance, their theoretical justification is less investigated in the literature, especially for contextual bandit problems. To fill this gap, we propose a theoretical framework to analyze the impact of approximate inference in stochastic linear bandits and conduct frequentist regret analysis on two Bayesian bandit algorithms, Linear Thompson Sampling (LinTS) and the extension of Bayesian Upper Confidence Bound, namely Linear Bayesian Upper Confidence Bound (LinBUCB). We demonstrate that when applied in approximate inference settings, LinTS and LinBUCB can universally preserve their original rates of regret upper bound but with a sacrifice of larger constant terms. These results hold for general Bayesian inference approaches, assuming the inference error measured by two different $\alpha$-divergences is bounded. Additionally, by introducing a new definition of well-behaved distributions, we show that LinBUCB expedites the regret rate of LinTS from $\tilde{O}(d^{3/2}\sqrt{T})$ to $\tilde{O}(d\sqrt{T})$, matching the minimax optimal rate. To our knowledge, this work provides the first regret bounds in the setting of stochastic linear bandits with bounded approximate inference errors.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clusterpath Gaussian Graphical Modeling</title>
<link>https://arxiv.org/abs/2407.00644</link>
<guid>https://arxiv.org/abs/2407.00644</guid>
<content:encoded><![CDATA[
arXiv:2407.00644v2 Announce Type: replace-cross 
Abstract: Graphical models serve as effective tools for visualizing conditional dependencies between variables. However, as the number of variables grows, interpretation becomes increasingly difficult, and estimation uncertainty increases due to the large number of parameters relative to the number of observations. To address these challenges, we introduce the Clusterpath estimator of the Gaussian Graphical Model (CGGM) that encourages variable clustering in the graphical model in a data-driven way. Through the use of an aggregation penalty, we group variables together, which in turn results in a block-structured precision matrix whose block structure remains preserved in the covariance matrix. The CGGM estimator is formulated as the solution to a convex optimization problem, making it easy to incorporate other popular penalization schemes which we illustrate through the combination of an aggregation and sparsity penalty. We present a computationally efficient implementation of the CGGM estimator by using a cyclic block coordinate descent algorithm. In simulations, we show that CGGM not only matches, but oftentimes outperforms other state-of-the-art methods for variable clustering in graphical models. We also demonstrate CGGM's practical advantages and versatility on a diverse collection of empirical applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-Theoretic Foundations for Machine Learning</title>
<link>https://arxiv.org/abs/2407.12288</link>
<guid>https://arxiv.org/abs/2407.12288</guid>
<content:encoded><![CDATA[
arXiv:2407.12288v4 Announce Type: replace-cross 
Abstract: The progress of machine learning over the past decade is undeniable. In retrospect, it is both remarkable and unsettling that this progress was achievable with little to no rigorous theory to guide experimentation. Despite this fact, practitioners have been able to guide their future experimentation via observations from previous large-scale empirical investigations. In this work, we propose a theoretical framework which attempts to provide rigor to existing practices in machine learning. To the theorist, we provide a framework which is mathematically rigorous and leaves open many interesting ideas for future exploration. To the practitioner, we provide a framework whose results are simple, and provide intuition to guide future investigations across a wide range of learning paradigms. Concretely, we provide a theoretical framework rooted in Bayesian statistics and Shannon's information theory which is general enough to unify the analysis of many phenomena in machine learning. Our framework characterizes the performance of an optimal Bayesian learner as it learns from a stream of experience. Unlike existing analyses that weaken with increasing data complexity, our theoretical tools provide accurate insights across diverse machine learning settings. Throughout this work, we derive theoretical results and demonstrate their generality by apply them to derive insights specific to settings. These settings range from learning from data which is independently and identically distributed under an unknown distribution, to data which is sequential, to data which exhibits hierarchical structure amenable to meta-learning, and finally to data which is not fully explainable under the learner's beliefs (misspecification). These results are particularly relevant as we strive to understand and overcome increasingly difficult machine learning challenges in this endlessly complex world.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of Pseudo-Labeling for Computer Vision</title>
<link>https://arxiv.org/abs/2408.07221</link>
<guid>https://arxiv.org/abs/2408.07221</guid>
<content:encoded><![CDATA[
arXiv:2408.07221v2 Announce Type: replace-cross 
Abstract: Deep neural models have achieved state of the art performance on a wide range of problems in computer science, especially in computer vision. However, deep neural networks often require large datasets of labeled samples to generalize effectively, and an important area of active research is semi-supervised learning, which attempts to instead utilize large quantities of (easily acquired) unlabeled samples. One family of methods in this space is pseudo-labeling, a class of algorithms that use model outputs to assign labels to unlabeled samples which are then used as labeled samples during training. Such assigned labels, called pseudo-labels, are most commonly associated with the field of semi-supervised learning. In this work we explore a broader interpretation of pseudo-labels within both self-supervised and unsupervised methods. By drawing the connection between these areas we identify new directions when advancements in one area would likely benefit others, such as curriculum learning and self-supervised regularization.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAE-QWGAN: Addressing Mode Collapse in Quantum GANs via Autoencoding Priors</title>
<link>https://arxiv.org/abs/2409.10339</link>
<guid>https://arxiv.org/abs/2409.10339</guid>
<content:encoded><![CDATA[
arXiv:2409.10339v2 Announce Type: replace-cross 
Abstract: Recent proposals for quantum generative adversarial networks (GANs) suffer from the issue of mode collapse, analogous to classical GANs, wherein the distribution learnt by the GAN fails to capture the high mode complexities of the target distribution. Mode collapse can arise due to the use of uninformed prior distributions in the generative learning task. To alleviate the issue of mode collapse for quantum GANs, this work presents a novel \textbf{hybrid quantum-classical generative model}, the VAE-QWGAN, which combines the strengths of a classical Variational AutoEncoder (VAE) with a hybrid Quantum Wasserstein GAN (QWGAN). The VAE-QWGAN fuses the VAE decoder and QWGAN generator into a single quantum model, and utilizes the VAE encoder for data-dependant latent vector sampling during training. This in turn, enhances the diversity and quality of generated images. To generate new data from the trained model at inference, we sample from a Gaussian mixture model (GMM) prior that is learnt on the latent vectors generated during training. We conduct extensive experiments for image generation QGANs on MNIST/Fashion-MNIST datasets and compute a range of metrics that measure the diversity and quality of generated samples. We show that VAE-QWGAN demonstrates significant improvement over existing QGAN approaches.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Benefit of Being Bayesian in Online Conformal Prediction</title>
<link>https://arxiv.org/abs/2410.02561</link>
<guid>https://arxiv.org/abs/2410.02561</guid>
<content:encoded><![CDATA[
arXiv:2410.02561v2 Announce Type: replace-cross 
Abstract: Based on the framework of Conformal Prediction (CP), we study the online construction of confidence sets given a black-box machine learning model. By converting the target confidence levels into quantile levels, the problem can be reduced to predicting the quantiles (in hindsight) of a sequentially revealed data sequence. Two very different approaches have been studied previously: (i) Assuming the data sequence is iid or exchangeable, one could maintain the empirical distribution of the observed data as an algorithmic belief, and directly predict its quantiles. (ii) Due to the fragility of statistical assumptions, a recent trend is to consider the non-distributional, adversarial setting and apply first-order online optimization algorithms to moving quantile losses. However, it requires the oracle knowledge of the target quantile level, and suffers from a previously overlooked monotonicity issue due to the associated loss linearization.
  This paper presents an adaptive CP algorithm that combines their strengths. Without any statistical assumption, it is able to answer multiple arbitrary confidence level queries with low regret, while also overcoming the monotonicity issue suffered by first-order optimization baselines. Furthermore, if the data sequence is actually iid, then the same algorithm is automatically equipped with the "correct" coverage probability guarantee.
  To achieve such strengths, our key technical innovation is to regularize the aforementioned algorithmic belief (the empirical distribution) by a Bayesian prior, which robustifies it by simulating a non-linearized Follow the Regularized Leader (FTRL) algorithm on the output. Such a belief update backbone is shared by prediction heads targeting different confidence levels, bringing practical benefits analogous to the recently proposed concept of U-calibration (Kleinberg et al., 2023).
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Spoofing Attempts on Language Model Watermarks</title>
<link>https://arxiv.org/abs/2410.02693</link>
<guid>https://arxiv.org/abs/2410.02693</guid>
<content:encoded><![CDATA[
arXiv:2410.02693v2 Announce Type: replace-cross 
Abstract: LLM watermarks stand out as a promising way to attribute ownership of LLM-generated text. One threat to watermark credibility comes from spoofing attacks, where an unauthorized third party forges the watermark, enabling it to falsely attribute arbitrary texts to a particular LLM. Despite recent work demonstrating that state-of-the-art schemes are, in fact, vulnerable to spoofing, no prior work has focused on post-hoc methods to discover spoofing attempts. In this work, we for the first time propose a reliable statistical method to distinguish spoofed from genuinely watermarked text, suggesting that current spoofing attacks are less effective than previously thought. In particular, we show that regardless of their underlying approach, all current learning-based spoofing methods consistently leave observable artifacts in spoofed texts, indicative of watermark forgery. We build upon these findings to propose rigorous statistical tests that reliably reveal the presence of such artifacts and thus demonstrate that a watermark has been spoofed. Our experimental evaluation shows high test power across all learning-based spoofing methods, providing insights into their fundamental limitations and suggesting a way to mitigate this threat. We make all our code available at https://github.com/eth-sri/watermark-spoofing-detection .
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLEE: A Unified Framework and Benchmark for Language-based Economic Environments</title>
<link>https://arxiv.org/abs/2410.05254</link>
<guid>https://arxiv.org/abs/2410.05254</guid>
<content:encoded><![CDATA[
arXiv:2410.05254v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) show significant potential in economic and strategic interactions, where communication via natural language is often prevalent. This raises key questions: Do LLMs behave rationally? How do they perform compared to humans? Do they tend to reach an efficient and fair outcome? What is the role of natural language in strategic interaction? How do characteristics of the economic environment influence these dynamics? These questions become crucial concerning the economic and societal implications of integrating LLM-based agents into real-world data-driven systems, such as online retail platforms and recommender systems. To answer these questions, we introduce a benchmark for standardizing research on two-player, sequential, language-based games. Inspired by the economic literature, we define three base families of games with consistent parameterization, degrees of freedom and economic measures to evaluate agents' performance (self-gain), as well as the game outcome (efficiency and fairness). We develop an open-source framework for interaction simulation and analysis, and utilize it to collect a dataset of LLM vs. LLM interactions across numerous game configurations and an additional dataset of human vs. LLM interactions. Through extensive experimentation, we demonstrate how our framework and dataset can be used to: (i) compare the behavior of LLM-based agents in various economic contexts; (ii) evaluate agents in both individual and collective performance measures; and (iii) quantify the effect of the economic characteristics of the environments on the behavior of agents. Our results suggest that the market parameters, as well as the choice of the LLMs, tend to have complex and interdependent effects on the economic outcome, which calls for careful design and analysis of the language-based economic ecosystem.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WGFormer: An SE(3)-Transformer Driven by Wasserstein Gradient Flows for Molecular Ground-State Conformation Prediction</title>
<link>https://arxiv.org/abs/2410.09795</link>
<guid>https://arxiv.org/abs/2410.09795</guid>
<content:encoded><![CDATA[
arXiv:2410.09795v5 Announce Type: replace-cross 
Abstract: Predicting molecular ground-state conformation (i.e., energy-minimized conformation) is crucial for many chemical applications such as molecular docking and property prediction. Classic energy-based simulation is time-consuming when solving this problem, while existing learning-based methods have advantages in computational efficiency but sacrifice accuracy and interpretability. In this work, we propose a novel and effective method to bridge the energy-based simulation and the learning-based strategy, which designs and learns a Wasserstein gradient flow-driven SE(3)-Transformer, called WGFormer, for ground-state conformation prediction. Specifically, our method tackles this task within an auto-encoding framework, which encodes low-quality conformations by the proposed WGFormer and decodes corresponding ground-state conformations by an MLP. The architecture of WGFormer corresponds to Wasserstein gradient flows -- it optimizes conformations by minimizing an energy function defined on the latent mixture models of atoms, thereby significantly improving performance and interpretability. Extensive experiments demonstrate that our method consistently outperforms state-of-the-art competitors, providing a new and insightful paradigm to predict ground-state conformation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Implicit Graphon Learning</title>
<link>https://arxiv.org/abs/2410.17464</link>
<guid>https://arxiv.org/abs/2410.17464</guid>
<content:encoded><![CDATA[
arXiv:2410.17464v2 Announce Type: replace-cross 
Abstract: Graphons are continuous models that represent the structure of graphs and allow the generation of graphs of varying sizes. We propose Scalable Implicit Graphon Learning (SIGL), a scalable method that combines implicit neural representations (INRs) and graph neural networks (GNNs) to estimate a graphon from observed graphs. Unlike existing methods, which face important limitations like fixed resolution and scalability issues, SIGL learns a continuous graphon at arbitrary resolutions. GNNs are used to determine the correct node ordering, improving graph alignment. Furthermore, we characterize the asymptotic consistency of our estimator, showing that more expressive INRs and GNNs lead to consistent estimators. We evaluate SIGL in synthetic and real-world graphs, showing that it outperforms existing methods and scales effectively to larger graphs, making it ideal for tasks like graph data augmentation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination</title>
<link>https://arxiv.org/abs/2410.17477</link>
<guid>https://arxiv.org/abs/2410.17477</guid>
<content:encoded><![CDATA[
arXiv:2410.17477v5 Announce Type: replace-cross 
Abstract: The growth in prominence of large language models (LLMs) in everyday life can be largely attributed to their generative abilities, yet some of this is also owed to the risks and costs associated with their use. On one front is their tendency to hallucinate false or misleading information, limiting their reliability. On another is the increasing focus on the computational limitations associated with traditional self-attention based LLMs, which has brought about new alternatives, in particular recurrent models, meant to overcome them. Yet it remains uncommon to consider these two concerns simultaneously. Do changes in architecture exacerbate/alleviate existing concerns about hallucinations? Do they affect how and where they occur? Through an extensive evaluation, we study how these architecture-based inductive biases affect the propensity to hallucinate. While hallucination remains a general phenomenon not limited to specific architectures, the situations in which they occur and the ease with which specific types of hallucinations can be induced can significantly differ based on the model architecture. These findings highlight the need for better understanding both these problems in conjunction with each other, as well as consider how to design more universal techniques for handling hallucinations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based Confidence Calibration for Large Language Models</title>
<link>https://arxiv.org/abs/2411.02454</link>
<guid>https://arxiv.org/abs/2411.02454</guid>
<content:encoded><![CDATA[
arXiv:2411.02454v2 Announce Type: replace-cross 
Abstract: Reliable confidence estimation is essential for enhancing the trustworthiness of large language models (LLMs), especially in high-stakes scenarios. Despite its importance, accurately estimating confidence in LLM responses remains a significant challenge. In this work, we propose using an auxiliary learning model to assess response correctness based on the self-consistency of multiple outputs generated by the LLM. Our method builds a consistency graph to represent the agreement among multiple responses and uses a graph neural network (GNN) to estimate the likelihood that each response is correct. Experiments demonstrate that this method has strong calibration performance on various benchmark datasets and generalizes well to out-of-domain cases.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Map Similarity Reduction in Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2411.03226</link>
<guid>https://arxiv.org/abs/2411.03226</guid>
<content:encoded><![CDATA[
arXiv:2411.03226v2 Announce Type: replace-cross 
Abstract: It has been observed that Convolutional Neural Networks (CNNs) suffer from redundancy in feature maps, leading to inefficient capacity utilization. Efforts to address this issue have largely focused on kernel orthogonality method. In this work, we theoretically and empirically demonstrate that kernel orthogonality does not necessarily lead to a reduction in feature map redundancy. Based on this analysis, we propose the Convolutional Similarity method to reduce feature map similarity, independently of the CNN's input. The Convolutional Similarity can be minimized as either a regularization term or an iterative initialization method. Experimental results show that minimizing Convolutional Similarity not only improves classification accuracy but also accelerates convergence. Furthermore, our method enables the use of significantly smaller models to achieve the same level of performance, promoting a more efficient use of model capacity. Future work will focus on coupling the iterative initialization method with the optimization momentum term and examining the method's impact on generative frameworks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRACE: Transformer-based Risk Assessment for Clinical Evaluation</title>
<link>https://arxiv.org/abs/2411.08701</link>
<guid>https://arxiv.org/abs/2411.08701</guid>
<content:encoded><![CDATA[
arXiv:2411.08701v2 Announce Type: replace-cross 
Abstract: We present TRACE (Transformer-based Risk Assessment for Clinical Evaluation), a novel method for clinical risk assessment based on clinical data, leveraging the self-attention mechanism for enhanced feature interaction and result interpretation. Our approach is able to handle different data modalities, including continuous, categorical and multiple-choice (checkbox) attributes. The proposed architecture features a shared representation of the clinical data obtained by integrating specialized embeddings of each data modality, enabling the detection of high-risk individuals using Transformer encoder layers. To assess the effectiveness of the proposed method, a strong baseline based on non-negative multi-layer perceptrons (MLPs) is introduced. The proposed method outperforms various baselines widely used in the domain of clinical risk assessment, while effectively handling missing values. In terms of explainability, our Transformer-based method offers easily interpretable results via attention weights, further enhancing the clinicians' decision-making process.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for Exploration and Exploitation in Recommender Systems</title>
<link>https://arxiv.org/abs/2411.13865</link>
<guid>https://arxiv.org/abs/2411.13865</guid>
<content:encoded><![CDATA[
arXiv:2411.13865v3 Announce Type: replace-cross 
Abstract: Modern recommender systems often create information cocoons, restricting users' exposure to diverse content. A key challenge lies in balancing content exploration and exploitation while allowing users to adjust their recommendation preferences. Intuitively, this balance can be modeled as a tree-structured representation, where depth search facilitates exploitation and breadth search enables exploration. However, existing approaches face two fundamental limitations: Euclidean methods struggle to capture hierarchical structures, while hyperbolic methods, despite their superior hierarchical modeling, lack semantic understanding of user and item profiles and fail to provide a principled mechanism for balancing exploration and exploitation. To address these challenges, we propose HERec, a hyperbolic graph-LLM framework that effectively balances exploration and exploitation in recommender systems. Our framework introduces two key innovations: (1) a semantic-enhanced hierarchical mechanism that aligns rich textual descriptions processed by large language models (LLMs) with collaborative information directly in hyperbolic space, allowing for more nuanced updates that respect the underlying hierarchical structure in user-item profiles; (2) an automatic hierarchical representation by optimizing Dasgupta's cost, which discovers hierarchical structures without requiring predefined hyperparameters, enabling user-adjustable exploration-exploitation trade-offs. Extensive experiments demonstrate that HERec consistently outperforms both Euclidean and hyperbolic baselines, achieving up to 5.49% improvement in utility metrics and 11.39% increase in diversity metrics, effectively mitigating information cocoons. We open-source our model implementation at https://github.com/Martin-qyma/HERec.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving</title>
<link>https://arxiv.org/abs/2411.14716</link>
<guid>https://arxiv.org/abs/2411.14716</guid>
<content:encoded><![CDATA[
arXiv:2411.14716v2 Announce Type: replace-cross 
Abstract: This paper introduces VisionPAD, a novel self-supervised pre-training paradigm designed for vision-centric algorithms in autonomous driving. In contrast to previous approaches that employ neural rendering with explicit depth supervision, VisionPAD utilizes more efficient 3D Gaussian Splatting to reconstruct multi-view representations using only images as supervision. Specifically, we introduce a self-supervised method for voxel velocity estimation. By warping voxels to adjacent frames and supervising the rendered outputs, the model effectively learns motion cues in the sequential data. Furthermore, we adopt a multi-frame photometric consistency approach to enhance geometric perception. It projects adjacent frames to the current frame based on rendered depths and relative poses, boosting the 3D geometric representation through pure image supervision. Extensive experiments on autonomous driving datasets demonstrate that VisionPAD significantly improves performance in 3D object detection, occupancy prediction and map segmentation, surpassing state-of-the-art pre-training strategies by a considerable margin.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Lack of Robustness of Binary Function Similarity Systems</title>
<link>https://arxiv.org/abs/2412.04163</link>
<guid>https://arxiv.org/abs/2412.04163</guid>
<content:encoded><![CDATA[
arXiv:2412.04163v2 Announce Type: replace-cross 
Abstract: Binary function similarity, which often relies on learning-based algorithms to identify what functions in a pool are most similar to a given query function, is a sought-after topic in different communities, including machine learning, software engineering, and security. Its importance stems from the impact it has in facilitating several crucial tasks, from reverse engineering and malware analysis to automated vulnerability detection. Whereas recent work cast light around performance on this long-studied problem, the research landscape remains largely lackluster in understanding the resiliency of the state-of-the-art machine learning models against adversarial attacks. As security requires to reason about adversaries, in this work we assess the robustness of such models through a simple yet effective black-box greedy attack, which modifies the topology and the content of the control flow of the attacked functions. We demonstrate that this attack is successful in compromising all the models, achieving average attack success rates of 57.06% and 95.81% depending on the problem settings (targeted and untargeted attacks). Our findings are insightful: top performance on clean data does not necessarily relate to top robustness properties, which explicitly highlights performance-robustness trade-offs one should consider when deploying such models, calling for further research.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor Control? A Comprehensive Study</title>
<link>https://arxiv.org/abs/2412.11764</link>
<guid>https://arxiv.org/abs/2412.11764</guid>
<content:encoded><![CDATA[
arXiv:2412.11764v4 Announce Type: replace-cross 
Abstract: Executing precise and agile flight maneuvers is critical for quadrotors in various applications. Traditional quadrotor control approaches are limited by their reliance on flat trajectories or time-consuming optimization, which restricts their flexibility. Recently, RL-based policy has emerged as a promising alternative due to its ability to directly map observations to actions, reducing the need for detailed system knowledge and actuation constraints. However, a significant challenge remains in bridging the sim-to-real gap, where RL-based policies often experience instability when deployed in real world. In this paper, we investigate key factors for learning robust RL-based control policies that are capable of zero-shot deployment in real-world quadrotors. We identify five critical factors and we develop a PPO-based training framework named SimpleFlight, which integrates these five techniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor, demonstrating that it achieves more than a 50% reduction in trajectory tracking error compared to state-of-the-art RL baselines. The policy derived by SimpleFlight consistently excels across both smooth polynominal trajectories and challenging infeasible zigzag trajectories on small thrust-to-weight quadrotors. In contrast, baseline methods struggle with high-speed or infeasible trajectories. To support further research and reproducibility, we integrate SimpleFlight into a GPU-based simulator Omnidrones and provide open-source access to the code and model checkpoints. We hope SimpleFlight will offer valuable insights for advancing RL-based quadrotor control. For more details, visit our project website at https://sites.google.com/view/simpleflight/.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Knowledge Transfer: Refined Feature Distillation for Zero-Shot Quantization in Edge Computing</title>
<link>https://arxiv.org/abs/2412.19125</link>
<guid>https://arxiv.org/abs/2412.19125</guid>
<content:encoded><![CDATA[
arXiv:2412.19125v2 Announce Type: replace-cross 
Abstract: We introduce AKT (Advanced Knowledge Transfer), a novel method to enhance the training ability of low-bit quantized (Q) models in the field of zero-shot quantization (ZSQ). Existing research in ZSQ has focused on generating high-quality data from full-precision (FP) models. However, these approaches struggle with reduced learning ability in low-bit quantization due to its limited information capacity. To overcome this limitation, we propose effective training strategy compared to data generation. Particularly, we analyzed that refining feature maps in the feature distillation process is an effective way to transfer knowledge to the Q model. Based on this analysis, AKT efficiently transfer core information from the FP model to the Q model. AKT is the first approach to utilize both spatial and channel attention information in feature distillation in ZSQ. Our method addresses the fundamental gradient exploding problem in low-bit Q models. Experiments on CIFAR-10 and CIFAR-100 datasets demonstrated the effectiveness of the AKT. Our method led to significant performance enhancement in existing generative models. Notably, AKT achieved significant accuracy improvements in low-bit Q models, achieving state-of-the-art in the 3,5bit scenarios on CIFAR-10. The code is available at https://github.com/Inpyo-Hong/AKT-Advanced-knowledge-Transfer.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strengthening Generative Robot Policies through Predictive World Modeling</title>
<link>https://arxiv.org/abs/2502.00622</link>
<guid>https://arxiv.org/abs/2502.00622</guid>
<content:encoded><![CDATA[
arXiv:2502.00622v2 Announce Type: replace-cross 
Abstract: We present generative predictive control (GPC), a learning control framework that (i) clones a generative diffusion-based policy from expert demonstrations, (ii) trains a predictive action-conditioned world model from both expert demonstrations and random explorations, and (iii) synthesizes an online planner that ranks and optimizes the action proposals from (i) by looking ahead into the future using the world model from (ii). Across a variety of robotic manipulation tasks, we demonstrate that GPC consistently outperforms behavior cloning in both state-based and vision-based settings, in simulation and in the real world.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization</title>
<link>https://arxiv.org/abs/2502.00691</link>
<guid>https://arxiv.org/abs/2502.00691</guid>
<content:encoded><![CDATA[
arXiv:2502.00691v3 Announce Type: replace-cross 
Abstract: Recent advances in mathematical problem-solving with language models (LMs) integrate chain-of-thought (CoT) reasoning and code execution to harness their complementary strengths. However, existing hybrid frameworks exhibit a critical limitation: they depend on externally dictated instructions or rigid code-integration templates, lacking metacognitive awareness -- the capacity to dynamically evaluate intrinsic capabilities and autonomously determine when and how to integrate tools. This rigidity motivates our study of autonomous code integration, enabling models to adapt tool-usage strategies as their reasoning abilities evolve during training.
  While reinforcement learning (RL) shows promise for boosting LLM reasoning at scale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning autonomous code integration due to inadequate exploration of the vast combinatorial space of CoT-code interleaving patterns. To address this challenge, we propose a novel Expectation-Maximization (EM) framework that synergizes structured exploration (E-step) with off-policy RL optimization (M-step), creating a self-reinforcing cycle between metacognitive tool-use decisions and evolving capabilities. Experiments reveal our method achieves superior results through improved exploration. Notably, our 7B model improves over 11% on MATH500 and 9.4% on AIME without o1-like CoT.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents</title>
<link>https://arxiv.org/abs/2502.01218</link>
<guid>https://arxiv.org/abs/2502.01218</guid>
<content:encoded><![CDATA[
arXiv:2502.01218v2 Announce Type: replace-cross 
Abstract: Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments on both simulated and real robots show that the pretrained features significantly enhance downstream manipulation tasks with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimSort: A Data-Driven Framework for Spike Sorting by Large-Scale Electrophysiology Simulation</title>
<link>https://arxiv.org/abs/2502.03198</link>
<guid>https://arxiv.org/abs/2502.03198</guid>
<content:encoded><![CDATA[
arXiv:2502.03198v2 Announce Type: replace-cross 
Abstract: Spike sorting is an essential process in neural recording, which identifies and separates electrical signals from individual neurons recorded by electrodes in the brain, enabling researchers to study how specific neurons communicate and process information. Although there exist a number of spike sorting methods which have contributed to significant neuroscientific breakthroughs, many are heuristically designed, making it challenging to verify their correctness due to the difficulty of obtaining ground truth labels from real-world neural recordings. In this work, we explore a data-driven, deep learning-based approach. We begin by creating a large-scale dataset through electrophysiology simulations using biologically realistic computational models. We then present SimSort, a pretraining framework for spike sorting. Trained solely on simulated data, SimSort demonstrates zero-shot generalizability to real-world spike sorting tasks, yielding consistent improvements over existing methods across multiple benchmarks. These results highlight the potential of simulation-driven pretraining to enhance the robustness and scalability of spike sorting in experimental neuroscience.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data</title>
<link>https://arxiv.org/abs/2502.04380</link>
<guid>https://arxiv.org/abs/2502.04380</guid>
<content:encoded><![CDATA[
arXiv:2502.04380v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) using diverse datasets is crucial for enhancing their overall performance across various domains. In practical scenarios, existing methods based on modeling the mixture proportions of data composition often struggle with data whose domain labels are missing, imprecise or non-normalized, while methods based on data selection usually encounter difficulties in balancing multi-domain performance. To address these challenges, in this work, we investigate the role of data diversity in enhancing the overall abilities of LLMs by empirically constructing contrastive data pools and theoretically deriving explanations. Building upon the insights gained, we propose a new method that gives the LLM a dual identity: an output model to cognitively probe and select data based on diversity reward, as well as an input model to be tuned with the selected data. Extensive experiments show that the proposed method notably boosts performance across domain-undetermined data and a series of foundational downstream tasks when applied to various advanced LLMs. We release our code and hope this study can shed light on the understanding of data diversity and advance feedback-driven data-model co-design for LLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2502.06205</link>
<guid>https://arxiv.org/abs/2502.06205</guid>
<content:encoded><![CDATA[
arXiv:2502.06205v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) systems face a fundamental challenge in aligning independently developed retrievers and large language models (LLMs). Existing approaches typically involve modifying either component or introducing simple intermediate modules, resulting in practical limitations and sub-optimal performance. Inspired by human search behavior -- typically involving a back-and-forth process of proposing search queries and reviewing documents, we propose C-3PO, a proxy-centric framework that facilitates communication between retrievers and LLMs through a lightweight multi-agent system. Our framework implements three specialized agents that collaboratively optimize the entire RAG pipeline without altering the retriever and LLMs. These agents work together to assess the need for retrieval, generate effective queries, and select information suitable for the LLMs. To enable effective multi-agent coordination, we develop a tree-structured rollout approach for reward credit assignment in reinforcement learning. Extensive experiments in both in-domain and out-of-distribution scenarios demonstrate that C-3PO significantly enhances RAG performance while maintaining plug-and-play flexibility and superior generalization capabilities.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is a Sketch-and-Precondition Derivation for Low-Rank Approximation? Inverse Power Error or Inverse Power Estimation?</title>
<link>https://arxiv.org/abs/2502.07993</link>
<guid>https://arxiv.org/abs/2502.07993</guid>
<content:encoded><![CDATA[
arXiv:2502.07993v2 Announce Type: replace-cross 
Abstract: Randomized sketching accelerates large-scale numerical linear algebra by reducing computational complexity. While the traditional sketch-and-solve approach reduces the problem size directly through sketching, the sketch-and-precondition method leverages sketching to construct a computational friendly preconditioner. This preconditioner improves the convergence speed of iterative solvers applied to the original problem, maintaining accuracy in the full space. Furthermore, the convergence rate of the solver improves at least linearly with the sketch size. Despite its potential, developing a sketch-and-precondition framework for randomized algorithms in low-rank matrix approximation remains an open challenge. We introduce the Error-Powered Sketched Inverse Iteration (EPSI) Method via run sketched Newton iteration for the Lagrange form as a sketch-and-precondition variant for randomized low-rank approximation. Our method achieves theoretical guarantees, including a convergence rate that improves at least linearly with the sketch size.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models</title>
<link>https://arxiv.org/abs/2502.09604</link>
<guid>https://arxiv.org/abs/2502.09604</guid>
<content:encoded><![CDATA[
arXiv:2502.09604v2 Announce Type: replace-cross 
Abstract: We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks. The source code is available at https://github.com/facebookresearch/SelfCite
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Integration Strategies in Autonomous Vehicle Prediction and Planning: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2502.10477</link>
<guid>https://arxiv.org/abs/2502.10477</guid>
<content:encoded><![CDATA[
arXiv:2502.10477v2 Announce Type: replace-cross 
Abstract: This comprehensive survey examines the integration of knowledge-based approaches in autonomous driving systems, specifically focusing on trajectory prediction and planning. We extensively analyze various methodologies for incorporating domain knowledge, traffic rules, and commonsense reasoning into autonomous driving systems. The survey categorizes and analyzes approaches based on their knowledge representation and integration methods, ranging from purely symbolic to hybrid neuro-symbolic architectures. We examine recent developments in logic programming, foundation models for knowledge representation, reinforcement learning frameworks, and other emerging technologies incorporating domain knowledge. This work systematically reviews recent approaches, identifying key challenges, opportunities, and future research directions in knowledge-enhanced autonomous driving systems. Our analysis reveals emerging trends in the field, including the increasing importance of interpretable AI, the role of formal verification in safety-critical systems, and the potential of hybrid approaches that combine traditional knowledge representation with modern machine learning techniques.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferring Textual Preferences to Vision-Language Understanding through Model Merging</title>
<link>https://arxiv.org/abs/2502.13487</link>
<guid>https://arxiv.org/abs/2502.13487</guid>
<content:encoded><![CDATA[
arXiv:2502.13487v2 Announce Type: replace-cross 
Abstract: Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is computationally expensive. This paper explores a training-free alternative by merging text-based reward models (RMs) with LVLMs to create VLRMs. Our approach shows that integrating these models leads to improved performance over LVLMs' scoring and text-based RMs, offering an efficient method for incorporating textual preferences into LVLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-ICL Lab: A Synthetic Framework for Studying Chain-of-Thought Learning from In-Context Demonstrations</title>
<link>https://arxiv.org/abs/2502.15132</link>
<guid>https://arxiv.org/abs/2502.15132</guid>
<content:encoded><![CDATA[
arXiv:2502.15132v3 Announce Type: replace-cross 
Abstract: We introduce CoT-ICL Lab, a framework and methodology to generate synthetic tokenized datasets and systematically study chain-of-thought (CoT) in-context learning (ICL) in language models. CoT-ICL Lab allows fine grained control over the complexity of in-context examples by decoupling (1) the causal structure involved in chain token generation from (2) the underlying token processing functions. We train decoder-only transformers (up to 700M parameters) on these datasets and show that CoT accelerates the accuracy transition to higher values across model sizes. In particular, we find that model depth is crucial for leveraging CoT with limited in-context examples, while more examples help shallow models match deeper model performance. Additionally, limiting the diversity of token processing functions throughout training improves causal structure learning via ICL. We also interpret these transitions by analyzing transformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a simple yet powerful testbed for theoretical and empirical insights into ICL and CoT in language models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-Sample Analysis of Policy Evaluation for Robust Average Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.16816</link>
<guid>https://arxiv.org/abs/2502.16816</guid>
<content:encoded><![CDATA[
arXiv:2502.16816v2 Announce Type: replace-cross 
Abstract: We present the first finite-sample analysis for policy evaluation in robust average-reward Markov Decision Processes (MDPs). Prior works in this setting have established only asymptotic convergence guarantees, leaving open the question of sample complexity. In this work, we address this gap by establishing that the robust Bellman operator is a contraction under the span semi-norm, and developing a stochastic approximation framework with controlled bias. Our approach builds upon Multi-Level Monte Carlo (MLMC) techniques to estimate the robust Bellman operator efficiently. To overcome the infinite expected sample complexity inherent in standard MLMC, we introduce a truncation mechanism based on a geometric distribution, ensuring a finite constant sample complexity while maintaining a small bias that decays exponentially with the truncation level. Our method achieves the order-optimal sample complexity of $\tilde{\mathcal{O}}(\epsilon^{-2})$ for robust policy evaluation and robust average reward estimation, marking a significant advancement in robust reinforcement learning theory.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Function-Space Learning Rates</title>
<link>https://arxiv.org/abs/2502.17405</link>
<guid>https://arxiv.org/abs/2502.17405</guid>
<content:encoded><![CDATA[
arXiv:2502.17405v2 Announce Type: replace-cross 
Abstract: We consider layerwise function-space learning rates, which measure the magnitude of the change in a neural network's output function in response to an update to a parameter tensor. This contrasts with traditional learning rates, which describe the magnitude of changes in parameter space. We develop efficient methods to measure and set function-space learning rates in arbitrary neural networks, requiring only minimal computational overhead through a few additional backward passes that can be performed at the start of, or periodically during, training. We demonstrate two key applications: (1) analysing the dynamics of standard neural network optimisers in function space, rather than parameter space, and (2) introducing FLeRM (Function-space Learning Rate Matching), a novel approach to hyperparameter transfer across model scales. FLeRM records function-space learning rates while training a small, cheap base model, then automatically adjusts parameter-space layerwise learning rates when training larger models to maintain consistent function-space updates. FLeRM gives hyperparameter transfer across model width, depth, initialisation scale, and LoRA rank in various architectures including MLPs with residual connections and transformers with different layer normalisation schemes.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progressive Local Alignment for Medical Multimodal Pre-training</title>
<link>https://arxiv.org/abs/2502.18047</link>
<guid>https://arxiv.org/abs/2502.18047</guid>
<content:encoded><![CDATA[
arXiv:2502.18047v2 Announce Type: replace-cross 
Abstract: Local alignment between medical images and text is essential for accurate diagnosis, though it remains challenging due to the absence of natural local pairings and the limitations of rigid region recognition methods. Traditional approaches rely on hard boundaries, which introduce uncertainty, whereas medical imaging demands flexible soft region recognition to handle irregular structures. To overcome these challenges, we propose the Progressive Local Alignment Network (PLAN), which designs a novel contrastive learning-based approach for local alignment to establish meaningful word-pixel relationships and introduces a progressive learning strategy to iteratively refine these relationships, enhancing alignment precision and robustness. By combining these techniques, PLAN effectively improves soft region recognition while suppressing noise interference. Extensive experiments on multiple medical datasets demonstrate that PLAN surpasses state-of-the-art methods in phrase grounding, image-text retrieval, object detection, and zero-shot classification, setting a new benchmark for medical image-text alignment.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2502.19918</link>
<guid>https://arxiv.org/abs/2502.19918</guid>
<content:encoded><![CDATA[
arXiv:2502.19918v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) increasingly rely on prolonged reasoning chains to solve complex tasks. However, this trial-and-error approach often leads to high computational overhead and error propagation, where early mistakes can derail subsequent steps. To address these issues, we introduce Meta-Reasoner, a framework that dynamically optimizes inference-time reasoning by enabling LLMs to \enquote{think about how to think.} Drawing inspiration from human meta-cognition and dual-process theory, Meta-Reasoner operates as a strategic advisor, decoupling high-level guidance from step-by-step generation. It employs contextual multi-armed bandits to iteratively evaluate reasoning progress and select optimal strategies (e.g., backtrack, clarify ambiguity, restart from scratch, or propose alternative approaches), and reallocates computational resources toward the most promising paths. Our evaluations on mathematical reasoning and puzzles highlight the potential of dynamic reasoning chains to overcome inherent challenges in the LLM reasoning process and also show promise in broader applications, offering a scalable and adaptable solution for reasoning-intensive tasks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization</title>
<link>https://arxiv.org/abs/2503.04598</link>
<guid>https://arxiv.org/abs/2503.04598</guid>
<content:encoded><![CDATA[
arXiv:2503.04598v3 Announce Type: replace-cross 
Abstract: Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the position of layer normalization. While Pre-Norm structures facilitate more stable training owing to their stronger identity path, they often lead to suboptimal performance compared to Post-Norm. In this paper, we propose $\textbf{HybridNorm}$, a simple yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. We provide both theoretical insights and empirical evidence demonstrating that HybridNorm improves gradient flow and model robustness. Extensive experiments on large-scale transformer models, including both dense and sparse variants, show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches across multiple benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. Code is available at https://github.com/BryceZhuo/HybridNorm.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence</title>
<link>https://arxiv.org/abs/2503.14749</link>
<guid>https://arxiv.org/abs/2503.14749</guid>
<content:encoded><![CDATA[
arXiv:2503.14749v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are increasingly used for factual question-answering, it becomes more important for LLMs to have the capability to communicate the likelihood that their answer is correct. For these verbalized expressions of uncertainty to be meaningful, they should reflect the error rates at the expressed level of confidence. However, when prompted to express confidence, the error rates of current LLMs are inconsistent with their communicated confidences, highlighting the need for uncertainty quantification methods. Many prior methods calculate lexical uncertainty, estimating a model's confidence in the specific string it generated. In some cases, however, it may be more useful to estimate semantic uncertainty, or the model's confidence in the answer regardless of how it is verbalized. We propose a simple procedure, uncertainty distillation, to teach an LLM to verbalize calibrated semantic confidences. Using held-out data to map initial uncertainty estimates to meaningful probabilities, we create examples annotated with verbalized probabilities for supervised fine-tuning. We compare uncertainty distillation to several strong baselines, and find that our method yields verbalized confidences that correlate well with observed error rates.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models</title>
<link>https://arxiv.org/abs/2503.20576</link>
<guid>https://arxiv.org/abs/2503.20576</guid>
<content:encoded><![CDATA[
arXiv:2503.20576v2 Announce Type: replace-cross 
Abstract: In this work, we explore the potential of large language models (LLMs) for generating functional test scripts, which necessitates understanding the dynamically evolving code structure of the target software. To achieve this, we propose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e., retrieve, reuse, revise, and retain), which maintains and leverages a case bank of test intent descriptions and corresponding test scripts to facilitate LLMs for test script generation. To improve user experience further, we introduce Re4, an optimization method for the CBR system, comprising reranking-based retrieval finetuning and reinforced reuse finetuning. Specifically, we first identify positive examples with high semantic and script similarity, providing reliable pseudo-labels for finetuning the retriever model without costly labeling. Then, we apply supervised finetuning, followed by a reinforcement learning finetuning stage, to align LLMs with our production scenarios, ensuring the faithful reuse of retrieved cases. Extensive experimental results on two product development units from Huawei Datacom demonstrate the superiority of the proposed CBR+Re4. Notably, we also show that the proposed Re4 method can help alleviate the repetitive generation issues with LLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Fine-Grained Detection of AI Generated Texts</title>
<link>https://arxiv.org/abs/2504.11952</link>
<guid>https://arxiv.org/abs/2504.11952</guid>
<content:encoded><![CDATA[
arXiv:2504.11952v2 Announce Type: replace-cross 
Abstract: An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators</title>
<link>https://arxiv.org/abs/2504.15253</link>
<guid>https://arxiv.org/abs/2504.15253</guid>
<content:encoded><![CDATA[
arXiv:2504.15253v2 Announce Type: replace-cross 
Abstract: Scaling test-time computation, or affording a generator large language model (LLM) extra compute during inference, typically employs the help of external non-generative evaluators (i.e., reward models). Concurrently, LLM-judges, models trained to generate evaluations and critiques (explanations) in natural language, are becoming increasingly popular in automatic evaluation. Despite judge empirical successes, their effectiveness as evaluators in test-time scaling settings is largely unknown. In this paper, we introduce the Judge Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge performance in three domains (math reasoning, code generation, and instruction following) under three task settings: response reranking, step-level beam search, and critique-based response refinement. We evaluate 10 different judge models (7B-70B parameters) for 8 different base generator models (6.7B-72B parameters). Our benchmark shows that while judges are competitive with outcome reward models in reranking, they are consistently worse than process reward models in beam search procedures. Furthermore, though unique to LLM-judges, their natural language critiques are currently ineffective in guiding the generator towards better responses.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTRL: Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.16084</link>
<guid>https://arxiv.org/abs/2504.16084</guid>
<content:encoded><![CDATA[
arXiv:2504.16084v2 Announce Type: replace-cross 
Abstract: This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 211% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the maj@n metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model maj@n, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spark: A System for Scientifically Creative Idea Generation</title>
<link>https://arxiv.org/abs/2504.20090</link>
<guid>https://arxiv.org/abs/2504.20090</guid>
<content:encoded><![CDATA[
arXiv:2504.20090v2 Announce Type: replace-cross 
Abstract: Recently, large language models (LLMs) have shown promising abilities to generate novel research ideas in science, a direction which coincides with many foundational principles in computational creativity (CC). In light of these developments, we present an idea generation system named Spark that couples retrieval-augmented idea generation using LLMs with a reviewer model named Judge trained on 600K scientific reviews from OpenReview. Our work is both a system demonstration and intended to inspire other CC researchers to explore grounding the generation and evaluation of scientific ideas within foundational CC principles. To this end, we release the annotated dataset used to train Judge, inviting other researchers to explore the use of LLMs for idea generation and creative evaluations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimation of discrete distributions in relative entropy, and the deviations of the missing mass</title>
<link>https://arxiv.org/abs/2504.21787</link>
<guid>https://arxiv.org/abs/2504.21787</guid>
<content:encoded><![CDATA[
arXiv:2504.21787v2 Announce Type: replace-cross 
Abstract: We study the problem of estimating a distribution over a finite alphabet from an i.i.d. sample, with accuracy measured in relative entropy (Kullback-Leibler divergence). While optimal expected risk bounds are known, high-probability guarantees remain less well-understood. First, we analyze the classical Laplace (add-one) estimator, obtaining matching upper and lower bounds on its performance and showing its optimality among confidence-independent estimators. We then characterize the minimax-optimal high-probability risk, which is attained via a simple confidence-dependent smoothing technique. Interestingly, the optimal non-asymptotic risk exhibits an additional logarithmic factor over the ideal asymptotic risk. Next, motivated by scenarios where the alphabet exceeds the sample size, we investigate methods that adapt to the sparsity of the distribution at hand. We introduce an estimator using data-dependent smoothing, for which we establish a high-probability risk bound depending on two effective sparsity parameters. As part of the analysis, we also derive a sharp high-probability upper bound on the missing mass.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SA-GAT-SR: Self-Adaptable Graph Attention Networks with Symbolic Regression for high-fidelity material property prediction</title>
<link>https://arxiv.org/abs/2505.00625</link>
<guid>https://arxiv.org/abs/2505.00625</guid>
<content:encoded><![CDATA[
arXiv:2505.00625v3 Announce Type: replace-cross 
Abstract: Recent advances in machine learning have demonstrated an enormous utility of deep learning approaches, particularly Graph Neural Networks (GNNs) for materials science. These methods have emerged as powerful tools for high-throughput prediction of material properties, offering a compelling enhancement and alternative to traditional first-principles calculations. While the community has predominantly focused on developing increasingly complex and universal models to enhance predictive accuracy, such approaches often lack physical interpretability and insights into materials behavior. Here, we introduce a novel computational paradigm, Self-Adaptable Graph Attention Networks integrated with Symbolic Regression (SA-GAT-SR), that synergistically combines the predictive capability of GNNs with the interpretative power of symbolic regression. Our framework employs a self-adaptable encoding algorithm that automatically identifies and adjust attention weights so as to screen critical features from an expansive 180-dimensional feature space while maintaining O(n) computational scaling. The integrated SR module subsequently distills these features into compact analytical expressions that explicitly reveal quantum-mechanically meaningful relationships, achieving 23 times acceleration compared to conventional SR implementations that heavily rely on first principle calculations-derived features as input. This work suggests a new framework in computational materials science, bridging the gap between predictive accuracy and physical interpretability, offering valuable physical insights into material behavior.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models</title>
<link>https://arxiv.org/abs/2505.03176</link>
<guid>https://arxiv.org/abs/2505.03176</guid>
<content:encoded><![CDATA[
arXiv:2505.03176v2 Announce Type: replace-cross 
Abstract: Current self-supervised algorithms commonly rely on transformations such as data augmentation and masking to learn visual representations. This is achieved by enforcing invariance or equivariance with respect to these transformations after encoding two views of an image. This dominant two-view paradigm often limits the flexibility of learned representations for downstream adaptation by creating performance trade-offs between high-level invariance-demanding tasks such as image classification and more fine-grained equivariance-related tasks. In this work, we proposes \emph{seq-JEPA}, a world modeling framework that introduces architectural inductive biases into joint-embedding predictive architectures to resolve this trade-off. Without relying on dual equivariance predictors or loss terms, seq-JEPA simultaneously learns two architecturally segregated representations: one equivariant to specified transformations and another invariant to them. To do so, our model processes short sequences of different views (observations) of inputs. Each encoded view is concatenated with an embedding of the relative transformation (action) that produces the next observation in the sequence. These view-action pairs are passed through a transformer encoder that outputs an aggregate representation. A predictor head then conditions this aggregate representation on the upcoming action to predict the representation of the next observation. Empirically, seq-JEPA demonstrates strong performance on both equivariant and invariant benchmarks without sacrificing one for the other. Furthermore, it excels at tasks that inherently require aggregating a sequence of observations, such as path integration across actions and predictive learning across eye movements.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Online Decision-Making: A Unified Framework</title>
<link>https://arxiv.org/abs/2505.07101</link>
<guid>https://arxiv.org/abs/2505.07101</guid>
<content:encoded><![CDATA[
arXiv:2505.07101v3 Announce Type: replace-cross 
Abstract: Contextual online decision-making problems with constraints appear in a wide range of real-world applications, such as adaptive experimental design under safety constraints, personalized recommendation with resource limits, and dynamic pricing under fairness requirements. In this paper, we investigate a general formulation of sequential decision-making with stage-wise feasibility constraints, where at each round, the learner must select an action based on observed context while ensuring that a problem-specific feasibility criterion is satisfied. We propose a unified algorithmic framework that captures many existing constrained learning problems, including constrained bandits, active learning with label budgets, online hypothesis testing with Type I error control, and model calibration. Central to our approach is the concept of upper counterfactual confidence bounds, which enables the design of practically efficient online algorithms with strong theoretical guarantees using any offline conditional density estimation oracle. To handle feasibility constraints in complex environments, we introduce a generalized notion of the eluder dimension, extending it from the classical setting based on square loss to a broader class of metric-like probability divergences. This allows us to capture the complexity of various density function classes and characterize the utility regret incurred due to feasibility constraint uncertainty. Our result offers a principled foundation for constrained sequential decision-making in both theory and practice.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Sparse Autoencoders Useful for Java Function Bug Detection?</title>
<link>https://arxiv.org/abs/2505.10375</link>
<guid>https://arxiv.org/abs/2505.10375</guid>
<content:encoded><![CDATA[
arXiv:2505.10375v2 Announce Type: replace-cross 
Abstract: Software vulnerabilities such as buffer overflows and SQL injections are a major source of security breaches. Traditional methods for vulnerability detection remain essential but are limited by high false positive rates, scalability issues, and reliance on manual effort. These constraints have driven interest in AI-based approaches to automated vulnerability detection and secure code generation. While Large Language Models (LLMs) have opened new avenues for classification tasks, their complexity and opacity pose challenges for interpretability and deployment. Sparse Autoencoder offer a promising solution to this problem. We explore whether SAEs can serve as a lightweight, interpretable alternative for bug detection in Java functions. We evaluate the effectiveness of SAEs when applied to representations from GPT-2 Small and Gemma 2B, examining their capacity to highlight buggy behaviour without fine-tuning the underlying LLMs. We found that SAE-derived features enable bug detection with an F1 score of up to 89%, consistently outperforming fine-tuned transformer encoder baselines. Our work provides the first empirical evidence that SAEs can be used to detect software bugs directly from the internal representations of pretrained LLMs, without any fine-tuning or task-specific supervision.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoE-World: Compositional World Modeling with Products of Programmatic Experts</title>
<link>https://arxiv.org/abs/2505.10819</link>
<guid>https://arxiv.org/abs/2505.10819</guid>
<content:encoded><![CDATA[
arXiv:2505.10819v2 Announce Type: replace-cross 
Abstract: Learning how the world works is central to building AI agents that can adapt to complex environments. Traditional world models based on deep learning demand vast amounts of training data, and do not flexibly update their knowledge from sparse observations. Recent advances in program synthesis using Large Language Models (LLMs) give an alternate approach which learns world models represented as source code, supporting strong generalization from little data. To date, application of program-structured world models remains limited to natural language and grid-world domains. We introduce a novel program synthesis method for effectively modeling complex, non-gridworld domains by representing a world model as an exponentially-weighted product of programmatic experts (PoE-World) synthesized by LLMs. We show that this approach can learn complex, stochastic world models from just a few observations. We evaluate the learned world models by embedding them in a model-based planning agent, demonstrating efficient performance and generalization to unseen levels on Atari's Pong and Montezuma's Revenge. We release our code and display the learned world models and videos of the agent's gameplay at https://topwasu.github.io/poe-world.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Models Can Generalize Also When Trained on Random Labels</title>
<link>https://arxiv.org/abs/2505.11006</link>
<guid>https://arxiv.org/abs/2505.11006</guid>
<content:encoded><![CDATA[
arXiv:2505.11006v2 Announce Type: replace-cross 
Abstract: The success of unsupervised learning raises the question of whether also supervised models can be trained without using the information in the output $y$. In this paper, we demonstrate that this is indeed possible. The key step is to formulate the model as a smoother, i.e. on the form $\hat{f}=Sy$, and to construct the smoother matrix $S$ independently of $y$, e.g. by training on random labels. We present a simple model selection criterion based on the distribution of the out-of-sample predictions and show that, in contrast to cross-validation, this criterion can be used also without access to $y$. We demonstrate on real and synthetic data that $y$-free trained versions of linear and kernel ridge regression, smoothing splines, and neural networks perform similarly to their standard, $y$-based, versions and, most importantly, significantly better than random guessing.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation</title>
<link>https://arxiv.org/abs/2505.11628</link>
<guid>https://arxiv.org/abs/2505.11628</guid>
<content:encoded><![CDATA[
arXiv:2505.11628v2 Announce Type: replace-cross 
Abstract: Supervised fine-tuning (SFT) using expert demonstrations often suffer from the imitation problem, where the model learns to reproduce the correct responses without understanding the underlying rationale. To address this limitation, we propose Critique-Guided Distillation (CGD), a novel multi-stage framework that integrates teacher model generated explanatory critiques and refined responses into the SFT process. A student model is then trained to map the triplet of prompt, teacher critique, and its own initial response to the corresponding refined teacher response, thereby learning both what to imitate and why. Using entropy-based analysis, we show that CGD reduces refinement uncertainty and can be interpreted as a Bayesian posterior update. We perform extensive empirical evaluation of CGD, on variety of benchmark tasks, and demonstrate significant gains on both math (AMC23 +17.5%) and language understanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format drift issues observed in previous critique fine-tuning (CFT) techniques.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMAC: A Broad Optimization Framework for LLM-Based Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2505.11765</link>
<guid>https://arxiv.org/abs/2505.11765</guid>
<content:encoded><![CDATA[
arXiv:2505.11765v2 Announce Type: replace-cross 
Abstract: Agents powered by advanced large language models (LLMs) have demonstrated impressive capabilities across diverse complex applications. Recently, Multi-Agent Systems (MAS), wherein multiple agents collaborate and communicate with each other, have exhibited enhanced capabilities in complex tasks, such as high-quality code generation and arithmetic reasoning. However, the development of such systems often relies on handcrafted methods, and the literature on systematic design and optimization of LLM-based MAS remains limited.
  In this work, we introduce OMAC, a general framework designed for holistic optimization of LLM-based MAS. Specifically, we identify five key optimization dimensions for MAS, encompassing both agent functionality and collaboration structure. Building upon these dimensions, we first propose a general algorithm, utilizing two actors termed the Semantic Initializer and the Contrastive Comparator, to optimize any single dimension. Then, we present an algorithm for joint optimization across multiple dimensions. Extensive experiments demonstrate the superior performance of OMAC on code generation, arithmetic reasoning, and general reasoning tasks against state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do different prompting methods yield a common task representation in language models?</title>
<link>https://arxiv.org/abs/2505.12075</link>
<guid>https://arxiv.org/abs/2505.12075</guid>
<content:encoded><![CDATA[
arXiv:2505.12075v2 Announce Type: replace-cross 
Abstract: Demonstrations and instructions are two primary approaches for prompting language models to perform in-context learning (ICL) tasks. Do identical tasks elicited in different ways result in similar representations of the task? An improved understanding of task representation mechanisms would offer interpretability insights and may aid in steering models. We study this through \textit{function vectors} (FVs), recently proposed as a mechanism to extract few-shot ICL task representations. We generalize FVs to alternative task presentations, focusing on short textual instruction prompts, and successfully extract instruction function vectors that promote zero-shot task accuracy. We find evidence that demonstration- and instruction-based function vectors leverage different model components, and offer several controls to dissociate their contributions to task performance. Our results suggest that different task promptings forms do not induce a common task representation through FVs but elicit different, partly overlapping mechanisms. Our findings offer principled support to the practice of combining instructions and task demonstrations, imply challenges in universally monitoring task inference across presentation forms, and encourage further examinations of LLM task inference mechanisms.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Merging in Pre-training of Large Language Models</title>
<link>https://arxiv.org/abs/2505.12082</link>
<guid>https://arxiv.org/abs/2505.12082</guid>
<content:encoded><![CDATA[
arXiv:2505.12082v3 Announce Type: replace-cross 
Abstract: Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Neural Granger Causality with xLSTMs: Unveiling Temporal Dependencies in Complex Data</title>
<link>https://arxiv.org/abs/2502.09981</link>
<guid>https://arxiv.org/abs/2502.09981</guid>
<content:encoded><![CDATA[
<div> Granger causality, time series analysis, extended Long Short-Term Memory (xLSTM), sparsity enforcement, dynamic loss penalty

Summary:
Granger causality analysis in time series can be challenging due to non-linear dependencies. The proposed Granger causal xLSTM (GC-xLSTM) model utilizes the extended Long Short-Term Memory architecture to capture long-range relations between variables. By enforcing sparsity through a dynamic loss penalty on the initial projection, the model identifies and improves sparsity candidates. Through joint optimization, Granger causal relations are robustly recovered. Experimental evaluation on six datasets showcases the effectiveness of the GC-xLSTM model, demonstrating its potential in analyzing and predicting causal relationships between time series variables.<br /><br />Summary: <div>
arXiv:2502.09981v4 Announce Type: replace 
Abstract: Causality in time series can be difficult to determine, especially in the presence of non-linear dependencies. The concept of Granger causality helps analyze potential relationships between variables, thereby offering a method to determine whether one time series can predict - Granger cause - future values of another. Although successful, Granger causal methods still struggle with capturing long-range relations between variables. To this end, we leverage the recently successful Extended Long Short-Term Memory (xLSTM) architecture and propose Granger causal xLSTMs (GC-xLSTM). It first enforces sparsity between the time series components by using a novel dynamic loss penalty on the initial projection. Specifically, we adaptively improve the model and identify sparsity candidates. Our joint optimization procedure then ensures that the Granger causal relations are recovered robustly. Our experimental evaluation on six diverse datasets demonstrates the overall efficacy of our proposed GC-xLSTM model.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Path to Universal Neural Cellular Automata</title>
<link>https://arxiv.org/abs/2505.13058</link>
<guid>https://arxiv.org/abs/2505.13058</guid>
<content:encoded><![CDATA[
<div> Cellular automata, continuous domains, universal computation, neural cellular automata, gradient descent <br />
Summary: 
This work explores the potential of neural cellular automata to achieve universal computation in a continuous setting. By training neural cellular automata through gradient descent, fundamental computational primitives such as matrix multiplication and transposition are successfully learned. The model is able to emulate a neural network solving the MNIST digit classification task within the cellular automata state. These results represent a foundational step towards creating analog general-purpose computers and provide insights into universal computation in continuous dynamics. The automated discovery of complex cellular automata behaviors through machine learning is advanced through this research. <div>
arXiv:2505.13058v2 Announce Type: replace 
Abstract: Cellular automata have long been celebrated for their ability to generate complex behaviors from simple, local rules, with well-known discrete models like Conway's Game of Life proven capable of universal computation. Recent advancements have extended cellular automata into continuous domains, raising the question of whether these systems retain the capacity for universal computation. In parallel, neural cellular automata have emerged as a powerful paradigm where rules are learned via gradient descent rather than manually designed. This work explores the potential of neural cellular automata to develop a continuous Universal Cellular Automaton through training by gradient descent. We introduce a cellular automaton model, objective functions and training strategies to guide neural cellular automata toward universal computation in a continuous setting. Our experiments demonstrate the successful training of fundamental computational primitives - such as matrix multiplication and transposition - culminating in the emulation of a neural network solving the MNIST digit classification task directly within the cellular automata state. These results represent a foundational step toward realizing analog general-purpose computers, with implications for understanding universal computation in continuous dynamics and advancing the automated discovery of complex cellular automata behaviors via machine learning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning and Pretrain-Finetuning</title>
<link>https://arxiv.org/abs/2505.13317</link>
<guid>https://arxiv.org/abs/2505.13317</guid>
<content:encoded><![CDATA[
<div> SSL, pre-trained models, few-shot SSL, VLMs, data labeling <br />
<br />
Summary:  This study compares semi-supervised learning (SSL) and pre-trained models in scenarios with limited labeled data. The Few-shot SSL framework is introduced to enable a fair comparison between the two approaches by controlling the amount of labeled data. Extensive experiments show that pre-trained Vision-Language Models (VLMs) generally outperform SSL methods, especially in high-resolution data with clear semantic structure. The study suggests that future SSL research should consider comparing with pre-trained models and exploring ways to integrate pre-trained knowledge for enhanced performance, such as using it to improve pseudo-labeling. To support further research, the authors have released a unified reproduction and evaluation framework, with codes available for access. <div>
arXiv:2505.13317v3 Announce Type: replace 
Abstract: Semi-supervised learning (SSL) alleviates the cost of data labeling process by exploiting unlabeled data, and has achieved promising results on various tasks such as image classification. Meanwhile, the Pretrain-Finetuning paradigm has garnered significant attention in recent years, and exploiting pre-trained models could also reduce the requirement of labeled data in downstream tasks. Therefore, a question naturally occurs: \emph{When the labeled data is scarce in the target tasks, should we exploit unlabeled data or pre-trained models?} To answer this question, we select pre-trained Vision-Language Models (VLMs) as representative pretrain-finetuning instances and propose \textit{Few-shot SSL} -- a framework that enables fair comparison between these two paradigms by controlling the amount of labeled data used. Extensive experiments across various settings demonstrate that pre-trained VLMs generally outperform SSL methods in nearly all cases, except when the data has low resolution or lacks clear semantic structure. Therefore, we encourage future SSL research to compare with pre-trained models and explore deeper integration, such as using pre-trained knowledge to enhance pseudo-labeling. To support future research, we release our unified reproduction and evaluation framework. Codes are available \href{https://anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566 }{here}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZenFlow: Enabling Stall-Free Offloading Training via Asynchronous Updates</title>
<link>https://arxiv.org/abs/2505.12242</link>
<guid>https://arxiv.org/abs/2505.12242</guid>
<content:encoded><![CDATA[
<div> ZenFlow, offloading framework, important parameters, GPU stalls, PCIe transfers <br />
Summary: 
ZenFlow is a novel offloading framework designed to fine-tune large language models efficiently. It prioritizes important parameters and updates them on the GPU, while less important ones are offloaded to the CPU for asynchronous processing. This approach prevents severe GPU stalls and reduces PCIe traffic, resulting in up to a 5x speedup in training speed. ZenFlow also introduces a lightweight gradient selection method that exploits the locality of important gradients, avoiding the need for costly global synchronization when scaling across multiple GPUs. Overall, ZenFlow achieves significant performance improvements while maintaining model accuracy. <br /><br />Summary: <div>
arXiv:2505.12242v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) often exceeds GPU memory limits, prompting systems to offload model states to CPU memory. However, existing offloaded training frameworks like ZeRO-Offload treat all parameters equally and update the full model on the CPU, causing severe GPU stalls, where fast, expensive GPUs sit idle waiting for slow CPU updates and limited-bandwidth PCIe transfers. We present ZenFlow, a new offloading framework that prioritizes important parameters and decouples updates between GPU and CPU. ZenFlow performs in-place updates of important gradients on GPU, while asynchronously offloading and accumulating less important ones on CPU, fully overlapping CPU work with GPU computation. To scale across GPUs, ZenFlow introduces a lightweight gradient selection method that exploits a novel spatial and temporal locality property of important gradients, avoiding costly global synchronization. ZenFlow achieves up to 5x end-to-end speedup, 2x lower PCIe traffic, and reduces GPU stalls by over 85 percent, all while preserving accuracy.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Fractional Neural Operators: A Symmetrized Approach to Modeling Turbulence in Complex Fluid Dynamics</title>
<link>https://arxiv.org/abs/2505.14700</link>
<guid>https://arxiv.org/abs/2505.14700</guid>
<content:encoded><![CDATA[
<div> neural network operators, memory effects, randomness, fractional derivatives, stochastic perturbations
Summary: 
This work introduces a new class of neural network operators designed to address problems involving memory effects and randomness. These operators combine symmetrized activation functions, Caputo-type fractional derivatives, and stochastic perturbations through It\^o type noise, providing a powerful framework for approximating functions with long-term memory and uncertain dynamics. The mathematical foundations of these operators are developed, with key theorems proving their asymptotic behavior, convergence in the mean-square sense, and consistency under fractional regularity assumptions. The estimates explicitly consider the memory parameter alpha and noise level sigma. In an application to the fractional Navier-Stokes equations with stochastic forcing, commonly used to model turbulence in fluid flows with memory, the proposed theory offers theoretical guarantees on approximation quality. This research suggests that these neural operators can be valuable in analyzing and simulating complex systems, particularly in turbulent phenomena and multiscale processes where memory and randomness play crucial roles. By integrating concepts from neural networks, fractional calculus, and stochastic analysis, this work paves the way for hybrid learning-based methods supported by strong analytical foundations. 
<br /><br />Summary: <div>
arXiv:2505.14700v1 Announce Type: new 
Abstract: In this work, we introduce a new class of neural network operators designed to handle problems where memory effects and randomness play a central role. In this work, we introduce a new class of neural network operators designed to handle problems where memory effects and randomness play a central role. These operators merge symmetrized activation functions, Caputo-type fractional derivatives, and stochastic perturbations introduced via It\^o type noise. The result is a powerful framework capable of approximating functions that evolve over time with both long-term memory and uncertain dynamics. We develop the mathematical foundations of these operators, proving three key theorems of Voronovskaya type. These results describe the asymptotic behavior of the operators, their convergence in the mean-square sense, and their consistency under fractional regularity assumptions. All estimates explicitly account for the influence of the memory parameter $\alpha$ and the noise level $\sigma$. As a practical application, we apply the proposed theory to the fractional Navier-Stokes equations with stochastic forcing, a model often used to describe turbulence in fluid flows with memory. Our approach provides theoretical guarantees for the approximation quality and suggests that these neural operators can serve as effective tools in the analysis and simulation of complex systems. By blending ideas from neural networks, fractional calculus, and stochastic analysis, this research opens new perspectives for modeling turbulent phenomena and other multiscale processes where memory and randomness are fundamental. The results lay the groundwork for hybrid learning-based methods with strong analytical backing.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents</title>
<link>https://arxiv.org/abs/2505.14727</link>
<guid>https://arxiv.org/abs/2505.14727</guid>
<content:encoded><![CDATA[
<div> Keywords: alpha returns, AI systems, taxonomy, large language models, financial agents <br />
Summary:<br />
This paper introduces a comprehensive taxonomy that tracks the evolution of alpha return investment strategies from manual to AI-powered systems. It covers stages such as manual strategies, statistical models, classical machine learning, deep learning, and large language models. The focus is on the shift towards context-aware financial agents capable of real-time reasoning and decision-making. Challenges in interpretability, data fragility, governance, and regulatory compliance are highlighted. The taxonomy provides a framework for evaluating maturity and guiding the responsible development of next-generation alpha systems. <div>
arXiv:2505.14727v1 Announce Type: new 
Abstract: The pursuit of alpha returns that exceed market benchmarks has undergone a profound transformation, evolving from intuition-driven investing to autonomous, AI powered systems. This paper introduces a comprehensive five stage taxonomy that traces this progression across manual strategies, statistical models, classical machine learning, deep learning, and agentic architectures powered by large language models (LLMs). Unlike prior surveys focused narrowly on modeling techniques, this review adopts a system level lens, integrating advances in representation learning, multimodal data fusion, and tool augmented LLM agents. The strategic shift from static predictors to contextaware financial agents capable of real time reasoning, scenario simulation, and cross modal decision making is emphasized. Key challenges in interpretability, data fragility, governance, and regulatory compliance areas critical to production deployment are examined. The proposed taxonomy offers a unified framework for evaluating maturity, aligning infrastructure, and guiding the responsible development of next generation alpha systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute</title>
<link>https://arxiv.org/abs/2505.14733</link>
<guid>https://arxiv.org/abs/2505.14733</guid>
<content:encoded><![CDATA[
<div> compute resources, language models, inference, accuracy-energy trade-offs, sustainability <br />
Summary: 
This study introduces the concept of test-time compute (TTC) as a means to enhance the efficiency of large language models (LLMs). By allocating additional computational resources during inference, TTC shows potential to achieve better accuracy-energy trade-offs compared to simply increasing model size. The research demonstrates that TTC outperforms traditional model scaling, particularly in tasks requiring complex reasoning. An important finding is the impact of TTC performance on output sequence length, highlighting the value of adjusting compute resources based on query complexity. This approach could lead to more sustainable, accurate, and adaptable deployment of future language models without the need for additional pretraining costs. <div>
arXiv:2505.14733v1 Announce Type: new 
Abstract: Scaling large language models (LLMs) has driven significant advancements, yet it faces diminishing returns and escalating energy demands. This work introduces test-time compute (TTC)-allocating additional computational resources during inference-as a compelling complement to conventional scaling strategies. Specifically, we investigate whether employing TTC can achieve superior accuracy-energy trade-offs compared to simply increasing model size. Our empirical analysis reveals that TTC surpasses traditional model scaling in accuracy/energy efficiency, with notable gains in tasks demanding complex reasoning rather than mere factual recall. Further, we identify a critical interaction between TTC performance and output sequence length, demonstrating that strategically adjusting compute resources at inference time according to query complexity can substantially enhance efficiency. Our findings advocate for TTC as a promising direction, enabling more sustainable, accurate, and adaptable deployment of future language models without incurring additional pretraining costs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Multivariate Long-Term History Representation for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.14737</link>
<guid>https://arxiv.org/abs/2505.14737</guid>
<content:encoded><![CDATA[
<div> Keywords: Multivariate Time Series, Spatial-Temporal Graph Neural Network, Long-term History Encoder, Hierarchical Representation Retriever, Transformer-based Aggregator 

Summary:
The article introduces a novel framework, the Long-term Multivariate History Representation (LMHR) Enhanced STGNN, for Multivariate Time Series (MTS) forecasting. This framework addresses the limitation of current STGNNs in capturing long-term spatial-temporal dependencies and similarities across MTS data for accurate forecasting. It incorporates a Long-term History Encoder to encode long-term history, a Hierarchical Representation Retriever to include spatial information, and a Transformer-based Aggregator to selectively fuse contextual representations. Experimental results show that LMHR outperforms typical STGNNs by 10.72% on average prediction horizons and state-of-the-art methods by 4.12% on real-world datasets. It also improves prediction accuracy by 9.8% on rapidly changing patterns in the datasets. The proposed framework demonstrates superior performance in capturing long-term spatial-temporal correlations and improving prediction accuracy in MTS forecasting.<br /><br />Summary: <div>
arXiv:2505.14737v1 Announce Type: new 
Abstract: Multivariate Time Series (MTS) forecasting has a wide range of applications in both industry and academia. Recent advances in Spatial-Temporal Graph Neural Network (STGNN) have achieved great progress in modelling spatial-temporal correlations. Limited by computational complexity, most STGNNs for MTS forecasting focus primarily on short-term and local spatial-temporal dependencies. Although some recent methods attempt to incorporate univariate history into modeling, they still overlook crucial long-term spatial-temporal similarities and correlations across MTS, which are essential for accurate forecasting. To fill this gap, we propose a framework called the Long-term Multivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting. Specifically, a Long-term History Encoder (LHEncoder) is adopted to effectively encode the long-term history into segment-level contextual representations and reduce point-level noise. A non-parametric Hierarchical Representation Retriever (HRetriever) is designed to include the spatial information in the long-term spatial-temporal dependency modelling and pick out the most valuable representations with no additional training. A Transformer-based Aggregator (TAggregator) selectively fuses the sparsely retrieved contextual representations based on the ranking positional embedding efficiently. Experimental results demonstrate that LMHR outperforms typical STGNNs by 10.72% on the average prediction horizons and state-of-the-art methods by 4.12% on several real-world datasets. Additionally, it consistently improves prediction accuracy by 9.8% on the top 10% of rapidly changing patterns across the datasets.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs</title>
<link>https://arxiv.org/abs/2505.14739</link>
<guid>https://arxiv.org/abs/2505.14739</guid>
<content:encoded><![CDATA[
<div> Keywords: denoising diffusion probabilistic models, synthetic sensor signals, similarity metrics, training optimization, generative models 

Summary: 
Denoising diffusion probabilistic models have the ability to generate synthetic sensor signals by training them using a loss function that measures the difference between added noise and predicted noise. However, the randomness in the process and the loss function itself make it challenging to assess the data quality. To address this, the study explores various similarity metrics and adapts an existing metric to monitor the training and data synthesis processes. This adapted metric can be fine-tuned on input data to align with classification task requirements. By using this approach, the researchers were able to significantly reduce the number of training epochs without compromising classification performance, leading to resource savings and faster training of generative models. <br /><br />Summary: <div>
arXiv:2505.14739v1 Announce Type: new 
Abstract: Denoising diffusion probabilistic models are able to generate synthetic sensor signals. The training process of such a model is controlled by a loss function which measures the difference between the noise that was added in the forward process and the noise that was predicted by the diffusion model. This enables the generation of realistic data. However, the randomness within the process and the loss function itself makes it difficult to estimate the quality of the data. Therefore, we examine multiple similarity metrics and adapt an existing metric to overcome this issue by monitoring the training and synthetisation process using those metrics. The adapted metric can even be fine-tuned on the input data to comply with the requirements of an underlying classification task. We were able to significantly reduce the amount of training epochs without a performance reduction in the classification task. An optimized training process not only saves resources, but also reduces the time for training generative models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism</title>
<link>https://arxiv.org/abs/2505.14741</link>
<guid>https://arxiv.org/abs/2505.14741</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, generative models, parallelization, inference latency, communication efficiency 

Summary:
Diffusion models are powerful in generating various types of data but suffer from slow inference due to sequential denoising processes. Traditional parallelization methods have high communication overhead. The ParaStep method proposed in this study utilizes a reuse-then-predict mechanism to parallelize diffusion inference by capitalizing on similarity between adjacent denoising steps. Unlike existing approaches, ParaStep minimizes communication overhead by employing lightweight, step-wise communication. The results demonstrate significant speedups of up to 3.88x on SVD, 2.43x on CogVideoX-2b, and 6.56x on AudioLDM2-large while maintaining generation quality. ParaStep proves to be a scalable and communication-efficient solution for accelerating diffusion inference, especially in environments with limited bandwidth.

<br /><br />Summary: <div>
arXiv:2505.14741v1 Announce Type: new 
Abstract: Diffusion models have emerged as a powerful class of generative models across various modalities, including image, video, and audio synthesis. However, their deployment is often limited by significant inference latency, primarily due to the inherently sequential nature of the denoising process. While existing parallelization strategies attempt to accelerate inference by distributing computation across multiple devices, they typically incur high communication overhead, hindering deployment on commercial hardware. To address this challenge, we propose \textbf{ParaStep}, a novel parallelization method based on a reuse-then-predict mechanism that parallelizes diffusion inference by exploiting similarity between adjacent denoising steps. Unlike prior approaches that rely on layer-wise or stage-wise communication, ParaStep employs lightweight, step-wise communication, substantially reducing overhead. ParaStep achieves end-to-end speedups of up to \textbf{3.88}$\times$ on SVD, \textbf{2.43}$\times$ on CogVideoX-2b, and \textbf{6.56}$\times$ on AudioLDM2-large, while maintaining generation quality. These results highlight ParaStep as a scalable and communication-efficient solution for accelerating diffusion inference, particularly in bandwidth-constrained environments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis</title>
<link>https://arxiv.org/abs/2505.14742</link>
<guid>https://arxiv.org/abs/2505.14742</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, quantization, fine-tuning, outlier spatial stability hypothesis, efficiency

Summary:<br /><br />Large language models face challenges when deployed on resource-constrained personal devices due to high computational and memory demands for task-specific fine-tuning. In this paper, the Outlier Spatial Stability Hypothesis (OSSH) is introduced, suggesting that certain activation outlier channels retain stable spatial positions during fine-tuning iterations. Building on OSSH, the Quaff framework is proposed, which optimizes low-precision activation representations through targeted momentum scaling. Quaff dynamically suppresses outliers in invariant channels using lightweight operations, reducing quantization errors and memory usage. Experimental results across multiple benchmarks show that Quaff improves efficiency, performance, and deployability. On the GPQA reasoning benchmark, Quaff achieves latency reduction, memory savings, and accuracy enhancement over full-precision fine-tuning, allowing for personalized LLM deployment on consumer-grade GPUs like the RTX 2080 Super without compromising model utility.<br /><br /> <div>
arXiv:2505.14742v1 Announce Type: new 
Abstract: Large language models (LLMs) have made exciting achievements across various domains, yet their deployment on resource-constrained personal devices remains hindered by the prohibitive computational and memory demands of task-specific fine-tuning. While quantization offers a pathway to efficiency, existing methods struggle to balance performance and overhead, either incurring high computational/memory costs or failing to address activation outliers, a critical bottleneck in quantized fine-tuning. To address these challenges, we propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning, certain activation outlier channels retain stable spatial positions across training iterations. Building on OSSH, we propose Quaff, a Quantized parameter-efficient fine-tuning framework for LLMs, optimizing low-precision activation representations through targeted momentum scaling. Quaff dynamically suppresses outliers exclusively in invariant channels using lightweight operations, eliminating full-precision weight storage and global rescaling while reducing quantization errors. Extensive experiments across ten benchmarks validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory savings over full-precision fine-tuning while improving accuracy by 0.6% on the Phi-3 model, reconciling the triple trade-off between efficiency, performance, and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080 Super) without sacrificing model utility, Quaff democratizes personalized LLM deployment. The code is available at https://github.com/Little0o0/Quaff.git.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Prediction of the Mechanical Properties of Composites with CNNs</title>
<link>https://arxiv.org/abs/2505.14745</link>
<guid>https://arxiv.org/abs/2505.14745</guid>
<content:encoded><![CDATA[
<div> Composites, Finite Element Modelling, Convolutional Neural Networks, Explainable AI, Mechanical Properties <br />
Summary: <br />
Composites are widely used materials, necessitating accurate assessment of their mechanical properties through Finite Element (FE) modelling. Current FE models are computationally expensive, leading to the use of AI models, but existing approaches lack accuracy and transparency. This study introduces Convolutional Neural Networks (CNNs) with Explainable AI (XAI) methods to predict composite mechanical properties with high accuracy. Custom CNNs trained on transverse tension test data outperform a baseline model, ResNet-34, in estimating Young's modulus and yield strength. XAI methods such as SHAP and Integrated Gradients are employed to explain predictions, showcasing the critical geometrical features influencing composite behavior. This transparency allows engineers to validate model trustworthiness and aligns with the science of composites, enhancing understanding and application in various industries. <br /> <div>
arXiv:2505.14745v1 Announce Type: new 
Abstract: Composites are amongst the most important materials manufactured today, as evidenced by their use in countless applications. In order to establish the suitability of composites in specific applications, finite element (FE) modelling, a numerical method based on partial differential equations, is the industry standard for assessing their mechanical properties. However, FE modelling is exceptionally costly from a computational viewpoint, a limitation which has led to efforts towards applying AI models to this task. However, in these approaches: the chosen model architectures were rudimentary, feed-forward neural networks giving limited accuracy; the studies focus on predicting elastic mechanical properties, without considering material strength limits; and the models lacked transparency, hindering trustworthiness by users. In this paper, we show that convolutional neural networks (CNNs) equipped with methods from explainable AI (XAI) can be successfully deployed to solve this problem. Our approach uses customised CNNs trained on a dataset we generate using transverse tension tests in FE modelling to predict composites' mechanical properties, i.e., Young's modulus and yield strength. We show empirically that our approach achieves high accuracy, outperforming a baseline, ResNet-34, in estimating the mechanical properties. We then use SHAP and Integrated Gradients, two post-hoc XAI methods, to explain the predictions, showing that the CNNs use the critical geometrical features that influence the composites' behaviour, thus allowing engineers to verify that the models are trustworthy by representing the science of composites.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Causal GraphSAGE</title>
<link>https://arxiv.org/abs/2505.14748</link>
<guid>https://arxiv.org/abs/2505.14748</guid>
<content:encoded><![CDATA[
<div> GraphSAGE, causal inference, Cooperative Causal GraphSAGE, cooperative game theory, CoCa-sampling <br />
Summary: <br />
1. Introduction of causal inference enhances GraphSAGE to create Causal GraphSAGE, measuring causal weighting among individual nodes.
2. Cooperative Causal GraphSAGE (CoCa-GraphSAGE) integrates cooperative game theory with Causal GraphSAGE to consider cooperative relationships among sampling nodes.
3. A cooperative causal structure model is constructed to capture cooperation based on the graph structure.
4. CoCa-sampling algorithm, using Shapley values, calculates the cooperative contribution of node sets and guides node selection during neighborhood sampling.
5. The proposed method, CoCa-GraphSAGE, demonstrates comparable classification performance to existing methods and shows improved robustness under perturbations, validating the effectiveness of CoCa-sampling in generating stable target node embeddings. <div>
arXiv:2505.14748v1 Announce Type: new 
Abstract: GraphSAGE is a widely used graph neural network. The introduction of causal inference has improved its robust performance and named as Causal GraphSAGE. However, Causal GraphSAGE focuses on measuring causal weighting among individual nodes, but neglecting the cooperative relationships among sampling nodes as a whole. To address this issue, this paper proposes Cooperative Causal GraphSAGE (CoCa-GraphSAGE), which combines cooperative game theory with Causal GraphSAGE. Initially, a cooperative causal structure model is constructed in the case of cooperation based on the graph structure. Subsequently, Cooperative Causal sampling (CoCa-sampling) algorithm is proposed, employing the Shapley values to calculate the cooperative contribution based on causal weights of the nodes sets. CoCa-sampling guides the selection of nodes with significant cooperative causal effects during the neighborhood sampling process, thus integrating the selected neighborhood features under cooperative relationships, which takes the sampled nodes as a whole and generates more stable target node embeddings. Experiments on publicly available datasets show that the proposed method has comparable classification performance to the compared methods and outperforms under perturbations, demonstrating the robustness improvement by CoCa-sampling.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self Distillation via Iterative Constructive Perturbations</title>
<link>https://arxiv.org/abs/2505.14751</link>
<guid>https://arxiv.org/abs/2505.14751</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Neural Networks, cyclic optimization, Iterative Constructive Perturbation, self-distillation, training variations

Summary: 
The paper introduces a novel framework for training Deep Neural Networks that addresses the challenge of balancing performance and generalization. The key innovation is the Iterative Constructive Perturbation (ICP) technique, which optimizes both the model and its input data concurrently. By iteratively perturbing the input based on the model's loss, enhanced representations are gradually constructed. These enhanced features are then used in a self-distillation framework to improve performance and generalization. The approach alternately adjusts the model's parameters and the input data, bridging the gap between fitting and generalization. Experimental results show that the proposed method overcomes common performance bottlenecks in neural networks and achieves significant improvements across various training variations. Through this cyclic optimization strategy, the framework offers a new perspective on training deep neural networks for enhanced performance and generalization.<br /><br />Summary: <div>
arXiv:2505.14751v1 Announce Type: new 
Abstract: Deep Neural Networks have achieved remarkable achievements across various domains, however balancing performance and generalization still remains a challenge while training these networks. In this paper, we propose a novel framework that uses a cyclic optimization strategy to concurrently optimize the model and its input data for better training, rethinking the traditional training paradigm. Central to our approach is Iterative Constructive Perturbation (ICP), which leverages the model's loss to iteratively perturb the input, progressively constructing an enhanced representation over some refinement steps. This ICP input is then fed back into the model to produce improved intermediate features, which serve as a target in a self-distillation framework against the original features. By alternately altering the model's parameters to the data and the data to the model, our method effectively addresses the gap between fitting and generalization, leading to enhanced performance. Extensive experiments demonstrate that our approach not only mitigates common performance bottlenecks in neural networks but also demonstrates significant improvements across training variations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Data Synthesis</title>
<link>https://arxiv.org/abs/2505.14752</link>
<guid>https://arxiv.org/abs/2505.14752</guid>
<content:encoded><![CDATA[
<div> framework, data synthesis, Large Language Models, simulation, statistical alignment
Summary:<br />
The article introduces a novel framework, LLMSynthor, for data synthesis that leverages Large Language Models (LLMs) to capture the statistical structure of real-world distributions. LLMSynthor transforms LLMs into structure-aware simulators by utilizing them as nonparametric copula simulators to model high-order dependencies. It introduces LLM Proposal Sampling to enhance sampling efficiency and alignment with real data, without rejection sampling. The iterative synthesis loop minimizes discrepancies in summary statistics space, gradually uncovering and refining generative structures. Evaluations in controlled and real-world settings demonstrate the high statistical fidelity, practical utility, and cross-data adaptability of the synthetic data generated. LLMSynthor proves valuable in various domains such as economics, social science, and urban studies by producing synthetic data that accurately represents real-world distributions. <br /><br /> <div>
arXiv:2505.14752v1 Announce Type: new 
Abstract: Generating synthetic data that faithfully captures the statistical structure of real-world distributions is a fundamental challenge in data modeling. Classical approaches often depend on strong parametric assumptions or manual structural design and struggle in high-dimensional or heterogeneous domains. Recent progress in Large Language Models (LLMs) reveals their potential as flexible, high-dimensional priors over real-world distributions. However, when applied to data synthesis, standard LLM-based sampling is inefficient, constrained by fixed context limits, and fails to ensure statistical alignment. Given this, we introduce LLMSynthor, a general framework for data synthesis that transforms LLMs into structure-aware simulators guided by distributional feedback. LLMSynthor treats the LLM as a nonparametric copula simulator for modeling high-order dependencies and introduces LLM Proposal Sampling to generate grounded proposal distributions that improve sampling efficiency without requiring rejection. By minimizing discrepancies in the summary statistics space, the iterative synthesis loop aligns real and synthetic data while gradually uncovering and refining the latent generative structure. We evaluate LLMSynthor in both controlled and real-world settings using heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce, population, and mobility) that encompass both structured and unstructured formats. The synthetic data produced by LLMSynthor shows high statistical fidelity, practical utility, and cross-data adaptability, positioning it as a valuable tool across economics, social science, urban studies, and beyond.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization</title>
<link>https://arxiv.org/abs/2505.14756</link>
<guid>https://arxiv.org/abs/2505.14756</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian optimization, Large Language Models, contextual knowledge, surrogate modeling, exploration-exploitation trade-off 

Summary:
Bayesian optimization (BO) is a popular tool for optimizing costly black-box functions. Large Language Models (LLMs) have shown promise in low-data scenarios, making them attractive for black-box optimization. However, using LLMs alone poses risks due to their lack of explicit surrogate modeling and calibrated uncertainty. To address this, LLINBO combines LLMs with statistical surrogate models like Gaussian Processes to improve optimization reliability. This hybrid approach leverages LLMs for early exploration and statistical models for efficient exploitation. The paper introduces three mechanisms for collaboration and establishes theoretical guarantees for their effectiveness. A real-world application in 3D printing showcases the proposed framework's efficacy. The code for reproducing the results is available on GitHub. 

<br /><br />Summary: <div>
arXiv:2505.14756v1 Announce Type: new 
Abstract: Bayesian optimization (BO) is a sequential decision-making tool widely used for optimizing expensive black-box functions. Recently, Large Language Models (LLMs) have shown remarkable adaptability in low-data regimes, making them promising tools for black-box optimization by leveraging contextual knowledge to propose high-quality query points. However, relying solely on LLMs as optimization agents introduces risks due to their lack of explicit surrogate modeling and calibrated uncertainty, as well as their inherently opaque internal mechanisms. This structural opacity makes it difficult to characterize or control the exploration-exploitation trade-off, ultimately undermining theoretical tractability and reliability. To address this, we propose LLINBO: LLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with statistical surrogate experts (e.g., Gaussian Processes (GP)). The core philosophy is to leverage contextual reasoning strengths of LLMs for early exploration, while relying on principled statistical models to guide efficient exploitation. Specifically, we introduce three mechanisms that enable this collaboration and establish their theoretical guarantees. We end the paper with a real-life proof-of-concept in the context of 3D printing. The code to reproduce the results can be found at https://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Forecasting of Boarding Patient Counts to Address ED Overcrowding</title>
<link>https://arxiv.org/abs/2505.14765</link>
<guid>https://arxiv.org/abs/2505.14765</guid>
<content:encoded><![CDATA[
<div> forecasting, deep learning models, emergency department, operational decision-making, feature engineering

Summary:<br />
This study focuses on developing deep learning models to forecast the number of patients in the emergency department boarding phase six hours in advance. Data from various sources were used to train the models, which included ResNetPlus, TSTPlus, TSiTPlus, and N-BEATSx. The best-performing model was N-BEATSx, achieving high accuracy metrics such as mean absolute error, mean squared error, root mean squared error, and coefficient of determination. The model showed stability even during high boarding count periods. The results demonstrate that accurate forecasts can be made without using patient-level clinical data, and the inclusion of additional features further improved prediction stability during extreme conditions. This framework provides a practical and generalizable approach for hospital systems to anticipate boarding levels and address ED overcrowding.<br /> <div>
arXiv:2505.14765v1 Announce Type: new 
Abstract: This study develops deep learning models to forecast the number of patients in the emergency department (ED) boarding phase six hours in advance, aiming to support proactive operational decision-making using only non-clinical, operational, and contextual features. Data were collected from five sources: ED tracking systems, inpatient census records, weather reports, federal holiday calendars, and local event schedules. After feature engineering, the data were aggregated at an hourly level, cleaned, and merged into a unified dataset for model training. Several time series deep learning models, including ResNetPlus, TSTPlus, TSiTPlus (from the tsai library), and N-BEATSx, were trained using Optuna and grid search for hyperparameter tuning. The average ED boarding count was 28.7, with a standard deviation of 11.2. N-BEATSx achieved the best performance, with a mean absolute error of 2.10, mean squared error of 7.08, root mean squared error of 2.66, and a coefficient of determination of 0.95. The model maintained stable accuracy even during periods of extremely high boarding counts, defined as values exceeding one, two, or three standard deviations above the mean. Results show that accurate six-hour-ahead forecasts are achievable without using patient-level clinical data. While strong performance was observed even with a basic feature set, the inclusion of additional features improved prediction stability under extreme conditions. This framework offers a practical and generalizable approach for hospital systems to anticipate boarding levels and help mitigate ED overcrowding.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>This Time is Different: An Observability Perspective on Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2505.14766</link>
<guid>https://arxiv.org/abs/2505.14766</guid>
<content:encoded><![CDATA[
<div> keywords: Toto, time series forecasting, multivariate observability, benchmark, open source <br />
Summary: <br />
The article introduces Toto, a time series forecasting foundation model with 151 million parameters. Toto utilizes a decoder-only architecture with innovative design features tailored for multivariate observability time series data. It is pre-trained on a diverse corpus, including observability data, open datasets, and synthetic data, making it significantly larger than other leading models. A benchmark named BOOM, comprising 350 million observations across 2,807 real-world time series, was also introduced. The model's performance on both BOOM and existing time series forecasting benchmarks demonstrates state-of-the-art results. Toto and BOOM are based on telemetry and internal observability metrics from Datadog. The model weights, inference code, and evaluation scripts for Toto, as well as the data and evaluation code for BOOM, are available as open source under the Apache 2.0 License. <div>
arXiv:2505.14766v1 Announce Type: new 
Abstract: We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10$\times$ larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches</title>
<link>https://arxiv.org/abs/2505.14777</link>
<guid>https://arxiv.org/abs/2505.14777</guid>
<content:encoded><![CDATA[
<div> optimizer, neural networks, Kinetics-inspired, parameter diversity, optimization algorithm <br />
<br />
Summary: 
The paper introduces KO (Kinetics-inspired Optimizer), a novel optimizer for neural networks inspired by kinetic theory and partial differential equation simulations. The optimizer reimagines the training dynamics of network parameters as the evolution of a particle system governed by kinetic principles, promoting parameter diversity during optimization. This helps mitigate parameter condensation, preventing the collapse of network parameters into low-dimensional subspaces. The approach is rooted in physics and models stochastic particle collisions using a numerical scheme for the Boltzmann transport equation. Mathematical proof and a physical interpretation of the property are established. Extensive experiments on image and text classification tasks show that KO consistently outperforms baseline optimizers like Adam and SGD, achieving accuracy improvements with comparable computation cost. <div>
arXiv:2505.14777v1 Announce Type: new 
Abstract: The design of optimization algorithms for neural networks remains a critical challenge, with most existing methods relying on heuristic adaptations of gradient-based approaches. This paper introduces KO (Kinetics-inspired Optimizer), a novel neural optimizer inspired by kinetic theory and partial differential equation (PDE) simulations. We reimagine the training dynamics of network parameters as the evolution of a particle system governed by kinetic principles, where parameter updates are simulated via a numerical scheme for the Boltzmann transport equation (BTE) that models stochastic particle collisions. This physics-driven approach inherently promotes parameter diversity during optimization, mitigating the phenomenon of parameter condensation, i.e. collapse of network parameters into low-dimensional subspaces, through mechanisms analogous to thermal diffusion in physical systems. We analyze this property, establishing both a mathematical proof and a physical interpretation. Extensive experiments on image classification (CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks demonstrate that KO consistently outperforms baseline optimizers (e.g., Adam, SGD), achieving accuracy improvements while computation cost remains comparable.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text embedding models can be great data engineers</title>
<link>https://arxiv.org/abs/2505.14802</link>
<guid>https://arxiv.org/abs/2505.14802</guid>
<content:encoded><![CDATA[
<div> Keywords: data engineering pipelines, predictive analytics, text embeddings, variational information bottleneck, automated implementation

Summary:<br />
Data engineering pipelines play a crucial role in predictive analytics frameworks but require significant time and expertise. ADEPT is introduced as an automated data engineering pipeline utilizing text embeddings. By leveraging text embeddings to represent data sources and applying a variational information bottleneck criteria, ADEPT addresses issues such as missing data and irregular timestamps. The framework offers superior predictive performance across diverse datasets in healthcare, finance, science, and industrial internet of things applications. ADEPT outperforms existing benchmarks and streamlines the data pipeline process, showcasing potential for efficient automation in data science applications. <br /><br />Summary: <div>
arXiv:2505.14802v1 Announce Type: new 
Abstract: Data engineering pipelines are essential - albeit costly - components of predictive analytics frameworks requiring significant engineering time and domain expertise for carrying out tasks such as data ingestion, preprocessing, feature extraction, and feature engineering. In this paper, we propose ADEPT, an automated data engineering pipeline via text embeddings. At the core of the ADEPT framework is a simple yet powerful idea that the entropy of embeddings corresponding to textually dense raw format representation of time series can be intuitively viewed as equivalent (or in many cases superior) to that of numerically dense vector representations obtained by data engineering pipelines. Consequently, ADEPT uses a two step approach that (i) leverages text embeddings to represent the diverse data sources, and (ii) constructs a variational information bottleneck criteria to mitigate entropy variance in text embeddings of time series data. ADEPT provides an end-to-end automated implementation of predictive models that offers superior predictive performance despite issues such as missing data, ill-formed records, improper or corrupted data formats and irregular timestamps. Through exhaustive experiments, we show that the ADEPT outperforms the best existing benchmarks in a diverse set of datasets from large-scale applications across healthcare, finance, science and industrial internet of things. Our results show that ADEPT can potentially leapfrog many conventional data pipeline steps thereby paving the way for efficient and scalable automation pathways for diverse data science applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis</title>
<link>https://arxiv.org/abs/2505.14803</link>
<guid>https://arxiv.org/abs/2505.14803</guid>
<content:encoded><![CDATA[
<div> SurvUnc, uncertainty quantification, survival analysis, meta-model, reliability<br />
<br />
Summary:<br />
Survival analysis is crucial in various real-world applications, but the uncertainty of predictions from survival models remains a challenge. SurvUnc is introduced as a model-agnostic framework for post-hoc uncertainty quantification in survival models. It leverages concordance knowledge and pairwise ranking performance to effectively estimate uncertainty without requiring modifications to the underlying survival model. Through extensive experiments on benchmark datasets and various survival models, SurvUnc demonstrates superior performance in multiple evaluation scenarios, including selective prediction, misprediction detection, and out-of-domain detection. The results highlight SurvUnc's ability to enhance model interpretability and reliability, making survival predictions more trustworthy in high-stakes domains like healthcare and risk assessment. <br /> <div>
arXiv:2505.14803v1 Announce Type: new 
Abstract: Survival analysis, which estimates the probability of event occurrence over time from censored data, is fundamental in numerous real-world applications, particularly in high-stakes domains such as healthcare and risk assessment. Despite advances in numerous survival models, quantifying the uncertainty of predictions from these models remains underexplored and challenging. The lack of reliable uncertainty quantification limits the interpretability and trustworthiness of survival models, hindering their adoption in clinical decision-making and other sensitive applications. To bridge this gap, in this work, we introduce SurvUnc, a novel meta-model based framework for post-hoc uncertainty quantification for survival models. SurvUnc introduces an anchor-based learning strategy that integrates concordance knowledge into meta-model optimization, leveraging pairwise ranking performance to estimate uncertainty effectively. Notably, our framework is model-agnostic, ensuring compatibility with any survival model without requiring modifications to its architecture or access to its internal parameters. Especially, we design a comprehensive evaluation pipeline tailored to this critical yet overlooked problem. Through extensive experiments on four publicly available benchmarking datasets and five representative survival models, we demonstrate the superiority of SurvUnc across multiple evaluation scenarios, including selective prediction, misprediction detection, and out-of-domain detection. Our results highlight the effectiveness of SurvUnc in enhancing model interpretability and reliability, paving the way for more trustworthy survival predictions in real-world applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitation Learning via Focused Satisficing</title>
<link>https://arxiv.org/abs/2505.14820</link>
<guid>https://arxiv.org/abs/2505.14820</guid>
<content:encoded><![CDATA[
<div> Keywords: imitation learning, satisficing theory, deep reinforcement learning, policy, demonstrations

Summary:
Focused on satisficing theory, this study challenges the assumption in imitation learning that demonstrations are near-optimal. Instead, it proposes a focused satisficing approach that aims to surpass the demonstrator's personal levels of aspiration without explicitly learning them. By using a margin-based objective to guide deep reinforcement learning, the approach focuses the policy on imitating the highest quality parts of demonstrations. Experimental results show that this method outperforms existing imitation learning techniques by providing higher rates of guaranteed acceptability to the demonstrator while achieving competitive true returns in various environments. This approach allows for the identification of acceptable behaviors that may not be optimal but still meet the demonstrator's aspiration levels, leading to more robust imitation learning outcomes. 

<br /><br />Summary: <div>
arXiv:2505.14820v1 Announce Type: new 
Abstract: Imitation learning often assumes that demonstrations are close to optimal according to some fixed, but unknown, cost function. However, according to satisficing theory, humans often choose acceptable behavior based on their personal (and potentially dynamic) levels of aspiration, rather than achieving (near-) optimality. For example, a lunar lander demonstration that successfully lands without crashing might be acceptable to a novice despite being slow or jerky. Using a margin-based objective to guide deep reinforcement learning, our focused satisficing approach to imitation learning seeks a policy that surpasses the demonstrator's aspiration levels -- defined over trajectories or portions of trajectories -- on unseen demonstrations without explicitly learning those aspirations. We show experimentally that this focuses the policy to imitate the highest quality (portions of) demonstrations better than existing imitation learning methods, providing much higher rates of guaranteed acceptability to the demonstrator, and competitive true returns on a range of environments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation</title>
<link>https://arxiv.org/abs/2505.14821</link>
<guid>https://arxiv.org/abs/2505.14821</guid>
<content:encoded><![CDATA[
<div> model-based CTRL, sample efficiency, computational efficiency, optimism-based confidence sets, continuous control tasks

Summary:
In this work, a model-based continuous-time reinforcement learning (CTRL) algorithm is proposed to achieve both sample and computational efficiency. The approach uses optimism-based confidence sets to establish the first sample complexity guarantee for CTRL with general function approximation. The algorithm can learn a near-optimal policy with a suboptimality gap of $\tilde{O}(\sqrt{d_{\mathcal{R}} + d_{\mathcal{F}}}N^{-1/2})$ using $N$ measurements, where $d_{\mathcal{R}}$ and $d_{\mathcal{F}}$ are the distributional Eluder dimensions of the reward and dynamic functions, respectively. Structured policy updates and an alternative measurement strategy are introduced to reduce the number of policy updates and rollouts while maintaining sample efficiency. Experimental results on continuous control tasks and diffusion model fine-tuning demonstrate the effectiveness of the proposed algorithms, achieving comparable performance with significantly fewer policy updates and rollouts. 

<br /><br />Summary: <div>
arXiv:2505.14821v1 Announce Type: new 
Abstract: Continuous-time reinforcement learning (CTRL) provides a principled framework for sequential decision-making in environments where interactions evolve continuously over time. Despite its empirical success, the theoretical understanding of CTRL remains limited, especially in settings with general function approximation. In this work, we propose a model-based CTRL algorithm that achieves both sample and computational efficiency. Our approach leverages optimism-based confidence sets to establish the first sample complexity guarantee for CTRL with general function approximation, showing that a near-optimal policy can be learned with a suboptimality gap of $\tilde{O}(\sqrt{d_{\mathcal{R}} + d_{\mathcal{F}}}N^{-1/2})$ using $N$ measurements, where $d_{\mathcal{R}}$ and $d_{\mathcal{F}}$ denote the distributional Eluder dimensions of the reward and dynamic functions, respectively, capturing the complexity of general function approximation in reinforcement learning. Moreover, we introduce structured policy updates and an alternative measurement strategy that significantly reduce the number of policy updates and rollouts while maintaining competitive sample efficiency. We implemented experiments to backup our proposed algorithms on continuous control tasks and diffusion model fine-tuning, demonstrating comparable performance with significantly fewer policy updates and rollouts.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assimilative Causal Inference</title>
<link>https://arxiv.org/abs/2505.14825</link>
<guid>https://arxiv.org/abs/2505.14825</guid>
<content:encoded><![CDATA[
<div> Bayesian data assimilation, causal inference, ensemble-based information transfer, dynamical system, high-dimensional problems<br />
<br />
Summary: <br />
The article introduces a new causal inference framework called assimilative causal inference (ACI) that determines cause-and-effect relationships in a dynamic system. Unlike traditional methods, ACI identifies instantaneous causal relationships and the dynamic evolution of causal influence ranges by solving an inverse problem using Bayesian data assimilation techniques. ACI captures the dynamic interplay of variables, where roles as causes and effects can shift over time. It utilizes a mathematically justified criteria to determine causal influence ranges without empirical thresholds and is scalable to high-dimensional problems. ACI is effective for short time series and incomplete datasets, making it applicable to complex dynamical systems showcasing intermittency and extreme events. Moreover, ACI does not require observations of potential causes, providing an advantage when drivers are unknown or unmeasured. <div>
arXiv:2505.14825v1 Announce Type: new 
Abstract: Causal inference determines cause-and-effect relationships between variables and has broad applications across disciplines. Traditional time-series methods often reveal causal links only in a time-averaged sense, while ensemble-based information transfer approaches detect the time evolution of short-term causal relationships but are typically limited to low-dimensional systems. In this paper, a new causal inference framework, called assimilative causal inference (ACI), is developed. Fundamentally different from the state-of-the-art methods, ACI uses a dynamical system and a single realization of a subset of the state variables to identify instantaneous causal relationships and the dynamic evolution of the associated causal influence range (CIR). Instead of quantifying how causes influence effects as done traditionally, ACI solves an inverse problem via Bayesian data assimilation, thus tracing causes backward from observed effects with an implicit Bayesian hypothesis. Causality is determined by assessing whether incorporating the information of the effect variables reduces the uncertainty in recovering the potential cause variables. ACI has several desirable features. First, it captures the dynamic interplay of variables, where their roles as causes and effects can shift repeatedly over time. Second, a mathematically justified objective criterion determines the CIR without empirical thresholds. Third, ACI is scalable to high-dimensional problems by leveraging computationally efficient Bayesian data assimilation techniques. Finally, ACI applies to short time series and incomplete datasets. Notably, ACI does not require observations of candidate causes, which is a key advantage since potential drivers are often unknown or unmeasured. The effectiveness of ACI is demonstrated by complex dynamical systems showcasing intermittency and extreme events.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain</title>
<link>https://arxiv.org/abs/2505.14826</link>
<guid>https://arxiv.org/abs/2505.14826</guid>
<content:encoded><![CDATA[
<div> Keywords: Supervised fine-tuning, large language models, information gain, computational efficiency, empirical performance

Summary:
Supervised fine-tuning (SFT) is commonly used to adapt large language models (LLMs) to new domains, but its statistical efficiency can be improved. This study introduces a method to increase efficiency by selecting an informative subset of training examples. By choosing examples that maximize information gain, measured by the Hessian of the log-likelihood of the LLM, the approach efficiently approximates the selection process. Using multinomial logistic regression models to linearize the LLM at the last layer aids in computational efficiency and analysis. Empirical testing on various problems confirms the method's effectiveness, supported by quantitative results and LLM evaluations. The proposed approach enhances SFT by optimizing the selection of training examples and demonstrates improved performance across different domains. 

<br /><br />Summary: <div>
arXiv:2505.14826v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) is a standard approach to adapting large language models (LLMs) to new domains. In this work, we improve the statistical efficiency of SFT by selecting an informative subset of training examples. Specifically, for a fixed budget of training examples, which determines the computational cost of fine-tuning, we determine the most informative ones. The key idea in our method is to select examples that maximize information gain, measured by the Hessian of the log-likelihood of the LLM. We approximate it efficiently by linearizing the LLM at the last layer using multinomial logistic regression models. Our approach is computationally efficient, analyzable, and performs well empirically. We demonstrate this on several problems, and back our claims with both quantitative results and an LLM evaluation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Koopman operator framework for causal discovery in nonlinear dynamical systems</title>
<link>https://arxiv.org/abs/2505.14828</link>
<guid>https://arxiv.org/abs/2505.14828</guid>
<content:encoded><![CDATA[
<div> deep Koopman operator-theoretic formalism, causal discovery algorithm, nonlinear dynamics, deep learning, El Niño-Southern Oscillation <br />
<br />
Summary: 
The article introduces a new causal discovery algorithm called Kausal, which utilizes a deep Koopman operator-theoretic formalism. This algorithm is designed to identify cause-effect mechanisms in complex systems with nonlinear dynamics. Traditional statistical approaches struggle with quantifying causal relationships in such systems due to feedback mechanisms and nonstationarity. Kausal leverages deep learning to infer optimal observables, which are then evaluated in a reproducing kernel Hilbert space to determine causal estimates. The algorithm outperforms existing methods in discovering and characterizing causal signals. The study also applies Kausal to analyze the El Niño-Southern Oscillation, demonstrating its effectiveness in real-world scenarios. The code for Kausal is openly available on GitHub for researchers to utilize. <br /> <div>
arXiv:2505.14828v1 Announce Type: new 
Abstract: We use a deep Koopman operator-theoretic formalism to develop a novel causal discovery algorithm, Kausal. Causal discovery aims to identify cause-effect mechanisms for better scientific understanding, explainable decision-making, and more accurate modeling. Standard statistical frameworks, such as Granger causality, lack the ability to quantify causal relationships in nonlinear dynamics due to the presence of complex feedback mechanisms, timescale mixing, and nonstationarity. This presents a challenge in studying many real-world systems, such as the Earth's climate. Meanwhile, Koopman operator methods have emerged as a promising tool for approximating nonlinear dynamics in a linear space of observables. In Kausal, we propose to leverage this powerful idea for causal analysis where optimal observables are inferred using deep learning. Causal estimates are then evaluated in a reproducing kernel Hilbert space, and defined as the distance between the marginal dynamics of the effect and the joint dynamics of the cause-effect observables. Our numerical experiments demonstrate Kausal's superior ability in discovering and characterizing causal signals compared to existing approaches of prescribed observables. Lastly, we extend our analysis to observations of El Ni\~no-Southern Oscillation highlighting our algorithm's applicability to real-world phenomena. Our code is available at https://github.com/juannat7/kausal.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subquadratic Algorithms and Hardness for Attention with Any Temperature</title>
<link>https://arxiv.org/abs/2505.14840</link>
<guid>https://arxiv.org/abs/2505.14840</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer architecture, Attention computation, Subquadratic time complexity, Large head dimension, Fine-grained complexity assumptions

Summary:
Efficient attention computation methods have been a topic of interest, especially concerning the Transformer architecture. Current algorithms for computing attention have quadratic time complexity in context length, but recent research has shown subquadratic time is possible with certain conditions. This study explores when attention can be computed efficiently without constraints on temperature and aims to identify fast algorithms that scale appropriately with entry size B. The research presents a subquadratic time algorithm for attention even with large B and low-rank matrices, as well as for attention gradient computation in training processes. The study suggests that significant improvements beyond the proposed algorithm are unlikely and demonstrates that under specific conditions, the standard algorithm remains optimal. Overall, the research contributes to understanding the efficiency and limitations of attention computation in complex models like Transformers. 

<br /><br />Summary: <div>
arXiv:2505.14840v1 Announce Type: new 
Abstract: Despite the popularity of the Transformer architecture, the standard algorithm for computing Attention suffers from quadratic time complexity in context length $n$. Alman and Song [NeurIPS 2023] showed that when the head dimension $d = \Theta(\log n)$, subquadratic Attention is possible if and only if the inputs have small entries bounded by $B = o(\sqrt{\log n})$ in absolute values, under the Strong Exponential Time Hypothesis ($\mathsf{SETH}$). Equivalently, subquadratic Attention is possible if and only if the softmax is applied with high temperature for $d=\Theta(\log n)$. Running times of these algorithms depend exponentially on $B$ and thus they do not lead to even a polynomial-time algorithm outside the specific range of $B$.
  This naturally leads to the question: when can Attention be computed efficiently without strong assumptions on temperature? Are there fast attention algorithms that scale polylogarithmically with entry size $B$? In this work, we resolve this question and characterize when fast Attention for arbitrary temperatures is possible. First, for all constant $d = O(1)$, we give the first subquadratic $\tilde{O}(n^{2 - 1/d} \cdot \mathrm{polylog}(B))$ time algorithm for Attention with large $B$. Our result holds even for matrices with large head dimension if they have low rank. In this regime, we also give a similar running time for Attention gradient computation, and therefore for the full LLM training process. Furthermore, we show that any substantial improvement on our algorithm is unlikely. In particular, we show that even when $d = 2^{\Theta(\log^* n)}$, Attention requires $n^{2 - o(1)}$ time under $\mathsf{SETH}$.
  Finally, in the regime where $d = \mathrm{poly}(n)$, we show that the standard algorithm is optimal under popular fine-grained complexity assumptions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A self-regulated convolutional neural network for classifying variable stars</title>
<link>https://arxiv.org/abs/2505.14877</link>
<guid>https://arxiv.org/abs/2505.14877</guid>
<content:encoded><![CDATA[
<div> convolutional neural networks, recurrent neural networks, transformer models, variable stars, biases<br />
Summary:<br />
Machine learning models have been successful in classifying variable stars, but they often encounter challenges with biases in the training data. This paper proposes a novel self-regulated training approach for variable star classification using synthetic samples generated by a physics-enhanced variational autoencoder. By incorporating six physical parameters from Gaia Data Release 3, the method facilitates a dynamic interaction between a classifier and a generative model to address biases and populate underrepresented regions in the physical parameter space. Experimental results demonstrate that the self-regulated training approach outperforms traditional methods in classifying variable stars on biased datasets, showing statistically significant improvements. This novel approach offers a promising solution to enhance the reliability of classifiers in variable star classification tasks. <br /><br /> <div>
arXiv:2505.14877v1 Announce Type: new 
Abstract: Over the last two decades, machine learning models have been widely applied and have proven effective in classifying variable stars, particularly with the adoption of deep learning architectures such as convolutional neural networks, recurrent neural networks, and transformer models. While these models have achieved high accuracy, they require high-quality, representative data and a large number of labelled samples for each star type to generalise well, which can be challenging in time-domain surveys. This challenge often leads to models learning and reinforcing biases inherent in the training data, an issue that is not easily detectable when validation is performed on subsamples from the same catalogue. The problem of biases in variable star data has been largely overlooked, and a definitive solution has yet to be established. In this paper, we propose a new approach to improve the reliability of classifiers in variable star classification by introducing a self-regulated training process. This process utilises synthetic samples generated by a physics-enhanced latent space variational autoencoder, incorporating six physical parameters from Gaia Data Release 3. Our method features a dynamic interaction between a classifier and a generative model, where the generative model produces ad-hoc synthetic light curves to reduce confusion during classifier training and populate underrepresented regions in the physical parameter space. Experiments conducted under various scenarios demonstrate that our self-regulated training approach outperforms traditional training methods for classifying variable stars on biased datasets, showing statistically significant improvements.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An active learning framework for multi-group mean estimation</title>
<link>https://arxiv.org/abs/2505.14882</link>
<guid>https://arxiv.org/abs/2505.14882</guid>
<content:encoded><![CDATA[
<div> Algorithm, Variance-UCB, active learning, sequential sampling, upper confidence bound<br />
<br />
Summary: 
The study focuses on learning the mean of multiple groups with unknown data distributions in a fair manner. It employs an active learning framework to sequentially collect samples with bandit feedback, updating estimates of group means and variances. The objective is to minimize collective noise in estimators by selecting groups based on upper confidence bounds on variance estimates. The proposed Variance-UCB algorithm improves upon existing bounds on regret, providing efficient learning from various distributions. The research offers new insights for adaptive experimentation in online platforms and clinical trials, with promising results for minimizing noise in mean estimators. <div>
arXiv:2505.14882v1 Announce Type: new 
Abstract: We study a fundamental learning problem over multiple groups with unknown data distributions, where an analyst would like to learn the mean of each group. Moreover, we want to ensure that this data is collected in a relatively fair manner such that the noise of the estimate of each group is reasonable. In particular, we focus on settings where data are collected dynamically, which is important in adaptive experimentation for online platforms or adaptive clinical trials for healthcare. In our model, we employ an active learning framework to sequentially collect samples with bandit feedback, observing a sample in each period from the chosen group. After observing a sample, the analyst updates their estimate of the mean and variance of that group and chooses the next group accordingly. The analyst's objective is to dynamically collect samples to minimize the collective noise of the estimators, measured by the norm of the vector of variances of the mean estimators.
  We propose an algorithm, Variance-UCB, that sequentially selects groups according to an upper confidence bound on the variance estimate. We provide a general theoretical framework for providing efficient bounds on learning from any underlying distribution where the variances can be estimated reasonably. This framework yields upper bounds on regret that improve significantly upon all existing bounds, as well as a collection of new results for different objectives and distributions than those previously studied.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity</title>
<link>https://arxiv.org/abs/2505.14884</link>
<guid>https://arxiv.org/abs/2505.14884</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, inference acceleration, contextual sparsity, Polar Sparsity, GPU kernels

Summary: 
Polar Sparsity introduces a new sparsity technique that focuses on Attention layers as batch size and sequence length scale, shifting importance from MLP layers. While batch efficiency improves for MLP layers, their sparsity decreases, whereas Attention layers maintain stable sparsity and become more computationally expensive at scale. The development of hardware-efficient GPU kernels for selective MLP and Attention computations results in significant speedups for models like OPT, LLaMA-2 & 3, without compromising accuracy. This work demonstrates that Polar Sparsity can effectively scale to large batch sizes, providing substantial inference acceleration and making it practical for high-throughput LLM deployment systems. The code for this project is accessible on GitHub at https://github.com/susavlsh10/Polar-Sparsity.<br /><br />Summary: <div>
arXiv:2505.14884v1 Announce Type: new 
Abstract: Accelerating large language model (LLM) inference is critical for real-world deployments requiring high throughput and low latency. Contextual sparsity, where each token dynamically activates only a small subset of the model parameters, shows promise but does not scale to large batch sizes due to union of active neurons quickly approaching dense computation. We introduce Polar Sparsity, highlighting a key shift in sparsity importance from MLP to Attention layers as we scale batch size and sequence length. While MLP layers become more compute-efficient under batching, their sparsity vanishes. In contrast, attention becomes increasingly more expensive at scale, while their head sparsity remains stable and batch-invariant. We develop hardware-efficient, sparsity-aware GPU kernels for selective MLP and Attention computations, delivering up to \(2.2\times\) end-to-end speedups for models like OPT, LLaMA-2 \& 3, across various batch sizes and sequence lengths without compromising accuracy. To our knowledge, this is the first work to demonstrate that contextual sparsity can scale effectively to large batch sizes, delivering substantial inference acceleration with minimal changes, making Polar Sparsity practical for large-scale, high-throughput LLM deployment systems. Our code is available at: https://github.com/susavlsh10/Polar-Sparsity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature-Weighted MMD-CORAL for Domain Adaptation in Power Transformer Fault Diagnosis</title>
<link>https://arxiv.org/abs/2505.14896</link>
<guid>https://arxiv.org/abs/2505.14896</guid>
<content:encoded><![CDATA[
<div> Keywords: power transformers, dissolved gas analysis, machine learning, domain adaptation, feature-weighted

Summary:
This study focuses on improving fault diagnosis for power transformers using Dissolved Gas Analysis (DGA) through a machine learning-based approach. Traditional heuristic methods lack consistency, so a feature-weighted domain adaptation technique is proposed to address distribution shifts in diagnostic data among different transformers. By combining Maximum Mean Discrepancy (MMD), Correlation Alignment (CORAL), and feature-specific weighting (MCW), this method effectively aligns source and target domains. Experimental results show a 7.9% improvement over Fine-Tuning and a 2.2% improvement over MMD-CORAL (MC), demonstrating robustness across various training sample sizes. The proposed technique outperforms existing methods and enhances diagnostic accuracy for power transformer fault detection, critical for ensuring grid stability and reliable operation. 

<br /><br />Summary: <div>
arXiv:2505.14896v1 Announce Type: new 
Abstract: Ensuring the reliable operation of power transformers is critical to grid stability. Dissolved Gas Analysis (DGA) is widely used for fault diagnosis, but traditional methods rely on heuristic rules, which may lead to inconsistent results. Machine learning (ML)-based approaches have improved diagnostic accuracy; however, power transformers operate under varying conditions, and differences in transformer type, environmental factors, and operational settings create distribution shifts in diagnostic data. Consequently, direct model transfer between transformers often fails, making techniques for domain adaptation a necessity. To tackle this issue, this work proposes a feature-weighted domain adaptation technique that combines Maximum Mean Discrepancy (MMD) and Correlation Alignment (CORAL) with feature-specific weighting (MCW). Kolmogorov-Smirnov (K-S) statistics are used to assign adaptable weights, prioritizing features with larger distributional discrepancies and thereby improving source and target domain alignment. Experimental evaluations on datasets for power transformers demonstrate the effectiveness of the proposed method, which achieves a 7.9% improvement over Fine-Tuning and a 2.2% improvement over MMD-CORAL (MC). Furthermore, it outperforms both techniques across various training sample sizes, confirming its robustness for domain adaptation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Channel Swin Transformer Framework for Bearing Remaining Useful Life Prediction</title>
<link>https://arxiv.org/abs/2505.14897</link>
<guid>https://arxiv.org/abs/2505.14897</guid>
<content:encoded><![CDATA[
<div> framework, wavelet-based denoising, multi-channel Swin Transformer model, customized loss function, predictive maintenance

Summary:
The paper introduces a novel framework for estimating the Remaining Useful Life (RUL) of rolling bearings in industrial systems. The framework combines wavelet-based denoising method, Wavelet Packet Decomposition (WPD), and a customized multi-channel Swin Transformer model (MCSFormer) to address degradation trends and noise presence. The model utilizes attention mechanisms for feature fusion to learn global and local degradation patterns, enhancing predictive performance. A customized loss function is developed to prioritize accurate early detection and minimize operation risks of late predictions. Evaluation with the PRONOSTIA dataset showed that MCSFormer outperformed state-of-the-art models in intra-condition experiments, achieved superior generalization in cross-condition testing, and effectively reduced late predictions with a custom loss function. The model's robust noise resistance, generalization capability, and focus on safety make MCSFormer a reliable predictive maintenance tool for industrial applications. 

<br /><br />Summary: <div>
arXiv:2505.14897v1 Announce Type: new 
Abstract: Precise estimation of the Remaining Useful Life (RUL) of rolling bearings is an important consideration to avoid unexpected failures, reduce downtime, and promote safety and efficiency in industrial systems. Complications in degradation trends, noise presence, and the necessity to detect faults in advance make estimation of RUL a challenging task. This paper introduces a novel framework that combines wavelet-based denoising method, Wavelet Packet Decomposition (WPD), and a customized multi-channel Swin Transformer model (MCSFormer) to address these problems. With attention mechanisms incorporated for feature fusion, the model is designed to learn global and local degradation patterns utilizing hierarchical representations for enhancing predictive performance. Additionally, a customized loss function is developed as a key distinction of this work to differentiate between early and late predictions, prioritizing accurate early detection and minimizing the high operation risks of late predictions. The proposed model was evaluated with the PRONOSTIA dataset using three experiments. Intra-condition experiments demonstrated that MCSFormer outperformed state-of-the-art models, including the Adaptive Transformer, MDAN, and CNN-SRU, achieving 41%, 64%, and 69% lower MAE on average across different operating conditions, respectively. In terms of cross-condition testing, it achieved superior generalization under varying operating conditions compared to the adapted ViT and Swin Transformer. Lastly, the custom loss function effectively reduced late predictions, as evidenced in a 6.3% improvement in the scoring metric while maintaining competitive overall performance. The model's robust noise resistance, generalization capability, and focus on safety make MCSFormer a trustworthy and effective predictive maintenance tool in industrial applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When to retrain a machine learning model</title>
<link>https://arxiv.org/abs/2505.14903</link>
<guid>https://arxiv.org/abs/2505.14903</guid>
<content:encoded><![CDATA[
<div> evolution of data, retraining, machine learning model, distribution shift detection, uncertainty-based method <br />
Summary: 
A new method is proposed to address the challenge of determining when to retrain machine learning models in response to evolving data. The problem is complicated by limited information, unknown distribution shifts, and the need to balance the cost of retraining with performance. Existing approaches are limited in different ways, prompting the development of a new uncertainty-based method. This method continually forecasts the model performance evolution using a bounded metric to make decisions on retraining. Experimental results on classification tasks across 7 datasets demonstrate the effectiveness of the proposed approach, consistently outperforming existing baselines. The method offers a comprehensive solution to the retraining problem by considering practical constraints, data scarcity, and cost trade-offs. <div>
arXiv:2505.14903v1 Announce Type: new 
Abstract: A significant challenge in maintaining real-world machine learning models is responding to the continuous and unpredictable evolution of data. Most practitioners are faced with the difficult question: when should I retrain or update my machine learning model? This seemingly straightforward problem is particularly challenging for three reasons: 1) decisions must be made based on very limited information - we usually have access to only a few examples, 2) the nature, extent, and impact of the distribution shift are unknown, and 3) it involves specifying a cost ratio between retraining and poor performance, which can be hard to characterize. Existing works address certain aspects of this problem, but none offer a comprehensive solution. Distribution shift detection falls short as it cannot account for the cost trade-off; the scarcity of the data, paired with its unusual structure, makes it a poor fit for existing offline reinforcement learning methods, and the online learning formulation overlooks key practical considerations. To address this, we present a principled formulation of the retraining problem and propose an uncertainty-based method that makes decisions by continually forecasting the evolution of model performance evaluated with a bounded metric. Our experiments addressing classification tasks show that the method consistently outperforms existing baselines on 7 datasets.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TxPert: Leveraging Biochemical Relationships for Out-of-Distribution Transcriptomic Perturbation Prediction</title>
<link>https://arxiv.org/abs/2505.14919</link>
<guid>https://arxiv.org/abs/2505.14919</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, gene-gene relationships, out-of-distribution prediction, state-of-the-art method, benchmarking framework

Summary: 
TxPert is introduced as a novel method utilizing biological knowledge graphs for predicting cellular responses to genetic perturbations in unseen conditions. The study explores three challenging settings: unseen single perturbations, unseen double perturbations, and unseen cell lines. Through an in-depth analysis, the impact of graphs, model architecture, and data on prediction performance is demonstrated. Additionally, an expanded benchmarking framework is presented to enhance evaluation standards for perturbation modeling. TxPert showcases significant advancements in accurately predicting transcriptional responses under out-of-distribution scenarios by leveraging multiple biological knowledge networks. <div>
arXiv:2505.14919v1 Announce Type: new 
Abstract: Accurately predicting cellular responses to genetic perturbations is essential for understanding disease mechanisms and designing effective therapies. Yet exhaustively exploring the space of possible perturbations (e.g., multi-gene perturbations or across tissues and cell types) is prohibitively expensive, motivating methods that can generalize to unseen conditions. In this work, we explore how knowledge graphs of gene-gene relationships can improve out-of-distribution (OOD) prediction across three challenging settings: unseen single perturbations; unseen double perturbations; and unseen cell lines. In particular, we present: (i) TxPert, a new state-of-the-art method that leverages multiple biological knowledge networks to predict transcriptional responses under OOD scenarios; (ii) an in-depth analysis demonstrating the impact of graphs, model architecture, and data on performance; and (iii) an expanded benchmarking framework that strengthens evaluation standards for perturbation modeling.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundations of Unknown-aware Machine Learning</title>
<link>https://arxiv.org/abs/2505.14933</link>
<guid>https://arxiv.org/abs/2505.14933</guid>
<content:encoded><![CDATA[
<div> frameworks, outlier synthesis, OOD detection, large language models, reliability guarantees
<br />
This thesis addresses the challenge of ensuring the reliability and safety of machine learning models in open-world deployment. It introduces novel frameworks that optimize for in-distribution accuracy and reliability to unseen data, including an unknown-aware learning framework. The development of outlier synthesis methods and the SAL framework leverages unlabeled data to enhance out-of-distribution (OOD) detection. The thesis extends reliable learning to foundation models, introducing tools like HaloScope for hallucination detection in large language models and MLLMGuard for defending against malicious prompts in multimodal models. Additionally, data cleaning methods are proposed to denoise human feedback for better alignment. These contributions promote unknown-aware learning as a new paradigm and provide formal reliability guarantees for AI systems with minimal human efforts.
<br /><br />Summary: <div>
arXiv:2505.14933v1 Announce Type: new 
Abstract: Ensuring the reliability and safety of machine learning models in open-world deployment is a central challenge in AI safety. This thesis develops both algorithmic and theoretical foundations to address key reliability issues arising from distributional uncertainty and unknown classes, from standard neural networks to modern foundation models like large language models (LLMs).
  Traditional learning paradigms, such as empirical risk minimization (ERM), assume no distribution shift between training and inference, often leading to overconfident predictions on out-of-distribution (OOD) inputs. This thesis introduces novel frameworks that jointly optimize for in-distribution accuracy and reliability to unseen data. A core contribution is the development of an unknown-aware learning framework that enables models to recognize and handle novel inputs without labeled OOD data.
  We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to generate informative unknowns during training. Building on this, we present SAL, a theoretical and algorithmic framework that leverages unlabeled in-the-wild data to enhance OOD detection under realistic deployment conditions. These methods demonstrate that abundant unlabeled data can be harnessed to recognize and adapt to unforeseen inputs, providing formal reliability guarantees.
  The thesis also extends reliable learning to foundation models. We develop HaloScope for hallucination detection in LLMs, MLLMGuard for defending against malicious prompts in multimodal models, and data cleaning methods to denoise human feedback used for better alignment. These tools target failure modes that threaten the safety of large-scale models in deployment.
  Overall, these contributions promote unknown-aware learning as a new paradigm, and we hope it can advance the reliability of AI systems with minimal human efforts.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities</title>
<link>https://arxiv.org/abs/2505.14943</link>
<guid>https://arxiv.org/abs/2505.14943</guid>
<content:encoded><![CDATA[
<div> approach, language models, optimized input embeddings, soft prompts, latent capabilities
Summary: 
This paper introduces a novel approach using optimized input embeddings, termed 'soft prompts,' to measure the conditional distance between language models and desired behaviors. The goal is to uncover latent capabilities of models and provide a quantitative metric for evaluating potentially concerning behaviors. The technique is designed to support automated red teaming and evaluation processes, offering insights into the accessibility of problematic behaviors in powerful language models. The evaluation framework utilizing soft prompts is demonstrated across various tasks like natural language processing, chess, and pathfinding. Additionally, the technique is enhanced with generalized conditional soft prompts to assist in designing task evaluations. This methodology can help in understanding the true capabilities of language models and their potential alignment to tasks, paving the way for more comprehensive assessments in the future.<br /><br />Summary: <div>
arXiv:2505.14943v1 Announce Type: new 
Abstract: To help evaluate and understand the latent capabilities of language models, this paper introduces an approach using optimized input embeddings, or 'soft prompts,' as a metric of conditional distance between a model and a target behavior. The technique aims to facilitate latent capability discovery as a part of automated red teaming/evaluation suites and to provide quantitative feedback about the accessibility of potentially concerning behaviors in a way that may scale to powerful future models, including those which may otherwise be capable of deceptive alignment. An evaluation framework using soft prompts is demonstrated in natural language, chess, and pathfinding, and the technique is extended with generalized conditional soft prompts to aid in constructing task evaluations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning Algorithmic Biases over Graphs</title>
<link>https://arxiv.org/abs/2505.14945</link>
<guid>https://arxiv.org/abs/2505.14945</guid>
<content:encoded><![CDATA[
<div> Keywords: certified unlearning, bias mitigation, graph data, machine learning, fairness enhancement

Summary: 
This study introduces a novel approach to bias mitigation in machine learning models trained on graph data. The researchers propose a training-free unlearning procedure that uses a single-step Newton update on the model weights to certify bias mitigation. By selectively unlearning nodal features and structurally important elements in the graph, the proposed method aims to reduce algorithmic bias while maintaining high utility in tasks such as node classification. Experimental results demonstrate the effectiveness of the unlearning strategies in mitigating bias, with favorable trade-offs between utility and complexity compared to retraining from scratch with augmented data. The development of fairness-aware unlearning strategies and refined bounds for certified unlearning further contribute to the field of graph unlearning and bias mitigation in machine learning models. <div>
arXiv:2505.14945v1 Announce Type: new 
Abstract: The growing enforcement of the right to be forgotten regulations has propelled recent advances in certified (graph) unlearning strategies to comply with data removal requests from deployed machine learning (ML) models. Motivated by the well-documented bias amplification predicament inherent to graph data, here we take a fresh look at graph unlearning and leverage it as a bias mitigation tool. Given a pre-trained graph ML model, we develop a training-free unlearning procedure that offers certifiable bias mitigation via a single-step Newton update on the model weights. This way, we contribute a computationally lightweight alternative to the prevalent training- and optimization-based fairness enhancement approaches, with quantifiable performance guarantees. We first develop a novel fairness-aware nodal feature unlearning strategy along with refined certified unlearning bounds for this setting, whose impact extends beyond the realm of graph unlearning. We then design structural unlearning methods endowed with principled selection mechanisms over nodes and edges informed by rigorous bias analyses. Unlearning these judiciously selected elements can mitigate algorithmic biases with minimal impact on downstream utility (e.g., node classification accuracy). Experimental results over real networks corroborate the bias mitigation efficacy of our unlearning strategies, and delineate markedly favorable utility-complexity trade-offs relative to retraining from scratch using augmented graph data obtained via removals.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Preserving Conversion Modeling in Data Clean Room</title>
<link>https://arxiv.org/abs/2505.14959</link>
<guid>https://arxiv.org/abs/2505.14959</guid>
<content:encoded><![CDATA[
<div> framework, collaborative model training, privacy-preserving, conversion rate prediction, digital advertising <br />
<br />
Summary: <br />
This paper introduces a novel framework for privacy-preserving conversion rate prediction in online advertising. It addresses the challenge of predicting conversion rates while respecting user privacy preferences and advertiser requirements. The proposed method enables collaborative model training without sharing sample-level gradients, using batch-level aggregated gradients to minimize privacy risks. It incorporates adapter-based parameter-efficient fine-tuning and gradient compression to reduce communication costs. Additionally, de-biasing techniques are employed to train the model under label differential privacy, maintaining accuracy despite privacy-enhanced label perturbations. Experimental results on industrial datasets show competitive performance in terms of ROCAUC, reduced communication overhead, and compliance with privacy requirements. This framework sets a new standard for high-performance CVR prediction while ensuring privacy in the digital advertising landscape. <div>
arXiv:2505.14959v1 Announce Type: new 
Abstract: In the realm of online advertising, accurately predicting the conversion rate (CVR) is crucial for enhancing advertising efficiency and user satisfaction. This paper addresses the challenge of CVR prediction while adhering to user privacy preferences and advertiser requirements. Traditional methods face obstacles such as the reluctance of advertisers to share sensitive conversion data and the limitations of model training in secure environments like data clean rooms. We propose a novel model training framework that enables collaborative model training without sharing sample-level gradients with the advertising platform. Our approach introduces several innovative components: (1) utilizing batch-level aggregated gradients instead of sample-level gradients to minimize privacy risks; (2) applying adapter-based parameter-efficient fine-tuning and gradient compression to reduce communication costs; and (3) employing de-biasing techniques to train the model under label differential privacy, thereby maintaining accuracy despite privacy-enhanced label perturbations. Our experimental results, conducted on industrial datasets, demonstrate that our method achieves competitive ROCAUC performance while significantly decreasing communication overhead and complying with both advertiser privacy requirements and user privacy choices. This framework establishes a new standard for privacy-preserving, high-performance CVR prediction in the digital advertising landscape.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models</title>
<link>https://arxiv.org/abs/2505.14964</link>
<guid>https://arxiv.org/abs/2505.14964</guid>
<content:encoded><![CDATA[
<div> Annotation, smart-sizing, rare-event detection, model performance, AI development pipeline
Summary: 
The paper introduces smart-sizing, a training data strategy, focusing on label diversity, model-guided selection, and stopping based on marginal utility. They implement this through Adaptive Label Optimization (ALO), prioritizing labels that improve model performance significantly. Experiments show that models trained on 20-40 percent of data can match or surpass full-data baselines, especially in rare-class recall and edge case generalization. The study also highlights the impact of latent labeling errors on evaluation, emphasizing the need for audit tools and performance-aware governance. Smart-sizing redefines annotation as a feedback-driven process aligned with mission outcomes, enabling robust models with fewer labels. It supports efficient AI development pipelines for cutting-edge models and operational systems. 
<br /><br />Summary: <div>
arXiv:2505.14964v1 Announce Type: new 
Abstract: AI systems in high-consequence domains such as defense, intelligence, and disaster response must detect rare, high-impact events while operating under tight resource constraints. Traditional annotation strategies that prioritize label volume over informational value introduce redundancy and noise, limiting model generalization. This paper introduces smart-sizing, a training data strategy that emphasizes label diversity, model-guided selection, and marginal utility-based stopping. We implement this through Adaptive Label Optimization (ALO), combining pre-labeling triage, annotator disagreement analysis, and iterative feedback to prioritize labels that meaningfully improve model performance. Experiments show that models trained on 20 to 40 percent of curated data can match or exceed full-data baselines, particularly in rare-class recall and edge-case generalization. We also demonstrate how latent labeling errors embedded in training and validation sets can distort evaluation, underscoring the need for embedded audit tools and performance-aware governance. Smart-sizing reframes annotation as a feedback-driven process aligned with mission outcomes, enabling more robust models with fewer labels and supporting efficient AI development pipelines for frontier models and operational systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly Detection Based on Critical Paths for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2505.14967</link>
<guid>https://arxiv.org/abs/2505.14967</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep neural networks, critical paths, anomaly detection, genetic evolution, ensemble detection<br />
Summary: <br />
- The study focuses on understanding and defending deep neural networks (DNNs) by extracting critical paths that can capture essential features in decision-making processes.
- Critical detection paths are identified using genetic evolution and mutation, considering that different paths in a DNN capture diverse features for the same target class.
- An ensemble approach is employed by combining results from multiple paths through random subspace sampling and a voting mechanism.
- Experimental results indicate that the proposed method surpasses existing techniques and exhibits high accuracy in detecting various anomaly types.
- The approach demonstrates superior performance in capturing outliers and adversarial inputs by leveraging the unique activation patterns on critical paths. <br /> 
Summary: <div>
arXiv:2505.14967v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) are notoriously hard to understand and difficult to defend. Extracting representative paths (including the neuron activation values and the connections between neurons) from DNNs using software engineering approaches has recently shown to be a promising approach in interpreting the decision making process of blackbox DNNs, as the extracted paths are often effective in capturing essential features. With this in mind, this work investigates a novel approach that extracts critical paths from DNNs and subsequently applies the extracted paths for the anomaly detection task, based on the observation that outliers and adversarial inputs do not usually induce the same activation pattern on those paths as normal (in-distribution) inputs.
  In our approach, we first identify critical detection paths via genetic evolution and mutation. Since different paths in a DNN often capture different features for the same target class, we ensemble detection results from multiple paths by integrating random subspace sampling and a voting mechanism. Compared with state-of-the-art methods, our experimental results suggest that our method not only outperforms them, but it is also suitable for the detection of a broad range of anomaly types with high accuracy.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STree: Speculative Tree Decoding for Hybrid State-Space Models</title>
<link>https://arxiv.org/abs/2505.14969</link>
<guid>https://arxiv.org/abs/2505.14969</guid>
<content:encoded><![CDATA[
<div> Speculative decoding, hardware concurrency, autoregressive Transformer models, state-space models, efficiency, token generation<br />
<br />
Summary: Speculative decoding is a technique that utilizes hardware concurrency to enhance the efficiency of large-scale autoregressive Transformer models. State-space models (SSMs) already offer better efficiency than AR Transformers as their state summarizes past data without token caching. However, existing approaches to speculative decoding in SSMs do not leverage tree-based verification methods. This paper introduces a scalable algorithm for tree-based speculative decoding in SSMs and hybrid SSM-Transformer architectures. By exploiting the structure of state transition matrices, the algorithm minimizes overhead and improves inference speed. The proposed hardware-aware implementation outperforms vanilla speculative decoding with SSMs on multiple benchmarks, paving the way for further speed enhancements with SSM and hybrid models. Code availability will follow paper acceptance. <br /><br /> <div>
arXiv:2505.14969v1 Announce Type: new 
Abstract: Speculative decoding is a technique to leverage hardware concurrency to improve the efficiency of large-scale autoregressive (AR) Transformer models by enabling multiple steps of token generation in a single forward pass. State-space models (SSMs) are already more efficient than AR Transformers, since their state summarizes all past data with no need to cache or re-process tokens in the sliding window context. However, their state can also comprise thousands of tokens; so, speculative decoding has recently been extended to SSMs. Existing approaches, however, do not leverage the tree-based verification methods, since current SSMs lack the means to compute a token tree efficiently. We propose the first scalable algorithm to perform tree-based speculative decoding in state-space models (SSMs) and hybrid architectures of SSMs and Transformer layers. We exploit the structure of accumulated state transition matrices to facilitate tree-based speculative decoding with minimal overhead to current SSM state update implementations. With the algorithm, we describe a hardware-aware implementation that improves naive application of AR Transformer tree-based speculative decoding methods to SSMs. Furthermore, we outperform vanilla speculative decoding with SSMs even with a baseline drafting model and tree structure on three different benchmarks, opening up opportunities for further speed up with SSM and hybrid model inference. Code will be released upon paper acceptance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flattening Hierarchies with Policy Bootstrapping</title>
<link>https://arxiv.org/abs/2505.14975</link>
<guid>https://arxiv.org/abs/2505.14975</guid>
<content:encoded><![CDATA[
<div> Hierarchical RL, goal-conditioned reinforcement learning, subgoal-conditioned policies, advantage-weighted importance sampling, state-based locomotion, pixel-based manipulation <br />
<br />Summary: 
This paper introduces a novel approach to training flat goal-conditioned policies by utilizing subgoal-conditioned policies with advantage-weighted importance sampling. By eliminating the need for a generative model over the goal space, the proposed method overcomes the challenges of scaling to high-dimensional control in large state spaces. The algorithm outperforms existing hierarchical and bootstrapping-based approaches in a variety of locomotion and manipulation benchmarks. The method achieves strong empirical results on long-horizon goal-reaching tasks where previous approaches struggle, demonstrating its effectiveness and scalability in complex environments. <div>
arXiv:2505.14975v1 Announce Type: new 
Abstract: Offline goal-conditioned reinforcement learning (GCRL) is a promising approach for pretraining generalist policies on large datasets of reward-free trajectories, akin to the self-supervised objectives used to train foundation models for computer vision and natural language processing. However, scaling GCRL to longer horizons remains challenging due to the combination of sparse rewards and discounting, which obscures the comparative advantages of primitive actions with respect to distant goals. Hierarchical RL methods achieve strong empirical results on long-horizon goal-reaching tasks, but their reliance on modular, timescale-specific policies and subgoal generation introduces significant additional complexity and hinders scaling to high-dimensional goal spaces. In this work, we introduce an algorithm to train a flat (non-hierarchical) goal-conditioned policy by bootstrapping on subgoal-conditioned policies with advantage-weighted importance sampling. Our approach eliminates the need for a generative model over the (sub)goal space, which we find is key for scaling to high-dimensional control in large state spaces. We further show that existing hierarchical and bootstrapping-based approaches correspond to specific design choices within our derivation. Across a comprehensive suite of state- and pixel-based locomotion and manipulation benchmarks, our method matches or surpasses state-of-the-art offline GCRL algorithms and scales to complex, long-horizon tasks where prior approaches fail.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision</title>
<link>https://arxiv.org/abs/2505.14999</link>
<guid>https://arxiv.org/abs/2505.14999</guid>
<content:encoded><![CDATA[
<div> Energy Outcome Reward Model, Large Language Models, Chain of Thought, Mathematical reasoning, Energy Based Models

Summary:
The Energy Outcome Reward Model (EORM) is introduced as a lightweight verifier for Large Language Models (LLMs) to improve mathematical reasoning. EORM leverages Energy Based Models (EBMs) to assign energy scores to Chain of Thought solutions, ranking candidates based on coherence and correctness. Without detailed annotations, EORM improves final answer accuracy on mathematical benchmarks such as GSM8k and MATH. It enhances LLM reasoning reliability by efficiently verifying solutions, matching or exceeding brute force sampling performance. EORM's streamlined post hoc verification process ensures robust multi step logical consistency in mathematical reasoning tasks. <br /><br />Summary: <div>
arXiv:2505.14999v1 Announce Type: new 
Abstract: Mathematical reasoning presents a significant challenge for Large Language Models (LLMs), often requiring robust multi step logical consistency. While Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee correctness, and improving reliability via extensive sampling is computationally costly. This paper introduces the Energy Outcome Reward Model (EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy Based Models (EBMs) to simplify the training of reward models by learning to assign a scalar energy score to CoT solutions using only outcome labels, thereby avoiding detailed annotations. It achieves this by interpreting discriminator output logits as negative energies, effectively ranking candidates where lower energy is assigned to solutions leading to correct final outcomes implicitly favoring coherent reasoning. On mathematical benchmarks (GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively leverages a given pool of candidate solutions to match or exceed the performance of brute force sampling, thereby enhancing LLM reasoning outcome reliability through its streamlined post hoc verification process.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know When to Abstain: Optimal Selective Classification with Likelihood Ratios</title>
<link>https://arxiv.org/abs/2505.15008</link>
<guid>https://arxiv.org/abs/2505.15008</guid>
<content:encoded><![CDATA[
<div> selective classification, predictive models, likelihood ratio test, covariate shift, vision-language tasks 
<br />
Summary: 
This article introduces a new approach to selective classification, aiming to enhance the reliability of predictive models by allowing them to abstain from making uncertain predictions. Through the application of the Neyman--Pearson lemma, the optimal rejection rule is characterized as a likelihood ratio test. This perspective unifies various post-hoc selection baselines and inspires new methods for selective classification. A key focus is on covariate shift, where the input distribution differs between training and testing. The proposed Neyman--Pearson-based methods outperform existing baselines across a range of vision and language tasks, including supervised learning and vision-language models. Through experiments, it is shown that likelihood ratio-based selection provides a robust mechanism for improving selective classification under covariate shifts. The code for these methods is publicly available for further exploration. 
<br /><br />Summary:  <div>
arXiv:2505.15008v1 Announce Type: new 
Abstract: Selective classification enhances the reliability of predictive models by allowing them to abstain from making uncertain predictions. In this work, we revisit the design of optimal selection functions through the lens of the Neyman--Pearson lemma, a classical result in statistics that characterizes the optimal rejection rule as a likelihood ratio test. We show that this perspective not only unifies the behavior of several post-hoc selection baselines, but also motivates new approaches to selective classification which we propose here. A central focus of our work is the setting of covariate shift, where the input distribution at test time differs from that at training. This realistic and challenging scenario remains relatively underexplored in the context of selective classification. We evaluate our proposed methods across a range of vision and language tasks, including both supervised learning and vision-language models. Our experiments demonstrate that our Neyman--Pearson-informed methods consistently outperform existing baselines, indicating that likelihood ratio-based selection offers a robust mechanism for improving selective classification under covariate shifts. Our code is publicly available at https://github.com/clear-nus/sc-likelihood-ratios.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks</title>
<link>https://arxiv.org/abs/2505.15009</link>
<guid>https://arxiv.org/abs/2505.15009</guid>
<content:encoded><![CDATA[
<div> Keywords: one-layer transformers, in-context reasoning, next-token prediction, convergence rates, generalization abilities

Summary:
This study investigates the approximation capabilities and convergence behaviors of one-layer transformers in noiseless and noisy in-context reasoning for next-token prediction. The research fills gaps in existing knowledge by proving the Bayes optimality of a class of one-layer transformers using linear and ReLU attention. Through finite-sample analysis, the study demonstrates that the expected loss of these transformers converges at a linear rate to the Bayes risk when trained with gradient descent. Additionally, the trained models exhibit generalization abilities to unseen samples and display observed learning behaviors. The theoretical findings are supported by extensive empirical validations, providing a comprehensive understanding of the performance of one-layer transformers in in-context reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2505.15009v1 Announce Type: new 
Abstract: We study the approximation capabilities and on-convergence behaviors of one-layer transformers on the noiseless and noisy in-context reasoning of next-token prediction. Existing theoretical results focus on understanding the in-context reasoning behaviors for either the first gradient step or when the number of samples is infinite. Furthermore, no convergence rates nor generalization abilities were known. Our work addresses these gaps by showing that there exists a class of one-layer transformers that are provably Bayes-optimal with both linear and ReLU attention. When being trained with gradient descent, we show via a finite-sample analysis that the expected loss of these transformers converges at linear rate to the Bayes risk. Moreover, we prove that the trained models generalize to unseen samples as well as exhibit learning behaviors that were empirically observed in previous works. Our theoretical findings are further supported by extensive empirical validations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Node Attention: Multi-Scale Harmonic Encoding for Feature-Wise Graph Message Passing</title>
<link>https://arxiv.org/abs/2505.15015</link>
<guid>https://arxiv.org/abs/2505.15015</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, MSH-GNN, feature relevance, message passing, harmonic projections<br />
<br />
Summary:<br />
MSH-GNN is a novel Graph Neural Network architecture that incorporates feature-wise adaptive message passing through node-specific harmonic projections. It dynamically projects neighbor features onto frequency-sensitive directions determined by the node's own representation and modulates them using sinusoidal encodings at multiple frequencies to capture structural patterns. It introduces a frequency-aware attention pooling mechanism to emphasize important nodes during readout. Theoretically, MSH-GNN approximates shift-invariant kernels and matches the expressive power of the 1-Weisfeiler-Lehman test. Empirically, MSH-GNN outperforms state-of-the-art models in graph and node classification tasks and excels in capturing structural asymmetries and high-frequency modulations for accurate graph discrimination. <div>
arXiv:2505.15015v1 Announce Type: new 
Abstract: Conventional Graph Neural Networks (GNNs) aggregate neighbor embeddings as holistic vectors, lacking the ability to identify fine-grained, direction-specific feature relevance. We propose MSH-GNN (Multi-Scale Harmonic Graph Neural Network), a novel architecture that performs feature-wise adaptive message passing through node-specific harmonic projections. For each node, MSH-GNN dynamically projects neighbor features onto frequency-sensitive directions determined by the target node's own representation. These projections are further modulated using learnable sinusoidal encodings at multiple frequencies, enabling the model to capture both smooth and oscillatory structural patterns across scales. A frequency-aware attention pooling mechanism is introduced to emphasize spectrally and structurally salient nodes during readout. Theoretically, we prove that MSH-GNN approximates shift-invariant kernels and matches the expressive power of the 1-Weisfeiler-Lehman (1-WL) test. Empirically, MSH-GNN consistently outperforms state-of-the-art models on a wide range of graph and node classification tasks. Furthermore, in challenging classification settings involving joint variations in graph topology and spectral frequency, MSH-GNN excels at capturing structural asymmetries and high-frequency modulations, enabling more accurate graph discrimination.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Large Language Models Locally: Empirical Results and Implications for AI PC</title>
<link>https://arxiv.org/abs/2505.15030</link>
<guid>https://arxiv.org/abs/2505.15030</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, on-device deployment, post-training quantization, performance optimization, edge devices

Summary: 
1) The study focuses on evaluating Large Language Models (LLMs) deployed on edge devices, considering model capability, development efficiency, and system resources.
2) System-level metrics show a near-linear relationship with effective bits-per-weight (BPW), with larger models performing better under low-bit quantization.
3) Low-bit quantization results in slight accuracy loss but significant memory savings.
4) Power consumption on the CPU is influenced by implementation details, where computation-heavy tasks consume more power than memory-intensive ones.
5) The findings provide valuable insights and guidelines for efficiently deploying and configuring LLMs on resource-constrained edge devices. The research codebase is available for further exploration. 

<br /><br />Summary: <div>
arXiv:2505.15030v1 Announce Type: new 
Abstract: The increasing deployment of Large Language Models (LLMs) on edge devices, driven by model advancements and hardware improvements, offers significant privacy benefits. However, these on-device LLMs inherently face performance limitations due to reduced model capacity and necessary compression techniques. To address this, we introduce a systematic methodology -- encompassing model capability, development efficiency, and system resources -- for evaluating on-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to 14B parameters and seven post-training quantization (PTQ) methods on commodity laptops, yields several critical insights: 1) System-level metrics exhibit near-linear scaling with effective bits-per-weight (BPW). 2) A practical threshold exists around $\sim$3.5 effective BPW, larger models subjected to low-bit quantization consistently outperform smaller models utilizing higher bit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but significant memory savings. 4) Determined by low-level implementation specifics power consumption on CPU, where computation-intensive operations spend more power than memory-intensive ones. These findings offer crucial insights and practical guidelines for the efficient deployment and optimized configuration of LLMs on resource-constrained edge devices. Our codebase is available at https://github.com/simmonssong/LLMOnDevice.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning</title>
<link>https://arxiv.org/abs/2505.15034</link>
<guid>https://arxiv.org/abs/2505.15034</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, language models, Tango framework, process-level verifier, generalization <br />
Summary:<br />
The article introduces Tango, a novel framework that uses reinforcement learning to concurrently train a large language model (LLM) generator and a process-level verifier in an interleaved manner. Unlike traditional approaches, Tango's verifier is trained solely based on outcome-level verification correctness rewards, leading to improved robustness and generalization. The generator within Tango achieves state-of-the-art performance across multiple math benchmarks and reasoning tasks, while the verifier excels on the ProcessBench dataset. Both components exhibit substantial improvements on challenging mathematical reasoning problems, showcasing the effectiveness of the Tango framework in enhancing reasoning capabilities of LLMs. The code for Tango is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2505.15034v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifiers that are fixed (rule-based or frozen pretrained) or trained discriminatively via supervised fine-tuning (SFT). Such designs are susceptible to reward hacking and generalize poorly beyond their training distributions. To overcome these limitations, we propose Tango, a novel framework that uses RL to concurrently train both an LLM generator and a verifier in an interleaved manner. A central innovation of Tango is its generative, process-level LLM verifier, which is trained via RL and co-evolves with the generator. Importantly, the verifier is trained solely based on outcome-level verification correctness rewards without requiring explicit process-level annotations. This generative RL-trained verifier exhibits improved robustness and superior generalization compared to deterministic or SFT-trained verifiers, fostering effective mutual reinforcement with the generator. Extensive experiments demonstrate that both components of Tango achieve state-of-the-art results among 7B/8B-scale models: the generator attains best-in-class performance across five competition-level math benchmarks and four challenging out-of-domain reasoning tasks, while the verifier leads on the ProcessBench dataset. Remarkably, both components exhibit particularly substantial improvements on the most difficult mathematical reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLBenchNet: The Right Network for the Right Reinforcement Learning Task</title>
<link>https://arxiv.org/abs/2505.15040</link>
<guid>https://arxiv.org/abs/2505.15040</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, Neural networks, LSTM, MLP, Transformer-XL

Summary:<br /><br />
1. MLPs are effective in fully observable continuous control tasks due to their optimal balance of performance and efficiency.
2. LSTM and GRU neural network architectures show strong performance in partially observable environments with moderate memory requirements.
3. Mamba models outperform LSTM and GRU with a higher throughput in various tasks while maintaining comparable performance.
4. Transformer-XL, Gated Transformer-XL, and Mamba-2 excel in memory-intensive tasks, with Mamba-2 requiring significantly less memory compared to Transformer-XL.
5. The study provides insights for researchers and practitioners to choose neural network architectures based on specific task characteristics and computational constraints. <div>
arXiv:2505.15040v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has seen significant advancements through the application of various neural network architectures. In this study, we systematically investigate the performance of several neural networks in RL tasks, including Long Short-Term Memory (LSTM), Multi-Layer Perceptron (MLP), Mamba/Mamba-2, Transformer-XL, Gated Transformer-XL, and Gated Recurrent Unit (GRU). Through comprehensive evaluation across continuous control, discrete decision-making, and memory-based environments, we identify architecture-specific strengths and limitations. Our results reveal that: (1) MLPs excel in fully observable continuous control tasks, providing an optimal balance of performance and efficiency; (2) recurrent architectures like LSTM and GRU offer robust performance in partially observable environments with moderate memory requirements; (3) Mamba models achieve a 4.5x higher throughput compared to LSTM and a 3.9x increase over GRU, all while maintaining comparable performance; and (4) only Transformer-XL, Gated Transformer-XL, and Mamba-2 successfully solve the most challenging memory-intensive tasks, with Mamba-2 requiring 8x less memory than Transformer-XL. These findings provide insights for researchers and practitioners, enabling more informed architecture selection based on specific task characteristics and computational constraints. Code is available at: https://github.com/SafeRL-Lab/RLBenchNet
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2505.15047</link>
<guid>https://arxiv.org/abs/2505.15047</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, multi-agent systems, automated scientific discovery, uncertainty reduction, PiFlow

Summary:
PiFlow is introduced as a framework for automated scientific discovery using Large Language Model (LLM)-based multi-agent systems. Unlike existing approaches, PiFlow integrates rationality constraints and principles (such as scientific laws) to guide discovery. In evaluations across three scientific domains, PiFlow demonstrates significant improvements in discovery efficiency and solution quality compared to traditional approaches. The method increases the Area Under the Curve (AUC) of property values versus exploration steps by 73.55% and enhances solution quality by 94.06%. PiFlow acts as a Plug-and-Play method, revolutionizing automated scientific discovery and paving the way for more robust and accelerated AI-driven research. The code for PiFlow is publicly available on GitHub. 

<br /><br />Summary: PiFlow, a new framework for automated scientific discovery, integrates rationality constraints and scientific principles to guide discovery in LLM-based multi-agent systems. In evaluations across three scientific domains, PiFlow significantly improves discovery efficiency and solution quality, demonstrating a paradigm shift towards highly efficient automated scientific exploration in AI-driven research. <div>
arXiv:2505.15047v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failure to consistently link hypotheses with evidence, thereby hindering systematic uncertainty reduction. Overcoming these limitations fundamentally requires systematic uncertainty reduction. We introduce \texttt{PiFlow}, an information-theoretical framework, treating automated scientific discovery as a structured uncertainty reduction problem guided by principles (e.g., scientific laws). In evaluations across three distinct scientific domains -- discovering nanomaterial structures, bio-molecules, and superconductor candidates with targeted properties -- our method significantly improves discovery efficiency, reflected by a 73.55\% increase in the Area Under the Curve (AUC) of property values versus exploration steps, and enhances solution quality by 94.06\% compared to a vanilla agent system. Overall, \texttt{PiFlow} serves as a Plug-and-Play method, establishing a novel paradigm shift in highly efficient automated scientific discovery, paving the way for more robust and accelerated AI-driven research. Code is publicly available at our \href{https://github.com/amair-lab/PiFlow}{GitHub}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization Through Growth: Hidden Dynamics Controls Depth Dependence</title>
<link>https://arxiv.org/abs/2505.15064</link>
<guid>https://arxiv.org/abs/2505.15064</guid>
<content:encoded><![CDATA[
<div> pseudo-metric, depth, generalization bounds, dynamics, expressivity
<br />
Summary:
The paper introduces a unified framework for deep learning in any pseudo-metric space, decoupling the architecture from implementation. It presents a generalization bound that isolates the depth contribution in the growth of hidden layers, revealing a geometric dichotomy between sublinear and depth-independent rates. By linking polynomial growth to virtually nilpotent dynamics and exponential growth to expanding dynamics, the paper offers insights into the theoretical foundations of deep learning. Additionally, it provides covering-number estimates that show an exponential parameter saving for models with expanding dynamics, enhancing compositional expressivity. The results have implications for test-time inference and diffusion models, offering architecture-agnostic guarantees with a focus on dynamical systems awareness. 
<br /> <div>
arXiv:2505.15064v1 Announce Type: new 
Abstract: Recent theory has reduced the depth dependence of generalization bounds from exponential to polynomial and even depth-independent rates, yet these results remain tied to specific architectures and Euclidean inputs. We present a unified framework for arbitrary \blue{pseudo-metric} spaces in which a depth-\(k\) network is the composition of continuous hidden maps \(f:\mathcal{X}\to \mathcal{X}\) and an output map \(h:\mathcal{X}\to \mathbb{R}\). The resulting bound $O(\sqrt{(\alpha + \log \beta(k))/n})$ isolates the sole depth contribution in \(\beta(k)\), the word-ball growth of the semigroup generated by the hidden layers. By Gromov's theorem polynomial (resp. exponential) growth corresponds to virtually nilpotent (resp. expanding) dynamics, revealing a geometric dichotomy behind existing $O(\sqrt{k})$ (sublinear depth) and $\tilde{O}(1)$ (depth-independent) rates. We further provide covering-number estimates showing that expanding dynamics yield an exponential parameter saving via compositional expressivity. Our results decouple specification from implementation, offering architecture-agnostic and dynamical-systems-aware guarantees applicable to modern deep-learning paradigms such as test-time inference and diffusion models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoTime: A Dataset Suite for Multimodal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.15072</link>
<guid>https://arxiv.org/abs/2505.15072</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal time series, forecasting, MoTime, external modalities, cold-start forecasting <br />
Summary: <br />
This article introduces MoTime, a set of multimodal time series forecasting datasets that combine temporal signals with external modalities like text, metadata, and images. The datasets cover various domains and enable the evaluation of modality usefulness in two scenarios. Firstly, in the common forecasting task where variable-length history is accessible, and secondly, in cold-start forecasting where no historical data is present. Experiments demonstrate that external modalities can enhance forecasting performance in both scenarios, especially benefiting short series in some datasets, though the impact varies based on data characteristics. By releasing datasets and results publicly, the aim is to facilitate more comprehensive and practical evaluations in future multimodal time series forecasting studies. <br /> <div>
arXiv:2505.15072v1 Announce Type: new 
Abstract: While multimodal data sources are increasingly available from real-world forecasting, most existing research remains on unimodal time series. In this work, we present MoTime, a suite of multimodal time series forecasting datasets that pair temporal signals with external modalities such as text, metadata, and images. Covering diverse domains, MoTime supports structured evaluation of modality utility under two scenarios: 1) the common forecasting task, where varying-length history is available, and 2) cold-start forecasting, where no historical data is available. Experiments show that external modalities can improve forecasting performance in both scenarios, with particularly strong benefits for short series in some datasets, though the impact varies depending on data characteristics. By making datasets and findings publicly available, we aim to support more comprehensive and realistic benchmarks in future multimodal time series forecasting research.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories</title>
<link>https://arxiv.org/abs/2505.15076</link>
<guid>https://arxiv.org/abs/2505.15076</guid>
<content:encoded><![CDATA[
<div> Keywords: feature engineering, agentic feature augmentation, Multi-Agent System, feature selection, feature generation

Summary:
Feature engineering is a crucial aspect of AI model performance improvement, where raw data is transformed into discriminative features. Existing methods often separate feature selection and generation, leading to a lack of balance between reducing redundancy and adding meaningful dimensions. To address this, a new approach called agentic feature augmentation is proposed, which unifies feature generation and selection through a Multi-Agent System with Long and Short-Term Memory (MAGS). This system includes a selector agent, a generator agent, and a router agent working together to optimize the feature space with immediate and long-term memory feedback. The router agent is trained using offline Proximal Policy Optimization to make effective decisions. Experimental results show that this unified agentic framework consistently outperforms traditional methods by intelligently coordinating feature selection and generation. 

<br /><br />Summary: <div>
arXiv:2505.15076v1 Announce Type: new 
Abstract: As a widely-used and practical tool, feature engineering transforms raw data into discriminative features to advance AI model performance. However, existing methods usually apply feature selection and generation separately, failing to strive a balance between reducing redundancy and adding meaningful dimensions. To fill this gap, we propose an agentic feature augmentation concept, where the unification of feature generation and selection is modeled as agentic teaming and planning. Specifically, we develop a Multi-Agent System with Long and Short-Term Memory (MAGS), comprising a selector agent to eliminate redundant features, a generator agent to produce informative new dimensions, and a router agent that strategically coordinates their actions. We leverage in-context learning with short-term memory for immediate feedback refinement and long-term memory for globally optimal guidance. Additionally, we employ offline Proximal Policy Optimization (PPO) reinforcement fine-tuning to train the router agent for effective decision-making to navigate a vast discrete feature space. Extensive experiments demonstrate that this unified agentic framework consistently achieves superior task performance by intelligently orchestrating feature selection and generation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUS backprop: linear backpropagation algorithm for long inputs in transformers</title>
<link>https://arxiv.org/abs/2505.15080</link>
<guid>https://arxiv.org/abs/2505.15080</guid>
<content:encoded><![CDATA[
<div> attention mechanism, transformer architecture, gradient estimator, backpropagation, computational graph

Summary:<br />
The paper introduces a method to optimize the backpropagation process in transformer models by cutting backpropagation through attention weights. By probabilistically selecting a parameter *c* to limit the number of interactions each token has per attention head, the compute requirements for attention backpropagation can be reduced from quadratic to linear complexity. Empirical results show that cutting 99% of attention gradient flow with an appropriate choice of *c* results in minimal increase in gradient variance, making the approach promising for training transformer models on long sequences efficiently. This method allows for a more efficient sparse matrix implementation, reducing the backward pass cost significantly compared to the forward pass cost. <div>
arXiv:2505.15080v1 Announce Type: new 
Abstract: It is straightforward to design an unbiased gradient estimator that stochastically cuts the backpropagation flow through any part of a computational graph. By cutting the parts that have little effect on the computation, one can potentially save a significant amount of back-propagation computation in exchange for a minimal increase in the stochastic gradient variance, in some situations. Such a situation occurs in the attention mechanism of the transformer architecture. For long sequences, attention becomes the limiting factor, as its compute requirements increase quadratically with sequence length $n$. At the same time, most attention weights become very small, as most attention heads tend to connect a given token with only a small fraction of other tokens in the sequence. These weights become promising targets for cutting backpropagation. We propose a simple probabilistic rule controlled by a single parameter $c$ that cuts backpropagation through most attention weights, leaving at most $c$ interactions per token per attention head. This brings a factor of $c/n$ reduction in the compute required for the attention backpropagation, turning it from quadratic $O(n^2)$ to linear complexity $O(nc)$. We have empirically verified that, for a typical transformer model, cutting $99\%$ of the attention gradient flow (i.e. choosing $c \sim 20-30$) results in relative gradient variance increase of only about $1\%$ for $n \sim 2000$, and it decreases with $n$. This approach is amenable to efficient sparse matrix implementation, thus being promising for making the cost of a backward pass negligible relative to the cost of a forward pass when training a transformer model on long sequences.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features</title>
<link>https://arxiv.org/abs/2505.15083</link>
<guid>https://arxiv.org/abs/2505.15083</guid>
<content:encoded><![CDATA[
arXiv:2505.15083v1 Announce Type: new 
Abstract: Time series forecasting plays a crucial role in various applications, particularly in healthcare, where accurate predictions of future health trajectories can significantly impact clinical decision-making. Ensuring transparency and explainability of the models responsible for these tasks is essential for their adoption in critical settings. Recent work has explored a top-down approach to bi-level transparency, focusing on understanding trends and properties of predicted time series using static features. In this work, we extend this framework by incorporating exogenous time series features alongside static features in a structured manner, while maintaining cohesive interpretation. Our approach leverages the insights of trajectory comprehension to introduce an encoding mechanism for exogenous time series, where they are decomposed into meaningful trends and properties, enabling the extraction of interpretable patterns. Through experiments on several synthetic datasets, we demonstrate that our approach remains predictive while preserving interpretability and robustness. This work represents a step towards developing robust, and generalized time series forecasting models. The code is available at https://github.com/jeremy-qin/TIMEVIEW
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-aware LLM-based Online Dataset Annotation</title>
<link>https://arxiv.org/abs/2505.15101</link>
<guid>https://arxiv.org/abs/2505.15101</guid>
<content:encoded><![CDATA[
arXiv:2505.15101v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled automated dataset labeling with minimal human supervision. While majority voting across multiple LLMs can improve label reliability by mitigating individual model biases, it incurs high computational costs due to repeated querying. In this work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo), for efficient and accurate LLM-based dataset annotation. CaMVo adaptively selects a subset of LLMs for each data instance based on contextual embeddings, balancing confidence and cost without requiring pre-training or ground-truth labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator over confidence scores, CaMVo estimates a lower bound on labeling accuracy for each LLM and aggregates responses through weighted majority voting. Our empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates that CaMVo achieves comparable or superior accuracy to full majority voting while significantly reducing labeling costs. This establishes CaMVo as a practical and robust solution for cost-efficient annotation in dynamic labeling environments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Khan-GCL: Kolmogorov-Arnold Network Based Graph Contrastive Learning with Hard Negatives</title>
<link>https://arxiv.org/abs/2505.15103</link>
<guid>https://arxiv.org/abs/2505.15103</guid>
<content:encoded><![CDATA[
arXiv:2505.15103v1 Announce Type: new 
Abstract: Graph contrastive learning (GCL) has demonstrated great promise for learning generalizable graph representations from unlabeled data. However, conventional GCL approaches face two critical limitations: (1) the restricted expressive capacity of multilayer perceptron (MLP) based encoders, and (2) suboptimal negative samples that either from random augmentations-failing to provide effective 'hard negatives'-or generated hard negatives without addressing the semantic distinctions crucial for discriminating graph data. To this end, we propose Khan-GCL, a novel framework that integrates the Kolmogorov-Arnold Network (KAN) into the GCL encoder architecture, substantially enhancing its representational capacity. Furthermore, we exploit the rich information embedded within KAN coefficient parameters to develop two novel critical feature identification techniques that enable the generation of semantically meaningful hard negative samples for each graph representation. These strategically constructed hard negatives guide the encoder to learn more discriminative features by emphasizing critical semantic differences between graphs. Extensive experiments demonstrate that our approach achieves state-of-the-art performance compared to existing GCL methods across a variety of datasets and tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Foundation Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.15116</link>
<guid>https://arxiv.org/abs/2505.15116</guid>
<content:encoded><![CDATA[
arXiv:2505.15116v1 Announce Type: new 
Abstract: Graph-structured data pervades domains such as social networks, biological systems, knowledge graphs, and recommender systems. While foundation models have transformed natural language processing, vision, and multimodal learning through large-scale pretraining and generalization, extending these capabilities to graphs -- characterized by non-Euclidean structures and complex relational semantics -- poses unique challenges and opens new opportunities. To this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose intelligence to structured data, enabling broad transfer across graph-centric tasks and domains. This survey provides a comprehensive overview of GFMs, unifying diverse efforts under a modular framework comprising three key components: backbone architectures, pretraining strategies, and adaptation mechanisms. We categorize GFMs by their generalization scope -- universal, task-specific, and domain-specific -- and review representative methods, key innovations, and theoretical insights within each category. Beyond methodology, we examine theoretical foundations including transferability and emergent capabilities, and highlight key challenges such as structural alignment, heterogeneity, scalability, and evaluation. Positioned at the intersection of graph learning and general-purpose AI, GFMs are poised to become foundational infrastructure for open-ended reasoning over structured data. This survey consolidates current progress and outlines future directions to guide research in this rapidly evolving field. Resources are available at https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.15130</link>
<guid>https://arxiv.org/abs/2505.15130</guid>
<content:encoded><![CDATA[
arXiv:2505.15130v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) such as CLIP have shown remarkable performance in cross-modal tasks through large-scale contrastive pre-training. To adapt these large transformer-based models efficiently for downstream tasks, Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as scalable alternatives to full fine-tuning, especially in few-shot scenarios. However, like traditional deep neural networks, VLMs are highly vulnerable to adversarial attacks, where imperceptible perturbations can significantly degrade model performance. Adversarial training remains the most effective strategy for improving model robustness in PEFT. In this work, we propose AdvCLIP-LoRA, the first algorithm designed to enhance the adversarial robustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method formulates adversarial fine-tuning as a minimax optimization problem and provides theoretical guarantees for convergence under smoothness and nonconvex-strong-concavity assumptions. Empirical results across eight datasets using ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly improves robustness against common adversarial attacks (e.g., FGSM, PGD), without sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA as a practical and theoretically grounded approach for robust adaptation of VLMs in resource-constrained settings.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.15134</link>
<guid>https://arxiv.org/abs/2505.15134</guid>
<content:encoded><![CDATA[
arXiv:2505.15134v1 Announce Type: new 
Abstract: Entropy minimization (EM) trains the model to concentrate even more probability mass on its most confident outputs. We show that this simple objective alone, without any labeled data, can substantially improve large language models' (LLMs) performance on challenging math, physics, and coding tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy similarly to instruction finetuning, but on unlabeled outputs drawn from the model; (2) EM-RL: reinforcement learning with negative entropy as the only reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce entropy without any training data or parameter updates. On Qwen-7B, EM-RL, without any labeled data, achieves comparable or better performance than strong RL baselines such as GRPO and RLOO that are trained on 60K labeled examples. Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the challenging SciCode benchmark, while being 3x more efficient than self-consistency and sequential refinement. Our findings reveal that many pretrained LLMs possess previously underappreciated reasoning capabilities that can be effectively elicited through entropy minimization alone, without any labeled data or even any parameter updates.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm</title>
<link>https://arxiv.org/abs/2505.15138</link>
<guid>https://arxiv.org/abs/2505.15138</guid>
<content:encoded><![CDATA[
arXiv:2505.15138v1 Announce Type: new 
Abstract: This paper investigates infinite-horizon average reward Constrained Markov Decision Processes (CMDPs) with general parametrization. We propose a Primal-Dual Natural Actor-Critic algorithm that adeptly manages constraints while ensuring a high convergence rate. In particular, our algorithm achieves global convergence and constraint violation rates of $\tilde{\mathcal{O}}(1/\sqrt{T})$ over a horizon of length $T$ when the mixing time, $\tau_{\mathrm{mix}}$, is known to the learner. In absence of knowledge of $\tau_{\mathrm{mix}}$, the achievable rates change to $\tilde{\mathcal{O}}(1/T^{0.5-\epsilon})$ provided that $T \geq \tilde{\mathcal{O}}\left(\tau_{\mathrm{mix}}^{2/\epsilon}\right)$. Our results match the theoretical lower bound for Markov Decision Processes and establish a new benchmark in the theoretical exploration of average reward CMDPs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression</title>
<link>https://arxiv.org/abs/2505.15140</link>
<guid>https://arxiv.org/abs/2505.15140</guid>
<content:encoded><![CDATA[
arXiv:2505.15140v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have been widely used for graph analysis. Federated Graph Learning (FGL) is an emerging learning framework to collaboratively train graph data from various clients. However, since clients are required to upload model parameters to the server in each round, this provides the server with an opportunity to infer each client's data privacy. In this paper, we focus on label distribution attacks(LDAs) that aim to infer the label distributions of the clients' local data. We take the first step to attack client's label distributions in FGL. Firstly, we observe that the effectiveness of LDA is closely related to the variance of node embeddings in GNNs. Next, we analyze the relation between them and we propose a new attack named EC-LDA, which significantly improves the attack effectiveness by compressing node embeddings. Thirdly, extensive experiments on node classification and link prediction tasks across six widely used graph datasets show that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimal values under both Cos-sim and JS-div evaluation metrics in the CoraFull and LastFM datasets. Finally, we explore the robustness of EC-LDA under differential privacy protection.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms</title>
<link>https://arxiv.org/abs/2505.15141</link>
<guid>https://arxiv.org/abs/2505.15141</guid>
<content:encoded><![CDATA[
arXiv:2505.15141v1 Announce Type: new 
Abstract: Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes a training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as a Multi-Armed Bandit problem and provide a general speculative decoding framework BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSpec is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filtering Learning Histories Enhances In-Context Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.15143</link>
<guid>https://arxiv.org/abs/2505.15143</guid>
<content:encoded><![CDATA[
arXiv:2505.15143v1 Announce Type: new 
Abstract: Transformer models (TMs) have exhibited remarkable in-context reinforcement learning (ICRL) capabilities, allowing them to generalize to and improve in previously unseen environments without re-training or fine-tuning. This is typically accomplished by imitating the complete learning histories of a source RL algorithm over a substantial amount of pretraining environments, which, however, may transfer suboptimal behaviors inherited from the source algorithm/dataset. Therefore, in this work, we address the issue of inheriting suboptimality from the perspective of dataset preprocessing. Motivated by the success of the weighted empirical risk minimization, we propose a simple yet effective approach, learning history filtering (LHF), to enhance ICRL by reweighting and filtering the learning histories based on their improvement and stability characteristics. To the best of our knowledge, LHF is the first approach to avoid source suboptimality by dataset preprocessing, and can be combined with the current state-of-the-art (SOTA) ICRL algorithms. We substantiate the effectiveness of LHF through a series of experiments conducted on the well-known ICRL benchmarks, encompassing both discrete environments and continuous robotic manipulation tasks, with three SOTA ICRL algorithms (AD, DPT, DICP) as the backbones. LHF exhibits robust performance across a variety of suboptimal scenarios, as well as under varying hyperparameters and sampling strategies. Notably, the superior performance of LHF becomes more pronounced in the presence of noisy data, indicating the significance of filtering learning histories.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Tracker: Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines</title>
<link>https://arxiv.org/abs/2505.15151</link>
<guid>https://arxiv.org/abs/2505.15151</guid>
<content:encoded><![CDATA[
arXiv:2505.15151v1 Announce Type: new 
Abstract: In the past few years, time series foundation models have achieved superior predicting accuracy. However, real-world time series often exhibit significant diversity in their temporal patterns across different time spans and domains, making it challenging for a single model architecture to fit all complex scenarios. In addition, time series data may have multiple variables exhibiting complex correlations between each other. Recent mainstream works have focused on modeling times series in a channel-independent manner in both pretraining and finetuning stages, overlooking the valuable inter-series dependencies. To this end, we propose \textbf{Time Tracker} for better predictions on multivariate time series data. Firstly, we leverage sparse mixture of experts (MoE) within Transformers to handle the modeling of diverse time series patterns, thereby alleviating the learning difficulties of a single model while improving its generalization. Besides, we propose Any-variate Attention, enabling a unified model structure to seamlessly handle both univariate and multivariate time series, thereby supporting channel-independent modeling during pretraining and channel-mixed modeling for finetuning. Furthermore, we design a graph learning module that constructs relations among sequences from frequency-domain features, providing more precise guidance to capture inter-series dependencies in channel-mixed modeling. Based on these advancements, Time Tracker achieves state-of-the-art performance in predicting accuracy, model generalization and adaptability.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation</title>
<link>https://arxiv.org/abs/2505.15152</link>
<guid>https://arxiv.org/abs/2505.15152</guid>
<content:encoded><![CDATA[
arXiv:2505.15152v1 Announce Type: new 
Abstract: Feature Transformation (FT) crafts new features from original ones via mathematical operations to enhance dataset expressiveness for downstream models. However, existing FT methods exhibit critical limitations: discrete search struggles with enormous combinatorial spaces, impeding practical use; and continuous search, being highly sensitive to initialization and step sizes, often becomes trapped in local optima, restricting global exploration. To overcome these limitations, DIFFT redefines FT as a reward-guided generative task. It first learns a compact and expressive latent space for feature sets using a Variational Auto-Encoder (VAE). A Latent Diffusion Model (LDM) then navigates this space to generate high-quality feature embeddings, its trajectory guided by a performance evaluator towards task-specific optima. This synthesis of global distribution learning (from LDM) and targeted optimization (reward guidance) produces potent embeddings, which a novel semi-autoregressive decoder efficiently converts into structured, discrete features, preserving intra-feature dependencies while allowing parallel inter-feature generation. Extensive experiments on 14 benchmark datasets show DIFFT consistently outperforms state-of-the-art baselines in predictive accuracy and robustness, with significantly lower training and inference times.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss</title>
<link>https://arxiv.org/abs/2505.15174</link>
<guid>https://arxiv.org/abs/2505.15174</guid>
<content:encoded><![CDATA[
arXiv:2505.15174v1 Announce Type: new 
Abstract: Lipschitz neural networks are well-known for providing certified robustness in deep learning. In this paper, we present a novel, efficient Block Reflector Orthogonal (BRO) layer that enhances the capability of orthogonal layers on constructing more expressive Lipschitz neural architectures. In addition, by theoretically analyzing the nature of Lipschitz neural networks, we introduce a new loss function that employs an annealing mechanism to increase margin for most data points. This enables Lipschitz models to provide better certified robustness. By employing our BRO layer and loss function, we design BRONet - a simple yet effective Lipschitz neural network that achieves state-of-the-art certified robustness. Extensive experiments and empirical analysis on CIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms existing baselines. The implementation is available at \href{https://github.com/ntuaislab/BRONet}{https://github.com/ntuaislab/BRONet}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpectralGap: Graph-Level Out-of-Distribution Detection via Laplacian Eigenvalue Gaps</title>
<link>https://arxiv.org/abs/2505.15177</link>
<guid>https://arxiv.org/abs/2505.15177</guid>
<content:encoded><![CDATA[
arXiv:2505.15177v1 Announce Type: new 
Abstract: The task of graph-level out-of-distribution (OOD) detection is crucial for deploying graph neural networks in real-world settings. In this paper, we observe a significant difference in the relationship between the largest and second-largest eigenvalues of the Laplacian matrix for in-distribution (ID) and OOD graph samples: \textit{OOD samples often exhibit anomalous spectral gaps (the difference between the largest and second-largest eigenvalues)}. This observation motivates us to propose SpecGap, an effective post-hoc approach for OOD detection on graphs. SpecGap adjusts features by subtracting the component associated with the second-largest eigenvalue, scaled by the spectral gap, from the high-level features (i.e., $\mathbf{X}-\left(\lambda_n-\lambda_{n-1}\right) \mathbf{u}_{n-1} \mathbf{v}_{n-1}^T$). SpecGap achieves state-of-the-art performance across multiple benchmark datasets. We present extensive ablation studies and comprehensive theoretical analyses to support our empirical results. As a parameter-free post-hoc method, SpecGap can be easily integrated into existing graph neural network models without requiring any additional training or model modification.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Gradient-based Framework for Task-agnostic Continual Learning-Unlearning</title>
<link>https://arxiv.org/abs/2505.15178</link>
<guid>https://arxiv.org/abs/2505.15178</guid>
<content:encoded><![CDATA[
arXiv:2505.15178v1 Announce Type: new 
Abstract: Recent advancements in deep models have highlighted the need for intelligent systems that combine continual learning (CL) for knowledge acquisition with machine unlearning (MU) for data removal, forming the Continual Learning-Unlearning (CLU) paradigm. While existing work treats CL and MU as separate processes, we reveal their intrinsic connection through a unified optimization framework based on Kullback-Leibler divergence minimization. This framework decomposes gradient updates for approximate CLU into four components: learning new knowledge, unlearning targeted data, preserving existing knowledge, and modulation via weight saliency. A critical challenge lies in balancing knowledge update and retention during sequential learning-unlearning cycles. To resolve this stability-plasticity dilemma, we introduce a remain-preserved manifold constraint to induce a remaining Hessian compensation for CLU iterations. A fast-slow weight adaptation mechanism is designed to efficiently approximate the second-order optimization direction, combined with adaptive weighting coefficients and a balanced weight saliency mask, proposing a unified implementation framework for gradient-based CLU. Furthermore, we pioneer task-agnostic CLU scenarios that support fine-grained unlearning at the cross-task category and random sample levels beyond the traditional task-aware setups. Experiments demonstrate that the proposed UG-CLU framework effectively coordinates incremental learning, precise unlearning, and knowledge stability across multiple datasets and model architectures, providing a theoretical foundation and methodological support for dynamic, compliant intelligent systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuBM: Mitigating Model Bias in Graph Neural Networks through Neutral Input Calibration</title>
<link>https://arxiv.org/abs/2505.15180</link>
<guid>https://arxiv.org/abs/2505.15180</guid>
<content:encoded><![CDATA[
arXiv:2505.15180v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have shown remarkable performance across various domains, yet they often struggle with model bias, particularly in the presence of class imbalance. This bias can lead to suboptimal performance and unfair predictions, especially for underrepresented classes. We introduce NeuBM (Neutral Bias Mitigation), a novel approach to mitigate model bias in GNNs through neutral input calibration. NeuBM leverages a dynamically updated neutral graph to estimate and correct the inherent biases of the model. By subtracting the logits obtained from the neutral graph from those of the input graph, NeuBM effectively recalibrates the model's predictions, reducing bias across different classes. Our method integrates seamlessly into existing GNN architectures and training procedures, requiring minimal computational overhead. Extensive experiments on multiple benchmark datasets demonstrate that NeuBM significantly improves the balanced accuracy and recall of minority classes, while maintaining strong overall performance. The effectiveness of NeuBM is particularly pronounced in scenarios with severe class imbalance and limited labeled data, where traditional methods often struggle. We provide theoretical insights into how NeuBM achieves bias mitigation, relating it to the concept of representation balancing. Our analysis reveals that NeuBM not only adjusts the final predictions but also influences the learning of balanced feature representations throughout the network.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing</title>
<link>https://arxiv.org/abs/2505.15195</link>
<guid>https://arxiv.org/abs/2505.15195</guid>
<content:encoded><![CDATA[
arXiv:2505.15195v1 Announce Type: new 
Abstract: Retraining a model using its own predictions together with the original, potentially noisy labels is a well-known strategy for improving the model performance. While prior works have demonstrated the benefits of specific heuristic retraining schemes, the question of how to optimally combine the model's predictions and the provided labels remains largely open. This paper addresses this fundamental question for binary classification tasks. We develop a principled framework based on approximate message passing (AMP) to analyze iterative retraining procedures for two ground truth settings: Gaussian mixture model (GMM) and generalized linear model (GLM). Our main contribution is the derivation of the Bayes optimal aggregator function to combine the current model's predictions and the given labels, which when used to retrain the same model, minimizes its prediction error. We also quantify the performance of this optimal retraining strategy over multiple rounds. We complement our theoretical results by proposing a practically usable version of the theoretically-optimal aggregator function for linear probing with the cross-entropy loss, and demonstrate its superiority over baseline methods in the high label noise regime.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems</title>
<link>https://arxiv.org/abs/2505.15201</link>
<guid>https://arxiv.org/abs/2505.15201</guid>
<content:encoded><![CDATA[
arXiv:2505.15201v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes the strength of isolated samples at the expense of the diversity and collective utility of sets of samples. This under-utilizes the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a transformation on the final rewards which leads to direct optimization of pass@k performance, thus optimizing for sets of samples that maximize reward when considered jointly. Our contribution is to derive novel low variance unbiased estimators for pass@k and its gradient, in both the binary and continuous reward settings. We show optimization with our estimators reduces to standard RL with rewards that have been jointly transformed by a stable and efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of trading off pass@1 performance for pass@k gains, our method allows annealing k during training, optimizing both metrics and often achieving strong pass@1 numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source LLM, GEMMA-2. We find that our transformation effectively optimizes for the target k. Furthermore, higher k values enable solving more and harder problems, while annealing k boosts both the pass@1 and pass@k . Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely due to better exploration by prioritizing joint utility over the utility of individual samples.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Distributionally Robust Optimization with Flexible Sample Queries</title>
<link>https://arxiv.org/abs/2505.15212</link>
<guid>https://arxiv.org/abs/2505.15212</guid>
<content:encoded><![CDATA[
arXiv:2505.15212v1 Announce Type: new 
Abstract: Group distributionally robust optimization (GDRO) aims to develop models that perform well across $m$ distributions simultaneously. Existing GDRO algorithms can only process a fixed number of samples per iteration, either 1 or $m$, and therefore can not support scenarios where the sample size varies dynamically. To address this limitation, we investigate GDRO with flexible sample queries and cast it as a two-player game: one player solves an online convex optimization problem, while the other tackles a prediction with limited advice (PLA) problem. Within such a game, we propose a novel PLA algorithm, constructing appropriate loss estimators for cases where the sample size is either 1 or not, and updating the decision using follow-the-regularized-leader. Then, we establish the first high-probability regret bound for non-oblivious PLA. Building upon the above approach, we develop a GDRO algorithm that allows an arbitrary and varying sample size per round, achieving a high-probability optimization error bound of $O\left(\frac{1}{t}\sqrt{\sum_{j=1}^t \frac{m}{r_j}\log m}\right)$, where $r_t$ denotes the sample size at round $t$. This result demonstrates that the optimization error decreases as the number of samples increases and implies a consistent sample complexity of $O(m\log (m)/\epsilon^2)$ for any fixed sample size $r\in[m]$, aligning with existing bounds for cases of $r=1$ or $m$. We validate our approach on synthetic binary and real-world multi-class datasets.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KernelOracle: Predicting the Linux Scheduler's Next Move with Deep Learning</title>
<link>https://arxiv.org/abs/2505.15213</link>
<guid>https://arxiv.org/abs/2505.15213</guid>
<content:encoded><![CDATA[
arXiv:2505.15213v1 Announce Type: new 
Abstract: Efficient task scheduling is paramount in the Linux kernel, where the Completely Fair Scheduler (CFS) meticulously manages CPU resources to balance high utilization with interactive responsiveness. This research pioneers the use of deep learning techniques to predict the sequence of tasks selected by CFS, aiming to evaluate the feasibility of a more generalized and potentially more adaptive task scheduler for diverse workloads. Our core contributions are twofold: first, the systematic generation and curation of a novel scheduling dataset from a running Linux kernel, capturing real-world CFS behavior; and second, the development, training, and evaluation of a Long Short-Term Memory (LSTM) network designed to accurately forecast the next task to be scheduled. This paper further discusses the practical pathways and implications of integrating such a predictive model into the kernel's scheduling framework. The findings and methodologies presented herein open avenues for data-driven advancements in kernel scheduling, with the full source code provided for reproducibility and further exploration.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Degree-Optimized Cumulative Polynomial Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2505.15228</link>
<guid>https://arxiv.org/abs/2505.15228</guid>
<content:encoded><![CDATA[
arXiv:2505.15228v1 Announce Type: new 
Abstract: We introduce cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a neural architecture combining Chebyshev polynomial basis functions and quadratic unconstrained binary optimization (QUBO). Our primary contribution involves reformulating the degree selection problem as a QUBO task, reducing the complexity from $O(D^N)$ to a single optimization step per layer. This approach enables efficient degree selection across neurons while maintaining computational tractability. The architecture performs well in regression tasks with limited data, showing good robustness to input scales and natural regularization properties from its polynomial basis. Additionally, theoretical analysis establishes connections between CP-KAN's performance and properties of financial time series. Our empirical validation across multiple domains demonstrates competitive performance compared to several traditional architectures tested, especially in scenarios where data efficiency and numerical stability are important. Our implementation, including strategies for managing computational overhead in larger networks is available in Ref.~\citep{cpkan_implementation}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding separatrices of dynamical flows with Deep Koopman Eigenfunctions</title>
<link>https://arxiv.org/abs/2505.15231</link>
<guid>https://arxiv.org/abs/2505.15231</guid>
<content:encoded><![CDATA[
arXiv:2505.15231v1 Announce Type: new 
Abstract: Many natural systems, including neural circuits involved in decision making, can be modeled as high-dimensional dynamical systems with multiple stable states. While existing analytical tools primarily describe behavior near stable equilibria, characterizing separatrices -- the manifolds that delineate boundaries between different basins of attraction -- remains challenging, particularly in high-dimensional settings. Here, we introduce a numerical framework leveraging Koopman Theory combined with Deep Neural Networks to effectively characterize separatrices. Specifically, we approximate Koopman Eigenfunctions (KEFs) associated with real positive eigenvalues, which vanish precisely at the separatrices. Utilizing these scalar KEFs, optimization methods efficiently locate separatrices even in complex systems. We demonstrate our approach on synthetic benchmarks, ecological network models, and recurrent neural networks trained on neuroscience-inspired tasks. Moreover, we illustrate the practical utility of our method by designing optimal perturbations that can shift systems across separatrices, enabling predictions relevant to optogenetic stimulation experiments in neuroscience.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers</title>
<link>https://arxiv.org/abs/2505.15239</link>
<guid>https://arxiv.org/abs/2505.15239</guid>
<content:encoded><![CDATA[
arXiv:2505.15239v1 Announce Type: new 
Abstract: The empirical emergence of neural collapse -- a surprising symmetry in the feature representations of the training data in the penultimate layer of deep neural networks -- has spurred a line of theoretical research aimed at its understanding. However, existing work focuses on data-agnostic models or, when data structure is taken into account, it remains limited to multi-layer perceptrons. Our paper fills both these gaps by analyzing modern architectures in a data-aware regime: we prove that global optima of deep regularized transformers and residual networks (ResNets) with LayerNorm trained with cross entropy or mean squared error loss are approximately collapsed, and the approximation gets tighter as the depth grows. More generally, we formally reduce any end-to-end large-depth ResNet or transformer training into an equivalent unconstrained features model, thus justifying its wide use in the literature even beyond data-agnostic settings. Our theoretical results are supported by experiments on computer vision and language datasets showing that, as the depth grows, neural collapse indeed becomes more prominent.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable Vertical Federated Learning in 5G Core Network Architecture</title>
<link>https://arxiv.org/abs/2505.15244</link>
<guid>https://arxiv.org/abs/2505.15244</guid>
<content:encoded><![CDATA[
arXiv:2505.15244v1 Announce Type: new 
Abstract: This work proposes a new algorithm to mitigate model generalization loss in Vertical Federated Learning (VFL) operating under client reliability constraints within 5G Core Networks (CNs). Recently studied and endorsed by 3GPP, VFL enables collaborative and load-balanced model training and inference across the CN. However, the performance of VFL significantly degrades when the Network Data Analytics Functions (NWDAFs) - which serve as primary clients for VFL model training and inference - experience reliability issues stemming from resource constraints and operational overhead. Unlike edge environments, CN environments adopt fundamentally different data management strategies, characterized by more centralized data orchestration capabilities. This presents opportunities to implement better distributed solutions that take full advantage of the CN data handling flexibility. Leveraging this flexibility, we propose a method that optimizes the vertical feature split among clients while centrally defining their local models based on reliability metrics. Our empirical evaluation demonstrates the effectiveness of our proposed algorithm, showing improved performance over traditional baseline methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Spurious Correlations with Causal Logit Perturbation</title>
<link>https://arxiv.org/abs/2505.15246</link>
<guid>https://arxiv.org/abs/2505.15246</guid>
<content:encoded><![CDATA[
arXiv:2505.15246v1 Announce Type: new 
Abstract: Deep learning has seen widespread success in various domains such as science, industry, and society. However, it is acknowledged that certain approaches suffer from non-robustness, relying on spurious correlations for predictions. Addressing these limitations is of paramount importance, necessitating the development of methods that can disentangle spurious correlations. {This study attempts to implement causal models via logit perturbations and introduces a novel Causal Logit Perturbation (CLP) framework to train classifiers with generated causal logit perturbations for individual samples, thereby mitigating the spurious associations between non-causal attributes (i.e., image backgrounds) and classes.} {Our framework employs a} perturbation network to generate sample-wise logit perturbations using a series of training characteristics of samples as inputs. The whole framework is optimized by an online meta-learning-based learning algorithm and leverages human causal knowledge by augmenting metadata in both counterfactual and factual manners. Empirical evaluations on four typical biased learning scenarios, including long-tail learning, noisy label learning, generalized long-tail learning, and subpopulation shift learning, demonstrate that CLP consistently achieves state-of-the-art performance. Moreover, visualization results support the effectiveness of the generated causal perturbations in redirecting model attention towards causal image attributes and dismantling spurious associations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Margin-aware Fuzzy Rough Feature Selection: Bridging Uncertainty Characterization and Pattern Classification</title>
<link>https://arxiv.org/abs/2505.15250</link>
<guid>https://arxiv.org/abs/2505.15250</guid>
<content:encoded><![CDATA[
arXiv:2505.15250v1 Announce Type: new 
Abstract: Fuzzy rough feature selection (FRFS) is an effective means of addressing the curse of dimensionality in high-dimensional data. By removing redundant and irrelevant features, FRFS helps mitigate classifier overfitting, enhance generalization performance, and lessen computational overhead. However, most existing FRFS algorithms primarily focus on reducing uncertainty in pattern classification, neglecting that lower uncertainty does not necessarily result in improved classification performance, despite it commonly being regarded as a key indicator of feature selection effectiveness in the FRFS literature. To bridge uncertainty characterization and pattern classification, we propose a Margin-aware Fuzzy Rough Feature Selection (MAFRFS) framework that considers both the compactness and separation of label classes. MAFRFS effectively reduces uncertainty in pattern classification tasks, while guiding the feature selection towards more separable and discriminative label class structures. Extensive experiments on 15 public datasets demonstrate that MAFRFS is highly scalable and more effective than FRFS. The algorithms developed using MAFRFS outperform six state-of-the-art feature selection algorithms.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Loss-Guided Auxiliary Agents for Overcoming Mode Collapse in GFlowNets</title>
<link>https://arxiv.org/abs/2505.15251</link>
<guid>https://arxiv.org/abs/2505.15251</guid>
<content:encoded><![CDATA[
arXiv:2505.15251v1 Announce Type: new 
Abstract: Although Generative Flow Networks (GFlowNets) are designed to capture multiple modes of a reward function, they often suffer from mode collapse in practice, getting trapped in early discovered modes and requiring prolonged training to find diverse solutions. Existing exploration techniques may rely on heuristic novelty signals. We propose Loss-Guided GFlowNets (LGGFN), a novel approach where an auxiliary GFlowNet's exploration is directly driven by the main GFlowNet's training loss. By prioritizing trajectories where the main model exhibits high loss, LGGFN focuses sampling on poorly understood regions of the state space. This targeted exploration significantly accelerates the discovery of diverse, high-reward samples. Empirically, across various benchmarks including grid environments, structured sequence generation, and Bayesian structure learning, LGGFN consistently enhances exploration efficiency and sample diversity compared to baselines. For instance, on a challenging sequence generation task, it discovered over 40 times more unique valid modes while simultaneously reducing the exploration error metric by approximately 99\%.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search</title>
<link>https://arxiv.org/abs/2505.15259</link>
<guid>https://arxiv.org/abs/2505.15259</guid>
<content:encoded><![CDATA[
arXiv:2505.15259v1 Announce Type: new 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled autonomous agents to interact with computers via Graphical User Interfaces (GUIs), where accurately localizing the coordinates of interface elements (e.g., buttons) is often required for fine-grained actions. However, this remains significantly challenging, leading prior works to rely on large-scale web datasets to improve the grounding accuracy. In this work, we propose Reasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a novel and effective framework for web grounding that enables MLLMs to learn data efficiently through self-generated reasoning and spatial-aware criticism. More specifically, ReGUIDE learns to (i) self-generate a language reasoning process for the localization via online reinforcement learning, and (ii) criticize the prediction using spatial priors that enforce equivariance under input transformations. At inference time, ReGUIDE further boosts performance through a test-time scaling strategy, which combines spatial search with coordinate aggregation. Our experiments demonstrate that ReGUIDE significantly advances web grounding performance across multiple benchmarks, outperforming baselines with substantially fewer training data points (e.g., only 0.2% samples compared to the best open-sourced baselines).
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Diffusion Transformers Efficiently via $\mu$P</title>
<link>https://arxiv.org/abs/2505.15270</link>
<guid>https://arxiv.org/abs/2505.15270</guid>
<content:encoded><![CDATA[
arXiv:2505.15270v1 Announce Type: new 
Abstract: Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization ($\mu$P) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether $\mu$P of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard $\mu$P to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that $\mu$P of mainstream diffusion Transformers, including DiT, U-ViT, PixArt-$\alpha$, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing $\mu$P methodologies. Leveraging this result, we systematically demonstrate that DiT-$\mu$P enjoys robust HP transferability. Notably, DiT-XL-2-$\mu$P with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of $\mu$P on text-to-image generation by scaling PixArt-$\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under $\mu$P outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-$\alpha$ and 3% of consumption by human experts for MMDiT-18B. These results establish $\mu$P as a principled and efficient framework for scaling diffusion Transformers.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel PCA for Out-of-Distribution Detection: Non-Linear Kernel Selections and Approximations</title>
<link>https://arxiv.org/abs/2505.15284</link>
<guid>https://arxiv.org/abs/2505.15284</guid>
<content:encoded><![CDATA[
arXiv:2505.15284v1 Announce Type: new 
Abstract: Out-of-Distribution (OoD) detection is vital for the reliability of deep neural networks, the key of which lies in effectively characterizing the disparities between OoD and In-Distribution (InD) data. In this work, such disparities are exploited through a fresh perspective of non-linear feature subspace. That is, a discriminative non-linear subspace is learned from InD features to capture representative patterns of InD, while informative patterns of OoD features cannot be well captured in such a subspace due to their different distribution. Grounded on this perspective, we exploit the deviations of InD and OoD features in such a non-linear subspace for effective OoD detection. To be specific, we leverage the framework of Kernel Principal Component Analysis (KPCA) to attain the discriminative non-linear subspace and deploy the reconstruction error on such subspace to distinguish InD and OoD data. Two challenges emerge: (i) the learning of an effective non-linear subspace, i.e., the selection of kernel function in KPCA, and (ii) the computation of the kernel matrix with large-scale InD data. For the former, we reveal two vital non-linear patterns that closely relate to the InD-OoD disparity, leading to the establishment of a Cosine-Gaussian kernel for constructing the subspace. For the latter, we introduce two techniques to approximate the Cosine-Gaussian kernel with significantly cheap computations. In particular, our approximation is further tailored by incorporating the InD data confidence, which is demonstrated to promote the learning of discriminative subspaces for OoD data. Our study presents new insights into the non-linear feature subspace for OoD detection and contributes practical explorations on the associated kernel design and efficient computations, yielding a KPCA detection method with distinctively improved efficacy and efficiency.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models</title>
<link>https://arxiv.org/abs/2505.15293</link>
<guid>https://arxiv.org/abs/2505.15293</guid>
<content:encoded><![CDATA[
arXiv:2505.15293v1 Announce Type: new 
Abstract: Policy exploration is critical in reinforcement learning (RL), where existing approaches include greedy, Gaussian process, etc. However, these approaches utilize preset stochastic processes and are indiscriminately applied in all kinds of RL tasks without considering task-specific features that influence policy exploration. Moreover, during RL training, the evolution of such stochastic processes is rigid, which typically only incorporates a decay in the variance, failing to adjust flexibly according to the agent's real-time learning status. Inspired by the analyzing and reasoning capability of large language models (LLMs), we design LLM-Explorer to adaptively generate task-specific exploration strategies with LLMs, enhancing the policy exploration in RL. In our design, we sample the learning trajectory of the agent during the RL training in a given task and prompt the LLM to analyze the agent's current policy learning status and then generate a probability distribution for future policy exploration. Updating the probability distribution periodically, we derive a stochastic process specialized for the particular task and dynamically adjusted to adapt to the learning process. Our design is a plug-in module compatible with various widely applied RL algorithms, including the DQN series, DDPG, TD3, and any possible variants developed based on them. Through extensive experiments on the Atari and MuJoCo benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy exploration, achieving an average performance improvement up to 37.27%. Our code is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for reproducibility.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Laplace Sample Information: Data Informativeness Through a Bayesian Lens</title>
<link>https://arxiv.org/abs/2505.15303</link>
<guid>https://arxiv.org/abs/2505.15303</guid>
<content:encoded><![CDATA[
arXiv:2505.15303v1 Announce Type: new 
Abstract: Accurately estimating the informativeness of individual samples in a dataset is an important objective in deep learning, as it can guide sample selection, which can improve model efficiency and accuracy by removing redundant or potentially harmful samples. We propose Laplace Sample Information (LSI) measure of sample informativeness grounded in information theory widely applicable across model architectures and learning settings. LSI leverages a Bayesian approximation to the weight posterior and the KL divergence to measure the change in the parameter distribution induced by a sample of interest from the dataset. We experimentally show that LSI is effective in ordering the data with respect to typicality, detecting mislabeled samples, measuring class-wise informativeness, and assessing dataset difficulty. We demonstrate these capabilities of LSI on image and text data in supervised and unsupervised settings. Moreover, we show that LSI can be computed efficiently through probes and transfers well to the training of large models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One</title>
<link>https://arxiv.org/abs/2505.15306</link>
<guid>https://arxiv.org/abs/2505.15306</guid>
<content:encoded><![CDATA[
arXiv:2505.15306v1 Announce Type: new 
Abstract: Model ensemble is a useful approach in reinforcement learning (RL) for training effective agents. Despite wide success of RL, training effective agents remains difficult due to the multitude of factors requiring careful tuning, such as algorithm selection, hyperparameter settings, and even random seed choices, all of which can significantly influence an agent's performance. Model ensemble helps overcome this challenge by combining multiple weak agents into a single, more powerful one, enhancing overall performance. However, existing ensemble methods, such as majority voting and Boltzmann addition, are designed as fixed strategies and lack a semantic understanding of specific tasks, limiting their adaptability and effectiveness. To address this, we propose LLM-Ens, a novel approach that enhances RL model ensemble with task-specific semantic understandings driven by large language models (LLMs). Given a task, we first design an LLM to categorize states in this task into distinct 'situations', incorporating high-level descriptions of the task conditions. Then, we statistically analyze the strengths and weaknesses of each individual agent to be used in the ensemble in each situation. During the inference time, LLM-Ens dynamically identifies the changing task situation and switches to the agent that performs best in the current situation, ensuring dynamic model selection in the evolving task condition. Our approach is designed to be compatible with agents trained with different random seeds, hyperparameter settings, and various RL algorithms. Extensive experiments on the Atari benchmark show that LLM-Ens significantly improves the RL model ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility, our code is open-source at https://anonymous.4open.science/r/LLM4RLensemble-F7EE.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.15311</link>
<guid>https://arxiv.org/abs/2505.15311</guid>
<content:encoded><![CDATA[
arXiv:2505.15311v1 Announce Type: new 
Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines for large language model (LLM) reasoning, leaving value-based approaches largely unexplored. We revisit the classical paradigm of Bellman Residual Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an algorithm that naturally adapts this idea to LLMs, yielding a simple yet effective off-policy algorithm that optimizes a single trajectory-level Bellman objective using the model's own logits as $Q$-values. TBRM removes the need for critics, importance-sampling ratios, or clipping, and operates with only one rollout per prompt. We prove convergence to the near-optimal KL-regularized policy from arbitrary off-policy data via an improved change-of-trajectory-measure analysis. Experiments on standard mathematical-reasoning benchmarks show that TBRM consistently outperforms policy-based baselines, like PPO and GRPO, with comparable or lower computational and memory overhead. Our results indicate that value-based RL might be a principled and efficient alternative for enhancing reasoning capabilities in LLMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.15312</link>
<guid>https://arxiv.org/abs/2505.15312</guid>
<content:encoded><![CDATA[
arXiv:2505.15312v1 Announce Type: new 
Abstract: Multivariable time series forecasting methods can integrate information from exogenous variables, leading to significant prediction accuracy gains. Transformer architecture has been widely applied in various time series forecasting models due to its ability to capture long-range sequential dependencies. However, a na\"ive application of transformers often struggles to effectively model complex relationships among variables over time. To mitigate against this, we propose a novel architecture, namely the Spectral Operator Neural Network (Sonnet). Sonnet applies learnable wavelet transformations to the input and incorporates spectral analysis using the Koopman operator. Its predictive skill relies on the Multivariable Coherence Attention (MVCA), an operation that leverages spectral coherence to model variable dependencies. Our empirical analysis shows that Sonnet yields the best performance on $34$ out of $47$ forecasting tasks with an average mean absolute error (MAE) reduction of $1.1\%$ against the most competitive baseline (different per task). We further show that MVCA -- when put in place of the na\"ive attention used in various deep learning models -- can remedy its deficiencies, reducing MAE by $10.7\%$ on average in the most challenging forecasting tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier-Invertible Neural Encoder (FINE) for Homogeneous Flows</title>
<link>https://arxiv.org/abs/2505.15329</link>
<guid>https://arxiv.org/abs/2505.15329</guid>
<content:encoded><![CDATA[
arXiv:2505.15329v1 Announce Type: new 
Abstract: Invertible neural architectures have recently attracted attention for their compactness, interpretability, and information-preserving properties. In this work, we propose the Fourier-Invertible Neural Encoder (FINE), which combines invertible monotonic activation functions with reversible filter structures, and could be extended using Invertible ResNets. This architecture is examined in learning low-dimensional representations of one-dimensional nonlinear wave interactions and exact circular translation symmetry. Dimensionality is preserved across layers, except for a Fourier truncation step in the latent space, which enables dimensionality reduction while maintaining shift equivariance and interpretability. Our results demonstrate that FINE significantly outperforms classical linear methods such as Discrete Fourier Transformation (DFT) and Proper Orthogonal Decomposition (POD), and achieves reconstruction accuracy better than conventional deep autoencoders with convolutional layers (CNN) - while using substantially smaller models and offering superior physical interpretability. These findings suggest that invertible single-neuron networks, when combined with spectral truncation, offer a promising framework for learning compact and interpretable representations of physics datasets, and symmetry-aware representation learning in physics-informed machine learning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSR: Speculative Parallel Scaling Reasoning in Test-time</title>
<link>https://arxiv.org/abs/2505.15340</link>
<guid>https://arxiv.org/abs/2505.15340</guid>
<content:encoded><![CDATA[
arXiv:2505.15340v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive results on multi-step mathematical reasoning, yet at the cost of high computational overhead. This challenge is particularly acute for test-time scaling methods such as parallel decoding, which increase answer diversity but scale poorly in efficiency. To address this efficiency-accuracy trade-off, we propose SSR (Speculative Parallel Scaling Reasoning), a training-free framework that leverages a key insight: by introducing speculative decoding at the step level, we can accelerate reasoning without sacrificing correctness. SSR integrates two components: a Selective Parallel Module (SPM) that identifies a small set of promising reasoning strategies via model-internal scoring, and Step-level Speculative Decoding (SSD), which enables efficient draft-target collaboration for fine-grained reasoning acceleration. Experiments on three mathematical benchmarks-AIME 2024, MATH-500, and LiveMathBench - demonstrate that SSR achieves strong gains over baselines. For instance, on LiveMathBench, SSR improves pass@1 accuracy by 13.84% while reducing computation to 80.5% of the baseline FLOPs. On MATH-500, SSR reduces compute to only 30% with no loss in accuracy.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hadamax Encoding: Elevating Performance in Model-Free Atari</title>
<link>https://arxiv.org/abs/2505.15345</link>
<guid>https://arxiv.org/abs/2505.15345</guid>
<content:encoded><![CDATA[
arXiv:2505.15345v1 Announce Type: new 
Abstract: Neural network architectures have a large impact in machine learning. In reinforcement learning, network architectures have remained notably simple, as changes often lead to small gains in performance. This work introduces a novel encoder architecture for pixel-based model-free reinforcement learning. The Hadamax (\textbf{Hada}mard \textbf{max}-pooling) encoder achieves state-of-the-art performance by max-pooling Hadamard products between GELU-activated parallel hidden layers. Based on the recent PQN algorithm, the Hadamax encoder achieves state-of-the-art model-free performance in the Atari-57 benchmark. Specifically, without applying any algorithmic hyperparameter modifications, Hadamax-PQN achieves an 80\% performance gain over vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility, the full code is available on \href{https://github.com/Jacobkooi/Hadamax}{GitHub}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human in the Loop Adaptive Optimization for Improved Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.15354</link>
<guid>https://arxiv.org/abs/2505.15354</guid>
<content:encoded><![CDATA[
arXiv:2505.15354v1 Announce Type: new 
Abstract: Time series forecasting models often produce systematic, predictable errors even in critical domains such as energy, finance, and healthcare. We introduce a novel post training adaptive optimization framework that improves forecast accuracy without retraining or architectural changes. Our method automatically applies expressive transformations optimized via reinforcement learning, contextual bandits, or genetic algorithms to correct model outputs in a lightweight and model agnostic way. Theoretically, we prove that affine corrections always reduce the mean squared error; practically, we extend this idea with dynamic action based optimization. The framework also supports an optional human in the loop component: domain experts can guide corrections using natural language, which is parsed into actions by a language model. Across multiple benchmarks (e.g., electricity, weather, traffic), we observe consistent accuracy gains with minimal computational overhead. Our interactive demo shows the framework's real time usability. By combining automated post hoc refinement with interpretable and extensible mechanisms, our approach offers a powerful new direction for practical forecasting systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Federated Learning with Client Drift Minimization</title>
<link>https://arxiv.org/abs/2505.15371</link>
<guid>https://arxiv.org/abs/2505.15371</guid>
<content:encoded><![CDATA[
arXiv:2505.15371v1 Announce Type: new 
Abstract: Federated learning (FL) faces critical challenges, particularly in heterogeneous environments where non-independent and identically distributed data across clients can lead to unfair and inefficient model performance. In this work, we introduce \textit{DRDM}, a novel algorithm that addresses these issues by combining a distributionally robust optimization (DRO) framework with dynamic regularization to mitigate client drift. \textit{DRDM} frames the training as a min-max optimization problem aimed at maximizing performance for the worst-case client, thereby promoting robustness and fairness. This robust objective is optimized through an algorithm leveraging dynamic regularization and efficient local updates, which significantly reduces the required number of communication rounds. Moreover, we provide a theoretical convergence analysis for convex smooth objectives under partial participation. Extensive experiments on three benchmark datasets, covering various model architectures and data heterogeneity levels, demonstrate that \textit{DRDM} significantly improves worst-case test accuracy while requiring fewer communication rounds than existing state-of-the-art baselines. Furthermore, we analyze the impact of signal-to-noise ratio (SNR) and bandwidth on the energy consumption of participating clients, demonstrating that the number of local update steps can be adaptively selected to achieve a target worst-case test accuracy with minimal total energy cost across diverse communication environments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InTreeger: An End-to-End Framework for Integer-Only Decision Tree Inference</title>
<link>https://arxiv.org/abs/2505.15391</link>
<guid>https://arxiv.org/abs/2505.15391</guid>
<content:encoded><![CDATA[
arXiv:2505.15391v1 Announce Type: new 
Abstract: Integer quantization has emerged as a critical technique to facilitate deployment on resource-constrained devices. Although they do reduce the complexity of the learning models, their inference performance is often prone to quantization-induced errors. To this end, we introduce InTreeger: an end-to-end framework that takes a training dataset as input, and outputs an architecture-agnostic integer-only C implementation of tree-based machine learning model, without loss of precision. This framework enables anyone, even those without prior experience in machine learning, to generate a highly optimized integer-only classification model that can run on any hardware simply by providing an input dataset and target variable. We evaluated our generated implementations across three different architectures (ARM, x86, and RISC-V), resulting in significant improvements in inference latency. In addition, we show the energy efficiency compared to typical decision tree implementations that rely on floating-point arithmetic. The results underscore the advantages of integer-only inference, making it particularly suitable for energy- and area-constrained devices such as embedded systems and edge computing platforms, while also enabling the execution of decision trees on existing ultra-low power devices.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOPSE: Scalable Higher-Order Positional and Structural Encoder for Combinatorial Representations</title>
<link>https://arxiv.org/abs/2505.15405</link>
<guid>https://arxiv.org/abs/2505.15405</guid>
<content:encoded><![CDATA[
arXiv:2505.15405v1 Announce Type: new 
Abstract: While Graph Neural Networks (GNNs) have proven highly effective at modeling relational data, pairwise connections cannot fully capture multi-way relationships naturally present in complex real-world systems. In response to this, Topological Deep Learning (TDL) leverages more general combinatorial representations -- such as simplicial or cellular complexes -- to accommodate higher-order interactions. Existing TDL methods often extend GNNs through Higher-Order Message Passing (HOMP), but face critical \emph{scalability challenges} due to \textit{(i)} a combinatorial explosion of message-passing routes, and \textit{(ii)} significant complexity overhead from the propagation mechanism. To overcome these limitations, we propose HOPSE (Higher-Order Positional and Structural Encoder) -- a \emph{message passing-free} framework that uses Hasse graph decompositions to derive efficient and expressive encodings over \emph{arbitrary higher-order domains}. Notably, HOPSE scales linearly with dataset size while preserving expressive power and permutation equivariance. Experiments on molecular, expressivity and topological benchmarks show that HOPSE matches or surpasses state-of-the-art performance while achieving up to 7 $times$ speedups over HOMP-based models, opening a new path for scalable TDL.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Differentiable Approximation of Generalized Low-rank Regularization</title>
<link>https://arxiv.org/abs/2505.15407</link>
<guid>https://arxiv.org/abs/2505.15407</guid>
<content:encoded><![CDATA[
arXiv:2505.15407v1 Announce Type: new 
Abstract: Low-rank regularization (LRR) has been widely applied in various machine learning tasks, but the associated optimization is challenging. Directly optimizing the rank function under constraints is NP-hard in general. To overcome this difficulty, various relaxations of the rank function were studied. However, optimization of these relaxed LRRs typically depends on singular value decomposition, which is a time-consuming and nondifferentiable operator that cannot be optimized with gradient-based techniques. To address these challenges, in this paper we propose an efficient differentiable approximation of the generalized LRR. The considered LRR form subsumes many popular choices like the nuclear norm, the Schatten-$p$ norm, and various nonconvex relaxations. Our method enables LRR terms to be appended to loss functions in a plug-and-play fashion, and the GPU-friendly operations enable efficient and convenient implementation. Furthermore, convergence analysis is presented, which rigorously shows that both the bias and the variance of our rank estimator rapidly reduce with increased sample size and iteration steps. In the experimental study, the proposed method is applied to various tasks, which demonstrates its versatility and efficiency. Code is available at https://github.com/naiqili/EDLRR.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Policy Optimization under Partial Observability</title>
<link>https://arxiv.org/abs/2505.15418</link>
<guid>https://arxiv.org/abs/2505.15418</guid>
<content:encoded><![CDATA[
arXiv:2505.15418v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) in partially observable environments poses significant challenges due to the complexity of learning under uncertainty. While additional information, such as that available in simulations, can enhance training, effectively leveraging it remains an open problem. To address this, we introduce Guided Policy Optimization (GPO), a framework that co-trains a guider and a learner. The guider takes advantage of privileged information while ensuring alignment with the learner's policy that is primarily trained via imitation learning. We theoretically demonstrate that this learning scheme achieves optimality comparable to direct RL, thereby overcoming key limitations inherent in existing approaches. Empirical evaluations show strong performance of GPO across various tasks, including continuous control with partial observability and noise, and memory-based challenges, significantly outperforming existing methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding</title>
<link>https://arxiv.org/abs/2505.15423</link>
<guid>https://arxiv.org/abs/2505.15423</guid>
<content:encoded><![CDATA[
arXiv:2505.15423v1 Announce Type: new 
Abstract: Capturing nonlinear relationships without sacrificing interpretability remains a persistent challenge in regression modeling. We introduce SplitWise, a novel framework that enhances stepwise regression. It adaptively transforms numeric predictors into threshold-based binary features using shallow decision trees, but only when such transformations improve model fit, as assessed by the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC). This approach preserves the transparency of linear models while flexibly capturing nonlinear effects. Implemented as a user-friendly R package, SplitWise is evaluated on both synthetic and real-world datasets. The results show that it consistently produces more parsimonious and generalizable models than traditional stepwise and penalized regression techniques.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Set-LLM: A Permutation-Invariant LLM</title>
<link>https://arxiv.org/abs/2505.15433</link>
<guid>https://arxiv.org/abs/2505.15433</guid>
<content:encoded><![CDATA[
arXiv:2505.15433v1 Announce Type: new 
Abstract: While large language models (LLMs) demonstrate impressive capabilities across numerous applications, their robustness remains a critical concern. This paper is motivated by a specific vulnerability: the order sensitivity of LLMs. This vulnerability manifests itself as the order bias observed when LLMs decide between possible options (for example, a preference for the first option) and the tendency of LLMs to provide different answers when options are reordered. The use cases for this scenario extend beyond the classical case of multiple-choice question answering to the use of LLMs as automated evaluators in AI pipelines, comparing output generated by different models. We introduce Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the processing of mixed set-text inputs with permutation invariance guarantees. The adaptations involve a new attention mask and new positional encodings specifically designed for sets. We provide a theoretical proof of invariance and demonstrate through experiments that Set-LLM can be trained effectively, achieving comparable or improved performance and maintaining the runtime of the original model, while eliminating order sensitivity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes</title>
<link>https://arxiv.org/abs/2505.15496</link>
<guid>https://arxiv.org/abs/2505.15496</guid>
<content:encoded><![CDATA[
arXiv:2505.15496v1 Announce Type: new 
Abstract: We present new fast-rate generalization bounds for multi-task and meta-learning in the unbalanced setting, i.e. when the tasks have training sets of different sizes, as is typically the case in real-world scenarios. Previously, only standard-rate bounds were known for this situation, while fast-rate bounds were limited to the setting where all training sets are of equal size. Our new bounds are numerically computable as well as interpretable, and we demonstrate their flexibility in handling a number of cases where they give stronger guarantees than previous bounds. Besides the bounds themselves, we also make conceptual contributions: we demonstrate that the unbalanced multi-task setting has different statistical properties than the balanced situation, specifically that proofs from the balanced situation do not carry over to the unbalanced setting. Additionally, we shed light on the fact that the unbalanced situation allows two meaningful definitions of multi-task risk, depending on whether if all tasks should be considered equally important or if sample-rich tasks should receive more weight than sample-poor ones.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certified Neural Approximations of Nonlinear Dynamics</title>
<link>https://arxiv.org/abs/2505.15497</link>
<guid>https://arxiv.org/abs/2505.15497</guid>
<content:encoded><![CDATA[
arXiv:2505.15497v1 Announce Type: new 
Abstract: Neural networks hold great potential to act as approximate models of nonlinear dynamical systems, with the resulting neural approximations enabling verification and control of such systems. However, in safety-critical contexts, the use of neural approximations requires formal bounds on their closeness to the underlying system. To address this fundamental challenge, we propose a novel, adaptive, and parallelizable verification method based on certified first-order models. Our approach provides formal error bounds on the neural approximations of dynamical systems, allowing them to be safely employed as surrogates by interpreting the error bound as bounded disturbances acting on the approximated dynamics. We demonstrate the effectiveness and scalability of our method on a range of established benchmarks from the literature, showing that it outperforms the state-of-the-art. Furthermore, we highlight the flexibility of our framework by applying it to two novel scenarios not previously explored in this context: neural network compression and an autoencoder-based deep learning architecture for learning Koopman operators, both yielding compelling results.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning</title>
<link>https://arxiv.org/abs/2505.15507</link>
<guid>https://arxiv.org/abs/2505.15507</guid>
<content:encoded><![CDATA[
arXiv:2505.15507v1 Announce Type: new 
Abstract: We introduce a new algebraic structure for multi-dimensional compositional embeddings, built on directional non-commutative monoidal operators. The core contribution of this work is this novel framework, which exhibits appealing theoretical properties (associativity along each dimension and an interchange law ensuring global consistency) while remaining compatible with modern machine learning architectures. Our construction defines a distinct composition operator circ_i for each axis i, ensuring associative combination along each axis without imposing global commutativity. Importantly, all axis-specific operators commute with one another, enforcing a global interchange law that enables consistent crossaxis compositions. This is, to our knowledge, the first approach that provides a common foundation that generalizes classical sequence-modeling paradigms (e.g., structured state-space models (SSMs) and transformer self-attention) to a unified multi-dimensional framework. For example, specific one-dimensional instances of our framework can recover the familiar affine transformation algebra, vanilla self-attention, and the SSM-style recurrence. The higher-dimensional generalizations naturally support recursive, structure-aware operations in embedding spaces. We outline several potential applications unlocked by this structure-including structured positional encodings in Transformers, directional image embeddings, and symbolic modeling of sequences or grids-indicating that it could inform future deep learning model designs. We formally establish the algebraic properties of our framework and discuss efficient implementations. Finally, as our focus is theoretical, we include no experiments here and defer empirical validation to future work, which we plan to undertake.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NOMAD Projection</title>
<link>https://arxiv.org/abs/2505.15511</link>
<guid>https://arxiv.org/abs/2505.15511</guid>
<content:encoded><![CDATA[
arXiv:2505.15511v1 Announce Type: new 
Abstract: The rapid adoption of generative AI has driven an explosion in the size of datasets consumed and produced by AI models. Traditional methods for unstructured data visualization, such as t-SNE and UMAP, have not kept up with the pace of dataset scaling. This presents a significant challenge for AI explainability, which relies on methods such as t-SNE and UMAP for exploratory data analysis. In this paper, we introduce Negative Or Mean Affinity Discrimination (NOMAD) Projection, the first method for unstructured data visualization via nonlinear dimensionality reduction that can run on multiple GPUs at train time. We provide theory that situates NOMAD Projection as an approximate upper bound on the InfoNC-t-SNE loss, and empirical results that demonstrate NOMAD Projection's superior performance and speed profile compared to existing state-of-the-art methods. We demonstrate the scalability of NOMAD Projection by computing the first complete data map of Multilingual Wikipedia.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2505.15514</link>
<guid>https://arxiv.org/abs/2505.15514</guid>
<content:encoded><![CDATA[
arXiv:2505.15514v1 Announce Type: new 
Abstract: Proximal Policy Optimization (PPO) is a widely used reinforcement learning algorithm that heavily relies on accurate advantage estimates for stable and efficient training. However, raw advantage signals can exhibit significant variance, noise, and scale-related issues, impeding optimal learning performance. To address this challenge, we introduce Advantage Modulation PPO (AM-PPO), a novel enhancement of PPO that adaptively modulates advantage estimates using a dynamic, non-linear scaling mechanism. This adaptive modulation employs an alpha controller that dynamically adjusts the scaling factor based on evolving statistical properties of the advantage signals, such as their norm, variance, and a predefined target saturation level. By incorporating a tanh-based gating function driven by these adaptively scaled advantages, AM-PPO reshapes the advantage signals to stabilize gradient updates and improve the conditioning of the policy gradient landscape. Crucially, this modulation also influences value function training by providing consistent and adaptively conditioned learning targets. Empirical evaluations across standard continuous control benchmarks demonstrate that AM-PPO achieves superior reward trajectories, exhibits sustained learning progression, and significantly reduces the clipping required by adaptive optimizers. These findings underscore the potential of advantage modulation as a broadly applicable technique for enhancing reinforcement learning optimization.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable embeddings with Distance Explainer</title>
<link>https://arxiv.org/abs/2505.15516</link>
<guid>https://arxiv.org/abs/2505.15516</guid>
<content:encoded><![CDATA[
arXiv:2505.15516v1 Announce Type: new 
Abstract: While eXplainable AI (XAI) has advanced significantly, few methods address interpretability in embedded vector spaces where dimensions represent complex abstractions. We introduce Distance Explainer, a novel method for generating local, post-hoc explanations of embedded spaces in machine learning models. Our approach adapts saliency-based techniques from RISE to explain the distance between two embedded data points by assigning attribution values through selective masking and distance-ranked mask filtering. We evaluate Distance Explainer on cross-modal embeddings (image-image and image-caption pairs) using established XAI metrics including Faithfulness, Sensitivity/Robustness, and Randomization. Experiments with ImageNet and CLIP models demonstrate that our method effectively identifies features contributing to similarity or dissimilarity between embedded data points while maintaining high robustness and consistency. We also explore how parameter tuning, particularly mask quantity and selection strategy, affects explanation quality. This work addresses a critical gap in XAI research and enhances transparency and trustworthiness in deep learning applications utilizing embedded spaces.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Temporal Difference Method for Stochastic Continuous Dynamics</title>
<link>https://arxiv.org/abs/2505.15544</link>
<guid>https://arxiv.org/abs/2505.15544</guid>
<content:encoded><![CDATA[
arXiv:2505.15544v1 Announce Type: new 
Abstract: For continuous systems modeled by dynamical equations such as ODEs and SDEs, Bellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman (HJB) equation, which provides the theoretical target of reinforcement learning (RL). Although recent advances in RL successfully leverage this formulation, the existing methods typically assume the underlying dynamics are known a priori because they need explicit access to the coefficient functions of dynamical equations to update the value function following the HJB equation. We address this inherent limitation of HJB-based RL; we propose a model-free approach still targeting the HJB equation and propose the corresponding temporal difference method. We demonstrate its potential advantages over transition kernel-based formulations, both qualitatively and empirically. The proposed formulation paves the way toward bridging stochastic optimal control and model-free reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oversmoothing, "Oversquashing", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning</title>
<link>https://arxiv.org/abs/2505.15547</link>
<guid>https://arxiv.org/abs/2505.15547</guid>
<content:encoded><![CDATA[
arXiv:2505.15547v1 Announce Type: new 
Abstract: After a renaissance phase in which researchers revisited the message-passing paradigm through the lens of deep learning, the graph machine learning community shifted its attention towards a deeper and practical understanding of message-passing's benefits and limitations. In this position paper, we notice how the fast pace of progress around the topics of oversmoothing and oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came with the consolidation of commonly accepted beliefs and assumptions that are not always true nor easy to distinguish from each other. We argue that this has led to ambiguities around the investigated problems, preventing researchers from focusing on and addressing precise research questions while causing a good amount of misunderstandings. Our contribution wants to make such common beliefs explicit and encourage critical thinking around these topics, supported by simple but noteworthy counterexamples. The hope is to clarify the distinction between the different issues and promote separate but intertwined research directions to address them.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Short-Range Dependency Effects on Transformer Instability and a Decomposed Attention Solution</title>
<link>https://arxiv.org/abs/2505.15548</link>
<guid>https://arxiv.org/abs/2505.15548</guid>
<content:encoded><![CDATA[
arXiv:2505.15548v1 Announce Type: new 
Abstract: Transformer language models have driven significant progress across various fields, including natural language processing and computer vision. A central component of these models is the self-attention (SA) mechanism, which learns rich vector representations of tokens by modeling their relationships with others in a sequence. However, despite extensive research, transformers continue to suffer from training instability -- often manifesting as spikes or divergence in the training loss during a run.
  In this work, we identify one source of this instability: SA's limited ability to capture short-range dependencies, especially in tasks like language modeling, where almost every token heavily relies on its nearby neighbors. This limitation causes the pre-softmax logits of SA to grow rapidly, destabilizing training. To address this, we propose decomposing the SA into local (short-range) and global (long-range) attention heads. This decomposed attention, referred to as Long Short-attention (LS-attention), mitigates logit explosion and results in more stable training compared to an equivalent multi-head self-attention (MHSA). Empirical comparisons with two alternative training stabilization methods show that LS-attention reduces the validation perplexity to nearly 2/5 of that achieved by one method and reaches a similar perplexity as the other method using only 1/20 of the GPU hours. Additionally, our experiments demonstrate that LS-attention reduces inference latency by up to 36% compared to a state-of-the-art implementation of equivalent MHSA.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Data Sparsity on Machine Learning for Fault Detection in Power System Protection</title>
<link>https://arxiv.org/abs/2505.15560</link>
<guid>https://arxiv.org/abs/2505.15560</guid>
<content:encoded><![CDATA[
arXiv:2505.15560v1 Announce Type: new 
Abstract: Germany's transition to a renewable energy-based power system is reshaping grid operations, requiring advanced monitoring and control to manage decentralized generation. Machine learning (ML) has emerged as a powerful tool for power system protection, particularly for fault detection (FD) and fault line identification (FLI) in transmission grids. However, ML model reliability depends on data quality and availability. Data sparsity resulting from sensor failures, communication disruptions, or reduced sampling rates poses a challenge to ML-based FD and FLI. Yet, its impact has not been systematically validated prior to this work. In response, we propose a framework to assess the impact of data sparsity on ML-based FD and FLI performance. We simulate realistic data sparsity scenarios, evaluate their impact, derive quantitative insights, and demonstrate the effectiveness of this evaluation strategy by applying it to an existing ML-based framework. Results show the ML model remains robust for FD, maintaining an F1-score of 0.999 $\pm$ 0.000 even after a 50x data reduction. In contrast, FLI is more sensitive, with performance decreasing by 55.61% for missing voltage measurements and 9.73% due to communication failures at critical network points. These findings offer actionable insights for optimizing ML models for real-world grid protection. This enables more efficient FD and supports targeted improvements in FLI.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refining Neural Activation Patterns for Layer-Level Concept Discovery in Neural Network-Based Receivers</title>
<link>https://arxiv.org/abs/2505.15570</link>
<guid>https://arxiv.org/abs/2505.15570</guid>
<content:encoded><![CDATA[
arXiv:2505.15570v1 Announce Type: new 
Abstract: Concept discovery in neural networks often targets individual neurons or human-interpretable features, overlooking distributed layer-wide patterns. We study the Neural Activation Pattern (NAP) methodology, which clusters full-layer activation distributions to identify such layer-level concepts. Applied to visual object recognition and radio receiver models, we propose improved normalization, distribution estimation, distance metrics, and varied cluster selection. In the radio receiver model, distinct concepts did not emerge; instead, a continuous activation manifold shaped by Signal-to-Noise Ratio (SNR) was observed -- highlighting SNR as a key learned factor, consistent with classical receiver behavior and supporting physical plausibility. Our enhancements to NAP improved in-distribution vs. out-of-distribution separation, suggesting better generalization and indirectly validating clustering quality. These results underscore the importance of clustering design and activation manifolds in interpreting and troubleshooting neural network behavior.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback</title>
<link>https://arxiv.org/abs/2505.15572</link>
<guid>https://arxiv.org/abs/2505.15572</guid>
<content:encoded><![CDATA[
arXiv:2505.15572v1 Announce Type: new 
Abstract: The data-to-equation (Data2Eqn) task aims to discover interpretable mathematical equations that map observed values to labels, offering physical insights and broad applicability across academic and industrial domains. Genetic programming and traditional deep learning-based approaches suffer from search inefficiency and poor generalization on small task-specific datasets. Foundation models showed promise in this area, but existing approaches suffer from: 1) They are pretrained on general-purpose data distributions, making them less effective for domain-specific tasks; and 2) their training objectives focus on token-level alignment, overlooking mathematical semantics, which can lead to inaccurate equations. To address these issues, we aim to enhance the domain adaptability of foundation models for Data2Eqn tasks. In this work, we propose a reinforcement learning-based finetuning framework that directly optimizes the generation policy of a pretrained model through reward signals derived from downstream numerical fitness. Our method allows the model to adapt to specific and complex data distributions and generate mathematically meaningful equations. Extensive experiments demonstrate that our approach improves both the accuracy and robustness of equation generation under complex distributions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions</title>
<link>https://arxiv.org/abs/2505.15579</link>
<guid>https://arxiv.org/abs/2505.15579</guid>
<content:encoded><![CDATA[
arXiv:2505.15579v1 Announce Type: new 
Abstract: Personalized federated learning has emerged as a popular approach to training on devices holding statistically heterogeneous data, known as clients. However, most existing approaches require a client to have labeled data for training or finetuning in order to obtain their own personalized model. In this paper we address this by proposing FLowDUP, a novel method that is able to generate a personalized model using only a forward pass with unlabeled data. The generated model parameters reside in a low-dimensional subspace, enabling efficient communication and computation. FLowDUP's learning objective is theoretically motivated by our new transductive multi-task PAC-Bayesian generalization bound, that provides performance guarantees for unlabeled clients. The objective is structured in such a way that it allows both clients with labeled data and clients with only unlabeled data to contribute to the training process. To supplement our theoretical results we carry out a thorough experimental evaluation of FLowDUP, demonstrating strong empirical performance on a range of datasets with differing sorts of statistically heterogeneous clients. Through numerous ablation studies, we test the efficacy of the individual components of the method.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World Models as Reference Trajectories for Rapid Motor Adaptation</title>
<link>https://arxiv.org/abs/2505.15589</link>
<guid>https://arxiv.org/abs/2505.15589</guid>
<content:encoded><![CDATA[
arXiv:2505.15589v1 Announce Type: new 
Abstract: Deploying learned control policies in real-world environments poses a fundamental challenge. When system dynamics change unexpectedly, performance degrades until models are retrained on new data. We introduce Reflexive World Models (RWM), a dual control framework that uses world model predictions as implicit reference trajectories for rapid adaptation. Our method separates the control problem into long-term reward maximization through reinforcement learning and robust motor execution through rapid latent control. This dual architecture achieves significantly faster adaptation with low online computational cost compared to model-based RL baselines, while maintaining near-optimal performance. The approach combines the benefits of flexible policy learning through reinforcement learning with rapid error correction capabilities, providing a principled approach to maintaining performance in high-dimensional continuous control tasks under varying dynamics.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off</title>
<link>https://arxiv.org/abs/2505.15594</link>
<guid>https://arxiv.org/abs/2505.15594</guid>
<content:encoded><![CDATA[
arXiv:2505.15594v1 Announce Type: new 
Abstract: While foundation models demonstrate impressive performance across various tasks, they remain vulnerable to adversarial inputs. Current research explores various approaches to enhance model robustness, with Diffusion Denoised Smoothing emerging as a particularly promising technique. This method employs a pretrained diffusion model to preprocess inputs before model inference. Yet, its effectiveness remains largely unexplored beyond classification. We aim to address this gap by analyzing three datasets with four distinct downstream tasks under three different adversarial attack algorithms. Our findings reveal that while foundation models maintain resilience against conventional transformations, applying high-noise diffusion denoising to clean images without any distortions significantly degrades performance by as high as 57%. Low-noise diffusion settings preserve performance but fail to provide adequate protection across all attack types. Moreover, we introduce a novel attack strategy specifically targeting the diffusion process itself, capable of circumventing defenses in the low-noise regime. Our results suggest that the trade-off between adversarial robustness and performance remains a challenge to be addressed.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning for Continuous-time Stochastic Control with Jumps</title>
<link>https://arxiv.org/abs/2505.15602</link>
<guid>https://arxiv.org/abs/2505.15602</guid>
<content:encoded><![CDATA[
arXiv:2505.15602v1 Announce Type: new 
Abstract: In this paper, we introduce a model-based deep-learning approach to solve finite-horizon continuous-time stochastic control problems with jumps. We iteratively train two neural networks: one to represent the optimal policy and the other to approximate the value function. Leveraging a continuous-time version of the dynamic programming principle, we derive two different training objectives based on the Hamilton-Jacobi-Bellman equation, ensuring that the networks capture the underlying stochastic dynamics. Empirical evaluations on different problems illustrate the accuracy and scalability of our approach, demonstrating its effectiveness in solving complex, high-dimensional stochastic control tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Energy and Latency in TinyML: A Novel Method for Resource-Constrained AI</title>
<link>https://arxiv.org/abs/2505.15622</link>
<guid>https://arxiv.org/abs/2505.15622</guid>
<content:encoded><![CDATA[
arXiv:2505.15622v1 Announce Type: new 
Abstract: The rise of IoT has increased the need for on-edge machine learning, with TinyML emerging as a promising solution for resource-constrained devices such as MCU. However, evaluating their performance remains challenging due to diverse architectures and application scenarios. Current solutions have many non-negligible limitations. This work introduces an alternative benchmarking methodology that integrates energy and latency measurements while distinguishing three execution phases pre-inference, inference, and post-inference. Additionally, the setup ensures that the device operates without being powered by an external measurement unit, while automated testing can be leveraged to enhance statistical significance. To evaluate our setup, we tested the STM32N6 MCU, which includes a NPU for executing neural networks. Two configurations were considered: high-performance and Low-power. The variation of the EDP was analyzed separately for each phase, providing insights into the impact of hardware configurations on energy efficiency. Each model was tested 1000 times to ensure statistically relevant results. Our findings demonstrate that reducing the core voltage and clock frequency improve the efficiency of pre- and post-processing without significantly affecting network execution performance. This approach can also be used for cross-platform comparisons to determine the most efficient inference platform and to quantify how pre- and post-processing overhead varies across different hardware implementations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Insights into Grokking from the Embedding Layer</title>
<link>https://arxiv.org/abs/2505.15624</link>
<guid>https://arxiv.org/abs/2505.15624</guid>
<content:encoded><![CDATA[
arXiv:2505.15624v1 Announce Type: new 
Abstract: Grokking, a delayed generalization in neural networks after perfect training performance, has been observed in Transformers and MLPs, but the components driving it remain underexplored. We show that embeddings are central to grokking: introducing them into MLPs induces delayed generalization in modular arithmetic tasks, whereas MLPs without embeddings can generalize immediately. Our analysis identifies two key mechanisms: (1) Embedding update dynamics, where rare tokens stagnate due to sparse gradient updates and weight decay, and (2) Bilinear coupling, where the interaction between embeddings and downstream weights introduces saddle points and increases sensitivity to initialization. To confirm these mechanisms, we investigate frequency-aware sampling, which balances token updates by minimizing gradient variance, and embedding-specific learning rates, derived from the asymmetric curvature of the bilinear loss landscape. We prove that an adaptive learning rate ratio, \(\frac{\eta_E}{\eta_W} \propto \frac{\sigma_{\max}(E)}{\sigma_{\max}(W)} \cdot \frac{f_W}{f_E}\), mitigates bilinear coupling effects, accelerating convergence. Our methods not only improve grokking dynamics but also extend to broader challenges in Transformer optimization, where bilinear interactions hinder efficient training.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Explanations with Human Communication</title>
<link>https://arxiv.org/abs/2505.15626</link>
<guid>https://arxiv.org/abs/2505.15626</guid>
<content:encoded><![CDATA[
arXiv:2505.15626v1 Announce Type: new 
Abstract: Machine learning explainability aims to make the decision-making process of black-box models more transparent by finding the most important input features for a given prediction task. Recent works have proposed composing explanations from semantic concepts (e.g., colors, patterns, shapes) that are inherently interpretable to the user of a model. However, these methods generally ignore the communicative context of explanation-the ability of the user to understand the prediction of the model from the explanation. For example, while a medical doctor might understand an explanation in terms of clinical markers, a patient may need a more accessible explanation to make sense of the same diagnosis. In this paper, we address this gap with listener-adaptive explanations. We propose an iterative procedure grounded in principles of pragmatic reasoning and the rational speech act to generate explanations that maximize communicative utility. Our procedure only needs access to pairwise preferences between candidate explanations, relevant in real-world scenarios where a listener model may not be available. We evaluate our method in image classification tasks, demonstrating improved alignment between explanations and listener preferences across three datasets. Furthermore, we perform a user study that demonstrates our explanations increase communicative utility.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks</title>
<link>https://arxiv.org/abs/2505.15631</link>
<guid>https://arxiv.org/abs/2505.15631</guid>
<content:encoded><![CDATA[
arXiv:2505.15631v1 Announce Type: new 
Abstract: Neural Architecture Search (NAS) accelerates progress in deep learning through systematic refinement of model architectures. The downside is increasingly large energy consumption during the search process. Surrogate-based benchmarking mitigates the cost of full training by querying a pre-trained surrogate to obtain an estimate for the quality of the model. Specifically, energy-aware benchmarking aims to make it possible for NAS to favourably trade off model energy consumption against accuracy. Towards this end, we propose three design principles for such energy-aware benchmarks: (i) reliable power measurements, (ii) a wide range of GPU usage, and (iii) holistic cost reporting. We analyse EA-HAS-Bench based on these principles and find that the choice of GPU measurement API has a large impact on the quality of results. Using the Nvidia System Management Interface (SMI) on top of its underlying library influences the sampling rate during the initial data collection, returning faulty low-power estimations. This results in poor correlation with accurate measurements obtained from an external power meter. With this study, we bring to attention several key considerations when performing energy-aware surrogate-based benchmarking and derive first guidelines that can help design novel benchmarks. We show a narrow usage range of the four GPUs attached to our device, ranging from 146 W to 305 W in a single-GPU setting, and narrowing down even further when using all four GPUs. To improve holistic energy reporting, we propose calibration experiments over assumptions made in popular tools, such as Code Carbon, thus achieving reductions in the maximum inaccuracy from 10.3 % to 8.9 % without and to 6.6 % with prior estimation of the expected load on the device.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes</title>
<link>https://arxiv.org/abs/2505.15638</link>
<guid>https://arxiv.org/abs/2505.15638</guid>
<content:encoded><![CDATA[
arXiv:2505.15638v1 Announce Type: new 
Abstract: We revisit the classical problem of Bayesian ensembles and address the challenge of learning optimal combinations of Bayesian models in an online, continual learning setting. To this end, we reinterpret existing approaches such as Bayesian model averaging (BMA) and Bayesian stacking through a novel empirical Bayes lens, shedding new light on the limitations and pathologies of BMA. Further motivated by insights from online optimization, we propose Online Bayesian Stacking (OBS), a method that optimizes the log-score over predictive distributions to adaptively combine Bayesian models. A key contribution of our work is establishing a novel connection between OBS and portfolio selection, bridging Bayesian ensemble learning with a rich, well-studied theoretical framework that offers efficient algorithms and extensive regret analysis. We further clarify the relationship between OBS and online BMA, showing that they optimize related but distinct cost functions. Through theoretical analysis and empirical evaluation, we identify scenarios where OBS outperforms online BMA and provide principled guidance on when practitioners should prefer one approach over the other.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima</title>
<link>https://arxiv.org/abs/2505.15643</link>
<guid>https://arxiv.org/abs/2505.15643</guid>
<content:encoded><![CDATA[
arXiv:2505.15643v1 Announce Type: new 
Abstract: We study the problem of best-arm identification in stochastic multi-armed bandits under the fixed-confidence setting, with a particular focus on instances that admit multiple optimal arms. While the Track-and-Stop algorithm of Garivier and Kaufmann (2016) is widely conjectured to be instance-optimal, its performance in the presence of multiple optima has remained insufficiently understood. In this work, we revisit the Track-and-Stop strategy and propose a modified stopping rule that ensures instance-optimality even when the set of optimal arms is not a singleton. Our analysis introduces a new information-theoretic lower bound that explicitly accounts for multiple optimal arms, and we demonstrate that our stopping rule tightly matches this bound.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Second-Order Convergence in Private Stochastic Non-Convex Optimization</title>
<link>https://arxiv.org/abs/2505.15647</link>
<guid>https://arxiv.org/abs/2505.15647</guid>
<content:encoded><![CDATA[
arXiv:2505.15647v1 Announce Type: new 
Abstract: We investigate the problem of finding second-order stationary points (SOSP) in differentially private (DP) stochastic non-convex optimization. Existing methods suffer from two key limitations: (i) inaccurate convergence error rate due to overlooking gradient variance in the saddle point escape analysis, and (ii) dependence on auxiliary private model selection procedures for identifying DP-SOSP, which can significantly impair utility, particularly in distributed settings. To address these issues, we propose a generic perturbed stochastic gradient descent (PSGD) framework built upon Gaussian noise injection and general gradient oracles. A core innovation of our framework is using model drift distance to determine whether PSGD escapes saddle points, ensuring convergence to approximate local minima without relying on second-order information or additional DP-SOSP identification. By leveraging the adaptive DP-SPIDER estimator as a specific gradient oracle, we develop a new DP algorithm that rectifies the convergence error rates reported in prior work. We further extend this algorithm to distributed learning with arbitrarily heterogeneous data, providing the first formal guarantees for finding DP-SOSP in such settings. Our analysis also highlights the detrimental impacts of private selection procedures in distributed learning under high-dimensional models, underscoring the practical benefits of our design. Numerical experiments on real-world datasets validate the efficacy of our approach.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Small Decision Trees with Few Outliers: A Parameterized Perspective</title>
<link>https://arxiv.org/abs/2505.15648</link>
<guid>https://arxiv.org/abs/2505.15648</guid>
<content:encoded><![CDATA[
arXiv:2505.15648v1 Announce Type: new 
Abstract: Decision trees are a fundamental tool in machine learning for representing, classifying, and generalizing data. It is desirable to construct ``small'' decision trees, by minimizing either the \textit{size} ($s$) or the \textit{depth} $(d)$ of the \textit{decision tree} (\textsc{DT}). Recently, the parameterized complexity of \textsc{Decision Tree Learning} has attracted a lot of attention. We consider a generalization of \textsc{Decision Tree Learning} where given a \textit{classification instance} $E$ and an integer $t$, the task is to find a ``small'' \textsc{DT} that disagrees with $E$ in at most $t$ examples. We consider two problems: \textsc{DTSO} and \textsc{DTDO}, where the goal is to construct a \textsc{DT} minimizing $s$ and $d$, respectively. We first establish that both \textsc{DTSO} and \textsc{DTDO} are W[1]-hard when parameterized by $s+\delta_{max}$ and $d+\delta_{max}$, respectively, where $\delta_{max}$ is the maximum number of features in which two differently labeled examples can differ. We complement this result by showing that these problems become \textsc{FPT} if we include the parameter $t$. We also consider the kernelization complexity of these problems and establish several positive and negative results for both \textsc{DTSO} and \textsc{DTDO}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought</title>
<link>https://arxiv.org/abs/2505.15657</link>
<guid>https://arxiv.org/abs/2505.15657</guid>
<content:encoded><![CDATA[
arXiv:2505.15657v1 Announce Type: new 
Abstract: Sample-wise learning curves plot performance versus training set size. They are useful for studying scaling laws and speeding up hyperparameter tuning and model selection. Learning curves are often assumed to be well-behaved: monotone (i.e. improving with more data) and convex. By constructing the Learning Curves Database 1.1 (LCDB 1.1), a large-scale database with high-resolution learning curves, we show that learning curves are less often well-behaved than previously thought. Using statistically rigorous methods, we observe significant ill-behavior in approximately 14% of the learning curves, almost twice as much as in previous estimates. We also identify which learners are to blame and show that specific learners are more ill-behaved than others. Additionally, we demonstrate that different feature scalings rarely resolve ill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such as learning curve fitting and model selection, and find it poses significant challenges, underscoring the relevance and potential of LCDB 1.1 as a challenging benchmark for future research.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery algorithms</title>
<link>https://arxiv.org/abs/2505.15661</link>
<guid>https://arxiv.org/abs/2505.15661</guid>
<content:encoded><![CDATA[
arXiv:2505.15661v1 Announce Type: new 
Abstract: Gradient-based learning imposes (deep) neural networks to be differentiable at all steps. This includes model-based architectures constructed by unrolling iterations of an iterative algorithm onto layers of a neural network, known as algorithm unrolling. However, greedy sparse recovery algorithms depend on the non-differentiable argsort operator, which hinders their integration into neural networks. In this paper, we address this challenge in Orthogonal Matching Pursuit (OMP) and Iterative Hard Thresholding (IHT), two popular representative algorithms in this class. We propose permutation-based variants of these algorithms and approximate permutation matrices using "soft" permutation matrices derived from softsort, a continuous relaxation of argsort. We demonstrate -- both theoretically and numerically -- that Soft-OMP and Soft-IHT, as differentiable counterparts of OMP and IHT and fully compatible with neural network training, effectively approximate these algorithms with a controllable degree of accuracy. This leads to the development of OMP- and IHT-Net, fully trainable network architectures based on Soft-OMP and Soft-IHT, respectively. Finally, by choosing weights as "structure-aware" trainable parameters, we connect our approach to structured sparse recovery and demonstrate its ability to extract latent sparsity patterns from data.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Conditional Flow Matching for Relational Data Generation</title>
<link>https://arxiv.org/abs/2505.15668</link>
<guid>https://arxiv.org/abs/2505.15668</guid>
<content:encoded><![CDATA[
arXiv:2505.15668v1 Announce Type: new 
Abstract: Data synthesis is gaining momentum as a privacy-enhancing technology. While single-table tabular data generation has seen considerable progress, current methods for multi-table data often lack the flexibility and expressiveness needed to capture complex relational structures. In particular, they struggle with long-range dependencies and complex foreign-key relationships, such as tables with multiple parent tables or multiple types of links between the same pair of tables. We propose a generative model for relational data that generates the content of a relational dataset given the graph formed by the foreign-key relationships. We do this by learning a deep generative model of the content of the whole relational database by flow matching, where the neural network trained to denoise records leverages a graph neural network to obtain information from connected records. Our method is flexible, as it can support relational datasets with complex structures, and expressive, as the generation of each record can be influenced by any other record within the same connected component. We evaluate our method on several benchmark datasets and show that it achieves state-of-the-art performance in terms of synthetic data fidelity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A packing lemma for VCN${}_k$-dimension and learning high-dimensional data</title>
<link>https://arxiv.org/abs/2505.15688</link>
<guid>https://arxiv.org/abs/2505.15688</guid>
<content:encoded><![CDATA[
arXiv:2505.15688v1 Announce Type: new 
Abstract: Recently, the authors introduced the theory of high-arity PAC learning, which is well-suited for learning graphs, hypergraphs and relational structures. In the same initial work, the authors proved a high-arity analogue of the Fundamental Theorem of Statistical Learning that almost completely characterizes all notions of high-arity PAC learning in terms of a combinatorial dimension, called the Vapnik--Chervonenkis--Natarajan (VCN${}_k$) $k$-dimension, leaving as an open problem only the characterization of non-partite, non-agnostic high-arity PAC learnability.
  In this work, we complete this characterization by proving that non-partite non-agnostic high-arity PAC learnability implies a high-arity version of the Haussler packing property, which in turn implies finiteness of VCN${}_k$-dimension. This is done by obtaining direct proofs that classic PAC learnability implies classic Haussler packing property, which in turn implies finite Natarajan dimension and noticing that these direct proofs nicely lift to high-arity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO</title>
<link>https://arxiv.org/abs/2505.15694</link>
<guid>https://arxiv.org/abs/2505.15694</guid>
<content:encoded><![CDATA[
arXiv:2505.15694v1 Announce Type: new 
Abstract: In this paper, we theoretically investigate the effects of noisy labels in offline alignment, with a focus on the interplay between privacy and robustness against adversarial corruption. Specifically, under linear modeling assumptions, we present a unified analysis covering both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under different privacy-corruption scenarios, such as Local differential privacy-then-Corruption (LTC), where human preference labels are privatized before being corrupted by an adversary, and Corruption-then-Local differential privacy (CTL), where labels are corrupted before privacy protection. Our analysis leverages a reduction framework that reduces the offline alignment problem under linear modeling assumptions to parameter estimation in logistic regression. This framework allows us to establish an interesting separation result between LTC and CTL, demonstrating that LTC presents a greater challenge than CTL in offline alignment, even under linear models. As important by-products, our findings also advance the state-of-the-art theoretical results in offline alignment under privacy-only or corruption-only scenarios.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Conformal Prediction Under Local Differential Privacy</title>
<link>https://arxiv.org/abs/2505.15721</link>
<guid>https://arxiv.org/abs/2505.15721</guid>
<content:encoded><![CDATA[
arXiv:2505.15721v1 Announce Type: new 
Abstract: Conformal prediction (CP) provides sets of candidate classes with a guaranteed probability of containing the true class. However, it typically relies on a calibration set with clean labels. We address privacy-sensitive scenarios where the aggregator is untrusted and can only access a perturbed version of the true labels. We propose two complementary approaches under local differential privacy (LDP). In the first approach, users do not access the model but instead provide their input features and a perturbed label using a k-ary randomized response. In the second approach, which enforces stricter privacy constraints, users add noise to their conformity score by binary search response. This method requires access to the classification model but preserves both data and label privacy. Both approaches compute the conformal threshold directly from noisy data without accessing the true labels. We prove finite-sample coverage guarantees and demonstrate robust coverage even under severe randomization. This approach unifies strong local privacy with predictive uncertainty control, making it well-suited for sensitive applications such as medical imaging or large language model queries, regardless of whether users can (or are willing to) compute their own scores.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-order Structure Boosts Link Prediction on Temporal Graphs</title>
<link>https://arxiv.org/abs/2505.15746</link>
<guid>https://arxiv.org/abs/2505.15746</guid>
<content:encoded><![CDATA[
arXiv:2505.15746v1 Announce Type: new 
Abstract: Temporal Graph Neural Networks (TGNNs) have gained growing attention for modeling and predicting structures in temporal graphs. However, existing TGNNs primarily focus on pairwise interactions while overlooking higher-order structures that are integral to link formation and evolution in real-world temporal graphs. Meanwhile, these models often suffer from efficiency bottlenecks, further limiting their expressive power. To tackle these challenges, we propose a Higher-order structure Temporal Graph Neural Network, which incorporates hypergraph representations into temporal graph learning. In particular, we develop an algorithm to identify the underlying higher-order structures, enhancing the model's ability to capture the group interactions. Furthermore, by aggregating multiple edge features into hyperedge representations, HTGN effectively reduces memory cost during training. We theoretically demonstrate the enhanced expressiveness of our approach and validate its effectiveness and efficiency through extensive experiments on various real-world temporal graphs. Experimental results show that HTGN achieves superior performance on dynamic link prediction while reducing memory costs by up to 50\% compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs</title>
<link>https://arxiv.org/abs/2505.15747</link>
<guid>https://arxiv.org/abs/2505.15747</guid>
<content:encoded><![CDATA[
arXiv:2505.15747v1 Announce Type: new 
Abstract: We propose a novel framework for integrating fragmented multi-modal data in Alzheimer's disease (AD) research using large language models (LLMs) and knowledge graphs. While traditional multimodal analysis requires matched patient IDs across datasets, our approach demonstrates population-level integration of MRI, gene expression, biomarkers, EEG, and clinical indicators from independent cohorts. Statistical analysis identified significant features in each modality, which were connected as nodes in a knowledge graph. LLMs then analyzed the graph to extract potential correlations and generate hypotheses in natural language. This approach revealed several novel relationships, including a potential pathway linking metabolic risk factors to tau protein abnormalities via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between frontal EEG channels and specific gene expression profiles (r=0.42-0.58, p<0.01). Cross-validation with independent datasets confirmed the robustness of major findings, with consistent effect sizes across cohorts (variance <15%). The reproducibility of these findings was further supported by expert review (Cohen's k=0.82) and computational validation. Our framework enables cross modal integration at a conceptual level without requiring patient ID matching, offering new possibilities for understanding AD pathology through fragmented data reuse and generating testable hypotheses for future research.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving planning and MBRL with temporally-extended actions</title>
<link>https://arxiv.org/abs/2505.15754</link>
<guid>https://arxiv.org/abs/2505.15754</guid>
<content:encoded><![CDATA[
arXiv:2505.15754v1 Announce Type: new 
Abstract: Continuous time systems are often modeled using discrete time dynamics but this requires a small simulation step to maintain accuracy. In turn, this requires a large planning horizon which leads to computationally demanding planning problems and reduced performance. Previous work in model free reinforcement learning has partially addressed this issue using action repeats where a policy is learned to determine a discrete action duration. Instead we propose to control the continuous decision timescale directly by using temporally-extended actions and letting the planner treat the duration of the action as an additional optimization variable along with the standard action variables. This additional structure has multiple advantages. It speeds up simulation time of trajectories and, importantly, it allows for deep horizon search in terms of primitive actions while using a shallow search depth in the planner. In addition, in the model based reinforcement learning (MBRL) setting, it reduces compounding errors from model learning and improves training time for models. We show that this idea is effective and that the range for action durations can be automatically selected using a multi-armed bandit formulation and integrated into the MBRL framework. An extensive experimental evaluation both in planning and in MBRL, shows that our approach yields faster planning, better solutions, and that it enables solutions to problems that are not solved in the standard formulation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projection-Based Correction for Enhancing Deep Inverse Networks</title>
<link>https://arxiv.org/abs/2505.15777</link>
<guid>https://arxiv.org/abs/2505.15777</guid>
<content:encoded><![CDATA[
arXiv:2505.15777v1 Announce Type: new 
Abstract: Deep learning-based models have demonstrated remarkable success in solving illposed inverse problems; however, many fail to strictly adhere to the physical constraints imposed by the measurement process. In this work, we introduce a projection-based correction method to enhance the inference of deep inverse networks by ensuring consistency with the forward model. Specifically, given an initial estimate from a learned reconstruction network, we apply a projection step that constrains the solution to lie within the valid solution space of the inverse problem. We theoretically demonstrate that if the recovery model is a well-trained deep inverse network, the solution can be decomposed into range-space and null-space components, where the projection-based correction reduces to an identity transformation. Extensive simulations and experiments validate the proposed method, demonstrating improved reconstruction accuracy across diverse inverse problems and deep network architectures.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving General-Utility Markov Decision Processes in the Single-Trial Regime with Online Planning</title>
<link>https://arxiv.org/abs/2505.15782</link>
<guid>https://arxiv.org/abs/2505.15782</guid>
<content:encoded><![CDATA[
arXiv:2505.15782v1 Announce Type: new 
Abstract: In this work, we contribute the first approach to solve infinite-horizon discounted general-utility Markov decision processes (GUMDPs) in the single-trial regime, i.e., when the agent's performance is evaluated based on a single trajectory. First, we provide some fundamental results regarding policy optimization in the single-trial regime, investigating which class of policies suffices for optimality, casting our problem as a particular MDP that is equivalent to our original problem, as well as studying the computational hardness of policy optimization in the single-trial regime. Second, we show how we can leverage online planning techniques, in particular a Monte-Carlo tree search algorithm, to solve GUMDPs in the single-trial regime. Third, we provide experimental results showcasing the superior performance of our approach in comparison to relevant baselines.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Computable Approximations to Solomonoff Induction</title>
<link>https://arxiv.org/abs/2505.15784</link>
<guid>https://arxiv.org/abs/2505.15784</guid>
<content:encoded><![CDATA[
arXiv:2505.15784v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) calls for a rigorous theoretical framework to explain their empirical success. While significant progress has been made in understanding LLM behaviors, existing theoretical frameworks remain fragmented in explaining emergent phenomena through a unified mathematical lens. We establish the first formal connection between LLM architectures and Algorithmic Information Theory (AIT) by proving two fundamental results: (1) the training process computationally approximates Solomonoff prior through loss minimization interpreted as program length optimization, and (2) next-token prediction implements approximate Solomonoff induction. We leverage AIT to provide a unified theoretical explanation for in-context learning, few-shot learning, and scaling laws. Furthermore, our theoretical insights lead to a principled method for few-shot example selection that prioritizes samples where models exhibit lower predictive confidence. We demonstrate through experiments on diverse text classification benchmarks that this strategy yields significant performance improvements, particularly for smaller model architectures, when compared to selecting high-confidence examples. Our framework bridges the gap between theoretical foundations and practical LLM behaviors, providing both explanatory power and actionable insights for future model development.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Supervised Learning Through Constraints on Smooth Nonconvex Unfairness-Measure Surrogates</title>
<link>https://arxiv.org/abs/2505.15788</link>
<guid>https://arxiv.org/abs/2505.15788</guid>
<content:encoded><![CDATA[
arXiv:2505.15788v1 Announce Type: new 
Abstract: A new strategy for fair supervised machine learning is proposed. The main advantages of the proposed strategy as compared to others in the literature are as follows. (a) We introduce a new smooth nonconvex surrogate to approximate the Heaviside functions involved in discontinuous unfairness measures. The surrogate is based on smoothing methods from the optimization literature, and is new for the fair supervised learning literature. The surrogate is a tight approximation which ensures the trained prediction models are fair, as opposed to other (e.g., convex) surrogates that can fail to lead to a fair prediction model in practice. (b) Rather than rely on regularizers (that lead to optimization problems that are difficult to solve) and corresponding regularization parameters (that can be expensive to tune), we propose a strategy that employs hard constraints so that specific tolerances for unfairness can be enforced without the complications associated with the use of regularization. (c)~Our proposed strategy readily allows for constraints on multiple (potentially conflicting) unfairness measures at the same time. Multiple measures can be considered with a regularization approach, but at the cost of having even more difficult optimization problems to solve and further expense for tuning. By contrast, through hard constraints, our strategy leads to optimization models that can be solved tractably with minimal tuning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning</title>
<link>https://arxiv.org/abs/2505.15798</link>
<guid>https://arxiv.org/abs/2505.15798</guid>
<content:encoded><![CDATA[
arXiv:2505.15798v1 Announce Type: new 
Abstract: Certifying the IID generalisation ability of deep networks is the first of many requirements for trusting AI in high-stakes applications from medicine to security. However, when instantiating generalisation bounds for deep networks it remains challenging to obtain non-vacuous guarantees, especially when applying contemporary large models on the small scale data prevalent in such high-stakes fields. In this paper, we draw a novel connection between a family of learning methods based on model fusion and generalisation certificates, and surprisingly show that with minor adjustment several existing learning strategies already provide non-trivial generalisation guarantees. Essentially, by focusing on data-driven learning of downstream tasks by fusion rather than fine-tuning, the certified generalisation gap becomes tiny and independent of the base network size, facilitating its certification. Our results show for the first time non-trivial generalisation guarantees for learning with as low as 100 examples, while using vision models such as VIT-B and language models such as mistral-7B. This observation is significant as it has immediate implications for facilitating the certification of existing systems as trustworthy, and opens up new directions for research at the intersection of practice and theory.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Framework for Two-Dimensional, Multi-Frequency Propagation Factor Estimation</title>
<link>https://arxiv.org/abs/2505.15802</link>
<guid>https://arxiv.org/abs/2505.15802</guid>
<content:encoded><![CDATA[
arXiv:2505.15802v1 Announce Type: new 
Abstract: Accurately estimating the refractive environment over multiple frequencies within the marine atmospheric boundary layer is crucial for the effective deployment of radar technologies. Traditional parabolic equation simulations, while effective, can be computationally expensive and time-intensive, limiting their practical application. This communication explores a novel approach using deep neural networks to estimate the pattern propagation factor, a critical parameter for characterizing environmental impacts on signal propagation. Image-to-image translation generators designed to ingest modified refractivity data and generate predictions of pattern propagation factors over the same domain were developed. Findings demonstrate that deep neural networks can be trained to analyze multiple frequencies and reasonably predict the pattern propagation factor, offering an alternative to traditional methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Estimation and Learning under Temporal Distribution Shift</title>
<link>https://arxiv.org/abs/2505.15803</link>
<guid>https://arxiv.org/abs/2505.15803</guid>
<content:encoded><![CDATA[
arXiv:2505.15803v1 Announce Type: new 
Abstract: In this paper, we study the problem of estimation and learning under temporal distribution shift. Consider an observation sequence of length $n$, which is a noisy realization of a time-varying groundtruth sequence. Our focus is to develop methods to estimate the groundtruth at the final time-step while providing sharp point-wise estimation error rates. We show that, without prior knowledge on the level of temporal shift, a wavelet soft-thresholding estimator provides an optimal estimation error bound for the groundtruth. Our proposed estimation method generalizes existing researches Mazzetto and Upfal (2023) by establishing a connection between the sequence's non-stationarity level and the sparsity in the wavelet-transformed domain. Our theoretical findings are validated by numerical experiments. Additionally, we applied the estimator to derive sparsity-aware excess risk bounds for binary classification under distribution shift and to develop computationally efficient training objectives. As a final contribution, we draw parallels between our results and the classical signal processing problem of total-variation denoising (Mammen and van de Geer,1997; Tibshirani, 2014), uncovering novel optimal algorithms for such task.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Conditional Transport Maps</title>
<link>https://arxiv.org/abs/2505.15808</link>
<guid>https://arxiv.org/abs/2505.15808</guid>
<content:encoded><![CDATA[
arXiv:2505.15808v1 Announce Type: new 
Abstract: We present a neural framework for learning conditional optimal transport (OT) maps between probability distributions. Our approach introduces a conditioning mechanism capable of processing both categorical and continuous conditioning variables simultaneously. At the core of our method lies a hypernetwork that generates transport layer parameters based on these inputs, creating adaptive mappings that outperform simpler conditioning methods. Comprehensive ablation studies demonstrate the superior performance of our method over baseline configurations. Furthermore, we showcase an application to global sensitivity analysis, offering high performance in computing OT-based sensitivity indices. This work advances the state-of-the-art in conditional optimal transport, enabling broader application of optimal transport principles to complex, high-dimensional domains such as generative modeling and black-box model explainability.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the creation of narrow AI: hierarchy and nonlocality of neural network skills</title>
<link>https://arxiv.org/abs/2505.15811</link>
<guid>https://arxiv.org/abs/2505.15811</guid>
<content:encoded><![CDATA[
arXiv:2505.15811v1 Announce Type: new 
Abstract: We study the problem of creating strong, yet narrow, AI systems. While recent AI progress has been driven by the training of large general-purpose foundation models, the creation of smaller models specialized for narrow domains could be valuable for both efficiency and safety. In this work, we explore two challenges involved in creating such systems, having to do with basic properties of how neural networks learn and structure their representations. The first challenge regards when it is possible to train narrow models from scratch. Through experiments on a synthetic task, we find that it is sometimes necessary to train networks on a wide distribution of data to learn certain narrow skills within that distribution. This effect arises when skills depend on each other hierarchically, and training on a broad distribution introduces a curriculum which substantially accelerates learning. The second challenge regards how to transfer particular skills from large general models into small specialized models. We find that model skills are often not perfectly localized to a particular set of prunable components. However, we find that methods based on pruning can still outperform distillation. We investigate the use of a regularization objective to align desired skills with prunable components while unlearning unnecessary skills.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex</title>
<link>https://arxiv.org/abs/2505.15813</link>
<guid>https://arxiv.org/abs/2505.15813</guid>
<content:encoded><![CDATA[
arXiv:2505.15813v1 Announce Type: new 
Abstract: Understanding functional representations within higher visual cortex is a fundamental question in computational neuroscience. While artificial neural networks pretrained on large-scale datasets exhibit striking representational alignment with human neural responses, learning image-computable models of visual cortex relies on individual-level, large-scale fMRI datasets. The necessity for expensive, time-intensive, and often impractical data acquisition limits the generalizability of encoders to new subjects and stimuli. BraInCoRL uses in-context learning to predict voxelwise neural responses from few-shot examples without any additional finetuning for novel subjects and stimuli. We leverage a transformer architecture that can flexibly condition on a variable number of in-context image stimuli, learning an inductive bias over multiple subjects. During training, we explicitly optimize the model for in-context learning. By jointly conditioning on image features and voxel activations, our model learns to directly generate better performing voxelwise models of higher visual cortex. We demonstrate that BraInCoRL consistently outperforms existing voxelwise encoder designs in a low-data regime when evaluated on entirely novel images, while also exhibiting strong test-time scaling behavior. The model also generalizes to an entirely new visual fMRI dataset, which uses different subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates better interpretability of neural signals in higher visual cortex by attending to semantically relevant stimuli. Finally, we show that our framework enables interpretable mappings from natural language queries to voxel selectivity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment Analysis in Software Engineering: Evaluating Generative Pre-trained Transformers</title>
<link>https://arxiv.org/abs/2505.14692</link>
<guid>https://arxiv.org/abs/2505.14692</guid>
<content:encoded><![CDATA[
arXiv:2505.14692v1 Announce Type: cross 
Abstract: Sentiment analysis plays a crucial role in understanding developer interactions, issue resolutions, and project dynamics within software engineering (SE). While traditional SE-specific sentiment analysis tools have made significant strides, they often fail to account for the nuanced and context-dependent language inherent to the domain. This study systematically evaluates the performance of bidirectional transformers, such as BERT, against generative pre-trained transformers, specifically GPT-4o-mini, in SE sentiment analysis. Using datasets from GitHub, Stack Overflow, and Jira, we benchmark the models' capabilities with fine-tuned and default configurations. The results reveal that fine-tuned GPT-4o-mini performs comparable to BERT and other bidirectional models on structured and balanced datasets like GitHub and Jira, achieving macro-averaged F1-scores of 0.93 and 0.98, respectively. However, on linguistically complex datasets with imbalanced sentiment distributions, such as Stack Overflow, the default GPT-4o-mini model exhibits superior generalization, achieving an accuracy of 85.3\% compared to the fine-tuned model's 13.1\%. These findings highlight the trade-offs between fine-tuning and leveraging pre-trained models for SE tasks. The study underscores the importance of aligning model architectures with dataset characteristics to optimize performance and proposes directions for future research in refining sentiment analysis tools tailored to the SE domain.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Description of Flutter Dynamics via Koopman Theory</title>
<link>https://arxiv.org/abs/2505.14697</link>
<guid>https://arxiv.org/abs/2505.14697</guid>
<content:encoded><![CDATA[
arXiv:2505.14697v1 Announce Type: cross 
Abstract: This paper presents a novel parametrization approach for aeroelastic systems utilizing Koopman theory, specifically leveraging the Koopman Bilinear Form (KBF) model. To address the limitations of linear parametric dependence in the KBF model, we introduce the Extended KBF (EKBF) model, which enables a global linear representation of aeroelastic dynamics while capturing stronger nonlinear dependence on, e.g., the flutter parameter. The effectiveness of the proposed methodology is demonstrated through two case studies: a 2D academic example and a panel flutter problem. Results show that EKBF effectively interpolates and extrapolates principal eigenvalues, capturing flutter mechanisms, and accurately predicting the flutter boundary even when the data is corrupted by noise. Furthermore, parameterized isostable and isochron identified by EKBF provides valuable insights into the nonlinear flutter system.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs</title>
<link>https://arxiv.org/abs/2505.14699</link>
<guid>https://arxiv.org/abs/2505.14699</guid>
<content:encoded><![CDATA[
arXiv:2505.14699v1 Announce Type: cross 
Abstract: The automatic analysis of document layouts in digital-born PDF documents remains a challenging problem due to the heterogeneous arrangement of textual and nontextual elements and the imprecision of the textual metadata in the Portable Document Format. In this work, we benchmark Graph Neural Network (GNN) architectures for the task of fine-grained layout classification of text blocks from digital native documents. We introduce two graph construction structures: a k-closest-neighbor graph and a fully connected graph, and generate node features via pre-trained text and vision models, thus avoiding manual feature engineering. Three experimental frameworks are evaluated: single-modality (text or visual), concatenated multimodal, and dual-branch multimodal. We evaluated four foundational GNN models and compared them with the baseline. Our experiments are specifically conducted on a rich dataset of public affairs documents that includes more than 20 sources (e.g., regional and national-level official gazettes), 37K PDF documents, with 441K pages in total. Our results demonstrate that GraphSAGE operating on the k-closest-neighbor graph in a dual-branch configuration achieves the highest per-class and overall accuracy, outperforming the baseline in some sources. These findings confirm the importance of local layout relationships and multimodal fusion exploited through GNNs for the analysis of native digital document layouts.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deployment of Traditional and Hybrid Machine Learning for Critical Heat Flux Prediction in the CTF Thermal Hydraulics Code</title>
<link>https://arxiv.org/abs/2505.14701</link>
<guid>https://arxiv.org/abs/2505.14701</guid>
<content:encoded><![CDATA[
arXiv:2505.14701v1 Announce Type: cross 
Abstract: Critical heat flux (CHF) marks the transition from nucleate to film boiling, where heat transfer to the working fluid can rapidly deteriorate. Accurate CHF prediction is essential for efficiency, safety, and preventing equipment damage, particularly in nuclear reactors. Although widely used, empirical correlations frequently exhibit discrepancies in comparison with experimental data, limiting their reliability in diverse operational conditions. Traditional machine learning (ML) approaches have demonstrated the potential for CHF prediction but have often suffered from limited interpretability, data scarcity, and insufficient knowledge of physical principles. Hybrid model approaches, which combine data-driven ML with physics-based models, mitigate these concerns by incorporating prior knowledge of the domain. This study integrated a purely data-driven ML model and two hybrid models (using the Biasi and Bowring CHF correlations) within the CTF subchannel code via a custom Fortran framework. Performance was evaluated using two validation cases: a subset of the Nuclear Regulatory Commission CHF database and the Bennett dryout experiments. In both cases, the hybrid models exhibited significantly lower error metrics in comparison with conventional empirical correlations. The pure ML model remained competitive with the hybrid models. Trend analysis of error parity indicates that ML-based models reduce the tendency for CHF overprediction, improving overall accuracy. These results demonstrate that ML-based CHF models can be effectively integrated into subchannel codes and can potentially increase performance in comparison with conventional methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards scalable surrogate models based on Neural Fields for large scale aerodynamic simulations</title>
<link>https://arxiv.org/abs/2505.14704</link>
<guid>https://arxiv.org/abs/2505.14704</guid>
<content:encoded><![CDATA[
arXiv:2505.14704v1 Announce Type: cross 
Abstract: This paper introduces a novel surrogate modeling framework for aerodynamic applications based on Neural Fields. The proposed approach, MARIO (Modulated Aerodynamic Resolution Invariant Operator), addresses non parametric geometric variability through an efficient shape encoding mechanism and exploits the discretization-invariant nature of Neural Fields. It enables training on significantly downsampled meshes, while maintaining consistent accuracy during full-resolution inference. These properties allow for efficient modeling of diverse flow conditions, while reducing computational cost and memory requirements compared to traditional CFD solvers and existing surrogate methods. The framework is validated on two complementary datasets that reflect industrial constraints. First, the AirfRANS dataset consists in a two-dimensional airfoil benchmark with non-parametric shape variations. Performance evaluation of MARIO on this case demonstrates an order of magnitude improvement in prediction accuracy over existing methods across velocity, pressure, and turbulent viscosity fields, while accurately capturing boundary layer phenomena and aerodynamic coefficients. Second, the NASA Common Research Model features three-dimensional pressure distributions on a full aircraft surface mesh, with parametric control surface deflections. This configuration confirms MARIO's accuracy and scalability. Benchmarking against state-of-the-art methods demonstrates that Neural Field surrogates can provide rapid and accurate aerodynamic predictions under the computational and data limitations characteristic of industrial applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation</title>
<link>https://arxiv.org/abs/2505.14705</link>
<guid>https://arxiv.org/abs/2505.14705</guid>
<content:encoded><![CDATA[
arXiv:2505.14705v1 Announce Type: cross 
Abstract: Multimodal Dataset Distillation (MDD) seeks to condense large-scale image-text datasets into compact surrogates while retaining their effectiveness for cross-modal learning. Despite recent progress, existing MDD approaches often suffer from \textit{\textbf{Modality Collapse}}, characterized by over-concentrated intra-modal representations and enlarged distributional gap across modalities. In this paper, at the first time, we identify this issue as stemming from a fundamental conflict between the over-compression behavior inherent in dataset distillation and the cross-modal supervision imposed by contrastive objectives. To alleviate modality collapse, we introduce \textbf{RepBlend}, a novel MDD framework that weakens overdominant cross-modal supervision via representation blending, thereby significantly enhancing intra-modal diversity. Additionally, we observe that current MDD methods impose asymmetric supervision across modalities, resulting in biased optimization. To address this, we propose symmetric projection trajectory matching, which synchronizes the optimization dynamics using modality-specific projection heads, thereby promoting balanced supervision and enhancing cross-modal alignment. Experiments on Flickr-30K and MS-COCO show that RepBlend consistently outperforms prior state-of-the-art MDD methods, achieving significant gains in retrieval performance (e.g., +9.4 IR@10, +6.3 TR@10 under the 100-pair setting) and offering up to 6.7$\times$ distillation speedup.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Processes with Modified Lognormal Distribution Featuring Flexible Upper Tail</title>
<link>https://arxiv.org/abs/2505.14713</link>
<guid>https://arxiv.org/abs/2505.14713</guid>
<content:encoded><![CDATA[
arXiv:2505.14713v1 Announce Type: cross 
Abstract: Asymmetric, non-Gaussian probability distributions are often observed in the analysis of natural and engineering datasets. The lognormal distribution is a standard model for data with skewed frequency histograms and fat tails. However, the lognormal law severely restricts the asymptotic dependence of the probability density and the hazard function for high values. Herein we present a family of three-parameter non-Gaussian probability density functions that are based on generalized kappa-exponential and kappa-logarithm functions and investigate its mathematical properties. These kappa-lognormal densities represent continuous deformations of the lognormal with lighter right tails, controlled by the parameter kappa. In addition, bimodal distributions are obtained for certain parameter combinations. We derive closed-form analytic expressions for the main statistical functions of the kappa-lognormal distribution. For the moments, we derive bounds that are based on hypergeometric functions as well as series expansions. Explicit expressions for the gradient and Hessian of the negative log-likelihood are obtained to facilitate numerical maximum-likelihood estimates of the kappa-lognormal parameters from data. We also formulate a joint probability density function for kappa-lognormal stochastic processes by applying Jacobi's multivariate theorem to a latent Gaussian process. Estimation of the kappa-lognormal distribution based on synthetic and real data is explored. Furthermore, we investigate applications of kappa-lognormal processes with different covariance kernels in time series forecasting and spatial interpolation using warped Gaussian process regression. Our results are of practical interest for modeling skewed distributions in various scientific and engineering fields.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Quantum Classical Pipeline for X Ray Based Fracture Diagnosis</title>
<link>https://arxiv.org/abs/2505.14716</link>
<guid>https://arxiv.org/abs/2505.14716</guid>
<content:encoded><![CDATA[
arXiv:2505.14716v1 Announce Type: cross 
Abstract: Bone fractures are a leading cause of morbidity and disability worldwide, imposing significant clinical and economic burdens on healthcare systems. Traditional X ray interpretation is time consuming and error prone, while existing machine learning and deep learning solutions often demand extensive feature engineering, large, annotated datasets, and high computational resources. To address these challenges, a distributed hybrid quantum classical pipeline is proposed that first applies Principal Component Analysis (PCA) for dimensionality reduction and then leverages a 4 qubit quantum amplitude encoding circuit for feature enrichment. By fusing eight PCA derived features with eight quantum enhanced features into a 16 dimensional vector and then classifying with different machine learning models achieving 99% accuracy using a public multi region X ray dataset on par with state of the art transfer learning models while reducing feature extraction time by 82%.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aneumo: A Large-Scale Multimodal Aneurysm Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks</title>
<link>https://arxiv.org/abs/2505.14717</link>
<guid>https://arxiv.org/abs/2505.14717</guid>
<content:encoded><![CDATA[
arXiv:2505.14717v1 Announce Type: cross 
Abstract: Intracranial aneurysms (IAs) are serious cerebrovascular lesions found in approximately 5\% of the general population. Their rupture may lead to high mortality. Current methods for assessing IA risk focus on morphological and patient-specific factors, but the hemodynamic influences on IA development and rupture remain unclear. While accurate for hemodynamic studies, conventional computational fluid dynamics (CFD) methods are computationally intensive, hindering their deployment in large-scale or real-time clinical applications. To address this challenge, we curated a large-scale, high-fidelity aneurysm CFD dataset to facilitate the development of efficient machine learning algorithms for such applications. Based on 427 real aneurysm geometries, we synthesized 10,660 3D shapes via controlled deformation to simulate aneurysm evolution. The authenticity of these synthetic shapes was confirmed by neurosurgeons. CFD computations were performed on each shape under eight steady-state mass flow conditions, generating a total of 85,280 blood flow dynamics data covering key parameters. Furthermore, the dataset includes segmentation masks, which can support tasks that use images, point clouds or other multimodal data as input. Additionally, we introduced a benchmark for estimating flow parameters to assess current modeling methods. This dataset aims to advance aneurysm research and promote data-driven approaches in biofluids, biomedical engineering, and clinical risk assessment. The code and dataset are available at: https://github.com/Xigui-Li/Aneumo.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComBAT Harmonization for diffusion MRI: Challenges and Best Practices</title>
<link>https://arxiv.org/abs/2505.14722</link>
<guid>https://arxiv.org/abs/2505.14722</guid>
<content:encoded><![CDATA[
arXiv:2505.14722v1 Announce Type: cross 
Abstract: Over the years, ComBAT has become the standard method for harmonizing MRI-derived measurements, with its ability to compensate for site-related additive and multiplicative biases while preserving biological variability. However, ComBAT relies on a set of assumptions that, when violated, can result in flawed harmonization. In this paper, we thoroughly review ComBAT's mathematical foundation, outlining these assumptions, and exploring their implications for the demographic composition necessary for optimal results.
  Through a series of experiments involving a slightly modified version of ComBAT called Pairwise-ComBAT tailored for normative modeling applications, we assess the impact of various population characteristics, including population size, age distribution, the absence of certain covariates, and the magnitude of additive and multiplicative factors. Based on these experiments, we present five essential recommendations that should be carefully considered to enhance consistency and supporting reproducibility, two essential factors for open science, collaborative research, and real-life clinical deployment.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUADS: QUAntized Distillation Framework for Efficient Speech Language Understanding</title>
<link>https://arxiv.org/abs/2505.14723</link>
<guid>https://arxiv.org/abs/2505.14723</guid>
<content:encoded><![CDATA[
arXiv:2505.14723v1 Announce Type: cross 
Abstract: Spoken Language Understanding (SLU) systems must balance performance and efficiency, particularly in resource-constrained environments. Existing methods apply distillation and quantization separately, leading to suboptimal compression as distillation ignores quantization constraints. We propose QUADS, a unified framework that optimizes both through multi-stage training with a pre-tuned model, enhancing adaptability to low-bit regimes while maintaining accuracy. QUADS achieves 71.13\% accuracy on SLURP and 99.20\% on FSC, with only minor degradations of up to 5.56\% compared to state-of-the-art models. Additionally, it reduces computational complexity by 60--73$\times$ (GMACs) and model size by 83--700$\times$, demonstrating strong robustness under extreme quantization. These results establish QUADS as a highly efficient solution for real-world, resource-constrained SLU applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HR-VILAGE-3K3M: A Human Respiratory Viral Immunization Longitudinal Gene Expression Dataset for Systems Immunity</title>
<link>https://arxiv.org/abs/2505.14725</link>
<guid>https://arxiv.org/abs/2505.14725</guid>
<content:encoded><![CDATA[
arXiv:2505.14725v1 Announce Type: cross 
Abstract: Respiratory viral infections pose a global health burden, yet the cellular immune responses driving protection or pathology remain unclear. Natural infection cohorts often lack pre-exposure baseline data and structured temporal sampling. In contrast, inoculation and vaccination trials generate insightful longitudinal transcriptomic data. However, the scattering of these datasets across platforms, along with inconsistent metadata and preprocessing procedure, hinders AI-driven discovery. To address these challenges, we developed the Human Respiratory Viral Immunization LongitudinAl Gene Expression (HR-VILAGE-3K3M) repository: an AI-ready, rigorously curated dataset that integrates 14,136 RNA-seq profiles from 3,178 subjects across 66 studies encompassing over 2.56 million cells. Spanning vaccination, inoculation, and mixed exposures, the dataset includes microarray, bulk RNA-seq, and single-cell RNA-seq from whole blood, PBMCs, and nasal swabs, sourced from GEO, ImmPort, and ArrayExpress. We harmonized subject-level metadata, standardized outcome measures, applied unified preprocessing pipelines with rigorous quality control, and aligned all data to official gene symbols. To demonstrate the utility of HR-VILAGE-3K3M, we performed predictive modeling of vaccine responders and evaluated batch-effect correction methods. Beyond these initial demonstrations, it supports diverse systems immunology applications and benchmarking of feature selection and transfer learning algorithms. Its scale and heterogeneity also make it ideal for pretraining foundation models of the human immune response and for advancing multimodal learning frameworks. As the largest longitudinal transcriptomic resource for human respiratory viral immunization, it provides an accessible platform for reproducible AI-driven research, accelerating systems immunology and vaccine development against emerging viral threats.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective climate policies for major emission reductions of ozone precursors: Global evidence from two decades</title>
<link>https://arxiv.org/abs/2505.14731</link>
<guid>https://arxiv.org/abs/2505.14731</guid>
<content:encoded><![CDATA[
arXiv:2505.14731v1 Announce Type: cross 
Abstract: Despite policymakers deploying various tools to mitigate emissions of ozone (O\textsubscript{3}) precursors, such as nitrogen oxides (NO\textsubscript{x}), carbon monoxide (CO), and volatile organic compounds (VOCs), the effectiveness of policy combinations remains uncertain. We employ an integrated framework that couples structural break detection with machine learning to pinpoint effective interventions across the building, electricity, industrial, and transport sectors, identifying treatment effects as abrupt changes without prior assumptions about policy treatment assignment and timing. Applied to two decades of global O\textsubscript{3} precursor emissions data, we detect 78, 77, and 78 structural breaks for NO\textsubscript{x}, CO, and VOCs, corresponding to cumulative emission reductions of 0.96-0.97 Gt, 2.84-2.88 Gt, and 0.47-0.48 Gt, respectively. Sector-level analysis shows that electricity sector structural policies cut NO\textsubscript{x} by up to 32.4\%, while in buildings, developed countries combined adoption subsidies with carbon taxes to achieve 42.7\% CO reductions and developing countries used financing plus fuel taxes to secure 52.3\%. VOCs abatement peaked at 38.5\% when fossil-fuel subsidy reforms were paired with financial incentives. Finally, hybrid strategies merging non-price measures (subsidies, bans, mandates) with pricing instruments delivered up to an additional 10\% co-benefit. These findings guide the sequencing and complementarity of context-specific policy portfolios for O\textsubscript{3} precursor mitigation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transductively Informed Inductive Program Synthesis</title>
<link>https://arxiv.org/abs/2505.14744</link>
<guid>https://arxiv.org/abs/2505.14744</guid>
<content:encoded><![CDATA[
arXiv:2505.14744v1 Announce Type: cross 
Abstract: Abstraction and reasoning in program synthesis has seen significant progress through both inductive and transductive paradigms. Inductive approaches generate a program or latent function from input-output examples, which can then be applied to new inputs. Transductive approaches directly predict output values for given inputs, effectively serving as the function themselves. Current approaches combine inductive and transductive models via isolated ensembling, but they do not explicitly model the interaction between both paradigms. In this work, we introduce \acs{tiips}, a novel framework that unifies transductive and inductive strategies by explicitly modeling their interactions through a cooperative mechanism: an inductive model generates programs, while a transductive model constrains, guides, and refines the search to improve synthesis accuracy and generalization. We evaluate \acs{tiips} on two widely studied program synthesis domains: string and list manipulation. Our results show that \acs{tiips} solves more tasks and yields functions that more closely match optimal solutions in syntax and semantics, particularly in out-of-distribution settings, yielding state-of-the-art performance. We believe that explicitly modeling the synergy between inductive and transductive reasoning opens promising avenues for general-purpose program synthesis and broader applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOD1 3D City Model from LiDAR: The Impact of Segmentation Accuracy on Quality of Urban 3D Modeling and Morphology Extraction</title>
<link>https://arxiv.org/abs/2505.14747</link>
<guid>https://arxiv.org/abs/2505.14747</guid>
<content:encoded><![CDATA[
arXiv:2505.14747v1 Announce Type: cross 
Abstract: Three-dimensional reconstruction of buildings, particularly at Level of Detail 1 (LOD1), plays a crucial role in various applications such as urban planning, urban environmental studies, and designing optimized transportation networks. This study focuses on assessing the potential of LiDAR data for accurate 3D building reconstruction at LOD1 and extracting morphological features from these models. Four deep semantic segmentation models, U-Net, Attention U-Net, U-Net3+, and DeepLabV3+, were used, applying transfer learning to extract building footprints from LiDAR data. The results showed that U-Net3+ and Attention U-Net outperformed the others, achieving IoU scores of 0.833 and 0.814, respectively. Various statistical measures, including maximum, range, mode, median, and the 90th percentile, were used to estimate building heights, resulting in the generation of 3D models at LOD1. As the main contribution of the research, the impact of segmentation accuracy on the quality of 3D building modeling and the accuracy of morphological features like building area and external wall surface area was investigated. The results showed that the accuracy of building identification (segmentation performance) significantly affects the 3D model quality and the estimation of morphological features, depending on the height calculation method. Overall, the UNet3+ method, utilizing the 90th percentile and median measures, leads to accurate height estimation of buildings and the extraction of morphological features.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Independent Machine Learning Approach for Nanometric Axial Localization and Tracking</title>
<link>https://arxiv.org/abs/2505.14754</link>
<guid>https://arxiv.org/abs/2505.14754</guid>
<content:encoded><![CDATA[
arXiv:2505.14754v1 Announce Type: cross 
Abstract: Accurately tracking particles and determining their position along the optical axis is a major challenge in optical microscopy, especially when extremely high precision is needed. In this study, we introduce a deep learning approach using convolutional neural networks (CNNs) that can determine axial positions from dual-focal plane images without relying on predefined models. Our method achieves an axial localization accuracy of 40 nanometers - six times better than traditional single-focal plane techniques. The model's simple design and strong performance make it suitable for a wide range of uses, including dark matter detection, proton therapy for cancer, and radiation protection in space. It also shows promise in fields like biological imaging, materials science, and environmental monitoring. This work highlights how machine learning can turn complex image data into reliable, precise information, offering a flexible and powerful tool for many scientific applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models</title>
<link>https://arxiv.org/abs/2505.14759</link>
<guid>https://arxiv.org/abs/2505.14759</guid>
<content:encoded><![CDATA[
arXiv:2505.14759v1 Announce Type: cross 
Abstract: Large Language Models for code often entail significant computational complexity, which grows significantly with the length of the input code sequence. We propose LeanCode for code simplification to reduce training and prediction time, leveraging code contexts in utilizing attention scores to represent the tokens' importance. We advocate for the selective removal of tokens based on the average context-aware attention scores rather than average scores across all inputs. LeanCode uses the attention scores of `CLS' tokens within the encoder for classification tasks, such as code search. It also employs the encoder-decoder attention scores to determine token significance for sequence-to-sequence tasks like code summarization.Our evaluation shows LeanCode's superiority over the SOTAs DietCode and Slimcode, with improvements of 60% and 16% for code search, and 29% and 27% for code summarization, respectively.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Journalistic Questions: A New Method for Extracting 5W1H in French</title>
<link>https://arxiv.org/abs/2505.14804</link>
<guid>https://arxiv.org/abs/2505.14804</guid>
<content:encoded><![CDATA[
arXiv:2505.14804v1 Announce Type: cross 
Abstract: The 5W1H questions -- who, what, when, where, why and how -- are commonly used in journalism to ensure that an article describes events clearly and systematically. Answering them is a crucial prerequisites for tasks such as summarization, clustering, and news aggregation. In this paper, we design the first automated extraction pipeline to get 5W1H information from French news articles. To evaluate the performance of our algo- rithm, we also create a corpus of 250 Quebec news articles with 5W1H answers marked by four human annotators. Our results demonstrate that our pipeline performs as well in this task as the large language model GPT-4o.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Place Cells as Position Embeddings of Multi-Time Random Walk Transition Kernels for Path Planning</title>
<link>https://arxiv.org/abs/2505.14806</link>
<guid>https://arxiv.org/abs/2505.14806</guid>
<content:encoded><![CDATA[
arXiv:2505.14806v1 Announce Type: cross 
Abstract: The hippocampus orchestrates spatial navigation through collective place cell encodings that form cognitive maps. We reconceptualize the population of place cells as position embeddings approximating multi-scale symmetric random walk transition kernels: the inner product $\langle h(x, t), h(y, t) \rangle = q(y|x, t)$ represents normalized transition probabilities, where $h(x, t)$ is the embedding at location $ x $, and $q(y|x, t)$ is the normalized symmetric transition probability over time $t$. The time parameter $\sqrt{t}$ defines a spatial scale hierarchy, mirroring the hippocampal dorsoventral axis. $q(y|x, t)$ defines spatial adjacency between $x$ and $y$ at scale or resolution $\sqrt{t}$, and the pairwise adjacency relationships $(q(y|x, t), \forall x, y)$ are reduced into individual embeddings $(h(x, t), \forall x)$ that collectively form a map of the environment at sale $\sqrt{t}$. Our framework employs gradient ascent on $q(y|x, t) = \langle h(x, t), h(y, t)\rangle$ with adaptive scale selection, choosing the time scale with maximal gradient at each step for trap-free, smooth trajectories. Efficient matrix squaring $P_{2t} = P_t^2$ builds global representations from local transitions $P_1$ without memorizing past trajectories, enabling hippocampal preplay-like path planning. This produces robust navigation through complex environments, aligning with hippocampal navigation. Experimental results show that our model captures place cell properties -- field size distribution, adaptability, and remapping -- while achieving computational efficiency. By modeling collective transition probabilities rather than individual place fields, we offer a biologically plausible, scalable framework for spatial navigation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-Distribution Generalization of In-Context Learning: A Low-Dimensional Subspace Perspective</title>
<link>https://arxiv.org/abs/2505.14808</link>
<guid>https://arxiv.org/abs/2505.14808</guid>
<content:encoded><![CDATA[
arXiv:2505.14808v1 Announce Type: cross 
Abstract: This work aims to demystify the out-of-distribution (OOD) capabilities of in-context learning (ICL) by studying linear regression tasks parameterized with low-rank covariance matrices. With such a parameterization, we can model distribution shifts as a varying angle between the subspace of the training and testing covariance matrices. We prove that a single-layer linear attention model incurs a test risk with a non-negligible dependence on the angle, illustrating that ICL is not robust to such distribution shifts. However, using this framework, we also prove an interesting property of ICL: when trained on task vectors drawn from a union of low-dimensional subspaces, ICL can generalize to any subspace within their span, given sufficiently long prompt lengths. This suggests that the OOD generalization ability of Transformers may actually stem from the new task lying within the span of those encountered during training. We empirically show that our results also hold for models such as GPT-2, and conclude with (i) experiments on how our observations extend to nonlinear function classes and (ii) results on how LoRA has the ability to capture distribution shifts.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation</title>
<link>https://arxiv.org/abs/2505.14848</link>
<guid>https://arxiv.org/abs/2505.14848</guid>
<content:encoded><![CDATA[
arXiv:2505.14848v1 Announce Type: cross 
Abstract: We present MAATS, a Multi Agent Automated Translation System that leverages the Multidimensional Quality Metrics (MQM) framework as a fine-grained signal for error detection and refinement. MAATS employs multiple specialized AI agents, each focused on a distinct MQM category (e.g., Accuracy, Fluency, Style, Terminology), followed by a synthesis agent that integrates the annotations to iteratively refine translations. This design contrasts with conventional single-agent methods that rely on self-correction.
  Evaluated across diverse language pairs and Large Language Models (LLMs), MAATS outperforms zero-shot and single-agent baselines with statistically significant gains in both automatic metrics and human assessments. It excels particularly in semantic accuracy, locale adaptation, and linguistically distant language pairs. Qualitative analysis highlights its strengths in multi-layered error diagnosis, omission detection across perspectives, and context-aware refinement. By aligning modular agent roles with interpretable MQM dimensions, MAATS narrows the gap between black-box LLMs and human translation workflows, shifting focus from surface fluency to deeper semantic and contextual fidelity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EasyMath: A 0-shot Math Benchmark for SLMs</title>
<link>https://arxiv.org/abs/2505.14852</link>
<guid>https://arxiv.org/abs/2505.14852</guid>
<content:encoded><![CDATA[
arXiv:2505.14852v1 Announce Type: cross 
Abstract: EasyMath is a compact benchmark for practical math reasoning in small language models. It covers thirteen categories, from basic arithmetic and order of operations to word problems, algebraic expressions, edge cases, and omits specialist topics. We tested 23 models (14M to 4B parameters) using exact, numerical, and symbolic checks on free-form answers in a zero-shot setting. Accuracy rises with size and training, chain-of-thought adds modest gains, and consistency improves at scale.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOBSTUR: A Local Bootstrap Framework for Tuning Unsupervised Representations in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.14867</link>
<guid>https://arxiv.org/abs/2505.14867</guid>
<content:encoded><![CDATA[
arXiv:2505.14867v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) are increasingly used in conjunction with unsupervised learning techniques to learn powerful node representations, but their deployment is hindered by their high sensitivity to hyperparameter tuning and the absence of established methodologies for selecting the optimal models. To address these challenges, we propose LOBSTUR-GNN ({\bf Lo}cal {\bf B}oot{\bf s}trap for {\bf T}uning {\bf U}nsupervised {\bf R}epresentations in GNNs) i), a novel framework designed to adapt bootstrapping techniques for unsupervised graph representation learning. LOBSTUR-GNN tackles two main challenges: (a) adapting the bootstrap edge and feature resampling process to account for local graph dependencies in creating alternative versions of the same graph, and (b) establishing robust metrics for evaluating learned representations without ground-truth labels. Using locally bootstrapped resampling and leveraging Canonical Correlation Analysis (CCA) to assess embedding consistency, LOBSTUR provides a principled approach for hyperparameter tuning in unsupervised GNNs. We validate the effectiveness and efficiency of our proposed method through extensive experiments on established academic datasets, showing an 65.9\% improvement in the classification accuracy compared to an uninformed selection of hyperparameters. Finally, we deploy our framework on a real-world application, thereby demonstrating its validity and practical utility in various settings. \footnote{The code is available at \href{https://github.com/sowonjeong/lobstur-graph-bootstrap}{github.com/sowonjeong/lobstur-graph-bootstrap}.}
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models</title>
<link>https://arxiv.org/abs/2505.14871</link>
<guid>https://arxiv.org/abs/2505.14871</guid>
<content:encoded><![CDATA[
arXiv:2505.14871v1 Announce Type: cross 
Abstract: The efficient implementation of large language models (LLMs) is crucial for deployment on resource-constrained devices. Low-rank tensor compression techniques, such as tensor-train (TT) networks, have been widely studied for over-parameterized neural networks. However, their applications to compress pre-trained large language models (LLMs) for downstream tasks (post-training) remains challenging due to the high-rank nature of pre-trained LLMs and the lack of access to pretraining data. In this study, we investigate low-rank tensorized LLMs during fine-tuning and propose sparse augmented tensor networks (Saten) to enhance their performance. The proposed Saten framework enables full model compression. Experimental results demonstrate that Saten enhances both accuracy and compression efficiency in tensorized language models, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications</title>
<link>https://arxiv.org/abs/2505.14918</link>
<guid>https://arxiv.org/abs/2505.14918</guid>
<content:encoded><![CDATA[
arXiv:2505.14918v1 Announce Type: cross 
Abstract: This study introduces a framework for evaluating consistency in large language model (LLM) binary text classification, addressing the lack of established reliability assessment methods. Adapting psychometric principles, we determine sample size requirements, develop metrics for invalid responses, and evaluate intra- and inter-rater reliability. Our case study examines financial news sentiment classification across 14 LLMs (including claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and command-r-plus), with five replicates per model on 1,350 articles. Models demonstrated high intra-rater consistency, achieving perfect agreement on 90-98% of examples, with minimal differences between expensive and economical models from the same families. When validated against StockNewsAPI labels, models achieved strong performance (accuracy 0.76-0.88), with smaller models like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger counterparts. All models performed at chance when predicting actual market movements, indicating task constraints rather than model limitations. Our framework provides systematic guidance for LLM selection, sample size planning, and reliability assessment, enabling organizations to optimize resources for classification tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SecCAN: An Extended CAN Controller with Embedded Intrusion Detection</title>
<link>https://arxiv.org/abs/2505.14924</link>
<guid>https://arxiv.org/abs/2505.14924</guid>
<content:encoded><![CDATA[
arXiv:2505.14924v1 Announce Type: cross 
Abstract: Recent research has highlighted the vulnerability of in-vehicle network protocols such as controller area networks (CAN) and proposed machine learning-based intrusion detection systems (IDSs) as an effective mitigation technique. However, their efficient integration into vehicular architecture is non-trivial, with existing methods relying on electronic control units (ECUs)-coupled IDS accelerators or dedicated ECUs as IDS accelerators. Here, initiating IDS requires complete reception of a CAN message from the controller, incurring data movement and software overheads. In this paper, we present SecCAN, a novel CAN controller architecture that embeds IDS capability within the datapath of the controller. This integration allows IDS to tap messages directly from within the CAN controller as they are received from the bus, removing overheads incurred by existing ML-based IDSs. A custom-quantised machine-learning accelerator is developed as the IDS engine and embedded into SecCAN's receive data path, with optimisations to overlap the IDS inference with the protocol's reception window. We implement SecCAN on AMD XCZU7EV FPGA to quantify its performance and benefits in hardware, using multiple attack datasets. We show that SecCAN can completely hide the IDS latency within the CAN reception window for all CAN packet sizes and detect multiple attacks with state-of-the-art accuracy with zero software overheads on the ECU and low energy overhead (73.7 uJ per message) for IDS inference. Also, SecCAN incurs limited resource overhead compared to a standard CAN controller (< 30% LUT, < 1% FF), making it ideally suited for automotive deployment.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels</title>
<link>https://arxiv.org/abs/2505.14925</link>
<guid>https://arxiv.org/abs/2505.14925</guid>
<content:encoded><![CDATA[
arXiv:2505.14925v1 Announce Type: cross 
Abstract: Although the context length of large language models (LLMs) has increased to millions of tokens, evaluating their effectiveness beyond needle-in-a-haystack approaches has proven difficult. We argue that novels provide a case study of subtle, complicated structure and long-range semantic dependencies often over 128k tokens in length. Inspired by work on computational novel analysis, we release the Too Long, Didn't Model (TLDM) benchmark, which tests a model's ability to report plot summary, storyworld configuration, and elapsed narrative time. We find that none of seven tested frontier LLMs retain stable understanding beyond 64k tokens. Our results suggest language model developers must look beyond "lost in the middle" benchmarks when evaluating model performance in complex long-context scenarios. To aid in further development we release the TLDM benchmark together with reference code and data.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmatic Video Prediction Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.14948</link>
<guid>https://arxiv.org/abs/2505.14948</guid>
<content:encoded><![CDATA[
arXiv:2505.14948v1 Announce Type: cross 
Abstract: The task of estimating the world model describing the dynamics of a real world process assumes immense importance for anticipating and preparing for future outcomes. For applications such as video surveillance, robotics applications, autonomous driving, etc. this objective entails synthesizing plausible visual futures, given a few frames of a video to set the visual context. Towards this end, we propose ProgGen, which undertakes the task of video frame prediction by representing the dynamics of the video using a set of neuro-symbolic, human-interpretable set of states (one per frame) by leveraging the inductive biases of Large (Vision) Language Models (LLM/VLM). In particular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate the states of the video, given the visual context (i.e. the frames); (ii) to predict the states corresponding to future time steps by estimating the transition dynamics; (iii) to render the predicted states as visual RGB-frames. Empirical evaluations reveal that our proposed method outperforms competing techniques at the task of video frame prediction in two challenging environments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits counter-factual reasoning and interpretable video generation attesting to its effectiveness and generalizability for video generation tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Evolving Curriculum for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14970</link>
<guid>https://arxiv.org/abs/2505.14970</guid>
<content:encoded><![CDATA[
arXiv:2505.14970v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, we propose Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JARVIS: A Multi-Agent Code Assistant for High-Quality EDA Script Generation</title>
<link>https://arxiv.org/abs/2505.14978</link>
<guid>https://arxiv.org/abs/2505.14978</guid>
<content:encoded><![CDATA[
arXiv:2505.14978v1 Announce Type: cross 
Abstract: This paper presents JARVIS, a novel multi-agent framework that leverages Large Language Models (LLMs) and domain expertise to generate high-quality scripts for specialized Electronic Design Automation (EDA) tasks. By combining a domain-specific LLM trained with synthetically generated data, a custom compiler for structural verification, rule enforcement, code fixing capabilities, and advanced retrieval mechanisms, our approach achieves significant improvements over state-of-the-art domain-specific models. Our framework addresses the challenges of data scarcity and hallucination errors in LLMs, demonstrating the potential of LLMs in specialized engineering domains. We evaluate our framework on multiple benchmarks and show that it outperforms existing models in terms of accuracy and reliability. Our work sets a new precedent for the application of LLMs in EDA and paves the way for future innovations in this field.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnyBody: A Benchmark Suite for Cross-Embodiment Manipulation</title>
<link>https://arxiv.org/abs/2505.14986</link>
<guid>https://arxiv.org/abs/2505.14986</guid>
<content:encoded><![CDATA[
arXiv:2505.14986v1 Announce Type: cross 
Abstract: Generalizing control policies to novel embodiments remains a fundamental challenge in enabling scalable and transferable learning in robotics. While prior works have explored this in locomotion, a systematic study in the context of manipulation tasks remains limited, partly due to the lack of standardized benchmarks. In this paper, we introduce a benchmark for learning cross-embodiment manipulation, focusing on two foundational tasks-reach and push-across a diverse range of morphologies. The benchmark is designed to test generalization along three axes: interpolation (testing performance within a robot category that shares the same link structure), extrapolation (testing on a robot with a different link structure), and composition (testing on combinations of link structures). On the benchmark, we evaluate the ability of different RL policies to learn from multiple morphologies and to generalize to novel ones. Our study aims to answer whether morphology-aware training can outperform single-embodiment baselines, whether zero-shot generalization to unseen morphologies is feasible, and how consistently these patterns hold across different generalization regimes. The results highlight the current limitations of multi-embodiment learning and provide insights into how architectural and training design choices influence policy generalization.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Design Matters: A Self-Design Multi-Agent System</title>
<link>https://arxiv.org/abs/2505.14996</link>
<guid>https://arxiv.org/abs/2505.14996</guid>
<content:encoded><![CDATA[
arXiv:2505.14996v1 Announce Type: cross 
Abstract: Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs' strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation-set for tuning and yield static MAS designs lacking adaptability during inference. We introduce SELF-MAS, the first self-supervised, inference-time only framework for automatic MAS design. SELF-MAS employs meta-level design to iteratively generate, evaluate, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic agent composition and problem decomposition through meta-feedback on solvability and completeness. Experiments across math, graduate-level QA, and software engineering benchmarks, using both closed-source and open-source LLM back-bones of varying sizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS baselines, achieving a 7.44% average accuracy improvement over the next strongest baseline while maintaining cost-efficiency. These findings underscore the promise of meta-level self-supervised design for creating effective and adaptive MAS.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling the iterative CHAD</title>
<link>https://arxiv.org/abs/2505.15002</link>
<guid>https://arxiv.org/abs/2505.15002</guid>
<content:encoded><![CDATA[
arXiv:2505.15002v1 Announce Type: cross 
Abstract: Combinatory Homomorphic Automatic Differentiation (CHAD) was originally formulated as a semantics-driven source transformation for reverse-mode AD in total programming languages. We extend this framework to partial languages with features such as potentially non-terminating operations, real-valued conditionals, and iteration constructs like while-loops, while preserving CHAD's structure-preserving semantics principle. A key contribution is the introduction of iteration-extensive indexed categories, which allow iteration in the base category to lift to parameterized initial algebras in the indexed category. This enables iteration to be interpreted in the Grothendieck construction of the target language in a principled way. The resulting fibred iterative structure cleanly models iteration in the categorical semantics. Consequently, the extended CHAD transformation remains the unique structure-preserving functor (an iterative Freyd category morphism) from the freely generated iterative Freyd category of the source language to the Grothendieck construction of the target's syntactic semantics, mapping each primitive operation to its derivative. We prove the correctness of this transformation using the universal property of the source language's syntax, showing that the transformed programs compute correct reverse-mode derivatives. Our development also contributes to understanding iteration constructs within dependently typed languages and categories of containers. As our primary motivation and application, we generalize CHAD to languages with data types, partial features, and iteration, providing the first rigorous categorical semantics for reverse-mode CHAD in such settings and formally guaranteeing the correctness of the source-to-source CHAD technique.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence of Adam in Deep ReLU Networks via Directional Complexity and Kakeya Bounds</title>
<link>https://arxiv.org/abs/2505.15013</link>
<guid>https://arxiv.org/abs/2505.15013</guid>
<content:encoded><![CDATA[
arXiv:2505.15013v1 Announce Type: cross 
Abstract: First-order adaptive optimization methods like Adam are the default choices for training modern deep neural networks. Despite their empirical success, the theoretical understanding of these methods in non-smooth settings, particularly in Deep ReLU networks, remains limited. ReLU activations create exponentially many region boundaries where standard smoothness assumptions break down. \textbf{We derive the first \(\tilde{O}\!\bigl(\sqrt{d_{\mathrm{eff}}/n}\bigr)\) generalization bound for Adam in Deep ReLU networks and the first global-optimal convergence for Adam in the non smooth, non convex relu landscape without a global PL or convexity assumption.} Our analysis is based on stratified Morse theory and novel results in Kakeya sets. We develop a multi-layer refinement framework that progressively tightens bounds on region crossings. We prove that the number of region crossings collapses from exponential to near-linear in the effective dimension. Using a Kakeya based method, we give a tighter generalization bound than PAC-Bayes approaches and showcase convergence using a mild uniform low barrier assumption.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infinite hierarchical contrastive clustering for personal digital envirotyping</title>
<link>https://arxiv.org/abs/2505.15022</link>
<guid>https://arxiv.org/abs/2505.15022</guid>
<content:encoded><![CDATA[
arXiv:2505.15022v1 Announce Type: cross 
Abstract: Daily environments have profound influence on our health and behavior. Recent work has shown that digital envirotyping, where computer vision is applied to images of daily environments taken during ecological momentary assessment (EMA), can be used to identify meaningful relationships between environmental features and health outcomes of interest. To systematically study such effects on an individual level, it is helpful to group images into distinct environments encountered in an individual's daily life; these may then be analyzed, further grouped into related environments with similar features, and linked to health outcomes. Here we introduce infinite hierarchical contrastive clustering to address this challenge. Building on the established contrastive clustering framework, our method a) allows an arbitrary number of clusters without requiring the full Dirichlet Process machinery by placing a stick-breaking prior on predicted cluster probabilities; and b) encourages distinct environments to form well-defined sub-clusters within each cluster of related environments by incorporating a participant-specific prediction loss. Our experiments show that our model effectively identifies distinct personal environments and groups these environments into meaningful environment types. We then illustrate how the resulting clusters can be linked to various health outcomes, highlighting the potential of our approach to advance the envirotyping paradigm.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-based Airflow Inertial Odometry for MAVs using Thermal Anemometers in a GPS and vision denied environment</title>
<link>https://arxiv.org/abs/2505.15044</link>
<guid>https://arxiv.org/abs/2505.15044</guid>
<content:encoded><![CDATA[
arXiv:2505.15044v1 Announce Type: cross 
Abstract: This work demonstrates an airflow inertial based odometry system with multi-sensor data fusion, including thermal anemometer, IMU, ESC, and barometer. This goal is challenging because low-cost IMUs and barometers have significant bias, and anemometer measurements are very susceptible to interference from spinning propellers and ground effects. We employ a GRU-based deep neural network to estimate relative air speed from noisy and disturbed anemometer measurements, and an observer with bias model to fuse the sensor data and thus estimate the state of aerial vehicle. A complete flight data, including takeoff and landing on the ground, shows that the approach is able to decouple the downwash induced wind speed caused by propellers and the ground effect, and accurately estimate the flight speed in a wind-free indoor environment. IMU, and barometer bias are effectively estimated, which significantly reduces the position integration drift, which is only 5.7m for 203s manual random flight. The open source is available on https://github.com/SyRoCo-ISIR/Flight-Speed-Estimation-Airflow.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation</title>
<link>https://arxiv.org/abs/2505.15054</link>
<guid>https://arxiv.org/abs/2505.15054</guid>
<content:encoded><![CDATA[
arXiv:2505.15054v1 Announce Type: cross 
Abstract: Precise recognition, editing, and generation of molecules are essential prerequisites for both chemists and AI systems tackling various chemical tasks. We present MolLangBench, a comprehensive benchmark designed to evaluate fundamental molecule-language interface tasks: language-prompted molecular structure recognition, editing, and generation. To ensure high-quality, unambiguous, and deterministic outputs, we construct the recognition tasks using automated cheminformatics tools, and curate editing and generation tasks through rigorous expert annotation and validation. MolLangBench supports the evaluation of models that interface language with different molecular representations, including linear strings, molecular images, and molecular graphs. Evaluations of state-of-the-art models reveal significant limitations: the strongest model (o3) achieves $79.2\%$ and $78.5\%$ accuracy on recognition and editing tasks, which are intuitively simple for humans, and performs even worse on the generation task, reaching only $29.0\%$ accuracy. These results highlight the shortcomings of current AI systems in handling even preliminary molecular recognition and manipulation tasks. We hope MolLangBench will catalyze further research toward more effective and reliable AI systems for chemical applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges</title>
<link>https://arxiv.org/abs/2505.15068</link>
<guid>https://arxiv.org/abs/2505.15068</guid>
<content:encoded><![CDATA[
arXiv:2505.15068v1 Announce Type: cross 
Abstract: Recent progress in large language models (LLMs) has enabled substantial advances in solving mathematical problems. However, existing benchmarks often fail to reflect the complexity of real-world problems, which demand open-ended, interdisciplinary reasoning and integration of computational tools. To address this gap, we introduce ModelingBench, a novel benchmark featuring real-world-inspired, open-ended problems from math modeling competitions across diverse domains, ranging from urban traffic optimization to ecosystem resource planning. These tasks require translating natural language into formal mathematical formulations, applying appropriate tools, and producing structured, defensible reports. ModelingBench also supports multiple valid solutions, capturing the ambiguity and creativity of practical modeling. We also present ModelingAgent, a multi-agent framework that coordinates tool use, supports structured workflows, and enables iterative self-refinement to generate well-grounded, creative solutions. To evaluate outputs, we further propose ModelingJudge, an expert-in-the-loop system leveraging LLMs as domain-specialized judges assessing solutions from multiple expert perspectives. Empirical results show that ModelingAgent substantially outperforms strong baselines and often produces solutions indistinguishable from those of human experts. Together, our work provides a comprehensive framework for evaluating and advancing real-world problem-solving in open-ended, interdisciplinary modeling challenges.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data</title>
<link>https://arxiv.org/abs/2505.15074</link>
<guid>https://arxiv.org/abs/2505.15074</guid>
<content:encoded><![CDATA[
arXiv:2505.15074v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly aligned with human preferences through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods, Group Relative Policy Optimization (GRPO) has gained attention for its simplicity and strong performance, notably eliminating the need for a learned value function. However, GRPO implicitly assumes a balanced domain distribution and uniform semantic alignment across groups - assumptions that rarely hold in real-world datasets. When applied to multi-domain, imbalanced data, GRPO disproportionately optimizes for dominant domains, neglecting underrepresented ones and resulting in poor generalization and fairness. We propose Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled extension to GRPO that addresses inter-group imbalance with two key innovations. Domain-aware reward scaling counteracts frequency bias by reweighting optimization based on domain prevalence. Difficulty-aware reward scaling leverages prompt-level self-consistency to identify and prioritize uncertain prompts that offer greater learning value. Together, these strategies promote more equitable and effective policy learning across domains. Extensive experiments across multiple LLMs and skewed training distributions show that DISCO improves generalization, outperforms existing GRPO variants by 5% on Qwen3 models, and sets new state-of-the-art results on multi-domain alignment benchmarks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2505.15075</link>
<guid>https://arxiv.org/abs/2505.15075</guid>
<content:encoded><![CDATA[
arXiv:2505.15075v1 Announce Type: cross 
Abstract: The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation</title>
<link>https://arxiv.org/abs/2505.15077</link>
<guid>https://arxiv.org/abs/2505.15077</guid>
<content:encoded><![CDATA[
arXiv:2505.15077v1 Announce Type: cross 
Abstract: Urban forests play a key role in enhancing environmental quality and supporting biodiversity in cities. Mapping and monitoring these green spaces are crucial for urban planning and conservation, yet accurately detecting trees is challenging due to complex landscapes and the variability in image resolution caused by different satellite sensors or UAV flight altitudes. While deep learning architectures have shown promise in addressing these challenges, their effectiveness remains strongly dependent on the availability of large and manually labeled datasets, which are often expensive and difficult to obtain in sufficient quantity. In this work, we propose a novel pipeline that integrates domain adaptation with GANs and Diffusion models to enhance the quality of low-resolution aerial images. Our proposed pipeline enhances low-resolution imagery while preserving semantic content, enabling effective tree segmentation without requiring large volumes of manually annotated data. Leveraging models such as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we generate realistic and structurally consistent synthetic samples that expand the training dataset and unify scale across domains. This approach not only improves the robustness of segmentation models across different acquisition conditions but also provides a scalable and replicable solution for remote sensing scenarios with scarce annotation resources. Experimental results demonstrated an improvement of over 50% in IoU for low-resolution images, highlighting the effectiveness of our method compared to traditional pipelines.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer</title>
<link>https://arxiv.org/abs/2505.15090</link>
<guid>https://arxiv.org/abs/2505.15090</guid>
<content:encoded><![CDATA[
arXiv:2505.15090v1 Announce Type: cross 
Abstract: Effective cross-lingual transfer remains a critical challenge in scaling the benefits of large language models from high-resource to low-resource languages. Towards this goal, prior studies have explored many approaches to combine task knowledge from task-specific data in a (high-resource) source language and language knowledge from unlabeled text in a (low-resource) target language. One notable approach proposed composable sparse fine-tuning (SFT) for cross-lingual transfer that learns task-specific and language-specific sparse masks to select a subset of the pretrained model's parameters that are further fine-tuned. These sparse fine-tuned vectors (SFTs) are subsequently composed with the pretrained model to facilitate zero-shot cross-lingual transfer to a task in a target language, using only task-specific data from a source language. These sparse masks for SFTs were identified using a simple magnitude-based pruning. In our work, we introduce DeFT-X, a novel composable SFT approach that denoises the weight matrices of a pretrained model before magnitude pruning using singular value decomposition, thus yielding more robust SFTs. We evaluate DeFT-X on a diverse set of extremely low-resource languages for sentiment classification (NusaX) and natural language inference (AmericasNLI) and demonstrate that it performs at par or outperforms SFT and other prominent cross-lingual transfer baselines.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Generative Models with Experimental Data for Protein Fitness Optimization</title>
<link>https://arxiv.org/abs/2505.15093</link>
<guid>https://arxiv.org/abs/2505.15093</guid>
<content:encoded><![CDATA[
arXiv:2505.15093v1 Announce Type: cross 
Abstract: Protein fitness optimization involves finding a protein sequence that maximizes desired quantitative properties in a combinatorially large design space of possible sequences. Recent developments in steering protein generative models (e.g diffusion models, language models) offer a promising approach. However, by and large, past studies have optimized surrogate rewards and/or utilized large amounts of labeled data for steering, making it unclear how well existing methods perform and compare to each other in real-world optimization campaigns where fitness is measured by low-throughput wet-lab assays. In this study, we explore fitness optimization using small amounts (hundreds) of labeled sequence-fitness pairs and comprehensively evaluate strategies such as classifier guidance and posterior sampling for guiding generation from different discrete diffusion models of protein sequences. We also demonstrate how guidance can be integrated into adaptive sequence selection akin to Thompson sampling in Bayesian optimization, showing that plug-and-play guidance strategies offer advantages compared to alternatives such as reinforcement learning with protein language models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lung Nodule-SSM: Self-Supervised Lung Nodule Detection and Classification in Thoracic CT Images</title>
<link>https://arxiv.org/abs/2505.15120</link>
<guid>https://arxiv.org/abs/2505.15120</guid>
<content:encoded><![CDATA[
arXiv:2505.15120v1 Announce Type: cross 
Abstract: Lung cancer remains among the deadliest types of cancer in recent decades, and early lung nodule detection is crucial for improving patient outcomes. The limited availability of annotated medical imaging data remains a bottleneck in developing accurate computer-aided diagnosis (CAD) systems. Self-supervised learning can help leverage large amounts of unlabeled data to develop more robust CAD systems. With the recent advent of transformer-based architecture and their ability to generalize to unseen tasks, there has been an effort within the healthcare community to adapt them to various medical downstream tasks. Thus, we propose a novel "LungNodule-SSM" method, which utilizes selfsupervised learning with DINOv2 as a backbone to enhance lung nodule detection and classification without annotated data. Our methodology has two stages: firstly, the DINOv2 model is pre-trained on unlabeled CT scans to learn robust feature representations, then secondly, these features are fine-tuned using transformer-based architectures for lesionlevel detection and accurate lung nodule diagnosis. The proposed method has been evaluated on the challenging LUNA 16 dataset, consisting of 888 CT scans, and compared with SOTA methods. Our experimental results show the superiority of our proposed method with an accuracy of 98.37%, explaining its effectiveness in lung nodule detection. The source code, datasets, and pre-processed data can be accessed using the link:https://github.com/EMeRALDsNRPU/Lung-Nodule-SSM-Self-Supervised-Lung-Nodule-Detection-and-Classification/tree/main
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation Trainer</title>
<link>https://arxiv.org/abs/2505.15133</link>
<guid>https://arxiv.org/abs/2505.15133</guid>
<content:encoded><![CDATA[
arXiv:2505.15133v1 Announce Type: cross 
Abstract: Recent advances in knowledge distillation have emphasized the importance of decoupling different knowledge components. While existing methods utilize momentum mechanisms to separate task-oriented and distillation gradients, they overlook the inherent conflict between target-class and non-target-class knowledge flows. Furthermore, low-confidence dark knowledge in non-target classes introduces noisy signals that hinder effective knowledge transfer. To address these limitations, we propose DeepKD, a novel training framework that integrates dual-level decoupling with adaptive denoising. First, through theoretical analysis of gradient signal-to-noise ratio (GSNR) characteristics in task-oriented and non-task-oriented knowledge distillation, we design independent momentum updaters for each component to prevent mutual interference. We observe that the optimal momentum coefficients for task-oriented gradient (TOG), target-class gradient (TCG), and non-target-class gradient (NCG) should be positively related to their GSNR. Second, we introduce a dynamic top-k mask (DTM) mechanism that gradually increases K from a small initial value to incorporate more non-target classes as training progresses, following curriculum learning principles. The DTM jointly filters low-confidence logits from both teacher and student models, effectively purifying dark knowledge during early training. Extensive experiments on CIFAR-100, ImageNet, and MS-COCO demonstrate DeepKD's effectiveness. Our code is available at https://github.com/haiduo/DeepKD.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization</title>
<link>https://arxiv.org/abs/2505.15155</link>
<guid>https://arxiv.org/abs/2505.15155</guid>
<content:encoded><![CDATA[
arXiv:2505.15155v1 Announce Type: cross 
Abstract: Financial markets pose fundamental challenges for asset return prediction due to their high dimensionality, non-stationarity, and persistent volatility. Despite advances in large language models and multi-agent systems, current quantitative research pipelines suffer from limited automation, weak interpretability, and fragmented coordination across key components such as factor mining and model innovation. In this paper, we propose R&amp;D-Agent for Quantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization. RD-Agent(Q) decomposes the quant process into two iterative stages: a Research stage that dynamically sets goal-aligned prompts, formulates hypotheses based on domain priors, and maps them to concrete tasks, and a Development stage that employs a code-generation agent, Co-STEER, to implement task-specific code, which is then executed in real-market backtests. The two stages are connected through a feedback stage that thoroughly evaluates experimental outcomes and informs subsequent iterations, with a multi-armed bandit scheduler for adaptive direction selection. Empirically, RD-Agent(Q) achieves up to 2X higher annualized returns than classical factor libraries using 70% fewer factors, and outperforms state-of-the-art deep time-series models on real markets. Its joint factor-model optimization delivers a strong balance between predictive accuracy and strategy robustness. Our code is available at: https://github.com/microsoft/RD-Agent.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascaded Diffusion Models for Neural Motion Planning</title>
<link>https://arxiv.org/abs/2505.15157</link>
<guid>https://arxiv.org/abs/2505.15157</guid>
<content:encoded><![CDATA[
arXiv:2505.15157v1 Announce Type: cross 
Abstract: Robots in the real world need to perceive and move to goals in complex environments without collisions. Avoiding collisions is especially difficult when relying on sensor perception and when goals are among clutter. Diffusion policies and other generative models have shown strong performance in solving local planning problems, but often struggle at avoiding all of the subtle constraint violations that characterize truly challenging global motion planning problems. In this work, we propose an approach for learning global motion planning using diffusion policies, allowing the robot to generate full trajectories through complex scenes and reasoning about multiple obstacles along the path. Our approach uses cascaded hierarchical models which unify global prediction and local refinement together with online plan repair to ensure the trajectories are collision free. Our method outperforms (by ~5%) a wide variety of baselines on challenging tasks in multiple domains including navigation and manipulation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Linear Approach to Data Poisoning</title>
<link>https://arxiv.org/abs/2505.15175</link>
<guid>https://arxiv.org/abs/2505.15175</guid>
<content:encoded><![CDATA[
arXiv:2505.15175v1 Announce Type: cross 
Abstract: We investigate the theoretical foundations of data poisoning attacks in machine learning models. Our analysis reveals that the Hessian with respect to the input serves as a diagnostic tool for detecting poisoning, exhibiting spectral signatures that characterize compromised datasets. We use random matrix theory (RMT) to develop a theory for the impact of poisoning proportion and regularisation on attack efficacy in linear regression. Through QR stepwise regression, we study the spectral signatures of the Hessian in multi-output regression. We perform experiments on deep networks to show experimentally that this theory extends to modern convolutional and transformer networks under the cross-entropy loss. Based on these insights we develop preliminary algorithms to determine if a network has been poisoned and remedies which do not require further training.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection</title>
<link>https://arxiv.org/abs/2505.15182</link>
<guid>https://arxiv.org/abs/2505.15182</guid>
<content:encoded><![CDATA[
arXiv:2505.15182v1 Announce Type: cross 
Abstract: Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent's actual state and goal. Our analysis finds that this stems from ReAct's inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent's state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG-Based Inter-Patient Epileptic Seizure Detection Combining Domain Adversarial Training with CNN-BiLSTM Network</title>
<link>https://arxiv.org/abs/2505.15203</link>
<guid>https://arxiv.org/abs/2505.15203</guid>
<content:encoded><![CDATA[
arXiv:2505.15203v1 Announce Type: cross 
Abstract: Automated epileptic seizure detection from electroencephalogram (EEG) remains challenging due to significant individual differences in EEG patterns across patients. While existing studies achieve high accuracy with patient-specific approaches, they face difficulties in generalizing to new patients. To address this, we propose a detection framework combining domain adversarial training with a convolutional neural network (CNN) and a bidirectional long short-term memory (BiLSTM). First, the CNN extracts local patient-invariant features through domain adversarial training, which optimizes seizure detection accuracy while minimizing patient-specific characteristics. Then, the BiLSTM captures temporal dependencies in the extracted features to model seizure evolution patterns. Evaluation using EEG recordings from 20 patients with focal epilepsy demonstrated superior performance over non-adversarial methods, achieving high detection accuracy across different patients. The integration of adversarial training with temporal modeling enables robust cross-patient seizure detection.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering and Pruning in Causal Data Fusion</title>
<link>https://arxiv.org/abs/2505.15215</link>
<guid>https://arxiv.org/abs/2505.15215</guid>
<content:encoded><![CDATA[
arXiv:2505.15215v1 Announce Type: cross 
Abstract: Data fusion, the process of combining observational and experimental data, can enable the identification of causal effects that would otherwise remain non-identifiable. Although identification algorithms have been developed for specific scenarios, do-calculus remains the only general-purpose tool for causal data fusion, particularly when variables are present in some data sources but not others. However, approaches based on do-calculus may encounter computational challenges as the number of variables increases and the causal graph grows in complexity. Consequently, there exists a need to reduce the size of such models while preserving the essential features. For this purpose, we propose pruning (removing unnecessary variables) and clustering (combining variables) as preprocessing operations for causal data fusion. We generalize earlier results on a single data source and derive conditions for applying pruning and clustering in the case of multiple data sources. We give sufficient conditions for inferring the identifiability or non-identifiability of a causal effect in a larger graph based on a smaller graph and show how to obtain the corresponding identifying functional for identifiable causal effects. Examples from epidemiology and social science demonstrate the use of the results.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems</title>
<link>https://arxiv.org/abs/2505.15216</link>
<guid>https://arxiv.org/abs/2505.15216</guid>
<content:encoded><![CDATA[
arXiv:2505.15216v1 Announce Type: cross 
Abstract: AI agents have the potential to significantly alter the cybersecurity landscape. To help us understand this change, we introduce the first framework to capture offensive and defensive cyber-capabilities in evolving real-world systems. Instantiating this framework with BountyBench, we set up 25 systems with complex, real-world codebases. To capture the vulnerability lifecycle, we define three task types: Detect (detecting a new vulnerability), Exploit (exploiting a specific vulnerability), and Patch (patching a specific vulnerability). For Detect, we construct a new success indicator, which is general across vulnerability types and provides localized evaluation. We manually set up the environment for each system, including installing packages, setting up server(s), and hydrating database(s). We add 40 bug bounties, which are vulnerabilities with monetary awards from \$10 to \$30,485, and cover 9 of the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy based on information to guide detection, interpolating from identifying a zero day to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code, OpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and Claude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing agents are Claude Code (5% on Detect, mapping to \$1,350), Custom Agent with Claude 3.7 Sonnet Thinking (5% on Detect, mapping to \$1,025; 67.5% on Exploit), and OpenAI Codex CLI (5% on Detect, mapping to \$2,400; 90% on Patch, mapping to \$14,422). OpenAI Codex CLI and Claude Code are more capable at defense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit scores of 32.5% and 57.5% respectively; in contrast, the custom agents are relatively balanced between offense and defense, achieving Exploit scores of 40-67.5% and Patch scores of 45-60%.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recognition of Unseen Combined Motions via Convex Combination-based EMG Pattern Synthesis for Myoelectric Control</title>
<link>https://arxiv.org/abs/2505.15218</link>
<guid>https://arxiv.org/abs/2505.15218</guid>
<content:encoded><![CDATA[
arXiv:2505.15218v1 Announce Type: cross 
Abstract: Electromyogram (EMG) signals recorded from the skin surface enable intuitive control of assistive devices such as prosthetic limbs. However, in EMG-based motion recognition, collecting comprehensive training data for all target motions remains challenging, particularly for complex combined motions. This paper proposes a method to efficiently recognize combined motions using synthetic EMG data generated through convex combinations of basic motion patterns. Instead of measuring all possible combined motions, the proposed method utilizes measured basic motion data along with synthetically combined motion data for training. This approach expands the range of recognizable combined motions while minimizing the required training data collection. We evaluated the effectiveness of the proposed method through an upper limb motion classification experiment with eight subjects. The experimental results demonstrated that the proposed method improved the classification accuracy for unseen combined motions by approximately 17%.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Versatile Reservoir Computing for Heterogeneous Complex Networks</title>
<link>https://arxiv.org/abs/2505.15219</link>
<guid>https://arxiv.org/abs/2505.15219</guid>
<content:encoded><![CDATA[
arXiv:2505.15219v1 Announce Type: cross 
Abstract: A new machine learning scheme, termed versatile reservoir computing, is proposed for sustaining the dynamics of heterogeneous complex networks. We show that a single, small-scale reservoir computer trained on time series from a subset of elements is able to replicate the dynamics of any element in a large-scale complex network, though the elements are of different intrinsic parameters and connectivities. Furthermore, by substituting failed elements with the trained machine, we demonstrate that the collective dynamics of the network can be preserved accurately over a finite time horizon. The capability and effectiveness of the proposed scheme are validated on three representative network models: a homogeneous complex network of non-identical phase oscillators, a heterogeneous complex network of non-identical phase oscillators, and a heterogeneous complex network of non-identical chaotic oscillators.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge</title>
<link>https://arxiv.org/abs/2505.15240</link>
<guid>https://arxiv.org/abs/2505.15240</guid>
<content:encoded><![CDATA[
arXiv:2505.15240v1 Announce Type: cross 
Abstract: This paper explores generalised probabilistic modelling and uncertainty estimation in comparative LLM-as-a-judge frameworks. We show that existing Product-of-Experts methods are specific cases of a broader framework, enabling diverse modelling options. Furthermore, we propose improved uncertainty estimates for individual comparisons, enabling more efficient selection and achieving strong performance with fewer evaluations. We also introduce a method for estimating overall ranking uncertainty. Finally, we demonstrate that combining absolute and comparative scoring improves performance. Experiments show that the specific expert model has a limited impact on final rankings but our proposed uncertainty estimates, especially the probability of reordering, significantly improve the efficiency of systems reducing the number of needed comparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used to identify low-performing predictions, where the nature of the probabilistic model has a notable impact on the quality of the overall uncertainty.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VET-DINO: Learning Anatomical Understanding Through Multi-View Distillation in Veterinary Imaging</title>
<link>https://arxiv.org/abs/2505.15248</link>
<guid>https://arxiv.org/abs/2505.15248</guid>
<content:encoded><![CDATA[
arXiv:2505.15248v1 Announce Type: cross 
Abstract: Self-supervised learning has emerged as a powerful paradigm for training deep neural networks, particularly in medical imaging where labeled data is scarce. While current approaches typically rely on synthetic augmentations of single images, we propose VET-DINO, a framework that leverages a unique characteristic of medical imaging: the availability of multiple standardized views from the same study. Using a series of clinical veterinary radiographs from the same patient study, we enable models to learn view-invariant anatomical structures and develop an implied 3D understanding from 2D projections. We demonstrate our approach on a dataset of 5 million veterinary radiographs from 668,000 canine studies. Through extensive experimentation, including view synthesis and downstream task performance, we show that learning from real multi-view pairs leads to superior anatomical understanding compared to purely synthetic augmentations. VET-DINO achieves state-of-the-art performance on various veterinary imaging tasks. Our work establishes a new paradigm for self-supervised learning in medical imaging that leverages domain-specific properties rather than merely adapting natural image techniques.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Private GPT Never Autoregressively Decodes</title>
<link>https://arxiv.org/abs/2505.15252</link>
<guid>https://arxiv.org/abs/2505.15252</guid>
<content:encoded><![CDATA[
arXiv:2505.15252v1 Announce Type: cross 
Abstract: The wide deployment of the generative pre-trained transformer (GPT) has raised privacy concerns for both clients and servers. While cryptographic primitives can be employed for secure GPT inference to protect the privacy of both parties, they introduce considerable performance overhead.To accelerate secure inference, this study proposes a public decoding and secure verification approach that utilizes public GPT models, motivated by the observation that securely decoding one and multiple tokens takes a similar latency. The client uses the public model to generate a set of tokens, which are then securely verified by the private model for acceptance. The efficiency of our approach depends on the acceptance ratio of tokens proposed by the public model, which we improve from two aspects: (1) a private sampling protocol optimized for cryptographic primitives and (2) model alignment using knowledge distillation. Our approach improves the efficiency of secure decoding while maintaining the same level of privacy and generation quality as standard secure decoding. Experiments demonstrate a $2.1\times \sim 6.0\times$ speedup compared to standard decoding across three pairs of public-private models and different network conditions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>gen2seg: Generative Models Enable Generalizable Instance Segmentation</title>
<link>https://arxiv.org/abs/2505.15263</link>
<guid>https://arxiv.org/abs/2505.15263</guid>
<content:encoded><![CDATA[
arXiv:2505.15263v1 Announce Type: cross 
Abstract: By pretraining to synthesize coherent images from perturbed inputs, generative models inherently learn to understand object boundaries and scene compositions. How can we repurpose these generative representations for general-purpose perceptual organization? We finetune Stable Diffusion and MAE (encoder+decoder) for category-agnostic instance segmentation using our instance coloring loss exclusively on a narrow set of object types (indoor furnishings and cars). Surprisingly, our models exhibit strong zero-shot generalization, accurately segmenting objects of types and styles unseen in finetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our best-performing models closely approach the heavily supervised SAM when evaluated on unseen object types and styles, and outperform it when segmenting fine structures and ambiguous boundaries. In contrast, existing promptable segmentation architectures or discriminatively pretrained models fail to generalize. This suggests that generative models learn an inherent grouping mechanism that transfers across categories and domains, even without internet-scale pretraining. Code, pretrained models, and demos are available on our website.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-based Autonomous Oversteer Control and Collision Avoidance</title>
<link>https://arxiv.org/abs/2505.15275</link>
<guid>https://arxiv.org/abs/2505.15275</guid>
<content:encoded><![CDATA[
arXiv:2505.15275v1 Announce Type: cross 
Abstract: Oversteer, wherein a vehicle's rear tires lose traction and induce unintentional excessive yaw, poses critical safety challenges. Failing to control oversteer often leads to severe traffic accidents. Although recent autonomous driving efforts have attempted to handle oversteer through stabilizing maneuvers, the majority rely on expert-defined trajectories or assume obstacle-free environments, limiting real-world applicability. This paper introduces a novel end-to-end (E2E) autonomous driving approach that tackles oversteer control and collision avoidance simultaneously. Existing E2E techniques, including Imitation Learning (IL), Reinforcement Learning (RL), and Hybrid Learning (HL), generally require near-optimal demonstrations or extensive experience. Yet even skilled human drivers struggle to provide perfect demonstrations under oversteer, and high transition variance hinders accumulating sufficient data. Hence, we present Q-Compared Soft Actor-Critic (QC-SAC), a new HL algorithm that effectively learns from suboptimal demonstration data and adapts rapidly to new conditions. To evaluate QC-SAC, we introduce a benchmark inspired by real-world driver training: a vehicle encounters sudden oversteer on a slippery surface and must avoid randomly placed obstacles ahead. Experimental results show QC-SAC attains near-optimal driving policies, significantly surpassing state-of-the-art IL, RL, and HL baselines. Our method demonstrates the world's first safe autonomous oversteer control with obstacle avoidance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Testing in Markov Decision Processes</title>
<link>https://arxiv.org/abs/2505.15342</link>
<guid>https://arxiv.org/abs/2505.15342</guid>
<content:encoded><![CDATA[
arXiv:2505.15342v1 Announce Type: cross 
Abstract: We study the policy testing problem in discounted Markov decision processes (MDPs) under the fixed-confidence setting. The goal is to determine whether the value of a given policy exceeds a specified threshold while minimizing the number of observations. We begin by deriving an instance-specific lower bound that any algorithm must satisfy. This lower bound is characterized as the solution to an optimization problem with non-convex constraints. We propose a policy testing algorithm inspired by this optimization problem--a common approach in pure exploration problems such as best-arm identification, where asymptotically optimal algorithms often stem from such optimization-based characterizations. As for other pure exploration tasks in MDPs, however, the non-convex constraints in the lower-bound problem present significant challenges, raising doubts about whether statistically optimal and computationally tractable algorithms can be designed. To address this, we reformulate the lower-bound problem by interchanging the roles of the objective and the constraints, yielding an alternative problem with a non-convex objective but convex constraints. Strikingly, this reformulated problem admits an interpretation as a policy optimization task in a newly constructed reversed MDP. Leveraging recent advances in policy gradient methods, we efficiently solve this problem and use it to design a policy testing algorithm that is statistically optimal--matching the instance-specific lower bound on sample complexity--while remaining computationally tractable. We validate our approach with numerical experiments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Super Emotion Dataset</title>
<link>https://arxiv.org/abs/2505.15348</link>
<guid>https://arxiv.org/abs/2505.15348</guid>
<content:encoded><![CDATA[
arXiv:2505.15348v1 Announce Type: cross 
Abstract: Despite the wide-scale usage and development of emotion classification datasets in NLP, the field lacks a standardized, large-scale resource that follows a psychologically grounded taxonomy. Existing datasets either use inconsistent emotion categories, suffer from limited sample size, or focus on specific domains. The Super Emotion Dataset addresses this gap by harmonizing diverse text sources into a unified framework based on Shaver's empirically validated emotion taxonomy, enabling more consistent cross-domain emotion recognition research.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Phone Pairs from MEG Signals Across Speech Modalities</title>
<link>https://arxiv.org/abs/2505.15355</link>
<guid>https://arxiv.org/abs/2505.15355</guid>
<content:encoded><![CDATA[
arXiv:2505.15355v1 Announce Type: cross 
Abstract: Understanding the neural mechanisms underlying speech production is essential for both advancing cognitive neuroscience theory and developing practical communication technologies. In this study, we investigated magnetoencephalography signals to decode phones from brain activity during speech production and perception (passive listening and voice playback) tasks. Using a dataset comprising 17 participants, we performed pairwise phone classification, extending our analysis to 15 phonetic pairs. Multiple machine learning approaches, including regularized linear models and neural network architectures, were compared to determine their effectiveness in decoding phonetic information. Our results demonstrate significantly higher decoding accuracy during speech production (76.6%) compared to passive listening and playback modalities (~51%), emphasizing the richer neural information available during overt speech. Among the models, the Elastic Net classifier consistently outperformed more complex neural networks, highlighting the effectiveness of traditional regularization techniques when applied to limited and high-dimensional MEG datasets. Besides, analysis of specific brain frequency bands revealed that low-frequency oscillations, particularly Delta (0.2-3 Hz) and Theta (4-7 Hz), contributed the most substantially to decoding accuracy, suggesting that these bands encode critical speech production-related neural processes. Despite using advanced denoising methods, it remains unclear whether decoding solely reflects neural activity or if residual muscular or movement artifacts also contributed, indicating the need for further methodological refinement. Overall, our findings underline the critical importance of examining overt speech production paradigms, which, despite their complexity, offer opportunities to improve brain-computer interfaces to help individuals with severe speech impairments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inter-Subject Variance Transfer Learning for EMG Pattern Classification Based on Bayesian Inference</title>
<link>https://arxiv.org/abs/2505.15381</link>
<guid>https://arxiv.org/abs/2505.15381</guid>
<content:encoded><![CDATA[
arXiv:2505.15381v1 Announce Type: cross 
Abstract: In electromyogram (EMG)-based motion recognition, a subject-specific classifier is typically trained with sufficient labeled data. However, this process demands extensive data collection over extended periods, burdening the subject. To address this, utilizing information from pre-training on multiple subjects for the training of the target subject could be beneficial. This paper proposes an inter-subject variance transfer learning method based on a Bayesian approach. This method is founded on the simple hypothesis that while the means of EMG features vary greatly across subjects, their variances may exhibit similar patterns. Our approach transfers variance information, acquired through pre-training on multiple source subjects, to a target subject within a Bayesian updating framework, thereby allowing accurate classification using limited target calibration data. A coefficient was also introduced to adjust the amount of information transferred for efficient transfer learning. Experimental evaluations using two EMG datasets demonstrated the effectiveness of our variance transfer strategy and its superiority compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Multimodal Learning via Entropy-Gated Contrastive Fusion</title>
<link>https://arxiv.org/abs/2505.15417</link>
<guid>https://arxiv.org/abs/2505.15417</guid>
<content:encoded><![CDATA[
arXiv:2505.15417v1 Announce Type: cross 
Abstract: Real-world multimodal systems routinely face missing-input scenarios, and in reality, robots lose audio in a factory or a clinical record omits lab tests at inference time. Standard fusion layers either preserve robustness or calibration but never both. We introduce Adaptive Entropy-Gated Contrastive Fusion (AECF), a single light-weight layer that (i) adapts its entropy coefficient per instance, (ii) enforces monotone calibration across all modality subsets, and (iii) drives a curriculum mask directly from training-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP by +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1% run-time. All back-bones remain frozen, making AECF an easy drop-in layer for robust, calibrated multimodal inference.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification in SVM prediction</title>
<link>https://arxiv.org/abs/2505.15429</link>
<guid>https://arxiv.org/abs/2505.15429</guid>
<content:encoded><![CDATA[
arXiv:2505.15429v1 Announce Type: cross 
Abstract: This paper explores Uncertainty Quantification (UQ) in SVM predictions, particularly for regression and forecasting tasks. Unlike the Neural Network, the SVM solutions are typically more stable, sparse, optimal and interpretable. However, there are only few literature which addresses the UQ in SVM prediction. At first, we provide a comprehensive summary of existing Prediction Interval (PI) estimation and probabilistic forecasting methods developed in the SVM framework and evaluate them against the key properties expected from an ideal PI model. We find that none of the existing SVM PI models achieves a sparse solution. To introduce sparsity in SVM model, we propose the Sparse Support Vector Quantile Regression (SSVQR) model, which constructs PIs and probabilistic forecasts by solving a pair of linear programs. Further, we develop a feature selection algorithm for PI estimation using SSVQR that effectively eliminates a significant number of features while improving PI quality in case of high-dimensional dataset. Finally we extend the SVM models in Conformal Regression setting for obtaining more stable prediction set with finite test set guarantees. Extensive experiments on artificial, real-world benchmark datasets compare the different characteristics of both existing and proposed SVM-based PI estimation methods and also highlight the advantages of the feature selection in PI estimation. Furthermore, we compare both, the existing and proposed SVM-based PI estimation models, with modern deep learning models for probabilistic forecasting tasks on benchmark datasets. Furthermore, SVM models show comparable or superior performance to modern complex deep learning models for probabilistic forecasting task in our experiments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Temperature Scaling with Conformal Prediction</title>
<link>https://arxiv.org/abs/2505.15437</link>
<guid>https://arxiv.org/abs/2505.15437</guid>
<content:encoded><![CDATA[
arXiv:2505.15437v1 Announce Type: cross 
Abstract: Conformal prediction enables the construction of high-coverage prediction sets for any pre-trained model, guaranteeing that the true label lies within the set with a specified probability. However, these sets do not provide probability estimates for individual labels, limiting their practical use. In this paper, we propose, to the best of our knowledge, the first method for assigning calibrated probabilities to elements of a conformal prediction set. Our approach frames this as an adaptive calibration problem, selecting an input-specific temperature parameter to match the desired coverage level. Experiments on several challenging image classification datasets demonstrate that our method maintains coverage guarantees while significantly reducing expected calibration error.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stronger ViTs With Octic Equivariance</title>
<link>https://arxiv.org/abs/2505.15441</link>
<guid>https://arxiv.org/abs/2505.15441</guid>
<content:encoded><![CDATA[
arXiv:2505.15441v1 Announce Type: cross 
Abstract: Recent efforts at scaling computer vision models have established Vision Transformers (ViTs) as the leading architecture. ViTs incorporate weight sharing over image patches as an important inductive bias. In this work, we show that ViTs benefit from incorporating equivariance under the octic group, i.e., reflections and 90-degree rotations, as a further inductive bias. We develop new architectures, octic ViTs, that use octic-equivariant layers and put them to the test on both supervised and self-supervised learning. Through extensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show that octic ViTs yield more computationally efficient networks while also improving performance. In particular, we achieve approximately 40% reduction in FLOPs for ViT-H while simultaneously improving both classification and segmentation results.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-based Decision Support System for Heritage Aircraft Corrosion Prevention</title>
<link>https://arxiv.org/abs/2505.15462</link>
<guid>https://arxiv.org/abs/2505.15462</guid>
<content:encoded><![CDATA[
arXiv:2505.15462v1 Announce Type: cross 
Abstract: The paper presents a decision support system for the long-term preservation of aeronautical heritage exhibited/stored in sheltered sites. The aeronautical heritage is characterized by diverse materials of which this heritage is constituted. Heritage aircraft are made of ancient aluminum alloys, (ply)wood, and particularly fabrics. The decision support system (DSS) designed, starting from a conceptual model, is knowledge-based on degradation/corrosion mechanisms of prevailing materials of aeronautical heritage. In the case of historical aircraft wooden parts, this knowledge base is filled in by the damage function models developed within former European projects. Model-based corrosion prediction is implemented within the new DSS for ancient aluminum alloys. The novelty of this DSS consists of supporting multi-material heritage protection and tailoring to peculiarities of aircraft exhibition/storage hangars and the needs of aviation museums. The novel DSS is tested on WWII aircraft heritage exhibited in the Aviation Museum Kbely, Military History Institute Prague, Czech Republic.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Derived Blood Input for Dynamic PET Images of Rat Heart</title>
<link>https://arxiv.org/abs/2505.15488</link>
<guid>https://arxiv.org/abs/2505.15488</guid>
<content:encoded><![CDATA[
arXiv:2505.15488v1 Announce Type: cross 
Abstract: Dynamic FDG PET imaging study of n = 52 rats including 26 control Wistar-Kyoto (WKY) rats and 26 experimental spontaneously hypertensive rats (SHR) were performed using a Siemens microPET and Albira trimodal scanner longitudinally at 1, 2, 3, 5, 9, 12 and 18 months of age. A 15-parameter dual output model correcting for spill over contamination and partial volume effects with peak fitting cost functions was developed for simultaneous estimation of model corrected blood input function (MCIF) and kinetic rate constants for dynamic FDG PET images of rat heart in vivo. Major drawbacks of this model are its dependence on manual annotations for the Image Derived Input Function (IDIF) and manual determination of crucial model parameters to compute MCIF. To overcome these limitations, we performed semi-automated segmentation and then formulated a Long-Short-Term Memory (LSTM) cell network to train and predict MCIF in test data using a concatenation of IDIFs and myocardial inputs and compared them with reference-modeled MCIF. Thresholding along 2D plane slices with two thresholds, with T1 representing high-intensity myocardium, and T2 representing lower-intensity rings, was used to segment the area of the LV blood pool. The resultant IDIF and myocardial TACs were used to compute the corresponding reference (model) MCIF for all data sets. The segmented IDIF and the myocardium formed the input for the LSTM network. A k-fold cross validation structure with a 33:8:11 split and 5 folds was utilized to create the model and evaluate the performance of the LSTM network for all datasets. To overcome the sparseness of data as time steps increase, midpoint interpolation was utilized to increase the density of datapoints beyond time = 10 minutes. The model utilizing midpoint interpolation was able to achieve a 56.4% improvement over previous Mean Squared Error (MSE).
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coloring Between the Lines: Personalization in the Null Space of Planning Constraints</title>
<link>https://arxiv.org/abs/2505.15503</link>
<guid>https://arxiv.org/abs/2505.15503</guid>
<content:encoded><![CDATA[
arXiv:2505.15503v1 Announce Type: cross 
Abstract: Generalist robots must personalize in-the-wild to meet the diverse needs and preferences of long-term users. How can we enable flexible personalization without sacrificing safety or competency? This paper proposes Coloring Between the Lines (CBTL), a method for personalization that exploits the null space of constraint satisfaction problems (CSPs) used in robot planning. CBTL begins with a CSP generator that ensures safe and competent behavior, then incrementally personalizes behavior by learning parameterized constraints from online interaction. By quantifying uncertainty and leveraging the compositionality of planning constraints, CBTL achieves sample-efficient adaptation without environment resets. We evaluate CBTL in (1) three diverse simulation environments; (2) a web-based user study; and (3) a real-robot assisted feeding system, finding that CBTL consistently achieves more effective personalization with fewer interactions than baselines. Our results demonstrate that CBTL provides a unified and practical approach for continual, flexible, active, and safe robot personalization. Website: https://emprise.cs.cornell.edu/cbtl/
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts</title>
<link>https://arxiv.org/abs/2505.15506</link>
<guid>https://arxiv.org/abs/2505.15506</guid>
<content:encoded><![CDATA[
arXiv:2505.15506v1 Announce Type: cross 
Abstract: Recently, Vision-Language foundation models like CLIP and ALIGN, which are pre-trained on large-scale data have shown remarkable zero-shot generalization to diverse datasets with different classes and even domains. In this work, we take a step further and analyze whether these models can be adapted to target datasets having very different distributions and classes compared to what these models have been trained on, using only a few labeled examples from the target dataset. In such scenarios, finetuning large pretrained models is challenging due to problems of overfitting as well as loss of generalization, and has not been well explored in prior literature. Since, the pre-training data of such models are unavailable, it is difficult to comprehend the performance on various downstream datasets. First, we try to answer the question: Given a target dataset with a few labelled examples, can we estimate whether further fine-tuning can enhance the performance compared to zero-shot evaluation? by analyzing the common vision-language embedding space. Based on the analysis, we propose a novel prompt-tuning method, PromptMargin for adapting such large-scale VLMs directly on the few target samples. PromptMargin effectively tunes the text as well as visual prompts for this task, and has two main modules: 1) Firstly, we use a selective augmentation strategy to complement the few training samples in each task; 2) Additionally, to ensure robust training in the presence of unfamiliar class names, we increase the inter-class margin for improved class discrimination using a novel Multimodal Margin Regularizer. Extensive experiments and analysis across fifteen target benchmark datasets, with varying degrees of distribution shifts from natural images, shows the effectiveness of the proposed framework over the existing state-of-the-art approaches applied to this setting. github.com/debarshigit/PromptMargin.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets</title>
<link>https://arxiv.org/abs/2505.15517</link>
<guid>https://arxiv.org/abs/2505.15517</guid>
<content:encoded><![CDATA[
arXiv:2505.15517v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm - using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual Question Answering (VQA) dataset generation framework for VLMs. Given a human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries - images with textural multiple-choice questions - based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Jump Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.15557</link>
<guid>https://arxiv.org/abs/2505.15557</guid>
<content:encoded><![CDATA[
arXiv:2505.15557v1 Announce Type: cross 
Abstract: Gaussian processes (GPs) furnish accurate nonlinear predictions with well-calibrated uncertainty. However, the typical GP setup has a built-in stationarity assumption, making it ill-suited for modeling data from processes with sudden changes, or "jumps" in the output variable. The "jump GP" (JGP) was developed for modeling data from such processes, combining local GPs and latent "level" variables under a joint inferential framework. But joint modeling can be fraught with difficulty. We aim to simplify by suggesting a more modular setup, eschewing joint inference but retaining the main JGP themes: (a) learning optimal neighborhood sizes that locally respect manifolds of discontinuity; and (b) a new cluster-based (latent) feature to capture regions of distinct output levels on both sides of the manifold. We show that each of (a) and (b) separately leads to dramatic improvements when modeling processes with jumps. In tandem (but without requiring joint inference) that benefit is compounded, as illustrated on real and synthetic benchmark examples from the recent literature.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robo-DM: Data Management For Large Robot Datasets</title>
<link>https://arxiv.org/abs/2505.15558</link>
<guid>https://arxiv.org/abs/2505.15558</guid>
<content:encoded><![CDATA[
arXiv:2505.15558v1 Announce Type: cross 
Abstract: Recent results suggest that very large datasets of teleoperated robot demonstrations can be used to train transformer-based models that have the potential to generalize to new scenes, robots, and tasks. However, curating, distributing, and loading large datasets of robot trajectories, which typically consist of video, textual, and numerical modalities - including streams from multiple cameras - remains challenging. We propose Robo-DM, an efficient open-source cloud-based data management toolkit for collecting, sharing, and learning with robot data. With Robo-DM, robot datasets are stored in a self-contained format with Extensible Binary Meta Language (EBML). Robo-DM can significantly reduce the size of robot trajectory data, transfer costs, and data load time during training. Compared to the RLDS format used in OXE datasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x (lossless). Robo-DM also accelerates data retrieval by load-balancing video decoding with memory-mapped decoding caches. Compared to LeRobot, a framework that also uses lossy video compression, Robo-DM is up to 50x faster when decoding sequentially. We physically evaluate a model trained by Robo-DM with lossy compression, a pick-and-place task, and In-Context Robot Transformer. Robo-DM uses 75x compression of the original dataset and does not suffer reduction in downstream task accuracy.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.15576</link>
<guid>https://arxiv.org/abs/2505.15576</guid>
<content:encoded><![CDATA[
arXiv:2505.15576v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) are essential for multimodal tasks, especially compositional reasoning (CR) tasks, which require distinguishing fine-grained semantic differences between visual and textual embeddings. However, existing methods primarily fine-tune the model by generating text-based hard negative samples, neglecting the importance of image-based negative samples, which results in insufficient training of the visual encoder and ultimately impacts the overall performance of the model. Moreover, negative samples are typically treated uniformly, without considering their difficulty levels, and the alignment of positive samples is insufficient, which leads to challenges in aligning difficult sample pairs. To address these issues, we propose Adaptive Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard negatives into the visual domain to generate semantically disturbed image-based negatives for training the model, thereby enhancing its overall performance. AHNPL also introduces a contrastive learning approach using a multimodal hard negative loss to improve the model's discrimination of hard negatives within each modality and a dynamic margin loss that adjusts the contrastive margin according to sample difficulty to enhance the distinction of challenging sample pairs. Experiments on three public datasets demonstrate that our method effectively boosts VLMs' performance on complex CR tasks. The source code is available at https://github.com/nynu-BDAI/AHNPL.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRB: Mathematical Information Retrieval Benchmark</title>
<link>https://arxiv.org/abs/2505.15585</link>
<guid>https://arxiv.org/abs/2505.15585</guid>
<content:encoded><![CDATA[
arXiv:2505.15585v1 Announce Type: cross 
Abstract: Mathematical Information Retrieval (MIR) is the task of retrieving information from mathematical documents and plays a key role in various applications, including theorem search in mathematical libraries, answer retrieval on math forums, and premise selection in automated theorem proving. However, a unified benchmark for evaluating these diverse retrieval tasks has been lacking. In this paper, we introduce MIRB (Mathematical Information Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB includes four tasks: semantic statement retrieval, question-answer retrieval, premise retrieval, and formula retrieval, spanning a total of 12 datasets. We evaluate 13 retrieval models on this benchmark and analyze the challenges inherent to MIR. We hope that MIRB provides a comprehensive framework for evaluating MIR systems and helps advance the development of more effective retrieval models tailored to the mathematical domain.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Reason Efficiently with Adaptive Length-based Reward Shaping</title>
<link>https://arxiv.org/abs/2505.15612</link>
<guid>https://arxiv.org/abs/2505.15612</guid>
<content:encoded><![CDATA[
arXiv:2505.15612v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper, we investigate RL-based approaches to promote reasoning efficiency. Specifically, we first present a unified framework that formulates various efficient reasoning methods through the lens of length-based reward shaping. Building on this perspective, we propose a novel Length-bAsed StEp Reward shaping method (LASER), which employs a step function as the reward, controlled by a target length. LASER surpasses previous methods, achieving a superior Pareto-optimal balance between performance and efficiency. Next, we further extend LASER based on two key intuitions: (1) The reasoning behavior of the model evolves during training, necessitating reward specifications that are also adaptive and dynamic; (2) Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. This approach is expected to facilitate a combination of fast and slow thinking, leading to a better overall tradeoff. The resulting method is termed LASER-D (Dynamic and Difficulty-aware). Experiments on DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both reasoning performance and response length efficiency. For instance, LASER-D and its variant achieve a +6.1 improvement on AIME2024 while reducing token usage by 63%. Further analysis reveals our RL-based compression produces more concise reasoning patterns with less redundant "self-reflections". Resources are at https://github.com/hkust-nlp/Laser.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.15623</link>
<guid>https://arxiv.org/abs/2505.15623</guid>
<content:encoded><![CDATA[
arXiv:2505.15623v1 Announce Type: cross 
Abstract: Large language models (LLMs) demonstrate considerable potential in various natural language tasks but face significant challenges in mathematical reasoning, particularly in executing precise, multi-step logic. However, current evaluation frameworks judge their performance solely based on accuracy, which only accounts for the final answer. This study explores these pitfalls by employing a novel evaluation framework. We propose an evaluation metric called the MAPLE score, which holistically quantifies reasoning misalignment by integrating error rates, redundancy, and validity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions</title>
<link>https://arxiv.org/abs/2505.15633</link>
<guid>https://arxiv.org/abs/2505.15633</guid>
<content:encoded><![CDATA[
arXiv:2505.15633v1 Announce Type: cross 
Abstract: Large language models that use retrieval augmented generation have the potential to unlock valuable knowledge for researchers, policymakers, and the public by making long and technical climate-related documents more accessible. While this approach can help alleviate factual hallucinations by relying on retrieved passages as additional context, its effectiveness depends on whether the model's output remains faithful to these passages. To address this, we explore the automatic assessment of faithfulness of different models in this setting. We then focus on ClimateGPT, a large language model specialised in climate science, to examine which factors in its instruction fine-tuning impact the model's faithfulness. By excluding unfaithful subsets of the model's training data, we develop ClimateGPT Faithful+, which achieves an improvement in faithfulness from 30% to 57% in supported atomic claims according to our automatic metric.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2505.15634</link>
<guid>https://arxiv.org/abs/2505.15634</guid>
<content:encoded><![CDATA[
arXiv:2505.15634v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate the ability to solve reasoning and mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT length, as seen in models such as DeepSeek-R1, significantly enhances this reasoning for complex problems, but requires costly and high-quality long CoT data and fine-tuning. This work, inspired by the deep thinking paradigm of DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of an LLM without external datasets. Our method first employs Sparse Autoencoders (SAEs) to extract interpretable features from vanilla CoT. These features are then used to steer the LLM's internal states during generation. Recognizing that many LLMs do not have corresponding pre-trained SAEs, we further introduce a novel SAE-free steering algorithm, which directly computes steering directions from the residual activations of an LLM, obviating the need for an explicit SAE. Experimental results demonstrate that both our SAE-based and subsequent SAE-free steering algorithms significantly enhance the reasoning capabilities of LLMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest Neighbor Search</title>
<link>https://arxiv.org/abs/2505.15636</link>
<guid>https://arxiv.org/abs/2505.15636</guid>
<content:encoded><![CDATA[
arXiv:2505.15636v1 Announce Type: cross 
Abstract: Nearest neighbor search is central in machine learning, information retrieval, and databases. For high-dimensional datasets, graph-based methods such as HNSW, DiskANN, and NSG have become popular thanks to their empirical accuracy and efficiency. These methods construct a directed graph over the dataset and perform beam search on the graph to find nodes close to a given query. While significant work has focused on practical refinements and theoretical understanding of graph-based methods, many questions remain. We propose a new distance-based termination condition for beam search to replace the commonly used condition based on beam width. We prove that, as long as the search graph is navigable, our resulting Adaptive Beam Search method is guaranteed to approximately solve the nearest-neighbor problem, establishing a connection between navigability and the performance of graph-based search. We also provide extensive experiments on our new termination condition for both navigable graphs and approximately navigable graphs used in practice, such as HNSW and Vamana graphs. We find that Adaptive Beam Search outperforms standard beam search over a range of recall values, data sets, graph constructions, and target number of nearest neighbors. It thus provides a simple and practical way to improve the performance of popular methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Simple Approximation Algorithm for Optimal Decision Tree</title>
<link>https://arxiv.org/abs/2505.15641</link>
<guid>https://arxiv.org/abs/2505.15641</guid>
<content:encoded><![CDATA[
arXiv:2505.15641v1 Announce Type: cross 
Abstract: Optimal decision tree (\odt) is a fundamental problem arising in applications such as active learning, entity identification, and medical diagnosis. An instance of \odt is given by $m$ hypotheses, out of which an unknown ``true'' hypothesis is drawn according to some probability distribution. An algorithm needs to identify the true hypothesis by making queries: each query incurs a cost and has a known response for each hypothesis. The goal is to minimize the expected query cost to identify the true hypothesis. We consider the most general setting with arbitrary costs, probabilities and responses. \odt is NP-hard to approximate better than $\ln m$ and there are $O(\ln m)$ approximation algorithms known for it. However, these algorithms and/or their analyses are quite complex. Moreover, the leading constant factors are large. We provide a simple algorithm and analysis for \odt, proving an approximation ratio of $8 \ln m$.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLARE: Robot Learning with Implicit World Modeling</title>
<link>https://arxiv.org/abs/2505.15659</link>
<guid>https://arxiv.org/abs/2505.15659</guid>
<content:encoded><![CDATA[
arXiv:2505.15659v1 Announce Type: cross 
Abstract: We introduce $\textbf{F}$uture $\textbf{LA}$tent $\textbf{RE}$presentation Alignment ($\textbf{FLARE}$), a novel framework that integrates predictive latent world modeling into robot policy learning. By aligning features from a diffusion transformer with latent embeddings of future observations, $\textbf{FLARE}$ enables a diffusion transformer policy to anticipate latent representations of future observations, allowing it to reason about long-term consequences while generating actions. Remarkably lightweight, $\textbf{FLARE}$ requires only minimal architectural modifications -- adding a few tokens to standard vision-language-action (VLA) models -- yet delivers substantial performance gains. Across two challenging multitask simulation imitation learning benchmarks spanning single-arm and humanoid tabletop manipulation, $\textbf{FLARE}$ achieves state-of-the-art performance, outperforming prior policy learning baselines by up to 26%. Moreover, $\textbf{FLARE}$ unlocks the ability to co-train with human egocentric video demonstrations without action labels, significantly boosting policy generalization to a novel object with unseen geometry with as few as a single robot demonstration. Our results establish $\textbf{FLARE}$ as a general and scalable approach for combining implicit world modeling with high-frequency robotic control.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities</title>
<link>https://arxiv.org/abs/2505.15692</link>
<guid>https://arxiv.org/abs/2505.15692</guid>
<content:encoded><![CDATA[
arXiv:2505.15692v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the model's output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance ("thought patterns"). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPO's potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation</title>
<link>https://arxiv.org/abs/2505.15696</link>
<guid>https://arxiv.org/abs/2505.15696</guid>
<content:encoded><![CDATA[
arXiv:2505.15696v1 Announce Type: cross 
Abstract: The [CLS] token in BERT is commonly used as a fixed-length representation for classification tasks, yet prior work has shown that both other tokens and intermediate layers encode valuable contextual information. In this work, we propose MaxPoolBERT, a lightweight extension to BERT that refines the [CLS] representation by aggregating information across layers and tokens. Specifically, we explore three modifications: (i) max-pooling the [CLS] token across multiple layers, (ii) enabling the [CLS] token to attend over the entire final layer using an additional multi-head attention (MHA) layer, and (iii) combining max-pooling across the full sequence with MHA. Our approach enhances BERT's classification accuracy (especially on low-resource tasks) without requiring pre-training or significantly increasing model size. Experiments on the GLUE benchmark show that MaxPoolBERT consistently achieves a better performance on the standard BERT-base model.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases</title>
<link>https://arxiv.org/abs/2505.15701</link>
<guid>https://arxiv.org/abs/2505.15701</guid>
<content:encoded><![CDATA[
arXiv:2505.15701v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated their potential in hardware design tasks, such as Hardware Description Language (HDL) generation and debugging. Yet, their performance in real-world, repository-level HDL projects with thousands or even tens of thousands of code lines is hindered. To this end, we propose HDLxGraph, a novel framework that integrates Graph Retrieval Augmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph representations by incorporating Abstract Syntax Trees (ASTs) and Data Flow Graphs (DFGs) to capture both code graph view and hardware graph view. HDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the limited recall issues inherent in similarity-based semantic retrieval by incorporating structural information, but also enhances its extensibility to various real-world tasks by a task-specific retrieval finetuning. Additionally, to address the lack of comprehensive HDL search benchmarks, we introduce HDLSearch, a multi-granularity evaluation dataset derived from real-world repository-level projects. Experimental results demonstrate that HDLxGraph significantly improves average search accuracy, debugging efficiency and completion quality by 12.04%, 12.22% and 5.04% compared to similarity-based RAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are available at https://github.com/Nick-Zheng-Q/HDLxGraph.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing LLM Safe Alignment with Safety Representation Ranking</title>
<link>https://arxiv.org/abs/2505.15710</link>
<guid>https://arxiv.org/abs/2505.15710</guid>
<content:encoded><![CDATA[
arXiv:2505.15710v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) has demonstrated milestone success in a variety of tasks, yet their potential for generating harmful content has raised significant safety concerns. Existing safety evaluation approaches typically operate directly on textual responses, overlooking the rich information embedded in the model's internal representations. In this paper, we propose Safety Representation Ranking (SRR), a listwise ranking framework that selects safe responses using hidden states from the LLM itself. SRR encodes both instructions and candidate completions using intermediate transformer representations and ranks candidates via a lightweight similarity-based scorer. Our approach directly leverages internal model states and supervision at the list level to capture subtle safety signals. Experiments across multiple benchmarks show that SRR significantly improves robustness to adversarial prompts. Our code will be available upon publication.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are machine learning interpretations reliable? A stability study on global interpretations</title>
<link>https://arxiv.org/abs/2505.15728</link>
<guid>https://arxiv.org/abs/2505.15728</guid>
<content:encoded><![CDATA[
arXiv:2505.15728v1 Announce Type: cross 
Abstract: As machine learning systems are increasingly used in high-stakes domains, there is a growing emphasis placed on making them interpretable to improve trust in these systems. In response, a range of interpretable machine learning (IML) methods have been developed to generate human-understandable insights into otherwise black box models. With these methods, a fundamental question arises: Are these interpretations reliable? Unlike with prediction accuracy or other evaluation metrics for supervised models, the proximity to the true interpretation is difficult to define. Instead, we ask a closely related question that we argue is a prerequisite for reliability: Are these interpretations stable? We define stability as findings that are consistent or reliable under small random perturbations to the data or algorithms. In this study, we conduct the first systematic, large-scale empirical stability study on popular machine learning global interpretations for both supervised and unsupervised tasks on tabular data. Our findings reveal that popular interpretation methods are frequently unstable, notably less stable than the predictions themselves, and that there is no association between the accuracy of machine learning predictions and the stability of their associated interpretations. Moreover, we show that no single method consistently provides the most stable interpretations across a range of benchmark datasets. Overall, these results suggest that interpretability alone does not warrant trust, and underscores the need for rigorous evaluation of interpretation stability in future work. To support these principles, we have developed and released an open source IML dashboard and Python package to enable researchers to assess the stability and reliability of their own data-driven interpretations and discoveries.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.15734</link>
<guid>https://arxiv.org/abs/2505.15734</guid>
<content:encoded><![CDATA[
arXiv:2505.15734v1 Announce Type: cross 
Abstract: Large language models (LLMs) have improved significantly in their reasoning through extensive training on massive datasets. However, relying solely on additional data for improvement is becoming increasingly impractical, highlighting the need for models to autonomously enhance their reasoning without external supervision. In this paper, we propose Debate, Train, Evolve (DTE), a novel ground truth-free training framework that uses multi-agent debate traces to evolve a single language model. We also introduce a new prompting strategy Reflect-Critique-Refine, to improve debate quality by explicitly instructing agents to critique and refine their reasoning. Extensive evaluations on five reasoning benchmarks with six open-weight models show that our DTE framework achieve substantial improvements, with an average accuracy gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe strong cross-domain generalization, with an average accuracy gain of 5.8% on all other benchmarks, suggesting that our method captures general reasoning capabilities.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses</title>
<link>https://arxiv.org/abs/2505.15738</link>
<guid>https://arxiv.org/abs/2505.15738</guid>
<content:encoded><![CDATA[
arXiv:2505.15738v1 Announce Type: cross 
Abstract: Large language models (LLMs) are rapidly deployed in real-world applications ranging from chatbots to agentic systems. Alignment is one of the main approaches used to defend against attacks such as prompt injection and jailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even against Greedy Coordinate Gradient (GCG), a white-box attack that generates adversarial suffixes to induce attacker-desired outputs. However, this search space over discrete tokens is extremely large, making the task of finding successful attacks difficult. GCG has, for instance, been shown to converge to local minima, making it sensitive to initialization choices. In this paper, we assess the future-proof robustness of these defenses using a more informed threat model: attackers who have access to some information about the alignment process. Specifically, we propose an informed white-box attack leveraging the intermediate model checkpoints to initialize GCG, with each checkpoint acting as a stepping stone for the next one. We show this approach to be highly effective across state-of-the-art (SOTA) defenses and models. We further show our informed initialization to outperform other initialization methods and show a gradient-informed checkpoint selection strategy to greatly improve attack performance and efficiency. Importantly, we also show our method to successfully find universal adversarial suffixes -- single suffixes effective across diverse inputs. Our results show that, contrary to previous beliefs, effective adversarial suffixes do exist against SOTA alignment-based defenses, that these can be found by existing attack methods when adversaries exploit alignment knowledge, and that even universal suffixes exist. Taken together, our results highlight the brittleness of current alignment-based methods and the need to consider stronger threat models when testing the safety of LLMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Argumentative Learning with Case-Based Reasoning</title>
<link>https://arxiv.org/abs/2505.15742</link>
<guid>https://arxiv.org/abs/2505.15742</guid>
<content:encoded><![CDATA[
arXiv:2505.15742v1 Announce Type: cross 
Abstract: We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual AA-CBR), a data-driven, neurosymbolic classification model in which the outcome is determined by an argumentation debate structure that is learned simultaneously with neural-based feature extractors. Each argument in the debate is an observed case from the training data, favouring their labelling. Cases attack or support those with opposing or agreeing labellings, with the strength of each argument and relationship learned through gradient-based methods. This argumentation debate structure provides human-aligned reasoning, improving model interpretability compared to traditional neural networks (NNs). Unlike the existing purely symbolic variant, Abstract Argumentation for Case-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class classification, automatic learning of feature and data point importance, assigning uncertainty values to outcomes, using all available data points, and does not require binary features. We show that Gradual AA-CBR performs comparably to NNs whilst significantly outperforming existing AA-CBR formulations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval</title>
<link>https://arxiv.org/abs/2505.15753</link>
<guid>https://arxiv.org/abs/2505.15753</guid>
<content:encoded><![CDATA[
arXiv:2505.15753v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are known to be vulnerable to jailbreaking attacks, wherein adversaries exploit carefully engineered prompts to induce harmful or unethical responses. Such threats have raised critical concerns about the safety and reliability of LLMs in real-world deployment. While existing defense mechanisms partially mitigate such risks, subsequent advancements in adversarial techniques have enabled novel jailbreaking methods to circumvent these protections, exposing the limitations of static defense frameworks. In this work, we explore defending against evolving jailbreaking threats through the lens of context retrieval. First, we conduct a preliminary study demonstrating that even a minimal set of safety-aligned examples against a particular jailbreak can significantly enhance robustness against this attack pattern. Building on this insight, we further leverage the retrieval-augmented generation (RAG) techniques and propose Safety Context Retrieval (SCR), a scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our comprehensive experiments demonstrate how SCR achieves superior defensive performance against both established and emerging jailbreaking tactics, contributing a new paradigm to LLM safety. Our code will be available upon publication.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer of Structural Knowledge from Synthetic Languages</title>
<link>https://arxiv.org/abs/2505.15769</link>
<guid>https://arxiv.org/abs/2505.15769</guid>
<content:encoded><![CDATA[
arXiv:2505.15769v1 Announce Type: cross 
Abstract: This work explores transfer learning from several synthetic languages to English. We investigate the structure of the embeddings in the fine-tuned models, the information they contain, and the capabilities of the fine-tuned models on simple linguistic tasks. We also introduce a new synthetic language that leads to better transfer to English than the languages used in previous research. Finally, we introduce Tiny-Cloze Benchmark - a new synthetic benchmark for natural language understanding that is more informative for less powerful models. We use Tiny-Cloze Benchmark to evaluate fine-tuned models in several domains demonstrating that fine-tuning on a new synthetic language allows for better performance on a variety of tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention</title>
<link>https://arxiv.org/abs/2505.15774</link>
<guid>https://arxiv.org/abs/2505.15774</guid>
<content:encoded><![CDATA[
arXiv:2505.15774v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) encounter significant challenges in long-sequence inference due to computational inefficiency and redundant processing, driving interest in context compression techniques. Existing methods often rely on token importance to perform hard local compression or encode context into latent representations for soft global compression. However, the uneven distribution of textual content relevance and the diversity of demands for user instructions mean these approaches frequently lead to the loss of potentially valuable information. To address this, we propose $\textbf{Hy}$brid $\textbf{Co}$ntext $\textbf{Co}$mpression (HyCo$_2$) for LLMs, which integrates both global and local perspectives to guide context compression while retaining both the essential semantics and critical details for task completion. Specifically, we employ a hybrid adapter to refine global semantics with the global view, based on the observation that different adapters excel at different tasks. Then we incorporate a classification layer that assigns a retention probability to each context token based on the local view, determining whether it should be retained or discarded. To foster a balanced integration of global and local compression, we introduce auxiliary paraphrasing and completion pretraining before instruction tuning. This promotes a synergistic integration that emphasizes instruction-relevant information while preserving essential local details, ultimately balancing local and global information retention in context compression. Experiments show that our HyCo$_2$ method significantly enhances long-text reasoning while reducing token usage. It improves the performance of various LLM series by an average of 13.1\% across seven knowledge-intensive QA benchmarks. Moreover, HyCo$_2$ matches the performance of uncompressed methods while reducing token consumption by 88.8\%.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL</title>
<link>https://arxiv.org/abs/2505.15791</link>
<guid>https://arxiv.org/abs/2505.15791</guid>
<content:encoded><![CDATA[
arXiv:2505.15791v1 Announce Type: cross 
Abstract: Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, efficient fine-tuning and support non-differentiable rewards. Furthermore, their reliance on sparse rewards provides inadequate supervision during intermediate steps, often resulting in suboptimal generation quality. To address these limitations, dense and differentiable signals are required throughout the diffusion process. Hence, we propose VAlue-based Reinforced Diffusion (VARD): a novel approach that first learns a value function predicting expection of rewards from intermediate states, and subsequently uses this value function with KL regularization to provide dense supervision throughout the generation process. Our method maintains proximity to the pretrained model while enabling effective and stable training via backpropagation. Experimental results demonstrate that our approach facilitates better trajectory guidance, improves training efficiency and extends the applicability of RL to diffusion models optimized for complex, non-differentiable reward functions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Form Information Alignment Evaluation Beyond Atomic Facts</title>
<link>https://arxiv.org/abs/2505.15792</link>
<guid>https://arxiv.org/abs/2505.15792</guid>
<content:encoded><![CDATA[
arXiv:2505.15792v1 Announce Type: cross 
Abstract: Information alignment evaluators are vital for various NLG evaluation tasks and trustworthy LLM deployment, reducing hallucinations and enhancing user trust. Current fine-grained methods, like FactScore, verify facts individually but neglect inter-fact dependencies, enabling subtle vulnerabilities. In this work, we introduce MontageLie, a challenging benchmark that constructs deceptive narratives by "montaging" truthful statements without introducing explicit hallucinations. We demonstrate that both coarse-grained LLM-based evaluators and current fine-grained frameworks are susceptible to this attack, with AUC-ROC scores falling below 65%. To enable more robust fine-grained evaluation, we propose DoveScore, a novel framework that jointly verifies factual accuracy and event-order consistency. By modeling inter-fact relationships, DoveScore outperforms existing fine-grained methods by over 8%, providing a more robust solution for long-form text alignment evaluation. Our code and datasets are available at https://github.com/dannalily/DoveScore.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.15793</link>
<guid>https://arxiv.org/abs/2505.15793</guid>
<content:encoded><![CDATA[
arXiv:2505.15793v1 Announce Type: cross 
Abstract: Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can enhance autonomous driving (AD) performance in complex scenarios. However, current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to hallucinations.Evaluations show that state-of-the-art LLM indicates a non-hallucination rate of only approximately 57.95% when assessed on essential driving-related tasks. Thus, in these methods, hallucinations from the LLM can directly jeopardize the performance of driving policies. This paper argues that maintaining relative independence between the LLM and the RL is vital for solving the hallucinations problem. Consequently, this paper is devoted to propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic hints for state augmentation and policy optimization to assist RL agent in motion planning, while the RL agent counteracts potential erroneous semantic indications through policy learning to achieve excellent driving performance. Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual Reinforcement Learning Motion Planner) architecture, which is designed that includes Augmented Semantic Representation Module to extend state space. Contextual Stability Anchor Module enhances the reliability of multi-critic weight hints by utilizing information from the knowledge base. Semantic Cache Module is employed to seamlessly integrate LLM low-frequency guidance with RL high-frequency control. Extensive experiments in CARLA validate HCRMP's strong overall driving performance. HCRMP achieves a task success rate of up to 80.3% under diverse driving conditions with different traffic densities. Under safety-critical driving conditions, HCRMP significantly reduces the collision rate by 11.4%, which effectively improves the driving performance in complex scenarios.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation</title>
<link>https://arxiv.org/abs/2505.15807</link>
<guid>https://arxiv.org/abs/2505.15807</guid>
<content:encoded><![CDATA[
arXiv:2505.15807v1 Announce Type: cross 
Abstract: Large language models are able to exploit in-context learning to access external knowledge beyond their training data through retrieval-augmentation. While promising, its inner workings remain unclear. In this work, we shed light on the mechanism of in-context retrieval augmentation for question answering by viewing a prompt as a composition of informational components. We propose an attribution-based method to identify specialized attention heads, revealing in-context heads that comprehend instructions and retrieve relevant contextual information, and parametric heads that store entities' relational knowledge. To better understand their roles, we extract function vectors and modify their attention weights to show how they can influence the answer generation process. Finally, we leverage the gained insights to trace the sources of knowledge used during inference, paving the way towards more safe and transparent language models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatially scalable recursive estimation of Gaussian process terrain maps using local basis functions</title>
<link>https://arxiv.org/abs/2210.09168</link>
<guid>https://arxiv.org/abs/2210.09168</guid>
<content:encoded><![CDATA[
arXiv:2210.09168v3 Announce Type: replace 
Abstract: When an agent, person, vehicle or robot is moving through an unknown environment without GNSS signals, online mapping of nonlinear terrains can be used to improve position estimates when the agent returns to a previously mapped area. Mapping algorithms using online Gaussian process (GP) regression are commonly integrated in algorithms for simultaneous localisation and mapping (SLAM). However, GP mapping algorithms have increasing computational demands as the mapped area expands relative to spatial field variations. This is due to the need for estimating an increasing amount of map parameters as the area of the map grows. Contrary to this, we propose a recursive GP mapping estimation algorithm which uses local basis functions in an information filter to achieve spatial scalability. Our proposed approximation employs a global grid of finite support basis functions but restricts computations to a localized subset around each prediction point. As our proposed algorithm is recursive, it can naturally be incorporated into existing algorithms that uses Gaussian process maps for SLAM. Incorporating our proposed algorithm into an extended Kalman filter (EKF) for magnetic field SLAM reduces the overall computational complexity of the algorithm. We show experimentally that our algorithm is faster than existing methods when the mapped area is large and the map is based on many measurements, both for recursive mapping tasks and for magnetic field SLAM.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Bayes Analysis of Object Trajectory Representation Models</title>
<link>https://arxiv.org/abs/2211.01696</link>
<guid>https://arxiv.org/abs/2211.01696</guid>
<content:encoded><![CDATA[
arXiv:2211.01696v5 Announce Type: replace 
Abstract: Linear trajectory models provide mathematical advantages to autonomous driving applications such as motion prediction. However, linear models' expressive power and bias for real-world trajectories have not been thoroughly analyzed. We present an in-depth empirical analysis of the trade-off between model complexity and fit error in modelling object trajectories. We analyze vehicle, cyclist, and pedestrian trajectories. Our methodology estimates observation noise and prior distributions over model parameters from several large-scale datasets. Incorporating these priors can then regularize prediction models. Our results show that linear models do represent real-world trajectories with high fidelity at very moderate model complexity. This suggests the feasibility of using linear trajectory models in future motion prediction systems with inherent mathematical advantages.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModSec-AdvLearn: Countering Adversarial SQL Injections with Robust Machine Learning</title>
<link>https://arxiv.org/abs/2308.04964</link>
<guid>https://arxiv.org/abs/2308.04964</guid>
<content:encoded><![CDATA[
arXiv:2308.04964v4 Announce Type: replace 
Abstract: Many Web Application Firewalls (WAFs) leverage the OWASP CRS to block incoming malicious requests. The CRS consists of different sets of rules designed by domain experts to detect well-known web attack patterns. Both the set of rules and the weights used to combine them are manually defined, yielding four different default configurations of the CRS. In this work, we focus on the detection of SQLi attacks, and show that the manual configurations of the CRS typically yield a suboptimal trade-off between detection and false alarm rates. Furthermore, we show that these configurations are not robust to adversarial SQLi attacks, i.e., carefully-crafted attacks that iteratively refine the malicious SQLi payload by querying the target WAF to bypass detection. To overcome these limitations, we propose (i) using machine learning to automate the selection of the set of rules to be combined along with their weights, i.e., customizing the CRS configuration based on the monitored web services; and (ii) leveraging adversarial training to significantly improve its robustness to adversarial SQLi manipulations. Our experiments, conducted using the well-known open-source ModSecurity WAF equipped with the CRS rules, show that our approach, named ModSec-AdvLearn, can (i) increase the detection rate up to 30%, while retaining negligible false alarm rates and discarding up to 50% of the CRS rules; and (ii) improve robustness against adversarial SQLi attacks up to 85%, marking a significant stride toward designing more effective and robust WAFs. We release our open-source code at https://github.com/pralab/modsec-advlearn.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HurriCast: Synthetic Tropical Cyclone Track Generation for Hurricane Forecasting</title>
<link>https://arxiv.org/abs/2309.07174</link>
<guid>https://arxiv.org/abs/2309.07174</guid>
<content:encoded><![CDATA[
arXiv:2309.07174v2 Announce Type: replace 
Abstract: The generation of synthetic tropical cyclone(TC) tracks for risk assessment is a critical application of preparedness for the impacts of climate change and disaster relief, particularly in North America. Insurance companies use these synthetic tracks to estimate the potential risks and financial impacts of future TCs. For governments and policymakers, understanding the potential impacts of TCs helps in developing effective emergency response strategies, updating building codes, and prioritizing investments in resilience and mitigation projects. In this study, many hypothetical but plausible TC scenarios are created based on historical TC data HURDAT2 (HURricane DATA 2nd generation). A hybrid methodology, combining the ARIMA and K-MEANS methods with Autoencoder, is employed to capture better historical TC behaviors and project future trajectories and intensities. It demonstrates an efficient and reliable in the field of climate modeling and risk assessment. By effectively capturing past hurricane patterns and providing detailed future projections, this approach not only validates the reliability of this method but also offers crucial insights for a range of applications, from disaster preparedness and emergency management to insurance risk analysis and policy formulation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty quantification in fine-tuned LLMs using LoRA ensembles</title>
<link>https://arxiv.org/abs/2402.12264</link>
<guid>https://arxiv.org/abs/2402.12264</guid>
<content:encoded><![CDATA[
arXiv:2402.12264v2 Announce Type: replace 
Abstract: Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and balance between retained prior knowledge and domain specific adaptation during and after fine-tuning. We identify unexpected retention of acquired knowledge during fine-tuning in the overfitting regime.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference</title>
<link>https://arxiv.org/abs/2403.04082</link>
<guid>https://arxiv.org/abs/2403.04082</guid>
<content:encoded><![CDATA[
arXiv:2403.04082v4 Announce Type: replace 
Abstract: Given time series data, how can we answer questions like "what will happen in the future?" and "how did we get here?" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix. In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations. We validate our theory using numerical simulations on tasks up to 46-dimensions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breast Cancer Classification Using Gradient Boosting Algorithms Focusing on Reducing the False Negative and SHAP for Explainability</title>
<link>https://arxiv.org/abs/2403.09548</link>
<guid>https://arxiv.org/abs/2403.09548</guid>
<content:encoded><![CDATA[
arXiv:2403.09548v3 Announce Type: replace 
Abstract: Cancer is one of the diseases that kill the most women in the world, with breast cancer being responsible for the highest number of cancer cases and consequently deaths. However, it can be prevented by early detection and, consequently, early treatment. Any development for detection or perdition this kind of cancer is important for a better healthy life. Many studies focus on a model with high accuracy in cancer prediction, but sometimes accuracy alone may not always be a reliable metric. This study implies an investigative approach to studying the performance of different machine learning algorithms based on boosting to predict breast cancer focusing on the recall metric. Boosting machine learning algorithms has been proven to be an effective tool for detecting medical diseases. The dataset of the University of California, Irvine (UCI) repository has been utilized to train and test the model classifier that contains their attributes. The main objective of this study is to use state-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and LightGBM to predict and diagnose breast cancer and to find the most effective metric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study is the first to use these four boosting algorithms with Optuna, a library for hyperparameter optimization, and the SHAP method to improve the interpretability of our model, which can be used as a support to identify and predict breast cancer. We were able to improve AUC or recall for all the models and reduce the False Negative for AdaBoost and LigthGBM the final AUC were more than 99.41\% for all models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Heuristics for Transit Network Design and Improvement with Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2404.05894</link>
<guid>https://arxiv.org/abs/2404.05894</guid>
<content:encoded><![CDATA[
arXiv:2404.05894v5 Announce Type: replace 
Abstract: Planning a network of public transit routes is a challenging optimization problem. Metaheuristic algorithms search through the space of possible transit networks by applying heuristics that randomly alter routes in a network. The design of these heuristics has a major impact on the quality of the result. In this paper, we use deep reinforcement learning to train a graph neural net to provide heuristics for an evolutionary algorithm. These neural heuristics improve the algorithm's results on benchmark synthetic cities with 70 nodes or more, and achieve new state-of-the-art results on the challenging Mumford benchmark. They also improve upon a simulation of the real transit network in the city of Laval, Canada, by 52% and 25% on two key metrics, and offer cost savings of up to 19% over the city's existing transit network.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPO: A Differential and Pointwise Control Approach to Reinforcement Learning</title>
<link>https://arxiv.org/abs/2404.15617</link>
<guid>https://arxiv.org/abs/2404.15617</guid>
<content:encoded><![CDATA[
arXiv:2404.15617v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) in continuous state-action spaces remains challenging in scientific computing due to poor sample efficiency and lack of pathwise physical consistency. We introduce Differential Reinforcement Learning (Differential RL), a novel framework that reformulates RL from a continuous-time control perspective via a differential dual formulation. This induces a Hamiltonian structure that embeds physics priors and ensures consistent trajectories without requiring explicit constraints. To implement Differential RL, we develop Differential Policy Optimization (DPO), a pointwise, stage-wise algorithm that refines local movement operators along the trajectory for improved sample efficiency and dynamic alignment. We establish pointwise convergence guarantees, a property not available in standard RL, and derive a competitive theoretical regret bound of $O(K^{5/6})$. Empirically, DPO outperforms standard RL baselines on representative scientific computing tasks, including surface modeling, grid control, and molecular dynamics, under low-data and physics-constrained conditions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPO Meets PPO: Reinforced Token Optimization for RLHF</title>
<link>https://arxiv.org/abs/2404.18922</link>
<guid>https://arxiv.org/abs/2404.18922</guid>
<content:encoded><![CDATA[
arXiv:2404.18922v4 Announce Type: replace 
Abstract: In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards -- a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of large language models, its open-source implementation is still largely sub-optimal. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Under this framework, we introduce an algorithm Reinforced Token Optimization (\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. Extensive experiments demonstrate that \texttt{RTO} performs better than PPO and other direct preference learning algorithms. In particular, RTO outperforms PPO by 7.5 points on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code and models are available at \href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Deep Learning with Decorrelated Backpropagation</title>
<link>https://arxiv.org/abs/2405.02385</link>
<guid>https://arxiv.org/abs/2405.02385</guid>
<content:encoded><![CDATA[
arXiv:2405.02385v4 Announce Type: replace 
Abstract: The backpropagation algorithm remains the dominant and most successful method for training deep neural networks (DNNs). At the same time, training DNNs at scale comes at a significant computational cost and therefore a high carbon footprint. Converging evidence suggests that input decorrelation may speed up deep learning. However, to date, this has not yet translated into substantial improvements in training efficiency in large-scale DNNs. This is mainly caused by the challenge of enforcing fast and stable network-wide decorrelation. Here, we show for the first time that much more efficient training of deep convolutional neural networks is feasible by embracing decorrelated backpropagation as a mechanism for learning. To achieve this goal we made use of a novel algorithm which induces network-wide input decorrelation using minimal computational overhead. By combining this algorithm with careful optimizations, we achieve a more than two-fold speed-up and higher test accuracy compared to backpropagation when training several deep networks up to a 50-layer ResNet model. This demonstrates that decorrelation provides exciting prospects for efficient deep learning at scale.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Initial Introduction to Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2405.06161</link>
<guid>https://arxiv.org/abs/2405.06161</guid>
<content:encoded><![CDATA[
arXiv:2405.06161v5 Announce Type: replace 
Abstract: Multi-agent reinforcement learning (MARL) has exploded in popularity in recent years. While numerous approaches have been developed, they can be broadly categorized into three main types: centralized training and execution (CTE), centralized training for decentralized execution (CTDE), and decentralized training and execution (DTE). CTE methods assume centralization during training and execution (e.g., with fast, free, and perfect communication) and have the most information during execution. CTDE methods are the most common, as they leverage centralized information during training while enabling decentralized execution -- using only information available to that agent during execution. Decentralized training and execution methods make the fewest assumptions and are often simple to implement.
  This text is an introduction to cooperative MARL -- MARL in which all agents share a single, joint reward. It is meant to explain the setting, basic concepts, and common methods for the CTE, CTDE, and DTE settings. It does not cover all work in cooperative MARL as the area is quite extensive. I have included work that I believe is important for understanding the main concepts in the area and apologize to those that I have omitted. Topics include simple applications of single-agent methods to CTE as well as some more scalable methods that exploit the multi-agent structure, independent Q-learning and policy gradient methods and their extensions, as well as value function factorization methods including the well-known VDN, QMIX, and QPLEX approaches, and centralized critic methods including MADDPG, COMA, and MAPPO. I also discuss common misconceptions, the relationship between different approaches, and some open questions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing</title>
<link>https://arxiv.org/abs/2405.11783</link>
<guid>https://arxiv.org/abs/2405.11783</guid>
<content:encoded><![CDATA[
arXiv:2405.11783v2 Announce Type: replace 
Abstract: In this study, we explore the potential of using quantum natural language processing (QNLP) to inverse design metal-organic frameworks (MOFs) with targeted properties. Specifically, by analyzing 450 hypothetical MOF structures consisting of 3 topologies, 10 metal nodes and 15 organic ligands, we categorize these structures into four distinct classes for pore volume and $CO_{2}$ Henry's constant values. We then compare various QNLP models (i.e. the bag-of-words, DisCoCat (Distributional Compositional Categorical), and sequence-based models) to identify the most effective approach to process the MOF dataset. Using a classical simulator provided by the IBM Qiskit, the bag-of-words model is identified to be the optimum model, achieving validation accuracies of 88.6% and 78.0% for binary classification tasks on pore volume and $CO_{2}$ Henry's constant, respectively. Further, we developed multi-class classification models tailored to the probabilistic nature of quantum circuits, with average test accuracies of 92% and 80% across different classes for pore volume and $CO_{2}$ Henry's constant datasets. Finally, the performance of generating MOF with target properties showed accuracies of 93.5% for pore volume and 87% for $CO_{2}$ Henry's constant, respectively. Although our investigation covers only a fraction of the vast MOF search space, it marks a promising first step towards using quantum computing for materials design, offering a new perspective through which to explore the complex landscape of MOFs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Real-world Debiasing: Rethinking Evaluation, Challenge, and Solution</title>
<link>https://arxiv.org/abs/2405.15240</link>
<guid>https://arxiv.org/abs/2405.15240</guid>
<content:encoded><![CDATA[
arXiv:2405.15240v4 Announce Type: replace 
Abstract: Spurious correlations in training data significantly hinder the generalization capability of machine learning models when faced with distribution shifts, leading to the proposition of numberous debiasing methods. However, it remains to be asked: \textit{Do existing benchmarks for debiasing really represent biases in the real world?} Recent works attempt to address such concerns by sampling from real-world data (instead of synthesizing) according to some predefined biased distributions to ensure the realism of individual samples. However, the realism of the biased distribution is more critical yet challenging and underexplored due to the complexity of real-world bias distributions. To tackle the problem, we propose a fine-grained framework for analyzing biased distributions, based on which we empirically and theoretically identify key characteristics of biased distributions in the real world that are poorly represented by existing benchmarks. Towards applicable debiasing in the real world, we further introduce two novel real-world-inspired biases to bridge this gap and build a systematic evaluation framework for real-world debiasing, RDBench\footnote{RDBench: Code to be released. Preliminary version in supplementary material for anonimized review.}. Furthermore, focusing on the practical setting of debiasing w/o bias label, we find real-world biases pose a novel \textit{Sparse bias capturing} challenge to the existing paradigm. We propose a simple yet effective approach named Debias in Destruction (DiD), to address the challenge, whose effectiveness is validated with extensive experiments on 8 datasets of various biased distributions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Trajectory-Based Bayesian Approach to Multi-Objective Hyperparameter Optimization with Epoch-Aware Trade-Offs</title>
<link>https://arxiv.org/abs/2405.15303</link>
<guid>https://arxiv.org/abs/2405.15303</guid>
<content:encoded><![CDATA[
arXiv:2405.15303v2 Announce Type: replace 
Abstract: Training machine learning models inherently involves a resource-intensive and noisy iterative learning procedure that allows epoch-wise monitoring of the model performance. However, the insights gained from the iterative learning procedure typically remain underutilized in multi-objective hyperparameter optimization scenarios. Despite the limited research in this area, existing methods commonly identify the trade-offs only at the end of model training, overlooking the fact that trade-offs can emerge at earlier epochs in cases such as overfitting. To bridge this gap, we propose an enhanced multi-objective hyperparameter optimization problem that treats the number of training epochs as a decision variable, rather than merely an auxiliary parameter, to account for trade-offs at an earlier training stage. To solve this problem and accommodate its iterative learning, we then present a trajectory-based multi-objective Bayesian optimization algorithm characterized by two features: 1) a novel acquisition function that captures the improvement along the predictive trajectory of model performances over epochs for any hyperparameter setting and 2) a multi-objective early stopping mechanism that determines when to terminate the training to maximize epoch efficiency. Experiments on synthetic simulations and hyperparameter tuning benchmarks demonstrate that our algorithm can effectively identify the desirable trade-offs while improving tuning efficiency.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the Generalised $h$-transform</title>
<link>https://arxiv.org/abs/2406.01781</link>
<guid>https://arxiv.org/abs/2406.01781</guid>
<content:encoded><![CDATA[
arXiv:2406.01781v5 Announce Type: replace 
Abstract: Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for conditional sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling. Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood Doob's h-transform. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional $h$-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On image reconstruction tasks, we achieve speedups of up to 1.6$\times$, while having the best perceptual quality on natural images and reconstruction performance on medical images. Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Heteroscedastic Count Regression with Deep Double Poisson Networks</title>
<link>https://arxiv.org/abs/2406.09262</link>
<guid>https://arxiv.org/abs/2406.09262</guid>
<content:encoded><![CDATA[
arXiv:2406.09262v3 Announce Type: replace 
Abstract: Neural networks that can produce accurate, input-conditional uncertainty representations are critical for real-world applications. Recent progress on heteroscedastic continuous regression has shown great promise for calibrated uncertainty quantification on complex tasks, like image regression. However, when these methods are applied to discrete regression tasks, such as crowd counting, ratings prediction, or inventory estimation, they tend to produce predictive distributions with numerous pathologies. Moreover, discrete models based on the Generalized Linear Model (GLM) framework either cannot process complex input or are not fully heterosedastic. To address these issues we propose the Deep Double Poisson Network (DDPN). In contrast to networks trained to minimize Gaussian negative log likelihood (NLL), discrete network parameterizations (i.e., Poisson, Negative binomial), and GLMs, DDPN can produce discrete predictive distributions of arbitrary flexibility. Additionally, we propose a technique to tune the prioritization of mean fit and probabilistic calibration during training. We show DDPN 1) vastly outperforms existing discrete models; 2) meets or exceeds the accuracy and flexibility of networks trained with Gaussian NLL; 3) produces proper predictive distributions over discrete counts; and 4) exhibits superior out-of-distribution detection. DDPN can easily be applied to a variety of count regression datasets including tabular, image, point cloud, and text data.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hitchhiker's guide on the relation of Energy-Based Models with other generative models, sampling and statistical physics: a comprehensive review</title>
<link>https://arxiv.org/abs/2406.13661</link>
<guid>https://arxiv.org/abs/2406.13661</guid>
<content:encoded><![CDATA[
arXiv:2406.13661v2 Announce Type: replace 
Abstract: Energy-Based Models have emerged as a powerful framework in the realm of generative modeling, offering a unique perspective that aligns closely with principles of statistical mechanics. This review aims to provide physicists with a comprehensive understanding of EBMs, delineating their connection to other generative models such as Generative Adversarial Networks, Variational Autoencoders, and Normalizing Flows. We explore the sampling techniques crucial for EBMs, including Markov Chain Monte Carlo (MCMC) methods, and draw parallels between EBM concepts and statistical mechanics, highlighting the significance of energy functions and partition functions. Furthermore, we delve into recent training methodologies for EBMs, covering recent advancements and their implications for enhanced model performance and efficiency. This review is designed to clarify the often complex interconnections between these models, which can be challenging due to the diverse communities working on the topic.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Potentials and Challenges of Deep Generative Models in Product Design Conception</title>
<link>https://arxiv.org/abs/2407.11104</link>
<guid>https://arxiv.org/abs/2407.11104</guid>
<content:encoded><![CDATA[
arXiv:2407.11104v2 Announce Type: replace 
Abstract: The synthesis of product design concepts stands at the crux of early-phase development processes for technical products, traditionally posing an intricate interdisciplinary challenge. The application of deep learning methods, particularly Deep Generative Models (DGMs), holds the promise of automating and streamlining manual iterations and therefore introducing heightened levels of innovation and efficiency. However, DGMs have yet to be widely adopted into the synthesis of product design concepts. This paper aims to explore the reasons behind this limited application and derive the requirements for successful integration of these technologies. We systematically analyze DGM-families (VAE, GAN, Diffusion, Transformer, Radiance Field), assessing their strengths, weaknesses, and general applicability for product design conception. Our objective is to provide insights that simplify the decision-making process for engineers, helping them determine which method might be most effective for their specific challenges. Recognizing the rapid evolution of this field, we hope that our analysis contributes to a fundamental understanding and guides practitioners towards the most promising approaches. This work seeks not only to illuminate current challenges but also to propose potential solutions, thereby offering a clear roadmap for leveraging DGMs in the realm of product design conception.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Fine-Tuning via Circular Convolution</title>
<link>https://arxiv.org/abs/2407.19342</link>
<guid>https://arxiv.org/abs/2407.19342</guid>
<content:encoded><![CDATA[
arXiv:2407.19342v3 Announce Type: replace 
Abstract: Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large foundation models, leveraging low-rank matrices $\mathbf{A}$ and $\mathbf{B}$ to represent weight changes (i.e., $\Delta \mathbf{W} = \mathbf{B} \mathbf{A}$). This method reduces trainable parameters and mitigates heavy memory consumption associated with full delta matrices by sequentially multiplying $\mathbf{A}$ and $\mathbf{B}$ with the activation. Despite its success, the intrinsic low-rank characteristic may limit its performance. Although several variants have been proposed to address this issue, they often overlook the crucial computational and memory efficiency brought by LoRA. In this paper, we propose Circular Convolution Adaptation (C$^3$A), which not only achieves high-rank adaptation with enhanced performance but also excels in both computational power and memory utilization. Extensive experiments demonstrate that C$^3$A consistently outperforms LoRA and its variants across various fine-tuning tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Entropy</title>
<link>https://arxiv.org/abs/2409.03817</link>
<guid>https://arxiv.org/abs/2409.03817</guid>
<content:encoded><![CDATA[
arXiv:2409.03817v2 Announce Type: replace 
Abstract: We explore the connection between deep learning and information theory through the paradigm of diffusion models. A diffusion model converts noise into structured data by reinstating, imperfectly, information that is erased when data was diffused to noise. This information is stored in a neural network during training. We quantify this information by introducing a measure called neural entropy, which is related to the total entropy produced by diffusion. Neural entropy is a function of not just the data distribution, but also the diffusive process itself. Measurements of neural entropy on a few simple image diffusion models reveal that they are extremely efficient at compressing large ensembles of structured data.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Data Augmentation for Few-Shot Time Series Forecasting: A Reinforcement Learning Approach Guided by a Model Zoo</title>
<link>https://arxiv.org/abs/2409.06282</link>
<guid>https://arxiv.org/abs/2409.06282</guid>
<content:encoded><![CDATA[
arXiv:2409.06282v3 Announce Type: replace 
Abstract: Time series forecasting, particularly in few-shot learning scenarios, is challenging due to the limited availability of high-quality training data. To address this, we present a pilot study on using reinforcement learning (RL) for time series data augmentation. Our method, ReAugment, tackles three critical questions: which parts of the training set should be augmented, how the augmentation should be performed, and what advantages RL brings to the process. Specifically, our approach maintains a forecasting model zoo, and by measuring prediction diversity across the models, we identify samples with higher probabilities for overfitting and use them as the anchor points for augmentation. Leveraging RL, our method adaptively transforms the overfit-prone samples into new data that not only enhances training set diversity but also directs the augmented data to target regions where the forecasting models are prone to overfitting. We validate the effectiveness of ReAugment across a wide range of base models, showing its advantages in both standard time series forecasting and few-shot learning tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiSSL: Enhancing the Alignment Between Self-Supervised Pretraining and Downstream Fine-Tuning via Bilevel Optimization</title>
<link>https://arxiv.org/abs/2410.02387</link>
<guid>https://arxiv.org/abs/2410.02387</guid>
<content:encoded><![CDATA[
arXiv:2410.02387v4 Announce Type: replace 
Abstract: Models initialized from self-supervised pretraining may suffer from poor alignment with downstream tasks, reducing the extent to which subsequent fine-tuning can adapt pretrained features toward downstream objectives. To mitigate this, we introduce BiSSL, a novel bilevel training framework that enhances the alignment of self-supervised pretrained models with downstream tasks prior to fine-tuning. BiSSL acts as an intermediate training stage conducted after conventional self-supervised pretraining and is tasked with solving a bilevel optimization problem that incorporates the pretext and downstream training objectives in its lower- and upper-level objectives, respectively. This approach explicitly models the interdependence between the pretraining and fine-tuning stages within the conventional self-supervised learning pipeline, facilitating enhanced information sharing between them that ultimately leads to a model initialization better aligned with the downstream task. We propose a general training algorithm for BiSSL that is compatible with a broad range of pretext and downstream tasks. Using SimCLR and Bootstrap Your Own Latent to pretrain ResNet-50 backbones on the ImageNet dataset, we demonstrate that our proposed framework significantly improves accuracy on the vast majority of 12 downstream image classification datasets, as well as on object detection. Exploratory analyses alongside investigative experiments further provide compelling evidence that BiSSL enhances downstream alignment.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedGraph: A Research Library and Benchmark for Federated Graph Learning</title>
<link>https://arxiv.org/abs/2410.06340</link>
<guid>https://arxiv.org/abs/2410.06340</guid>
<content:encoded><![CDATA[
arXiv:2410.06340v3 Announce Type: replace 
Abstract: Federated graph learning is an emerging field with significant practical challenges. While algorithms have been proposed to improve the accuracy of training graph neural networks, such as node classification on federated graphs, the system performance is often overlooked, despite it is crucial for real-world deployment. To bridge this gap, we introduce FedGraph, a research library designed for practical distributed training and comprehensive benchmarking of FGL algorithms. FedGraph supports a range of state-of-the-art graph learning methods and includes a monitoring class that evaluates system performance, with a particular focus on communication and computation costs during training. Unlike existing federated learning platforms, FedGraph natively integrates homomorphic encryption to enhance privacy preservation and supports scalable deployment across multiple physical machines with system-level performance evaluation to guide the system design of future algorithms. To enhance efficiency and privacy, we propose a low-rank communication scheme for algorithms like FedGCN that require pre-training communication, accelerating both the pre-training and training phases. Extensive experiments benchmark different FGL algorithms on three major graph learning tasks and demonstrate FedGraph as the first efficient FGL framework to support encrypted low-rank communication and scale to graphs with 100 million nodes.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Feature Space Universality Across Large Language Models via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2410.06981</link>
<guid>https://arxiv.org/abs/2410.06981</guid>
<content:encoded><![CDATA[
arXiv:2410.06981v4 Announce Type: replace 
Abstract: The Universality Hypothesis in large language models (LLMs) claims that different models converge towards similar concept representations in their latent spaces. Providing evidence for this hypothesis would enable researchers to exploit universal properties, facilitating the generalization of mechanistic interpretability techniques across models. Previous works studied if LLMs learned the same features, which are internal representations that activate on specific concepts. Since comparing features across LLMs is challenging due to polysemanticity, in which LLM neurons often correspond to multiple unrelated features rather than to distinct concepts, sparse autoencoders (SAEs) have been employed to disentangle LLM neurons into SAE features corresponding to distinct concepts. In this paper, we introduce a new variation of the universality hypothesis called Analogous Feature Universality: we hypothesize that even if SAEs across different models learn different feature representations, the spaces spanned by SAE features are similar, such that one SAE space is similar to another SAE space under rotation-invariant transformations. Evidence for this hypothesis would imply that interpretability techniques related to latent spaces, such as steering vectors, may be transferred across models via certain transformations. To investigate this hypothesis, we first pair SAE features across different models via activation correlation, and then measure spatial relation similarities between paired features via representational similarity measures, which transform spaces into representations that reveal hidden relational similarities. Our experiments demonstrate high similarities for SAE feature spaces across various LLMs, providing evidence for feature space universality.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter Efficient Fine-tuning via Explained Variance Adaptation</title>
<link>https://arxiv.org/abs/2410.07170</link>
<guid>https://arxiv.org/abs/2410.07170</guid>
<content:encoded><![CDATA[
arXiv:2410.07170v4 Announce Type: replace 
Abstract: Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned for a specific downstream task. The most common fine-tuning method is to update pretrained weights via low-rank adaptation (LoRA). Existing initialization strategies for LoRA often rely on singular value decompositions (SVD) of gradients or weight matrices. However, they do not provably maximize the expected gradient signal, which is critical for fast adaptation. To this end, we introduce Explained Variance Adaptation (EVA), an initialization scheme that uses the directions capturing the most activation variance, provably maximizing the expected gradient signal and accelerating fine-tuning. EVA performs incremental SVD on minibatches of activation vectors and selects the right-singular vectors for initialization once they converged. Further, by selecting the directions that capture the most activation-variance for a given rank budget, EVA accommodates adaptive ranks that reduce the number of trainable parameters, while maintaining or improving downstream performance. We apply EVA to a variety of fine-tuning tasks as language generation and understanding, image classification, and reinforcement learning. EVA exhibits faster convergence than competitors and achieves the highest average score across a multitude of tasks per domain while reducing the number of trainable parameters through rank redistribution.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Loss Optimization in the Infinite Width: Stable Parameterization of Predictive Coding Networks and Target Propagation</title>
<link>https://arxiv.org/abs/2411.02001</link>
<guid>https://arxiv.org/abs/2411.02001</guid>
<content:encoded><![CDATA[
arXiv:2411.02001v2 Announce Type: replace 
Abstract: Local learning, which trains a network through layer-wise local targets and losses, has been studied as an alternative to backpropagation (BP) in neural computation. However, its algorithms often become more complex or require additional hyperparameters because of the locality, making it challenging to identify desirable settings in which the algorithm progresses in a stable manner. To provide theoretical and quantitative insights, we introduce the maximal update parameterization ($\mu$P) in the infinite-width limit for two representative designs of local targets: predictive coding (PC) and target propagation (TP). We verified that $\mu$P enables hyperparameter transfer across models of different widths. Furthermore, our analysis revealed unique and intriguing properties of $\mu$P that are not present in conventional BP. By analyzing deep linear networks, we found that PC's gradients interpolate between first-order and Gauss-Newton-like gradients, depending on the parameterization. We demonstrate that, in specific standard settings, PC in the infinite-width limit behaves more similarly to the first-order gradient. For TP, even with the standard scaling of the last layer, which differs from classical $\mu$P, its local loss optimization favors the feature learning regime over the kernel regime.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised detection of semantic correlations in big data</title>
<link>https://arxiv.org/abs/2411.02126</link>
<guid>https://arxiv.org/abs/2411.02126</guid>
<content:encoded><![CDATA[
arXiv:2411.02126v3 Announce Type: replace 
Abstract: In real-world data, information is stored in extremely large feature vectors. These variables are typically correlated due to complex interactions involving many features simultaneously. Such correlations qualitatively correspond to semantic roles and are naturally recognized by both the human brain and artificial neural networks. This recognition enables, for instance, the prediction of missing parts of an image or text based on their context. We present a method to detect these correlations in high-dimensional data represented as binary numbers. We estimate the binary intrinsic dimension of a dataset, which quantifies the minimum number of independent coordinates needed to describe the data, and is therefore a proxy of semantic complexity. The proposed algorithm is largely insensitive to the so-called curse of dimensionality, and can therefore be used in big data analysis. We test this approach identifying phase transitions in model magnetic systems and we then apply it to the detection of semantic correlations of images and text inside deep neural networks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers</title>
<link>https://arxiv.org/abs/2411.15370</link>
<guid>https://arxiv.org/abs/2411.15370</guid>
<content:encoded><![CDATA[
arXiv:2411.15370v2 Announce Type: replace 
Abstract: Modern deep policy gradient methods achieve effective performance on simulated robotic tasks, but they all require large replay buffers or expensive batch updates, or both, making them incompatible for real systems with resource-limited computers. We show that these methods fail catastrophically when limited to small replay buffers or during incremental learning, where updates only use the most recent sample without batch updates or a replay buffer. We propose a novel incremental deep policy gradient method -- Action Value Gradient (AVG) and a set of normalization and scaling techniques to address the challenges of instability in incremental learning. On robotic simulation benchmarks, we show that AVG is the only incremental method that learns effectively, often achieving final performance comparable to batch policy gradient methods. This advancement enabled us to show for the first time effective deep reinforcement learning with real robots using only incremental updates, employing a robotic manipulator and a mobile robot.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.00661</link>
<guid>https://arxiv.org/abs/2412.00661</guid>
<content:encoded><![CDATA[
arXiv:2412.00661v3 Announce Type: replace 
Abstract: Designing efficient algorithms for multi-agent reinforcement learning (MARL) is fundamentally challenging because the size of the joint state and action spaces grows exponentially in the number of agents. These difficulties are exacerbated when balancing sequential global decision-making with local agent interactions. In this work, we propose a new algorithm $\texttt{SUBSAMPLE-MFQ}$ ($\textbf{Subsample}$-$\textbf{M}$ean-$\textbf{F}$ield-$\textbf{Q}$-learning) and a decentralized randomized policy for a system with $n$ agents. For any $k\leq n$, our algorithm learns a policy for the system in time polynomial in $k$. We prove that this learned policy converges to the optimal policy on the order of $\tilde{O}(1/\sqrt{k})$ as the number of subsampled agents $k$ increases. In particular, this bound is independent of the number of agents $n$.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-based Method for Satellite Pattern-of-Life Identification</title>
<link>https://arxiv.org/abs/2412.10814</link>
<guid>https://arxiv.org/abs/2412.10814</guid>
<content:encoded><![CDATA[
arXiv:2412.10814v2 Announce Type: replace 
Abstract: Satellite pattern-of-life (PoL) identification is crucial for space safety and satellite monitoring, involving the analysis of typical satellite behaviors such as station-keeping, drift, etc. However, existing PoL identification methods remain underdeveloped due to the complexity of aerospace systems, variability in satellite behaviors, and fluctuating observation sampling rates. In a first attempt, we developed a domain expertise-informed machine learning method (Expert-ML) to combine satellite orbital movement knowledge and machine learning models. The Expert-ML method achieved high accuracy results in simulation data and real-world data with normal sampling rate. However, this approach lacks of generality as it requires domain expertise and its performance degraded significantly when data sampling rate varied. To achieve generality, we propose a novel diffusion-based PoL identification method. Distinct from prior approaches, the proposed method leverages a diffusion model to achieve end-to-end identification without manual refinement or domain-specific knowledge. Specifically, we employ a multivariate time-series encoder to capture hidden representations of satellite positional data. The encoded features are subsequently incorporated as conditional information in the denoising process to generate PoL labels. Through experimentation across real-world satellite settings, our proposed diffusion-based method demonstrates its high identification quality and provides a robust solution even with reduced data sampling rates, indicating its great potential in practical satellite behavior pattern identification, tracking and related mission deployment.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating Data Corruption in Machine Learning: Balancing Quality, Quantity, and Imputation Strategies</title>
<link>https://arxiv.org/abs/2412.18296</link>
<guid>https://arxiv.org/abs/2412.18296</guid>
<content:encoded><![CDATA[
arXiv:2412.18296v2 Announce Type: replace 
Abstract: Data corruption, including missing and noisy data, poses significant challenges in real-world machine learning. This study investigates the effects of data corruption on model performance and explores strategies to mitigate these effects through two experimental setups: supervised learning with NLP tasks (NLP-SL) and deep reinforcement learning for traffic signal optimization (Signal-RL). We analyze the relationship between data corruption levels and model performance, evaluate the effectiveness of data imputation methods, and assess the utility of enlarging datasets to address data corruption.
  Our results show that model performance under data corruption follows a diminishing return curve, modeled by the exponential function. Missing data, while detrimental, is less harmful than noisy data, which causes severe performance degradation and training instability, particularly in sequential decision-making tasks like Signal-RL. Imputation strategies involve a trade-off: they recover missing information but may introduce noise. Their effectiveness depends on imputation accuracy and corruption ratio. We identify distinct regions in the imputation advantage heatmap, including an "imputation advantageous corner" and an "imputation disadvantageous edge" and classify tasks as "noise-sensitive" or "noise-insensitive" based on their decision boundaries.
  Furthermore, we find that increasing dataset size mitigates but cannot fully overcome the effects of data corruption. The marginal utility of additional data diminishes as corruption increases. An empirical rule emerges: approximately 30% of the data is critical for determining performance, while the remaining 70% has minimal impact.
  These findings provide actionable insights into data preprocessing, imputation strategies, and data collection practices, guiding the development of robust machine learning systems in noisy environments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty quantification for improving radiomic-based models in radiation pneumonitis prediction</title>
<link>https://arxiv.org/abs/2412.19511</link>
<guid>https://arxiv.org/abs/2412.19511</guid>
<content:encoded><![CDATA[
arXiv:2412.19511v3 Announce Type: replace 
Abstract: Background: Radiation pneumonitis is a side effect of thoracic radiation therapy. Recently, machine learning models with radiomic features have improved radiation pneumonitis prediction by capturing spatial information. To further support clinical decision-making, this study explores the role of post hoc uncertainty quantification methods in enhancing model uncertainty estimate. Methods: We retrospectively analyzed a cohort of 101 esophageal cancer patients. This study evaluated four machine learning models: logistic regression, support vector machines, extreme gradient boosting, and random forest, using 15 dosimetric, 79 dosiomic, and 237 radiomic features to predict radiation pneumonitis. We applied uncertainty quantification methods, including Platt scaling, isotonic regression, Venn-ABERS predictor, and conformal prediction, to quantify uncertainty. Model performance was assessed through an area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPRC), and adaptive calibration error using leave-one-out cross-validation. Results: Highest AUROC is achieved by the logistic regression model with the conformal prediction method (AUROC 0.75+-0.01, AUPRC 0.74+-0.01) at a certainty cut point of 0.8. Highest AUPRC of 0.82+-0.02 (with AUROC of 0.67+-0.04) achieved by The extreme gradient boosting model with conformal prediction at the 0.9 certainty threshold. Radiomic and dosiomic features improve both discriminative and calibration performance. Conclusions: Integrating uncertainty quantification into machine learning models with radiomic and dosiomic features may improve both predictive accuracy and calibration, supporting more reliable clinical decision-making. The findings emphasize the value of uncertainty quantification methods in enhancing applicability of predictive models for radiation pneumonitis in healthcare settings.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monotonic Learning in the PAC Framework: A New Perspective</title>
<link>https://arxiv.org/abs/2501.05493</link>
<guid>https://arxiv.org/abs/2501.05493</guid>
<content:encoded><![CDATA[
arXiv:2501.05493v2 Announce Type: replace 
Abstract: Monotone learning describes learning processes in which expected performance consistently improves as the amount of training data increases. However, recent studies challenge this conventional wisdom, revealing significant gaps in the understanding of generalization in machine learning. Addressing these gaps is crucial for advancing the theoretical foundations of the field. In this work, we utilize Probably Approximately Correct (PAC) learning theory to construct a theoretical risk distribution that approximates a learning algorithm's actual performance. We rigorously prove that this theoretical distribution exhibits monotonicity as sample sizes increase. We identify two scenarios under which deterministic algorithms based on Empirical Risk Minimization (ERM) are monotone: (1) the hypothesis space is finite, or (2) the hypothesis space has finite VC-dimension. Experiments on two classical learning problems validate our findings by demonstrating that the monotonicity of the algorithms' generalization error is guaranteed, as its theoretical risk upper bound monotonically converges to 0.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Learning in Energy-based Models with Attractor Structures</title>
<link>https://arxiv.org/abs/2501.13997</link>
<guid>https://arxiv.org/abs/2501.13997</guid>
<content:encoded><![CDATA[
arXiv:2501.13997v2 Announce Type: replace 
Abstract: Predictive models are highly advanced in understanding the mechanisms of brain function. Recent advances in machine learning further underscore the power of prediction for optimal representation in learning. However, there remains a gap in creating a biologically plausible model that explains how the neural system achieves prediction. In this paper, we introduce a framework that employs an energy-based model (EBM) to capture the nuanced processes of predicting observation after action within the neural system, encompassing prediction, learning, and inference. We implement the EBM with a hierarchical structure and integrate a continuous attractor neural network for memory, constructing a biologically plausible model. In experimental evaluations, our model demonstrates efficacy across diverse scenarios. The range of actions includes eye movement, motion in environments, head turning, and static observation while the environment changes. Our model not only makes accurate predictions for environments it was trained on, but also provides reasonable predictions for unseen environments, matching the performances of machine learning methods in multiple tasks. We hope that this study contributes to a deep understanding of how the neural system performs prediction.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Conformal Prediction via Message Passing</title>
<link>https://arxiv.org/abs/2501.14544</link>
<guid>https://arxiv.org/abs/2501.14544</guid>
<content:encoded><![CDATA[
arXiv:2501.14544v2 Announce Type: replace 
Abstract: Post-hoc calibration of pre-trained models is critical for ensuring reliable inference, especially in safety-critical domains such as healthcare. Conformal Prediction (CP) offers a robust post-hoc calibration framework, providing distribution-free statistical coverage guarantees for prediction sets by leveraging held-out datasets. In this work, we address a decentralized setting where each device has limited calibration data and can communicate only with its neighbors over an arbitrary graph topology. We propose two message-passing-based approaches for achieving reliable inference via CP: quantile-based distributed conformal prediction (Q-DCP) and histogram-based distributed conformal prediction (H-DCP). Q-DCP employs distributed quantile regression enhanced with tailored smoothing and regularization terms to accelerate convergence, while H-DCP uses a consensus-based histogram estimation approach. Through extensive experiments, we investigate the trade-offs between hyperparameter tuning requirements, communication overhead, coverage guarantees, and prediction set sizes across different network topologies. The code of our work is released on: https://github.com/HaifengWen/Distributed-Conformal-Prediction.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transport on Categorical Data for Counterfactuals using Compositional Data and Dirichlet Transport</title>
<link>https://arxiv.org/abs/2501.15549</link>
<guid>https://arxiv.org/abs/2501.15549</guid>
<content:encoded><![CDATA[
arXiv:2501.15549v2 Announce Type: replace 
Abstract: Recently, optimal transport-based approaches have gained attention for deriving counterfactuals, e.g., to quantify algorithmic discrimination. However, in the general multivariate setting, these methods are often opaque and difficult to interpret. To address this, alternative methodologies have been proposed, using causal graphs combined with iterative quantile regressions (Ple\v{c}ko and Meinshausen (2020)) or sequential transport (Fernandes Machado et al. (2025)) to examine fairness at the individual level, often referred to as ``counterfactual fairness.'' Despite these advancements, transporting categorical variables remains a significant challenge in practical applications with real datasets. In this paper, we propose a novel approach to address this issue. Our method involves (1) converting categorical variables into compositional data and (2) transporting these compositions within the probabilistic simplex of $\mathbb{R}^d$. We demonstrate the applicability and effectiveness of this approach through an illustration on real-world data, and discuss limitations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Width Neural Networks</title>
<link>https://arxiv.org/abs/2501.15889</link>
<guid>https://arxiv.org/abs/2501.15889</guid>
<content:encoded><![CDATA[
arXiv:2501.15889v4 Announce Type: replace 
Abstract: For almost 70 years, researchers have mostly relied on hyper-parameter tuning to select the width of neural networks' layers. This paper challenges the status quo by introducing an easy-to-use technique to learn an unbounded width of a neural network's layer during training. The technique does not rely on alternate optimization nor hand-crafted gradient heuristics; rather, it jointly optimizes the width and the parameters of each layer via simple backpropagation. We apply the technique to a broad range of data domains such as tables, images, text, sequences, and graphs, showing how the width adapts to the task's difficulty. The method imposes a soft ordering of importance among neurons, by which it also is possible to truncate the trained network at virtually zero cost, achieving a smooth trade-off between performance and compute resources in a structured way. Alternatively, one can dynamically compress the network with no performance degradation. In light of recent foundation models trained on large datasets, believed to require billions of parameters and where hyper-parameter tuning is unfeasible due to humongous training costs, our approach stands as a viable alternative for width learning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Low Intrinsic Dimensionality to Non-Vacuous Generalization Bounds in Deep Multi-Task Learning</title>
<link>https://arxiv.org/abs/2501.19067</link>
<guid>https://arxiv.org/abs/2501.19067</guid>
<content:encoded><![CDATA[
arXiv:2501.19067v2 Announce Type: replace 
Abstract: Deep learning methods are known to generalize well from training to future data, even in an overparametrized regime, where they could easily overfit. One explanation for this phenomenon is that even when their *ambient dimensionality*, (i.e. the number of parameters) is large, the models' *intrinsic dimensionality* is small; specifically, their learning takes place in a small subspace of all possible weight configurations. In this work, we confirm this phenomenon in the setting of *deep multi-task learning*. We introduce a method to parametrize multi-task network directly in the low-dimensional space, facilitated by the use of *random expansions* techniques. We then show that high-accuracy multi-task solutions can be found with much smaller intrinsic dimensionality (fewer free parameters) than what single-task learning requires. Subsequently, we show that the low-dimensional representations in combination with *weight compression* and *PAC-Bayesian* reasoning lead to the *first non-vacuous generalization bounds* for deep multi-task networks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence</title>
<link>https://arxiv.org/abs/2502.00678</link>
<guid>https://arxiv.org/abs/2502.00678</guid>
<content:encoded><![CDATA[
arXiv:2502.00678v2 Announce Type: replace 
Abstract: Dataset contamination, where evaluation datasets overlap with pre-training corpora, inflates performance metrics and undermines the reliability of model evaluations. Measuring dataset contamination thus becomes essential to ensure that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than relying on memorized examples. To address this problem, we propose Kernel Divergence Score (KDS), a novel method that evaluates dataset contamination by computing the divergence between the kernel similarity matrix of sample embeddings, before and after fine-tuning on the benchmark dataset. Leveraging the insight that fine-tuning affects unseen samples more significantly than seen ones, KDS provides a reliable measure of contamination. Through extensive experiments on controlled contamination scenarios, KDS demonstrates a near-perfect correlation with contamination levels and outperforms existing baselines. Additionally, we perform comprehensive ablation studies to analyze the impact of key design choices, providing deeper insights into the components and effectiveness of KDS. These ablations highlight the importance of leveraging fine-grained kernel-based information and confirm the reliability of the proposed framework across diverse datasets and settings. Code is released in https://github.com/deeplearning-wisc/kernel-divergence-score.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation</title>
<link>https://arxiv.org/abs/2502.01068</link>
<guid>https://arxiv.org/abs/2502.01068</guid>
<content:encoded><![CDATA[
arXiv:2502.01068v2 Announce Type: replace 
Abstract: While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to reduce latency for long-context inference. FastKV improves processing speed while preserving accuracy by adopting Token-Selective Propagation (TSP). This approach preserves full-context information in early layers of LLMs and selectively propagates only a portion of this information in later layers. This design enables FastKV to minimize redundant computation without sacrificing contextual fidelity. Our experimental results show that FastKV achieves up to 1.97$\times$ and 4.82$\times$ improvements in time-to-first-token (TTFT) and throughput, respectively, compared to baseline without KV cache compression. Moreover, FastKV successfully maintains accuracy within 1\% of the baseline on long-context benchmarks. Our code is available at https://github.com/dongwonjo/FastKV.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Fused State Representations for Control from Multi-View Observations</title>
<link>https://arxiv.org/abs/2502.01316</link>
<guid>https://arxiv.org/abs/2502.01316</guid>
<content:encoded><![CDATA[
arXiv:2502.01316v2 Announce Type: replace 
Abstract: Multi-View Reinforcement Learning (MVRL) seeks to provide agents with multi-view observations, enabling them to perceive environment with greater effectiveness and precision. Recent advancements in MVRL focus on extracting latent representations from multiview observations and leveraging them in control tasks. However, it is not straightforward to learn compact and task-relevant representations, particularly in the presence of redundancy, distracting information, or missing views. In this paper, we propose Multi-view Fusion State for Control (MFSC), firstly incorporating bisimulation metric learning into MVRL to learn task-relevant representations. Furthermore, we propose a multiview-based mask and latent reconstruction auxiliary task that exploits shared information across views and improves MFSC's robustness in missing views by introducing a mask token. Extensive experimental results demonstrate that our method outperforms existing approaches in MVRL tasks. Even in more realistic scenarios with interference or missing views, MFSC consistently maintains high performance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning with Differentially Private (Sliced) Wasserstein Gradients</title>
<link>https://arxiv.org/abs/2502.01701</link>
<guid>https://arxiv.org/abs/2502.01701</guid>
<content:encoded><![CDATA[
arXiv:2502.01701v2 Announce Type: replace 
Abstract: In this work, we introduce a novel framework for privately optimizing objectives that rely on Wasserstein distances between data-dependent empirical measures. Our main theoretical contribution is, based on an explicit formulation of the Wasserstein gradient in a fully discrete setting, a control on the sensitivity of this gradient to individual data points, allowing strong privacy guarantees at minimal utility cost. Building on these insights, we develop a deep learning approach that incorporates gradient and activations clipping, originally designed for DP training of problems with a finite-sum structure. We further demonstrate that privacy accounting methods extend to Wasserstein-based objectives, facilitating large-scale private training. Empirical results confirm that our framework effectively balances accuracy and privacy, offering a theoretically sound solution for privacy-preserving machine learning tasks relying on optimal transport distances such as Wasserstein distance or sliced-Wasserstein distance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Data Generation Using Diffusion Models</title>
<link>https://arxiv.org/abs/2502.02448</link>
<guid>https://arxiv.org/abs/2502.02448</guid>
<content:encoded><![CDATA[
arXiv:2502.02448v2 Announce Type: replace 
Abstract: Sparse data is ubiquitous, appearing in numerous domains, from economics and recommender systems to astronomy and biomedical sciences. However, efficiently generating high-fidelity synthetic sparse data remains a significant challenge. We introduce Sparse Data Diffusion (SDD), a novel method for generating sparse data. SDD extends continuous state-space diffusion models with an explicit representation of exact zeros by modeling sparsity through the introduction of Sparsity Bits. Empirical validation in various domains, including two scientific applications in physics and biology, demonstrates that SDD achieves high fidelity in representing data sparsity while preserving the quality of the generated data.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling laws in wearable human activity recognition</title>
<link>https://arxiv.org/abs/2502.03364</link>
<guid>https://arxiv.org/abs/2502.03364</guid>
<content:encoded><![CDATA[
arXiv:2502.03364v2 Announce Type: replace 
Abstract: Many deep architectures and self-supervised pre-training techniques have been proposed for human activity recognition (HAR) from wearable multimodal sensors. Scaling laws have the potential to help move towards more principled design by linking model capacity with pre-training data volume. Yet, scaling laws have not been established for HAR to the same extent as in language and vision. By conducting an exhaustive grid search on both amount of pre-training data and Transformer architectures, we establish the first known scaling laws for HAR. We show that pre-training loss scales with a power law relationship to amount of data and parameter count and that increasing the number of users in a dataset results in a steeper improvement in performance than increasing data per user, indicating that diversity of pre-training data is important, which contrasts to some previously reported findings in self-supervised HAR. We show that these scaling laws translate to downstream performance improvements on three HAR benchmark datasets of postures, modes of locomotion and activities of daily living: UCI HAR and WISDM Phone and WISDM Watch. Finally, we suggest some previously published works should be revisited in light of these scaling laws with more adequate model capacities.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Features to Transformers: Redefining Ranking for Scalable Impact</title>
<link>https://arxiv.org/abs/2502.03417</link>
<guid>https://arxiv.org/abs/2502.03417</guid>
<content:encoded><![CDATA[
arXiv:2502.03417v2 Announce Type: replace 
Abstract: We present LiGR, a large-scale ranking framework developed at LinkedIn that brings state-of-the-art transformer-based modeling architectures into production. We introduce a modified transformer architecture that incorporates learned normalization and simultaneous set-wise attention to user history and ranked items. This architecture enables several breakthrough achievements, including: (1) the deprecation of most manually designed feature engineering, outperforming the prior state-of-the-art system using only few features (compared to hundreds in the baseline), (2) validation of the scaling law for ranking systems, showing improved performance with larger models, more training data, and longer context sequences, and (3) simultaneous joint scoring of items in a set-wise manner, leading to automated improvements in diversity. To enable efficient serving of large ranking models, we describe techniques to scale inference effectively using single-pass processing of user history and set-wise attention. We also summarize key insights from various ablation studies and A/B tests, highlighting the most impactful technical approaches.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning Dynamics</title>
<link>https://arxiv.org/abs/2502.03654</link>
<guid>https://arxiv.org/abs/2502.03654</guid>
<content:encoded><![CDATA[
arXiv:2502.03654v2 Announce Type: replace 
Abstract: Activation functions are fundamental elements of deep learning architectures as they significantly influence training dynamics. ReLU, while widely used, is prone to the dying neuron problem, which has been mitigated by variants such as LeakyReLU, PReLU, and ELU that better handle negative neuron outputs. Recently, self-gated activations like GELU and Swish have emerged as state-of-the-art alternatives, leveraging their smoothness to ensure stable gradient flow and prevent neuron inactivity. In this work, we introduce the Gompertz Linear Unit (GoLU), a novel self-gated activation function defined as $\mathrm{GoLU}(x) = x \, \mathrm{Gompertz}(x)$, where $\mathrm{Gompertz}(x) = e^{-e^{-x}}$. The GoLU activation leverages the right-skewed asymmetry in the Gompertz function to reduce variance in the latent space more effectively compared to GELU and Swish, while preserving robust gradient flow. Extensive experiments across diverse tasks, including Image Classification, Language Modeling, Semantic Segmentation, Object Detection, Instance Segmentation, and Diffusion, highlight GoLU's superior performance relative to state-of-the-art activation functions, establishing GoLU as a robust alternative to existing activation functions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based Perspective</title>
<link>https://arxiv.org/abs/2502.04591</link>
<guid>https://arxiv.org/abs/2502.04591</guid>
<content:encoded><![CDATA[
arXiv:2502.04591v3 Announce Type: replace 
Abstract: Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing in the nonlinear setting without assumptions on the boundedness of the weight matrices. Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Link Prediction for Directed Graphs</title>
<link>https://arxiv.org/abs/2502.05724</link>
<guid>https://arxiv.org/abs/2502.05724</guid>
<content:encoded><![CDATA[
arXiv:2502.05724v2 Announce Type: replace 
Abstract: Link prediction for directed graphs is a crucial task with diverse real-world applications. Recent advances in embedding methods and Graph Neural Networks (GNNs) have shown promising improvements. However, these methods often lack a thorough analysis of their expressiveness and suffer from effective benchmarks for a fair evaluation. In this paper, we propose a unified framework to assess the expressiveness of existing methods, highlighting the impact of dual embeddings and decoder design on directed link prediction performance. To address limitations in current benchmark setups, we introduce DirLinkBench, a robust new benchmark with comprehensive coverage, standardized evaluation, and modular extensibility. The results on DirLinkBench show that current methods struggle to achieve strong performance, while DiGAE outperforms other baselines overall. We further revisit DiGAE theoretically, showing its graph convolution aligns with GCN on an undirected bipartite graph. Inspired by these insights, we propose a novel Spectral Directed Graph Auto-Encoder SDGAE that achieves state-of-the-art average performance on DirLinkBench. Finally, we analyze key factors influencing directed link prediction and highlight open challenges in this field.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution</title>
<link>https://arxiv.org/abs/2502.06809</link>
<guid>https://arxiv.org/abs/2502.06809</guid>
<content:encoded><![CDATA[
arXiv:2502.06809v2 Announce Type: replace 
Abstract: Interpreting the internal mechanisms of large language models (LLMs) is crucial for improving their trustworthiness and utility. Prior work has primarily focused on mapping individual neurons to discrete semantic concepts. However, such mappings struggle to handle the inherent polysemanticity in LLMs, where individual neurons encode multiple, distinct concepts. Through a comprehensive analysis of both encoder and decoder-based LLMs across diverse datasets, we observe that even highly salient neurons, identified via various attribution techniques for specific semantic concepts, consistently exhibit polysemantic behavior. Importantly, activation magnitudes for fine-grained concepts follow distinct, often Gaussian-like distributions with minimal overlap. This observation motivates a shift from neuron attribution to range-based interpretation. We hypothesize that interpreting and manipulating neuron activation ranges would enable more precise interpretability and targeted interventions in LLMs. To validate our hypothesis, we introduce NeuronLens, a novel range-based interpretation and manipulation framework that provides a finer view of neuron activation distributions to localize concept attribution within a neuron. Extensive empirical evaluations demonstrate that NeuronLens significantly reduces unintended interference, while maintaining precise manipulation of targeted concepts, outperforming neuron attribution.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning on Dyads to Enhance Medication Adherence</title>
<link>https://arxiv.org/abs/2502.06835</link>
<guid>https://arxiv.org/abs/2502.06835</guid>
<content:encoded><![CDATA[
arXiv:2502.06835v2 Announce Type: replace 
Abstract: Medication adherence is critical for the recovery of adolescents and young adults (AYAs) who have undergone hematopoietic cell transplantation (HCT). However, maintaining adherence is challenging for AYAs after hospital discharge, who experience both individual (e.g. physical and emotional symptoms) and interpersonal barriers (e.g., relational difficulties with their care partner, who is often involved in medication management). To optimize the effectiveness of a three-component digital intervention targeting both members of the dyad as well as their relationship, we propose a novel Multi-Agent Reinforcement Learning (MARL) approach to personalize the delivery of interventions. By incorporating the domain knowledge, the MARL framework, where each agent is responsible for the delivery of one intervention component, allows for faster learning compared with a flattened agent. Evaluation using a dyadic simulator environment, based on real clinical data, shows a significant improvement in medication adherence (approximately 3%) compared to purely random intervention delivery. The effectiveness of this approach will be further evaluated in an upcoming trial.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Is Not the Bottleneck: Cost-Efficient Continual Learning via Weight Space Consolidation</title>
<link>https://arxiv.org/abs/2502.07274</link>
<guid>https://arxiv.org/abs/2502.07274</guid>
<content:encoded><![CDATA[
arXiv:2502.07274v3 Announce Type: replace 
Abstract: Continual learning (CL) has traditionally emphasized minimizing exemplar memory usage, assuming that memory is the primary bottleneck. However, in modern computing environments-particularly those involving large foundation models-memory is inexpensive and abundant, while GPU time constitutes the main cost. This paper re-examines CL under a more realistic setting with sufficient exemplar memory, where the system can retain a representative portion of past data. We find that, under this regime, stability improves due to reduced forgetting, but plasticity diminishes as the model becomes biased toward prior tasks and struggles to adapt to new ones. Notably, even simple baselines like naive replay can match or exceed the performance of state-of-the-art methods at a fraction of the computational cost. Building on this insight, we propose a lightweight yet effective method called Weight Space Consolidation, which directly operates in the model's weight space via two core mechanisms: (1) rank-based parameter resets to recover plasticity, and (2) weight averaging to enhance stability. Our approach outperforms strong baselines across class-incremental learning with image classifiers and continual instruction tuning with large language models, while requiring only one-third to one-fourth of the training cost. These findings challenge long-standing CL assumptions and establish a new, cost-efficient baseline for real-world continual learning systems where exemplar memory is no longer the limiting factor.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Autonomous VLM Agents via Variational Subgoal-Conditioned Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.07949</link>
<guid>https://arxiv.org/abs/2502.07949</guid>
<content:encoded><![CDATA[
arXiv:2502.07949v2 Announce Type: replace 
Abstract: State-of-the-art (SOTA) reinforcement learning (RL) methods have enabled vision-language model (VLM) agents to learn from interaction with online environments without human supervision. However, these methods often struggle with learning inefficiencies when applied to complex, real-world decision-making tasks with sparse rewards and long-horizon dependencies. We propose a novel framework, Variational Subgoal-Conditioned Reinforcement Learning (VSC-RL), advancing the VLM agents in resolving challenging decision-making tasks. Fundamentally distinct from existing methods, VSC-RL reformulates the decision-making problem as a variational subgoal-conditioned RL problem with the newly derived optimization objective, Subgoal Evidence Lower BOund (SGC-ELBO), which comprises two key components: (a) maximizing the subgoal-conditioned return, and (b) minimizing the divergence from a reference goal-conditioned policy. We theoretically and empirically demonstrate that the VSC-RL can efficiently improve the learning efficiency without compromising performance guarantees. Across a diverse set of challenging benchmarks, including mobile device and web control tasks, VSC-RL consistently outperforms existing SOTA methods, achieving superior learning efficiency and performance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Asynchronous Federated Learning: A~Delicate Trade-Off Between Model-Parameter Staleness and Update Frequency</title>
<link>https://arxiv.org/abs/2502.08206</link>
<guid>https://arxiv.org/abs/2502.08206</guid>
<content:encoded><![CDATA[
arXiv:2502.08206v2 Announce Type: replace 
Abstract: Synchronous federated learning (FL) scales poorly with the number of clients due to the straggler effect. Algorithms like FedAsync and GeneralizedFedAsync address this limitation by enabling asynchronous communication between clients and the central server. In this work, we rely on stochastic modeling and analysis to better understand the impact of design choices in asynchronous FL algorithms, such as the concurrency level and routing probabilities, and we leverage this knowledge to optimize loss. Compared to most existing studies, we account for the joint impact of heterogeneous and variable service speeds and heterogeneous datasets at the clients. We characterize in particular a fundamental trade-off for optimizing asynchronous FL: minimizing gradient estimation errors by avoiding model parameter staleness, while also speeding up the system by increasing the throughput of model updates. Our two main contributions can be summarized as follows. First, we prove a discrete variant of Little's law to derive a closed-form expression for relative delay, a metric that quantifies staleness. This allows us to efficiently minimize the average loss per model update, which has been the gold standard in literature to date. Second, we observe that naively optimizing this metric leads us to slow down the system drastically by overemphazing staleness at the detriment of throughput. This motivates us to introduce an alternative metric that also takes system speed into account, for which we derive a tractable upper-bound that can be minimized numerically. Extensive numerical results show that these optimizations enhance accuracy by 10% to 30%.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Semantic Routing in Large Mixture-of-Expert Models</title>
<link>https://arxiv.org/abs/2502.10928</link>
<guid>https://arxiv.org/abs/2502.10928</guid>
<content:encoded><![CDATA[
arXiv:2502.10928v2 Announce Type: replace 
Abstract: In the past year, large (>100B parameter) mixture-of-expert (MoE) models have become increasingly common in the open domain. While their advantages are often framed in terms of efficiency, prior work has also explored functional differentiation through routing behavior. We investigate whether expert routing in large MoE models is influenced by the semantics of the inputs. To test this, we design two controlled experiments. First, we compare activations on sentence pairs with a shared target word used in the same or different senses. Second, we fix context and substitute the target word with semantically similar or dissimilar alternatives. Comparing expert overlap across these conditions reveals clear, statistically significant evidence of semantic routing in large MoE models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GiFT: Gibbs Fine-Tuning for Code Generation</title>
<link>https://arxiv.org/abs/2502.11466</link>
<guid>https://arxiv.org/abs/2502.11466</guid>
<content:encoded><![CDATA[
arXiv:2502.11466v2 Announce Type: replace 
Abstract: Training Large Language Models (LLMs) with synthetic data is a prevalent practice in code generation. A key approach is self-training, where LLMs are iteratively trained on self-generated correct code snippets. In this case, the self-generated codes are drawn from a conditional distribution, conditioned on a specific seed description. However, the seed description is not the only valid representation that aligns with its intended meaning. With all valid descriptions and codes forming a joint space, codes drawn from the conditional distribution would lead to an underrepresentation of the full description-code space. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training method inspired by Gibbs sampling. GiFT allows self-generated data to be drawn from the marginal distribution of the joint space, thereby mitigating the biases inherent in conditional sampling. We provide a theoretical analysis demonstrating the potential benefits of fine-tuning LLMs with code derived from the marginal distribution. Furthermore, we propose a perplexity-based code selection method to mitigate the imbalanced long-tail distribution of the self-generated codes. Empirical evaluation of two LLMs across four datasets demonstrates that GiFT achieves superior performance, particularly on more challenging benchmarks. Source code is available at https://github.com/Alex-HaochenLi/GiFT.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities</title>
<link>https://arxiv.org/abs/2502.12128</link>
<guid>https://arxiv.org/abs/2502.12128</guid>
<content:encoded><![CDATA[
arXiv:2502.12128v3 Announce Type: replace 
Abstract: Generative models are spearheading recent progress in deep learning, showcasing strong promise for trajectory sampling in dynamical systems as well. However, whereas latent space modeling paradigms have transformed image and video generation, similar approaches are more difficult for most dynamical systems. Such systems -- from chemical molecule structures to collective human behavior -- are described by interactions of entities, making them inherently linked to connectivity patterns, entity conservation, and the traceability of entities over time. Our approach, LaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked Entities), bridges the gap between: (1) keeping the traceability of individual entities in a latent system representation, and (2) leveraging the efficiency and scalability of recent advances in image and video generation, where pre-trained encoder and decoder enable generative modeling directly in latent space. The core idea of LaM-SLidE is the introduction of identifier representations (IDs) that enable the retrieval of entity properties and entity composition from latent system representations, thus fostering traceability. Experimentally, across different domains, we show that LaM-SLidE performs favorably in terms of speed, accuracy, and generalizability. Code is available at https://github.com/ml-jku/LaM-SLidE .
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis</title>
<link>https://arxiv.org/abs/2502.13178</link>
<guid>https://arxiv.org/abs/2502.13178</guid>
<content:encoded><![CDATA[
arXiv:2502.13178v4 Announce Type: replace 
Abstract: Post-training Quantization (PTQ) technique has been extensively adopted for large language models (LLMs) compression owing to its efficiency and low resource requirement. However, current research lacks a in-depth analysis of the superior and applicable scenarios of each PTQ strategy. In addition, existing algorithms focus primarily on performance, overlooking the trade-off among model size, performance, and quantization bitwidth. To mitigate these confusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly, in order to support our benchmark, we propose a comprehensive taxonomy for existing mainstream methods by scrutinizing their computational strategies (e.g., optimization-based, compensation-based, etc.). Then, we conduct extensive experiments with the baseline within each class, covering models with various sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1), architectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and VILA1.5) on a wide range of evaluation metrics.Through comparative analysis on the results, we summarize the superior of each PTQ strategy and modelsize-bitwidth trade-off considering the performance. For example, our benchmark reveals that compensation-based technique demonstrates outstanding cross-architecture robustness and extremely low-bit PTQ for ultra large models should be reexamined. Finally, we further accordingly claim that a practical combination of compensation and other PTQ strategy can achieve SOTA various robustness. We believe that our benchmark will provide valuable recommendations for the deployment of LLMs and future research on PTQ approaches.We conduct an repository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Transformer Inference: Optimization Strategies for Time Series Classification</title>
<link>https://arxiv.org/abs/2502.16627</link>
<guid>https://arxiv.org/abs/2502.16627</guid>
<content:encoded><![CDATA[
arXiv:2502.16627v4 Announce Type: replace 
Abstract: The increasing computational demands of transformer models in time series classification necessitate effective optimization strategies for energy-efficient deployment. Our study presents a systematic investigation of optimization techniques, focusing on structured pruning and quantization methods for transformer architectures. Through extensive experimentation on three distinct datasets (RefrigerationDevices, ElectricDevices, and PLAID), we quantitatively evaluate model performance and energy efficiency across different transformer configurations. Our experimental results demonstrate that static quantization reduces energy consumption by 29.14% while maintaining classification performance, and L1 pruning achieves a 63% improvement in inference speed with minimal accuracy degradation. Our findings provide valuable insights into the effectiveness of optimization strategies for transformer-based time series classification, establishing a foundation for efficient model deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Vision-Language Alignment by Cauchy-Schwarz Divergence</title>
<link>https://arxiv.org/abs/2502.17028</link>
<guid>https://arxiv.org/abs/2502.17028</guid>
<content:encoded><![CDATA[
arXiv:2502.17028v2 Announce Type: replace 
Abstract: Multimodal alignment is crucial for various downstream tasks such as cross-modal generation and retrieval. Previous multimodal approaches like CLIP utilize InfoNCE to maximize mutual information, primarily aligning pairwise samples across modalities while overlooking distributional differences. In addition, InfoNCE has inherent conflict in terms of alignment and uniformity in multimodality, leading to suboptimal alignment with modality gaps. To overcome the limitations, we propose CS-Aligner, a novel framework that performs distributional vision-language alignment by integrating Cauchy-Schwarz (CS) divergence with mutual information. CS-Aligner captures both the global distribution information of each modality and the pairwise semantic relationships. We find that the CS divergence seamlessly addresses the InfoNCE's alignment-uniformity conflict and serves complementary roles with InfoNCE, yielding tighter and more precise alignment. Moreover, by introducing distributional alignment, CS-Aligner enables incorporating additional information from unpaired data and token-level representations, enhancing flexible and fine-grained alignment in practice. Experiments on text-to-image generation and cross-modality retrieval tasks demonstrate the effectiveness of our method on vision-language alignment.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Powerful Electronic Health Record Encoders</title>
<link>https://arxiv.org/abs/2502.17403</link>
<guid>https://arxiv.org/abs/2502.17403</guid>
<content:encoded><![CDATA[
arXiv:2502.17403v3 Announce Type: replace 
Abstract: Electronic Health Records (EHRs) offer considerable potential for clinical prediction, but their complexity and heterogeneity present significant challenges for traditional machine learning methods. Recently, domain-specific EHR foundation models trained on large volumes of unlabeled EHR data have shown improved predictive accuracy and generalization. However, their development is constrained by limited access to diverse, high-quality datasets, and by inconsistencies in coding standards and clinical practices. In this study, we explore the use of general-purpose Large Language Models (LLMs) to encode EHR into high-dimensional representations for downstream clinical prediction tasks. We convert structured EHR data into markdown-formatted plain text documents by replacing medical codes with natural language descriptions. This enables the use of LLMs and their extensive semantic understanding and generalization capabilities as effective encoders of EHRs without requiring access to private medical training data. We show that LLM-based embeddings can often match or even surpass the performance of a specialized EHR foundation model, CLMBR-T-Base, across 15 diverse clinical tasks from the EHRSHOT benchmark. To demonstrate generalizability, we further evaluate the approach on the UK Biobank (UKB) cohort, a population distinct from that used to train CLMBR-T-Base. Notably, one of the tested LLM-based models achieves superior performance for disease onset, hospitalization, and mortality prediction, highlighting robustness to shifts in patient populations. Our findings suggest that repurposed general-purpose LLMs for EHR encoding provide a scalable and generalizable alternative to domain-specific models for clinical prediction.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Metric Distance to Autoregressive Multimodal Foundational Models</title>
<link>https://arxiv.org/abs/2503.02379</link>
<guid>https://arxiv.org/abs/2503.02379</guid>
<content:encoded><![CDATA[
arXiv:2503.02379v2 Announce Type: replace 
Abstract: As large language models expand beyond natural language to domains such as mathematics, multimodal understanding, and embodied agents, tokens increasingly reflect metric relationships rather than purely linguistic meaning. We introduce DIST2Loss, a distance-aware framework designed to train autoregressive discrete models by leveraging predefined distance relationships among output tokens. At its core, DIST2Loss transforms continuous exponential family distributions derived from inherent distance metrics into discrete, categorical optimization targets compatible with the models' architectures. This approach enables the models to learn and preserve meaningful distance relationships during token generation while maintaining compatibility with existing architectures. Empirical evaluations show consistent performance gains in diverse multimodal applications, including visual grounding, robotic manipulation, generative reward modeling, and image generation using vector-quantized features. These improvements are most notable in low-data regimes, demonstrating DIST2Loss's strength under resource constraints.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining</title>
<link>https://arxiv.org/abs/2503.04715</link>
<guid>https://arxiv.org/abs/2503.04715</guid>
<content:encoded><![CDATA[
arXiv:2503.04715v5 Announce Type: replace 
Abstract: The impressive capabilities of Large Language Models (LLMs) across diverse tasks are now well-established, yet their effective deployment necessitates careful hyperparameter optimization. Through extensive empirical studies involving grid searches across diverse configurations, we discover universal scaling laws governing these hyperparameters: optimal learning rate follows a power-law relationship with both model parameters and data sizes, while optimal batch size scales primarily with data sizes. Our analysis reveals a convex optimization landscape for hyperparameters under fixed models and data size conditions. This convexity implies an optimal hyperparameter plateau. We contribute a universal, plug-and-play optimal hyperparameter tool for the community. Its estimated values on the test set are merely 0.09% away from the globally optimal LLM performance found via an exhaustive search. These laws demonstrate remarkable robustness across variations in model sparsity, training data distribution, and model shape. To our best known, this is the first work that unifies different model shapes and structures, such as Mixture-of-Experts models and dense transformers, as well as establishes optimal hyperparameter scaling laws across diverse data distributions. This exhaustive optimization process demands substantial computational resources, utilizing nearly one million NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and hyperparameters from scratch and consuming approximately 100 trillion tokens in total. To facilitate reproducibility and further research, we will progressively release all loss measurements and model checkpoints through our designated repository https://step-law.github.io/
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising Score Distillation: From Noisy Diffusion Pretraining to One-Step High-Quality Generation</title>
<link>https://arxiv.org/abs/2503.07578</link>
<guid>https://arxiv.org/abs/2503.07578</guid>
<content:encoded><![CDATA[
arXiv:2503.07578v2 Announce Type: replace 
Abstract: Diffusion models have achieved remarkable success in generating high-resolution, realistic images across diverse natural distributions. However, their performance heavily relies on high-quality training data, making it challenging to learn meaningful distributions from corrupted samples. This limitation restricts their applicability in scientific domains where clean data is scarce or costly to obtain. In this work, we introduce denoising score distillation (DSD), a surprisingly effective and novel approach for training high-quality generative models from low-quality data. DSD first pretrains a diffusion model exclusively on noisy, corrupted samples and then distills it into a one-step generator capable of producing refined, clean outputs. While score distillation is traditionally viewed as a method to accelerate diffusion models, we show that it can also significantly enhance sample quality, particularly when starting from a degraded teacher model. Across varying noise levels and datasets, DSD consistently improves generative performancewe summarize our empirical evidence in Fig. 1. Furthermore, we provide theoretical insights showing that, in a linear model setting, DSD identifies the eigenspace of the clean data distributions covariance matrix, implicitly regularizing the generator. This perspective reframes score distillation as not only a tool for efficiency but also a mechanism for improving generative models, particularly in low-quality data settings.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRACE: Time SeRies PArameter EffiCient FinE-tuning</title>
<link>https://arxiv.org/abs/2503.16991</link>
<guid>https://arxiv.org/abs/2503.16991</guid>
<content:encoded><![CDATA[
arXiv:2503.16991v2 Announce Type: replace 
Abstract: We propose an efficient fine-tuning method for time series foundation models, termed TRACE: Time Series Parameter Efficient Fine-tuning. While pretrained time series foundation models are gaining popularity, they face the following challenges: (1) Unlike natural language tasks, time series data vary in frequency, channel numbers, historical/prediction lengths. For long-term forecasting tasks in particular, tailored fine-tuning can significantly enhance performance.(2) Existing parameter-efficient tuning methods like LoRA remain applicable but require adaptation to temporal characteristics.
  To address these challenges, our TRACE framework introduces two key innovations: (1) Gated DSIC (Gated Dynamic Simulation Importance Calculation), an unbiased LoRA module importance selection mechanism that ensures conditional parameter consistency before and after masking. Experiments demonstrate that Gated DSIC outperforms common fine-tuning. (2) Reconstructed prediction heads for long-term forecasting tasks, which achieve comparable or superior performance to linear probing heads while drastically reducing parameter counts.
  Extensive experiments on long-/short-term forecasting, anomaly detection and natural language tasks across diverse datasets, coupled with ablation studies, validate the effectiveness of our method.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effectively Controlling Reasoning Models through Thinking Intervention</title>
<link>https://arxiv.org/abs/2503.24370</link>
<guid>https://arxiv.org/abs/2503.24370</guid>
<content:encoded><![CDATA[
arXiv:2503.24370v3 Announce Type: replace 
Abstract: Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We find that the Thinking Intervention paradigm enhances the capabilities of reasoning models across a wide range of tasks, including instruction following on IFEval and Overthinking, instruction hierarchy on SEP, and safety alignment on XSTest and SorryBench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plain Transformers Can be Powerful Graph Learners</title>
<link>https://arxiv.org/abs/2504.12588</link>
<guid>https://arxiv.org/abs/2504.12588</guid>
<content:encoded><![CDATA[
arXiv:2504.12588v2 Announce Type: replace 
Abstract: Transformers have attained outstanding performance across various modalities, owing to their simple but powerful scaled-dot-product (SDP) attention mechanisms. Researchers have attempted to migrate Transformers to graph learning, but most advanced Graph Transformers (GTs) have strayed far from plain Transformers, exhibiting major architectural differences either by integrating message-passing or incorporating sophisticated attention mechanisms. These divergences hinder the easy adoption of training advances for Transformers developed in other domains. Contrary to previous GTs, this work demonstrates that the plain Transformer architecture can be a powerful graph learner. To achieve this, we propose to incorporate three simple, minimal, and easy-to-implement modifications to the plain Transformer architecture to construct our Powerful Plain Graph Transformers (PPGT): (1) simplified $L_2$ attention for measuring the magnitude closeness among tokens; (2) adaptive root-mean-square normalization to preserve token magnitude information; and (3) a simple MLP-based stem for graph positional encoding. Consistent with its theoretical expressivity, PPGT demonstrates noteworthy realized expressivity on the empirical graph expressivity benchmark, comparing favorably to more complicated competitors such as subgraph GNNs and higher-order GNNs. Its outstanding empirical performance across various graph datasets also justifies the practical effectiveness of PPGT.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Inference: Interpreting Small Language Models with Susceptibilities</title>
<link>https://arxiv.org/abs/2504.18274</link>
<guid>https://arxiv.org/abs/2504.18274</guid>
<content:encoded><![CDATA[
arXiv:2504.18274v2 Announce Type: replace 
Abstract: We develop a linear response framework for interpretability that treats a neural network as a Bayesian statistical mechanical system. A small perturbation of the data distribution, for example shifting the Pile toward GitHub or legal text, induces a first-order change in the posterior expectation of an observable localized on a chosen component of the network. The resulting susceptibility can be estimated efficiently with local SGLD samples and factorizes into signed, per-token contributions that serve as attribution scores. We combine these susceptibilities into a response matrix whose low-rank structure separates functional modules such as multigram and induction heads in a 3M-parameter transformer.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoseX: AI Defeats Physics Approaches on Protein-Ligand Cross Docking</title>
<link>https://arxiv.org/abs/2505.01700</link>
<guid>https://arxiv.org/abs/2505.01700</guid>
<content:encoded><![CDATA[
arXiv:2505.01700v2 Announce Type: replace 
Abstract: Existing protein-ligand docking studies typically focus on the self-docking scenario, which is less practical in real applications. Moreover, some studies involve heavy frameworks requiring extensive training, posing challenges for convenient and efficient assessment of docking methods. To fill these gaps, we design PoseX, an open-source benchmark to evaluate both self-docking and cross-docking, enabling a practical and comprehensive assessment of algorithmic advances. Specifically, we curated a novel dataset comprising 718 entries for self-docking and 1,312 entries for cross-docking; second, we incorporated 23 docking methods in three methodological categories, including physics-based methods (e.g., Schr\"odinger Glide), AI docking methods (e.g., DiffDock) and AI co-folding methods (e.g., AlphaFold3); third, we developed a relaxation method for post-processing to minimize conformational energy and refine binding poses; fourth, we built a leaderboard to rank submitted models in real-time. We derived some key insights and conclusions from extensive experiments: (1) AI approaches have consistently outperformed physics-based methods in overall docking success rate. (2) Most intra- and intermolecular clashes of AI approaches can be greatly alleviated with relaxation, which means combining AI modeling with physics-based post-processing could achieve excellent performance. (3) AI co-folding methods exhibit ligand chirality issues, except for Boltz-1x, which introduced physics-inspired potentials to fix hallucinations, suggesting modeling on stereochemistry improves the structural plausibility markedly. (4) Specifying binding pockets significantly promotes docking performance, indicating that pocket information can be leveraged adequately, particularly for AI co-folding methods, in future modeling efforts. The code, dataset, and leaderboard are released at https://github.com/CataAI/PoseX.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2505.03230</link>
<guid>https://arxiv.org/abs/2505.03230</guid>
<content:encoded><![CDATA[
arXiv:2505.03230v2 Announce Type: replace 
Abstract: The integration of simultaneous wireless information and power transfer (SWIPT) technology in 6G Internet of Things (IoT) networks faces significant challenges in remote areas and disaster scenarios where ground infrastructure is unavailable. This paper proposes a novel unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) system enhanced by directional antennas to provide both computational resources and energy support for ground IoT terminals. However, such systems require multiple trade-off policies to balance UAV energy consumption, terminal battery levels, and computational resource allocation under various constraints, including limited UAV battery capacity, non-linear energy harvesting characteristics, and dynamic task arrivals. To address these challenges comprehensively, we formulate a bi-objective optimization problem that simultaneously considers system energy efficiency and terminal battery sustainability. We then reformulate this non-convex problem with a hybrid solution space as a Markov decision process (MDP) and propose an improved soft actor-critic (SAC) algorithm with an action simplification mechanism to enhance its convergence and generalization capabilities. Simulation results have demonstrated that our proposed approach outperforms various baselines in different scenarios, achieving efficient energy management while maintaining high computational performance. Furthermore, our method shows strong generalization ability across different scenarios, particularly in complex environments, validating the effectiveness of our designed boundary penalty and charging reward mechanisms.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks</title>
<link>https://arxiv.org/abs/2505.04046</link>
<guid>https://arxiv.org/abs/2505.04046</guid>
<content:encoded><![CDATA[
arXiv:2505.04046v2 Announce Type: replace 
Abstract: Trustworthy multi-view learning has attracted extensive attention because evidence learning can provide reliable uncertainty estimation to enhance the credibility of multi-view predictions. Existing trusted multi-view learning methods implicitly assume that multi-view data is secure. However, in safety-sensitive applications such as autonomous driving and security monitoring, multi-view data often faces threats from adversarial perturbations, thereby deceiving or disrupting multi-view models. This inevitably leads to the adversarial unreliability problem (AUP) in trusted multi-view learning. To overcome this tricky problem, we propose a novel multi-view learning framework, namely Reliable Disentanglement Multi-view Learning (RDML). Specifically, we first propose evidential disentanglement learning to decompose each view into clean and adversarial parts under the guidance of corresponding evidences, which is extracted by a pretrained evidence extractor. Then, we employ the feature recalibration module to mitigate the negative impact of adversarial perturbations and extract potential informative features from them. Finally, to further ignore the irreparable adversarial interferences, a view-level evidential attention mechanism is designed. Extensive experiments on multi-view classification tasks with adversarial attacks show that RDML outperforms the state-of-the-art methods by a relatively large margin. Our code is available at https://github.com/Willy1005/2025-IJCAI-RDML.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Chain of Thoughts via Elastic Reasoning</title>
<link>https://arxiv.org/abs/2505.05315</link>
<guid>https://arxiv.org/abs/2505.05315</guid>
<content:encoded><![CDATA[
arXiv:2505.05315v2 Announce Type: replace 
Abstract: Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases--thinking and solution--with independently allocated budgets. At test time, Elastic Reasoning prioritizes the completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Our code has been made available at https://github.com/SalesforceAIResearch/Elastic-Reasoning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRUNE: A Patching Based Repair Framework for Certifiable Unlearning of Neural Networks</title>
<link>https://arxiv.org/abs/2505.06520</link>
<guid>https://arxiv.org/abs/2505.06520</guid>
<content:encoded><![CDATA[
arXiv:2505.06520v2 Announce Type: replace 
Abstract: It is often desirable to remove (a.k.a. unlearn) a specific part of the training data from a trained neural network model. A typical application scenario is to protect the data holder's right to be forgotten, which has been promoted by many recent regulation rules. Existing unlearning methods involve training alternative models with remaining data, which may be costly and challenging to verify from the data holder or a thirdparty auditor's perspective. In this work, we provide a new angle and propose a novel unlearning approach by imposing carefully crafted "patch" on the original neural network to achieve targeted "forgetting" of the requested data to delete. Specifically, inspired by the research line of neural network repair, we propose to strategically seek a lightweight minimum "patch" for unlearning a given data point with certifiable guarantee. Furthermore, to unlearn a considerable amount of data points (or an entire class), we propose to iteratively select a small subset of representative data points to unlearn, which achieves the effect of unlearning the whole set. Extensive experiments on multiple categorical datasets demonstrates our approach's effectiveness, achieving measurable unlearning while preserving the model's performance and being competitive in efficiency and memory consumption compared to various baseline methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreqMoE: Dynamic Frequency Enhancement for Neural PDE Solvers</title>
<link>https://arxiv.org/abs/2505.06858</link>
<guid>https://arxiv.org/abs/2505.06858</guid>
<content:encoded><![CDATA[
arXiv:2505.06858v2 Announce Type: replace 
Abstract: Fourier Neural Operators (FNO) have emerged as promising solutions for efficiently solving partial differential equations (PDEs) by learning infinite-dimensional function mappings through frequency domain transformations. However, the sparsity of high-frequency signals limits computational efficiency for high-dimensional inputs, and fixed-pattern truncation often causes high-frequency signal loss, reducing performance in scenarios such as high-resolution inputs or long-term predictions. To address these challenges, we propose FreqMoE, an efficient and progressive training framework that exploits the dependency of high-frequency signals on low-frequency components. The model first learns low-frequency weights and then applies a sparse upward-cycling strategy to construct a mixture of experts (MoE) in the frequency domain, effectively extending the learned weights to high-frequency regions. Experiments on both regular and irregular grid PDEs demonstrate that FreqMoE achieves up to 16.6% accuracy improvement while using merely 2.1% parameters (47.32x reduction) compared to dense FNO. Furthermore, the approach demonstrates remarkable stability in long-term predictions and generalizes seamlessly to various FNO variants and grid structures, establishing a new ``Low frequency Pretraining, High frequency Fine-tuning'' paradigm for solving PDEs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.07527</link>
<guid>https://arxiv.org/abs/2505.07527</guid>
<content:encoded><![CDATA[
arXiv:2505.07527v2 Announce Type: replace 
Abstract: Reward baseline is important for Reinforcement Learning (RL) algorithms to reduce variance in policy gradient estimates. Recently, for language modeling, Group Relative Policy Optimization (GRPO) is proposed to compute the advantage for each output by subtracting the mean reward, as the baseline, for all outputs in the group. However, it can lead to inaccurate advantage estimates in environments with highly noisy rewards, potentially introducing bias. In this work, we propose a model, called Kalman Filter Enhanced Group Relative Policy Optimization (KRPO), by using lightweight Kalman filtering to dynamically estimate the latent reward mean and variance. This filtering technique replaces the naive batch mean baseline, enabling more adaptive advantage normalization. Our method does not require additional learned parameters over GRPO. This approach offers a simple yet effective way to incorporate multiple outputs of GRPO into advantage estimation, improving policy optimization in settings where highly dynamic reward signals are difficult to model for language models. Through accuracy and rewards obtained from math question answering and reasoning, we show that using a more adaptive advantage estimation model, KRPO can improve the stability and performance of GRPO. The code is available at https://github.com/billhhh/KRPO_LLMs_RL.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feasibility-Aware Pessimistic Estimation: Toward Long-Horizon Safety in Offline RL</title>
<link>https://arxiv.org/abs/2505.08179</link>
<guid>https://arxiv.org/abs/2505.08179</guid>
<content:encoded><![CDATA[
arXiv:2505.08179v2 Announce Type: replace 
Abstract: Offline safe reinforcement learning(OSRL) derives constraint-satisfying policies from pre-collected datasets, offers a promising avenue for deploying RL in safety-critical real-world domains such as robotics. However, the majority of existing approaches emphasize only short-term safety, neglecting long-horizon considerations. Consequently, they may violate safety constraints and fail to ensure sustained protection during online deployment. Moreover, the learned policies often struggle to handle states and actions that are not present or out-of-distribution(OOD) from the offline dataset, and exhibit limited sample efficiency. To address these challenges, we propose a novel framework Feasibility-Aware offline Safe Reinforcement Learning with CVAE-based Pessimism (FASP). First, we employ Hamilton-Jacobi (H-J) reachability analysis to generate reliable safety labels, which serve as supervisory signals for training both a conditional variational autoencoder (CVAE) and a safety classifier. This approach not only ensures high sampling efficiency but also provides rigorous long-horizon safety guarantees. Furthermore, we utilize pessimistic estimation methods to estimate the Q-value of reward and cost, which mitigates the extrapolation errors induces by OOD actions, and penalize unsafe actions to enabled the agent to proactively avoid high-risk behaviors. Moreover, we theoretically prove the validity of this pessimistic estimation. Extensive experiments on DSRL benchmarks demonstrate that FASP algorithm achieves competitive performance across multiple experimental tasks, particularly outperforming state-of-the-art algorithms in terms of safety.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Predictive Optimization and Generation for Business AI</title>
<link>https://arxiv.org/abs/2505.09847</link>
<guid>https://arxiv.org/abs/2505.09847</guid>
<content:encoded><![CDATA[
arXiv:2505.09847v2 Announce Type: replace 
Abstract: The sales process involves sales functions converting leads or opportunities to customers and selling more products to existing customers. The optimization of the sales process thus is key to success of any B2B business. In this work, we introduce a principled approach to sales optimization and business AI, namely the Causal Predictive Optimization and Generation, which includes three layers: 1) prediction layer with causal ML 2) optimization layer with constraint optimization and contextual bandit 3) serving layer with Generative AI and feedback-loop for system enhancement. We detail the implementation and deployment of the system in LinkedIn, showcasing significant wins over legacy systems and sharing learning and insight broadly applicable to this field.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training NTK to Generalize with KARE</title>
<link>https://arxiv.org/abs/2505.11347</link>
<guid>https://arxiv.org/abs/2505.11347</guid>
<content:encoded><![CDATA[
arXiv:2505.11347v2 Announce Type: replace 
Abstract: The performance of the data-dependent neural tangent kernel (NTK; Jacot et al. (2018)) associated with a trained deep neural network (DNN) often matches or exceeds that of the full network. This implies that DNN training via gradient descent implicitly performs kernel learning by optimizing the NTK. In this paper, we propose instead to optimize the NTK explicitly. Rather than minimizing empirical risk, we train the NTK to minimize its generalization error using the recently developed Kernel Alignment Risk Estimator (KARE; Jacot et al. (2020)). Our simulations and real data experiments show that NTKs trained with KARE consistently match or significantly outperform the original DNN and the DNN- induced NTK (the after-kernel). These results suggest that explicitly trained kernels can outperform traditional end-to-end DNN optimization in certain settings, challenging the conventional dominance of DNNs. We argue that explicit training of NTK is a form of over-parametrized feature learning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems</title>
<link>https://arxiv.org/abs/2505.11415</link>
<guid>https://arxiv.org/abs/2505.11415</guid>
<content:encoded><![CDATA[
arXiv:2505.11415v2 Announce Type: replace 
Abstract: The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for scaling Large Language Models (LLMs) efficiently, but it depends on heterogeneous compute and memory resources. These factors jointly affect system Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing benchmarks often fail to capture these trade-offs accurately, complicating practical deployment decisions. To address this, we introduce MoE-CAP, a benchmark specifically designed for MoE systems. Our analysis reveals that achieving an optimal balance across CAP is difficult with current hardware; MoE systems typically optimize two of the three dimensions at the expense of the third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose the CAP Radar Diagram. We further introduce sparsity-aware performance metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems across diverse hardware platforms and deployment scenarios.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-fid: Quantum Circuit Fidelity Improvement with LSTM Networks</title>
<link>https://arxiv.org/abs/2303.17523</link>
<guid>https://arxiv.org/abs/2303.17523</guid>
<content:encoded><![CDATA[
arXiv:2303.17523v4 Announce Type: replace-cross 
Abstract: The fidelity of quantum circuits (QC) is influenced by several factors, including hardware characteristics, calibration status, and the transpilation process, all of which impact their susceptibility to noise. However, existing methods struggle to estimate and compare the noise performance of different circuit layouts due to fluctuating error rates and the absence of a standardized fidelity metric. In this work, Q-fid is introduced, a Long Short-Term Memory (LSTM) based fidelity prediction system accompanied by a novel metric designed to quantify the fidelity of quantum circuits. Q-fid provides an intuitive way to predict the noise performance of Noisy Intermediate-Scale Quantum (NISQ) circuits. This approach frames fidelity prediction as a Time Series Forecasting problem to analyze the tokenized circuits, capturing the causal dependence of the gate sequences and their impact on overall fidelity. Additionally, the model is capable of dynamically adapting to changes in hardware characteristics, ensuring accurate fidelity predictions under varying conditions. Q-fid achieves a high prediction accuracy with an average RMSE of 0.0515, up to 24.7x more accurate than the Qiskit transpile tool mapomatic. By offering a reliable method for fidelity prediction, Q-fid empowers developers to optimize transpilation strategies, leading to more efficient and noise-resilient quantum circuit implementations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network</title>
<link>https://arxiv.org/abs/2310.06488</link>
<guid>https://arxiv.org/abs/2310.06488</guid>
<content:encoded><![CDATA[
arXiv:2310.06488v4 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) have emerged as a promising alternative to conventional Artificial Neural Networks (ANNs), demonstrating comparable performance in both visual and linguistic tasks while offering the advantage of improved energy efficiency. Despite these advancements, the integration of linguistic and visual features into a unified representation through spike trains poses a significant challenge, and the application of SNNs to multimodal scenarios remains largely unexplored. This paper presents SpikeCLIP, a novel framework designed to bridge the modality gap in spike-based computation. Our approach employs a two-step recipe: an ``alignment pre-training'' to align features across modalities, followed by a ``dual-loss fine-tuning'' to refine the model's performance. Extensive experiments reveal that SNNs achieve results on par with ANNs while substantially reducing energy consumption across various datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust image classification capabilities, even when dealing with classes that fall outside predefined categories. This study marks a significant advancement in the development of energy-efficient and biologically plausible multimodal learning systems. Our code is available at https://github.com/Lvchangze/SpikeCLIP.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing Medical Image Representations via Quaternion Wavelet Networks</title>
<link>https://arxiv.org/abs/2310.10224</link>
<guid>https://arxiv.org/abs/2310.10224</guid>
<content:encoded><![CDATA[
arXiv:2310.10224v5 Announce Type: replace-cross 
Abstract: Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency/approximation bands and high-frequency/fine-grained features. Then, it weighs the most representative set of sub-bands to be involved as input to any other neural model for image processing, replacing standard data samples. We conduct an extensive experimental evaluation comprising different datasets, diverse image analysis, and synthesis tasks including reconstruction, segmentation, and modality translation. We also evaluate QUAVE in combination with both real and quaternion-valued models. Results demonstrate the effectiveness and the generalizability of the proposed framework that improves network performance while being flexible to be adopted in manifold scenarios and robust to domain shifts. The full code is available at: https://github.com/ispamm/QWT.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review: Quantum Architecture Search with Unsupervised Representation Learning</title>
<link>https://arxiv.org/abs/2401.11576</link>
<guid>https://arxiv.org/abs/2401.11576</guid>
<content:encoded><![CDATA[
arXiv:2401.11576v4 Announce Type: replace-cross 
Abstract: Unsupervised representation learning presents new opportunities for advancing Quantum Architecture Search (QAS) on Noisy Intermediate-Scale Quantum (NISQ) devices. QAS is designed to optimize quantum circuits for Variational Quantum Algorithms (VQAs). Most QAS algorithms tightly couple the search space and search algorithm, typically requiring the evaluation of numerous quantum circuits, resulting in high computational costs and limiting scalability to larger quantum circuits. Predictor-based QAS algorithms mitigate this issue by estimating circuit performance based on structure or embedding. However, these methods often demand time-intensive labeling to optimize gate parameters across many circuits, which is crucial for training accurate predictors. Inspired by the classical neural architecture search algorithm Arch2vec, we investigate the potential of unsupervised representation learning for QAS without relying on predictors. Our framework decouples unsupervised architecture representation learning from the search process, enabling the learned representations to be applied across various downstream tasks. Additionally, it integrates an improved quantum circuit graph encoding scheme, addressing the limitations of existing representations and enhancing search efficiency. This predictor-free approach removes the need for large labeled datasets. During the search, we employ REINFORCE and Bayesian Optimization to explore the latent representation space and compare their performance against baseline methods. We further validate our approach by executing the best-discovered MaxCut circuits on IBM's ibm_sherbrooke quantum processor, confirming that the architectures retain optimal performance even under real hardware noise. Our results demonstrate that the framework efficiently identifies high-performing quantum circuits with fewer search iterations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Functional SDE approximation inspired by a deep operator network architecture</title>
<link>https://arxiv.org/abs/2402.03028</link>
<guid>https://arxiv.org/abs/2402.03028</guid>
<content:encoded><![CDATA[
arXiv:2402.03028v2 Announce Type: replace-cross 
Abstract: A novel approach to approximate solutions of Stochastic Differential Equations (SDEs) by Deep Neural Networks is derived and analysed. The architecture is inspired by the notion of Deep Operator Networks (DeepONets), which is based on operator learning in function spaces in terms of a reduced basis also represented in the network. In our setting, we make use of a polynomial chaos expansion (PCE) of stochastic processes and call the corresponding architecture SDEONet. The PCE has been used extensively in the area of uncertainty quantification (UQ) with parametric partial differential equations. This however is not the case with SDE, where classical sampling methods dominate and functional approaches are seen rarely. A main challenge with truncated PCEs occurs due to the drastic growth of the number of components with respect to the maximum polynomial degree and the number of basis elements. The proposed SDEONet architecture aims to alleviate the issue of exponential complexity by learning an optimal sparse truncation of the Wiener chaos expansion. A complete convergence and complexity analysis is presented, making use of recent Neural Network approximation results. Numerical experiments illustrate the promising performance of the suggested approach in 1D and higher dimensions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Memory Kernels in Generalized Langevin Equations</title>
<link>https://arxiv.org/abs/2402.11705</link>
<guid>https://arxiv.org/abs/2402.11705</guid>
<content:encoded><![CDATA[
arXiv:2402.11705v3 Announce Type: replace-cross 
Abstract: We introduce a novel approach for learning memory kernels in Generalized Langevin Equations. This approach initially utilizes a regularized Prony method to estimate correlation functions from trajectory data, followed by regression over a Sobolev norm-based loss function with RKHS regularization. Our method guarantees improved performance within an exponentially weighted L^2 space, with the kernel estimation error controlled by the error in estimated correlation functions. We demonstrate the superiority of our estimator compared to other regression estimators that rely on L^2 loss functions and also an estimator derived from the inverse Laplace transform, using numerical examples that highlight its consistent advantage across various weight parameter selections. Additionally, we provide examples that include the application of force and drift terms in the equation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Aware Remote Estimation of Multiple Markov Sources Under Constraints</title>
<link>https://arxiv.org/abs/2403.16855</link>
<guid>https://arxiv.org/abs/2403.16855</guid>
<content:encoded><![CDATA[
arXiv:2403.16855v2 Announce Type: replace-cross 
Abstract: This paper studies the remote estimation of multiple Markov sources over a lossy and rate-constrained channel. Unlike most existing studies that treat all source states equally, we exploit the \emph{semantics of information} and consider that the remote actuator has different tolerances for the estimation errors. We aim to find an optimal scheduling policy that minimizes the long-term \textit{state-dependent} costs of estimation errors under a transmission frequency constraint. The optimal scheduling problem is formulated as a \emph{constrained Markov decision process} (CMDP). We show that the optimal Lagrangian cost follows a piece-wise linear and concave (PWLC) function, and the optimal policy is, at most, a randomized mixture of two simple deterministic policies. By exploiting the structural results, we develop a new \textit{intersection search} algorithm that finds the optimal policy using only a few iterations. We further propose a reinforcement learning (RL) algorithm to compute the optimal policy without knowing \textit{a priori} the channel and source statistics. To avoid the ``curse of dimensionality" in MDPs, we propose an online low-complexity \textit{drift-plus-penalty} (DPP) algorithm. Numerical results show that continuous transmission is inefficient, and remarkably, our semantic-aware policies can attain the optimum by strategically utilizing fewer transmissions by exploiting the timing of the important information.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Memory Wall for Heterogeneous Federated Learning via Progressive Training</title>
<link>https://arxiv.org/abs/2404.13349</link>
<guid>https://arxiv.org/abs/2404.13349</guid>
<content:encoded><![CDATA[
arXiv:2404.13349v2 Announce Type: replace-cross 
Abstract: This paper presents ProFL, a new framework that effectively addresses the memory constraints in FL. Rather than updating the full model during local training, ProFL partitions the model into blocks based on its original architecture and trains each block in a progressive fashion. It first trains the front blocks and safely freezes them after convergence. Training of the next block is then triggered. This process progressively grows the model to be trained until the training of the full model is completed. In this way, the peak memory footprint is effectively reduced for feasible deployment on heterogeneous devices. In order to preserve the feature representation of each block, the training process is divided into two stages: model shrinking and model growing. During the model shrinking stage, we meticulously design corresponding output modules to assist each block in learning the expected feature representation and obtain the initialization model parameters. Subsequently, the obtained output modules and initialization model parameters are utilized in the corresponding model growing stage, which progressively trains the full model. Additionally, a novel metric from the scalar perspective is proposed to assess the learning status of each block, enabling us to securely freeze it after convergence and initiate the training of the next one. Finally, we theoretically prove the convergence of ProFL and conduct extensive experiments on representative models and datasets to evaluate its effectiveness. The results demonstrate that ProFL effectively reduces the peak memory footprint by up to 57.4% and improves model accuracy by up to 82.4%.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Clustering for Lung Cancer Image Classification via Sparse Solution Technique</title>
<link>https://arxiv.org/abs/2407.08800</link>
<guid>https://arxiv.org/abs/2407.08800</guid>
<content:encoded><![CDATA[
arXiv:2407.08800v2 Announce Type: replace-cross 
Abstract: In this work, we propose to use a local clustering approach based on the sparse solution technique to study the medical image, especially the lung cancer image classification task. We view images as the vertices in a weighted graph and the similarity between a pair of images as the edges in the graph. The vertices within the same cluster can be assumed to share similar features and properties, thus making the applications of graph clustering techniques very useful for image classification. Recently, the approach based on the sparse solutions of linear systems for graph clustering has been found to identify clusters more efficiently than traditional clustering methods such as spectral clustering. We propose to use the two newly developed local clustering methods based on sparse solution of linear system for image classification. In addition, we employ a box spline-based tight-wavelet-framelet method to clean these images and help build a better adjacency matrix before clustering. The performance of our methods is shown to be very effective in classifying images. Our approach is significantly more efficient and either favorable or equally effective compared with other state-of-the-art approaches. Finally, we shall make a remark by pointing out two image deformation methods to build up more artificial image data to increase the number of labeled images.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IntentRec: Predicting User Session Intent with Hierarchical Multi-Task Learning</title>
<link>https://arxiv.org/abs/2408.05353</link>
<guid>https://arxiv.org/abs/2408.05353</guid>
<content:encoded><![CDATA[
arXiv:2408.05353v2 Announce Type: replace-cross 
Abstract: Recommender systems have played a critical role in diverse digital services such as e-commerce, streaming media, social networks, etc. If we know what a user's intent is in a given session (e.g. do they want to watch short videos or a movie or play games; are they shopping for a camping trip), it becomes easier to provide high-quality recommendations. In this paper, we introduce IntentRec, a novel recommendation framework based on hierarchical multi-task neural network architecture that tries to estimate a user's latent intent using their short- and long-term implicit signals as proxies and uses the intent prediction to predict the next item user is likely to engage with. By directly leveraging the intent prediction, we can offer accurate and personalized recommendations to users. Our comprehensive experiments on Netflix user engagement data show that IntentRec outperforms the state-of-the-art next-item and next-intent predictors. We also share several findings and downstream applications of IntentRec.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An In-Depth Investigation of Data Collection in LLM App Ecosystems</title>
<link>https://arxiv.org/abs/2408.13247</link>
<guid>https://arxiv.org/abs/2408.13247</guid>
<content:encoded><![CDATA[
arXiv:2408.13247v2 Announce Type: replace-cross 
Abstract: LLM app (tool) ecosystems are rapidly evolving to support sophisticated use cases that often require extensive user data collection. Given that LLM apps are developed by third parties and anecdotal evidence indicating inconsistent enforcement of policies by LLM platforms, sharing user data with these apps presents significant privacy risks. In this paper, we aim to bring transparency in data practices of LLM app ecosystems. We examine OpenAI's GPT app ecosystem as a case study. We propose an LLM-based framework to analyze the natural language specifications of GPT Actions (custom tools) and assess their data collection practices. Our analysis reveals that Actions collect excessive data across 24 categories and 145 data types, with third-party Actions collecting 6.03% more data on average. We find that several Actions violate OpenAI's policies by collecting sensitive information, such as passwords, which is explicitly prohibited by OpenAI. Lastly, we develop an LLM-based privacy policy analysis framework to automatically check the consistency of data collection by Actions with disclosures in their privacy policies. Our measurements indicate that the disclosures for most of the collected data types are omitted, with only 5.8% of Actions clearly disclosing their data collection practices.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automate Strategy Finding with LLM in Quant Investment</title>
<link>https://arxiv.org/abs/2409.06289</link>
<guid>https://arxiv.org/abs/2409.06289</guid>
<content:encoded><![CDATA[
arXiv:2409.06289v3 Announce Type: replace-cross 
Abstract: We present a novel three-stage framework leveraging Large Language Models (LLMs) within a risk-aware multi-agent system for automate strategy finding in quantitative finance. Our approach addresses the brittleness of traditional deep learning models in financial applications by: employing prompt-engineered LLMs to generate executable alpha factor candidates across diverse financial data, implementing multimodal agent-based evaluation that filters factors based on market status, predictive quality while maintaining category balance, and deploying dynamic weight optimization that adapts to market conditions. Experimental results demonstrate the robust performance of the strategy in Chinese & US market regimes compared to established benchmarks. Our work extends LLMs capabilities to quantitative trading, providing a scalable architecture for financial signal extraction and portfolio construction. The overall framework significantly outperforms all benchmarks with 53.17% cumulative return on SSE50 (Jan 2023 to Jan 2024), demonstrating superior risk-adjusted performance and downside protection on the market.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning Large Language Models for Entity Matching</title>
<link>https://arxiv.org/abs/2409.08185</link>
<guid>https://arxiv.org/abs/2409.08185</guid>
<content:encoded><![CDATA[
arXiv:2409.08185v2 Announce Type: replace-cross 
Abstract: Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) the representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the models ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods, only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o-mini.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Computation with DVFS using Deep Reinforcement Learning for Multi-Task Systems in Edge Computing</title>
<link>https://arxiv.org/abs/2409.19434</link>
<guid>https://arxiv.org/abs/2409.19434</guid>
<content:encoded><![CDATA[
arXiv:2409.19434v3 Announce Type: replace-cross 
Abstract: Finding an optimal energy-efficient policy that is adaptable to underlying edge devices while meeting deadlines for tasks has always been challenging. This research studies generalized systems with multi-task, multi-deadline scenarios with reinforcement learning-based DVFS for energy saving for periodic soft real-time applications on edge devices. This work addresses the limitation of previous work that models a periodic system as a single task and single-deadline scenario, which is too simplified to cope with complex situations. The method encodes time series data in the Linux kernel into information that is easy to interpret for reinforcement learning, allowing the system to generate DVFS policies to adapt system patterns based on the general workload. For encoding, we present two different methods for comparison. Both methods use only one performance counter: system utilization, and the kernel only needs minimal information from the userspace. Our method is implemented on Jetson Nano Board (2GB) and is tested with three fixed multitask workloads, which are three, five, and eight tasks in the workload, respectively. For randomness and generalization, we also designed a random workload generator to build different multitask workloads to test. Based on the test results, our method could save 3%-10% power compared to Linux built-in governors.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry-Robust 3D Orientation Estimation</title>
<link>https://arxiv.org/abs/2410.02101</link>
<guid>https://arxiv.org/abs/2410.02101</guid>
<content:encoded><![CDATA[
arXiv:2410.02101v3 Announce Type: replace-cross 
Abstract: Orientation estimation is a fundamental task in 3D shape analysis which consists of estimating a shape's orientation axes: its side-, up-, and front-axes. Using this data, one can rotate a shape into canonical orientation, where its orientation axes are aligned with the coordinate axes. Developing an orientation algorithm that reliably estimates complete orientations of general shapes remains an open problem. We introduce a two-stage orientation pipeline that achieves state of the art performance on up-axis estimation and further demonstrate its efficacy on full-orientation estimation, where one seeks all three orientation axes. Unlike previous work, we train and evaluate our method on all of Shapenet rather than a subset of classes. We motivate our engineering contributions by theory describing fundamental obstacles to orientation estimation for rotationally-symmetric shapes, and show how our method avoids these obstacles.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look at Machine Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2410.08109</link>
<guid>https://arxiv.org/abs/2410.08109</guid>
<content:encoded><![CDATA[
arXiv:2410.08109v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrospective Learning from Interactions</title>
<link>https://arxiv.org/abs/2410.13852</link>
<guid>https://arxiv.org/abs/2410.13852</guid>
<content:encoded><![CDATA[
arXiv:2410.13852v2 Announce Type: replace-cross 
Abstract: Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy a relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. We introduce ReSpect, a method to learn from such signals in past interactions via retrospection without additional annotations. We deploy ReSpect in a new multimodal interaction scenario, where humans instruct a multimodal LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, we show how ReSpect gradually improves task completion rate from 31% to 82%, all without any external annotation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bilinear Sequence Regression: A Model for Learning from Long Sequences of High-dimensional Tokens</title>
<link>https://arxiv.org/abs/2410.18858</link>
<guid>https://arxiv.org/abs/2410.18858</guid>
<content:encoded><![CDATA[
arXiv:2410.18858v2 Announce Type: replace-cross 
Abstract: Current progress in artificial intelligence is centered around so-called large language models that consist of neural networks processing long sequences of high-dimensional vectors called tokens. Statistical physics provides powerful tools to study the functioning of learning with neural networks and has played a recognized role in the development of modern machine learning. The statistical physics approach relies on simplified and analytically tractable models of data. However, simple tractable models for long sequences of high-dimensional tokens are largely underexplored. Inspired by the crucial role models such as the single-layer teacher-student perceptron (aka generalized linear regression) played in the theory of fully connected neural networks, in this paper, we introduce and study the bilinear sequence regression (BSR) as one of the most basic models for sequences of tokens. We note that modern architectures naturally subsume the BSR model due to the skip connections. Building on recent methodological progress, we compute the Bayes-optimal generalization error for the model in the limit of long sequences of high-dimensional tokens, and provide a message-passing algorithm that matches this performance. We quantify the improvement that optimal learning brings with respect to vectorizing the sequence of tokens and learning via simple linear regression. We also unveil surprising properties of the gradient descent algorithms in the BSR model.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoStep: Locally adaptive involutive MCMC</title>
<link>https://arxiv.org/abs/2410.18929</link>
<guid>https://arxiv.org/abs/2410.18929</guid>
<content:encoded><![CDATA[
arXiv:2410.18929v3 Announce Type: replace-cross 
Abstract: Many common Markov chain Monte Carlo (MCMC) kernels can be formulated using a deterministic involutive proposal with a step size parameter. Selecting an appropriate step size is often a challenging task in practice; and for complex multiscale targets, there may not be one choice of step size that works well globally. In this work, we address this problem with a novel class of involutive MCMC methods -- AutoStep MCMC -- that selects an appropriate step size at each iteration adapted to the local geometry of the target distribution. We prove that under mild conditions AutoStep MCMC is $\pi$-invariant, irreducible, and aperiodic, and obtain bounds on expected energy jump distance and cost per iteration. Empirical results examine the robustness and efficacy of our proposed step size selection procedure, and show that AutoStep MCMC is competitive with state-of-the-art methods in terms of effective sample size per unit cost on a range of challenging target distributions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative Diffusion Model to Solve Inverse Problems for Robust in-NICU Neonatal MRI</title>
<link>https://arxiv.org/abs/2410.21602</link>
<guid>https://arxiv.org/abs/2410.21602</guid>
<content:encoded><![CDATA[
arXiv:2410.21602v2 Announce Type: replace-cross 
Abstract: We present the first acquisition-agnostic diffusion generative model for Magnetic Resonance Imaging (MRI) in the neonatal intensive care unit (NICU) to solve a range of inverse problems for shortening scan time and improving motion robustness. In-NICU MRI scanners leverage permanent magnets at lower field-strengths (i.e., below 1.5 Tesla) for non-invasive assessment of potential brain abnormalities during the critical phase of early live development, but suffer from long scan times and motion artifacts. In this setting, training data sizes are small and intrinsically suffer from low signal-to-noise ratio (SNR). This work trains a diffusion probabilistic generative model using such a real-world training dataset of clinical neonatal MRI by applying several novel signal processing and machine learning methods to handle the low SNR and low quantity of data. The model is then used as a statistical image prior to solve various inverse problems at inference time without requiring any retraining. Experiments demonstrate the generative model's utility for three real-world applications of neonatal MRI: accelerated reconstruction, motion correction, and super-resolution.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond The Rainbow: High Performance Deep Reinforcement Learning on a Desktop PC</title>
<link>https://arxiv.org/abs/2411.03820</link>
<guid>https://arxiv.org/abs/2411.03820</guid>
<content:encoded><![CDATA[
arXiv:2411.03820v2 Announce Type: replace-cross 
Abstract: Rainbow Deep Q-Network (DQN) demonstrated combining multiple independent enhancements could significantly boost a reinforcement learning (RL) agent's performance. In this paper, we present "Beyond The Rainbow" (BTR), a novel algorithm that integrates six improvements from across the RL literature to Rainbow DQN, establishing a new state-of-the-art for RL using a desktop PC, with a human-normalized interquartile mean (IQM) of 7.4 on Atari-60. Beyond Atari, we demonstrate BTR's capability to handle complex 3D games, successfully training agents to play Super Mario Galaxy, Mario Kart, and Mortal Kombat with minimal algorithmic changes. Designing BTR with computational efficiency in mind, agents can be trained using a high-end desktop PC on 200 million Atari frames within 12 hours. Additionally, we conduct detailed ablation studies of each component, analyzing the performance and impact using numerous measures. Code is available at https://github.com/VIPTankz/BTR.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization</title>
<link>https://arxiv.org/abs/2412.06141</link>
<guid>https://arxiv.org/abs/2412.06141</guid>
<content:encoded><![CDATA[
arXiv:2412.06141v3 Announce Type: replace-cross 
Abstract: The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize textual knowledge over visual input, leading to hallucinations that contradict information in medical images. Previous attempts to enhance modality alignment in Med-LVLMs through preference optimization have inadequately mitigated clinical relevance in preference data, making these samples easily distinguishable and reducing alignment effectiveness. To address this challenge, we propose MMedPO, a novel multimodal medical preference optimization approach that considers the clinical relevance of preference samples to enhance Med-LVLM alignment. MMedPO curates multimodal preference data by introducing two types of dispreference: (1) plausible hallucinations injected through target Med-LVLMs or GPT-4o to produce medically inaccurate responses, and (2) lesion region neglect achieved through local lesion-noising, disrupting visual understanding of critical areas. We then calculate clinical relevance for each sample based on scores from multiple Med-LLMs and visual tools, and integrate these scores into the preference optimization process as weights, enabling effective alignment. Our experiments demonstrate that MMedPO significantly enhances factual accuracy in Med-LVLMs, achieving substantial improvements over existing preference optimization methods by averaging 14.2% and 51.7% across the Med-VQA and report generation tasks. Our code are available in https://github.com/aiming-lab/MMedPO.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeSyA: Neurosymbolic Automata</title>
<link>https://arxiv.org/abs/2412.07331</link>
<guid>https://arxiv.org/abs/2412.07331</guid>
<content:encoded><![CDATA[
arXiv:2412.07331v2 Announce Type: replace-cross 
Abstract: Neurosymbolic (NeSy) AI has emerged as a promising direction to integrate neural and symbolic reasoning. Unfortunately, little effort has been given to developing NeSy systems tailored to sequential/temporal problems. We identify symbolic automata (which combine the power of automata for temporal reasoning with that of propositional logic for static reasoning) as a suitable formalism for expressing knowledge in temporal domains. Focusing on the task of sequence classification and tagging we show that symbolic automata can be integrated with neural-based perception, under probabilistic semantics towards an end-to-end differentiable model. Our proposed hybrid model, termed NeSyA (Neuro Symbolic Automata) is shown to either scale or perform more accurately than previous NeSy systems in a synthetic benchmark and to provide benefits in terms of generalization compared to purely neural systems in a real-world event recognition task.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for Customer Service Dialogues</title>
<link>https://arxiv.org/abs/2412.09049</link>
<guid>https://arxiv.org/abs/2412.09049</guid>
<content:encoded><![CDATA[
arXiv:2412.09049v3 Announce Type: replace-cross 
Abstract: Discovering customer intentions in dialogue conversations is crucial for automated service agents. However, existing intent clustering methods often fail to align with human perceptions due to a heavy reliance on embedding distance metrics and a tendency to overlook underlying semantic structures. This paper proposes an LLM-in-the-loop (LLM-ITL) intent clustering framework, integrating the semantic understanding capabilities of LLMs into conventional clustering algorithms. Specifically, this paper (1) investigates the effectiveness of fine-tuned LLMs in semantic coherence evaluation and intent cluster naming, achieving over 95% accuracy aligned with human judgments; (2) designs an LLM-ITL framework that facilitates the iterative discovery of coherent intent clusters and the optimal number of clusters; and (3) proposes context-aware techniques tailored for customer service dialogue. As existing English benchmarks offer limited semantic diversity and intent groups, we introduce a comprehensive Chinese dialogue intent dataset, comprising over 100k real customer service calls and 1,507 human-annotated intent clusters. The proposed approaches significantly outperform LLM-guided baselines, achieving notable enhancements in clustering quality and lower computational cost. Combined with several best practices, our findings highlight the potential of LLM-in-the-loop techniques for scalable and human-aligned intent clustering.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Novel Skills from Language-Generated Demonstrations</title>
<link>https://arxiv.org/abs/2412.09286</link>
<guid>https://arxiv.org/abs/2412.09286</guid>
<content:encoded><![CDATA[
arXiv:2412.09286v2 Announce Type: replace-cross 
Abstract: Robots are increasingly deployed across diverse domains to tackle tasks requiring novel skills. However, current robot learning algorithms for acquiring novel skills often rely on demonstration datasets or environment interactions, resulting in high labor costs and potential safety risks. To address these challenges, this study proposes DemoGen, a skill-learning framework that enables robots to acquire novel skills from natural language instructions. DemoGen leverages the vision-language model and the video diffusion model to generate demonstration videos of novel skills, which enabling robots to learn new skills effectively. Experimental evaluations in the MetaWorld simulation environments demonstrate the pipeline's capability to generate high-fidelity and reliable demonstrations. Using the generated demonstrations, various skill learning algorithms achieve an accomplishment rate three times the original on novel tasks. These results highlight a novel approach to robot learning, offering a foundation for the intuitive and intelligent acquisition of novel robotic skills. (Project website: https://aoqunjin.github.io/LNSLGD/)
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From KAN to GR-KAN: Advancing Speech Enhancement with KAN-Based Methodology</title>
<link>https://arxiv.org/abs/2412.17778</link>
<guid>https://arxiv.org/abs/2412.17778</guid>
<content:encoded><![CDATA[
arXiv:2412.17778v2 Announce Type: replace-cross 
Abstract: Deep neural network (DNN)-based speech enhancement (SE) usually uses conventional activation functions, which lack the expressiveness to capture complex multiscale structures needed for high-fidelity SE. Group-Rational KAN (GR-KAN), a variant of Kolmogorov-Arnold Networks (KAN), retains KAN's expressiveness while improving scalability on complex tasks. We adapt GR-KAN to existing DNN-based SE by replacing dense layers with GR-KAN layers in the time-frequency (T-F) domain MP-SENet and adapting GR-KAN's activations into the 1D CNN layers in the time-domain Demucs. Results on Voicebank-DEMAND show that GR-KAN requires up to 4x fewer parameters while improving PESQ by up to 0.1. In contrast, KAN, facing scalability issues, outperforms MLP on a small-scale signal modeling task but fails to improve MP-SENet. We demonstrate the first successful use of KAN-based methods for consistent improvement in both time- and SoTA TF-domain SE, establishing GR-KAN as a promising alternative for SE.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Log-Concavity and Score Regularity: Improved Convergence Bounds for Score-Based Generative Models in W2-distance</title>
<link>https://arxiv.org/abs/2501.02298</link>
<guid>https://arxiv.org/abs/2501.02298</guid>
<content:encoded><![CDATA[
arXiv:2501.02298v2 Announce Type: replace-cross 
Abstract: Score-based Generative Models (SGMs) aim to sample from a target distribution by learning score functions using samples perturbed by Gaussian noise. Existing convergence bounds for SGMs in the $\mathcal{W}_2$-distance rely on stringent assumptions about the data distribution. In this work, we present a novel framework for analyzing $\mathcal{W}_2$-convergence in SGMs, significantly relaxing traditional assumptions such as log-concavity and score regularity. Leveraging the regularization properties of the Ornstein--Uhlenbeck (OU) process, we show that weak log-concavity of the data distribution evolves into log-concavity over time. This transition is rigorously quantified through a PDE-based analysis of the Hamilton--Jacobi--Bellman equation governing the log-density of the forward process. Moreover, we establish that the drift of the time-reversed OU process alternates between contractive and non-contractive regimes, reflecting the dynamics of concavity. Our approach circumvents the need for stringent regularity conditions on the score function and its estimators, relying instead on milder, more practical assumptions. We demonstrate the wide applicability of this framework through explicit computations on Gaussian mixture models, illustrating its versatility and potential for broader classes of data distributions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABPT: Amended Backpropagation through Time with Partially Differentiable Rewards</title>
<link>https://arxiv.org/abs/2501.14513</link>
<guid>https://arxiv.org/abs/2501.14513</guid>
<content:encoded><![CDATA[
arXiv:2501.14513v2 Announce Type: replace-cross 
Abstract: Quadrotor control policies can be trained with high performance using the exact gradients of the rewards to directly optimize policy parameters via backpropagation-through-time (BPTT). However, designing a fully differentiable reward architecture is often challenging. Partially differentiable rewards will result in biased gradient propagation that degrades training performance. To overcome this limitation, we propose Amended Backpropagation-through-Time (ABPT), a novel approach that mitigates gradient bias while preserving the training efficiency of BPTT. ABPT combines 0-step and N-step returns, effectively reducing the bias by leveraging value gradients from the learned Q-value function. Additionally, it adopts entropy regularization and state initialization mechanisms to encourage exploration during training. We evaluate ABPT on four representative quadrotor flight tasks \li{in both real world and simulation}. Experimental results demonstrate that ABPT converges significantly faster and achieves higher ultimate rewards than existing learning algorithms, particularly in tasks involving partially differentiable rewards. The code will be released at http://github.com/Fanxing-LI/ABPT.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Quantum Reinforcement Learning</title>
<link>https://arxiv.org/abs/2501.15893</link>
<guid>https://arxiv.org/abs/2501.15893</guid>
<content:encoded><![CDATA[
arXiv:2501.15893v2 Announce Type: replace-cross 
Abstract: Benchmarking and establishing proper statistical validation metrics for reinforcement learning (RL) remain ongoing challenges, where no consensus has been established yet. The emergence of quantum computing and its potential applications in quantum reinforcement learning (QRL) further complicate benchmarking efforts. To enable valid performance comparisons and to streamline current research in this area, we propose a novel benchmarking methodology, which is based on a statistical estimator for sample complexity and a definition of statistical outperformance. Furthermore, considering QRL, our methodology casts doubt on some previous claims regarding its superiority. We conducted experiments on a novel benchmarking environment with flexible levels of complexity. While we still identify possible advantages, our findings are more nuanced overall. We discuss the potential limitations of these results and explore their implications for empirical research on quantum advantage in QRL.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lifelong Knowledge Editing requires Better Regularization</title>
<link>https://arxiv.org/abs/2502.01636</link>
<guid>https://arxiv.org/abs/2502.01636</guid>
<content:encoded><![CDATA[
arXiv:2502.01636v2 Announce Type: replace-cross 
Abstract: Knowledge editing is a promising way to improve factuality in large language models, but recent studies have shown significant model degradation during sequential editing. In this paper, we formalize the popular locate-then-edit methods as a two-step fine-tuning process, allowing us to precisely identify the root cause of this degradation. We show that model degradation occurs due to (1) over-optimization of internal activations and (2) continuous norm-growth of edited matrices. To mitigate these issues, we introduce two regularization techniques: (1) Most-Probable Early Stopping (MPES) and (2) explicit Frobenius norm-constraint. We demonstrate that applying these simple yet effective regularization techniques at key points in the editing process can substantially mitigate model degradation. Combining these regularization methods enables scaling locate-then-edit methods to 10,000 edits while reducing editing time by 42-61%. These results show that targeted regularization is essential for lifelong knowledge editing.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BARE: Leveraging Base Language Models for Few-Shot Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2502.01697</link>
<guid>https://arxiv.org/abs/2502.01697</guid>
<content:encoded><![CDATA[
arXiv:2502.01697v3 Announce Type: replace-cross 
Abstract: As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. However, current data generation methods rely on seed sets containing tens of thousands of examples to prompt instruction-tuned models. This reliance can be especially problematic when the curation of high-quality examples is expensive or difficult. In this paper we explore the novel few-shot synthetic data generation setting -- generating a high-quality dataset from a few examples. We show that when working with only a few seed examples, instruction-tuned models used in current synthetic data methods produce insufficient diversity for downstream tasks. In contrast, we show that base models without post-training, largely untapped for synthetic data generation, offer substantially greater output diversity, albeit with lower instruction following abilities. Leveraging this insight, we propose Base-Refine (BARE), a novel two-stage method that combines the diversity of base models with the quality assurance of instruction-tuned models. BARE excels in few-shot synthetic data generation: using only 3 seed examples it generates diverse, high-quality datasets that significantly improve downstream task performance. We show that fine-tuning Llama 3.1 8B with 1,000 BARE-generated samples achieves performance comparable to state-of-the-art similarly sized models on LiveCodeBench tasks. Furthermore, data generated with BARE enables a 101% improvement for a fine-tuned Llama 3.2 1B on GSM8K over data generated by only instruction-models, and an 18.4% improvement for a fine-tuned Llama 3.1 8B over the state-of-the-art RAFT method for RAG data generation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis for Reasoning Bias of Language Models with Small Initialization</title>
<link>https://arxiv.org/abs/2502.04375</link>
<guid>https://arxiv.org/abs/2502.04375</guid>
<content:encoded><![CDATA[
arXiv:2502.04375v2 Announce Type: replace-cross 
Abstract: Transformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Collusion by Collectives on Learning Platforms</title>
<link>https://arxiv.org/abs/2502.04879</link>
<guid>https://arxiv.org/abs/2502.04879</guid>
<content:encoded><![CDATA[
arXiv:2502.04879v2 Announce Type: replace-cross 
Abstract: As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence of TD(0) under Polynomial Mixing with Nonlinear Function Approximation</title>
<link>https://arxiv.org/abs/2502.05706</link>
<guid>https://arxiv.org/abs/2502.05706</guid>
<content:encoded><![CDATA[
arXiv:2502.05706v2 Announce Type: replace-cross 
Abstract: Temporal Difference Learning (TD(0)) is fundamental in reinforcement learning, yet its finite-sample behavior under non-i.i.d. data and nonlinear approximation remains unknown. We provide the first high-probability, finite-sample analysis of vanilla TD(0) on polynomially mixing Markov data, assuming only Holder continuity and bounded generalized gradients. This breaks with previous work, which often requires subsampling, projections, or instance-dependent step-sizes. Concretely, for mixing exponent $\beta > 1$, Holder continuity exponent $\gamma$, and step-size decay rate $\eta \in (1/2, 1]$, we show that, with high probability, \[ \| \theta_t - \theta^* \| \leq C(\beta, \gamma, \eta)\, t^{-\beta/2} + C'(\gamma, \eta)\, t^{-\eta\gamma} \] after $t = \mathcal{O}(1/\varepsilon^2)$ iterations. These bounds match the known i.i.d. rates and hold even when initialization is nonstationary. Central to our proof is a novel discrete-time coupling that bypasses geometric ergodicity, yielding the first such guarantee for nonlinear TD(0) under realistic mixing.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition</title>
<link>https://arxiv.org/abs/2502.10447</link>
<guid>https://arxiv.org/abs/2502.10447</guid>
<content:encoded><![CDATA[
arXiv:2502.10447v2 Announce Type: replace-cross 
Abstract: Audio-visual speech recognition (AVSR) has become critical for enhancing speech recognition in noisy environments by integrating both auditory and visual modalities. However, existing AVSR systems struggle to scale up without compromising computational efficiency. In this study, we introduce MoHAVE (Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework designed to address these scalability constraints. By leveraging a Mixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific expert groups, ensuring dynamic adaptation to various audio-visual inputs with minimal computational overhead. Key contributions of MoHAVE include: (1) a sparse MoE framework that efficiently scales AVSR model capacity, (2) a hierarchical gating mechanism that dynamically utilizes the expert groups based on input context, enhancing adaptability and robustness, and (3) remarkable performance across robust AVSR benchmarks, including LRS3 and MuAViC transcription and translation tasks, setting a new standard for scalable speech recognition systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Antimatter Annihilation Vertex Reconstruction with Deep Learning for ALPHA-g Radial Time Projection Chamber</title>
<link>https://arxiv.org/abs/2502.12169</link>
<guid>https://arxiv.org/abs/2502.12169</guid>
<content:encoded><![CDATA[
arXiv:2502.12169v2 Announce Type: replace-cross 
Abstract: The ALPHA-g experiment at CERN aims to precisely measure the terrestrial gravitational acceleration of antihydrogen atoms. A radial Time Projection Chamber (rTPC), that surrounds the ALPHA-g magnetic trap, is employed to determine the annihilation location, called the vertex. The standard approach requires identifying the trajectories of the ionizing particles in the rTPC from the location of their interaction in the gas (spacepoints), and inferring the vertex positions by finding the point where those trajectories (helices) pass closest to one another. In this work, we present a novel approach to vertex reconstruction using an ensemble of models based on the PointNet deep learning architecture. The newly developed model, PointNet Ensemble for Annihilation Reconstruction (PEAR), directly learns the relation between the location of the vertices and the rTPC spacepoints, thus eliminating the need to identify and fit the particle tracks. PEAR shows strong performance in reconstructing vertical vertex positions from simulated data, that is superior to the standard approach for all metrics considered. Furthermore, the deep learning approach can reconstruct the vertical vertex position when the standard approach fails.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid Word Learning Through Meta In-Context Learning</title>
<link>https://arxiv.org/abs/2502.14791</link>
<guid>https://arxiv.org/abs/2502.14791</guid>
<content:encoded><![CDATA[
arXiv:2502.14791v2 Announce Type: replace-cross 
Abstract: Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts. Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored. In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow). This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word. This training is repeated on many new words to develop a general word-learning ability. We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data. Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples. These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPD: Sync-Point Drop for efficient tensor parallelism of Large Language Models</title>
<link>https://arxiv.org/abs/2502.20727</link>
<guid>https://arxiv.org/abs/2502.20727</guid>
<content:encoded><![CDATA[
arXiv:2502.20727v3 Announce Type: replace-cross 
Abstract: With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD), to reduce communication overheads in tensor parallelism by selectively dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we apply different SPD strategies to attention blocks based on their sensitivity to the model accuracy. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for diverse distributed environments: SPD offered about 20% overall inference latency reduction with < 1% accuracy regression for LLaMA2-70B inference over 8 GPUs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating and Enhancing Vision-Audio Capability in Omnimodal Large Language Models</title>
<link>https://arxiv.org/abs/2503.00059</link>
<guid>https://arxiv.org/abs/2503.00059</guid>
<content:encoded><![CDATA[
arXiv:2503.00059v2 Announce Type: replace-cross 
Abstract: Omnimodal Large Language Models (OLLMs) have shown significant progress in integrating vision and text, but still struggle with integrating vision and audio, often exhibiting suboptimal performance when processing audio queries compared to text queries. This disparity is primarily due to insufficient alignment between vision and audio modalities during training, leading to inadequate attention to visual information when using audio queries. To mitigate this issue, we propose a Self-Knowledge Distillation (Self-KD) training method where the vision-text component of the OLLM serves as the teacher and the vision-audio component as the student. This enables the model to process audio in a manner analogous to its text processing. Our experimental results demonstrate that Self-KD is an effective method for enhancing the vision-audio capabilities of OLLMs by learning from the vision-text components, which subsequently improves the interaction between audio and images and results in improved performance on multimodal tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the statistical efficiency of cross-conformal prediction</title>
<link>https://arxiv.org/abs/2503.01495</link>
<guid>https://arxiv.org/abs/2503.01495</guid>
<content:encoded><![CDATA[
arXiv:2503.01495v2 Announce Type: replace-cross 
Abstract: Vovk (2015) introduced cross-conformal prediction, a modification of split conformal designed to improve the width of prediction sets. The method, when trained with a miscoverage rate equal to $\alpha$ and $n \gg K$, ensures a marginal coverage of at least $1 - 2\alpha - 2(1-\alpha)(K-1)/(n+K)$, where $n$ is the number of observations and $K$ denotes the number of folds. A simple modification of the method achieves coverage of at least $1-2\alpha$. In this work, we propose new variants of both methods that yield smaller prediction sets without compromising the latter theoretical guarantees. The proposed methods are based on recent results deriving more statistically efficient combination of p-values that leverage exchangeability and randomization. Simulations confirm the theoretical findings and bring out some important tradeoffs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptively profiling models with task elicitation</title>
<link>https://arxiv.org/abs/2503.01986</link>
<guid>https://arxiv.org/abs/2503.01986</guid>
<content:encoded><![CDATA[
arXiv:2503.01986v2 Announce Type: replace-cross 
Abstract: Language model evaluations often fail to characterize consequential failure modes, forcing experts to inspect outputs and build new benchmarks. We introduce task elicitation, a method that automatically builds new evaluations to profile model behavior. Task elicitation finds hundreds of natural-language tasks -- an order of magnitude more than prior work -- where frontier models exhibit systematic failures, in domains ranging from forecasting to online harassment. For example, we find that Sonnet 3.5 over-associates quantum computing and AGI and that o3-mini is prone to hallucination when fabrications are repeated in-context.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Many-Shot In-Context Learning with Self-Generated Annotations</title>
<link>https://arxiv.org/abs/2503.03062</link>
<guid>https://arxiv.org/abs/2503.03062</guid>
<content:encoded><![CDATA[
arXiv:2503.03062v2 Announce Type: replace-cross 
Abstract: The high cost of obtaining high-quality annotated data for in-context learning (ICL) has motivated the development of methods that use self-generated annotations in place of ground-truth labels. While these approaches have shown promising results in few-shot settings, they generally do not scale to many-shot scenarios. In this work, we study ICL with self-generated examples using a framework analogous to traditional semi-supervised learning, consisting of annotation generation, demonstration selection, and in-context inference. Within this framework, we propose a simple baseline that outperforms ground-truth ICL in zero-shot, few-shot, and many-shot settings. Notably, we observe a scaling law with this baseline, where optimal performance is achieved with more than 1,000 demonstrations. To fully exploit the many-shot capabilities of semi-supervised ICL, we introduce IterPSD, an iterative annotation approach that integrates iterative refinement and curriculum pseudo-labeling techniques from semi-supervised learning, yielding up to 6.8% additional gains on classification tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching</title>
<link>https://arxiv.org/abs/2503.05179</link>
<guid>https://arxiv.org/abs/2503.05179</guid>
<content:encoded><![CDATA[
arXiv:2503.05179v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have enabled strong reasoning capabilities through Chain-of-Thought (CoT) prompting, which elicits step-by-step problem solving, but often at the cost of excessive verbosity in intermediate outputs, leading to increased computational overhead. We propose Sketch-of-Thought (SoT), a prompting framework that integrates cognitively inspired reasoning paradigms with linguistic constraints to reduce token usage while preserving reasoning accuracy. SoT is designed as a flexible, modular approach and is instantiated with three paradigms--Conceptual Chaining, Chunked Symbolism, and Expert Lexicons--each tailored to distinct reasoning tasks and selected dynamically at test-time by a lightweight routing model. Across 15 reasoning datasets spanning multiple domains, languages, and modalities, SoT achieves token reductions of up to 78% with minimal accuracy loss. In tasks such as mathematical and multi-hop reasoning, it even improves accuracy while shortening outputs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource Heterogeneity-Aware and Utilization-Enhanced Scheduling for Deep Learning Clusters</title>
<link>https://arxiv.org/abs/2503.10918</link>
<guid>https://arxiv.org/abs/2503.10918</guid>
<content:encoded><![CDATA[
arXiv:2503.10918v2 Announce Type: replace-cross 
Abstract: Scheduling deep learning (DL) models to train on powerful clusters with accelerators like GPUs and TPUs, presently falls short, either lacking fine-grained heterogeneity awareness or leaving resources substantially under-utilized. To fill this gap, we propose a novel design of a task-level heterogeneity-aware scheduler, Hadar, based on an optimization framework that can boost resource utilization. Hadar leverages the performance traits of DL jobs on a heterogeneous DL cluster, characterizes the task-level performance heterogeneity in the optimization problem, and makes scheduling decisions across both spatial and temporal dimensions. It involves the primal-dual framework employing a dual subroutine, to solve the optimization problem and guide the scheduling design. Our trace-driven simulation with representative DL model training workloads demonstrates that Hadar accelerates the total time duration by 1.20x when compared with its state-of-the-art heterogeneity-aware counterpart, Gavel. Further, our Hadar scheduler is enhanced to HadarE by forking each job into multiple copies to let a job train concurrently on heterogeneous GPUs resided on separate available nodes (i.e., machines or servers) for resource utilization enhancement. HadarE is evaluated extensively on physical DL clusters for comparison with Hadar and Gavel. With substantial enhancement in cluster resource utilization (by 1.45x), HadarE exhibits considerable speed-ups in DL model training, reducing the total time duration by 50% (or 80%) on an Amazon's AWS (or our lab) cluster, while producing trained DL models with consistently better inference quality than those trained by Hadar.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast filtering of non-Gaussian models using Amortized Optimal Transport Maps</title>
<link>https://arxiv.org/abs/2503.12633</link>
<guid>https://arxiv.org/abs/2503.12633</guid>
<content:encoded><![CDATA[
arXiv:2503.12633v2 Announce Type: replace-cross 
Abstract: In this paper, we present the amortized optimal transport filter (A-OTF) designed to mitigate the computational burden associated with the real-time training of optimal transport filters (OTFs). OTFs can perform accurate non-Gaussian Bayesian updates in the filtering procedure, but they require training at every time step, which makes them expensive. The proposed A-OTF framework exploits the similarity between OTF maps during an initial/offline training stage in order to reduce the cost of inference during online calculations. More precisely, we use clustering algorithms to select relevant subsets of pre-trained maps whose weighted average is used to compute the A-OTF model akin to a mixture of experts. A series of numerical experiments validate that A-OTF achieves substantial computational savings during online inference while preserving the inherent flexibility and accuracy of OTF.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical clustering with maximum density paths and mixture models</title>
<link>https://arxiv.org/abs/2503.15582</link>
<guid>https://arxiv.org/abs/2503.15582</guid>
<content:encoded><![CDATA[
arXiv:2503.15582v2 Announce Type: replace-cross 
Abstract: Hierarchical clustering is an effective, interpretable method for analyzing structure in data. It reveals insights at multiple scales without requiring a predefined number of clusters and captures nested patterns and subtle relationships, which are often missed by flat clustering approaches. However, existing hierarchical clustering methods struggle with high-dimensional data, especially when there are no clear density gaps between modes. In this work, we introduce t-NEB, a probabilistically grounded hierarchical clustering method, which yields state-of-the-art clustering performance on naturalistic high-dimensional data. t-NEB consists of three steps: (1) density estimation via overclustering; (2) finding maximum density paths between clusters; (3) creating a hierarchical structure via bottom-up cluster merging. t-NEB uses a probabilistic parametric density model for both overclustering and cluster merging, which yields both high clustering performance and a meaningful hierarchy, making it a valuable tool for exploratory data analysis. Code is available at https://github.com/ecker-lab/tneb clustering.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design and Implementation of an FPGA-Based Hardware Accelerator for Transformer</title>
<link>https://arxiv.org/abs/2503.16731</link>
<guid>https://arxiv.org/abs/2503.16731</guid>
<content:encoded><![CDATA[
arXiv:2503.16731v3 Announce Type: replace-cross 
Abstract: Transformer-based large language models (LLMs) rely heavily on intensive matrix multiplications for attention and feed-forward layers, with the Q, K, and V linear projections in the Multi-Head Self-Attention (MHA) module constituting a decisive performance bottleneck. In this work, we introduce a highly optimized tiled matrix multiplication accelerator on a resource-constrained Xilinx KV260 FPGA that not only addresses this challenge but sets a new standard for efficiency and performance. Our design exploits persistent on-chip storage, a robust two-level tiling strategy for maximal data reuse, and a systolic-like unrolled compute engine that together deliver unparalleled speed and energy efficiency. Integrated with DistilBERT for Q, K, and V projections, our accelerator achieves an unequivocal 7x speedup over ARM CPU implementations (PyTorch) and an extraordinary 200x improvement over naive NumPy, reaching a throughput of up to 3.1~GFLOPs for matrix multiplications on (64,768) x (768,3072) matrices while operating at a conservative 100 MHz. These results decisively demonstrate the transformative potential of FPGA-based acceleration for critical Transformer operations, paving the way for scalable and energy-efficient deep learning inference on edge devices.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Benchmark for RNA 3D Structure-Function Modeling</title>
<link>https://arxiv.org/abs/2503.21681</link>
<guid>https://arxiv.org/abs/2503.21681</guid>
<content:encoded><![CDATA[
arXiv:2503.21681v2 Announce Type: replace-cross 
Abstract: The relationship between RNA structure and function has recently attracted interest within the deep learning community, a trend expected to intensify as nucleic acid structure models advance. Despite this momentum, a lack of standardized, accessible benchmarks for applying deep learning to RNA 3D structures hinders progress. To this end, we introduce a collection of seven benchmarking datasets specifically designed to support RNA structure-function prediction. Built on top of the established Python package rnaglib, our library streamlines data distribution and encoding, provides tools for dataset splitting and evaluation, and offers a comprehensive, user-friendly environment for model comparison. The modular and reproducible design of our datasets encourages community contributions and enables rapid customization. To demonstrate the utility of our benchmarks, we report baseline results for all tasks using a relational graph neural network.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Optimization on the Oblique Manifold for Sparse Simplex Constraints via Multiplicative Updates</title>
<link>https://arxiv.org/abs/2503.24075</link>
<guid>https://arxiv.org/abs/2503.24075</guid>
<content:encoded><![CDATA[
arXiv:2503.24075v2 Announce Type: replace-cross 
Abstract: Low-rank optimization problems with sparse simplex constraints involve variables that must satisfy nonnegativity, sparsity, and sum-to-one conditions, making their optimization particularly challenging due to the interplay between low-rank structures and constraints. These problems arise in various applications, including machine learning, signal processing, environmental fields, and computational biology. In this paper, we propose a novel manifold optimization approach to tackle these problems efficiently. Our method leverages the geometry of oblique rotation manifolds to reformulate the problem and introduces a new Riemannian optimization method based on Riemannian gradient descent that strictly maintains the simplex constraints. By exploiting the underlying manifold structure, our approach improves optimization efficiency. Experiments on synthetic datasets compared to standard Euclidean and Riemannian methods show the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think When You Need: Self-Adaptive Chain-of-Thought Learning</title>
<link>https://arxiv.org/abs/2504.03234</link>
<guid>https://arxiv.org/abs/2504.03234</guid>
<content:encoded><![CDATA[
arXiv:2504.03234v2 Announce Type: replace-cross 
Abstract: Chain of Thought (CoT) reasoning enhances language models' performance but often leads to inefficient "overthinking" on simple problems. We identify that existing approaches directly penalizing reasoning length fail to account for varying problem complexity. Our approach constructs rewards through length and quality comparisons, guided by theoretical assumptions that jointly enhance solution correctness with conciseness. Moreover, we further demonstrate our method to fuzzy tasks where ground truth is unavailable. Experiments across multiple reasoning benchmarks demonstrate that our method maintains accuracy while generating significantly more concise explanations, effectively teaching models to "think when needed."
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Pathology Foundation Model: Progress and Future Directions</title>
<link>https://arxiv.org/abs/2504.04045</link>
<guid>https://arxiv.org/abs/2504.04045</guid>
<content:encoded><![CDATA[
arXiv:2504.04045v2 Announce Type: replace-cross 
Abstract: Computational pathology, which involves analyzing whole slide images for automated cancer diagnosis, relies on multiple instance learning, where performance depends heavily on the feature extractor and aggregator. Recent Pathology Foundation Models (PFMs), pretrained on large-scale histopathology data, have significantly enhanced both the extractor and aggregator, but they lack a systematic analysis framework. In this survey, we present a hierarchical taxonomy organizing PFMs through a top-down philosophy applicable to foundation model analysis in any domain: model scope, model pretraining, and model design. Additionally, we systematically categorize PFM evaluation tasks into slide-level, patch-level, multimodal, and biological tasks, providing comprehensive benchmarking criteria. Our analysis identifies critical challenges in both PFM development (pathology-specific methodology, end-to-end pretraining, data-model scalability) and utilization (effective adaptation, model maintenance), paving the way for future directions in this promising field. Resources referenced in this survey are available at https://github.com/BearCleverProud/AwesomeWSI.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-guided Antibiotic Discovery Pipeline from Target Selection to Compound Identification</title>
<link>https://arxiv.org/abs/2504.11091</link>
<guid>https://arxiv.org/abs/2504.11091</guid>
<content:encoded><![CDATA[
arXiv:2504.11091v2 Announce Type: replace-cross 
Abstract: Antibiotic resistance presents a growing global health crisis, demanding new therapeutic strategies that target novel bacterial mechanisms. Recent advances in protein structure prediction and machine learning-driven molecule generation offer a promising opportunity to accelerate drug discovery. However, practical guidance on selecting and integrating these models into real-world pipelines remains limited. In this study, we develop an end-to-end, artificial intelligence-guided antibiotic discovery pipeline that spans target identification to compound realization. We leverage structure-based clustering across predicted proteomes of multiple pathogens to identify conserved, essential, and non-human-homologous targets. We then systematically evaluate six leading 3D-structure-aware generative models$\unicode{x2014}$spanning diffusion, autoregressive, graph neural network, and language model architectures$\unicode{x2014}$on their usability, chemical validity, and biological relevance. Rigorous post-processing filters and commercial analogue searches reduce over 100 000 generated compounds to a focused, synthesizable set. Our results highlight DeepBlock and TamGen as top performers across diverse criteria, while also revealing critical trade-offs between model complexity, usability, and output quality. This work provides a comparative benchmark and blueprint for deploying artificial intelligence in early-stage antibiotic development.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOLCE: Decomposing Off-Policy Evaluation/Learning into Lagged and Current Effects</title>
<link>https://arxiv.org/abs/2505.00961</link>
<guid>https://arxiv.org/abs/2505.00961</guid>
<content:encoded><![CDATA[
arXiv:2505.00961v2 Announce Type: replace-cross 
Abstract: Off-policy evaluation (OPE) and off-policy learning (OPL) for contextual bandit policies leverage historical data to evaluate and optimize a target policy. Most existing OPE/OPL methods--based on importance weighting or imputation--assume common support between the target and logging policies. When this assumption is violated, these methods typically require unstable extrapolation, truncation, or conservative strategies for individuals outside the common support assumption. However, such approaches can be inadequate in settings where explicit evaluation or optimization for such individuals is required. To address this issue, we propose DOLCE: Decomposing Off-policy evaluation/learning into Lagged and Current Effects, a novel estimator that leverages contextual information from multiple time points to decompose rewards into lagged and current effects. By incorporating both past and present contexts, DOLCE effectively handles individuals who violate the common support assumption. We show that the proposed estimator is unbiased under two assumptions--local correctness and conditional independence. Our experiments demonstrate that DOLCE achieves substantial improvements in OPE and OPL, particularly as the proportion of individuals outside the common support assumption increases.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs</title>
<link>https://arxiv.org/abs/2505.02009</link>
<guid>https://arxiv.org/abs/2505.02009</guid>
<content:encoded><![CDATA[
arXiv:2505.02009v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have become integral to various real-world applications, leveraging massive, web-sourced datasets like Common Crawl, C4, and FineWeb for pretraining. While these datasets provide linguistic data essential for high-quality natural language generation, they often contain harmful content, such as hate speech, misinformation, and biased narratives. Training LLMs on such unfiltered data risks perpetuating toxic behaviors, spreading misinformation, and amplifying societal biases which can undermine trust in LLM-driven applications and raise ethical concerns about their use. This paper presents a large-scale analysis of inappropriate content across these datasets, offering a comprehensive taxonomy that categorizes harmful webpages into Topical and Toxic based on their intent. We also introduce a prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and a transformer-based model (HarmFormer) for harmful content filtering. Additionally, we create a new multi-harm open-ended toxicity benchmark (HAVOC) and provide crucial insights into how models respond to adversarial toxic inputs. We share TTP, TTP-Eval, HAVOC and a sample of C4 inferenced on HarmFormer. Our work offers insights into ensuring safer LLM pretraining and serves as a resource for Responsible AI (RAI) compliance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG</title>
<link>https://arxiv.org/abs/2505.06569</link>
<guid>https://arxiv.org/abs/2505.06569</guid>
<content:encoded><![CDATA[
arXiv:2505.06569v2 Announce Type: replace-cross 
Abstract: Long-context large language models (LC LLMs) combined with retrieval-augmented generation (RAG) hold strong potential for complex multi-hop and large-document tasks. However, existing RAG systems often suffer from imprecise retrieval, incomplete context coverage under constrained windows, and fragmented information from suboptimal context construction. We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical RAG framework that compresses and partitions documents into coarse-to-fine granularities, then adaptively merges relevant contexts through real-time chunk- and document-level expansions. By initiating with finest-level retrieval and progressively incorporating broader, higher-level context, MacRAG constructs effective query-specific long contexts, optimizing both precision and coverage. Evaluations on challenging LongBench expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm MacRAG consistently surpasses baseline RAG pipelines in single- and multi-step generation using Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish MacRAG as an efficient, scalable solution for real-world long-context, multi-hop reasoning. Our code is available at https://github.com/Leezekun/MacRAG.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Balancing for Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.11117</link>
<guid>https://arxiv.org/abs/2505.11117</guid>
<content:encoded><![CDATA[
<div> PINNs, physics-informed neural networks, PDEs, dual-balanced PINN, imbalance mitigation<br />
Summary:<br />
Physics-informed neural networks (PINNs) have revolutionized the solving of partial differential equations (PDEs) but face accuracy and convergence challenges. The Dual-Balanced PINN (DB-PINN) proposed in this paper integrates inter-balancing and intra-balancing to address these issues. Inter-balancing mitigates gradient imbalances between PDE residual and condition-fitting losses, while intra-balancing allocates weights based on fitting difficulty. A robust weight update strategy prevents abrupt spikes. Extensive experiments show that DB-PINN outperforms popular weighting methods in both convergence speed and prediction accuracy. The code and supplementary material are available at https://github.com/chenhong-zhou/DualBalanced-PINNs.<br /> <div>
arXiv:2505.11117v3 Announce Type: replace 
Abstract: Physics-informed neural networks (PINNs) have emerged as a new learning paradigm for solving partial differential equations (PDEs) by enforcing the constraints of physical equations, boundary conditions (BCs), and initial conditions (ICs) into the loss function. Despite their successes, vanilla PINNs still suffer from poor accuracy and slow convergence due to the intractable multi-objective optimization issue. In this paper, we propose a novel Dual-Balanced PINN (DB-PINN), which dynamically adjusts loss weights by integrating inter-balancing and intra-balancing to alleviate two imbalance issues in PINNs. Inter-balancing aims to mitigate the gradient imbalance between PDE residual loss and condition-fitting losses by determining an aggregated weight that offsets their gradient distribution discrepancies. Intra-balancing acts on condition-fitting losses to tackle the imbalance in fitting difficulty across diverse conditions. By evaluating the fitting difficulty based on the loss records, intra-balancing can allocate the aggregated weight proportionally to each condition loss according to its fitting difficulty level. We further introduce a robust weight update strategy to prevent abrupt spikes and arithmetic overflow in instantaneous weight values caused by large loss variances, enabling smooth weight updating and stable training. Extensive experiments demonstrate that DB-PINN achieves significantly superior performance than those popular gradient-based weighting methods in terms of convergence speed and prediction accuracy. Our code and supplementary material are available at https://github.com/chenhong-zhou/DualBalanced-PINNs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity-aware clustering: Computational Complexity and Approximation Algorithms</title>
<link>https://arxiv.org/abs/2401.05502</link>
<guid>https://arxiv.org/abs/2401.05502</guid>
<content:encoded><![CDATA[
<div> complexity, diversity-aware clustering, approximation algorithm, parameterized approximation, NP-hardness

Summary:
This work investigates diversity-aware clustering problems where data points have multiple attributes and intersecting groups. The objective is to form clusters while adhering to lower and upper bound thresholds for each group and minimizing the clustering objective (e.g., $k$-median, $k$-means, $k$-supplier). The computational complexity of these problems is analyzed, revealing NP-hardness, polynomial-time inapproximability, and fixed-parameter intractability. The study presents parameterized approximation algorithms with approximation ratios approximately 1.736, 3.943, and 5 for diversity-aware $k$-median, diversity-aware $k$-means, and diversity-aware $k$-supplier, respectively. These approximation ratios are shown to be tight under Gap-ETH assumptions. The results also extend to fair variants of the problems with disjoint groups, such as fair $k$-median, fair $k$-means, and fair $k$-supplier, with lower bound requirements. <div>
arXiv:2401.05502v3 Announce Type: replace-cross 
Abstract: In this work, we study diversity-aware clustering problems where the data points are associated with multiple attributes resulting in intersecting groups. A clustering solution needs to ensure that the number of chosen cluster centers from each group should be within the range defined by a lower and upper bound threshold for each group, while simultaneously minimizing the clustering objective, which can be either $k$-median, $k$-means or $k$-supplier. We study the computational complexity of the proposed problems, offering insights into their NP-hardness, polynomial-time inapproximability, and fixed-parameter intractability. We present parameterized approximation algorithms with approximation ratios $1+ \frac{2}{e} + \epsilon \approx 1.736$, $1+\frac{8}{e} + \epsilon \approx 3.943$, and $5$ for diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware $k$-supplier, respectively. Assuming Gap-ETH, the approximation ratios are tight for the diversity-aware $k$-median and diversity-aware $k$-means problems. Our results imply the same approximation factors for their respective fair variants with disjoint groups -- fair $k$-median, fair $k$-means, and fair $k$-supplier -- with lower bound requirements.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisDiff: SDF-Guided Polygon Generation for Visibility Reconstruction and Recognition</title>
<link>https://arxiv.org/abs/2410.05530</link>
<guid>https://arxiv.org/abs/2410.05530</guid>
<content:encoded><![CDATA[
<div> diffusion-based approach, visibility reconstruction, combinatorial structures, representation learning, signed distance function

Summary:<br />
- The paper addresses the challenge of generating a polygon from a given visibility graph, focusing on structures without well-defined features and distance metrics.
- The VisDiff approach is proposed, which utilizes a diffusion-based method to estimate the signed distance function (SDF) associated with the polygon before extracting vertex locations.
- By learning the visibility relationship through the SDF, VisDiff outperforms existing methods, achieving a 26% improvement in F1-Score.
- A carefully curated dataset is created to train VisDiff and benchmark its performance against standard and state-of-the-art approaches.
- The novelty of incorporating the SDF in the generation process enhances the effectiveness of capturing the visibility relationship in combinatorial structures. 

Summary:<br />
The paper introduces the VisDiff approach to tackle the visibility reconstruction problem for polygons, emphasizing structures lacking well-defined features and distance metrics. By leveraging a diffusion-based method to estimate the signed distance function (SDF) before extracting vertex locations, VisDiff surpasses existing techniques with a 26% enhancement in F1-Score. The use of a meticulously curated dataset for training enables thorough benchmarking against standard and state-of-the-art methods. The innovative incorporation of the SDF in the generation process significantly enhances the capacity to capture visibility relationships in complex combinatorial structures. <div>
arXiv:2410.05530v3 Announce Type: replace-cross 
Abstract: The ability to capture rich representations of combinatorial structures has enabled the application of machine learning to tasks such as analysis and generation of floorplans, terrains, images, and animations. Recent work has primarily focused on understanding structures with well-defined features, neighborhoods, or underlying distance metrics, while those lacking such characteristics remain largely unstudied. Examples of these combinatorial structures can be found in polygons, where a small change in the vertex locations causes a significant rearrangement of the combinatorial structure, expressed as a visibility or triangulation graphs. Current representation learning approaches fail to capture structures without well-defined features and distance metrics. In this paper, we study the open problem of Visibility Reconstruction: Given a visibility graph $G$, construct a polygon $P$ whose visibility graph is $G$.
  We introduce VisDiff, a novel diffusion-based approach to generate polygon $P$ from the input visibility graph $G$. The main novelty of our approach is that, rather than generating the polygon's vertex set directly, we first estimate the signed distance function (SDF) associated with the polygon. The SDF is then used to extract the vertex location representing the final polygon. We show that going through the SDF allows VisDiff to learn the visibility relationship much more effectively than generating vertex locations directly. In order to train VisDiff, we create a carefully curated dataset. We use this dataset to benchmark our method and achieve 26% improvement in F1-Score over standard methods as well as state of the art approaches.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Action Learning Requires Supervision in the Presence of Distractors</title>
<link>https://arxiv.org/abs/2502.00379</link>
<guid>https://arxiv.org/abs/2502.00379</guid>
<content:encoded><![CDATA[
<div> latent action learning, LAPO, distractors, Distracting Control Suite, LAOM

Summary:

The study explores the impact of distractors on latent action learning, particularly addressing challenges that arise when real-world videos contain action-correlated distractions. The research introduces LAOM, a modified version of LAPO that significantly enhances the quality of latent actions. By using the Distracting Control Suite (DCS) for empirical analysis, the study reveals that LAPO struggles in the presence of distractors, emphasizing the importance of supervision with ground-truth actions during training. Surprisingly, providing supervision with as little as 2.5% of the full dataset improves downstream performance by 4.2 times on average. These results challenge the traditional approach of solely focusing on learning Latent Action Models (LAM) before decoding from latent to ground-truth actions, highlighting the critical role of incorporating supervision in the training phase. <br /><br />Summary: <div>
arXiv:2502.00379v4 Announce Type: replace-cross 
Abstract: Recently, latent action learning, pioneered by Latent Action Policies (LAPO), have shown remarkable pre-training efficiency on observation-only data, offering potential for leveraging vast amounts of video available on the web for embodied AI. However, prior work has focused on distractor-free data, where changes between observations are primarily explained by ground-truth actions. Unfortunately, real-world videos contain action-correlated distractors that may hinder latent action learning. Using Distracting Control Suite (DCS) we empirically investigate the effect of distractors on latent action learning and demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO modification that improves the quality of latent actions by 8x, as measured by linear probing. Importantly, we show that providing supervision with ground-truth actions, as few as 2.5% of the full dataset, during latent action learning improves downstream performance by 4.2x on average. Our findings suggest that integrating supervision during Latent Action Models (LAM) training is critical in the presence of distractors, challenging the conventional pipeline of first learning LAM and only then decoding from latent to ground-truth actions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding Assistants</title>
<link>https://arxiv.org/abs/2503.14281</link>
<guid>https://arxiv.org/abs/2503.14281</guid>
<content:encoded><![CDATA[
<div> Keywords: AI coding assistants, Cross-Origin Context Poisoning, adversarial code modifications, black-box attack algorithm, security measures

Summary: 
AI coding assistants used for tasks like code generation now gather automatic context from various sources, making them vulnerable to attacks like Cross-Origin Context Poisoning (XOXO). This attack manipulates coding assistants into generating incorrect outputs by subtly poisoning input with semantically equivalent adversarial code modifications. Traditional program analysis techniques struggle to detect these manipulations, allowing attackers to shift blame to victim developers. A novel black-box attack algorithm, GCGS, systematically explores the transformation space using a Cayley Graph, achieving a 75.72% success rate across multiple tasks and models. Defenses like adversarial fine-tuning are ineffective against this attack, highlighting the need for new security measures in AI coding tools. 

<br /><br />Summary: <div>
arXiv:2503.14281v3 Announce Type: replace-cross 
Abstract: AI coding assistants are widely used for tasks like code generation. These tools now require large and complex contexts, automatically sourced from various origins$\unicode{x2014}$across files, projects, and contributors$\unicode{x2014}$forming part of the prompt fed to underlying LLMs. This automatic context-gathering introduces new vulnerabilities, allowing attackers to subtly poison input to compromise the assistant's outputs, potentially generating vulnerable code or introducing critical errors. We propose a novel attack, Cross-Origin Context Poisoning (XOXO), that is challenging to detect as it relies on adversarial code modifications that are semantically equivalent. Traditional program analysis techniques struggle to identify these perturbations since the semantics of the code remains correct, making it appear legitimate. This allows attackers to manipulate coding assistants into producing incorrect outputs, while shifting the blame to the victim developer. We introduce a novel, task-agnostic, black-box attack algorithm GCGS that systematically searches the transformation space using a Cayley Graph, achieving a 75.72% attack success rate on average across five tasks and eleven models, including GPT 4.1 and Claude 3.5 Sonnet v2 used by popular AI coding assistants. Furthermore, defenses like adversarial fine-tuning are ineffective against our attack, underscoring the need for new security measures in LLM-powered coding tools.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRoQ-Loco: Generalist and Robot-agnostic Quadruped Locomotion Control using Offline Datasets</title>
<link>https://arxiv.org/abs/2505.10973</link>
<guid>https://arxiv.org/abs/2505.10973</guid>
<content:encoded><![CDATA[
<div> learn, locomotion, robotics, attention-based, transfer-learning

Summary:
- The researchers propose GRoQ-Loco, an attention-based framework for generalist locomotion policy learning across multiple quadruped robots and terrains using only offline datasets.
- The framework leverages expert demonstrations of stair traversal and flat terrain traversal behaviors to train a single generalist model without robot-specific encodings.
- The policy can be deployed on an Intel i7 nuc with low-latency control outputs without test-time optimization.
- Strong zero-shot transfer is demonstrated across diverse robots and terrains, including successful deployment on the Unitree Go1 robot.
- Successful transfer of flat walking and stair traversal behaviors is observed in challenging cross-robot training setups, with promising preliminary results on the Stoch 5 robot on various terrains <br /><br />Summary: <div>
arXiv:2505.10973v2 Announce Type: replace-cross 
Abstract: Recent advancements in large-scale offline training have demonstrated the potential of generalist policy learning for complex robotic tasks. However, applying these principles to legged locomotion remains a challenge due to continuous dynamics and the need for real-time adaptation across diverse terrains and robot morphologies. In this work, we propose GRoQ-Loco, a scalable, attention-based framework that learns a single generalist locomotion policy across multiple quadruped robots and terrains, relying solely on offline datasets. Our approach leverages expert demonstrations from two distinct locomotion behaviors - stair traversal (non-periodic gaits) and flat terrain traversal (periodic gaits) - collected across multiple quadruped robots, to train a generalist model that enables behavior fusion for both behaviors. Crucially, our framework operates directly on proprioceptive data from all robots without incorporating any robot-specific encodings. The policy is directly deployable on an Intel i7 nuc, producing low-latency control outputs without any test-time optimization. Our extensive experiments demonstrate strong zero-shot transfer across highly diverse quadruped robots and terrains, including hardware deployment on the Unitree Go1, a commercially available 12kg robot. Notably, we evaluate challenging cross-robot training setups where different locomotion skills are unevenly distributed across robots, yet observe successful transfer of both flat walking and stair traversal behaviors to all robots at test time. We also show preliminary walking on Stoch 5, a 70kg quadruped, on flat and outdoor terrains without requiring any fine tuning. These results highlight the potential for robust generalist locomotion across diverse robots and terrains.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Most General Explanations of Tree Ensembles (Extended Version)</title>
<link>https://arxiv.org/abs/2505.10991</link>
<guid>https://arxiv.org/abs/2505.10991</guid>
<content:encoded><![CDATA[
<div> Explainable Artificial Intelligence, XAI, formal approaches, abductive explanations, inflated explanations  
Summary:  
- Explainable Artificial Intelligence (XAI) is important for trust in AI systems.  
- Formal XAI models use abductive explanations to answer why decisions were made.  
- Inflated explanations are more general for numeric inputs.  
- This paper presents a method to find the most general abductive explanation for AI decisions.  
- The most general explanation covers a wide input space and is most likely to make sense to humans.  
<br /><br />Summary: <div>
arXiv:2505.10991v3 Announce Type: replace-cross 
Abstract: Explainable Artificial Intelligence (XAI) is critical for attaining trust in the operation of AI systems. A key question of an AI system is ``why was this decision made this way''. Formal approaches to XAI use a formal model of the AI system to identify abductive explanations. While abductive explanations may be applicable to a large number of inputs sharing the same concrete values, more general explanations may be preferred for numeric inputs. So-called inflated abductive explanations give intervals for each feature ensuring that any input whose values fall withing these intervals is still guaranteed to make the same prediction. Inflated explanations cover a larger portion of the input space, and hence are deemed more general explanations. But there can be many (inflated) abductive explanations for an instance. Which is the best? In this paper, we show how to find a most general abductive explanation for an AI decision. This explanation covers as much of the input space as possible, while still being a correct formal explanation of the model's behaviour. Given that we only want to give a human one explanation for a decision, the most general explanation gives us the explanation with the broadest applicability, and hence the one most likely to seem sensible. (The paper has been accepted at IJCAI2025 conference.)
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tuning Learning Rates with the Cumulative-Learning Constant</title>
<link>https://arxiv.org/abs/2505.13457</link>
<guid>https://arxiv.org/abs/2505.13457</guid>
<content:encoded><![CDATA[
<div> Keywords: learning rates, dataset sizes, training dynamics, learning constant, machine learning applications

Summary:
This paper presents a new approach to optimizing learning rates in machine learning by uncovering a direct relationship between learning rates and dataset sizes. The study also introduces a cumulative learning constant, which aids in the design and optimization of advanced learning rate schedules. By understanding the impact of dataset scale on training dynamics, this research provides valuable insights that can enhance training efficiency and performance in various machine learning applications. By leveraging these discoveries, researchers and practitioners can improve their training processes and achieve better results in a wide range of machine learning tasks.<br /><br />Summary: <div>
arXiv:2505.13457v1 Announce Type: new 
Abstract: This paper introduces a novel method for optimizing learning rates in machine learning. A previously unrecognized proportionality between learning rates and dataset sizes is discovered, providing valuable insights into how dataset scale influences training dynamics. Additionally, a cumulative learning constant is identified, offering a framework for designing and optimizing advanced learning rate schedules. These findings have the potential to enhance training efficiency and performance across a wide range of machine learning applications.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FPGA-based Acceleration for Convolutional Neural Networks: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2505.13461</link>
<guid>https://arxiv.org/abs/2505.13461</guid>
<content:encoded><![CDATA[
<div> Keywords: CNNs, FPGA, hardware accelerators, optimization strategies, performance evaluation

Summary:
Field-Programmable Gate Arrays (FPGAs) have become a popular hardware accelerator for Convolutional Neural Networks (CNNs) due to their reconfigurability, parallelism, and energy efficiency. This paper provides a comprehensive review of FPGA-based hardware accelerators optimized for CNNs. It outlines a performance evaluation framework based on existing studies and explores key optimization strategies like parallel computing, dataflow optimization, and hardware-software co-design. Additionally, the paper compares different FPGA architectures in terms of latency, throughput, compute efficiency, power consumption, and resource utilization. Future challenges and opportunities in the field are also discussed, highlighting the potential for continued innovation in FPGA-based hardware accelerators for CNNs.<br /><br />Summary: <div>
arXiv:2505.13461v1 Announce Type: new 
Abstract: Convolutional Neural Networks (CNNs) are fundamental to deep learning, driving applications across various domains. However, their growing complexity has significantly increased computational demands, necessitating efficient hardware accelerators. Field-Programmable Gate Arrays (FPGAs) have emerged as a leading solution, offering reconfigurability, parallelism, and energy efficiency. This paper provides a comprehensive review of FPGA-based hardware accelerators specifically designed for CNNs. It presents and summarizes the performance evaluation framework grounded in existing studies and explores key optimization strategies, such as parallel computing, dataflow optimization, and hardware-software co-design. It also compares various FPGA architectures in terms of latency, throughput, compute efficiency, power consumption, and resource utilization. Finally, the paper highlights future challenges and opportunities, emphasizing the potential for continued innovation in this field.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-end fully-binarized network design: from Generic Learned Thermometer to Block Pruning</title>
<link>https://arxiv.org/abs/2505.13462</link>
<guid>https://arxiv.org/abs/2505.13462</guid>
<content:encoded><![CDATA[
<div> Keywords: Binary Neural Network, Generic Learned Thermometer, Data binarization, Compact topology, Block pruning<br />
Summary: <br />
This article introduces a novel technique called Generic Learned Thermometer (GLT) to improve input data representation for Binary Neural Networks (BNN). GLT relies on learning non-linear quantization thresholds through multiple data binarizations, replacing traditional Analog to Digital Conversion (ADC). The proposed compact topology includes light-weight grouped convolutions trained using block pruning and Knowledge Distillation (KD) to reduce model size and computational complexity. The study demonstrates that GLT enhances the versatility of BNNs by performing global tone mapping, leading to significant accuracy improvements on datasets like STL-10 and VWW. By combining GLT with block pruning, the researchers successfully develop lightweight, fully-binarized models under 1Mb in size that are suitable for in-sensor always-on inference applications. <div>
arXiv:2505.13462v1 Announce Type: new 
Abstract: Existing works on Binary Neural Network (BNN) mainly focus on model's weights and activations while discarding considerations on the input raw data. This article introduces Generic Learned Thermometer (GLT), an encoding technique to improve input data representation for BNN, relying on learning non linear quantization thresholds. This technique consists in multiple data binarizations which can advantageously replace a conventional Analog to Digital Conversion (ADC) that uses natural binary coding. Additionally, we jointly propose a compact topology with light-weight grouped convolutions being trained thanks to block pruning and Knowledge Distillation (KD), aiming at reducing furthermore the model size so as its computational complexity. We show that GLT brings versatility to the BNN by intrinsically performing global tone mapping, enabling significant accuracy gains in practice (demonstrated by simulations on the STL-10 and VWW datasets). Moreover, when combining GLT with our proposed block-pruning technique, we successfully achieve lightweight (under 1Mb), fully-binarized models with limited accuracy degradation while being suitable for in-sensor always-on inference use cases.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting The Evolution of Interfaces with Fourier Neural Operators</title>
<link>https://arxiv.org/abs/2505.13463</link>
<guid>https://arxiv.org/abs/2505.13463</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, neural operators, partial differential equations, multi-phase flows, computational fluid dynamics

Summary: 
This article discusses the use of neural operators in predicting the evolution of partial differential equations, particularly in complex problems like liquid-vapour multiphase flows. The traditional computational fluid dynamics models have limitations in forecasting abilities for fast industrial processes due to their inability to handle strong discontinuities in the computational domain. Neural operators show promise in bridging this gap by demonstrating a comparable time scale for predictions as required by multi-phase applications. These operators can be trained using various methods, including experimental data and simulations. In this study, neural operators were trained in volume of fluid simulations, resulting in highly accurate predictions, especially in forecasting the liquid-vapour interface evolution crucial for multi-phase process control. This research paves the way for the application of neural operators in controlling processes that demand quick responses. 

Summary: <div>
arXiv:2505.13463v1 Announce Type: new 
Abstract: Recent progress in AI has established neural operators as powerful tools that can predict the evolution of partial differential equations, such as the Navier-Stokes equations. Some complex problems rely on sophisticated algorithms to deal with strong discontinuities in the computational domain. For example, liquid-vapour multiphase flows are a challenging problem in many configurations, particularly those involving large density gradients or phase change. The complexity mentioned above has not allowed for fine control of fast industrial processes or applications because computational fluid dynamics (CFD) models do not have a quick enough forecasting ability. This work demonstrates that the time scale of neural operators-based predictions is comparable to the time scale of multi-phase applications, thus proving they can be used to control processes that require fast response. Neural Operators can be trained using experimental data, simulations or a combination. In the following, neural operators were trained in volume of fluid simulations, and the resulting predictions showed very high accuracy, particularly in predicting the evolution of the liquid-vapour interface, one of the most critical tasks in a multi-phase process controller.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Spotlight Resonance Method: Resolving the Alignment of Embedded Activations</title>
<link>https://arxiv.org/abs/2505.13471</link>
<guid>https://arxiv.org/abs/2505.13471</guid>
<content:encoded><![CDATA[
<div> visualisation tool, deep learning model, axis alignment, privileged basis, activation functions 

Summary: 
This paper introduces a novel visualisation tool for understanding how deep learning models represent data by determining the axis alignment of embedded data at any layer. The tool evaluates the distribution around planes defined by the network's privileged basis vectors, providing a comprehensive metric for interpreting activation distribution. It shows that representations tend to align with the privileged basis vectors, which are influenced by activation functions. This explains the observed tendency of representations to align with neurons and uncovers the causal link between functional form symmetry breaking and representational alignment. The method also identifies grandmother neurons in various networks. <div>
arXiv:2505.13471v1 Announce Type: new 
Abstract: Understanding how deep learning models represent data is currently difficult due to the limited number of methodologies available. This paper demonstrates a versatile and novel visualisation tool for determining the axis alignment of embedded data at any layer in any deep learning model. In particular, it evaluates the distribution around planes defined by the network's privileged basis vectors. This method provides both an atomistic and a holistic, intuitive metric for interpreting the distribution of activations across all planes. It ensures that both positive and negative signals contribute, treating the activation vector as a whole. Depending on the application, several variations of this technique are presented, with a resolution scale hyperparameter to probe different angular scales. Using this method, multiple examples are provided that demonstrate embedded representations tend to be axis-aligned with the privileged basis. This is not necessarily the standard basis, and it is found that activation functions directly result in privileged bases. Hence, it provides a direct causal link between functional form symmetry breaking and representational alignment, explaining why representations have a tendency to align with the neuron basis. Therefore, using this method, we begin to answer the fundamental question of what causes the observed tendency of representations to align with neurons. Finally, examples of so-called grandmother neurons are found in a variety of networks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency</title>
<link>https://arxiv.org/abs/2505.13499</link>
<guid>https://arxiv.org/abs/2505.13499</guid>
<content:encoded><![CDATA[
<div> Transformers, optimal control theory, training, architecture design, theoretical guarantees<br />
<br />
Summary:<br />
This study presents a new framework that applies optimal control theory to Transformers, improving performance while ensuring generalization and robustness. By integrating seamlessly with existing models and requiring minimal changes, the framework demonstrated significant enhancements in various tasks such as text generation, sentiment analysis, image classification, and point cloud classification. Experiments showed a reduction in test loss for nanoGPT and GPT-2 models while being more parameter-efficient. The framework offers a systematic, theory-driven approach to improving Transformer models, moving away from costly trial-and-error methods. This work represents the first application of optimal control theory to both training and architecture of Transformers, providing a new foundation for future advancements in the field. <div>
arXiv:2505.13499v1 Announce Type: new 
Abstract: We study Transformers through the perspective of optimal control theory, using tools from continuous-time formulations to derive actionable insights into training and architecture design. This framework improves the performance of existing Transformer models while providing desirable theoretical guarantees, including generalization and robustness. Our framework is designed to be plug-and-play, enabling seamless integration with established Transformer models and requiring only slight changes to the implementation. We conduct seven extensive experiments on tasks motivated by text generation, sentiment analysis, image classification, and point cloud classification. Experimental results show that the framework improves the test performance of the baselines, while being more parameter-efficient. On character-level text generation with nanoGPT, our framework achieves a 46% reduction in final test loss while using 42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction in final test loss, demonstrating scalability to larger models. To the best of our knowledge, this is the first work that applies optimal control theory to both the training and architecture of Transformers. It offers a new foundation for systematic, theory-driven improvements and moves beyond costly trial-and-error approaches.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPIEDiff: robust learning of long-time macroscopic dynamics from short-time particle simulations with quantified epistemic uncertainty</title>
<link>https://arxiv.org/abs/2505.13501</link>
<guid>https://arxiv.org/abs/2505.13501</guid>
<content:encoded><![CDATA[
<div> Statistical-Physics Informed Epistemic Diffusion Models, SPIEDiff, dissipative systems, particle fidelity, machine learning framework<br />
<br />
Summary:<br />
The paper introduces Statistical-Physics Informed Epistemic Diffusion Models (SPIEDiff) to address challenges in discovering long-time macroscopic dynamics and thermodynamics of dissipative systems. SPIEDiff leverages statistical physics, conditional diffusion models, and epinets to overcome limitations such as time-scale constraints in particle simulations, non-uniqueness of thermodynamic potentials, and efficient uncertainty quantification. Through evaluation on stochastic Arrhenius particle processes, SPIEDiff accurately uncovers thermodynamics and kinetics, facilitating reliable long-time macroscopic predictions using short-time particle simulation data. It provides accurate predictions with quantified uncertainty in minutes, significantly reducing computational demand compared to direct particle simulations. SPIEDiff offers a robust and trustworthy approach for data-driven discovery of thermodynamic models. <br /><br />Summary: <div>
arXiv:2505.13501v1 Announce Type: new 
Abstract: The data-driven discovery of long-time macroscopic dynamics and thermodynamics of dissipative systems with particle fidelity is hampered by significant obstacles. These include the strong time-scale limitations inherent to particle simulations, the non-uniqueness of the thermodynamic potentials and operators from given macroscopic dynamics, and the need for efficient uncertainty quantification. This paper introduces Statistical-Physics Informed Epistemic Diffusion Models (SPIEDiff), a machine learning framework designed to overcome these limitations in the context of purely dissipative systems by leveraging statistical physics, conditional diffusion models, and epinets. We evaluate the proposed framework on stochastic Arrhenius particle processes and demonstrate that SPIEDiff can accurately uncover both thermodynamics and kinetics, while enabling reliable long-time macroscopic predictions using only short-time particle simulation data. SPIEDiff can deliver accurate predictions with quantified uncertainty in minutes, drastically reducing the computational demand compared to direct particle simulations, which would take days or years in the examples considered. Overall, SPIEDiff offers a robust and trustworthy pathway for the data-driven discovery of thermodynamic models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Low-Rank Adaptation for Foundation Models: A Survey</title>
<link>https://arxiv.org/abs/2505.13502</link>
<guid>https://arxiv.org/abs/2505.13502</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Low-Rank Adaptation, Fine-tuning, Data Privacy, Efficiency

Summary: 
Federated Learning (FL) and Low-Rank Adaptation (LoRA) are two collaborative frameworks that aim to fine-tune foundation models while addressing data privacy concerns and reducing trainable parameters. The integration of LoRA into federated fine-tuning for foundation models, known as FedLoRA, faces challenges in distributed learning, heterogeneity, and efficiency. Existing research in this area is categorized based on the methods used to tackle these challenges. The survey highlights open research questions and suggests future directions for advancing FedLoRA, emphasizing the need for efficiency and data privacy in fine-tuning processes. This comprehensive overview sheds light on the current state of FedLoRA and provides a roadmap for further investigation in this field. 

<br /><br />Summary: <div>
arXiv:2505.13502v1 Announce Type: new 
Abstract: Effectively leveraging private datasets remains a significant challenge in developing foundation models. Federated Learning (FL) has recently emerged as a collaborative framework that enables multiple users to fine-tune these models while mitigating data privacy risks. Meanwhile, Low-Rank Adaptation (LoRA) offers a resource-efficient alternative for fine-tuning foundation models by dramatically reducing the number of trainable parameters. This survey examines how LoRA has been integrated into federated fine-tuning for foundation models, an area we term FedLoRA, by focusing on three key challenges: distributed learning, heterogeneity, and efficiency. We further categorize existing work based on the specific methods used to address each challenge. Finally, we discuss open research questions and highlight promising directions for future investigation, outlining the next steps for advancing FedLoRA.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation</title>
<link>https://arxiv.org/abs/2505.13507</link>
<guid>https://arxiv.org/abs/2505.13507</guid>
<content:encoded><![CDATA[
<div> Prompt-driven cross-domain alignment, Learnable textual prompts, Contrastive Language-Image Pretraining, Gradient-aware open-set separation, Office-Home dataset <br />
Summary: 
- Learnable textual prompts based on domain discrepancy metrics dynamically adapt the CLIP text encoder for semantic consistency between domains without explicit unknown-class supervision.
- Gradient analysis module quantifies domain shift by comparing gradients' L2-norm from learned prompts, enabling distinct gradient behaviors for known and unknown samples.
- Proposed method outperforms CLIP baseline and standard methods on Office-Home dataset evaluations.
- Ablation studies highlight the critical role of gradient norms in the approach's success. <div>
arXiv:2505.13507v1 Announce Type: new 
Abstract: Open-Set Domain Adaptation (OSDA) confronts the dual challenge of aligning known-class distributions across domains while identifying target-domain-specific unknown categories. Current approaches often fail to leverage semantic relationships between modalities and struggle with error accumulation in unknown sample detection. We propose to harness Contrastive Language-Image Pretraining (CLIP) to address these limitations through two key innovations: 1) Prompt-driven cross-domain alignment: Learnable textual prompts conditioned on domain discrepancy metrics dynamically adapt CLIP's text encoder, enabling semantic consistency between source and target domains without explicit unknown-class supervision. 2) Gradient-aware open-set separation: A gradient analysis module quantifies domain shift by comparing the L2-norm of gradients from the learned prompts, where known/unknown samples exhibit statistically distinct gradient behaviors. Evaluations on Office-Home show that our method consistently outperforms CLIP baseline and standard baseline. Ablation studies confirm the gradient norm's critical role.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the definition and importance of interpretability in scientific machine learning</title>
<link>https://arxiv.org/abs/2505.13510</link>
<guid>https://arxiv.org/abs/2505.13510</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, interpretability, physical sciences, equation discovery, mechanism

<br />
Summary:
Researchers have noted a disconnect between neural networks' success in predicting physical phenomena and their lack of transparent, integratable findings. Interpretability, often equated with sparsity, plays a crucial role in addressing this issue. However, existing definitions of interpretability from non-scientific fields may not fully apply to scientific machine learning (SciML). This work proposes an operational definition of interpretability for the physical sciences, emphasizing understanding the mechanism over mathematical sparsity. The focus on mechanisms highlights that sparsity is not always necessary for interpretability and raises questions about making interpretable scientific discoveries without prior knowledge. Developing a precise definition of interpretability in SciML can help researchers better address challenges in leveraging data-driven approaches for scientific advancement.
<br /><br />Summary: <div>
arXiv:2505.13510v1 Announce Type: new 
Abstract: Though neural networks trained on large data sets have been successfully used to describe and predict many physical phenomena, there is a sense among scientists that, unlike traditional scientific models, where relationships come packaged in the form of simple mathematical expressions, the findings of the neural network cannot be integrated into the body of scientific knowledge. Critics of ML's inability to produce human-understandable relationships have converged on the concept of "interpretability" as its point of departure from more traditional forms of science. As the growing interest in interpretability has shown, researchers in the physical sciences seek not just predictive models, but also to uncover the fundamental principles that govern a system of interest. However, clarity around a definition of interpretability and the precise role that it plays in science is lacking in the literature. In this work, we argue that researchers in equation discovery and symbolic regression tend to conflate the concept of sparsity with interpretability. We review key papers on interpretable ML from outside the scientific community and argue that, though the definitions and methods they propose can inform questions of interpretability for SciML, they are inadequate for this new purpose. Noting these deficiencies, we propose an operational definition of interpretability for the physical sciences. Our notion of interpretability emphasizes understanding of the mechanism over mathematical sparsity. Innocuous though it may seem, this emphasis on mechanism shows that sparsity is often unnecessary. It also questions the possibility of interpretable scientific discovery when prior knowledge is lacking. We believe a precise and philosophically informed definition of interpretability in SciML will help focus research efforts toward the most significant obstacles to realizing a data-driven scientific future.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades</title>
<link>https://arxiv.org/abs/2505.13515</link>
<guid>https://arxiv.org/abs/2505.13515</guid>
<content:encoded><![CDATA[
<div> transfer matrix, LoRASuite, fine-tuning, large language models, computational efficiency<br />
Summary:<br />
The article introduces LoRASuite, a modular approach designed to efficiently adapt existing LoRA weights to newer versions of Large Language Models (LLMs). By utilizing transfer matrices, centered kernel alignment, and cosine similarity metrics, LoRASuite allocates corresponding layers and attention heads, followed by skillful fine-tuning for numerical stability. Experimental results show that LoRASuite outperforms small-scale vanilla LoRA methods, surpassing even full-scale LoRA retraining on backbone LLMs like MiniCPM and Qwen, with notable improvements on math tasks. Furthermore, LoRASuite significantly reduces memory consumption and computational time, making it a cost-effective and time-efficient solution for adapting to evolving LLM updates. <br /><br />Summary: <div>
arXiv:2505.13515v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are frequently updated, LoRA weights trained on earlier versions quickly become obsolete. The conventional practice of retraining LoRA weights from scratch on the latest model is costly, time-consuming, and environmentally detrimental, particularly as the diversity of LLMs and downstream tasks expands. This motivates a critical question: "How can we efficiently leverage existing LoRA weights to adapt to newer model versions?" To address this, we propose LoRASuite, a modular approach tailored specifically to various types of LLM updates. First, we compute a transfer matrix utilizing known parameters from both old and new LLMs. Next, we allocate corresponding layers and attention heads based on centered kernel alignment and cosine similarity metrics, respectively. A subsequent small-scale, skillful fine-tuning step ensures numerical stability. Experimental evaluations demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even exceeds the performance of full-scale LoRA retraining, with average improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally, LoRASuite significantly reduces memory consumption by 5.5 GB and computational time by 78.23%.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Forecasting Mortality Rates: A Global Study</title>
<link>https://arxiv.org/abs/2505.13521</link>
<guid>https://arxiv.org/abs/2505.13521</guid>
<content:encoded><![CDATA[
<div> CHRONOS, TimesFM, mortality rates, zero-shot forecasting, Random Forest model
Summary:
Zero-shot time series forecasting, using pre-trained models, was studied for predicting mortality rates without task-specific fine-tuning. CHRONOS outperformed traditional methods for shorter-term forecasts, while TimesFM consistently underperformed. Fine-tuning CHRONOS with mortality data improved long-term accuracy significantly. A Random Forest model trained on mortality data yielded the best overall performance. The study highlights the potential of zero-shot forecasting but stresses the importance of careful model selection and domain-specific adaptation.<br /><br />Summary: <div>
arXiv:2505.13521v1 Announce Type: new 
Abstract: This study explores the potential of zero-shot time series forecasting, an innovative approach leveraging pre-trained foundation models, to forecast mortality rates without task-specific fine-tuning. We evaluate two state-of-the-art foundation models, TimesFM and CHRONOS, alongside traditional and machine learning-based methods across three forecasting horizons (5, 10, and 20 years) using data from 50 countries and 111 age groups. In our investigations, zero-shot models showed varying results: while CHRONOS delivered competitive shorter-term forecasts, outperforming traditional methods like ARIMA and the Lee-Carter model, TimesFM consistently underperformed. Fine-tuning CHRONOS on mortality data significantly improved long-term accuracy. A Random Forest model, trained on mortality data, achieved the best overall performance. These findings underscore the potential of zero-shot forecasting while highlighting the need for careful model selection and domain-specific adaptation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-head Temporal Latent Attention</title>
<link>https://arxiv.org/abs/2505.13544</link>
<guid>https://arxiv.org/abs/2505.13544</guid>
<content:encoded><![CDATA[
<div> cache, inference efficiency, memory footprint, multi-head attention, speech translation
<br />
Multi-head Temporal Latent Attention (MTLA) is introduced as a method to reduce the Key-Value (KV) cache size in Transformer self-attention, particularly along the temporal dimension. By utilizing a hyper-network to merge adjacent KV cache vectors and implementing a stride-aware causal mask for efficient training, MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA) while significantly improving inference speed and GPU memory usage. Experimental results across various tasks like speech translation, recognition, understanding, and text summarization show that MTLA offers notable speedup and memory reduction benefits without compromising translation quality. On a specific English-German speech translation task, MTLA demonstrates a 5.3x increase in speed and an impressive 8.3x reduction in GPU memory usage compared to MHA. 
<br /><br />Summary: <div>
arXiv:2505.13544v1 Announce Type: new 
Abstract: While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3x speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Federated Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2505.13547</link>
<guid>https://arxiv.org/abs/2505.13547</guid>
<content:encoded><![CDATA[
<div> pruning, LLMs, Federated learning, privacy-preserving compression, collaborative pruning<br />
Summary:<br />
FedPrLLM is a federated pruning framework designed for compressing large language models (LLMs) in privacy-sensitive domains. It allows for collaborative pruning of the global model using local calibration data from each client, ensuring privacy while achieving compression. Extensive experiments within the FedPrLLM framework show that one-shot pruning with layer comparison and no weight scaling is the most effective approach. The framework aims to guide future efforts in pruning LLMs for privacy-sensitive applications. The code is available on GitHub for further exploration and implementation. <div>
arXiv:2505.13547v1 Announce Type: new 
Abstract: LLM pruning has emerged as a promising technology for compressing LLMs, enabling their deployment on resource-limited devices. However, current methodologies typically require access to public calibration samples, which can be challenging to obtain in privacy-sensitive domains. To address this issue, we introduce FedPrLLM, a comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs to calculate a pruning mask matrix based on its local calibration data and share it with the server to prune the global model. This approach allows for collaborative pruning of the global model with the knowledge of each client while maintaining local data privacy. Additionally, we conduct extensive experiments to explore various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and the decision to scale weights. Our extensive evaluation reveals that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework. We hope our work will help guide future efforts in pruning LLMs in privacy-sensitive fields. Our code is available at https://github.com/Pengxin-Guo/FedPrLLM.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression</title>
<link>https://arxiv.org/abs/2505.13563</link>
<guid>https://arxiv.org/abs/2505.13563</guid>
<content:encoded><![CDATA[
<div> fine-tuned--pretrained paradigm, delta compression, UltraDelta, high compression, strong performance <br />
Summary: <br />
UltraDelta is introduced as a data-free delta compression method that achieves ultra-high compression and strong performance by optimizing various dimensions. The method includes Variance-Based Mixed Sparsity Allocation to allocate sparsity based on variance to maintain inter-layer information, Distribution-Aware Compression for better preservation of intra-layer distribution through group-wise pruning, and Trace-Norm-Guided Rescaling to enhance model stability under higher compression. The approach is tested across various large language models, general NLP models, vision models, and multi-modal models, showcasing superior performance compared to existing methods, especially under ultra-high compression ratios. <div>
arXiv:2505.13563v1 Announce Type: new 
Abstract: With the rise of the fine-tuned--pretrained paradigm, storing numerous fine-tuned models for multi-tasking creates significant storage overhead. Delta compression alleviates this by storing only the pretrained model and the highly compressed delta weights (the differences between fine-tuned and pretrained model weights). However, existing methods fail to maintain both high compression and performance, and often rely on data. To address these challenges, we propose UltraDelta, the first data-free delta compression pipeline that achieves both ultra-high compression and strong performance. UltraDelta is designed to minimize redundancy, maximize information, and stabilize performance across inter-layer, intra-layer, and global dimensions, using three key components: (1) Variance-Based Mixed Sparsity Allocation assigns sparsity based on variance, giving lower sparsity to high-variance layers to preserve inter-layer information. (2) Distribution-Aware Compression applies uniform quantization and then groups parameters by value, followed by group-wise pruning, to better preserve intra-layer distribution. (3) Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a global rescaling factor, improving model stability under higher compression. Extensive experiments across (a) large language models (fine-tuned on LLaMA-2 7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base) with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and (d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that UltraDelta consistently outperforms existing methods, especially under ultra-high compression.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Decision-Focused Learning</title>
<link>https://arxiv.org/abs/2505.13564</link>
<guid>https://arxiv.org/abs/2505.13564</guid>
<content:encoded><![CDATA[
<div> decision-focused learning, predictive models, dynamic environments, differentiable objective, online algorithm

Summary:
In this study, the focus is on decision-focused learning (DFL) in dynamic environments, where the objective function and data distribution change over time. The challenges in such scenarios include zero or undefined gradients in the objective function and non-convexity. To overcome these challenges, the researchers regularize the objective function to make it differentiable and apply the optimism principle with an appropriate perturbation based on a near-optimal oracle. This results in the development of a practical online algorithm with established bounds on the expected dynamic regret. The algorithm's performance is demonstrated through comparison with a classic prediction-focused approach in a knapsack experiment. The findings showcase the effectiveness of the proposed approach in tackling evolving decision-making tasks. 

<br /><br />Summary: <div>
arXiv:2505.13564v1 Announce Type: new 
Abstract: Decision-focused learning (DFL) is an increasingly popular paradigm for training predictive models whose outputs are used in decision-making tasks. Instead of merely optimizing for predictive accuracy, DFL trains models to directly minimize the loss associated with downstream decisions. This end-to-end strategy holds promise for tackling complex combinatorial problems; however, existing studies focus solely on scenarios where a fixed batch of data is available and the objective function does not change over time. We instead investigate DFL in dynamic environments where the objective function and data distribution evolve over time. This setting is challenging because the objective function has zero or undefined gradients -- which prevents the use of standard first-order optimization methods -- and is generally non-convex. To address these difficulties, we (i) regularize the objective to make it differentiable and (ii) make use of the optimism principle, based on a near-optimal oracle along with an appropriate perturbation. This leads to a practical online algorithm for which we establish bounds on the expected dynamic regret, both when the decision space is a simplex and when it is a general bounded convex polytope. Finally, we demonstrate the effectiveness of our algorithm by comparing its performance with a classic prediction-focused approach on a simple knapsack experiment.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dynamics of RNNs in Closed-Loop Environments</title>
<link>https://arxiv.org/abs/2505.13567</link>
<guid>https://arxiv.org/abs/2505.13567</guid>
<content:encoded><![CDATA[
<div> learning dynamics, recurrent neural networks, closed-loop, neuroscience-inspired tasks, motor control

Summary: 
- The article introduces a mathematical theory that describes the learning dynamics of linear recurrent neural networks (RNNs) trained in closed-loop environments.
- Two identical RNNs trained in closed- and open-loop modes exhibit different learning trajectories, highlighting the importance of closed-loop training settings.
- Analytically characterizing the closed-loop case reveals distinct stages associated with the evolution of the training loss, emphasizing the interplay between short-term policy improvement and long-term stability in the agent-environment interaction.
- The learning dynamics of closed-loop RNNs are governed by the balance between these competing objectives, contrasting with open-loop settings.
- The framework is applied to a realistic motor control task, demonstrating its broader applicability in understanding brain computation and the importance of modeling closed-loop dynamics in biologically plausible settings. 

<br /><br />Summary: <div>
arXiv:2505.13567v1 Announce Type: new 
Abstract: Recurrent neural networks (RNNs) trained on neuroscience-inspired tasks offer powerful models of brain computation. However, typical training paradigms rely on open-loop, supervised settings, whereas real-world learning unfolds in closed-loop environments. Here, we develop a mathematical theory describing the learning dynamics of linear RNNs trained in closed-loop contexts. We first demonstrate that two otherwise identical RNNs, trained in either closed- or open-loop modes, follow markedly different learning trajectories. To probe this divergence, we analytically characterize the closed-loop case, revealing distinct stages aligned with the evolution of the training loss. Specifically, we show that the learning dynamics of closed-loop RNNs, in contrast to open-loop ones, are governed by an interplay between two competing objectives: short-term policy improvement and long-term stability of the agent-environment interaction. Finally, we apply our framework to a realistic motor control task, highlighting its broader applicability. Taken together, our results underscore the importance of modeling closed-loop dynamics in a biologically plausible setting.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate Modeling of 3D Rayleigh-Benard Convection with Equivariant Autoencoders</title>
<link>https://arxiv.org/abs/2505.13569</link>
<guid>https://arxiv.org/abs/2505.13569</guid>
<content:encoded><![CDATA[
<div> machine learning, physics systems, surrogate model, Rayleigh-Bénard convection, equivariant

Summary:
- The article discusses the use of machine learning in modeling and controlling large-scale physics systems.
- An end-to-end equivariant surrogate model is proposed for handling complex dynamics in systems governed by partial differential equations.
- The model consists of an equivariant convolutional autoencoder and an equivariant convolutional LSTM using G-steerable kernels.
- The case study focuses on three-dimensional Rayleigh-Bénard convection, showcasing the efficacy of the proposed architecture.
- Significant improvements are observed in sample and parameter efficiency, as well as scaling to larger Rayleigh numbers. 
<br /><br />Summary: <div>
arXiv:2505.13569v1 Announce Type: new 
Abstract: The use of machine learning for modeling, understanding, and controlling large-scale physics systems is quickly gaining in popularity, with examples ranging from electromagnetism over nuclear fusion reactors and magneto-hydrodynamics to fluid mechanics and climate modeling. These systems -- governed by partial differential equations -- present unique challenges regarding the large number of degrees of freedom and the complex dynamics over many scales both in space and time, and additional measures to improve accuracy and sample efficiency are highly desirable. We present an end-to-end equivariant surrogate model consisting of an equivariant convolutional autoencoder and an equivariant convolutional LSTM using $G$-steerable kernels. As a case study, we consider the three-dimensional Rayleigh-B\'enard convection, which describes the buoyancy-driven fluid flow between a heated bottom and a cooled top plate. While the system is E(2)-equivariant in the horizontal plane, the boundary conditions break the translational equivariance in the vertical direction. Our architecture leverages vertically stacked layers of $D_4$-steerable kernels, with additional partial kernel sharing in the vertical direction for further efficiency improvement. Our results demonstrate significant gains both in sample and parameter efficiency, as well as a better scaling to more complex dynamics, that is, larger Rayleigh numbers. The accompanying code is available under https://github.com/FynnFromme/equivariant-rb-forecasting.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Overview of Arithmetic Adaptations for Inference of Convolutional Neural Networks on Re-configurable Hardware</title>
<link>https://arxiv.org/abs/2505.13575</link>
<guid>https://arxiv.org/abs/2505.13575</guid>
<content:encoded><![CDATA[
<div> FPGAs, CNNs, TinyYOLOv3, object detection, XILINX Artix-7<br />
<br />
Summary:<br />
Convolutional Neural Networks (CNNs) are widely used in computer vision applications, but face challenges when deployed on embedded platforms like FPGAs. This study presents best practices for running a TinyYOLOv3 detector network on a XILINX Artix-7 FPGA. Techniques such as fusion of batch normalization, filter pruning, and post-training network quantization are utilized to optimize performance. FPGAs require strategies to address high computational intensity, memory requirements, and arithmetic constraints when running CNNs. By implementing these methods, the deployment of CNNs on FPGAs can be significantly improved for tasks such as object detection in images or video streams. <div>
arXiv:2505.13575v1 Announce Type: new 
Abstract: Convolutional Neural Networks (CNNs) have gained high popularity as a tool for computer vision tasks and for that reason are used in various applications. There are many different concepts, like single shot detectors, that have been published for detecting objects in images or video streams. However, CNNs suffer from disadvantages regarding the deployment on embedded platforms such as re-configurable hardware like Field Programmable Gate Arrays (FPGAs). Due to the high computational intensity, memory requirements and arithmetic conditions, a variety of strategies for running CNNs on FPGAs have been developed. The following methods showcase our best practice approaches for a TinyYOLOv3 detector network on a XILINX Artix-7 FPGA using techniques like fusion of batch normalization, filter pruning and post training network quantization.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexFed: Mitigating Catastrophic Forgetting in Heterogeneous Federated Learning in Pervasive Computing Environments</title>
<link>https://arxiv.org/abs/2505.13576</link>
<guid>https://arxiv.org/abs/2505.13576</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Human Activity Recognition, Catastrophic forgetting, FlexFed, Dynamic distributions 
Summary: 
FlexFed is a novel Federated Learning (FL) approach designed to address the issue of catastrophic forgetting (CF) in resource-constrained Human Activity Recognition (HAR) environments. The proposed approach prioritizes data retention for efficient memory use and adjusts offline training frequency based on distribution shifts and client capabilities. A new metric is introduced to quantify CF in FL, considering under-represented data for more accurate evaluations. An evaluation framework simulating streaming data, dynamic distributions, and varying availability shows that FlexFed effectively mitigates CF, improving FL efficiency by 10 to 15% and achieving faster convergence. This approach is particularly beneficial for infrequent or under-represented data, enhancing stability and performance in HAR environments. <br /><br />Summary: <div>
arXiv:2505.13576v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training while preserving privacy by allowing clients to share model updates instead of raw data. Pervasive computing environments (e.g., for Human Activity Recognition, HAR), which we focus on in this paper, are characterized by resource-constrained end devices, streaming sensor data and intermittent client participation. Variations in user behavior, common in HAR environments, often result in non-stationary data distributions. As such, existing FL approaches face challenges in HAR settings due to differing assumptions. The combined effects of HAR characteristics, namely heterogeneous data and intermittent participation, can lead to a severe issue called catastrophic forgetting (CF). Unlike Continuous Learning (CL), which addresses CF using memory and replay mechanisms, FL's privacy constraints prohibit such strategies.
  To tackle CF in HAR environments, we propose FlexFed, a novel FL approach that prioritizes data retention for efficient memory use and dynamically adjusts offline training frequency based on distribution shifts, client capability and offline duration. To better quantify CF in FL, we introduce a new metric that accounts for under-represented data, enabling more accurate evaluations. We also develop a realistic HAR-based evaluation framework that simulates streaming data, dynamic distributions, imbalances and varying availability. Experiments show that FlexFed mitigates CF more effectively, improves FL efficiency by 10 to 15 % and achieves faster, more stable convergence, especially for infrequent or under-represented data.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry-Breaking Descent for Invariant Cost Functionals</title>
<link>https://arxiv.org/abs/2505.13578</link>
<guid>https://arxiv.org/abs/2505.13578</guid>
<content:encoded><![CDATA[
<div> optimization, symmetry, cost functional, black-box, variational method

Summary:
This study focuses on reducing task cost functionals that are defined over Sobolev-class signals and invariant under a global symmetry group. The cost metrics are non-differentiable and internally generated, common in machine learning and imaging. A variational method is proposed to create explicit deformations of input signals by using a gauge field to induce symmetry-breaking deformations. These deformations are shown to decrease the cost along a direction transverse to the symmetry orbit of the signal. The approach does not require access to model gradients or labels and operates at test time. By utilizing Lie-algebraic variational flows, this method offers a way to optimize invariant cost functionals in scenarios where symmetries are present, such as black-box models and symmetry-constrained tasks. The exceptional degeneracies are of negligible Gaussian measure, making the approach effective and principled in cost reduction while maintaining symmetry constraints. 

<br /><br />Summary: <div>
arXiv:2505.13578v1 Announce Type: new 
Abstract: We study the problem of reducing a task cost functional $W(S)$, defined over Sobolev-class signals $S$, when the cost is invariant under a global symmetry group $G \subset \mathrm{Diff}(M)$ and accessible only as a black-box. Such scenarios arise in machine learning, imaging, and inverse problems, where cost metrics reflect model outputs or performance scores but are non-differentiable and model-internal. We propose a variational method that exploits the symmetry structure to construct explicit, symmetry-breaking deformations of the input signal. A gauge field $\phi$, obtained by minimizing an auxiliary energy functional, induces a deformation $h = A_\phi[S]$ that generically lies transverse to the $G$-orbit of $S$. We prove that, under mild regularity, the cost $W$ strictly decreases along this direction -- either via Clarke subdifferential descent or by escaping locally flat plateaus. The exceptional set of degeneracies has zero Gaussian measure. Our approach requires no access to model gradients or labels and operates entirely at test time. It provides a principled tool for optimizing invariant cost functionals via Lie-algebraic variational flows, with applications to black-box models and symmetry-constrained tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making</title>
<link>https://arxiv.org/abs/2505.13580</link>
<guid>https://arxiv.org/abs/2505.13580</guid>
<content:encoded><![CDATA[
<div> Transformer, Generative Pre-trained Transformer, sequential decision making, operations research, management science

Summary: 
The article introduces a new model called OMGPT, which is a Generative Pre-trained Transformer designed specifically for solving sequential decision making tasks in operations research and management science. The model is built from scratch and offers a general sequence modeling framework that covers various operational decision making tasks such as dynamic pricing, inventory management, resource allocation, and queueing control. Unlike existing methods, OMGPT leverages pre-trained data and does not rely on any analytical model structure, providing a direct mapping from historical information to future actions. The Bayesian perspective is used to understand the model's performance, which is linked to the diversity of pre-training tasks and the discrepancy between testing and pre-training tasks. Numerical experiments demonstrate the effectiveness of OMGPT across all considered tasks. <div>
arXiv:2505.13580v1 Announce Type: new 
Abstract: We build a Generative Pre-trained Transformer (GPT) model from scratch to solve sequential decision making tasks arising in contexts of operations research and management science which we call OMGPT. We first propose a general sequence modeling framework to cover several operational decision making tasks as special cases, such as dynamic pricing, inventory management, resource allocation, and queueing control. Under the framework, all these tasks can be viewed as a sequential prediction problem where the goal is to predict the optimal future action given all the historical information. Then we train a transformer-based neural network model (OMGPT) as a natural and powerful architecture for sequential modeling. This marks a paradigm shift compared to the existing methods for these OR/OM tasks in that (i) the OMGPT model can take advantage of the huge amount of pre-trained data; (ii) when tackling these problems, OMGPT does not assume any analytical model structure and enables a direct and rich mapping from the history to the future actions. Either of these two aspects, to the best of our knowledge, is not achieved by any existing method. We establish a Bayesian perspective to theoretically understand the working mechanism of the OMGPT on these tasks, which relates its performance with the pre-training task diversity and the divergence between the testing task and pre-training tasks. Numerically, we observe a surprising performance of the proposed model across all the above tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Critical Sets of Deep Neural Networks via Sample-Independent Critical Lifting</title>
<link>https://arxiv.org/abs/2505.13582</link>
<guid>https://arxiv.org/abs/2505.13582</guid>
<content:encoded><![CDATA[
<div> sample dependence, critical points, neural networks, lifting operator, saddles
Summary:<br />
- The paper explores the sample dependence of critical points in neural networks.
- A sample-independent critical lifting operator is introduced to connect parameters of different networks.
- Sample-dependent and sample-independent lifted critical points are defined using this operator.
- Previously studied critical embeddings do not encompass all sample-independent lifted critical points.
- Sample-dependent lifted critical points, including saddles, are shown to exist for large sample sizes. 
<br />Summary: <div>
arXiv:2505.13582v1 Announce Type: new 
Abstract: This paper investigates the sample dependence of critical points for neural networks. We introduce a sample-independent critical lifting operator that associates a parameter of one network with a set of parameters of another, thus defining sample-dependent and sample-independent lifted critical points. We then show by example that previously studied critical embeddings do not capture all sample-independent lifted critical points. Finally, we demonstrate the existence of sample-dependent lifted critical points for sufficiently large sample sizes and prove that saddles appear among them.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Half Search Space is All You Need</title>
<link>https://arxiv.org/abs/2505.13586</link>
<guid>https://arxiv.org/abs/2505.13586</guid>
<content:encoded><![CDATA[
<div> Neural Architecture Search, NAS, DARTS, One-Shot NAS, search efficiency<br />
<br />
Summary: <br />
Neural Architecture Search (NAS) is automated architecture design, with One-Shot NAS like DARTS popular for their efficiency and simplicity. However, high GPU memory requirements during search are a challenge. To address this, the study introduces a method to automatically prune the search space, reducing memory usage and search time while maintaining accuracy. Zero-Shot NAS is utilized to eliminate low-performing designs before applying One-Shot NAS to the pruned space. The experimental results on the DARTS search space demonstrate an 81% reduction in memory consumption compared to the baseline One-Shot setup without sacrificing accuracy. <div>
arXiv:2505.13586v1 Announce Type: new 
Abstract: Neural Architecture Search (NAS) is a powerful tool for automating architecture design. One-Shot NAS techniques, such as DARTS, have gained substantial popularity due to their combination of search efficiency with simplicity of implementation. By design, One-Shot methods have high GPU memory requirements during the search. To mitigate this issue, we propose to prune the search space in an efficient automatic manner to reduce memory consumption and search time while preserving the search accuracy. Specifically, we utilise Zero-Shot NAS to efficiently remove low-performing architectures from the search space before applying One-Shot NAS to the pruned search space. Experimental results on the DARTS search space show that our approach reduces memory consumption by 81% compared to the baseline One-Shot setup while achieving the same level of accuracy.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deterministic Bounds and Random Estimates of Metric Tensors on Neuromanifolds</title>
<link>https://arxiv.org/abs/2505.13614</link>
<guid>https://arxiv.org/abs/2505.13614</guid>
<content:encoded><![CDATA[
<div> Fisher information, deep neural networks, metric tensor, Riemannian metric, Hutchinson's trace estimator 
Summary:
The article discusses the analysis of the unique metric tensor on the high dimensional parameter space of deep neural networks, termed as the neuromanifold, using the Fisher information. By focusing on a low-dimensional space of probability distributions called the core space, the spectrum of its Riemannian metric is carefully examined, leading to deterministic bounds on the metric tensor for classification networks. An unbiased random estimate of the metric tensor and its bounds is introduced based on Hutchinson's trace estimator, allowing for efficient evaluation through a single backward pass. This estimate can be used to estimate the diagonal, block diagonal, or full tensor with guaranteed quality and bounded standard deviation. The methodology proposed in this study provides a reliable approach for estimating and analyzing the metric tensor in deep learning networks. 
Summary: <div>
arXiv:2505.13614v1 Announce Type: new 
Abstract: The high dimensional parameter space of modern deep neural networks -- the neuromanifold -- is endowed with a unique metric tensor defined by the Fisher information, estimating which is crucial for both theory and practical methods in deep learning. To analyze this tensor for classification networks, we return to a low dimensional space of probability distributions -- the core space -- and carefully analyze the spectrum of its Riemannian metric. We extend our discoveries there into deterministic bounds of the metric tensor on the neuromanifold. We introduce an unbiased random estimate of the metric tensor and its bounds based on Hutchinson's trace estimator. It can be evaluated efficiently through a single backward pass and can be used to estimate the diagonal, or block diagonal, or the full tensor. Its quality is guaranteed with a standard deviation bounded by the true value up to scaling.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning (Approximately) Equivariant Networks via Constrained Optimization</title>
<link>https://arxiv.org/abs/2505.13631</link>
<guid>https://arxiv.org/abs/2505.13631</guid>
<content:encoded><![CDATA[
<div> Adaptive Constrained Equivariance, neural networks, symmetries, optimization approach, data-driven equilibrium
<br />
Summary:
<br />
Equivariant neural networks aim to leverage symmetries in data distributions for improved generalization and sample efficiency. However, real-world data often deviates from perfect symmetry due to various factors like noise and measurement biases. Strictly equivariant models may struggle to fit such asymmetric data, while unconstrained models lack a principled way to handle partial symmetries. The Adaptive Constrained Equivariance (ACE) approach introduced in this paper starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance during optimization. This method smooths training, leading to improved performance metrics, sample efficiency, and robustness to input perturbations compared to strictly equivariant models. ACE strikes a balance between equivariance and non-equivariance, providing a more effective way to handle data with varying degrees of symmetry. <div>
arXiv:2505.13631v1 Announce Type: new 
Abstract: Equivariant neural networks are designed to respect symmetries through their architecture, boosting generalization and sample efficiency when those symmetries are present in the data distribution. Real-world data, however, often departs from perfect symmetry because of noise, structural variation, measurement bias, or other symmetry-breaking effects. Strictly equivariant models may struggle to fit the data, while unconstrained models lack a principled way to leverage partial symmetries. Even when the data is fully symmetric, enforcing equivariance can hurt training by limiting the model to a restricted region of the parameter space. Guided by homotopy principles, where an optimization problem is solved by gradually transforming a simpler problem into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a constrained optimization approach that starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance. This gradual tightening smooths training early on and settles the model at a data-driven equilibrium, balancing between equivariance and non-equivariance. Across multiple architectures and tasks, our method consistently improves performance metrics, sample efficiency, and robustness to input perturbations compared with strictly equivariant models and heuristic equivariance relaxations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Truthful Language Models via Peer Elicitation Games</title>
<link>https://arxiv.org/abs/2505.13636</link>
<guid>https://arxiv.org/abs/2505.13636</guid>
<content:encoded><![CDATA[
<div> framework, Large Language Models, Peer Elicitation Games, discriminators, truthful reporting
Summary:
Peer Elicitation Games (PEG) is introduced as a game-theoretic framework to align Large Language Models (LLMs) by utilizing a peer elicitation mechanism involving multiple discriminators. The discriminators interact in a peer evaluation setting, incentivized to provide truthful feedback without the need for ground-truth labels. Through theoretical guarantees, each agent improves its performance over time, converging to a stable and truthful Nash equilibrium. Empirical evaluations across various benchmarks show significant enhancements in factual accuracy. PEG proves to be a practical solution for eliciting truthful behavior from LLMs without the need for supervision or fine-tuning. <br /><br />Summary: <div>
arXiv:2505.13636v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong generative capabilities but remain prone to inconsistencies and hallucinations. We introduce Peer Elicitation Games (PEG), a training-free, game-theoretic framework for aligning LLMs through a peer elicitation mechanism involving a generator and multiple discriminators instantiated from distinct base models. Discriminators interact in a peer evaluation setting, where rewards are computed using a determinant-based mutual information score that provably incentivizes truthful reporting without requiring ground-truth labels. We establish theoretical guarantees showing that each agent, via online learning, achieves sublinear regret in the sense their cumulative performance approaches that of the best fixed truthful strategy in hindsight. Moreover, we prove last-iterate convergence to a truthful Nash equilibrium, ensuring that the actual policies used by agents converge to stable and truthful behavior over time. Empirical evaluations across multiple benchmarks demonstrate significant improvements in factual accuracy. These results position PEG as a practical approach for eliciting truthful behavior from LLMs without supervision or fine-tuning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4Hammer: a board-game reinforcement learning environment for the hour long time frame</title>
<link>https://arxiv.org/abs/2505.13638</link>
<guid>https://arxiv.org/abs/2505.13638</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reinforcement learning, board games, Warhammer 40,000, digital twin simulation

Summary:<br />
- Large Language Models (LLMs) excel in short-term tasks but struggle with longer durations.
- There is a lack of complex board games designed for reinforcement learning and LLM evaluation.
- The 4Hammer reinforcement learning environment is introduced as a simulation of Warhammer 40,000.
- Warhammer 40,000 is a complex board game with intricate rules requiring deep understanding and strategic gameplay.
- This environment aims to provide a challenging and diverse testbed for evaluating LLMs and reinforcement learning algorithms.<br /> 

Summary: <div>
arXiv:2505.13638v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong performance on tasks with short time frames, but struggle with tasks requiring longer durations. While datasets covering extended-duration tasks, such as software engineering tasks or video games, do exist, there are currently few implementations of complex board games specifically designed for reinforcement learning and LLM evaluation. To address this gap, we propose the 4Hammer reinforcement learning environment, a digital twin simulation of a subset of Warhammer 40,000-a complex, zero-sum board game. Warhammer 40,000 features intricate rules, requiring human players to thoroughly read and understand over 50 pages of detailed natural language rules, grasp the interactions between their game pieces and those of their opponents, and independently track and communicate the evolving game state.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning</title>
<link>https://arxiv.org/abs/2505.13643</link>
<guid>https://arxiv.org/abs/2505.13643</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Test-Time Adaptation, Privacy-preserving, Computational efficiency, Scalability<br />
Summary:<br />
Federated Continual Test-Time Adaptation (FedCTTA) is proposed as a framework for privacy-preserving and computationally efficient adaptation in Federated Learning (FL). Unlike existing methods, FedCTTA avoids direct feature exchange by utilizing similarity-aware aggregation based on model output distributions over noise samples. This ensures adaptive knowledge sharing while maintaining data privacy. The framework minimizes entropy at each client for continual adaptation, enhancing model confidence in evolving target distributions. FedCTTA does not require server-side training during adaptation and maintains a constant memory footprint, enabling scalability as the number of clients or training rounds increases. Extensive experiments demonstrate the superiority of FedCTTA over existing methods in various scenarios of temporal and spatial heterogeneity.<br /> <div>
arXiv:2505.13643v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across distributed clients without sharing raw data, making it ideal for privacy-sensitive applications. However, FL models often suffer performance degradation due to distribution shifts between training and deployment. Test-Time Adaptation (TTA) offers a promising solution by allowing models to adapt using only test samples. However, existing TTA methods in FL face challenges such as computational overhead, privacy risks from feature sharing, and scalability concerns due to memory constraints. To address these limitations, we propose Federated Continual Test-Time Adaptation (FedCTTA), a privacy-preserving and computationally efficient framework for federated adaptation. Unlike prior methods that rely on sharing local feature statistics, FedCTTA avoids direct feature exchange by leveraging similarity-aware aggregation based on model output distributions over randomly generated noise samples. This approach ensures adaptive knowledge sharing while preserving data privacy. Furthermore, FedCTTA minimizes the entropy at each client for continual adaptation, enhancing the model's confidence in evolving target distributions. Our method eliminates the need for server-side training during adaptation and maintains a constant memory footprint, making it scalable even as the number of clients or training rounds increases. Extensive experiments show that FedCTTA surpasses existing methods across diverse temporal and spatial heterogeneity scenarios.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collapsing Taylor Mode Automatic Differentiation</title>
<link>https://arxiv.org/abs/2505.13644</link>
<guid>https://arxiv.org/abs/2505.13644</guid>
<content:encoded><![CDATA[
<div> Keywords: partial differential equation, nested backpropagation, computational graph, optimization technique, linear PDE operators

Summary: 
This paper introduces an optimization technique for computing partial differential equation (PDE) operators using nested backpropagation. The authors propose a method to 'collapse' derivatives by rewriting the computational graph, making the process more efficient. By propagating a sum up the computational graph, the complexity is hidden from users, making it suitable for machine learning compilers. The technique is applied to general linear PDE operators and randomized Taylor mode, accelerating the computation and outperforming traditional nested backpropagation. This advancement in forward schemes for computing PDE operators opens up new possibilities for scientific machine learning applications. <br /><br />Summary: <div>
arXiv:2505.13644v1 Announce Type: new 
Abstract: Computing partial differential equation (PDE) operators via nested backpropagation is expensive, yet popular, and severely restricts their utility for scientific machine learning. Recent advances, like the forward Laplacian and randomizing Taylor mode automatic differentiation (AD), propose forward schemes to address this. We introduce an optimization technique for Taylor mode that 'collapses' derivatives by rewriting the computational graph, and demonstrate how to apply it to general linear PDE operators, and randomized Taylor mode. The modifications simply require propagating a sum up the computational graph, which could -- or should -- be done by a machine learning compiler, without exposing complexity to users. We implement our collapsing procedure and evaluate it on popular PDE operators, confirming it accelerates Taylor mode and outperforms nested backpropagation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Reinforced Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.13650</link>
<guid>https://arxiv.org/abs/2505.13650</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Contrastive Learning, Self-Reinforced, Positive Pair Generator, Manifold Hypothesis, Probabilistic Mechanism

Summary: 
SRGCL proposes a novel framework for Graph Contrastive Learning that dynamically evaluates and selects high-quality positive pairs using the model's own encoder. It leverages multiple augmentation strategies and a selector guided by the manifold hypothesis to maintain the underlying geometry of the latent space. By adopting a probabilistic mechanism, SRGCL iteratively refines its assessment of pair quality as the encoder's representational power improves. Experimental results on graph-level classification tasks show that SRGCL outperforms state-of-the-art GCL methods, demonstrating its adaptability and effectiveness across various domains. <div>
arXiv:2505.13650v1 Announce Type: new 
Abstract: Graphs serve as versatile data structures in numerous real-world domains-including social networks, molecular biology, and knowledge graphs-by capturing intricate relational information among entities. Among graph-based learning techniques, Graph Contrastive Learning (GCL) has gained significant attention for its ability to derive robust, self-supervised graph representations through the contrasting of positive and negative sample pairs. However, a critical challenge lies in ensuring high-quality positive pairs so that the intrinsic semantic and structural properties of the original graph are preserved rather than distorted. To address this issue, we propose SRGCL (Self-Reinforced Graph Contrastive Learning), a novel framework that leverages the model's own encoder to dynamically evaluate and select high-quality positive pairs. We designed a unified positive pair generator employing multiple augmentation strategies, and a selector guided by the manifold hypothesis to maintain the underlying geometry of the latent space. By adopting a probabilistic mechanism for selecting positive pairs, SRGCL iteratively refines its assessment of pair quality as the encoder's representational power improves. Extensive experiments on diverse graph-level classification tasks demonstrate that SRGCL, as a plug-in module, consistently outperforms state-of-the-art GCL methods, underscoring its adaptability and efficacy across various domains.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs</title>
<link>https://arxiv.org/abs/2505.13697</link>
<guid>https://arxiv.org/abs/2505.13697</guid>
<content:encoded><![CDATA[
<div> Assumptions, Markov Decision Process, Reinforcement Learning, Language Models, Supervised Fine-tuning <br />
Summary: The article critiques reinforcement learning-based post-training of large language models (LLMs) by examining the assumptions and formulation. The structural assumptions made in modeling LLM training as a Markov Decision Process (MDP) are highlighted, leading to a degenerate MDP that doesn't require reinforcement learning. The simplifying assumptions effectively equate the approach to outcome-driven supervised learning. Experiments on benchmarks show that iterative supervised fine-tuning achieves comparable performance to reinforcement learning-based training. The structural assumptions incentivize the RL to generate longer sequences of intermediate tokens, questioning the narrative of "RL generating longer thinking traces." While reinforcement learning may have its uses in improving LLM reasoning abilities, the simplistic assumptions in modeling the MDP raise doubts about the popular LLM RL frameworks and their interpretations. <br /><br /> <div>
arXiv:2505.13697v1 Announce Type: new 
Abstract: Reinforcement learning-based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing hype around improved reasoning abilities attributed to RL post-training, we critically examine the formulation and assumptions underlying these methods. We start by highlighting the popular structural assumptions made in modeling LLM training as a Markov Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't quite need the RL/GRPO apparatus. The two critical structural assumptions include (1) making the MDP states be just a concatenation of the actions-with states becoming the context window and the actions becoming the tokens in LLMs and (2) splitting the reward of a state-action trajectory uniformly across the trajectory. Through a comprehensive analysis, we demonstrate that these simplifying assumptions make the approach effectively equivalent to an outcome-driven supervised learning. Our experiments on benchmarks including GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised fine-tuning, incorporating both positive and negative samples, achieves performance comparable to GRPO-based training. We will also argue that the structural assumptions indirectly incentivize the RL to generate longer sequences of intermediate tokens-which in turn feeds into the narrative of "RL generating longer thinking traces." While RL may well be a very useful technique for improving the reasoning abilities of LLMs, our analysis shows that the simplistic structural assumptions made in modeling the underlying MDP render the popular LLM RL frameworks and their interpretations questionable.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised anomaly detection in MeV ultrafast electron diffraction</title>
<link>https://arxiv.org/abs/2505.13702</link>
<guid>https://arxiv.org/abs/2505.13702</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised anomaly detection, faulty images, MUED, uncertainty, decision-making

Summary:
An unsupervised anomaly detection methodology is proposed for detecting faulty images in MUED. This methodology aims to eliminate the need for manual labeling of training data, allowing the machine to autonomously identify anomalies in the dataset. By providing a measure of uncertainty in the detection process, users can make informed decisions based on the detection results. This approach saves time by avoiding the manual examination of images and enhances efficiency by automating the anomaly detection process. The focus on unsupervised techniques emphasizes the importance of machine learning in anomaly detection tasks. Overall, the methodology aims to improve anomaly detection accuracy while streamlining the detection process for faulty images in MUED. 

<br /><br />Summary: <div>
arXiv:2505.13702v1 Announce Type: new 
Abstract: This study focus in the construction of an unsupervised anomaly detection methodology to detect faulty images in MUED. We believe that unsupervised techniques are the best choice for our purposes because the data used to train the detector does not need to be manually labeled, and instead, the machine is intended to detect by itself the anomalies in the dataset, which liberates the user of tedious, time-consuming initial image examination. The structure must, additionally, provide the user with some measure of uncertainty in the detection, so the user can take decisions based on this measure.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.13709</link>
<guid>https://arxiv.org/abs/2505.13709</guid>
<content:encoded><![CDATA[
<div> offline reinforcement learning, model-based RL, world model, policy learning, robustness

Summary:<br />
The article presents a novel framework for offline model-based RL that aims to improve data efficiency and generalization. Unlike existing methods, this framework dynamically adapts the world model alongside the policy to enhance robustness and prevent performance degradation from small adversarial noise. The proposed method combines Stackelberg learning dynamics to solve a maximin optimization problem, optimizing both the world model and policy simultaneously. The theoretical analysis supports the design, and computationally efficient implementations are introduced. Benchmarking on twelve noisy D4RL MuJoCo tasks and three stochastic Tokamak Control tasks showcases the algorithm's state-of-the-art performance. By addressing the objective mismatch in existing offline MBRL methods and focusing on improving robustness, the proposed framework offers a promising approach for data-driven control in RL applications. <br />Summary: <div>
arXiv:2505.13709v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) offers a powerful paradigm for data-driven control. Compared to model-free approaches, offline model-based RL (MBRL) explicitly learns a world model from a static dataset and uses it as a surrogate simulator, improving data efficiency and enabling potential generalization beyond the dataset support. However, most existing offline MBRL methods follow a two-stage training procedure: first learning a world model by maximizing the likelihood of the observed transitions, then optimizing a policy to maximize its expected return under the learned model. This objective mismatch results in a world model that is not necessarily optimized for effective policy learning. Moreover, we observe that policies learned via offline MBRL often lack robustness during deployment, and small adversarial noise in the environment can lead to significant performance degradation. To address these, we propose a framework that dynamically adapts the world model alongside the policy under a unified learning objective aimed at improving robustness. At the core of our method is a maximin optimization problem, which we solve by innovatively utilizing Stackelberg learning dynamics. We provide theoretical analysis to support our design and introduce computationally efficient implementations. We benchmark our algorithm on twelve noisy D4RL MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turbocharging Gaussian Process Inference with Approximate Sketch-and-Project</title>
<link>https://arxiv.org/abs/2505.13723</link>
<guid>https://arxiv.org/abs/2505.13723</guid>
<content:encoded><![CDATA[
<div> Gaussian processes; biostatistics; Bayesian optimization; scalable inference; sketch-and-project algorithm <br />
Summary: <br />
The article introduces an approximate, distributed, accelerated sketch-and-project algorithm (ADASAP) for improving the scalability of Gaussian process (GP) inference in handling large datasets. By leveraging determinantal point processes theory, ADASAP shows rapid convergence of the posterior mean to the true posterior mean, providing an efficient and condition number-free method for estimating the posterior mean along the top spectral basis functions. Comparative analysis demonstrates ADASAP's superior performance over existing solvers such as conjugate gradient and coordinate descent across various benchmark datasets and a large-scale Bayesian optimization task. Remarkably, ADASAP showcases scalability to datasets exceeding 3e8 samples, a significant achievement not previously reported in the literature. ADASAP emerges as a principled and efficient approach for GP inference, offering significant advancements in scalability and performance. <br /> <div>
arXiv:2505.13723v1 Announce Type: new 
Abstract: Gaussian processes (GPs) play an essential role in biostatistics, scientific machine learning, and Bayesian optimization for their ability to provide probabilistic predictions and model uncertainty. However, GP inference struggles to scale to large datasets (which are common in modern applications), since it requires the solution of a linear system whose size scales quadratically with the number of samples in the dataset. We propose an approximate, distributed, accelerated sketch-and-project algorithm ($\texttt{ADASAP}$) for solving these linear systems, which improves scalability. We use the theory of determinantal point processes to show that the posterior mean induced by sketch-and-project rapidly converges to the true posterior mean. In particular, this yields the first efficient, condition number-free algorithm for estimating the posterior mean along the top spectral basis functions, showing that our approach is principled for GP inference. $\texttt{ADASAP}$ outperforms state-of-the-art solvers based on conjugate gradient and coordinate descent across several benchmark datasets and a large-scale Bayesian optimization task. Moreover, $\texttt{ADASAP}$ scales to a dataset with $> 3 \cdot 10^8$ samples, a feat which has not been accomplished in the literature.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training</title>
<link>https://arxiv.org/abs/2505.13738</link>
<guid>https://arxiv.org/abs/2505.13738</guid>
<content:encoded><![CDATA[
<div> scaling laws, hyperparameters, model size, dataset size, batch size

Summary:<br /><br />Efficient Large Language Model (LLM) pre-training necessitates well-optimized hyperparameters, particularly the learning rate and weight decay. The study explores scaling laws for hyperparameters by examining their relationship with model size, dataset size, and batch size. The AdamW timescale remains consistent across training settings, with optimal weight decay scaling linearly with batch size for fixed model and dataset sizes. Furthermore, the optimal timescale follows a precise power law determined by the tokens-per-parameter ratio as model and dataset sizes increase. The study also investigates scaling laws for optimal batch size for minimizing loss and critical batch size signaling the point beyond which additional data parallelism loses effectiveness. Both optimal and critical batch sizes scale as power laws in dataset size, regardless of model size. These findings offer insights for selecting optimal model and dataset sizes based on dual training time and compute objectives.<br /><br /> <div>
arXiv:2505.13738v1 Announce Type: new 
Abstract: Efficient LLM pre-training requires well-tuned hyperparameters (HPs), including learning rate {\eta} and weight decay {\lambda}. We study scaling laws for HPs: formulas for how to scale HPs as we scale model size N, dataset size D, and batch size B. Recent work suggests the AdamW timescale, B/({\eta}{\lambda}D), should remain constant across training settings, and we verify the implication that optimal {\lambda} scales linearly with B, for a fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise power law in the tokens-per-parameter ratio, D/N. This law thus provides a method to accurately predict {\lambda}opt in advance of large-scale training. We also study scaling laws for optimal batch size Bopt (the B enabling lowest loss at a given N,D) and critical batch size Bcrit (the B beyond which further data parallelism becomes ineffective). In contrast with prior work, we find both Bopt and Bcrit scale as power laws in D, independent of model size, N. Finally, we analyze how these findings inform the real-world selection of Pareto-optimal N and D under dual training time and compute objectives.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Compositional Generation with Diffusion Models Using Lift Scores</title>
<link>https://arxiv.org/abs/2505.13740</link>
<guid>https://arxiv.org/abs/2505.13740</guid>
<content:encoded><![CDATA[
<div> Keywords: resampling criterion, compositional generation, diffusion models, lift scores, condition alignment <br />
<br />
Summary: 
The article introduces a novel resampling criterion using lift scores to enhance compositional generation in diffusion models. By assessing whether generated samples align with each individual condition and then combining the results to determine overall satisfaction with the composed prompt, the authors aim to improve the quality of generated outputs. They demonstrate that lift scores can be effectively approximated using only the original diffusion model, without requiring external training or modules. An optimized variant is proposed to reduce computational overhead during inference while maintaining effectiveness. Extensive experiments across various datasets, including 2D synthetic data, CLEVR position tasks, and text-to-image synthesis, show that lift scores lead to significant improvements in condition alignment for compositional generation. The code for the proposed method is available on GitHub for further exploration and use. <br /> <div>
arXiv:2505.13740v1 Announce Type: new 
Abstract: We introduce a novel resampling criterion using lift scores, for improving compositional generation in diffusion models. By leveraging the lift scores, we evaluate whether generated samples align with each single condition and then compose the results to determine whether the composed prompt is satisfied. Our key insight is that lift scores can be efficiently approximated using only the original diffusion model, requiring no additional training or external modules. We develop an optimized variant that achieves relatively lower computational overhead during inference while maintaining effectiveness. Through extensive experiments, we demonstrate that lift scores significantly improved the condition alignment for compositional generation across 2D synthetic data, CLEVR position tasks, and text-to-image synthesis. Our code is available at http://github.com/rainorangelemon/complift.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Task Representations in Neural Networks via Bayesian Ablation</title>
<link>https://arxiv.org/abs/2505.13742</link>
<guid>https://arxiv.org/abs/2505.13742</guid>
<content:encoded><![CDATA[
<div> Framework, Bayesian inference, neural networks, latent task representations, probabilistic distribution
Summary:
The article presents a novel probabilistic framework for interpreting latent task representations in neural networks. Inspired by Bayesian inference, the approach defines a distribution over representational units to infer their causal contributions to task performance. To illuminate key model properties, the authors propose a suite of tools and metrics, including representational distributedness, manifold complexity, and polysemanticity. This innovative approach addresses the challenge of interpreting learned representations in neural networks, offering insights into the sub-symbolic semantics through a probabilistic lens. The framework provides a means to analyze and understand the underlying mechanisms of neural networks, enhancing our understanding of how these powerful cognitive modeling tools function. <div>
arXiv:2505.13742v1 Announce Type: new 
Abstract: Neural networks are powerful tools for cognitive modeling due to their flexibility and emergent properties. However, interpreting their learned representations remains challenging due to their sub-symbolic semantics. In this work, we introduce a novel probabilistic framework for interpreting latent task representations in neural networks. Inspired by Bayesian inference, our approach defines a distribution over representational units to infer their causal contributions to task performance. Using ideas from information theory, we propose a suite of tools and metrics to illuminate key model properties, including representational distributedness, manifold complexity, and polysemanticity.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Non-stationary Data Streams for Recognition of the Unknown</title>
<link>https://arxiv.org/abs/2505.13745</link>
<guid>https://arxiv.org/abs/2505.13745</guid>
<content:encoded><![CDATA[
<div> data stream processing, concept drifts, novel classes, open set recognition, unsupervised drift detectors
<br />
The article introduces a novel approach to generating synthetic data streams that incorporate both concept drifts and the emergence of new, previously unknown classes. Traditionally, methods focus on either detecting concept drifts or identifying novel classes, but this study addresses both challenges simultaneously. By utilizing unsupervised drift detectors, the strategy presented in this research is able to effectively detect novelty and concept drifts in data streams. The generated data streams can also be leveraged for open set recognition tasks, where the goal is to classify known classes accurately while recognizing objects outside the model's expertise. This comprehensive approach to data stream generation provides a valuable framework for tackling the complexities of non-stationary data environments.
<br /><br />Summary: <div>
arXiv:2505.13745v1 Announce Type: new 
Abstract: The problem of data non-stationarity is commonly addressed in data stream processing. In a dynamic environment, methods should continuously be ready to analyze time-varying data -- hence, they should enable incremental training and respond to concept drifts. An equally important variability typical for non-stationary data stream environments is the emergence of new, previously unknown classes. Often, methods focus on one of these two phenomena -- detection of concept drifts or detection of novel classes -- while both difficulties can be observed in data streams. Additionally, concerning previously unknown observations, the topic of open set of classes has become particularly important in recent years, where the goal of methods is to efficiently classify within known classes and recognize objects outside the model competence. This article presents a strategy for synthetic data stream generation in which both concept drifts and the emergence of new classes representing unknown objects occur. The presented research shows how unsupervised drift detectors address the task of detecting novelty and concept drifts and demonstrates how the generated data streams can be utilized in the open set recognition task.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Maximum Independent Sets in Dynamic Graphs using Unsupervised Learning</title>
<link>https://arxiv.org/abs/2505.13754</link>
<guid>https://arxiv.org/abs/2505.13754</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised learning, dynamic graphs, Maximum Independent Sets, graph neural networks, scalability <br />
<br />
Summary: 
The article presents an unsupervised learning model for finding Maximum Independent Sets (MaxIS) in dynamic graphs, where edges change over time. The model combines structural learning from graph neural networks (GNNs) with a distributed update mechanism to infer MaxIS membership in a single step when edge addition or deletion events occur. The model's performance-runtime tradeoffs are investigated for various dynamic graph topologies, showing competitive approximation ratios and excellent scalability across synthetic and real-world dynamic graphs. It outperforms state-of-the-art methods for static graphs, including a heuristic learning framework. The model generalizes well on graphs 100x larger than the training set, achieving performance on par with greedy techniques and commercial solvers while running significantly faster. This work demonstrates the effectiveness of utilizing GNNs for dynamic graph optimization problems. <br /> <div>
arXiv:2505.13754v1 Announce Type: new 
Abstract: We present the first unsupervised learning model for finding Maximum Independent Sets (MaxIS) in dynamic graphs where edges change over time. Our method combines structural learning from graph neural networks (GNNs) with a learned distributed update mechanism that, given an edge addition or deletion event, modifies nodes' internal memories and infers their MaxIS membership in a single, parallel step. We parameterize our model by the update mechanism's radius and investigate the resulting performance-runtime tradeoffs for various dynamic graph topologies. We evaluate our model against state-of-the-art MaxIS methods for static graphs, including a mixed integer programming solver, deterministic rule-based algorithms, and a heuristic learning framework based on dynamic programming and GNNs. Across synthetic and real-world dynamic graphs of 100-10,000 nodes, our model achieves competitive approximation ratios with excellent scalability; on large graphs, it significantly outperforms the state-of-the-art heuristic learning framework in solution quality, runtime, and memory usage. Our model generalizes well on graphs 100x larger than the ones used for training, achieving performance at par with both a greedy technique and a commercial mixed integer programming solver while running 1.5-23x faster than greedy.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Panda: A pretrained forecast model for universal representation of chaotic dynamics</title>
<link>https://arxiv.org/abs/2505.13755</link>
<guid>https://arxiv.org/abs/2505.13755</guid>
<content:encoded><![CDATA[
<div> Keywords: Chaotic systems, predictive modeling, attention mechanism, differential equations, neural scaling law

Summary: 
Panda, a Patched Attention model for Nonlinear Dynamics, is developed to construct predictive data-driven models for chaotic systems. Trained on a synthetic dataset of chaotic dynamical systems, Panda shows zero-shot forecasting capabilities for real-world chaotic systems. It exhibits nonlinear resonance patterns in cross-channel attention heads and can predict partial differential equations without retraining. The model demonstrates a neural scaling law for differential equations, highlighting its potential for exploring abstract mathematical domains like nonlinear dynamics. Panda's training on low-dimensional ordinary differential equations showcases its ability to generalize to more complex systems, making it a valuable tool for studying and forecasting chaotic dynamical systems. <br /><br />Summary: <div>
arXiv:2505.13755v1 Announce Type: new 
Abstract: Chaotic systems are intrinsically sensitive to small errors, challenging efforts to construct predictive data-driven models of real-world dynamical systems such as fluid flows or neuronal activity. Prior efforts comprise either specialized models trained separately on individual time series, or foundation models trained on vast time series databases with little underlying dynamical structure. Motivated by dynamical systems theory, we present Panda, Patched Attention for Nonlinear DynAmics. We train Panda on a novel synthetic, extensible dataset of $2 \times 10^4$ chaotic dynamical systems that we discover using an evolutionary algorithm. Trained purely on simulated data, Panda exhibits emergent properties: zero-shot forecasting of unseen real world chaotic systems, and nonlinear resonance patterns in cross-channel attention heads. Despite having been trained only on low-dimensional ordinary differential equations, Panda spontaneously develops the ability to predict partial differential equations without retraining. We demonstrate a neural scaling law for differential equations, underscoring the potential of pretrained models for probing abstract mathematical domains like nonlinear dynamics.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistency Conditions for Differentiable Surrogate Losses</title>
<link>https://arxiv.org/abs/2505.13760</link>
<guid>https://arxiv.org/abs/2505.13760</guid>
<content:encoded><![CDATA[
<div> convex differentiable losses, indirect elicitation, calibration, consistency, surrogate losses <br />
<br />
Summary: 
This article explores the equivalence between indirect elicitation (IE) and calibration for convex differentiable losses in the context of discrete prediction tasks. The study first establishes that for one-dimensional losses in this class, IE and calibration are equivalent under mild conditions. However, a counter-example reveals that this equivalence does not hold in higher dimensions. To address this, the concept of strong IE is introduced as a stronger condition that still offers ease of verification. The study shows that strong IE implies calibration for differentiable surrogates and is necessary and sufficient for strongly convex, differentiable surrogates. The findings highlight the importance of IE and strong IE in the design and analysis of consistent differentiable surrogates for a variety of problems. <div>
arXiv:2505.13760v1 Announce Type: new 
Abstract: The statistical consistency of surrogate losses for discrete prediction tasks is often checked via the condition of calibration. However, directly verifying calibration can be arduous. Recent work shows that for polyhedral surrogates, a less arduous condition, indirect elicitation (IE), is still equivalent to calibration. We give the first results of this type for non-polyhedral surrogates, specifically the class of convex differentiable losses. We first prove that under mild conditions, IE and calibration are equivalent for one-dimensional losses in this class. We construct a counter-example that shows that this equivalence fails in higher dimensions. This motivates the introduction of strong IE, a strengthened form of IE that is equally easy to verify. We establish that strong IE implies calibration for differentiable surrogates and is both necessary and sufficient for strongly convex, differentiable surrogates. Finally, we apply these results to a range of problems to demonstrate the power of IE and strong IE for designing and analyzing consistent differentiable surrogates.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WIND: Accelerated RNN-T Decoding with Windowed Inference for Non-blank Detection</title>
<link>https://arxiv.org/abs/2505.13765</link>
<guid>https://arxiv.org/abs/2505.13765</guid>
<content:encoded><![CDATA[
<div> acceleration, inference, RNN-T, non-blank detection, decoding 
<br /> 
Summary: 
<br /> 
The article introduces Windowed Inference for Non-blank Detection (WIND) as a method to accelerate RNN-T inference without compromising accuracy. WIND processes multiple frames simultaneously within a window, speeding up the model's ability to identify non-blank predictions during decoding. The implementation includes greedy decoding, batched greedy decoding with label-looping techniques, and a novel beam-search decoding method. Experimental results on various datasets demonstrate up to 2.4X speed improvement compared to sequential processing while maintaining the same Word Error Rate (WER) performance. The beam-search algorithm shows slightly better accuracy than alternative methods, with significantly improved speed. The authors plan to release the open-source WIND implementation for further research and development. <div>
arXiv:2505.13765v1 Announce Type: new 
Abstract: We propose Windowed Inference for Non-blank Detection (WIND), a novel strategy that significantly accelerates RNN-T inference without compromising model accuracy. During model inference, instead of processing frames sequentially, WIND processes multiple frames simultaneously within a window in parallel, allowing the model to quickly locate non-blank predictions during decoding, resulting in significant speed-ups. We implement WIND for greedy decoding, batched greedy decoding with label-looping techniques, and also propose a novel beam-search decoding method. Experiments on multiple datasets with different conditions show that our method, when operating in greedy modes, speeds up as much as 2.4X compared to the baseline sequential approach while maintaining identical Word Error Rate (WER) performance. Our beam-search algorithm achieves slightly better accuracy than alternative methods, with significantly improved speed. We will open-source our WIND implementation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis</title>
<link>https://arxiv.org/abs/2505.13768</link>
<guid>https://arxiv.org/abs/2505.13768</guid>
<content:encoded><![CDATA[
<div> RL, hybrid learning framework, offline dataset, online interactions, optimal policy<br />
Summary:<br />
This paper introduces a hybrid learning framework for reinforcement learning (RL) that combines offline datasets and online interactions to improve policy learning. By augmenting confidence-based online RL algorithms with offline data, the proposed algorithm achieves state-of-the-art results in terms of sub-optimality gap and regret minimization. The algorithm's performance is characterized by a sub-optimality gap of $\tilde{O}(\sqrt{1/(N_0/\mathtt{C}(\pi^*|\rho)+N_1}) )$ and a regret speed-up of $\tilde{O}( \sqrt{N_1/(N_0/\mathtt{C}(\pi^{-}|\rho)+N_1)} )$ compared to pure online learning. The study highlights the importance of the offline dataset's coverage properties for different learning metrics. Experimental validation on various RL models, including linear contextual bandits and Markov decision processes, confirms the theoretical findings' effectiveness.<br /> <div>
arXiv:2505.13768v1 Announce Type: new 
Abstract: This paper investigates a hybrid learning framework for reinforcement learning (RL) in which the agent can leverage both an offline dataset and online interactions to learn the optimal policy. We present a unified algorithm and analysis and show that augmenting confidence-based online RL algorithms with the offline dataset outperforms any pure online or offline algorithm alone and achieves state-of-the-art results under two learning metrics, i.e., sub-optimality gap and online learning regret. Specifically, we show that our algorithm achieves a sub-optimality gap $\tilde{O}(\sqrt{1/(N_0/\mathtt{C}(\pi^*|\rho)+N_1}) )$, where $\mathtt{C}(\pi^*|\rho)$ is a new concentrability coefficient, $N_0$ and $N_1$ are the numbers of offline and online samples, respectively. For regret minimization, we show that it achieves a constant $\tilde{O}( \sqrt{N_1/(N_0/\mathtt{C}(\pi^{-}|\rho)+N_1)} )$ speed-up compared to pure online learning, where $\mathtt{C}(\pi^-|\rho)$ is the concentrability coefficient over all sub-optimal policies. Our results also reveal an interesting separation on the desired coverage properties of the offline dataset for sub-optimality gap minimization and regret minimization. We further validate our theoretical findings in several experiments in special RL models such as linear contextual bandits and Markov decision processes (MDPs).
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens</title>
<link>https://arxiv.org/abs/2505.13775</link>
<guid>https://arxiv.org/abs/2505.13775</guid>
<content:encoded><![CDATA[
<div> Chains of Thought, transformer models, reasoning traces, solution accuracy, formal interpreter<br />
Summary:<br />
This paper investigates the impact of intermediate tokens, or "Chains of Thought," on the performance of transformer models trained on reasoning tasks. By training models on formally verifiable reasoning traces and solutions, the study examines the influence of correct intermediate steps on final solution accuracy. Surprisingly, even when trained on entirely correct traces, models still produce invalid reasoning traces while arriving at correct solutions. Furthermore, training models on noisy, corrupted traces shows that trace accuracy does not strongly correlate with solution accuracy. This challenges the assumption that Chains of Thought induce predictable reasoning behaviors and highlights the importance of not over-interpreting such outputs as evidence of human-like behaviors in language models. <div>
arXiv:2505.13775v1 Announce Type: new 
Abstract: Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), and especially of the process of training on CoTs sampled from base LLMs in order to help find new reasoning patterns. In this paper, we critically examine that interpretation by investigating how the semantics of intermediate tokens-often anthropomorphized as "thoughts" or reasoning traces and which are claimed to display behaviors like backtracking, self-verification etc.-actually influence model performance. We train transformer models on formally verifiable reasoning traces and solutions, constraining both intermediate steps and final outputs to align with those of a formal solver (in our case, A* search). By constructing a formal interpreter of the semantics of our problems and intended algorithm, we systematically evaluate not only solution accuracy but also the correctness of intermediate traces, thus allowing us to evaluate whether the latter causally influences the former. We notice that, despite significant improvements on the solution-only baseline, models trained on entirely correct traces still produce invalid reasoning traces when arriving at correct solutions. To further show that trace accuracy is only loosely connected to solution accuracy, we then train models on noisy, corrupted traces which have no relation to the specific problem each is paired with, and find that not only does performance remain largely consistent with models trained on correct data, but in some cases can improve upon it and generalize more robustly on out-of-distribution tasks. These results challenge the assumption that intermediate tokens or "Chains of Thought" induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly correct forms) as evidence of human-like or algorithmic behaviors in language models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference Learning with Lie Detectors can Induce Honesty or Evasion</title>
<link>https://arxiv.org/abs/2505.13787</link>
<guid>https://arxiv.org/abs/2505.13787</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, deceptive behavior, lie detector, training pipeline, policy learning

Summary: 
The study investigates the use of lie detectors in training AI systems to detect deceptive behavior. It explores concerns about contamination and objective hacking when incorporating lie detectors in the training pipeline for large language models. Using a dataset of truthful and deceptive responses, the researchers identify key factors affecting the honesty of learned policies, including exploration during preference learning, lie detector accuracy, and regularization strength. The results show that training with lie detectors can lead to policies that evade detection, with high deception rates, but higher lie detector true positive rates or regularization strength can promote honesty in the learned policies. Off-policy algorithms consistently result in lower deception rates. The findings suggest that the use of lie detectors in training can be effective for oversight but may also pose risks of undetectable misalignment in certain contexts. 

<br /><br />Summary: <div>
arXiv:2505.13787v1 Announce Type: new 
Abstract: As AI systems become more capable, deceptive behaviors can undermine evaluation and mislead users at deployment. Recent work has shown that lie detectors can accurately classify deceptive behavior, but they are not typically used in the training pipeline due to concerns around contamination and objective hacking. We examine these concerns by incorporating a lie detector into the labelling step of LLM post-training and evaluating whether the learned policy is genuinely more honest, or instead learns to fool the lie detector while remaining deceptive. Using DolusChat, a novel 65k-example dataset with paired truthful/deceptive responses, we identify three key factors that determine the honesty of learned policies: amount of exploration during preference learning, lie detector accuracy, and KL regularization strength. We find that preference learning with lie detectors and GRPO can lead to policies which evade lie detectors, with deception rates of over 85\%. However, if the lie detector true positive rate (TPR) or KL regularization is sufficiently high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO) consistently lead to deception rates under 25\% for realistic TPRs. Our results illustrate a more complex picture than previously assumed: depending on the context, lie-detector-enhanced training can be a powerful tool for scalable oversight, or a counterproductive method encouraging undetectable misalignment.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Autoregressive 3D Molecule Generation</title>
<link>https://arxiv.org/abs/2505.13791</link>
<guid>https://arxiv.org/abs/2505.13791</guid>
<content:encoded><![CDATA[
<div> Transformer, autoregressive model, 3D molecule generation, Quetzal, scalability

Summary: 
- Quetzal is a new autoregressive model designed for 3D molecule generation that combines a causal transformer with a Diffusion MLP.
- It outperforms existing autoregressive models in generation quality and competes with state-of-the-art diffusion models.
- Quetzal enables faster generation speed and exact likelihood computation by reducing the number of forward passes through a dense transformer.
- It can handle variable-size tasks like hydrogen decoration and scaffold completion without architectural changes.
- The work highlights the importance of scalability and generality in generative modeling of 3D molecules<br /><br /> <div>
arXiv:2505.13791v1 Announce Type: new 
Abstract: Generative models of 3D molecular structure play a rapidly growing role in the design and simulation of molecules. Diffusion models currently dominate the space of 3D molecule generation, while autoregressive models have trailed behind. In this work, we present Quetzal, a simple but scalable autoregressive model that builds molecules atom-by-atom in 3D. Treating each molecule as an ordered sequence of atoms, Quetzal combines a causal transformer that predicts the next atom's discrete type with a smaller Diffusion MLP that models the continuous next-position distribution. Compared to existing autoregressive baselines, Quetzal achieves substantial improvements in generation quality and is competitive with the performance of state-of-the-art diffusion models. In addition, by reducing the number of expensive forward passes through a dense transformer, Quetzal enables significantly faster generation speed, as well as exact divergence-based likelihood computation. Finally, without any architectural changes, Quetzal natively handles variable-size tasks like hydrogen decoration and scaffold completion. We hope that our work motivates a perspective on scalability and generality for generative modelling of 3D molecules.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Free Synthetic Data Mitigates Forgetting</title>
<link>https://arxiv.org/abs/2505.13811</link>
<guid>https://arxiv.org/abs/2505.13811</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, fine-tuning, catastrophic forgetting, KL divergence, context-free generation

Summary:
Context-free generation is proposed as a method to mitigate catastrophic forgetting in fine-tuning language models without access to training data. By penalizing the KL divergence between the original model and the fine-tuned model, the method helps preserve zero-shot performance and reasoning abilities. The study demonstrates that context-free generations are more effective in preventing forgetting than contextual synthetic data or a portion of pretraining data. Factors such as generation temperature and data ratios are investigated for their impact on model performance. Results on OLMo-1B and R1-Distill-Llama-8B models show the effectiveness of the proposed approach in maintaining and improving model performance during fine-tuning.<br /><br />Summary: Context-free generation is a successful approach for mitigating catastrophic forgetting in fine-tuning language models, preserving zero-shot performance and reasoning abilities. The method outperforms using contextual synthetic data or a portion of pretraining data, and the study investigates the impact of various factors on model performance. Results on different models demonstrate the effectiveness of this approach. <div>
arXiv:2505.13811v1 Announce Type: new 
Abstract: Fine-tuning a language model often results in a degradation of its existing performance on other tasks, due to a shift in the model parameters; this phenomenon is often referred to as (catastrophic) forgetting. We are interested in mitigating this, in settings where we only have access to the model weights but no access to its training data/recipe. A natural approach is to penalize the KL divergence between the original model and the new one. Our main realization is that a simple process - which we term context-free generation - allows for an approximate unbiased estimation of this KL divergence. We show that augmenting a fine-tuning dataset with context-free generations mitigates forgetting, in two settings: (a) preserving the zero-shot performance of pretrained-only models, and (b) preserving the reasoning performance of thinking models. We show that contextual synthetic data, and even a portion of the pretraining data, are less effective. We also investigate the effect of choices like generation temperature, data ratios etc. We present our results for OLMo-1B for pretrained-only setting and R1-Distill-Llama-8B for the reasoning setting.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer</title>
<link>https://arxiv.org/abs/2505.13813</link>
<guid>https://arxiv.org/abs/2505.13813</guid>
<content:encoded><![CDATA[
<div> Kolmogorov-Arnold Network, KAN, Kolmogorov-Arnold Transformer, KAT, Group-Rational KAN, GR-KAN, FlashKAT <br />
Summary: <br />
The Kolmogorov-Arnold Network (KAN) and the Kolmogorov-Arnold Transformer (KAT) are alternative models to the multi-layer perceptron (MLP), offering increased expressiveness and interpretability. The KAT, leveraging Group-Rational KAN (GR-KAN), aims to match FLOPs with traditional Transformers but suffers from significant training slowdown due to memory stalls in the backward pass. A new approach, FlashKAT, addresses this issue by minimizing gradient accumulation through atomic adds and accessing slow memory efficiently. Evaluations show an 86.5x training speedup compared to KAT, reducing rounding errors in coefficient gradients. FlashKAT's implementation is available on GitHub, providing a promising solution to the performance bottlenecks in Kolmogorov-Arnold models. <br /> <div>
arXiv:2505.13813v1 Announce Type: new 
Abstract: The Kolmogorov-Arnold Network (KAN) has been gaining popularity as an alternative to the multi-layer perceptron (MLP) with its increased expressiveness and interpretability. However, the KAN can be orders of magnitude slower due to its increased computational cost and training instability, limiting its applicability to larger-scale tasks. Recently, the Kolmogorov-Arnold Transformer (KAT) has been proposed, which can achieve FLOPs similar to the traditional Transformer with MLPs by leveraging Group-Rational KAN (GR-KAN). Unfortunately, despite the comparable FLOPs, our characterizations reveal that the KAT is still 123x slower in training speeds, indicating that there are other performance bottlenecks beyond FLOPs. In this paper, we conduct a series of experiments to understand the root cause of the slowdown in KAT. We uncover that the slowdown can be isolated to memory stalls and, more specifically, in the backward pass of GR-KAN caused by inefficient gradient accumulation. To address this memory bottleneck, we propose FlashKAT, which builds on our restructured kernel that minimizes gradient accumulation with atomic adds and accesses to slow memory. Evaluations demonstrate that FlashKAT can achieve a training speedup of 86.5x compared with the state-of-the-art KAT, while reducing rounding errors in the coefficient gradients. Our code is available at https://github.com/OSU-STARLAB/FlashKAT.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fragments to Facts: Partial-Information Fragment Inference from LLMs</title>
<link>https://arxiv.org/abs/2505.13819</link>
<guid>https://arxiv.org/abs/2505.13819</guid>
<content:encoded><![CDATA[
<div> Vulnerability, Large language models, Information leakage, Fragment-specific extraction attacks, PRISM<br />
Summary:<br />
Large language models (LLMs) can leak sensitive training data through memorization and membership inference attacks. This paper introduces a more general threat model where adversaries have only partial, unordered sample information, making fine-tuned LLMs susceptible to fragment-specific extraction attacks. Two data-blind methods, a likelihood ratio attack and PRISM, are proposed to systematically investigate these attacks. These methods show competitive performance with a data-aware baseline classifier, highlighting their robustness. Examples from medical and legal domains demonstrate the effectiveness of these attacks in extracting sensitive information from fine-tuned LLMs. This research sheds light on the vulnerabilities of LLMs in protecting sensitive data and emphasizes the need for robust defenses against information leakage through model queries. <br /> <div>
arXiv:2505.13819v1 Announce Type: new 
Abstract: Large language models (LLMs) can leak sensitive training data through memorization and membership inference attacks. Prior work has primarily focused on strong adversarial assumptions, including attacker access to entire samples or long, ordered prefixes, leaving open the question of how vulnerable LLMs are when adversaries have only partial, unordered sample information. For example, if an attacker knows a patient has "hypertension," under what conditions can they query a model fine-tuned on patient data to learn the patient also has "osteoarthritis?" In this paper, we introduce a more general threat model under this weaker assumption and show that fine-tuned LLMs are susceptible to these fragment-specific extraction attacks. To systematically investigate these attacks, we propose two data-blind methods: (1) a likelihood ratio attack inspired by methods from membership inference, and (2) a novel approach, PRISM, which regularizes the ratio by leveraging an external prior. Using examples from both medical and legal settings, we show that both methods are competitive with a data-aware baseline classifier that assumes access to labeled in-distribution data, underscoring their robustness.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Agent Distillation for Large Language Model</title>
<link>https://arxiv.org/abs/2505.13820</link>
<guid>https://arxiv.org/abs/2505.13820</guid>
<content:encoded><![CDATA[
arXiv:2505.13820v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit strong capabilities as decision-making agents by interleaving reasoning and actions, as seen in ReAct-style frameworks. Yet, their practical deployment is constrained by high inference costs and large model sizes. We propose Structured Agent Distillation, a framework that compresses large LLM-based agents into smaller student models while preserving both reasoning fidelity and action consistency. Unlike standard token-level distillation, our method segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align each component with the teacher's behavior. This structure-aware supervision enables compact agents to better replicate the teacher's decision process. Experiments on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop. Scaling and ablation results further highlight the importance of span-level alignment for efficient and deployable agents.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethink the Role of Deep Learning towards Large-scale Quantum Systems</title>
<link>https://arxiv.org/abs/2505.13852</link>
<guid>https://arxiv.org/abs/2505.13852</guid>
<content:encoded><![CDATA[
arXiv:2505.13852v1 Announce Type: new 
Abstract: Characterizing the ground state properties of quantum systems is fundamental to capturing their behavior but computationally challenging. Recent advances in AI have introduced novel approaches, with diverse machine learning (ML) and deep learning (DL) models proposed for this purpose. However, the necessity and specific role of DL models in these tasks remain unclear, as prior studies often employ varied or impractical quantum resources to construct datasets, resulting in unfair comparisons. To address this, we systematically benchmark DL models against traditional ML approaches across three families of Hamiltonian, scaling up to 127 qubits in three crucial ground-state learning tasks while enforcing equivalent quantum resource usage. Our results reveal that ML models often achieve performance comparable to or even exceeding that of DL approaches across all tasks. Furthermore, a randomization test demonstrates that measurement input features have minimal impact on DL models' prediction performance. These findings challenge the necessity of current DL models in many quantum system learning scenarios and provide valuable insights into their effective utilization.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer</title>
<link>https://arxiv.org/abs/2505.13857</link>
<guid>https://arxiv.org/abs/2505.13857</guid>
<content:encoded><![CDATA[
arXiv:2505.13857v1 Announce Type: new 
Abstract: In real-world applications, GPS trajectories often suffer from low sampling rates, with large and irregular intervals between consecutive GPS points. This sparse characteristic presents challenges for their direct use in GPS-based systems. This paper addresses the task of map-constrained trajectory recovery, aiming to enhance trajectory sampling rates of GPS trajectories. Previous studies commonly adopt a sequence-to-sequence framework, where an encoder captures the trajectory patterns and a decoder reconstructs the target trajectory. Within this framework, effectively representing the road network and extracting relevant trajectory features are crucial for overall performance. Despite advancements in these models, they fail to fully leverage the complex spatio-temporal dynamics present in both the trajectory and the road network.
  To overcome these limitations, we categorize the spatio-temporal dynamics of trajectory data into two distinct aspects: spatial-temporal traffic dynamics and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce PD-GNN, which models periodic patterns and learns topologically aware dynamics concurrently for each road segment. For spatio-temporal trajectory dynamics, we present TedFormer, a time-aware Transformer that incorporates temporal dynamics for each GPS location by integrating closed-form neural ordinary differential equations into the attention mechanism. This allows TedFormer to effectively handle irregularly sampled data. Extensive experiments on three real-world datasets demonstrate the superior performance of TedTrajRec. The code is publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules</title>
<link>https://arxiv.org/abs/2505.13858</link>
<guid>https://arxiv.org/abs/2505.13858</guid>
<content:encoded><![CDATA[
arXiv:2505.13858v1 Announce Type: new 
Abstract: Deep learning models are increasingly deployed in safety-critical tasks where predictions must satisfy hard constraints, such as physical laws, fairness requirements, or safety limits. However, standard architectures lack built-in mechanisms to enforce such constraints, and existing approaches based on regularization or projection are often limited to simple constraints, computationally expensive, or lack feasibility guarantees. This paper proposes a model-agnostic framework for enforcing input-dependent linear equality and inequality constraints on neural network outputs. The architecture combines a task network trained for prediction accuracy with a safe network trained using decision rules from the stochastic and robust optimization literature to ensure feasibility across the entire input space. The final prediction is a convex combination of the two subnetworks, guaranteeing constraint satisfaction during both training and inference without iterative procedures or runtime optimization. We prove that the architecture is a universal approximator of constrained functions and derive computationally tractable formulations based on linear decision rules. Empirical results on benchmark regression tasks show that our method consistently satisfies constraints while maintaining competitive accuracy and low inference latency.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model</title>
<link>https://arxiv.org/abs/2505.13873</link>
<guid>https://arxiv.org/abs/2505.13873</guid>
<content:encoded><![CDATA[
arXiv:2505.13873v1 Announce Type: new 
Abstract: Weather forecasting has long posed a significant challenge for humanity. While recent AI-based models have surpassed traditional numerical weather prediction (NWP) methods in global forecasting tasks, overfitting remains a critical issue due to the limited availability of real-world weather data spanning only a few decades. Unlike fields like computer vision or natural language processing, where data abundance can mitigate overfitting, weather forecasting demands innovative strategies to address this challenge with existing data. In this paper, we explore pre-training methods for weather forecasting, finding that selecting an appropriately challenging pre-training task introduces locality bias, effectively mitigating overfitting and enhancing performance. We introduce Baguan, a novel data-driven model for medium-range weather forecasting, built on a Siamese Autoencoder pre-trained in a self-supervised manner and fine-tuned for different lead times. Experimental results show that Baguan outperforms traditional methods, delivering more accurate forecasts. Additionally, the pre-trained Baguan demonstrates robust overfitting control and excels in downstream tasks, such as subseasonal-to-seasonal (S2S) modeling and regional forecasting, after fine-tuning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models</title>
<link>https://arxiv.org/abs/2505.13878</link>
<guid>https://arxiv.org/abs/2505.13878</guid>
<content:encoded><![CDATA[
arXiv:2505.13878v1 Announce Type: new 
Abstract: Model fusion combines multiple Large Language Models (LLMs) with different strengths into a more powerful, integrated model through lightweight training methods. Existing works on model fusion focus primarily on supervised fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for enhancing LLM performance--largely unexplored. The current few fusion methods on PA phase, like WRPO, simplify the process by utilizing only response outputs from source models while discarding their probability information. To address this limitation, we propose InfiFPO, a preference optimization method for implicit model fusion. InfiFPO replaces the reference model in Direct Preference Optimization (DPO) with a fused source model that synthesizes multi-source probabilities at the sequence level, circumventing complex vocabulary alignment challenges in previous works and meanwhile maintaining the probability information. By introducing probability clipping and max-margin fusion strategies, InfiFPO enables the pivot model to align with human preferences while effectively distilling knowledge from source models. Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO consistently outperforms existing model fusion and preference optimization methods. When using Phi-4 as the pivot model, InfiFPO improve its average performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its capabilities in mathematics, coding, and reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAFT: Time Series Forecasting with Cross-Future Behavior Awareness</title>
<link>https://arxiv.org/abs/2505.13896</link>
<guid>https://arxiv.org/abs/2505.13896</guid>
<content:encoded><![CDATA[
arXiv:2505.13896v1 Announce Type: new 
Abstract: The past decades witness the significant advancements in time series forecasting (TSF) across various real-world domains, including e-commerce and disease spread prediction. However, TSF is usually constrained by the uncertainty dilemma of predicting future data with limited past observations. To settle this question, we explore the use of Cross-Future Behavior (CFB) in TSF, which occurs before the current time but takes effect in the future. We leverage CFB features and propose the CRoss-Future Behavior Awareness based Time Series Forecasting method (CRAFT). The core idea of CRAFT is to utilize the trend of cross-future behavior to mine the trend of time series data to be predicted. Specifically, to settle the sparse and partial flaws of cross-future behavior, CRAFT employs the Koopman Predictor Module to extract the key trend and the Internal Trend Mining Module to supplement the unknown area of the cross-future behavior matrix. Then, we introduce the External Trend Guide Module with a hierarchical structure to acquire more representative trends from higher levels. Finally, we apply the demand-constrained loss to calibrate the distribution deviation of prediction results. We conduct experiments on real-world dataset. Experiments on both offline large-scale dataset and online A/B test demonstrate the effectiveness of CRAFT. Our dataset and code is available at https://github.com/CRAFTinTSF/CRAFT.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Language Models Use Their Depth Efficiently?</title>
<link>https://arxiv.org/abs/2505.13898</link>
<guid>https://arxiv.org/abs/2505.13898</guid>
<content:encoded><![CDATA[
arXiv:2505.13898v1 Announce Type: new 
Abstract: Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers? To address these questions, we analyze the residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First, comparing the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, with a clear phase transition between the two halves. Second, skipping layers in the second half has a much smaller effect on future computations and output predictions. Third, for multihop tasks, we are unable to find evidence that models are using increased depth to compose subresults in examples involving many hops. Fourth, we seek to directly address whether deeper models are using their additional layers to perform new kinds of computation. To do this, we train linear maps from the residual stream of a shallow model to a deeper one. We find that layers with the same relative depth map best to each other, suggesting that the larger model simply spreads the same computations out over its many layers. All this evidence suggests that deeper models are not using their depth to learn new kinds of computation, but only using the greater depth to perform more fine-grained adjustments to the residual. This may help explain why increasing scale leads to diminishing returns for stacked Transformer architectures.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Causes of Representational Similarity in Machine Learning Models</title>
<link>https://arxiv.org/abs/2505.13899</link>
<guid>https://arxiv.org/abs/2505.13899</guid>
<content:encoded><![CDATA[
arXiv:2505.13899v1 Announce Type: new 
Abstract: Numerous works have noted significant similarities in how machine learning models represent the world, even across modalities. Although much effort has been devoted to uncovering properties and metrics on which these models align, surprisingly little work has explored causes of this similarity. To advance this line of inquiry, this work explores how two possible causal factors -- dataset overlap and task overlap -- influence downstream model similarity. The exploration of dataset overlap is motivated by the reality that large-scale generative AI models are often trained on overlapping datasets of scraped internet data, while the exploration of task overlap seeks to substantiate claims from a recent work, the Platonic Representation Hypothesis, that task similarity may drive model similarity. We evaluate the effects of both factors through a broad set of experiments. We find that both positively correlate with higher representational similarity and that combining them provides the strongest effect. Our code and dataset are published.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Evidence of the Two-Phase Learning Dynamics of Neural Networks</title>
<link>https://arxiv.org/abs/2505.13900</link>
<guid>https://arxiv.org/abs/2505.13900</guid>
<content:encoded><![CDATA[
arXiv:2505.13900v1 Announce Type: new 
Abstract: Understanding how deep neural networks learn remains a fundamental challenge in modern machine learning. A growing body of evidence suggests that training dynamics undergo a distinct phase transition, yet our understanding of this transition is still incomplete. In this paper, we introduce an interval-wise perspective that compares network states across a time window, revealing two new phenomena that illuminate the two-phase nature of deep learning. i) \textbf{The Chaos Effect.} By injecting an imperceptibly small parameter perturbation at various stages, we show that the response of the network to the perturbation exhibits a transition from chaotic to stable, suggesting there is an early critical period where the network is highly sensitive to initial conditions; ii) \textbf{The Cone Effect.} Tracking the evolution of the empirical Neural Tangent Kernel (eNTK), we find that after this transition point the model's functional trajectory is confined to a narrow cone-shaped subset: while the kernel continues to change, it gets trapped into a tight angular region. Together, these effects provide a structural, dynamical view of how deep networks transition from sensitive exploration to stable refinement during training.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Insert for Constructive Neural Vehicle Routing Solver</title>
<link>https://arxiv.org/abs/2505.13904</link>
<guid>https://arxiv.org/abs/2505.13904</guid>
<content:encoded><![CDATA[
arXiv:2505.13904v1 Announce Type: new 
Abstract: Neural Combinatorial Optimisation (NCO) is a promising learning-based approach for solving Vehicle Routing Problems (VRPs) without extensive manual design. While existing constructive NCO methods typically follow an appending-based paradigm that sequentially adds unvisited nodes to partial solutions, this rigid approach often leads to suboptimal results. To overcome this limitation, we explore the idea of insertion-based paradigm and propose Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel learning-based method for constructive NCO. Unlike traditional approaches, L2C-Insert builds solutions by strategically inserting unvisited nodes at any valid position in the current partial solution, which can significantly enhance the flexibility and solution quality. The proposed framework introduces three key components: a novel model architecture for precise insertion position prediction, an efficient training scheme for model optimization, and an advanced inference technique that fully exploits the insertion paradigm's flexibility. Extensive experiments on both synthetic and real-world instances of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior performance across various problem sizes.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Domain Diffusion with Progressive Alignment for Efficient Adaptive Retrieval</title>
<link>https://arxiv.org/abs/2505.13907</link>
<guid>https://arxiv.org/abs/2505.13907</guid>
<content:encoded><![CDATA[
arXiv:2505.13907v1 Announce Type: new 
Abstract: Unsupervised efficient domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, while maintaining low storage cost and high retrieval efficiency. However, existing methods typically fail to address potential noise in the target domain, and directly align high-level features across domains, thus resulting in suboptimal retrieval performance. To address these challenges, we propose a novel Cross-Domain Diffusion with Progressive Alignment method (COUPLE). This approach revisits unsupervised efficient domain adaptive retrieval from a graph diffusion perspective, simulating cross-domain adaptation dynamics to achieve a stable target domain adaptation process. First, we construct a cross-domain relationship graph and leverage noise-robust graph flow diffusion to simulate the transfer dynamics from the source domain to the target domain, identifying lower noise clusters. We then leverage the graph diffusion results for discriminative hash code learning, effectively learning from the target domain while reducing the negative impact of noise. Furthermore, we employ a hierarchical Mixup operation for progressive domain alignment, which is performed along the cross-domain random walk paths. Utilizing target domain discriminative hash learning and progressive domain alignment, COUPLE enables effective domain adaptive hash learning. Extensive experiments demonstrate COUPLE's effectiveness on competitive benchmarks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models</title>
<link>https://arxiv.org/abs/2505.13910</link>
<guid>https://arxiv.org/abs/2505.13910</guid>
<content:encoded><![CDATA[
arXiv:2505.13910v1 Announce Type: new 
Abstract: Deep learning models often achieve high performance by inadvertently learning spurious correlations between targets and non-essential features. For example, an image classifier may identify an object via its background that spuriously correlates with it. This prediction behavior, known as spurious bias, severely degrades model performance on data that lacks the learned spurious correlations. Existing methods on spurious bias mitigation typically require a variety of data groups with spurious correlation annotations called group labels. However, group labels require costly human annotations and often fail to capture subtle spurious biases such as relying on specific pixels for predictions. In this paper, we propose a novel post hoc spurious bias mitigation framework without requiring group labels. Our framework, termed ShortcutProbe, identifies prediction shortcuts that reflect potential non-robustness in predictions in a given model's latent space. The model is then retrained to be invariant to the identified prediction shortcuts for improved robustness. We theoretically analyze the effectiveness of the framework and empirically demonstrate that it is an efficient and practical tool for improving a model's robustness to spurious bias on diverse datasets.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLVR-World: Training World Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.13934</link>
<guid>https://arxiv.org/abs/2505.13934</guid>
<content:encoded><![CDATA[
arXiv:2505.13934v1 Announce Type: new 
Abstract: World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, a unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both language- and video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers a promising post-training paradigm for enhancing the utility of generative models more broadly.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEVER: A Curated Benchmark for Formally Verified Code Generation</title>
<link>https://arxiv.org/abs/2505.13938</link>
<guid>https://arxiv.org/abs/2505.13938</guid>
<content:encoded><![CDATA[
arXiv:2505.13938v1 Announce Type: new 
Abstract: We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of 161 problems for end-to-end verified code generation in Lean. Each problem consists of (1) the task of generating a specification that matches a held-out ground-truth specification, and (2) the task of generating a Lean implementation that provably satisfies this specification. Unlike prior benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated annotations, and specifications that leak implementation logic or allow vacuous solutions. All outputs are verified post-hoc using Lean's type checker to ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to evaluate several few-shot and agentic approaches based on state-of-the-art language models. These methods all struggle to achieve full verification, establishing it as a challenging frontier benchmark for program synthesis and formal reasoning. Our benchmark can be found on GitHub(https://github.com/trishullab/clever) as well as HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our evaluation code is also available online(https://github.com/trishullab/clever-prover).
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAMO: Efficient Large-Scale Nonconvex Optimization via Adaptive Zeroth Order Variance Reduction</title>
<link>https://arxiv.org/abs/2505.13954</link>
<guid>https://arxiv.org/abs/2505.13954</guid>
<content:encoded><![CDATA[
arXiv:2505.13954v1 Announce Type: new 
Abstract: Optimizing large-scale nonconvex problems, common in machine learning, demands balancing rapid convergence with computational efficiency. First-order (FO) stochastic methods like SVRG provide fast convergence and good generalization but incur high costs due to full-batch gradients in large models. Conversely, zeroth-order (ZO) algorithms reduce this burden using estimated gradients, yet their slow convergence in high-dimensional settings limits practicality. We introduce VAMO (VAriance-reduced Mixed-gradient Optimizer), a stochastic variance-reduced method combining FO mini-batch gradients with lightweight ZO finite-difference probes under an SVRG-style framework. VAMO's hybrid design uses a two-point ZO estimator to achieve a dimension-agnostic convergence rate of $\mathcal{O}(1/T + 1/b)$, where $T$ is the number of iterations and $b$ is the batch-size, surpassing the dimension-dependent slowdown of purely ZO methods and significantly improving over SGD's $\mathcal{O}(1/\sqrt{T})$ rate. Additionally, we propose a multi-point ZO variant that mitigates the $O(1/b)$ error by adjusting number of estimation points to balance convergence and cost, making it ideal for a whole range of computationally constrained scenarios. Experiments including traditional neural network training and LLM finetuning show VAMO outperforms established FO and ZO methods, offering a faster, more flexible option for improved efficiency.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty</title>
<link>https://arxiv.org/abs/2505.13989</link>
<guid>https://arxiv.org/abs/2505.13989</guid>
<content:encoded><![CDATA[
arXiv:2505.13989v1 Announce Type: new 
Abstract: Recently, large language models (LLMs) have significantly advanced text-attributed graph (TAG) learning. However, existing methods inadequately handle data uncertainty in open-world scenarios, especially concerning limited labeling and unknown-class nodes. Prior solutions typically rely on isolated semantic or structural approaches for unknown-class rejection, lacking effective annotation pipelines. To address these limitations, we propose Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive label traceability, which integrates semantics and topology for unknown-class rejection, and a graph label annotator to enable model updates using newly annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and practicality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.14005</link>
<guid>https://arxiv.org/abs/2505.14005</guid>
<content:encoded><![CDATA[
arXiv:2505.14005v1 Announce Type: new 
Abstract: To enhance the reliability and credibility of graph neural networks (GNNs) and improve the transparency of their decision logic, a new field of explainability of GNNs (XGNN) has emerged. However, two major limitations severely degrade the performance and hinder the generalizability of existing XGNN methods: they (a) fail to capture the complete decision logic of GNNs across diverse distributions in the entire dataset's sample space, and (b) impose strict prerequisites on edge properties and GNN internal accessibility. To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as the first work in the literature, can infer and partition the entire dataset's sample space into multiple environments, each containing graphs that follow a distinct distribution. OPEN further learns the decision logic of GNNs across different distributions by sampling subgraphs from each environment and analyzing their predictions, thus eliminating the need for strict prerequisites. Experimental results demonstrate that OPEN captures nearly complete decision logic of GNNs, outperforms state-of-the-art methods in fidelity while maintaining similar efficiency, and enhances robustness in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Sentencing Prediction with Guaranteed Accuracy and Legal Interpretability</title>
<link>https://arxiv.org/abs/2505.14011</link>
<guid>https://arxiv.org/abs/2505.14011</guid>
<content:encoded><![CDATA[
arXiv:2505.14011v1 Announce Type: new 
Abstract: Existing research on judicial sentencing prediction predominantly relies on end-to-end models, which often neglect the inherent sentencing logic and lack interpretability-a critical requirement for both scholarly research and judicial practice. To address this challenge, we make three key contributions:First, we propose a novel Saturated Mechanistic Sentencing (SMS) model, which provides inherent legal interpretability by virtue of its foundation in China's Criminal Law. We also introduce the corresponding Momentum Least Mean Squares (MLMS) adaptive algorithm for this model. Second, for the MLMS algorithm based adaptive sentencing predictor, we establish a mathematical theory on the accuracy of adaptive prediction without resorting to any stationarity and independence assumptions on the data. We also provide a best possible upper bound for the prediction accuracy achievable by the best predictor designed in the known parameters case. Third, we construct a Chinese Intentional Bodily Harm (CIBH) dataset. Utilizing this real-world data, extensive experiments demonstrate that our approach achieves a prediction accuracy that is not far from the best possible theoretical upper bound, validating both the model's suitability and the algorithm's accuracy.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Training from Mean Field Perspective</title>
<link>https://arxiv.org/abs/2505.14021</link>
<guid>https://arxiv.org/abs/2505.14021</guid>
<content:encoded><![CDATA[
arXiv:2505.14021v1 Announce Type: new 
Abstract: Although adversarial training is known to be effective against adversarial examples, training dynamics are not well understood. In this study, we present the first theoretical analysis of adversarial training in random deep neural networks without any assumptions on data distributions. We introduce a new theoretical framework based on mean field theory, which addresses the limitations of existing mean field-based approaches. Based on this framework, we derive (empirically tight) upper bounds of $\ell_q$ norm-based adversarial loss with $\ell_p$ norm-based adversarial examples for various values of $p$ and $q$. Moreover, we prove that networks without shortcuts are generally not adversarially trainable and that adversarial training reduces network capacity. We also show that network width alleviates these issues. Furthermore, we present the various impacts of the input and output dimensions on the upper bounds and time evolution of the weight variance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix</title>
<link>https://arxiv.org/abs/2505.14024</link>
<guid>https://arxiv.org/abs/2505.14024</guid>
<content:encoded><![CDATA[
arXiv:2505.14024v1 Announce Type: new 
Abstract: Federated Learning (FL) enables geographically distributed clients to collaboratively train machine learning models by sharing only their local models, ensuring data privacy. However, FL is vulnerable to untargeted attacks that aim to degrade the global model's performance on the underlying data distribution. Existing defense mechanisms attempt to improve FL's resilience against such attacks, but their effectiveness is limited in practical FL environments due to data heterogeneity. On the contrary, we aim to detect and remove the attacks to mitigate their impact. Generalization contribution plays a crucial role in distinguishing untargeted attacks. Our observations indicate that, with limited data, the divergence between embeddings representing different classes provides a better measure of generalization than direct accuracy. In light of this, we propose a novel robust aggregation method, FedGraM, designed to defend against untargeted attacks in FL. The server maintains an auxiliary dataset containing one sample per class to support aggregation. This dataset is fed to the local models to extract embeddings. Then, the server calculates the norm of the Gram Matrix of the embeddings for each local model. The norm serves as an indicator of each model's inter-class separation capability in the embedding space. FedGraM identifies and removes potentially malicious models by filtering out those with the largest norms, then averages the remaining local models to form the global model. We conduct extensive experiments to evaluate the performance of FedGraM. Our empirical results show that with limited data samples used to construct the auxiliary dataset, FedGraM achieves exceptional performance, outperforming state-of-the-art defense methods.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partition-wise Graph Filtering: A Unified Perspective Through the Lens of Graph Coarsening</title>
<link>https://arxiv.org/abs/2505.14033</link>
<guid>https://arxiv.org/abs/2505.14033</guid>
<content:encoded><![CDATA[
arXiv:2505.14033v1 Announce Type: new 
Abstract: Filtering-based graph neural networks (GNNs) constitute a distinct class of GNNs that employ graph filters to handle graph-structured data, achieving notable success in various graph-related tasks. Conventional methods adopt a graph-wise filtering paradigm, imposing a uniform filter across all nodes, yet recent findings suggest that this rigid paradigm struggles with heterophilic graphs. To overcome this, recent works have introduced node-wise filtering, which assigns distinct filters to individual nodes, offering enhanced adaptability. However, a fundamental gap remains: a comprehensive framework unifying these two strategies is still absent, limiting theoretical insights into the filtering paradigms. Moreover, through the lens of Contextual Stochastic Block Model, we reveal that a synthesis of graph-wise and node-wise filtering provides a sufficient solution for classification on graphs exhibiting both homophily and heterophily, suggesting the risk of excessive parameterization and potential overfitting with node-wise filtering. To address the limitations, this paper introduces Coarsening-guided Partition-wise Filtering (CPF). CPF innovates by performing filtering on node partitions. The method begins with structure-aware partition-wise filtering, which filters node partitions obtained via graph coarsening algorithms, and then performs feature-aware partition-wise filtering, refining node embeddings via filtering on clusters produced by $k$-means clustering over features. In-depth analysis is conducted for each phase of CPF, showing its superiority over other paradigms. Finally, benchmark node classification experiments, along with a real-world graph anomaly detection application, validate CPF's efficacy and practical utility.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Cyclic Diffusion for Inference Scaling</title>
<link>https://arxiv.org/abs/2505.14036</link>
<guid>https://arxiv.org/abs/2505.14036</guid>
<content:encoded><![CDATA[
arXiv:2505.14036v1 Announce Type: new 
Abstract: Diffusion models have demonstrated strong generative capabilities across domains ranging from image synthesis to complex reasoning tasks. However, most inference-time scaling methods rely on fixed denoising schedules, limiting their ability to allocate computation based on instance difficulty or task-specific demands adaptively. We introduce the challenge of adaptive inference-time scaling-dynamically adjusting computational effort during inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a flexible, search-based inference framework. ABCD refines outputs through bi-directional diffusion cycles while adaptively controlling exploration depth and termination. It comprises three components: Cyclic Diffusion Search, Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time. Experiments show that ABCD improves performance across diverse tasks while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning High-dimensional Ionic Model Dynamics Using Fourier Neural Operators</title>
<link>https://arxiv.org/abs/2505.14039</link>
<guid>https://arxiv.org/abs/2505.14039</guid>
<content:encoded><![CDATA[
arXiv:2505.14039v1 Announce Type: new 
Abstract: Ionic models, described by systems of stiff ordinary differential equations, are fundamental tools for simulating the complex dynamics of excitable cells in both Computational Neuroscience and Cardiology. Approximating these models using Artificial Neural Networks poses significant challenges due to their inherent stiffness, multiscale nonlinearities, and the wide range of dynamical behaviors they exhibit, including multiple equilibrium points, limit cycles, and intricate interactions. While in previous studies the dynamics of the transmembrane potential has been predicted in low dimensionality settings, in the present study we extend these results by investigating whether Fourier Neural Operators can effectively learn the evolution of all the state variables within these dynamical systems in higher dimensions. We demonstrate the effectiveness of this approach by accurately learning the dynamics of three well-established ionic models with increasing dimensionality: the two-variable FitzHugh-Nagumo model, the four-variable Hodgkin-Huxley model, and the forty-one-variable O'Hara-Rudy model. To ensure the selection of near-optimal configurations for the Fourier Neural Operator, we conducted automatic hyperparameter tuning under two scenarios: an unconstrained setting, where the number of trainable parameters is not limited, and a constrained case with a fixed number of trainable parameters. Both constrained and unconstrained architectures achieve comparable results in terms of accuracy across all the models considered. However, the unconstrained architecture required approximately half the number of training epochs to achieve similar error levels, as evidenced by the loss function values recorded during training. These results underline the capabilities of Fourier Neural Operators to accurately capture complex multiscale dynamics, even in high-dimensional dynamical systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Graph Clustering with Deep Structural Entropy</title>
<link>https://arxiv.org/abs/2505.14040</link>
<guid>https://arxiv.org/abs/2505.14040</guid>
<content:encoded><![CDATA[
arXiv:2505.14040v1 Announce Type: new 
Abstract: Research on Graph Structure Learning (GSL) provides key insights for graph-based clustering, yet current methods like Graph Neural Networks (GNNs), Graph Attention Networks (GATs), and contrastive learning often rely heavily on the original graph structure. Their performance deteriorates when the original graph's adjacency matrix is too sparse or contains noisy edges unrelated to clustering. Moreover, these methods depend on learning node embeddings and using traditional techniques like k-means to form clusters, which may not fully capture the underlying graph structure between nodes. To address these limitations, this paper introduces DeSE, a novel unsupervised graph clustering framework incorporating Deep Structural Entropy. It enhances the original graph with quantified structural information and deep neural networks to form clusters. Specifically, we first propose a method for calculating structural entropy with soft assignment, which quantifies structure in a differentiable form. Next, we design a Structural Learning layer (SLL) to generate an attributed graph from the original feature data, serving as a target to enhance and optimize the original structural graph, thereby mitigating the issue of sparse connections between graph nodes. Finally, our clustering assignment method (ASS), based on GNNs, learns node embeddings and a soft assignment matrix to cluster on the enhanced graph. The ASS layer can be stacked to meet downstream task requirements, minimizing structural entropy for stable clustering and maximizing node consistency with edge-based cross-entropy loss. Extensive comparative experiments are conducted on four benchmark datasets against eight representative unsupervised graph clustering baselines, demonstrating the superiority of the DeSE in both effectiveness and interpretability.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarially Pretrained Transformers may be Universally Robust In-Context Learners</title>
<link>https://arxiv.org/abs/2505.14042</link>
<guid>https://arxiv.org/abs/2505.14042</guid>
<content:encoded><![CDATA[
arXiv:2505.14042v1 Announce Type: new 
Abstract: Adversarial training is one of the most effective adversarial defenses, but it incurs a high computational cost. In this study, we show that transformers adversarially pretrained on diverse tasks can serve as robust foundation models and eliminate the need for adversarial training in downstream tasks. Specifically, we theoretically demonstrate that through in-context learning, a single adversarially pretrained transformer can robustly generalize to multiple unseen tasks without any additional training, i.e., without any parameter updates. This robustness stems from the model's focus on robust features and its resistance to attacks that exploit non-predictive features. Besides these positive findings, we also identify several limitations. Under certain conditions (though unrealistic), no universally robust single-layer transformers exist. Moreover, robust transformers exhibit an accuracy--robustness trade-off and require a large number of in-context demonstrations. The code is available at https://github.com/s-kumano/universally-robust-in-context-learner.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Category Discovery via Token Manifold Capacity Learning</title>
<link>https://arxiv.org/abs/2505.14044</link>
<guid>https://arxiv.org/abs/2505.14044</guid>
<content:encoded><![CDATA[
arXiv:2505.14044v1 Announce Type: new 
Abstract: Generalized category discovery (GCD) is essential for improving deep learning models' robustness in open-world scenarios by clustering unlabeled data containing both known and novel categories. Traditional GCD methods focus on minimizing intra-cluster variations, often sacrificing manifold capacity, which limits the richness of intra-class representations. In this paper, we propose a novel approach, Maximum Token Manifold Capacity (MTMC), that prioritizes maximizing the manifold capacity of class tokens to preserve the diversity and complexity of data. MTMC leverages the nuclear norm of singular values as a measure of manifold capacity, ensuring that the representation of samples remains informative and well-structured. This method enhances the discriminability of clusters, allowing the model to capture detailed semantic features and avoid the loss of critical information during clustering. Through theoretical analysis and extensive experiments on coarse- and fine-grained datasets, we demonstrate that MTMC outperforms existing GCD methods, improving both clustering accuracy and the estimation of category numbers. The integration of MTMC leads to more complete representations, better inter-class separability, and a reduction in dimensional collapse, establishing MTMC as a vital component for robust open-world learning. Code is in github.com/lytang63/MTMC.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.14071</link>
<guid>https://arxiv.org/abs/2505.14071</guid>
<content:encoded><![CDATA[
arXiv:2505.14071v1 Announce Type: new 
Abstract: Steering methods have emerged as effective and targeted tools for guiding large language models' (LLMs) behavior without modifying their parameters. Multimodal large language models (MLLMs), however, do not currently enjoy the same suite of techniques, due in part to their recency and architectural diversity. Inspired by this gap, we investigate whether MLLMs can be steered using vectors derived from their text-only LLM backbone, via sparse autoencoders (SAEs), mean shift, and linear probing. We find that text-derived steering consistently enhances multimodal accuracy across diverse MLLM architectures and visual tasks. In particular, mean shift boosts spatial relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to +3.3%, outperforming prompting and exhibiting strong generalization to out-of-distribution datasets. These results highlight textual steering vectors as a powerful, efficient mechanism for enhancing grounding in MLLMs with minimal additional data collection and computational overhead.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Unlabeled Data Optimization</title>
<link>https://arxiv.org/abs/2505.14117</link>
<guid>https://arxiv.org/abs/2505.14117</guid>
<content:encoded><![CDATA[
arXiv:2505.14117v1 Announce Type: new 
Abstract: This paper pioneers a novel data-centric paradigm to maximize the utility of unlabeled data, tackling a critical question: How can we enhance the efficiency and sustainability of deep learning training by optimizing the data itself? We begin by identifying three key limitations in existing model-centric approaches, all rooted in a shared bottleneck: knowledge extracted from data is locked to model parameters, hindering its reusability and scalability. To this end, we propose CoOpt, a highly efficient, parallelized framework for collaborative unlabeled data optimization, thereby effectively encoding knowledge into the data itself. By distributing unlabeled data and leveraging publicly available task-agnostic models, CoOpt facilitates scalable, reusable, and sustainable training pipelines. Extensive experiments across diverse datasets and architectures demonstrate its efficacy and efficiency, achieving 13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively, with training speedups of $1.94 \times $ and $1.2 \times$.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing wildfire susceptibility in Iran: Leveraging machine learning for geospatial analysis of climatic and anthropogenic factors</title>
<link>https://arxiv.org/abs/2505.14122</link>
<guid>https://arxiv.org/abs/2505.14122</guid>
<content:encoded><![CDATA[
arXiv:2505.14122v1 Announce Type: new 
Abstract: This study investigates the multifaceted factors influencing wildfire risk in Iran, focusing on the interplay between climatic conditions and human activities. Utilizing advanced remote sensing, geospatial information system (GIS) processing techniques such as cloud computing, and machine learning algorithms, this research analyzed the impact of climatic parameters, topographic features, and human-related factors on wildfire susceptibility assessment and prediction in Iran. Multiple scenarios were developed for this purpose based on the data sampling strategy. The findings revealed that climatic elements such as soil moisture, temperature, and humidity significantly contribute to wildfire susceptibility, while human activities-particularly population density and proximity to powerlines-also played a crucial role. Furthermore, the seasonal impact of each parameter was separately assessed during warm and cold seasons. The results indicated that human-related factors, rather than climatic variables, had a more prominent influence during the seasonal analyses. This research provided new insights into wildfire dynamics in Iran by generating high-resolution wildfire susceptibility maps using advanced machine learning classifiers. The generated maps identified high risk areas, particularly in the central Zagros region, the northeastern Hyrcanian Forest, and the northern Arasbaran forest, highlighting the urgent need for effective fire management strategies.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning</title>
<link>https://arxiv.org/abs/2505.14125</link>
<guid>https://arxiv.org/abs/2505.14125</guid>
<content:encoded><![CDATA[
arXiv:2505.14125v1 Announce Type: new 
Abstract: Biological brains learn continually from a stream of unlabeled data, while integrating specialized information from sparsely labeled examples without compromising their ability to generalize. Meanwhile, machine learning methods are susceptible to catastrophic forgetting in this natural learning setting, as supervised specialist fine-tuning degrades performance on the original task. We introduce task-modulated contrastive learning (TMCL), which takes inspiration from the biophysical machinery in the neocortex, using predictive coding principles to integrate top-down information continually and without supervision. We follow the idea that these principles build a view-invariant representation space, and that this can be implemented using a contrastive loss. Then, whenever labeled samples of a new class occur, new affine modulations are learned that improve separation of the new class from all others, without affecting feedforward weights. By co-opting the view-invariance learning mechanism, we then train feedforward weights to match the unmodulated representation of a data sample to its modulated counterparts. This introduces modulation invariance into the representation space, and, by also using past modulations, stabilizes it. Our experiments show improvements in both class-incremental and transfer learning over state-of-the-art unsupervised approaches, as well as over comparable supervised approaches, using as few as 1% of available labels. Taken together, our work suggests that top-down modulations play a crucial role in balancing stability and plasticity.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow</title>
<link>https://arxiv.org/abs/2505.14126</link>
<guid>https://arxiv.org/abs/2505.14126</guid>
<content:encoded><![CDATA[
arXiv:2505.14126v1 Announce Type: new 
Abstract: Knowledge components (KCs) are the fundamental units of knowledge in the field of education. A KC graph illustrates the relationships and dependencies between KCs. An accurate KC graph can assist educators in identifying the root causes of learners' poor performance on specific KCs, thereby enabling targeted instructional interventions. To achieve this, we have developed a KC graph structure learning algorithm, named MAS-KCL, which employs a multi-agent system driven by large language models for adaptive modification and optimization of the KC graph. Additionally, a bidirectional feedback mechanism is integrated into the algorithm, where AI agents leverage this mechanism to assess the value of edges within the KC graph and adjust the distribution of generation probabilities for different edges, thereby accelerating the efficiency of structure learning. We applied the proposed algorithm to 5 synthetic datasets and 4 real-world educational datasets, and experimental results validate its effectiveness in learning path recognition. By accurately identifying learners' learning paths, teachers are able to design more comprehensive learning plans, enabling learners to achieve their educational goals more effectively, thus promoting the sustainable development of education.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Methodological Framework for Measuring Spatial Labeling Similarity</title>
<link>https://arxiv.org/abs/2505.14128</link>
<guid>https://arxiv.org/abs/2505.14128</guid>
<content:encoded><![CDATA[
arXiv:2505.14128v1 Announce Type: new 
Abstract: Spatial labeling assigns labels to specific spatial locations to characterize their spatial properties and relationships, with broad applications in scientific research and practice. Measuring the similarity between two spatial labelings is essential for understanding their differences and the contributing factors, such as changes in location properties or labeling methods. An adequate and unbiased measurement of spatial labeling similarity should consider the number of matched labels (label agreement), the topology of spatial label distribution, and the heterogeneous impacts of mismatched labels. However, existing methods often fail to account for all these aspects. To address this gap, we propose a methodological framework to guide the development of methods that meet these requirements. Given two spatial labelings, the framework transforms them into graphs based on location organization, labels, and attributes (e.g., location significance). The distributions of their graph attributes are then extracted, enabling an efficient computation of distributional discrepancy to reflect the dissimilarity level between the two labelings. We further provide a concrete implementation of this framework, termed Spatial Labeling Analogy Metric (SLAM), along with an analysis of its theoretical foundation, for evaluating spatial labeling results in spatial transcriptomics (ST) \textit{as per} their similarity with ground truth labeling. Through a series of carefully designed experimental cases involving both simulated and real ST data, we demonstrate that SLAM provides a comprehensive and accurate reflection of labeling quality compared to other well-established evaluation metrics. Our code is available at https://github.com/YihDu/SLAM.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging</title>
<link>https://arxiv.org/abs/2505.14136</link>
<guid>https://arxiv.org/abs/2505.14136</guid>
<content:encoded><![CDATA[
arXiv:2505.14136v1 Announce Type: new 
Abstract: Mixture of expert (MoE) models are a promising approach to increasing model capacity without increasing inference cost, and are core components of many state-of-the-art language models. However, current MoE models typically use only few experts due to prohibitive training and inference cost. We propose Test-Time Model Merging (TTMM) which scales the MoE paradigm to an order of magnitude more experts and uses model merging to avoid almost any test-time overhead. We show that TTMM is an approximation of test-time training (TTT), which fine-tunes an expert model for each prediction task, i.e., prompt. TTT has recently been shown to significantly improve language models, but is computationally expensive. We find that performance of TTMM improves with more experts and approaches the performance of TTT. Moreover, we find that with a 1B parameter base model, TTMM is more than 100x faster than TTT at test-time by amortizing the cost of TTT at train-time. Thus, TTMM offers a promising cost-effective approach to scale test-time training.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.14139</link>
<guid>https://arxiv.org/abs/2505.14139</guid>
<content:encoded><![CDATA[
arXiv:2505.14139v1 Announce Type: new 
Abstract: The use of guidance to steer sampling toward desired outcomes has been widely explored within diffusion models, especially in applications such as image and trajectory generation. However, incorporating guidance during training remains relatively underexplored. In this work, we introduce energy-guided flow matching, a novel approach that enhances the training of flow models and eliminates the need for guidance at inference time. We learn a conditional velocity field corresponding to the flow policy by approximating an energy-guided probability path as a Gaussian path. Learning guided trajectories is appealing for tasks where the target distribution is defined by a combination of data and an energy function, as in reinforcement learning. Diffusion-based policies have recently attracted attention for their expressive power and ability to capture multi-modal action distributions. Typically, these policies are optimized using weighted objectives or by back-propagating gradients through actions sampled by the policy. As an alternative, we propose FlowQ, an offline reinforcement learning algorithm based on energy-guided flow matching. Our method achieves competitive performance while the policy training time is constant in the number of flow sampling steps.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation</title>
<link>https://arxiv.org/abs/2505.14161</link>
<guid>https://arxiv.org/abs/2505.14161</guid>
<content:encoded><![CDATA[
arXiv:2505.14161v1 Announce Type: new 
Abstract: Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client data and quantifies uncertainty by combining personalization with Bayesian inference. However, existing PBFL methods face two limitations: restrictive parametric assumptions in client posterior inference and naive parameter averaging for server aggregation. To overcome these issues, we propose FedWBA, a novel PBFL method that enhances both local inference and global aggregation. At the client level, we use particle-based variational inference for nonparametric posterior representation. At the server level, we introduce particle-based Wasserstein barycenter aggregation, offering a more geometrically meaningful approach. Theoretically, we provide local and global convergence guarantees for FedWBA. Locally, we prove a KL divergence decrease lower bound per iteration for variational inference convergence. Globally, we show that the Wasserstein barycenter converges to the true parameter as the client data size increases. Empirically, experiments show that FedWBA outperforms baselines in prediction accuracy, uncertainty calibration, and convergence rate, with ablation studies confirming its robustness.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonparametric Teaching for Graph Property Learners</title>
<link>https://arxiv.org/abs/2505.14170</link>
<guid>https://arxiv.org/abs/2505.14170</guid>
<content:encoded><![CDATA[
arXiv:2505.14170v1 Announce Type: new 
Abstract: Inferring properties of graph-structured data, e.g., the solubility of molecules, essentially involves learning the implicit mapping from graphs to their properties. This learning process is often costly for graph property learners like Graph Convolutional Networks (GCNs). To address this, we propose a paradigm called Graph Neural Teaching (GraNT) that reinterprets the learning process through a novel nonparametric teaching perspective. Specifically, the latter offers a theoretical framework for teaching implicitly defined (i.e., nonparametric) mappings via example selection. Such an implicit mapping is realized by a dense set of graph-property pairs, with the GraNT teacher selecting a subset of them to promote faster convergence in GCN training. By analytically examining the impact of graph structure on parameter-based gradient descent during training, and recasting the evolution of GCNs--shaped by parameter updates--through functional gradient descent in nonparametric teaching, we show for the first time that teaching graph property learners (i.e., GCNs) is consistent with teaching structure-aware nonparametric learners. These new findings readily commit GraNT to enhancing learning efficiency of the graph property learner, showing significant reductions in training time for graph-level regression (-36.62%), graph-level classification (-38.19%), node-level regression (-30.97%) and node-level classification (-47.30%), all while maintaining its generalization performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Subspaces are Not Distinct: A Fine-Tuning Case Study</title>
<link>https://arxiv.org/abs/2505.14185</link>
<guid>https://arxiv.org/abs/2505.14185</guid>
<content:encoded><![CDATA[
arXiv:2505.14185v1 Announce Type: new 
Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. This is typically achieved through instruction tuning and reinforcement learning from human feedback. However, this alignment is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable geometric directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this geometric perspective. We examine whether safety-relevant behavior is concentrated in specific subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in internal representations. Across both parameter and activation space, our findings are consistent: subspaces that amplify safe behaviors also amplify unsafe ones, and prompts with different safety implications activate overlapping representations. We find no evidence of a subspace that selectively governs safety. These results challenge the assumption that alignment is geometrically localized. Rather than residing in distinct directions, safety appears to emerge from entangled, high-impact components of the model's broader learning dynamics. This suggests that subspace-based defenses may face fundamental limitations and underscores the need for alternative strategies to preserve alignment under continued training. We corroborate these findings through multiple experiments on five open-source LLMs. Our code is publicly available at: https://github.com/CERT-Lab/safety-subspaces.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\alpha$-GAN by R\'{e}nyi Cross Entropy</title>
<link>https://arxiv.org/abs/2505.14190</link>
<guid>https://arxiv.org/abs/2505.14190</guid>
<content:encoded><![CDATA[
arXiv:2505.14190v1 Announce Type: new 
Abstract: This paper proposes $\alpha$-GAN, a generative adversarial network using R\'{e}nyi measures. The value function is formulated, by R\'{e}nyi cross entropy, as an expected certainty measure incurred by the discriminator's soft decision as to where the sample is from, true population or the generator. The discriminator tries to maximize the R\'{e}nyi certainty about sample source, while the generator wants to reduce it by injecting fake samples. This forms a min-max problem with the solution parameterized by the R\'{e}nyi order $\alpha$. This $\alpha$-GAN reduces to vanilla GAN at $\alpha = 1$, where the value function is exactly the binary cross entropy. The optimization of $\alpha$-GAN is over probability (vector) space. It is shown that the gradient is exponentially enlarged when R\'{e}nyi order is in the range $\alpha \in (0,1)$. This makes convergence faster, which is verified by experimental results. A discussion shows that choosing $\alpha \in (0,1)$ may be able to solve some common problems, e.g., vanishing gradient. A following observation reveals that this range has not been fully explored in the existing R\'{e}nyi version GANs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLASH-D: FlashAttention with Hidden Softmax Division</title>
<link>https://arxiv.org/abs/2505.14201</link>
<guid>https://arxiv.org/abs/2505.14201</guid>
<content:encoded><![CDATA[
arXiv:2505.14201v1 Announce Type: new 
Abstract: The transformer's attention mechanism has revolutionized AI and machine learning, with its efficient computation being crucial to its performance. However, calculating attention involves matrix operations interspersed with softmax rescaling, which inherently slows down computation and requires processing the entire input sequence. Building on online softmax computation, FlashAttention integrates softmax calculation with matrix arithmetic, enabling tiled computation independent of sequence length. While optimized for GPUs, FlashAttention's simplicity makes it amenable to direct hardware acceleration. This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a mathematically equivalent, yet simplified, formulation that achieves: (a) hiding softmax division within other non-linear function evaluations; (b) inherently numerically stable computation of exponentials, eliminating the need for maximum value subtraction; and (c) a reduction in computational cost without introducing numerical approximations to the FlashAttention kernel. Importantly, the essential FlashAttention properties that facilitate efficient tiled implementation are fully preserved. Hardware implementation results at 28nm demonstrate that this proposed formulation achieves a 22.8% reduction in area and a 20.3% reduction in power, on average, compared to state-of-the-art parallel hardware architectures without any performance penalty.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSDformer: Multi-scale Discrete Transformer For Time Series Generation</title>
<link>https://arxiv.org/abs/2505.14202</link>
<guid>https://arxiv.org/abs/2505.14202</guid>
<content:encoded><![CDATA[
arXiv:2505.14202v1 Announce Type: new 
Abstract: Discrete Token Modeling (DTM), which employs vector quantization techniques, has demonstrated remarkable success in modeling non-natural language modalities, particularly in time series generation. While our prior work SDformer established the first DTM-based framework to achieve state-of-the-art performance in this domain, two critical limitations persist in existing DTM approaches: 1) their inability to capture multi-scale temporal patterns inherent to complex time series data, and 2) the absence of theoretical foundations to guide model optimization. To address these challenges, we proposes a novel multi-scale DTM-based time series generation method, called Multi-Scale Discrete Transformer (MSDformer). MSDformer employs a multi-scale time series tokenizer to learn discrete token representations at multiple scales, which jointly characterize the complex nature of time series data. Subsequently, MSDformer applies a multi-scale autoregressive token modeling technique to capture the multi-scale patterns of time series within the discrete latent space. Theoretically, we validate the effectiveness of the DTM method and the rationality of MSDformer through the rate-distortion theorem. Comprehensive experiments demonstrate that MSDformer significantly outperforms state-of-the-art methods. Both theoretical analysis and experimental results demonstrate that incorporating multi-scale information and modeling multi-scale patterns can substantially enhance the quality of generated time series in DTM-based approaches. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data</title>
<link>https://arxiv.org/abs/2505.14206</link>
<guid>https://arxiv.org/abs/2505.14206</guid>
<content:encoded><![CDATA[
arXiv:2505.14206v1 Announce Type: new 
Abstract: The widespread adoption of mobile sensors has the potential to provide massive and heterogeneous time series data, driving Artificial Intelligence applications in mHealth. However, data collection remains limited due to stringent ethical regulations, privacy concerns, and other constraints, hindering progress in the field. Synthetic data generation, particularly through Generative Adversarial Networks and Diffusion Models, has emerged as a promising solution to address both data scarcity and privacy issues. Yet, these models are often limited to short-term, unimodal signal patterns. This paper presents a systematic evaluation of state-of-the-art generative models for time series synthesis, with a focus on their ability to jointly handle multi-modality, long-range dependencies, and conditional generation-key challenges in the mHealth domain. To ensure a fair comparison, we introduce a novel evaluation framework designed to measure both the intrinsic quality of synthetic data and its utility in downstream predictive tasks. Our findings reveal critical limitations in the existing approaches, particularly in maintaining cross-modal consistency, preserving temporal coherence, and ensuring robust performance in train-on-synthetic, test-on-real, and data augmentation scenarios. Finally, we present our future research directions to enhance synthetic time series generation and improve the applicability of generative models in mHealth.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A PID-Controlled Tensor Wheel Decomposition Model for Dynamic Link Prediction</title>
<link>https://arxiv.org/abs/2505.14211</link>
<guid>https://arxiv.org/abs/2505.14211</guid>
<content:encoded><![CDATA[
arXiv:2505.14211v1 Announce Type: new 
Abstract: Link prediction in dynamic networks remains a fundamental challenge in network science, requiring the inference of potential interactions and their evolving strengths through spatiotemporal pattern analysis. Traditional static network methods have inherent limitations in capturing temporal dependencies and weight dynamics, while tensor-based methods offer a promising paradigm by encoding dynamic networks into high-order tensors to explicitly model multidimensional interactions across nodes and time. Among them, tensor wheel decomposition (TWD) stands out for its innovative topological structure, which decomposes high-order tensors into cyclic factors and core tensors to maintain structural integrity. To improve the prediction accuracy, this study introduces a PID-controlled tensor wheel decomposition (PTWD) model, which mainly adopts the following two ideas: 1) exploiting the representation power of TWD to capture the latent features of dynamic network topology and weight evolution, and 2) integrating the proportional-integral-derivative (PID) control principle into the optimization process to obtain a stable model parameter learning scheme. The performance on four real datasets verifies that the proposed PTWD model has more accurate link prediction capabilities compared to other models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regularized least squares learning with heavy-tailed noise is minimax optimal</title>
<link>https://arxiv.org/abs/2505.14214</link>
<guid>https://arxiv.org/abs/2505.14214</guid>
<content:encoded><![CDATA[
arXiv:2505.14214v1 Announce Type: new 
Abstract: This paper examines the performance of ridge regression in reproducing kernel Hilbert spaces in the presence of noise that exhibits a finite number of higher moments. We establish excess risk bounds consisting of subgaussian and polynomial terms based on the well known integral operator framework. The dominant subgaussian component allows to achieve convergence rates that have previously only been derived under subexponential noise - a prevalent assumption in related work from the last two decades. These rates are optimal under standard eigenvalue decay conditions, demonstrating the asymptotic robustness of regularized least squares against heavy-tailed noise. Our derivations are based on a Fuk-Nagaev inequality for Hilbert-space valued random variables.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned</title>
<link>https://arxiv.org/abs/2505.14217</link>
<guid>https://arxiv.org/abs/2505.14217</guid>
<content:encoded><![CDATA[
arXiv:2505.14217v1 Announce Type: new 
Abstract: This study explores the use of Federated Learning (FL) for tuberculosis (TB) diagnosis using chest X-rays in low-resource settings across Africa. FL allows hospitals to collaboratively train AI models without sharing raw patient data, addressing privacy concerns and data scarcity that hinder traditional centralized models. The research involved hospitals and research centers in eight African countries. Most sites used local datasets, while Ghana and The Gambia used public ones. The study compared locally trained models with a federated model built across all institutions to evaluate FL's real-world feasibility. Despite its promise, implementing FL in sub-Saharan Africa faces challenges such as poor infrastructure, unreliable internet, limited digital literacy, and weak AI regulations. Some institutions were also reluctant to share model updates due to data control concerns. In conclusion, FL shows strong potential for enabling AI-driven healthcare in underserved regions, but broader adoption will require improvements in infrastructure, education, and regulatory support.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and close Shannon entropy approximation</title>
<link>https://arxiv.org/abs/2505.14234</link>
<guid>https://arxiv.org/abs/2505.14234</guid>
<content:encoded><![CDATA[
arXiv:2505.14234v1 Announce Type: new 
Abstract: Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy are key components in many tools used in physics, information theory, machine learning (ML) and quantum computing. Besides of the significant amounts of SE computations required in these fields, the singularity of the SE gradient is one of the central mathematical reason inducing the high cost, frequently low robustness and slow convergence of such tools. Here we propose the Fast Entropy Approximation (FEA) - a non-singular rational approximation of Shannon entropy and its gradient that achieves a mean absolute error of $10^{-3}$, which is approximately $20$ times lower than comparable state-of-the-art methods. FEA allows around $50\%$ faster computation, requiring only $5$ to $6$ elementary computational operations, as compared to tens of elementary operations behind the fastest entropy computation algorithms with table look-ups, bitshifts, or series approximations. On a set of common benchmarks for the feature selection problem in machine learning, we show that the combined effect of fewer elementary operations, low approximation error, and a non-singular gradient allows significantly better model quality and enables ML feature extraction that is two to three orders of magnitude faster and computationally cheaper when incorporating FEA into AI tools.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning with Local Search MCMC Layers</title>
<link>https://arxiv.org/abs/2505.14240</link>
<guid>https://arxiv.org/abs/2505.14240</guid>
<content:encoded><![CDATA[
arXiv:2505.14240v1 Announce Type: new 
Abstract: Integrating combinatorial optimization layers into neural networks has recently attracted significant research interest. However, many existing approaches lack theoretical guarantees or fail to perform adequately when relying on inexact solvers. This is a critical limitation, as many operations research problems are NP-hard, often necessitating the use of neighborhood-based local search heuristics. These heuristics iteratively generate and evaluate candidate solutions based on an acceptance rule. In this paper, we introduce a theoretically-principled approach for learning with such inexact combinatorial solvers. Inspired by the connection between simulated annealing and Metropolis-Hastings, we propose to transform problem-specific neighborhood systems used in local search heuristics into proposal distributions, implementing MCMC on the combinatorial space of feasible solutions. This allows us to construct differentiable combinatorial layers and associated loss functions. Replacing an exact solver by a local search strongly reduces the computational burden of learning on many applications. We demonstrate our approach on a large-scale dynamic vehicle routing problem with time windows.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input</title>
<link>https://arxiv.org/abs/2505.14251</link>
<guid>https://arxiv.org/abs/2505.14251</guid>
<content:encoded><![CDATA[
arXiv:2505.14251v1 Announce Type: new 
Abstract: We study the problem of differentially private second moment estimation and present a new algorithm that achieve strong privacy-utility trade-offs even for worst-case inputs under subsamplability assumptions on the data. We call an input $(m,\alpha,\beta)$-subsamplable if a random subsample of size $m$ (or larger) preserves w.p $\geq 1-\beta$ the spectral structure of the original second moment matrix up to a multiplicative factor of $1\pm \alpha$. Building upon subsamplability, we give a recursive algorithmic framework similar to Kamath et al 2019, that abides zero-Concentrated Differential Privacy (zCDP) while preserving w.h.p. the accuracy of the second moment estimation upto an arbitrary factor of $(1\pm\gamma)$. We then show how to apply our algorithm to approximate the second moment matrix of a distribution $\mathcal{D}$, even when a noticeable fraction of the input are outliers.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.14252</link>
<guid>https://arxiv.org/abs/2505.14252</guid>
<content:encoded><![CDATA[
arXiv:2505.14252v1 Announce Type: new 
Abstract: In this work, we explore the integration of Sequence Encoding for Online Parameter Identification with Physics-Informed Neural Networks to create a model that, once trained, can be utilized for real time applications with variable parameters, boundary conditions, and initial conditions. Recently, the combination of PINNs with Sparse Regression has emerged as a method for performing dynamical system identification through supervised learning and sparse regression optimization, while also solving the dynamics using PINNs. However, this approach can be limited by variations in parameters or boundary and initial conditions, requiring retraining of the model whenever changes occur. In this work, we introduce an architecture that employs Deep Sets or Sequence Encoders to encode dynamic parameters, boundary conditions, and initial conditions, using these encoded features as inputs for the PINN, enabling the model to adapt to changes in parameters, BCs, and ICs. We apply this approach to three different problems. First, we analyze the Rossler ODE system, demonstrating the robustness of the model with respect to noise and its ability to generalize. Next, we explore the model's capability in a 2D Navier-Stokes PDE problem involving flow past a cylinder with a parametric sinusoidal inlet velocity function, showing that the model can encode pressure data from a few points to identify the inlet velocity profile and utilize physics to compute velocity and pressure throughout the domain. Finally, we address a 1D heat monitoring problem using real data from the heating of glass fiber and thermoplastic composite plates.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum</title>
<link>https://arxiv.org/abs/2505.14264</link>
<guid>https://arxiv.org/abs/2505.14264</guid>
<content:encoded><![CDATA[
arXiv:2505.14264v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has emerged as an effective approach for enhancing the reasoning capabilities of large language models (LLMs), especially in scenarios where supervised fine-tuning (SFT) falls short due to limited chain-of-thought (CoT) data. Among RL-based post-training methods, group relative advantage estimation, as exemplified by Group Relative Policy Optimization (GRPO), has attracted considerable attention for eliminating the dependency on the value model, thereby simplifying training compared to traditional approaches like Proximal Policy Optimization (PPO). However, we observe that exsiting group relative advantage estimation method still suffers from training inefficiencies, particularly when the estimated advantage approaches zero. To address this limitation, we propose Advantage-Augmented Policy Optimization (AAPO), a novel RL algorithm that optimizes the cross-entropy (CE) loss using advantages enhanced through a momentum-based estimation scheme. This approach effectively mitigates the inefficiencies associated with group relative advantage estimation. Experimental results on multiple mathematical reasoning benchmarks demonstrate the superior performance of AAPO.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning</title>
<link>https://arxiv.org/abs/2505.14273</link>
<guid>https://arxiv.org/abs/2505.14273</guid>
<content:encoded><![CDATA[
arXiv:2505.14273v1 Announce Type: new 
Abstract: Function approximation is a critical task in various fields. However, existing neural network approaches struggle with locally complex or discontinuous functions due to their reliance on a single global model covering the entire problem space. We propose X-KAN, a novel method that optimizes multiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary rule-based machine learning framework called XCSF. X-KAN combines KAN's high expressiveness with XCSF's adaptive partitioning capability by implementing local KAN models as rule consequents and defining local regions via rule antecedents. Our experimental results on artificial test functions and real-world datasets demonstrate that X-KAN significantly outperforms conventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms of approximation accuracy. Notably, X-KAN effectively handles functions with locally complex or discontinuous structures that are challenging for conventional KAN, using a compact set of rules (average 7.2 $\pm$ 2.3 rules). These results validate the effectiveness of using KAN as a local model in XCSF, which evaluates the rule fitness based on both accuracy and generality. Our X-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Law for Quantization-Aware Training</title>
<link>https://arxiv.org/abs/2505.14302</link>
<guid>https://arxiv.org/abs/2505.14302</guid>
<content:encoded><![CDATA[
arXiv:2505.14302v1 Announce Type: new 
Abstract: Large language models (LLMs) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training (QAT) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of QAT, especially at 4-bit precision (W4A4), is not well understood. Existing QAT scaling laws often ignore key factors such as the number of training tokens and quantization granularity, which limits their applicability. This paper proposes a unified scaling law for QAT that models quantization error as a function of model size, training data volume, and quantization group size. Through 268 QAT experiments, we show that quantization error decreases as model size increases, but rises with more training tokens and coarser quantization granularity. To identify the sources of W4A4 quantization error, we decompose it into weight and activation components. Both components follow the overall trend of W4A4 quantization error, but with different sensitivities. Specifically, weight quantization error increases more rapidly with more training tokens. Further analysis shows that the activation quantization error in the FC2 layer, caused by outliers, is the primary bottleneck of W4A4 QAT quantization error. By applying mixed-precision quantization to address this bottleneck, we demonstrate that weight and activation quantization errors can converge to similar levels. Additionally, with more training data, weight quantization error eventually exceeds activation quantization error, suggesting that reducing weight quantization error is also important in such scenarios. These findings offer key insights for improving QAT research and development.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains</title>
<link>https://arxiv.org/abs/2505.14312</link>
<guid>https://arxiv.org/abs/2505.14312</guid>
<content:encoded><![CDATA[
arXiv:2505.14312v1 Announce Type: new 
Abstract: Despite the widespread use of tabular data in real-world applications, most benchmarks rely on average-case metrics, which fail to reveal how model behavior varies across diverse data regimes. To address this, we propose MultiTab, a benchmark suite and evaluation framework for multi-dimensional, data-aware analysis of tabular learning algorithms. Rather than comparing models only in aggregate, MultiTab categorizes 196 publicly available datasets along key data characteristics, including sample size, label imbalance, and feature interaction, and evaluates 13 representative models spanning a range of inductive biases. Our analysis shows that model performance is highly sensitive to such regimes: for example, models using sample-level similarity excel on datasets with large sample sizes or high inter-feature correlation, while models encoding inter-feature dependencies perform best with weakly correlated features. These findings reveal that inductive biases do not always behave as intended, and that regime-aware evaluation is essential for understanding and improving model behavior. MultiTab enables more principled model design and offers practical guidance for selecting models tailored to specific data characteristics. All datasets, code, and optimization logs are publicly available at https://huggingface.co/datasets/LGAI-DILab/Multitab.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Neural Network Expressivity: Subdividing the Simplex</title>
<link>https://arxiv.org/abs/2505.14338</link>
<guid>https://arxiv.org/abs/2505.14338</guid>
<content:encoded><![CDATA[
arXiv:2505.14338v1 Announce Type: new 
Abstract: This work studies the expressivity of ReLU neural networks with a focus on their depth. A sequence of previous works showed that $\lceil \log_2(n+1) \rceil$ hidden layers are sufficient to compute all continuous piecewise linear (CPWL) functions on $\mathbb{R}^n$. Hertrich, Basu, Di Summa, and Skutella (NeurIPS'21) conjectured that this result is optimal in the sense that there are CPWL functions on $\mathbb{R}^n$, like the maximum function, that require this depth. We disprove the conjecture and show that $\lceil\log_3(n-1)\rceil+1$ hidden layers are sufficient to compute all CPWL functions on $\mathbb{R}^n$.
  A key step in the proof is that ReLU neural networks with two hidden layers can exactly represent the maximum function of five inputs. More generally, we show that $\lceil\log_3(n-2)\rceil+1$ hidden layers are sufficient to compute the maximum of $n\geq 4$ numbers. Our constructions almost match the $\lceil\log_3(n)\rceil$ lower bound of Averkov, Hojny, and Merkert (ICLR'25) in the special case of ReLU networks with weights that are decimal fractions. The constructions have a geometric interpretation via polyhedral subdivisions of the simplex into ``easier'' polytopes.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights</title>
<link>https://arxiv.org/abs/2505.14345</link>
<guid>https://arxiv.org/abs/2505.14345</guid>
<content:encoded><![CDATA[
arXiv:2505.14345v1 Announce Type: new 
Abstract: Recent advancements in semi-supervised deep learning have introduced effective strategies for leveraging both labeled and unlabeled data to improve classification performance. This work proposes a semi-supervised framework that utilizes a distance-based weighting mechanism to prioritize critical training samples based on their proximity to test data. By focusing on the most informative examples, the method enhances model generalization and robustness, particularly in challenging scenarios with noisy or imbalanced datasets. Building on techniques such as uncertainty consistency and graph-based representations, the approach addresses key challenges of limited labeled data while maintaining scalability. Experiments on twelve benchmark datasets demonstrate significant improvements across key metrics, including accuracy, precision, and recall, consistently outperforming existing methods. This framework provides a robust and practical solution for semi-supervised learning, with potential applications in domains such as healthcare and security where data limitations pose significant challenges.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards eliciting latent knowledge from LLMs with mechanistic interpretability</title>
<link>https://arxiv.org/abs/2505.14352</link>
<guid>https://arxiv.org/abs/2505.14352</guid>
<content:encoded><![CDATA[
arXiv:2505.14352v1 Announce Type: new 
Abstract: As language models become more powerful and sophisticated, it is crucial that they remain trustworthy and reliable. There is concerning preliminary evidence that models may attempt to deceive or keep secrets from their operators. To explore the ability of current techniques to elicit such hidden knowledge, we train a Taboo model: a language model that describes a specific secret word without explicitly stating it. Importantly, the secret word is not presented to the model in its training data or prompt. We then investigate methods to uncover this secret. First, we evaluate non-interpretability (black-box) approaches. Subsequently, we develop largely automated strategies based on mechanistic interpretability techniques, including logit lens and sparse autoencoders. Evaluation shows that both approaches are effective in eliciting the secret word in our proof-of-concept setting. Our findings highlight the promise of these approaches for eliciting hidden knowledge and suggest several promising avenues for future work, including testing and refining these methods on more complex model organisms. This work aims to be a step towards addressing the crucial problem of eliciting secret knowledge from language models, thereby contributing to their safe and reliable deployment.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layer-wise Quantization for Quantized Optimistic Dual Averaging</title>
<link>https://arxiv.org/abs/2505.14371</link>
<guid>https://arxiv.org/abs/2505.14371</guid>
<content:encoded><![CDATA[
arXiv:2505.14371v1 Announce Type: new 
Abstract: Modern deep neural networks exhibit heterogeneity across numerous layers of various types such as residuals, multi-head attention, etc., due to varying structures (dimensions, activation functions, etc.), distinct representation characteristics, which impact predictions. We develop a general layer-wise quantization framework with tight variance and code-length bounds, adapting to the heterogeneities over the course of training. We then apply a new layer-wise quantization technique within distributed variational inequalities (VIs), proposing a novel Quantized Optimistic Dual Averaging (QODA) algorithm with adaptive learning rates, which achieves competitive convergence rates for monotone VIs. We empirically show that QODA achieves up to a $150\%$ speedup over the baselines in end-to-end training time for training Wasserstein GAN on $12+$ GPUs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes</title>
<link>https://arxiv.org/abs/2505.14388</link>
<guid>https://arxiv.org/abs/2505.14388</guid>
<content:encoded><![CDATA[
arXiv:2505.14388v1 Announce Type: new 
Abstract: Algorithmic tools are increasingly used in hiring to improve fairness and diversity, often by enforcing constraints such as gender-balanced candidate shortlists. However, we show theoretically and empirically that enforcing equal representation at the shortlist stage does not necessarily translate into more diverse final hires, even when there is no gender bias in the hiring stage. We identify a crucial factor influencing this outcome: the correlation between the algorithm's screening criteria and the human hiring manager's evaluation criteria -- higher correlation leads to lower diversity in final hires. Using a large-scale empirical analysis of nearly 800,000 job applications across multiple technology firms, we find that enforcing equal shortlists yields limited improvements in hire diversity when the algorithmic screening closely mirrors the hiring manager's preferences. We propose a complementary algorithmic approach designed explicitly to diversify shortlists by selecting candidates likely to be overlooked by managers, yet still competitive according to their evaluation criteria. Empirical simulations show that this approach significantly enhances gender diversity in final hires without substantially compromising hire quality. These findings highlight the importance of algorithmic design choices in achieving organizational diversity goals and provide actionable guidance for practitioners implementing fairness-oriented hiring algorithms.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach</title>
<link>https://arxiv.org/abs/2505.14407</link>
<guid>https://arxiv.org/abs/2505.14407</guid>
<content:encoded><![CDATA[
arXiv:2505.14407v1 Announce Type: new 
Abstract: Autonomous systems that rely on Machine Learning (ML) utilize online fault tolerance mechanisms, such as runtime monitors, to detect ML prediction errors and maintain safety during operation. However, the lack of human-interpretable explanations for these errors can hinder the creation of strong assurances about the system's safety and reliability. This paper introduces a novel fuzzy-based monitor tailored for ML perception components. It provides human-interpretable explanations about how different operating conditions affect the reliability of perception components and also functions as a runtime safety monitor. We evaluated our proposed monitor using naturalistic driving datasets as part of an automated driving case study. The interpretability of the monitor was evaluated and we identified a set of operating conditions in which the perception component performs reliably. Additionally, we created an assurance case that links unit-level evidence of \textit{correct} ML operation to system-level \textit{safety}. The benchmarking demonstrated that our monitor achieved a better increase in safety (i.e., absence of hazardous situations) while maintaining availability (i.e., ability to perform the mission) compared to state-of-the-art runtime ML monitors in the evaluated dataset.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Byte Pair Encoding for Efficient Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.14411</link>
<guid>https://arxiv.org/abs/2505.14411</guid>
<content:encoded><![CDATA[
arXiv:2505.14411v1 Announce Type: new 
Abstract: Existing time series tokenization methods predominantly encode a constant number of samples into individual tokens. This inflexible approach can generate excessive tokens for even simple patterns like extended constant values, resulting in substantial computational overhead. Inspired by the success of byte pair encoding, we propose the first pattern-centric tokenization scheme for time series analysis. Based on a discrete vocabulary of frequent motifs, our method merges samples with underlying patterns into tokens, compressing time series adaptively. Exploiting our finite set of motifs and the continuous properties of time series, we further introduce conditional decoding as a lightweight yet powerful post-hoc optimization method, which requires no gradient computation and adds no computational overhead. On recent time series foundation models, our motif-based tokenization improves forecasting performance by 36% and boosts efficiency by 1990% on average. Conditional decoding further reduces MSE by up to 44%. In an extensive analysis, we demonstrate the adaptiveness of our tokenization to diverse temporal patterns, its generalization to unseen data, and its meaningful token representations capturing distinct time series properties, including statistical moments and trends.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Table Foundation Models: on knowledge pre-training for tabular learning</title>
<link>https://arxiv.org/abs/2505.14415</link>
<guid>https://arxiv.org/abs/2505.14415</guid>
<content:encoded><![CDATA[
arXiv:2505.14415v1 Announce Type: new 
Abstract: Table foundation models bring high hopes to data science: pre-trained on tabular data to embark knowledge or priors, they should facilitate downstream tasks on tables. One specific challenge is that of data semantics: numerical entries take their meaning from context, e.g., column name. Pre-trained neural networks that jointly model column names and table entries have recently boosted prediction accuracy. While these models outline the promises of world knowledge to interpret table values, they lack the convenience of popular foundation models in text or vision. Indeed, they must be fine-tuned to bring benefits, come with sizeable computation costs, and cannot easily be reused or combined with other architectures. Here we introduce TARTE, a foundation model that transforms tables to knowledge-enhanced vector representations using the string to capture semantics. Pre-trained on large relational data, TARTE yields representations that facilitate subsequent learning with little additional cost. These representations can be fine-tuned or combined with other learners, giving models that push the state-of-the-art prediction performance and improve the prediction/computation performance trade-off. Specialized to a task or a domain, TARTE gives domain-specific representations that facilitate further learning. Our study demonstrates an effective approach to knowledge pre-training for tabular learning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Neural Networks with Reasons</title>
<link>https://arxiv.org/abs/2505.14424</link>
<guid>https://arxiv.org/abs/2505.14424</guid>
<content:encoded><![CDATA[
arXiv:2505.14424v1 Announce Type: new 
Abstract: We propose a new interpretability method for neural networks, which is based on a novel mathematico-philosophical theory of reasons. Our method computes a vector for each neuron, called its reasons vector. We then can compute how strongly this reasons vector speaks for various propositions, e.g., the proposition that the input image depicts digit 2 or that the input prompt has a negative sentiment. This yields an interpretation of neurons, and groups thereof, that combines a logical and a Bayesian perspective, and accounts for polysemanticity (i.e., that a single neuron can figure in multiple concepts). We show, both theoretically and empirically, that this method is: (1) grounded in a philosophically established notion of explanation, (2) uniform, i.e., applies to the common neural network architectures and modalities, (3) scalable, since computing reason vectors only involves forward-passes in the neural network, (4) faithful, i.e., intervening on a neuron based on its reason vector leads to expected changes in model output, (5) correct in that the model's reasons structure matches that of the data source, (6) trainable, i.e., neural networks can be trained to improve their reason strengths, (7) useful, i.e., it delivers on the needs for interpretability by increasing, e.g., robustness and fairness.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications</title>
<link>https://arxiv.org/abs/2505.14428</link>
<guid>https://arxiv.org/abs/2505.14428</guid>
<content:encoded><![CDATA[
arXiv:2505.14428v1 Announce Type: new 
Abstract: The objective of this proposal is to bridge the gap between Deep Learning (DL) and System Dynamics (SD) by developing an interpretable neural system dynamics framework. While DL excels at learning complex models and making accurate predictions, it lacks interpretability and causal reliability. Traditional SD approaches, on the other hand, provide transparency and causal insights but are limited in scalability and require extensive domain knowledge. To overcome these limitations, this project introduces a Neural System Dynamics pipeline, integrating Concept-Based Interpretability, Mechanistic Interpretability, and Causal Machine Learning. This framework combines the predictive power of DL with the interpretability of traditional SD models, resulting in both causal reliability and scalability. The efficacy of the proposed pipeline will be validated through real-world applications of the EU-funded AutoMoTIF project, which is focused on autonomous multimodal transportation systems. The long-term goal is to collect actionable insights that support the integration of explainability and safety in autonomous systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation</title>
<link>https://arxiv.org/abs/2505.14451</link>
<guid>https://arxiv.org/abs/2505.14451</guid>
<content:encoded><![CDATA[
arXiv:2505.14451v1 Announce Type: new 
Abstract: Missing values in high-dimensional, mixed-type datasets pose significant challenges for data imputation, particularly under Missing Not At Random (MNAR) mechanisms. Existing methods struggle to integrate local and global data characteristics, limiting performance in MNAR and high-dimensional settings. We propose an innovative framework, RefiDiff, combining local machine learning predictions with a novel Mamba-based denoising network capturing interrelationships among distant features and samples. Our approach leverages pre-refinement for initial warm-up imputations and post-refinement to polish results, enhancing stability and accuracy. By encoding mixed-type data into unified tokens, RefiDiff enables robust imputation without architectural or hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods across missing-value settings, excelling in MNAR with a 4x faster training time than SOTA DDPM-based approaches. Extensive evaluations on nine real-world datasets demonstrate its robustness, scalability, and effectiveness in handling complex missingness patterns.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2505.14459</link>
<guid>https://arxiv.org/abs/2505.14459</guid>
<content:encoded><![CDATA[
arXiv:2505.14459v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has been increasingly applied to network control problems, such as load balancing. However, existing RL approaches often suffer from lack of interpretability and difficulty in extracting controller equations. In this paper, we propose the use of Kolmogorov-Arnold Networks (KAN) for interpretable RL in network control. We employ a PPO agent with a 1-layer actor KAN model and an MLP Critic network to learn load balancing policies that maximise throughput utility, minimize loss as well as delay. Our approach allows us to extract controller equations from the learned neural networks, providing insights into the decision-making process. We evaluate our approach using different reward functions demonstrating its effectiveness in improving network performance while providing interpretable policies.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium</title>
<link>https://arxiv.org/abs/2505.14463</link>
<guid>https://arxiv.org/abs/2505.14463</guid>
<content:encoded><![CDATA[
arXiv:2505.14463v1 Announce Type: new 
Abstract: Adversarial attacks to graph analytics are gaining increased attention. To date, two lines of countermeasures have been proposed to resist various graph adversarial attacks from the perspectives of either graph per se or graph neural networks. Nevertheless, a fundamental question lies in whether there exists an intrinsic adversarial resilience state within a graph regime and how to find out such a critical state if exists. This paper contributes to tackle the above research questions from three unique perspectives: i) we regard the process of adversarial learning on graph as a complex multi-object dynamic system, and model the behavior of adversarial attack; ii) we propose a generalized theoretical framework to show the existence of critical adversarial resilience state; and iii) we develop a condensed one-dimensional function to capture the dynamic variation of graph regime under perturbations, and pinpoint the critical state through solving the equilibrium point of dynamic system. Multi-facet experiments are conducted to show our proposed approach can significantly outperform the state-of-the-art defense methods under five commonly-used real-world datasets and three representative attacks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs</title>
<link>https://arxiv.org/abs/2505.14468</link>
<guid>https://arxiv.org/abs/2505.14468</guid>
<content:encoded><![CDATA[
arXiv:2505.14468v1 Announce Type: new 
Abstract: Serverless computing has grown rapidly for serving Large Language Model (LLM) inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid scaling. However, our analysis reveals that current serverless can effectively serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to three key limitations: 1) massive parameter redundancy among functions where 99% of weights are unnecessarily duplicated, 2) costly artifact loading latency beyond LLM loading, and 3) magnified resource contention when serving multiple LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased Time-To-First-Token (TTFT), and high monetary costs.
  We propose ServerlessLoRA, a novel serverless inference system designed for faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM sharing across isolated LoRA functions to reduce redundancy. We design a pre-loading method that pre-loads comprehensive LoRA artifacts to minimize cold-start latency. Furthermore, ServerlessLoRA employs contention aware batching and offloading to mitigate GPU resource conflicts during bursty workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to state-of-the-art LLM inference solutions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalised Insulin Adjustment with Reinforcement Learning: An In-Silico Validation for People with Diabetes on Intensive Insulin Treatment</title>
<link>https://arxiv.org/abs/2505.14477</link>
<guid>https://arxiv.org/abs/2505.14477</guid>
<content:encoded><![CDATA[
arXiv:2505.14477v1 Announce Type: new 
Abstract: Despite recent advances in insulin preparations and technology, adjusting insulin remains an ongoing challenge for the majority of people with type 1 diabetes (T1D) and longstanding type 2 diabetes (T2D). In this study, we propose the Adaptive Basal-Bolus Advisor (ABBA), a personalised insulin treatment recommendation approach based on reinforcement learning for individuals with T1D and T2D, performing self-monitoring blood glucose measurements and multiple daily insulin injection therapy. We developed and evaluated the ability of ABBA to achieve better time-in-range (TIR) for individuals with T1D and T2D, compared to a standard basal-bolus advisor (BBA). The in-silico test was performed using an FDA-accepted population, including 101 simulated adults with T1D and 101 with T2D. An in-silico evaluation shows that ABBA significantly improved TIR and significantly reduced both times below- and above-range, compared to BBA. ABBA's performance continued to improve over two months, whereas BBA exhibited only modest changes. This personalised method for adjusting insulin has the potential to further optimise glycaemic control and support people with T1D and T2D in their daily self-management. Our results warrant ABBA to be trialed for the first time in humans.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Integrate Diffusion ODEs by Averaging the Derivatives</title>
<link>https://arxiv.org/abs/2505.14502</link>
<guid>https://arxiv.org/abs/2505.14502</guid>
<content:encoded><![CDATA[
arXiv:2505.14502v1 Announce Type: new 
Abstract: To accelerate diffusion model inference, numerical solvers perform poorly at extremely small steps, while distillation techniques often introduce complexity and instability. This work presents an intermediate strategy, balancing performance and cost, by learning ODE integration using loss functions derived from the derivative-integral relationship, inspired by Monte Carlo integration and Picard iteration. From a geometric perspective, the losses operate by gradually extending the tangent to the secant, thus are named as secant losses. The secant losses can rapidly convert (via fine-tuning or distillation) a pretrained diffusion model into its secant version. In our experiments, the secant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the secant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID of $1.96$ on ImageNet-$256\times256$. Code will be available.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just One Layer Norm Guarantees Stable Extrapolation</title>
<link>https://arxiv.org/abs/2505.14512</link>
<guid>https://arxiv.org/abs/2505.14512</guid>
<content:encoded><![CDATA[
arXiv:2505.14512v1 Announce Type: new 
Abstract: In spite of their prevalence, the behaviour of Neural Networks when extrapolating far from the training distribution remains poorly understood, with existing results limited to specific cases. In this work, we prove general results -- the first of their kind -- by applying Neural Tangent Kernel (NTK) theory to analyse infinitely-wide neural networks trained until convergence and prove that the inclusion of just one Layer Norm (LN) fundamentally alters the induced NTK, transforming it into a bounded-variance kernel. As a result, the output of an infinitely wide network with at least one LN remains bounded, even on inputs far from the training data. In contrast, we show that a broad class of networks without LN can produce pathologically large outputs for certain inputs. We support these theoretical findings with empirical experiments on finite-width networks, demonstrating that while standard NNs often exhibit uncontrolled growth outside the training domain, a single LN layer effectively mitigates this instability. Finally, we explore real-world implications of this extrapolatory stability, including applications to predicting residue sizes in proteins larger than those seen during training and estimating age from facial images of underrepresented ethnicities absent from the training set.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Flow Transformer</title>
<link>https://arxiv.org/abs/2505.14513</link>
<guid>https://arxiv.org/abs/2505.14513</guid>
<content:encoded><![CDATA[
arXiv:2505.14513v1 Announce Type: new 
Abstract: Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in \textit{preserving coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities</title>
<link>https://arxiv.org/abs/2505.14522</link>
<guid>https://arxiv.org/abs/2505.14522</guid>
<content:encoded><![CDATA[
arXiv:2505.14522v1 Announce Type: new 
Abstract: Wind hazards such as tornadoes and straight-line winds frequently affect vulnerable communities in the Great Plains of the United States, where limited infrastructure and sparse data coverage hinder effective emergency response. Existing forecasting systems focus primarily on meteorological elements and often fail to capture community-specific vulnerabilities, limiting their utility for localized risk assessment and resilience planning. To address this gap, we propose an interpretable dual-stream learning framework that integrates structured numerical weather data with unstructured textual event narratives. Our architecture combines a Random Forest and RoBERTa-based transformer through a late fusion mechanism, enabling robust and context-aware wind hazard prediction. The system is tailored for underserved tribal communities and supports block-level risk assessment. Experimental results show significant performance gains over traditional baselines. Furthermore, gradient-based sensitivity and ablation studies provide insight into the model's decision-making process, enhancing transparency and operational trust. The findings demonstrate both predictive effectiveness and practical value in supporting emergency preparedness and advancing community resilience.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SifterNet: A Generalized and Model-Agnostic Trigger Purification Approach</title>
<link>https://arxiv.org/abs/2505.14531</link>
<guid>https://arxiv.org/abs/2505.14531</guid>
<content:encoded><![CDATA[
arXiv:2505.14531v1 Announce Type: new 
Abstract: Aiming at resisting backdoor attacks in convolution neural networks and vision Transformer-based large model, this paper proposes a generalized and model-agnostic trigger-purification approach resorting to the classic Ising model. To date, existing trigger detection/removal studies usually require to know the detailed knowledge of target model in advance, access to a large number of clean samples or even model-retraining authorization, which brings the huge inconvenience for practical applications, especially inaccessible to target model. An ideal countermeasure ought to eliminate the implanted trigger without regarding whatever the target models are. To this end, a lightweight and black-box defense approach SifterNet is proposed through leveraging the memorization-association functionality of Hopfield network, by which the triggers of input samples can be effectively purified in a proper manner. The main novelty of our proposed approach lies in the introduction of ideology of Ising model. Extensive experiments also validate the effectiveness of our approach in terms of proper trigger purification and high accuracy achievement, and compared to the state-of-the-art baselines under several commonly-used datasets, our SiferNet has a significant superior performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Deep Reinforcement Learning with Spiking Transformers</title>
<link>https://arxiv.org/abs/2505.14533</link>
<guid>https://arxiv.org/abs/2505.14533</guid>
<content:encoded><![CDATA[
arXiv:2505.14533v1 Announce Type: new 
Abstract: Agent-based Transformers have been widely adopted in recent reinforcement learning advances due to their demonstrated ability to solve complex tasks. However, the high computational complexity of Transformers often results in significant energy consumption, limiting their deployment in real-world autonomous systems. Spiking neural networks (SNNs), with their biologically inspired structure, offer an energy-efficient alternative for machine learning. In this paper, a novel Spike-Transformer Reinforcement Learning (STRL) algorithm that combines the energy efficiency of SNNs with the powerful decision-making capabilities of reinforcement learning is developed. Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons and attention mechanisms capable of processing spatio-temporal patterns over multiple time steps is designed. The architecture is further enhanced with state, action, and reward encodings to create a Transformer-like structure optimized for reinforcement learning tasks. Comprehensive numerical experiments conducted on state-of-the-art benchmarks demonstrate that the proposed SNN Transformer achieves significantly improved policy performance compared to conventional agent-based Transformers. With both enhanced energy efficiency and policy optimality, this work highlights a promising direction for deploying bio-inspired, low-cost machine learning models in complex real-world decision-making scenarios.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spiking Neural Networks with Temporal Attention-Guided Adaptive Fusion for imbalanced Multi-modal Learning</title>
<link>https://arxiv.org/abs/2505.14535</link>
<guid>https://arxiv.org/abs/2505.14535</guid>
<content:encoded><![CDATA[
arXiv:2505.14535v1 Announce Type: new 
Abstract: Multimodal spiking neural networks (SNNs) hold significant potential for energy-efficient sensory processing but face critical challenges in modality imbalance and temporal misalignment. Current approaches suffer from uncoordinated convergence speeds across modalities and static fusion mechanisms that ignore time-varying cross-modal interactions. We propose the temporal attention-guided adaptive fusion framework for multimodal SNNs with two synergistic innovations: 1) The Temporal Attention-guided Adaptive Fusion (TAAF) module that dynamically assigns importance scores to fused spiking features at each timestep, enabling hierarchical integration of temporally heterogeneous spike-based features; 2) The temporal adaptive balanced fusion loss that modulates learning rates per modality based on the above attention scores, preventing dominant modalities from monopolizing optimization. The proposed framework implements adaptive fusion, especially in the temporal dimension, and alleviates the modality imbalance during multimodal learning, mimicking cortical multisensory integration principles. Evaluations on CREMA-D, AVE, and EAD datasets demonstrate state-of-the-art performance (77.55\%, 70.65\% and 97.5\%accuracy, respectively) with energy efficiency. The system resolves temporal misalignment through learnable time-warping operations and faster modality convergence coordination than baseline SNNs. This work establishes a new paradigm for temporally coherent multimodal learning in neuromorphic systems, bridging the gap between biological sensory processing and efficient machine intelligence.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions</title>
<link>https://arxiv.org/abs/2505.14543</link>
<guid>https://arxiv.org/abs/2505.14543</guid>
<content:encoded><![CDATA[
arXiv:2505.14543v1 Announce Type: new 
Abstract: Traditional time series models are task-specific and often depend on dataset-specific training and extensive feature engineering. While Transformer-based architectures have improved scalability, foundation models, commonplace in text, vision, and audio, remain under-explored for time series and are largely restricted to forecasting. We introduce $\textbf{CHARM}$, a foundation embedding model for multivariate time series that learns shared, transferable, and domain-aware representations. To address the unique difficulties of time series foundation learning, $\textbf{CHARM}$ incorporates architectural innovations that integrate channel-level textual descriptions while remaining invariant to channel order. The model is trained using a Joint Embedding Predictive Architecture (JEPA), with novel augmentation schemes and a loss function designed to improve interpretability and training stability. Our $7$M-parameter model achieves state-of-the-art performance across diverse downstream tasks, setting a new benchmark for time series representation learning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting</title>
<link>https://arxiv.org/abs/2505.14555</link>
<guid>https://arxiv.org/abs/2505.14555</guid>
<content:encoded><![CDATA[
arXiv:2505.14555v1 Announce Type: new 
Abstract: Weather forecasting is essential but remains computationally intensive and physically incomplete in traditional numerical weather prediction (NWP) methods. Deep learning (DL) models offer efficiency and accuracy but often ignore physical laws, limiting interpretability and generalization. We propose PhyDL-NWP, a physics-guided deep learning framework that integrates physical equations with latent force parameterization into data-driven models. It predicts weather variables from arbitrary spatiotemporal coordinates, computes physical terms via automatic differentiation, and uses a physics-informed loss to align predictions with governing dynamics. PhyDL-NWP enables resolution-free downscaling by modeling weather as a continuous function and fine-tunes pre-trained models with minimal overhead, achieving up to 170x faster inference with only 55K parameters. Experiments show that PhyDL-NWP improves both forecasting performance and physical consistency.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bellman operator convergence enhancements in reinforcement learning algorithms</title>
<link>https://arxiv.org/abs/2505.14564</link>
<guid>https://arxiv.org/abs/2505.14564</guid>
<content:encoded><![CDATA[
arXiv:2505.14564v1 Announce Type: new 
Abstract: This paper reviews the topological groundwork for the study of reinforcement learning (RL) by focusing on the structure of state, action, and policy spaces. We begin by recalling key mathematical concepts such as complete metric spaces, which form the foundation for expressing RL problems. By leveraging the Banach contraction principle, we illustrate how the Banach fixed-point theorem explains the convergence of RL algorithms and how Bellman operators, expressed as operators on Banach spaces, ensure this convergence. The work serves as a bridge between theoretical mathematics and practical algorithm design, offering new approaches to enhance the efficiency of RL. In particular, we investigate alternative formulations of Bellman operators and demonstrate their impact on improving convergence rates and performance in standard RL environments such as MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper mathematical understanding of RL can lead to more effective algorithms for decision-making problems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KIPPO: Koopman-Inspired Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2505.14566</link>
<guid>https://arxiv.org/abs/2505.14566</guid>
<content:encoded><![CDATA[
arXiv:2505.14566v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has made significant strides in various domains, and policy gradient methods like Proximal Policy Optimization (PPO) have gained popularity due to their balance in performance, training stability, and computational efficiency. These methods directly optimize policies through gradient-based updates. However, developing effective control policies for environments with complex and non-linear dynamics remains a challenge. High variance in gradient estimates and non-convex optimization landscapes often lead to unstable learning trajectories. Koopman Operator Theory has emerged as a powerful framework for studying non-linear systems through an infinite-dimensional linear operator that acts on a higher-dimensional space of measurement functions. In contrast with their non-linear counterparts, linear systems are simpler, more predictable, and easier to analyze. In this paper, we present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an approximately linear latent-space representation of the underlying system's dynamics while retaining essential features for effective policy learning. This is achieved through a Koopman-approximation auxiliary network that can be added to the baseline policy optimization algorithms without altering the architecture of the core policy or value function. Extensive experimental results demonstrate consistent improvements over the PPO baseline with 6-60% increased performance while reducing variability by up to 91% when evaluated on various continuous control tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded Intrusion Detection on the Edge</title>
<link>https://arxiv.org/abs/2505.14592</link>
<guid>https://arxiv.org/abs/2505.14592</guid>
<content:encoded><![CDATA[
arXiv:2505.14592v1 Announce Type: new 
Abstract: Artificial neural network pruning is a method in which artificial neural network sizes can be reduced while attempting to preserve the predicting capabilities of the network. This is done to make the model smaller or faster during inference time. In this work we analyze the ability of a selection of artificial neural network pruning methods to generalize to a new cybersecurity dataset utilizing a simpler network type than was designed for. We analyze each method using a variety of pruning degrees to best understand how each algorithm responds to the new environment. This has allowed us to determine the most well fit pruning method of those we searched for the task. Unexpectedly, we have found that many of them do not generalize to the problem well, leaving only a few algorithms working to an acceptable degree.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers</title>
<link>https://arxiv.org/abs/2505.14595</link>
<guid>https://arxiv.org/abs/2505.14595</guid>
<content:encoded><![CDATA[
arXiv:2505.14595v1 Announce Type: new 
Abstract: Reduced-order modeling (ROM) of time-dependent and parameterized differential equations aims to accelerate the simulation of complex high-dimensional systems by learning a compact latent manifold representation that captures the characteristics of the solution fields and their time-dependent dynamics. Although high-fidelity numerical solvers generate the training datasets, they have thus far been excluded from the training process, causing the learned latent dynamics to drift away from the discretized governing physics. This mismatch often limits generalization and forecasting capabilities. In this work, we propose Physics-informed ROM ($\Phi$-ROM) by incorporating differentiable PDE solvers into the training procedure. Specifically, the latent space dynamics and its dependence on PDE parameters are shaped directly by the governing physics encoded in the solver, ensuring a strong correspondence between the full and reduced systems. Our model outperforms state-of-the-art data-driven ROMs and other physics-informed strategies by accurately generalizing to new dynamics arising from unseen parameters, enabling long-term forecasting beyond the training horizon, maintaining continuity in both time and space, and reducing the data cost. Furthermore, $\Phi$-ROM learns to recover and forecast the solution fields even when trained or evaluated with sparse and irregular observations of the fields, providing a flexible framework for field reconstruction and data assimilation. We demonstrate the framework's robustness across different PDE solvers and highlight its broad applicability by providing an open-source JAX implementation readily extensible to other PDE systems and differentiable solvers.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSTS: A Benchmark for the Discovery of Correlation Structures in Time Series Clustering</title>
<link>https://arxiv.org/abs/2505.14596</link>
<guid>https://arxiv.org/abs/2505.14596</guid>
<content:encoded><![CDATA[
arXiv:2505.14596v1 Announce Type: new 
Abstract: Time series clustering promises to uncover hidden structural patterns in data with applications across healthcare, finance, industrial systems, and other critical domains. However, without validated ground truth information, researchers cannot objectively assess clustering quality or determine whether poor results stem from absent structures in the data, algorithmic limitations, or inappropriate validation methods, raising the question whether clustering is "more art than science" (Guyon et al., 2009). To address these challenges, we introduce CSTS (Correlation Structures in Time Series), a synthetic benchmark for evaluating the discovery of correlation structures in multivariate time series data. CSTS provides a clean benchmark that enables researchers to isolate and identify specific causes of clustering failures by differentiating between correlation structure deterioration and limitations of clustering algorithms and validation methods. Our contributions are: (1) a comprehensive benchmark for correlation structure discovery with distinct correlation structures, systematically varied data conditions, established performance thresholds, and recommended evaluation protocols; (2) empirical validation of correlation structure preservation showing moderate distortion from downsampling and minimal effects from distribution shifts and sparsification; and (3) an extensible data generation framework enabling structure-first clustering evaluation. A case study demonstrates CSTS's practical utility by identifying an algorithm's previously undocumented sensitivity to non-normal distributions, illustrating how the benchmark enables precise diagnosis of methodological limitations. CSTS advances rigorous evaluation standards for correlation-based time series clustering.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials</title>
<link>https://arxiv.org/abs/2505.14606</link>
<guid>https://arxiv.org/abs/2505.14606</guid>
<content:encoded><![CDATA[
arXiv:2505.14606v1 Announce Type: new 
Abstract: Recent advances in neural network interatomic potentials have emerged as a promising research direction. However, popular deep learning models often lack auxiliary constraints grounded in physical laws, which could accelerate training and improve fidelity through physics-based regularization. In this work, we introduce $\Phi$-Module, a universal plugin module that enforces Poisson's equation within the message-passing framework to learn electrostatic interactions in a self-supervised manner. Specifically, each atom-wise representation is encouraged to satisfy a discretized Poisson's equation, making it possible to acquire a potential $\boldsymbol{\phi}$ and a corresponding charge density $\boldsymbol{\rho}$ linked to the learnable Laplacian eigenbasis coefficients of a given molecular graph. We then derive an electrostatic energy term, crucial for improved total energy predictions. This approach integrates seamlessly into any existing neural potential with insignificant computational overhead. Experiments on the OE62 and MD22 benchmarks confirm that models combined with $\Phi$-Module achieve robust improvements over baseline counterparts. For OE62 error reduction ranges from 4.5\% to 17.8\%, and for MD22, baseline equipped with $\Phi$-Module achieves best results on 5 out of 14 cases. Our results underscore how embedding a first-principles constraint in neural interatomic potentials can significantly improve performance while remaining hyperparameter-friendly, memory-efficient and lightweight in training. Code will be available at \href{https://github.com/dunnolab/phi-module}{dunnolab/phi-module}.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMD-Newton Method for Multi-objective Optimization</title>
<link>https://arxiv.org/abs/2505.14610</link>
<guid>https://arxiv.org/abs/2505.14610</guid>
<content:encoded><![CDATA[
arXiv:2505.14610v1 Announce Type: new 
Abstract: Maximum mean discrepancy (MMD) has been widely employed to measure the distance between probability distributions. In this paper, we propose using MMD to solve continuous multi-objective optimization problems (MOPs). For solving MOPs, a common approach is to minimize the distance (e.g., Hausdorff) between a finite approximate set of the Pareto front and a reference set. Viewing these two sets as empirical measures, we propose using MMD to measure the distance between them. To minimize the MMD value, we provide the analytical expression of its gradient and Hessian matrix w.r.t. the search variables, and use them to devise a novel set-oriented, MMD-based Newton (MMDN) method. Also, we analyze the theoretical properties of MMD's gradient and Hessian, including the first-order stationary condition and the eigenspectrum of the Hessian, which are important for verifying the correctness of MMDN. To solve complicated problems, we propose hybridizing MMDN with multiobjective evolutionary algorithms (MOEAs), where we first execute an EA for several iterations to get close to the global Pareto front and then warm-start MMDN with the result of the MOEA to efficiently refine the approximation. We empirically test the hybrid algorithm on 11 widely used benchmark problems, and the results show the hybrid (MMDN + MOEA) can achieve a much better optimization accuracy than EA alone with the same computation budget.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Cells: Predict, Explain, Discover</title>
<link>https://arxiv.org/abs/2505.14613</link>
<guid>https://arxiv.org/abs/2505.14613</guid>
<content:encoded><![CDATA[
arXiv:2505.14613v1 Announce Type: new 
Abstract: Drug discovery is fundamentally a process of inferring the effects of treatments on patients, and would therefore benefit immensely from computational models that can reliably simulate patient responses, enabling researchers to generate and test large numbers of therapeutic hypotheses safely and economically before initiating costly clinical trials. Even a more specific model that predicts the functional response of cells to a wide range of perturbations would be tremendously valuable for discovering safe and effective treatments that successfully translate to the clinic. Creating such virtual cells has long been a goal of the computational research community that unfortunately remains unachieved given the daunting complexity and scale of cellular biology. Nevertheless, recent advances in AI, computing power, lab automation, and high-throughput cellular profiling provide new opportunities for reaching this goal. In this perspective, we present a vision for developing and evaluating virtual cells that builds on our experience at Recursion. We argue that in order to be a useful tool to discover novel biology, virtual cells must accurately predict the functional response of a cell to perturbations and explain how the predicted response is a consequence of modifications to key biomolecular interactions. We then introduce key principles for designing therapeutically-relevant virtual cells, describe a lab-in-the-loop approach for generating novel insights with them, and advocate for biologically-grounded benchmarks to guide virtual cell development. Finally, we make the case that our approach to virtual cells provides a useful framework for building other models at higher levels of organization, including virtual patients. We hope that these directions prove useful to the research community in developing virtual models optimized for positive impact on drug discovery outcomes.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs</title>
<link>https://arxiv.org/abs/2505.14620</link>
<guid>https://arxiv.org/abs/2505.14620</guid>
<content:encoded><![CDATA[
arXiv:2505.14620v1 Announce Type: new 
Abstract: Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and scalable method to fine-tune and customize large language models (LLMs) for application-specific needs. However, tasks that require complex reasoning or deep contextual understanding are often hindered by biases or interference from the base model when using typical decoding methods like greedy or beam search. These biases can lead to generic or task-agnostic responses from the base model instead of leveraging the LoRA-specific adaptations. In this paper, we introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed to maximize the use of task-specific knowledge in LoRA-adapted models, resulting in better downstream performance. CoLD uses contrastive decoding by scoring candidate tokens based on the divergence between the probability distributions of a LoRA-adapted expert model and the corresponding base model. This approach prioritizes tokens that better align with the LoRA's learned representations, enhancing performance for specialized tasks. While effective, a naive implementation of CoLD is computationally expensive because each decoding step requires evaluating multiple token candidates across both models. To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD achieves up to a 5.54% increase in task accuracy while reducing end-to-end latency by 28% compared to greedy decoding. This work provides practical and efficient decoding strategies for fine-tuned LLMs in resource-constrained environments and has broad implications for applied data science in both cloud and on-premises settings.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14625</link>
<guid>https://arxiv.org/abs/2505.14625</guid>
<content:encoded><![CDATA[
arXiv:2505.14625v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at https://github.com/uw-nsl/TinyV.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models</title>
<link>https://arxiv.org/abs/2505.14629</link>
<guid>https://arxiv.org/abs/2505.14629</guid>
<content:encoded><![CDATA[
arXiv:2505.14629v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at https://github.com/mohbattharani/KERL.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep Learning</title>
<link>https://arxiv.org/abs/2505.14635</link>
<guid>https://arxiv.org/abs/2505.14635</guid>
<content:encoded><![CDATA[
arXiv:2505.14635v1 Announce Type: new 
Abstract: We present the first theoretical framework that connects predictive coding (PC), a biologically inspired local learning rule, with the minimum description length (MDL) principle in deep networks. We prove that layerwise PC performs block-coordinate descent on the MDL two-part code objective, thereby jointly minimizing empirical risk and model complexity. Using Hoeffding's inequality and a prefix-code prior, we derive a novel generalization bound of the form $R(\theta) \le \^{R}(\theta) + \frac{L(\theta)}{N}$, capturing the tradeoff between fit and compression. We further prove that each PC sweep monotonically decreases the empirical two-part codelength, yielding tighter high-probability risk bounds than unconstrained gradient descent. Finally, we show that repeated PC updates converge to a block-coordinate stationary point, providing an approximate MDL-optimal solution. To our knowledge, this is the first result offering formal generalization and convergence guarantees for PC-trained deep models, positioning PC as a theoretically grounded and biologically plausible alternative to backpropagation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Diagnosis of Atrial Fibrillation Recurrence: A Large Tabular Model Approach with Structured and Unstructured Clinical Data</title>
<link>https://arxiv.org/abs/2505.14643</link>
<guid>https://arxiv.org/abs/2505.14643</guid>
<content:encoded><![CDATA[
arXiv:2505.14643v1 Announce Type: new 
Abstract: BACKGROUND: Atrial fibrillation (AF), the most common arrhythmia, is linked to high morbidity and mortality. In a fast-evolving AF rhythm control treatment era, predicting AF recurrence after its onset may be crucial to achieve the optimal therapeutic approach, yet traditional scores like CHADS2-VASc, HATCH, and APPLE show limited predictive accuracy. Moreover, early diagnosis studies often rely on codified electronic health record (EHR) data, which may contain errors and missing information.
  OBJECTIVE: This study aims to predict AF recurrence between one month and two years after onset by evaluating traditional clinical scores, ML models, and our LTM approach. Moreover, another objective is to develop a methodology for integrating structured and unstructured data to enhance tabular dataset quality.
  METHODS: A tabular dataset was generated by combining structured clinical data with free-text discharge reports processed through natural language processing techniques, reducing errors and annotation effort. A total of 1,508 patients with documented AF onset were identified, and models were evaluated on a manually annotated test set. The proposed approach includes a LTM compared against traditional clinical scores and ML models.
  RESULTS: The proposed LTM approach achieved the highest predictive performance, surpassing both traditional clinical scores and ML models. Additionally, the gender and age bias analyses revealed demographic disparities.
  CONCLUSION: The integration of structured data and free-text sources resulted in a high-quality dataset. The findings emphasize the limitations of traditional clinical scores in predicting AF recurrence and highlight the potential of ML-based approaches, particularly our LTM model.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks</title>
<link>https://arxiv.org/abs/2505.14659</link>
<guid>https://arxiv.org/abs/2505.14659</guid>
<content:encoded><![CDATA[
arXiv:2505.14659v1 Announce Type: new 
Abstract: As healthcare systems increasingly adopt advanced wireless networks and connected devices, securing medical applications has become critical. The integration of Internet of Medical Things devices, such as robotic surgical tools, intensive care systems, and wearable monitors has enhanced patient care but introduced serious security risks. Cyberattacks on these devices can lead to life threatening consequences, including surgical errors, equipment failure, and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative role in healthcare through AI and cloud integration, it also raises new security concerns. This paper explores how explainable AI techniques like SHAP, LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve trust and transparency in 6G enabled healthcare. We support our approach with experimental analysis and highlight promising results.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quartet: Native FP4 Training Can Be Optimal for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14669</link>
<guid>https://arxiv.org/abs/2505.14669</guid>
<content:encoded><![CDATA[
arXiv:2505.14669v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has been paralleled by unprecedented increases in computational demands, with training costs for state-of-the-art models doubling every few months. Training models directly in low-precision arithmetic offers a solution, by improving both computational throughput and energy efficiency. Specifically, NVIDIA's recent Blackwell architecture facilitates extremely low-precision operations, specifically FP4 variants, promising substantial efficiency gains. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we systematically investigate hardware-supported FP4 training and introduce Quartet, a new approach enabling accurate, end-to-end FP4 training with all the major computations (in e.g. linear layers) being performed in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across varying bit-widths and allows us to identify a "near-optimal" low-precision training technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve state-of-the-art accuracy for FP4 precision, successfully training billion-scale models. Our method demonstrates that fully FP4-based training is a competitive alternative to standard-precision and FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting LLM-based Relevance Modeling with Distribution-Aware Robust Learning</title>
<link>https://arxiv.org/abs/2412.12504</link>
<guid>https://arxiv.org/abs/2412.12504</guid>
<content:encoded><![CDATA[
arXiv:2412.12504v1 Announce Type: cross 
Abstract: With the rapid advancement of pre-trained large language models (LLMs), recent endeavors have leveraged the capabilities of LLMs in relevance modeling, resulting in enhanced performance. This is usually done through the process of fine-tuning LLMs on specifically annotated datasets to determine the relevance between queries and items. However, there are two limitations when LLMs are naively employed for relevance modeling through fine-tuning and inference. First, it is not inherently efficient for performing nuanced tasks beyond simple yes or no answers, such as assessing search relevance. It may therefore tend to be overconfident and struggle to distinguish fine-grained degrees of relevance (e.g., strong relevance, weak relevance, irrelevance) used in search engines. Second, it exhibits significant performance degradation when confronted with data distribution shift in real-world scenarios. In this paper, we propose a novel Distribution-Aware Robust Learning framework (DaRL) for relevance modeling in Alipay Search. Specifically, we design an effective loss function to enhance the discriminability of LLM-based relevance modeling across various fine-grained degrees of query-item relevance. To improve the generalizability of LLM-based relevance modeling, we first propose the Distribution-Aware Sample Augmentation (DASA) module. This module utilizes out-of-distribution (OOD) detection techniques to actively select appropriate samples that are not well covered by the original training set for model fine-tuning. Furthermore, we adopt a multi-stage fine-tuning strategy to simultaneously improve in-distribution (ID) and OOD performance, bridging the performance gap between them. DaRL has been deployed online to serve the Alipay's insurance product search...
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors</title>
<link>https://arxiv.org/abs/2505.11325</link>
<guid>https://arxiv.org/abs/2505.11325</guid>
<content:encoded><![CDATA[
arXiv:2505.11325v1 Announce Type: cross 
Abstract: Prior-data fitted networks (PFNs) have emerged as promising foundation models for prediction from tabular data sets, achieving state-of-the-art performance on small to moderate data sizes without tuning. While PFNs are motivated by Bayesian ideas, they do not provide any uncertainty quantification for predictive means, quantiles, or similar quantities. We propose a principled and efficient sampling procedure to construct Bayesian posteriors for such estimates based on Martingale posteriors, and prove its convergence. Several simulated and real-world data examples showcase the uncertainty quantification of our method in inference applications.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLOT: Sample-specific Language Model Optimization at Test-time</title>
<link>https://arxiv.org/abs/2505.12392</link>
<guid>https://arxiv.org/abs/2505.12392</guid>
<content:encoded><![CDATA[
arXiv:2505.12392v1 Announce Type: cross 
Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a novel and parameter-efficient test-time inference approach that enhances a language model's ability to more accurately respond to individual prompts. Existing Large Language Models (LLMs) often struggle with complex instructions, leading to poor performances on those not well represented among general samples. To address this, SLOT conducts few optimization steps at test-time to update a light-weight sample-specific parameter vector. It is added to the final hidden layer before the output head, and enables efficient adaptation by caching the last layer features during per-sample optimization. By minimizing the cross-entropy loss on the input prompt only, SLOT helps the model better aligned with and follow each given instruction. In experiments, we demonstrate that our method outperforms the compared models across multiple benchmarks and LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is available at https://github.com/maple-research-lab/SLOT.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Edge AI Solution for Space Object Detection</title>
<link>https://arxiv.org/abs/2505.13468</link>
<guid>https://arxiv.org/abs/2505.13468</guid>
<content:encoded><![CDATA[
arXiv:2505.13468v1 Announce Type: cross 
Abstract: Effective Edge AI for space object detection (SOD) tasks that can facilitate real-time collision assessment and avoidance is essential with the increasing space assets in near-Earth orbits. In SOD, low Earth orbit (LEO) satellites must detect other objects with high precision and minimal delay. We explore an Edge AI solution based on deep-learning-based vision sensing for SOD tasks and propose a deep learning model based on Squeeze-and-Excitation (SE) layers, Vision Transformers (ViT), and YOLOv9 framework. We evaluate the performance of these models across various realistic SOD scenarios, demonstrating their ability to detect multiple satellites with high accuracy and very low latency.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Tradeoffs in Fair Lending: Profitability, Compliance, and Long-Term Impact</title>
<link>https://arxiv.org/abs/2505.13469</link>
<guid>https://arxiv.org/abs/2505.13469</guid>
<content:encoded><![CDATA[
arXiv:2505.13469v1 Announce Type: cross 
Abstract: As financial institutions increasingly rely on machine learning models to automate lending decisions, concerns about algorithmic fairness have risen. This paper explores the tradeoff between enforcing fairness constraints (such as demographic parity or equal opportunity) and maximizing lender profitability. Through simulations on synthetic data that reflects real-world lending patterns, we quantify how different fairness interventions impact profit margins and default rates. Our results demonstrate that equal opportunity constraints typically impose lower profit costs than demographic parity, but surprisingly, removing protected attributes from the model (fairness through unawareness) outperforms explicit fairness interventions in both fairness and profitability metrics. We further identify the specific economic conditions under which fair lending becomes profitable and analyze the feature-specific drivers of unfairness. These findings offer practical guidance for designing lending algorithms that balance ethical considerations with business objectives.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RTL++: Graph-enhanced LLM for RTL Code Generation</title>
<link>https://arxiv.org/abs/2505.13479</link>
<guid>https://arxiv.org/abs/2505.13479</guid>
<content:encoded><![CDATA[
arXiv:2505.13479v1 Announce Type: cross 
Abstract: As hardware design complexity escalates, there is an urgent need for advanced automation in electronic design automation (EDA). Traditional register transfer level (RTL) design methods are manual, time-consuming, and prone to errors. While commercial (instruction-tuned) large language models (LLMs) shows promising performance for automation, they pose security and privacy concerns. Open-source models offer alternatives; however, they frequently fall short in quality/correctness, largely due to limited, high-quality RTL code data essential for effective training and generalization. This paper proposes RTL++, a first-of-its-kind LLM-assisted method for RTL code generation that utilizes graph representations of code structures to enhance the quality of generated code. By encoding RTL code into a textualized control flowgraphs (CFG) and data flow graphs (DFG), RTL++ captures the inherent hierarchy, dependencies, and relationships within the code. This structured graph-based approach enhances the context available to LLMs, enabling them to better understand and generate instructions. By focusing on data generation through graph representations, RTL++ addresses the limitations of previous approaches that rely solely on code and suffer from lack of diversity. Experimental results demonstrate that RTL++ outperforms state-of-the-art models fine-tuned for RTL generation, as evaluated using the VerilogEval benchmark's Pass@1/5/10 metric, as well as the RTLLM1.1 model, which highlight the effectiveness of graph-enhanced context in advancing the capabilities of LLM-assisted RTL code generation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale</title>
<link>https://arxiv.org/abs/2505.13480</link>
<guid>https://arxiv.org/abs/2505.13480</guid>
<content:encoded><![CDATA[
arXiv:2505.13480v1 Announce Type: cross 
Abstract: Suicide prevention remains a critical public health challenge. While online platforms such as Reddit's r/SuicideWatch have historically provided spaces for individuals to express suicidal thoughts and seek community support, the advent of large language models (LLMs) introduces a new paradigm-where individuals may begin disclosing ideation to AI systems instead of humans. This study evaluates the capability of LLMs to perform automated suicide risk assessment using the Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot performance of six models-including Claude, GPT, Mistral, and LLaMA-in classifying posts across a 7-point severity scale (Levels 0-6). Results indicate that Claude and GPT closely align with human annotations, while Mistral achieves the lowest ordinal prediction error. Most models exhibit ordinal sensitivity, with misclassifications typically occurring between adjacent severity levels. We further analyze confusion patterns, misclassification sources, and ethical considerations, underscoring the importance of human oversight, transparency, and cautious deployment. Full code and supplementary materials are available at https://github.com/av9ash/llm_cssrs_code.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProdRev: A DNN framework for empowering customers using generative pre-trained transformers</title>
<link>https://arxiv.org/abs/2505.13491</link>
<guid>https://arxiv.org/abs/2505.13491</guid>
<content:encoded><![CDATA[
arXiv:2505.13491v1 Announce Type: cross 
Abstract: Following the pandemic, customers, preference for using e-commerce has accelerated. Since much information is available in multiple reviews (sometimes running in thousands) for a single product, it can create decision paralysis for the buyer. This scenario disempowers the consumer, who cannot be expected to go over so many reviews since its time consuming and can confuse them. Various commercial tools are available, that use a scoring mechanism to arrive at an adjusted score. It can alert the user to potential review manipulations. This paper proposes a framework that fine-tunes a generative pre-trained transformer to understand these reviews better. Furthermore, using "common-sense" to make better decisions. These models have more than 13 billion parameters. To fine-tune the model for our requirement, we use the curie engine from generative pre-trained transformer (GPT3). By using generative models, we are introducing abstractive summarization. Instead of using a simple extractive method of summarizing the reviews. This brings out the true relationship between the reviews and not simply copy-paste. This introduces an element of "common sense" for the user and helps them to quickly make the right decisions. The user is provided the pros and cons of the processed reviews. Thus the user/customer can take their own decisions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polymer Data Challenges in the AI Era: Bridging Gaps for Next-Generation Energy Materials</title>
<link>https://arxiv.org/abs/2505.13494</link>
<guid>https://arxiv.org/abs/2505.13494</guid>
<content:encoded><![CDATA[
arXiv:2505.13494v1 Announce Type: cross 
Abstract: The pursuit of advanced polymers for energy technologies, spanning photovoltaics, solid-state batteries, and hydrogen storage, is hindered by fragmented data ecosystems that fail to capture the hierarchical complexity of these materials. Polymer science lacks interoperable databases, forcing reliance on disconnected literature and legacy records riddled with unstructured formats and irreproducible testing protocols. This fragmentation stifles machine learning (ML) applications and delays the discovery of materials critical for global decarbonization. Three systemic barriers compound the challenge. First, academic-industrial data silos restrict access to proprietary industrial datasets, while academic publications often omit critical synthesis details. Second, inconsistent testing methods undermine cross-study comparability. Third, incomplete metadata in existing databases limits their utility for training reliable ML models. Emerging solutions address these gaps through technological and collaborative innovation. Natural language processing (NLP) tools extract structured polymer data from decades of literature, while high-throughput robotic platforms generate self-consistent datasets via autonomous experimentation. Central to these advances is the adoption of FAIR (Findable, Accessible, Interoperable, Reusable) principles, adapted to polymer-specific ontologies, ensuring machine-readability and reproducibility. Future breakthroughs hinge on cultural shifts toward open science, accelerated by decentralized data markets and autonomous laboratories that merge robotic experimentation with real-time ML validation. By addressing data fragmentation through technological innovation, collaborative governance, and ethical stewardship, the polymer community can transform bottlenecks into accelerants.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADALog: Adaptive Unsupervised Anomaly detection in Logs with Self-attention Masked Language Model</title>
<link>https://arxiv.org/abs/2505.13496</link>
<guid>https://arxiv.org/abs/2505.13496</guid>
<content:encoded><![CDATA[
arXiv:2505.13496v1 Announce Type: cross 
Abstract: Modern software systems generate extensive heterogeneous log data with dynamic formats, fragmented event sequences, and varying temporal patterns, making anomaly detection both crucial and challenging. To address these complexities, we propose ADALog, an adaptive, unsupervised anomaly detection framework designed for practical applicability across diverse real-world environments. Unlike traditional methods reliant on log parsing, strict sequence dependencies, or labeled data, ADALog operates on individual unstructured logs, extracts intra-log contextual relationships, and performs adaptive thresholding on normal data. The proposed approach utilizes a transformer-based, pretrained bidirectional encoder with a masked language modeling task, fine-tuned on normal logs to capture domain-specific syntactic and semantic patterns essential for accurate anomaly detection. Anomalies are identified via token-level reconstruction probabilities, aggregated into log-level scores, with adaptive percentile-based thresholding calibrated only on normal data. This allows the model to dynamically adapt to evolving system behaviors while avoiding rigid, heuristic-based thresholds common in traditional systems. We evaluate ADALog on benchmark datasets BGL, Thunderbird, and Spirit, showing strong generalization and competitive performance compared to state-of-the-art supervised and unsupervised methods. Additional ablation studies examine the effects of masking, fine-tuning, and token positioning on model behavior and interpretability.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-R1: Towards Comprehensive Temporal Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2505.13508</link>
<guid>https://arxiv.org/abs/2505.13508</guid>
<content:encoded><![CDATA[
arXiv:2505.13508v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce \textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a \textit{reinforcement learning (RL) curriculum} driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release \textit{Time-Bench}, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of \textit{Time-R1} checkpoints.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuck the Algorithm: Conceptual Issues in Algorithmic Bias</title>
<link>https://arxiv.org/abs/2505.13509</link>
<guid>https://arxiv.org/abs/2505.13509</guid>
<content:encoded><![CDATA[
arXiv:2505.13509v1 Announce Type: cross 
Abstract: Algorithmic bias has been the subject of much recent controversy. To clarify what is at stake and to make progress resolving the controversy, a better understanding of the concepts involved would be helpful. The discussion here focuses on the disputed claim that algorithms themselves cannot be biased. To clarify this claim we need to know what kind of thing 'algorithms themselves' are, and to disambiguate the several meanings of 'bias' at play. This further involves showing how bias of moral import can result from statistical biases, and drawing connections to previous conceptual work about political artifacts and oppressive things. Data bias has been identified in domains like hiring, policing and medicine. Examples where algorithms themselves have been pinpointed as the locus of bias include recommender systems that influence media consumption, academic search engines that influence citation patterns, and the 2020 UK algorithmically-moderated A-level grades. Recognition that algorithms are a kind of thing that can be biased is key to making decisions about responsibility for harm, and preventing algorithmically mediated discrimination.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale</title>
<link>https://arxiv.org/abs/2505.13511</link>
<guid>https://arxiv.org/abs/2505.13511</guid>
<content:encoded><![CDATA[
arXiv:2505.13511v1 Announce Type: cross 
Abstract: This study explores Large Language Models (LLMs) as autonomous agents for real-world tasks, including freelance software development. This work presents a new benchmark that evaluates LLMs on freelance programming and data analysis tasks derived from economic data. We construct the benchmark using synthetic tasks created from a Kaggle Freelancer dataset of job postings, with all job prices standardized to USD (median fixed-project price around $250, and an average of $306). Each task is accompanied by structured input-output test cases and an estimated price tag, enabling automated correctness checking and a monetary performance valuation. This approach is inspired by OpenAI's recent SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our framework simplifies evaluation using programmatically testable tasks and predicted price values, making it highly scalable and repeatable. On this benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen 2.5, and Mistral. We report each model's accuracy (task success rate and test-case pass rate) and the total "freelance earnings" it achieves (sum of prices of solved tasks). Our results show that Claude 3.5 Haiku performs best, earning approximately $1.52 million USD, followed closely by GPT-4o-mini at $1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the distribution of errors per task and observe that the strongest models solve the most tasks and rarely fail completely on any project. We discuss the implications of these results for the feasibility of AI as a freelance developer, the advantages and limitations of our automated benchmark approach, and the gap between performance on structured tasks versus the true complexity of real-world freelance jobs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Balancing Strategies: A Survey of Resampling and Augmentation Methods</title>
<link>https://arxiv.org/abs/2505.13518</link>
<guid>https://arxiv.org/abs/2505.13518</guid>
<content:encoded><![CDATA[
arXiv:2505.13518v1 Announce Type: cross 
Abstract: Imbalanced data poses a significant obstacle in machine learning, as an unequal distribution of class labels often results in skewed predictions and diminished model accuracy. To mitigate this problem, various resampling strategies have been developed, encompassing both oversampling and undersampling techniques aimed at modifying class proportions. Conventional oversampling approaches like SMOTE enhance the representation of the minority class, whereas undersampling methods focus on trimming down the majority class. Advances in deep learning have facilitated the creation of more complex solutions, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), which are capable of producing high-quality synthetic examples. This paper reviews a broad spectrum of data balancing methods, classifying them into categories including synthetic oversampling, adaptive techniques, generative models, ensemble-based strategies, hybrid approaches, undersampling, and neighbor-based methods. Furthermore, it highlights current developments in resampling techniques and discusses practical implementations and case studies that validate their effectiveness. The paper concludes by offering perspectives on potential directions for future exploration in this domain.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Domain Generalization</title>
<link>https://arxiv.org/abs/2505.13519</link>
<guid>https://arxiv.org/abs/2505.13519</guid>
<content:encoded><![CDATA[
arXiv:2505.13519v1 Announce Type: cross 
Abstract: Real-world data distributions often shift continuously across multiple latent factors such as time, geography, and socioeconomic context. However, existing domain generalization approaches typically treat domains as discrete or evolving along a single axis (e.g., time), which fails to capture the complex, multi-dimensional nature of real-world variation. This paper introduces the task of Continuous Domain Generalization (CDG), which aims to generalize predictive models to unseen domains defined by arbitrary combinations of continuous variation descriptors. We present a principled framework grounded in geometric and algebraic theory, showing that optimal model parameters across domains lie on a low-dimensional manifold. To model this structure, we propose a Neural Lie Transport Operator (NeuralLTO), which enables structured parameter transitions by enforcing geometric continuity and algebraic consistency. To handle noisy or incomplete domain descriptors, we introduce a gating mechanism to suppress irrelevant dimensions and a local chart-based strategy for robust generalization. Extensive experiments on synthetic and real-world datasets-including remote sensing, scientific documents, and traffic forecasting-demonstrate that our method significantly outperforms existing baselines in generalization accuracy and robustness under descriptor imperfections.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Program Quantum Measurements for Machine Learning</title>
<link>https://arxiv.org/abs/2505.13525</link>
<guid>https://arxiv.org/abs/2505.13525</guid>
<content:encoded><![CDATA[
arXiv:2505.13525v1 Announce Type: cross 
Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML) have sparked significant interest, driving extensive exploration of quantum machine learning (QML) algorithms to address a wide range of complex challenges. The development of high-performance QML models requires expert-level expertise, presenting a key challenge to the widespread adoption of QML. Critical obstacles include the design of effective data encoding strategies and parameterized quantum circuits, both of which are vital for the performance of QML models. Furthermore, the measurement process is often neglected-most existing QML models employ predefined measurement schemes that may not align with the specific requirements of the targeted problem. We propose an innovative framework that renders the observable of a quantum system-specifically, the Hermitian matrix-trainable. This approach employs an end-to-end differentiable learning framework, enabling simultaneous optimization of the neural network used to program the parameterized observables and the standard quantum circuit parameters. Notably, the quantum observable parameters are dynamically programmed by the neural network, allowing the observables to adapt in real time based on the input data stream. Through numerical simulations, we demonstrate that the proposed method effectively programs observables dynamically within variational quantum circuits, achieving superior results compared to existing approaches. Notably, it delivers enhanced performance metrics, such as higher classification accuracy, thereby significantly improving the overall effectiveness of QML models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs</title>
<link>https://arxiv.org/abs/2505.13529</link>
<guid>https://arxiv.org/abs/2505.13529</guid>
<content:encoded><![CDATA[
arXiv:2505.13529v1 Announce Type: cross 
Abstract: Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don't know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL-a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs</title>
<link>https://arxiv.org/abs/2505.13533</link>
<guid>https://arxiv.org/abs/2505.13533</guid>
<content:encoded><![CDATA[
arXiv:2505.13533v1 Announce Type: cross 
Abstract: Financial tasks are pivotal to global economic stability; however, their execution faces challenges including labor intensive processes, low error tolerance, data fragmentation, and tool limitations. Although large language models (LLMs) have succeeded in various natural language processing tasks and have shown potential in automating workflows through reasoning and contextual understanding, current benchmarks for evaluating LLMs in finance lack sufficient domain-specific data, have simplistic task design, and incomplete evaluation frameworks. To address these gaps, this article presents FinMaster, a comprehensive financial benchmark designed to systematically assess the capabilities of LLM in financial literacy, accounting, auditing, and consulting. Specifically, FinMaster comprises three main modules: i) FinSim, which builds simulators that generate synthetic, privacy-compliant financial data for companies to replicate market dynamics; ii) FinSuite, which provides tasks in core financial domains, spanning 183 tasks of various types and difficulty levels; and iii) FinEval, which develops a unified interface for evaluation. Extensive experiments over state-of-the-art LLMs reveal critical capability gaps in financial reasoning, with accuracy dropping from over 90% on basic tasks to merely 40% on complex scenarios requiring multi-step reasoning. This degradation exhibits the propagation of computational errors, where single-metric calculations initially demonstrating 58% accuracy decreased to 37% in multimetric scenarios. To the best of our knowledge, FinMaster is the first benchmark that covers full-pipeline financial workflows with challenging tasks. We hope that FinMaster can bridge the gap between research and industry practitioners, driving the adoption of LLMs in real-world financial practices to enhance efficiency and accuracy.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EuLearn: A 3D database for learning Euler characteristics</title>
<link>https://arxiv.org/abs/2505.13539</link>
<guid>https://arxiv.org/abs/2505.13539</guid>
<content:encoded><![CDATA[
arXiv:2505.13539v1 Announce Type: cross 
Abstract: We present EuLearn, the first surface datasets equitably representing a diversity of topological types. We designed our embedded surfaces of uniformly varying genera relying on random knots, thus allowing our surfaces to knot with themselves. EuLearn contributes new topological datasets of meshes, point clouds, and scalar fields in 3D. We aim to facilitate the training of machine learning systems that can discern topological features. We experimented with specific emblematic 3D neural network architectures, finding that their vanilla implementations perform poorly on genus classification. To enhance performance, we developed a novel, non-Euclidean, statistical sampling method adapted to graph and manifold data. We also introduce adjacency-informed adaptations of PointNet and Transformer architectures that rely on our non-Euclidean sampling strategy. Our results demonstrate that incorporating topological information into deep learning workflows significantly improves performance on these otherwise challenging EuLearn datasets.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPIRIT: Patching Speech Language Models against Jailbreak Attacks</title>
<link>https://arxiv.org/abs/2505.13541</link>
<guid>https://arxiv.org/abs/2505.13541</guid>
<content:encoded><![CDATA[
arXiv:2505.13541v1 Announce Type: cross 
Abstract: Speech Language Models (SLMs) enable natural interactions via spoken instructions, which more effectively capture user intent by detecting nuances in speech. The richer speech signal introduces new security risks compared to text-based models, as adversaries can better bypass safety mechanisms by injecting imperceptible noise to speech. We analyze adversarial attacks and find that SLMs are substantially more vulnerable to jailbreak attacks, which can achieve a perfect 100% attack success rate in some instances. To improve security, we propose post-hoc patching defenses used to intervene during inference by modifying the SLM's activations that improve robustness up to 99% with (i) negligible impact on utility and (ii) without any re-training. We conduct ablation studies to maximize the efficacy of our defenses and improve the utility/security trade-off, validated with large-scale benchmarks unique to SLMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Code Generation for Functional Guarantees</title>
<link>https://arxiv.org/abs/2505.13553</link>
<guid>https://arxiv.org/abs/2505.13553</guid>
<content:encoded><![CDATA[
arXiv:2505.13553v1 Announce Type: cross 
Abstract: Large language models (LLMs) show human-level performance and their specialized descendants, code generation models, play core roles in solving complex tasks, including mathematical reasoning and software development. On the downside, the hallucination of LLMs mainly hinders their applicability to systems requiring higher safety standards, thus drawing the attention of the AI community. However, the hallucination of code generation models is rarely considered. One critical bottleneck in considering code hallucination is the intricate property of code to identify whether generated code has the intended functionality due to its un-natural form, different to natural languages. Handful of unit tests have been considered to address this issue, but scaling-up its size is extremely expensive. We address this core bottleneck by automatically generating unit tests using dynamic code analysis tools, which leverages the \emph{executable nature} of code. Given generated unit tests from true code for measuring functional correctness of generated code, we propose to learn a \emph{selective code generator}, which abstains from answering for unsure generation, to control the rate of code hallucination among non-abstaining answers in terms of a false discovery rate. This learning algorithm provides a controllability guarantee, providing trustworthiness of code generation. Finally, we propose to use generated unit tests in evaluation as well as in learning for precise code evaluation, calling this evaluation paradigm \emph{FuzzEval}. We demonstrate the efficacy of our selective code generator over open and closed code generators, showing clear benefit of leveraging generated unit tests along with the controllability of code hallucination and reasonable selection efficiency via our selective code generator.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Collision Risk from Naturalistic Driving with Generalised Surrogate Safety Measures</title>
<link>https://arxiv.org/abs/2505.13556</link>
<guid>https://arxiv.org/abs/2505.13556</guid>
<content:encoded><![CDATA[
arXiv:2505.13556v1 Announce Type: cross 
Abstract: Accurate and timely alerts for drivers or automated systems to unfolding collisions remains a challenge in road safety, particularly in highly interactive urban traffic. Existing approaches require labour-intensive annotation of sparse risk, struggle to consider varying interaction context, or are useful only in the scenarios they are designed for. To address these limits, this study introduces the generalised surrogate safety measure (GSSM), a new approach that learns exclusively from naturalistic driving without crash or risk labels. GSSM captures the patterns of normal driving and estimates the extent to which a traffic interaction deviates from the norm towards unsafe extreme. Utilising neural networks, normal interactions are characterised by context-conditioned distributions of multi-directional spacing between road users. In the same interaction context, a spacing closer than normal entails higher risk of potential collision. Then a context-adaptive risk score and its associated probability can be calculated based on the theory of extreme values. Any measurable factors, such as motion kinematics, weather, lighting, can serve as part of the context, allowing for diverse coverage of safety-critical interactions. Multiple public driving datasets are used to train GSSMs, which are tested with 4,875 real-world crashes and near-crashes reconstructed from the SHRP2 NDS. A vanilla GSSM using only instantaneous states achieves AUPRC of 0.9 and secures a median time advance of 2.6 seconds to prevent potential collisions. Additional data and contextual factors provide further performance gains. Across various interaction types such as rear-end, merging, and crossing, the accuracy and timeliness of GSSM consistently outperforms existing baselines. GSSM therefore establishes a scalable, context-aware, and generalisable foundation to proactively quantify collision risk in traffic interactions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CATS: Clustering-Aggregated and Time Series for Business Customer Purchase Intention Prediction</title>
<link>https://arxiv.org/abs/2505.13558</link>
<guid>https://arxiv.org/abs/2505.13558</guid>
<content:encoded><![CDATA[
arXiv:2505.13558v1 Announce Type: cross 
Abstract: Accurately predicting customers' purchase intentions is critical to the success of a business strategy. Current researches mainly focus on analyzing the specific types of products that customers are likely to purchase in the future, little attention has been paid to the critical factor of whether customers will engage in repurchase behavior. Predicting whether a customer will make the next purchase is a classic time series forecasting task. However, in real-world purchasing behavior, customer groups typically exhibit imbalance - i.e., there are a large number of occasional buyers and a small number of loyal customers. This head-to-tail distribution makes traditional time series forecasting methods face certain limitations when dealing with such problems. To address the above challenges, this paper proposes a unified Clustering and Attention mechanism GRU model (CAGRU) that leverages multi-modal data for customer purchase intention prediction. The framework first performs customer profiling with respect to the customer characteristics and clusters the customers to delineate the different customer clusters that contain similar features. Then, the time series features of different customer clusters are extracted by GRU neural network and an attention mechanism is introduced to capture the significance of sequence locations. Furthermore, to mitigate the head-to-tail distribution of customer segments, we train the model separately for each customer segment, to adapt and capture more accurately the differences in behavioral characteristics between different customer segments, as well as the similar characteristics of the customers within the same customer segment. We constructed four datasets and conducted extensive experiments to demonstrate the superiority of the proposed CAGRU approach.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models</title>
<link>https://arxiv.org/abs/2505.13559</link>
<guid>https://arxiv.org/abs/2505.13559</guid>
<content:encoded><![CDATA[
arXiv:2505.13559v1 Announce Type: cross 
Abstract: Code-switching (CS) poses a significant challenge for Large Language Models (LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue to English summarization. CS-Sum is the first benchmark for CS dialogue summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language pair. Evaluating ten LLMs, including open and closed-source models, we analyze performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA on synthetic data) approaches. Our findings show that though the scores on automated metrics are high, LLMs make subtle mistakes that alter the complete meaning of the dialogue. To this end, we introduce 3 most common type of errors that LLMs make when handling CS input. Error rates vary across CS pairs and LLMs, with some LLMs showing more frequent errors on certain language pairs, underscoring the need for specialized training on code-switched data.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit Feedback</title>
<link>https://arxiv.org/abs/2505.13562</link>
<guid>https://arxiv.org/abs/2505.13562</guid>
<content:encoded><![CDATA[
arXiv:2505.13562v1 Announce Type: cross 
Abstract: Learning in games is a fundamental problem in machine learning and artificial intelligence, with numerous applications~\citep{silver2016mastering,schrittwieser2020mastering}. This work investigates two-player zero-sum matrix games with an unknown payoff matrix and bandit feedback, where each player observes their actions and the corresponding noisy payoff. Prior studies have proposed algorithms for this setting~\citep{o2021matrix,maiti2023query,cai2024uncoupled}, with \citet{o2021matrix} demonstrating the effectiveness of deterministic optimism (e.g., \ucb) in achieving sublinear regret. However, the potential of randomised optimism in matrix games remains theoretically unexplored.
  We propose Competitive Co-evolutionary Bandit Learning (\coebl), a novel algorithm that integrates evolutionary algorithms (EAs) into the bandit framework to implement randomised optimism through EA variation operators. We prove that \coebl achieves sublinear regret, matching the performance of deterministic optimism-based methods. To the best of our knowledge, this is the first theoretical regret analysis of an evolutionary bandit learning algorithm in matrix games.
  Empirical evaluations on diverse matrix game benchmarks demonstrate that \coebl not only achieves sublinear regret but also consistently outperforms classical bandit algorithms, including \exptr~\citep{auer2002nonstochastic}, the variant \exptrni~\citep{cai2024uncoupled}, and \ucb~\citep{o2021matrix}. These results highlight the potential of evolutionary bandit learning, particularly the efficacy of randomised optimism via evolutionary algorithms in game-theoretic settings.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous nanoparticle synthesis by design</title>
<link>https://arxiv.org/abs/2505.13571</link>
<guid>https://arxiv.org/abs/2505.13571</guid>
<content:encoded><![CDATA[
arXiv:2505.13571v1 Announce Type: cross 
Abstract: Controlled synthesis of materials with specified atomic structures underpins technological advances yet remains reliant on iterative, trial-and-error approaches. Nanoparticles (NPs), whose atomic arrangement dictates their emergent properties, are particularly challenging to synthesise due to numerous tunable parameters. Here, we introduce an autonomous approach explicitly targeting synthesis of atomic-scale structures. Our method autonomously designs synthesis protocols by matching real time experimental total scattering (TS) and pair distribution function (PDF) data to simulated target patterns, without requiring prior synthesis knowledge. We demonstrate this capability at a synchrotron, successfully synthesising two structurally distinct gold NPs: 5 nm decahedral and 10 nm face-centred cubic structures. Ultimately, specifying a simulated target scattering pattern, thus representing a bespoke atomic structure, and obtaining both the synthesised material and its reproducible synthesis protocol on demand may revolutionise materials design. Thus, ScatterLab provides a generalisable blueprint for autonomous, atomic structure-targeted synthesis across diverse systems and applications.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction</title>
<link>https://arxiv.org/abs/2505.13579</link>
<guid>https://arxiv.org/abs/2505.13579</guid>
<content:encoded><![CDATA[
arXiv:2505.13579v1 Announce Type: cross 
Abstract: Cone-Beam Computed Tomography (CBCT) is essential in medical imaging, and the Feldkamp-Davis-Kress (FDK) algorithm is a popular choice for reconstruction due to its efficiency. However, FDK is susceptible to noise and artifacts. While recent deep learning methods offer improved image quality, they often increase computational complexity and lack the interpretability of traditional methods. In this paper, we introduce an enhanced FDK-based neural network that maintains the classical algorithm's interpretability by selectively integrating trainable elements into the cosine weighting and filtering stages. Recognizing the challenge of a large parameter space inherent in 3D CBCT data, we leverage wavelet transformations to create sparse representations of the cosine weights and filters. This strategic sparsification reduces the parameter count by $93.75\%$ without compromising performance, accelerates convergence, and importantly, maintains the inference computational cost equivalent to the classical FDK algorithm. Our method not only ensures volumetric consistency and boosts robustness to noise, but is also designed for straightforward integration into existing CT reconstruction pipelines. This presents a pragmatic enhancement that can benefit clinical applications, particularly in environments with computational limitations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Bayesian Monte Carlo: fast uncertainty estimation beyond deep ensembles</title>
<link>https://arxiv.org/abs/2505.13585</link>
<guid>https://arxiv.org/abs/2505.13585</guid>
<content:encoded><![CDATA[
arXiv:2505.13585v1 Announce Type: cross 
Abstract: This work introduces a new method called scalable Bayesian Monte Carlo (SBMC). The model interpolates between a point estimator and the posterior, and the algorithm is a parallel implementation of a consistent (asymptotically unbiased) Bayesian deep learning algorithm: sequential Monte Carlo (SMC) or Markov chain Monte Carlo (MCMC). The method is motivated theoretically, and its utility is demonstrated on practical examples: MNIST, CIFAR, IMDb. A systematic numerical study reveals that parallel implementations of SMC and MCMC are comparable to serial implementations in terms of performance and total cost, and they achieve accuracy at or beyond the state-of-the-art (SOTA) methods like deep ensembles at convergence, along with substantially improved uncertainty quantification (UQ)--in particular, epistemic UQ. But even parallel implementations are expensive, with an irreducible time barrier much larger than the cost of the MAP estimator. Compressing time further leads to rapid degradation of accuracy, whereas UQ remains valuable. By anchoring to a point estimator we can recover accuracy, while retaining valuable UQ, ultimately delivering strong performance across metrics for a cost comparable to the SOTA.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses</title>
<link>https://arxiv.org/abs/2505.13617</link>
<guid>https://arxiv.org/abs/2505.13617</guid>
<content:encoded><![CDATA[
arXiv:2505.13617v1 Announce Type: cross 
Abstract: The characteristics of a sound field are intrinsically linked to the geometric and spatial properties of the environment surrounding a sound source and a listener. The physics of sound propagation is captured in a time-domain signal known as a room impulse response (RIR). Prior work using neural fields (NFs) has allowed learning spatially-continuous representations of RIRs from finite RIR measurements. However, previous NF-based methods have focused on monaural omnidirectional or at most binaural listeners, which does not precisely capture the directional characteristics of a real sound field at a single point. We propose a direction-aware neural field (DANF) that more explicitly incorporates the directional information by Ambisonic-format RIRs. While DANF inherently captures spatial relations between sources and listeners, we further propose a direction-aware loss. In addition, we investigate the ability of DANF to adapt to new rooms in various ways including low-rank adaptation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traceable Black-box Watermarks for Federated Learning</title>
<link>https://arxiv.org/abs/2505.13651</link>
<guid>https://arxiv.org/abs/2505.13651</guid>
<content:encoded><![CDATA[
arXiv:2505.13651v1 Announce Type: cross 
Abstract: Due to the distributed nature of Federated Learning (FL) systems, each local client has access to the global model, posing a critical risk of model leakage. Existing works have explored injecting watermarks into local models to enable intellectual property protection. However, these methods either focus on non-traceable watermarks or traceable but white-box watermarks. We identify a gap in the literature regarding the formal definition of traceable black-box watermarking and the formulation of the problem of injecting such watermarks into FL systems. In this work, we first formalize the problem of injecting traceable black-box watermarks into FL. Based on the problem, we propose a novel server-side watermarking method, $\mathbf{TraMark}$, which creates a traceable watermarked model for each client, enabling verification of model leakage in black-box settings. To achieve this, $\mathbf{TraMark}$ partitions the model parameter space into two distinct regions: the main task region and the watermarking region. Subsequently, a personalized global model is constructed for each client by aggregating only the main task region while preserving the watermarking region. Each model then learns a unique watermark exclusively within the watermarking region using a distinct watermark dataset before being sent back to the local client. Extensive results across various FL systems demonstrate that $\mathbf{TraMark}$ ensures the traceability of all watermarked models while preserving their main task performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Client Sampling in Federated Learning with Client-Level Heterogeneous Differential Privacy</title>
<link>https://arxiv.org/abs/2505.13655</link>
<guid>https://arxiv.org/abs/2505.13655</guid>
<content:encoded><![CDATA[
arXiv:2505.13655v1 Announce Type: cross 
Abstract: Federated Learning with client-level differential privacy (DP) provides a promising framework for collaboratively training models while rigorously protecting clients' privacy. However, classic approaches like DP-FedAvg struggle when clients have heterogeneous privacy requirements, as they must uniformly enforce the strictest privacy level across clients, leading to excessive DP noise and significant model utility degradation. Existing methods to improve the model utility in such heterogeneous privacy settings often assume a trusted server and are largely heuristic, resulting in suboptimal performance and lacking strong theoretical underpinnings. In this work, we address these challenges under a practical attack model where both clients and the server are honest-but-curious. We propose GDPFed, which partitions clients into groups based on their privacy budgets and achieves client-level DP within each group to reduce the privacy budget waste and hence improve the model utility. Based on the privacy and convergence analysis of GDPFed, we find that the magnitude of DP noise depends on both model dimensionality and the per-group client sampling ratios. To further improve the performance of GDPFed, we introduce GDPFed$^+$, which integrates model sparsification to eliminate unnecessary noise and optimizes per-group client sampling ratios to minimize convergence error. Extensive empirical evaluations on multiple benchmark datasets demonstrate the effectiveness of GDPFed$^+$, showing substantial performance gains compared with state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sobolev Gradient Ascent for Optimal Transport: Barycenter Optimization and Convergence Analysis</title>
<link>https://arxiv.org/abs/2505.13660</link>
<guid>https://arxiv.org/abs/2505.13660</guid>
<content:encoded><![CDATA[
arXiv:2505.13660v1 Announce Type: cross 
Abstract: This paper introduces a new constraint-free concave dual formulation for the Wasserstein barycenter. Tailoring the vanilla dual gradient ascent algorithm to the Sobolev geometry, we derive a scalable Sobolev gradient ascent (SGA) algorithm to compute the barycenter for input distributions supported on a regular grid. Despite the algorithmic simplicity, we provide a global convergence analysis that achieves the same rate as the classical subgradient descent methods for minimizing nonsmooth convex functions in the Euclidean space. A central feature of our SGA algorithm is that the computationally expensive $c$-concavity projection operator enforced on the Kantorovich dual potentials is unnecessary to guarantee convergence, leading to significant algorithmic and theoretical simplifications over all existing primal and dual methods for computing the exact barycenter. Our numerical experiments demonstrate the superior empirical performance of SGA over the existing optimal transport barycenter solvers.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAFA: A multi-agent framework for annotation</title>
<link>https://arxiv.org/abs/2505.13668</link>
<guid>https://arxiv.org/abs/2505.13668</guid>
<content:encoded><![CDATA[
arXiv:2505.13668v1 Announce Type: cross 
Abstract: Modern applications require accurate and efficient retrieval of information in response to user queries. Mapping user utterances to the most relevant Frequently Asked Questions (FAQs) is a crucial component of these systems. Traditional approaches often rely on a single model or technique, which may not capture the nuances of diverse user inquiries. In this paper, we introduce a multi-agent framework for FAQ annotation that combines multiple specialized agents with different approaches and a judge agent that reranks candidates to produce optimal results. Our agents utilize a structured reasoning approach inspired by Attentive Reasoning Queries (ARQs), which guides them through systematic reasoning steps using targeted, task-specific JSON queries. Our framework features a specialized few-shot example strategy, where each agent receives different few-shots, enhancing ensemble diversity and coverage of the query space. We evaluate our framework on a real-world banking dataset as well as public benchmark datasets (LCQMC and FiQA), demonstrating significant improvements over single-agent approaches across multiple metrics, including a 14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12% improvement in Mean Reciprocal Rank on our dataset, and similar gains on public benchmarks when compared with traditional single agent annotation techniques. Our framework is particularly effective at handling ambiguous queries, making it well-suited for deployment in production applications while showing strong generalization capabilities across different domains and languages.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HarmonE: A Self-Adaptive Approach to Architecting Sustainable MLOps</title>
<link>https://arxiv.org/abs/2505.13693</link>
<guid>https://arxiv.org/abs/2505.13693</guid>
<content:encoded><![CDATA[
arXiv:2505.13693v1 Announce Type: cross 
Abstract: Machine Learning Enabled Systems (MLS) are becoming integral to real-world applications, but ensuring their sustainable performance over time remains a significant challenge. These systems operate in dynamic environments and face runtime uncertainties like data drift and model degradation, which affect the sustainability of MLS across multiple dimensions: technical, economical, environmental, and social. While Machine Learning Operations (MLOps) addresses the technical dimension by streamlining the ML model lifecycle, it overlooks other dimensions. Furthermore, some traditional practices, such as frequent retraining, incur substantial energy and computational overhead, thus amplifying sustainability concerns. To address them, we introduce HarmonE, an architectural approach that enables self-adaptive capabilities in MLOps pipelines using the MAPE-K loop. HarmonE allows system architects to define explicit sustainability goals and adaptation thresholds at design time, and performs runtime monitoring of key metrics, such as prediction accuracy, energy consumption, and data distribution shifts, to trigger appropriate adaptation strategies. We validate our approach using a Digital Twin (DT) of an Intelligent Transportation System (ITS), focusing on traffic flow prediction as our primary use case. The DT employs time series ML models to simulate real-time traffic and assess various flow scenarios. Our results show that HarmonE adapts effectively to evolving conditions while maintaining accuracy and meeting sustainability goals.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust learning of halfspaces under log-concave marginals</title>
<link>https://arxiv.org/abs/2505.13708</link>
<guid>https://arxiv.org/abs/2505.13708</guid>
<content:encoded><![CDATA[
arXiv:2505.13708v1 Announce Type: cross 
Abstract: We say that a classifier is \emph{adversarially robust} to perturbations of norm $r$ if, with high probability over a point $x$ drawn from the input distribution, there is no point within distance $\le r$ from $x$ that is classified differently. The \emph{boundary volume} is the probability that a point falls within distance $r$ of a point with a different label. This work studies the task of computationally efficient learning of hypotheses with small boundary volume, where the input is distributed as a subgaussian isotropic log-concave distribution over $\mathbb{R}^d$.
  Linear threshold functions are adversarially robust; they have boundary volume proportional to $r$. Such concept classes are efficiently learnable by polynomial regression, which produces a polynomial threshold function (PTF), but PTFs in general may have boundary volume $\Omega(1)$, even for $r \ll 1$.
  We give an algorithm that agnostically learns linear threshold functions and returns a classifier with boundary volume $O(r+\varepsilon)$ at radius of perturbation $r$. The time and sample complexity of $d^{\tilde{O}(1/\varepsilon^2)}$ matches the complexity of polynomial regression.
  Our algorithm augments the classic approach of polynomial regression with three additional steps: a) performing the $\ell_1$-error regression under noise sensitivity constraints, b) a structured partitioning and rounding step that returns a Boolean classifier with error $\textsf{opt} + O(\varepsilon)$ and noise sensitivity $O(r+\varepsilon)$ simultaneously, and c) a local corrector that ``smooths'' a function with low noise sensitivity into a function that is adversarially robust.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backward Conformal Prediction</title>
<link>https://arxiv.org/abs/2505.13732</link>
<guid>https://arxiv.org/abs/2505.13732</guid>
<content:encoded><![CDATA[
arXiv:2505.13732v1 Announce Type: cross 
Abstract: We introduce $\textit{Backward Conformal Prediction}$, a method that guarantees conformal coverage while providing flexible control over the size of prediction sets. Unlike standard conformal prediction, which fixes the coverage level and allows the conformal set size to vary, our approach defines a rule that constrains how prediction set sizes behave based on the observed data, and adapts the coverage level accordingly. Our method builds on two key foundations: (i) recent results by Gauthier et al. [2025] on post-hoc validity using e-values, which ensure marginal coverage of the form $\mathbb{P}(Y_{\rm test} \in \hat C_n^{\tilde{\alpha}}(X_{\rm test})) \ge 1 - \mathbb{E}[\tilde{\alpha}]$ up to a first-order Taylor approximation for any data-dependent miscoverage $\tilde{\alpha}$, and (ii) a novel leave-one-out estimator $\hat{\alpha}^{\rm LOO}$ of the marginal miscoverage $\mathbb{E}[\tilde{\alpha}]$ based on the calibration set, ensuring that the theoretical guarantees remain computable in practice. This approach is particularly useful in applications where large prediction sets are impractical such as medical diagnosis. We provide theoretical results and empirical evidence supporting the validity of our method, demonstrating that it maintains computable coverage guarantees while ensuring interpretable, well-controlled prediction set sizes.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference</title>
<link>https://arxiv.org/abs/2505.13770</link>
<guid>https://arxiv.org/abs/2505.13770</guid>
<content:encoded><![CDATA[
arXiv:2505.13770v1 Announce Type: cross 
Abstract: Reliable causal inference is essential for making decisions in high-stakes areas like medicine, economics, and public policy. However, it remains unclear whether large language models (LLMs) can handle rigorous and trustworthy statistical causal inference. Current benchmarks usually involve simplified tasks. For example, these tasks might only ask LLMs to identify semantic causal relationships or draw conclusions directly from raw data. As a result, models may overlook important statistical pitfalls, such as Simpson's paradox or selection bias. This oversight limits the applicability of LLMs in the real world. To address these limitations, we propose CausalPitfalls, a comprehensive benchmark designed to rigorously evaluate the capability of LLMs in overcoming common causal inference pitfalls. Our benchmark features structured challenges across multiple difficulty levels, each paired with grading rubrics. This approach allows us to quantitatively measure both causal reasoning capabilities and the reliability of LLMs' responses. We evaluate models using two protocols: (1) direct prompting, which assesses intrinsic causal reasoning, and (2) code-assisted prompting, where models generate executable code for explicit statistical analysis. Additionally, we validate the effectiveness of this judge by comparing its scoring with assessments from human experts. Our results reveal significant limitations in current LLMs when performing statistical causal inference. The CausalPitfalls benchmark provides essential guidance and quantitative metrics to advance the development of trustworthy causal reasoning systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score-Based Training for Energy-Based TTS Models</title>
<link>https://arxiv.org/abs/2505.13771</link>
<guid>https://arxiv.org/abs/2505.13771</guid>
<content:encoded><![CDATA[
arXiv:2505.13771v1 Announce Type: cross 
Abstract: Noise contrastive estimation (NCE) is a popular method for training energy-based models (EBM) with intractable normalisation terms. The key idea of NCE is to learn by comparing unnormalised log-likelihoods of the reference and noisy samples, thus avoiding explicitly computing normalisation terms. However, NCE critically relies on the quality of noisy samples. Recently, sliced score matching (SSM) has been popularised by closely related diffusion models (DM). Unlike NCE, SSM learns a gradient of log-likelihood, or score, by learning distribution of its projections on randomly chosen directions. However, both NCE and SSM disregard the form of log-likelihood function, which is problematic given that EBMs and DMs make use of first-order optimisation during inference. This paper proposes a new criterion that learns scores more suitable for first-order schemes. Experiments contrasts these approaches for training EBMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements</title>
<link>https://arxiv.org/abs/2505.13837</link>
<guid>https://arxiv.org/abs/2505.13837</guid>
<content:encoded><![CDATA[
arXiv:2505.13837v1 Announce Type: cross 
Abstract: Robots navigating complex environments must manage uncertainty from sensor noise, environmental changes, and incomplete information, with different tasks requiring varying levels of precision in different areas. For example, precise localization may be crucial near obstacles but less critical in open spaces. We present GUIDE (Generalized Uncertainty Integration for Decision-Making and Execution), a framework that integrates these task-specific requirements into navigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning acceptable uncertainty levels to different locations, TSUMs enable robots to adapt uncertainty management based on context. When combined with reinforcement learning, GUIDE learns policies that balance task completion and uncertainty management without extensive reward engineering. Real-world tests show significant performance gains over methods lacking task-specific uncertainty awareness.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Gating Ensemble Networks for AI-Generated Text Detection</title>
<link>https://arxiv.org/abs/2505.13855</link>
<guid>https://arxiv.org/abs/2505.13855</guid>
<content:encoded><![CDATA[
arXiv:2505.13855v1 Announce Type: cross 
Abstract: As state-of-the-art language models continue to improve, the need for robust detection of machine-generated text becomes increasingly critical. However, current state-of-the-art machine text detectors struggle to adapt to new unseen domains and generative models. In this paper we present DoGEN (Domain Gating Ensemble Networks), a technique that allows detectors to adapt to unseen domains by ensembling a set of domain expert detector models using weights from a domain classifier. We test DoGEN on a wide variety of domains from leading benchmarks and find that it achieves state-of-the-art performance on in-domain detection while outperforming models twice its size on out-of-domain detection. We release our code and trained models to assist in future research in domain-adaptive AI detection.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graphon Mixtures</title>
<link>https://arxiv.org/abs/2505.13864</link>
<guid>https://arxiv.org/abs/2505.13864</guid>
<content:encoded><![CDATA[
arXiv:2505.13864v1 Announce Type: cross 
Abstract: Social networks have a small number of large hubs, and a large number of small dense communities. We propose a generative model that captures both hub and dense structures. Based on recent results about graphons on line graphs, our model is a graphon mixture, enabling us to generate sequences of graphs where each graph is a combination of sparse and dense graphs. We propose a new condition on sparse graphs (the max-degree), which enables us to identify hubs. We show theoretically that we can estimate the normalized degree of the hubs, as well as estimate the graphon corresponding to sparse components of graph mixtures. We illustrate our approach on synthetic data, citation graphs, and social networks, showing the benefits of explicitly modeling sparse graphs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias Intrinsically from Regression Models in Recommender Systems</title>
<link>https://arxiv.org/abs/2505.13881</link>
<guid>https://arxiv.org/abs/2505.13881</guid>
<content:encoded><![CDATA[
arXiv:2505.13881v1 Announce Type: cross 
Abstract: Regression models are crucial in recommender systems. However, retransformation bias problem has been conspicuously neglected within the community. While many works in other fields have devised effective bias correction methods, all of them are post-hoc cures externally to the model, facing practical challenges when applied to real-world recommender systems. Hence, we propose a preemptive paradigm to eradicate the bias intrinsically from the models via minor model refinement. Specifically, a novel TranSUN method is proposed with a joint bias learning manner to offer theoretically guaranteed unbiasedness under empirical superior convergence. It is further generalized into a novel generic regression model family, termed Generalized TranSUN (GTS), which not only offers more theoretical insights but also serves as a generic framework for flexibly developing various bias-free models. Comprehensive experimental results demonstrate the superiority of our methods across data from various domains, which have been successfully deployed in two real-world industrial recommendation scenarios, i.e. product and short video recommendation scenarios in Guess What You Like business domain in the homepage of Taobao App (a leading e-commerce platform), to serve the major online traffic. Codes will be released after this paper is published.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certifiably Safe Manipulation of Deformable Linear Objects via Joint Shape and Tension Prediction</title>
<link>https://arxiv.org/abs/2505.13889</link>
<guid>https://arxiv.org/abs/2505.13889</guid>
<content:encoded><![CDATA[
arXiv:2505.13889v1 Announce Type: cross 
Abstract: Manipulating deformable linear objects (DLOs) is challenging due to their complex dynamics and the need for safe interaction in contact-rich environments. Most existing models focus on shape prediction alone and fail to account for contact and tension constraints, which can lead to damage to both the DLO and the robot. In this work, we propose a certifiably safe motion planning and control framework for DLO manipulation. At the core of our method is a predictive model that jointly estimates the DLO's future shape and tension. These predictions are integrated into a real-time trajectory optimizer based on polynomial zonotopes, allowing us to enforce safety constraints throughout the execution. We evaluate our framework on a simulated wire harness assembly task using a 7-DOF robotic arm. Compared to state-of-the-art methods, our approach achieves a higher task success rate while avoiding all safety violations. The results demonstrate that our method enables robust and safe DLO manipulation in contact-rich environments.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Asymptotic Equation Linking WAIC and WBIC in Singular Models</title>
<link>https://arxiv.org/abs/2505.13902</link>
<guid>https://arxiv.org/abs/2505.13902</guid>
<content:encoded><![CDATA[
arXiv:2505.13902v1 Announce Type: cross 
Abstract: In statistical learning, models are classified as regular or singular depending on whether the mapping from parameters to probability distributions is injective. Most models with hierarchical structures or latent variables are singular, for which conventional criteria such as the Akaike Information Criterion and the Bayesian Information Criterion are inapplicable due to the breakdown of normal approximations for the likelihood and posterior. To address this, the Widely Applicable Information Criterion (WAIC) and the Widely Applicable Bayesian Information Criterion (WBIC) have been proposed. Since WAIC and WBIC are computed using posterior distributions at different temperature settings, separate posterior sampling is generally required. In this paper, we theoretically derive an asymptotic equation that links WAIC and WBIC, despite their dependence on different posteriors. This equation yields an asymptotically unbiased expression of WAIC in terms of the posterior distribution used for WBIC. The result clarifies the structural relationship between these criteria within the framework of singular learning theory, and deepens understanding of their asymptotic behavior. This theoretical contribution provides a foundation for future developments in the computational efficiency of model selection in singular models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Agent Training for Computer Use</title>
<link>https://arxiv.org/abs/2505.13909</link>
<guid>https://arxiv.org/abs/2505.13909</guid>
<content:encoded><![CDATA[
arXiv:2505.13909v1 Announce Type: cross 
Abstract: Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated computer use trajectories, we further improved data quality by synthesizing diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched trajectories, our PC Agent-E model achieved a remarkable 141% relative improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld. Our findings suggest that strong computer use capabilities can be stimulated from a small amount of high-quality trajectory data.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.13925</link>
<guid>https://arxiv.org/abs/2505.13925</guid>
<content:encoded><![CDATA[
arXiv:2505.13925v1 Announce Type: cross 
Abstract: Symmetry is pervasive in robotics and has been widely exploited to improve sample efficiency in deep reinforcement learning (DRL). However, existing approaches primarily focus on spatial symmetries, such as reflection, rotation, and translation, while largely neglecting temporal symmetries. To address this gap, we explore time reversal symmetry, a form of temporal symmetry commonly found in robotics tasks such as door opening and closing. We propose Time Reversal symmetry enhanced Deep Reinforcement Learning (TR-DRL), a framework that combines trajectory reversal augmentation and time reversal guided reward shaping to efficiently solve temporally symmetric tasks. Our method generates reversed transitions from fully reversible transitions, identified by a proposed dynamics-consistent filter, to augment the training data. For partially reversible transitions, we apply reward shaping to guide learning, according to successful trajectories from the reversed task. Extensive experiments on the Robosuite and MetaWorld benchmarks demonstrate that TR-DRL is effective in both single-task and multi-task settings, achieving higher sample efficiency and stronger final performance compared to baseline methods.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLZero: A Multi-Agent System for End-to-end Machine Learning Automation</title>
<link>https://arxiv.org/abs/2505.13941</link>
<guid>https://arxiv.org/abs/2505.13941</guid>
<content:encoded><![CDATA[
arXiv:2505.13941v1 Announce Type: cross 
Abstract: Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly when handling multimodal data. We introduce MLZero, a novel multi-agent framework powered by Large Language Models (LLMs) that enables end-to-end ML automation across diverse data modalities with minimal human intervention. A cognitive perception module is first employed, transforming raw multimodal inputs into perceptual context that effectively guides the subsequent workflow. To address key limitations of LLMs, such as hallucinated code generation and outdated API knowledge, we enhance the iterative code generation process with semantic and episodic memory. MLZero demonstrates superior performance on MLE-Bench Lite, outperforming all competitors in both success rate and solution quality, securing six gold medals. Additionally, when evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more challenging tasks spanning diverse data modalities, MLZero outperforms the competing methods by a large margin with a success rate of 0.92 (+263.6\%) and an average rank of 2.28. Our approach maintains its robust effectiveness even with a compact 8B LLM, outperforming full-size systems from existing solutions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Probabilistic Perspective on Model Collapse</title>
<link>https://arxiv.org/abs/2505.13947</link>
<guid>https://arxiv.org/abs/2505.13947</guid>
<content:encoded><![CDATA[
arXiv:2505.13947v1 Announce Type: cross 
Abstract: In recent years, model collapse has become a critical issue in language model training, making it essential to understand the underlying mechanisms driving this phenomenon. In this paper, we investigate recursive parametric model training from a probabilistic perspective, aiming to characterize the conditions under which model collapse occurs and, crucially, how it can be mitigated. We conceptualize the recursive training process as a random walk of the model estimate, highlighting how the sample size influences the step size and how the estimation procedure determines the direction and potential bias of the random walk. Under mild conditions, we rigorously show that progressively increasing the sample size at each training step is necessary to prevent model collapse. In particular, when the estimation is unbiased, the required growth rate follows a superlinear pattern. This rate needs to be accelerated even further in the presence of substantial estimation bias. Building on this probabilistic framework, we also investigate the probability that recursive training on synthetic data yields models that outperform those trained solely on real data. Moreover, we extend these results to general parametric model family in an asymptotic regime. Finally, we validate our theoretical results through extensive simulations and a real-world dataset.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability</title>
<link>https://arxiv.org/abs/2505.13963</link>
<guid>https://arxiv.org/abs/2505.13963</guid>
<content:encoded><![CDATA[
arXiv:2505.13963v1 Announce Type: cross 
Abstract: Quantization methods are widely used to accelerate inference and streamline the deployment of large language models (LLMs). While prior research has extensively investigated the degradation of various LLM capabilities due to quantization, its effects on model explainability and interpretability, which are crucial for understanding decision-making processes, remain unexplored. To address this gap, we conduct comprehensive experiments using three common quantization techniques at distinct bit widths, in conjunction with two explainability methods, counterfactual examples and natural language explanations, as well as two interpretability approaches, knowledge memorization analysis and latent multi-hop reasoning analysis. We complement our analysis with a thorough user study, evaluating selected explainability methods. Our findings reveal that, depending on the configuration, quantization can significantly impact model explainability and interpretability. Notably, the direction of this effect is not consistent, as it strongly depends on (1) the quantization method, (2) the explainability or interpretability approach, and (3) the evaluation protocol. In some settings, human evaluation shows that quantization degrades explainability, while in others, it even leads to improvements. Our work serves as a cautionary tale, demonstrating that quantization can unpredictably affect model transparency. This insight has important implications for deploying LLMs in applications where transparency is a critical requirement.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Normalized Cut Problem with Constrained Action Space</title>
<link>https://arxiv.org/abs/2505.13986</link>
<guid>https://arxiv.org/abs/2505.13986</guid>
<content:encoded><![CDATA[
arXiv:2505.13986v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has emerged as an important paradigm to solve combinatorial optimization problems primarily due to its ability to learn heuristics that can generalize across problem instances. However, integrating external knowledge that will steer combinatorial optimization problem solutions towards domain appropriate outcomes remains an extremely challenging task. In this paper, we propose the first RL solution that uses constrained action spaces to guide the normalized cut problem towards pre-defined template instances. Using transportation networks as an example domain, we create a Wedge and Ring Transformer that results in graph partitions that are shaped in form of Wedges and Rings and which are likely to be closer to natural optimal partitions. However, our approach is general as it is based on principles that can be generalized to other domains.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThermoONet -- a deep learning-based small body thermophysical network: applications to modelling water activity of comets</title>
<link>https://arxiv.org/abs/2505.14016</link>
<guid>https://arxiv.org/abs/2505.14016</guid>
<content:encoded><![CDATA[
arXiv:2505.14016v1 Announce Type: cross 
Abstract: Cometary activity is a compelling subject of study, with thermophysical models playing a pivotal role in its understanding. However, traditional numerical solutions for small body thermophysical models are computationally intensive, posing challenges for investigations requiring high-resolution or repetitive modeling. To address this limitation, we employed a machine learning approach to develop ThermoONet - a neural network designed to predict the temperature and water ice sublimation flux of comets. Performance evaluations indicate that ThermoONet achieves a low average error in subsurface temperature of approximately 2% relative to the numerical simulation, while reducing computational time by nearly six orders of magnitude. We applied ThermoONet to model the water activity of comets 67P/Churyumov-Gerasimenko and 21P/Giacobini-Zinner. By successfully fitting the water production rate curves of these comets, as obtained by the Rosetta mission and the SOHO telescope, respectively, we demonstrate the network's effectiveness and efficiency. Furthermore, when combined with a global optimization algorithm, ThermoONet proves capable of retrieving the physical properties of target bodies.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning</title>
<link>https://arxiv.org/abs/2505.14020</link>
<guid>https://arxiv.org/abs/2505.14020</guid>
<content:encoded><![CDATA[
arXiv:2505.14020v1 Announce Type: cross 
Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs (KGs), incorporate the temporal feature to express the transience of knowledge by describing when facts occur. TKG extrapolation aims to infer possible future facts based on known history, which has garnered significant attention in recent years. Some existing methods treat TKG as a sequence of independent subgraphs to model temporal evolution patterns, demonstrating impressive reasoning performance. However, they still have limitations: 1) In modeling subgraph semantic evolution, they usually neglect the internal structural interactions between subgraphs, which are actually crucial for encoding TKGs. 2) They overlook the potential smooth features that do not lead to semantic changes, which should be distinguished from the semantic evolution process. Therefore, we propose a novel Disentangled Multi-span Evolutionary Network (DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution strategy that captures local neighbor features while perceiving historical neighbor semantic information, thus enabling internal interactions between subgraphs during the evolution process. To maximize the capture of semantic change patterns, we design a disentangle component that adaptively separates nodes' active and stable features, used to dynamically control the influence of historical semantics on future evolution. Extensive experiments conducted on four real-world TKG datasets show that DiMNet demonstrates substantial performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7% in MRR.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI</title>
<link>https://arxiv.org/abs/2505.14064</link>
<guid>https://arxiv.org/abs/2505.14064</guid>
<content:encoded><![CDATA[
arXiv:2505.14064v1 Announce Type: cross 
Abstract: In many real-world applications, deployed models encounter inputs that differ from the data seen during training. Out-of-distribution detection identifies whether an input stems from an unseen distribution, while open-world recognition flags such inputs to ensure the system remains robust as ever-emerging, previously $unknown$ categories appear and must be addressed without retraining. Foundation and vision-language models are pre-trained on large and diverse datasets with the expectation of broad generalization across domains, including medical imaging. However, benchmarking these models on test sets with only a few common outlier types silently collapses the evaluation back to a closed-set problem, masking failures on rare or truly novel conditions encountered in clinical use.
  We therefore present $NOVA$, a challenging, real-life $evaluation-only$ benchmark of $\sim$900 brain MRI scans that span 281 rare pathologies and heterogeneous acquisition protocols. Each case includes rich clinical narratives and double-blinded expert bounding-box annotations. Together, these enable joint assessment of anomaly localisation, visual captioning, and diagnostic reasoning. Because NOVA is never used for training, it serves as an $extreme$ stress-test of out-of-distribution generalisation: models must bridge a distribution gap both in sample appearance and in semantic space. Baseline results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B) reveal substantial performance drops across all tasks, establishing NOVA as a rigorous testbed for advancing models that can detect, localize, and reason about truly unknown anomalies.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Student Knowledge Modeling for Future Learning Resource Prediction</title>
<link>https://arxiv.org/abs/2505.14072</link>
<guid>https://arxiv.org/abs/2505.14072</guid>
<content:encoded><![CDATA[
arXiv:2505.14072v1 Announce Type: cross 
Abstract: Despite advances in deep learning for education, student knowledge tracing and behavior modeling face persistent challenges: limited personalization, inadequate modeling of diverse learning activities (especially non-assessed materials), and overlooking the interplay between knowledge acquisition and behavioral patterns. Practical limitations, such as fixed-size sequence segmentation, frequently lead to the loss of contextual information vital for personalized learning. Moreover, reliance on student performance on assessed materials limits the modeling scope, excluding non-assessed interactions like lectures. To overcome these shortcomings, we propose Knowledge Modeling and Material Prediction (KMaP), a stateful multi-task approach designed for personalized and simultaneous modeling of student knowledge and behavior. KMaP employs clustering-based student profiling to create personalized student representations, improving predictions of future learning resource preferences. Extensive experiments on two real-world datasets confirm significant behavioral differences across student clusters and validate the efficacy of the KMaP model.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized and Resilient Distributed Learning Through Opinion Dynamics</title>
<link>https://arxiv.org/abs/2505.14081</link>
<guid>https://arxiv.org/abs/2505.14081</guid>
<content:encoded><![CDATA[
arXiv:2505.14081v1 Announce Type: cross 
Abstract: In this paper, we address two practical challenges of distributed learning in multi-agent network systems, namely personalization and resilience. Personalization is the need of heterogeneous agents to learn local models tailored to their own data and tasks, while still generalizing well; on the other hand, the learning process must be resilient to cyberattacks or anomalous training data to avoid disruption. Motivated by a conceptual affinity between these two requirements, we devise a distributed learning algorithm that combines distributed gradient descent and the Friedkin-Johnsen model of opinion dynamics to fulfill both of them. We quantify its convergence speed and the neighborhood that contains the final learned models, which can be easily controlled by tuning the algorithm parameters to enforce a more personalized/resilient behavior. We numerically showcase the effectiveness of our algorithm on synthetic and real-world distributed learning tasks, where it achieves high global accuracy both for personalized models and with malicious agents compared to standard strategies.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Efficiency under Covariate Shift in Kernel Ridge Regression</title>
<link>https://arxiv.org/abs/2505.14083</link>
<guid>https://arxiv.org/abs/2505.14083</guid>
<content:encoded><![CDATA[
arXiv:2505.14083v1 Announce Type: cross 
Abstract: This paper addresses the covariate shift problem in the context of nonparametric regression within reproducing kernel Hilbert spaces (RKHSs). Covariate shift arises in supervised learning when the input distributions of the training and test data differ, presenting additional challenges for learning. Although kernel methods have optimal statistical properties, their high computational demands in terms of time and, particularly, memory, limit their scalability to large datasets. To address this limitation, the main focus of this paper is to explore the trade-off between computational efficiency and statistical accuracy under covariate shift. We investigate the use of random projections where the hypothesis space consists of a random subspace within a given RKHS. Our results show that, even in the presence of covariate shift, significant computational savings can be achieved without compromising learning performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-dimensional Nonparametric Contextual Bandit Problem</title>
<link>https://arxiv.org/abs/2505.14102</link>
<guid>https://arxiv.org/abs/2505.14102</guid>
<content:encoded><![CDATA[
arXiv:2505.14102v1 Announce Type: cross 
Abstract: We consider the kernelized contextual bandit problem with a large feature space. This problem involves $K$ arms, and the goal of the forecaster is to maximize the cumulative rewards through learning the relationship between the contexts and the rewards. It serves as a general framework for various decision-making scenarios, such as personalized online advertising and recommendation systems. Kernelized contextual bandits generalize the linear contextual bandit problem and offers a greater modeling flexibility. Existing methods, when applied to Gaussian kernels, yield a trivial bound of $O(T)$ when we consider $\Omega(\log T)$ feature dimensions. To address this, we introduce stochastic assumptions on the context distribution and show that no-regret learning is achievable even when the number of dimensions grows up to the number of samples. Furthermore, we analyze lenient regret, which allows a per-round regret of at most $\Delta > 0$. We derive the rate of lenient regret in terms of $\Delta$.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models</title>
<link>https://arxiv.org/abs/2505.14103</link>
<guid>https://arxiv.org/abs/2505.14103</guid>
<content:encoded><![CDATA[
arXiv:2505.14103v1 Announce Type: cross 
Abstract: Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website https://audiojailbreak.github.io/AudioJailbreak.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition</title>
<link>https://arxiv.org/abs/2505.14113</link>
<guid>https://arxiv.org/abs/2505.14113</guid>
<content:encoded><![CDATA[
arXiv:2505.14113v1 Announce Type: cross 
Abstract: Most machine learning-based image segmentation models produce pixel-wise confidence scores - typically derived from softmax outputs - that represent the model's predicted probability for each class label at every pixel. While this information can be particularly valuable in high-stakes domains such as medical imaging, these (uncalibrated) scores are heuristic in nature and do not constitute rigorous quantitative uncertainty estimates. Conformal prediction (CP) provides a principled framework for transforming heuristic confidence scores into statistically valid uncertainty estimates. However, applying CP directly to image segmentation ignores the spatial correlations between pixels, a fundamental characteristic of image data. This can result in overly conservative and less interpretable uncertainty estimates. To address this, we propose CONSIGN (Conformal Segmentation Informed by Spatial Groupings via Decomposition), a CP-based method that incorporates spatial correlations to improve uncertainty quantification in image segmentation. Our method generates meaningful prediction sets that come with user-specified, high-probability error guarantees. It is compatible with any pre-trained segmentation model capable of generating multiple sample outputs - such as those using dropout, Bayesian modeling, or ensembles. We evaluate CONSIGN against a standard pixel-wise CP approach across three medical imaging datasets and two COCO dataset subsets, using three different pre-trained segmentation models. Results demonstrate that accounting for spatial structure significantly improves performance across multiple metrics and enhances the quality of uncertainty estimates.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Alignment of Time Sensitive Facts with Activation Engineering</title>
<link>https://arxiv.org/abs/2505.14158</link>
<guid>https://arxiv.org/abs/2505.14158</guid>
<content:encoded><![CDATA[
arXiv:2505.14158v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are trained on diverse and often conflicting knowledge spanning multiple domains and time periods. Some of this knowledge is only valid within specific temporal contexts, such as answering the question, "Who is the President of the United States in 2022?" Ensuring LLMs generate time appropriate responses is crucial for maintaining relevance and accuracy. In this work we explore activation engineering as a method for temporally aligning LLMs to improve factual recall without any training or dataset creation. In this research we explore an activation engineering technique to ground three versions of LLaMA 2 to specific points in time and examine the effects of varying injection layers and prompting strategies. Our experiments demonstrate up to a 44% and 16% improvement in relative and explicit prompting respectively, achieving comparable performance to the fine-tuning method proposed by Zhao et al. (2024) . Notably, our approach achieves similar results to the fine-tuning baseline while being significantly more computationally efficient and requiring no pre-aligned datasets.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models</title>
<link>https://arxiv.org/abs/2505.14160</link>
<guid>https://arxiv.org/abs/2505.14160</guid>
<content:encoded><![CDATA[
arXiv:2505.14160v1 Announce Type: cross 
Abstract: Multilingual vision-language models promise universal image-text retrieval, yet their social biases remain under-explored. We present the first systematic audit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and CAPIVARA-CLIP -- across ten languages that vary in resource availability and grammatical gender. Using balanced subsets of \textsc{FairFace} and the \textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and gender bias and measure stereotype amplification. Contrary to the assumption that multilinguality mitigates bias, every model exhibits stronger gender bias than its English-only baseline. CAPIVARA-CLIP shows its largest biases precisely in the low-resource languages it targets, while the shared cross-lingual encoder of NLLB-CLIP transports English gender stereotypes into gender-neutral languages; loosely coupled encoders largely avoid this transfer. Highly gendered languages consistently magnify all measured bias types, but even gender-neutral languages remain vulnerable when cross-lingual weight sharing imports foreign stereotypes. Aggregated metrics conceal language-specific ``hot spots,'' underscoring the need for fine-grained, language-aware bias evaluation in future multilingual vision-language research.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Bernstein Normalizing Flows for Flexible Multivariate Density Regression with Interpretable Marginals</title>
<link>https://arxiv.org/abs/2505.14164</link>
<guid>https://arxiv.org/abs/2505.14164</guid>
<content:encoded><![CDATA[
arXiv:2505.14164v1 Announce Type: cross 
Abstract: Density regression models allow a comprehensive understanding of data by modeling the complete conditional probability distribution. While flexible estimation approaches such as normalizing flows (NF) work particularly well in multiple dimensions, interpreting the input-output relationship of such models is often difficult, due to the black-box character of deep learning models. In contrast, existing statistical methods for multivariate outcomes such as multivariate conditional transformation models (MCTM) are restricted in flexibility and are often not expressive enough to represent complex multivariate probability distributions. In this paper, we combine MCTM with state-of-the-art and autoregressive NF to leverage the transparency of MCTM for modeling interpretable feature effects on the marginal distributions in the first step and the flexibility of neural-network-based NF techniques to account for complex and non-linear relationships in the joint data distribution. We demonstrate our method's versatility in various numerical experiments and compare it with MCTM and other NF models on both simulated and real-world data.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore</title>
<link>https://arxiv.org/abs/2505.14165</link>
<guid>https://arxiv.org/abs/2505.14165</guid>
<content:encoded><![CDATA[
arXiv:2505.14165v1 Announce Type: cross 
Abstract: Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity toward specific aspects within a text, enabling more precise opinion mining in domains such as product reviews and social media. However, traditional FGSA approaches often require task-specific architectures and extensive annotated data, limiting their generalization and scalability. To address these challenges, we propose PL-FGSA, a unified prompt learning-based framework implemented using the MindSpore platform, which integrates prompt design with a lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task prompt-augmented generation problem, jointly tackling aspect extraction, sentiment classification, and causal explanation in a unified paradigm. By leveraging prompt-based guidance, PL-FGSA enhances interpretability and achieves strong performance under both full-data and low-resource conditions. Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and MAMS-demonstrate that our model consistently outperforms traditional fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597, respectively. These results validate the effectiveness of prompt-based generalization and highlight the practical value of PL-FGSA for real-world sentiment analysis tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.14174</link>
<guid>https://arxiv.org/abs/2505.14174</guid>
<content:encoded><![CDATA[
arXiv:2505.14174v1 Announce Type: cross 
Abstract: LLMs are effective at code generation tasks like text-to-SQL, but is it worth the cost? Many state-of-the-art approaches use non-task-specific LLM techniques including Chain-of-Thought (CoT), self-consistency, and fine-tuning. These methods can be costly at inference time, sometimes requiring over a hundred LLM calls with reasoning, incurring average costs of up to \$0.46 per query, while fine-tuning models can cost thousands of dollars. We introduce "N-rep" consistency, a more cost-efficient text-to-SQL approach that achieves similar BIRD benchmark scores as other more expensive methods, at only \$0.039 per query. N-rep leverages multiple representations of the same schema input to mitigate weaknesses in any single representation, making the solution more robust and allowing the use of smaller and cheaper models without any reasoning or fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL approach in its cost range.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling</title>
<link>https://arxiv.org/abs/2505.14177</link>
<guid>https://arxiv.org/abs/2505.14177</guid>
<content:encoded><![CDATA[
arXiv:2505.14177v1 Announce Type: cross 
Abstract: We consider the problem of sampling distributions stemming from non-convex potentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of the discrete-time ULA to drift approximations under the assumption that the potential is strongly convex at infinity. In many context, e.g. imaging inverse problems, potentials are non-convex and non-smooth. Proximal Stochastic Gradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such potentials. It combines the forward-backward optimization algorithm with a ULA step. Our main stability result combined with properties of the Moreau envelope allows us to derive the first proof of convergence of the PSGLA for non-convex potentials. We empirically validate our methodology on synthetic data and in the context of imaging inverse problems. In particular, we observe that PSGLA exhibits faster convergence rates than Stochastic Gradient Langevin Algorithm for posterior sampling while preserving its restoration properties.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QSVM-QNN: Quantum Support Vector Machine Based Quantum Neural Network Learning Algorithm for Brain-Computer Interfacing Systems</title>
<link>https://arxiv.org/abs/2505.14192</link>
<guid>https://arxiv.org/abs/2505.14192</guid>
<content:encoded><![CDATA[
arXiv:2505.14192v1 Announce Type: cross 
Abstract: A brain-computer interface (BCI) system enables direct communication between the brain and external devices, offering significant potential for assistive technologies and advanced human-computer interaction. Despite progress, BCI systems face persistent challenges, including signal variability, classification inefficiency, and difficulty adapting to individual users in real time. In this study, we propose a novel hybrid quantum learning model, termed QSVM-QNN, which integrates a Quantum Support Vector Machine (QSVM) with a Quantum Neural Network (QNN), to improve classification accuracy and robustness in EEG-based BCI tasks. Unlike existing models, QSVM-QNN combines the decision boundary capabilities of QSVM with the expressive learning power of QNN, leading to superior generalization performance. The proposed model is evaluated on two benchmark EEG datasets, achieving high accuracies of 0.990 and 0.950, outperforming both classical and standalone quantum models. To demonstrate real-world viability, we further validated the robustness of QNN, QSVM, and QSVM-QNN against six realistic quantum noise models, including bit flip and phase damping. These experiments reveal that QSVM-QNN maintains stable performance under noisy conditions, establishing its applicability for deployment in practical, noisy quantum environments. Beyond BCI, the proposed hybrid quantum architecture is generalizable to other biomedical and time-series classification tasks, offering a scalable and noise-resilient solution for next-generation neurotechnological systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Fine-tuning for In-context Learning</title>
<link>https://arxiv.org/abs/2505.14233</link>
<guid>https://arxiv.org/abs/2505.14233</guid>
<content:encoded><![CDATA[
arXiv:2505.14233v1 Announce Type: cross 
Abstract: In-context Learning (ICL) utilizes structured demonstration-query inputs to induce few-shot learning on Language Models (LMs), which are not originally pre-trained on ICL-style data. To bridge the gap between ICL and pre-training, some approaches fine-tune LMs on large ICL-style datasets by an end-to-end paradigm with massive computational costs. To reduce such costs, in this paper, we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous findings on the inner mechanism of ICL, building training objectives on the attention scores instead of the final outputs, to force the attention scores to focus on the correct label tokens presented in the context and mitigate attention scores from the wrong label tokens. Our experiments on 9 modern LMs and 8 datasets empirically find that ABFT outperforms in performance, robustness, unbiasedness, and efficiency, with only around 0.01% data cost compared to the previous methods. Moreover, our subsequent analysis finds that the end-to-end training objective contains the ABFT objective, suggesting the implicit bias of ICL-style data to the emergence of induction heads. Our work demonstrates the possibility of controlling specific module sequences within LMs to improve their behavior, opening up the future application of mechanistic interpretability.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14238</link>
<guid>https://arxiv.org/abs/2505.14238</guid>
<content:encoded><![CDATA[
arXiv:2505.14238v1 Announce Type: cross 
Abstract: Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trained weights fixed. The prevailing approach, LoRA, models updates using a low-rank decomposition, but its expressivity is inherently constrained by the rank. Recent methods like HiRA aim to increase expressivity by incorporating a Hadamard product with the frozen weights, but still rely on the structure of the pre-trained model. We introduce ABBA, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget. We formally analyze ABBA's expressive capacity and validate its advantages through matrix reconstruction experiments. Empirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models. Our code is publicly available at: https://github.com/CERT-Lab/abba.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technical Report on classification of literature related to children speech disorder</title>
<link>https://arxiv.org/abs/2505.14242</link>
<guid>https://arxiv.org/abs/2505.14242</guid>
<content:encoded><![CDATA[
arXiv:2505.14242v1 Announce Type: cross 
Abstract: This technical report presents a natural language processing (NLP)-based approach for systematically classifying scientific literature on childhood speech disorders. We retrieved and filtered 4,804 relevant articles published after 2015 from the PubMed database using domain-specific keywords. After cleaning and pre-processing the abstracts, we applied two topic modeling techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify latent thematic structures in the corpus. Our models uncovered 14 clinically meaningful clusters, such as infantile hyperactivity and abnormal epileptic behavior. To improve relevance and precision, we incorporated a custom stop word list tailored to speech pathology. Evaluation results showed that the LDA model achieved a coherence score of 0.42 and a perplexity of -7.5, indicating strong topic coherence and predictive performance. The BERTopic model exhibited a low proportion of outlier topics (less than 20%), demonstrating its capacity to classify heterogeneous literature effectively. These results provide a foundation for automating literature reviews in speech-language pathology.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path-integral molecular dynamics with actively-trained and universal machine learning force fields</title>
<link>https://arxiv.org/abs/2505.14245</link>
<guid>https://arxiv.org/abs/2505.14245</guid>
<content:encoded><![CDATA[
arXiv:2505.14245v1 Announce Type: cross 
Abstract: Accounting for nuclear quantum effects (NQEs) can significantly alter material properties at finite temperatures. Atomic modeling using the path-integral molecular dynamics (PIMD) method can fully account for such effects, but requires computationally efficient and accurate models of interatomic interactions. Empirical potentials are fast but may lack sufficient accuracy, whereas quantum-mechanical calculations are highly accurate but computationally expensive. Machine-learned interatomic potentials offer a solution to this challenge, providing near-quantum-mechanical accuracy while maintaining high computational efficiency compared to density functional theory (DFT) calculations. In this context, an interface was developed to integrate moment tensor potentials (MTPs) from the MLIP-2 software package into PIMD calculations using the i-PI software package. This interface was then applied to active learning of potentials and to investigate the influence of NQEs on material properties, namely the temperature dependence of lattice parameters and thermal expansion coefficients, as well as radial distribution functions, for lithium hydride (LiH) and silicon (Si) systems. The results were compared with experimental data, quasi-harmonic approximation calculations, and predictions from the universal machine learning force field MatterSim. These comparisons demonstrated the high accuracy and effectiveness of the MTP-PIMD approach.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors</title>
<link>https://arxiv.org/abs/2505.14300</link>
<guid>https://arxiv.org/abs/2505.14300</guid>
<content:encoded><![CDATA[
arXiv:2505.14300v1 Announce Type: cross 
Abstract: High-risk industries like nuclear and aviation use real-time monitoring to detect dangerous system conditions. Similarly, Large Language Models (LLMs) need monitoring safeguards. We propose a real-time framework to predict harmful AI outputs before they occur by using an unsupervised approach that treats normal behavior as the baseline and harmful outputs as outliers. Our study focuses specifically on backdoor-triggered responses -- where specific input phrases activate hidden vulnerabilities causing the model to generate unsafe content like violence, pornography, or hate speech. We address two key challenges: (1) identifying true causal indicators rather than surface correlations, and (2) preventing advanced models from deception -- deliberately evading monitoring systems. Hence, we approach this problem from an unsupervised lens by drawing parallels to human deception: just as humans exhibit physical indicators while lying, we investigate whether LLMs display distinct internal behavioral signatures when generating harmful content. Our study addresses two critical challenges: 1) designing monitoring systems that capture true causal indicators rather than superficial correlations; and 2)preventing intentional evasion by increasingly capable "Future models''. Our findings show that models can produce harmful content through causal mechanisms and can become deceptive by: (a) alternating between linear and non-linear representations, and (b) modifying feature relationships. To counter this, we developed Safety-Net -- a multi-detector framework that monitors different representation dimensions, successfully detecting harmful behavior even when information is shifted across representational spaces to evade individual monitors. Our evaluation shows 96% accuracy in detecting harmful cases using our unsupervised ensemble approach.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Binary and Ternary Neural Network Inference on RRAM Crossbars using CIM-Explorer</title>
<link>https://arxiv.org/abs/2505.14303</link>
<guid>https://arxiv.org/abs/2505.14303</guid>
<content:encoded><![CDATA[
arXiv:2505.14303v1 Announce Type: cross 
Abstract: Using Resistive Random Access Memory (RRAM) crossbars in Computing-in-Memory (CIM) architectures offers a promising solution to overcome the von Neumann bottleneck. Due to non-idealities like cell variability, RRAM crossbars are often operated in binary mode, utilizing only two states: Low Resistive State (LRS) and High Resistive State (HRS). Binary Neural Networks (BNNs) and Ternary Neural Networks (TNNs) are well-suited for this hardware due to their efficient mapping. Existing software projects for RRAM-based CIM typically focus on only one aspect: compilation, simulation, or Design Space Exploration (DSE). Moreover, they often rely on classical 8 bit quantization. To address these limitations, we introduce CIM-Explorer, a modular toolkit for optimizing BNN and TNN inference on RRAM crossbars. CIM-Explorer includes an end-to-end compiler stack, multiple mapping options, and simulators, enabling a DSE flow for accuracy estimation across different crossbar parameters and mappings. CIM-Explorer can accompany the entire design process, from early accuracy estimation for specific crossbar parameters, to selecting an appropriate mapping, and compiling BNNs and TNNs for a finalized crossbar chip. In DSE case studies, we demonstrate the expected accuracy for various mappings and crossbar parameters. CIM-Explorer can be found on GitHub.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Recommendation Bias with Causal Intervention on Evolving Personal Popularity</title>
<link>https://arxiv.org/abs/2505.14310</link>
<guid>https://arxiv.org/abs/2505.14310</guid>
<content:encoded><![CDATA[
arXiv:2505.14310v1 Announce Type: cross 
Abstract: Popularity bias occurs when popular items are recommended far more frequently than they should be, negatively impacting both user experience and recommendation accuracy. Existing debiasing methods mitigate popularity bias often uniformly across all users and only partially consider the time evolution of users or items. However, users have different levels of preference for item popularity, and this preference is evolving over time. To address these issues, we propose a novel method called CausalEPP (Causal Intervention on Evolving Personal Popularity) for taming recommendation bias, which accounts for the evolving personal popularity of users. Specifically, we first introduce a metric called {Evolving Personal Popularity} to quantify each user's preference for popular items. Then, we design a causal graph that integrates evolving personal popularity into the conformity effect, and apply deconfounded training to mitigate the popularity bias of the causal graph. During inference, we consider the evolution consistency between users and items to achieve a better recommendation. Empirical studies demonstrate that CausalEPP outperforms baseline methods in reducing popularity bias while improving recommendation accuracy.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators</title>
<link>https://arxiv.org/abs/2505.14314</link>
<guid>https://arxiv.org/abs/2505.14314</guid>
<content:encoded><![CDATA[
arXiv:2505.14314v1 Announce Type: cross 
Abstract: Attention mechanisms, particularly within Transformer architectures and large language models (LLMs), have revolutionized sequence modeling in machine learning and artificial intelligence applications. To compute attention for increasingly long sequences, specialized accelerators have been proposed to execute key attention steps directly in hardware. Among the various recently proposed architectures, those based on variants of the FlashAttention algorithm, originally designed for GPUs, stand out due to their optimized computation, tiling capabilities, and reduced memory traffic. In this work, we focus on optimizing the kernel of floating-point-based FlashAttention using new hardware operators that fuse the computation of exponentials and vector multiplications, e.g., e^x, V. The proposed ExpMul hardware operators significantly reduce the area and power costs of FlashAttention-based hardware accelerators. When implemented in a 28nm ASIC technology, they achieve improvements of 28.8% in area and 17.6% in power, on average, compared to state-of-the-art hardware architectures with separate exponentials and vector multiplications hardware operators.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime</title>
<link>https://arxiv.org/abs/2505.14323</link>
<guid>https://arxiv.org/abs/2505.14323</guid>
<content:encoded><![CDATA[
arXiv:2505.14323v1 Announce Type: cross 
Abstract: Training data reconstruction attacks enable adversaries to recover portions of a released model's training data. We consider the attacks where a reconstructor neural network learns to invert the (random) mapping between training data and model weights. Prior work has shown that an informed adversary with access to released model's weights and all but one training data point can achieve high-quality reconstructions in this way. However, differential privacy can defend against such an attack with little to no loss in model's utility when the amount of training data is sufficiently large. In this work we consider a more realistic adversary who only knows the distribution from which a small training dataset has been sampled and who attacks a transfer-learned neural network classifier that has been trained on this dataset. We exhibit an attack that works in this realistic threat model and demonstrate that in the small-data regime it cannot be defended against by DP-SGD without severely damaging the classifier accuracy. This raises significant concerns about the use of such transfer-learned classifiers when protection of training-data is paramount. We demonstrate the effectiveness and robustness of our attack on VGG, EfficientNet and ResNet image classifiers transfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, we point out that the commonly used (true-positive) reconstruction success rate metric fails to reliably quantify the actual reconstruction effectiveness. Instead, we make use of the Neyman-Pearson lemma to construct the receiver operating characteristic curve and consider the associated true-positive reconstruction rate at a fixed level of the false-positive reconstruction rate.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handloom Design Generation Using Generative Networks</title>
<link>https://arxiv.org/abs/2505.14330</link>
<guid>https://arxiv.org/abs/2505.14330</guid>
<content:encoded><![CDATA[
arXiv:2505.14330v1 Announce Type: cross 
Abstract: This paper proposes deep learning techniques of generating designs for clothing, focused on handloom fabric and discusses the associated challenges along with its application. The capability of generative neural network models in understanding artistic designs and synthesizing those is not yet explored well. In this work, multiple methods are employed incorporating the current state of the art generative models and style transfer algorithms to study and observe their performance for the task. The results are then evaluated through user score. This work also provides a new dataset NeuralLoom for the task of the design generation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey</title>
<link>https://arxiv.org/abs/2505.14340</link>
<guid>https://arxiv.org/abs/2505.14340</guid>
<content:encoded><![CDATA[
arXiv:2505.14340v1 Announce Type: cross 
Abstract: Plane geometry problem solving (PGPS) has recently gained significant attention as a benchmark to assess the multi-modal reasoning capabilities of large vision-language models. Despite the growing interest in PGPS, the research community still lacks a comprehensive overview that systematically synthesizes recent work in PGPS. To fill this gap, we present a survey of existing PGPS studies. We first categorize PGPS methods into an encoder-decoder framework and summarize the corresponding output formats used by their encoders and decoders. Subsequently, we classify and analyze these encoders and decoders according to their architectural designs. Finally, we outline major challenges and promising directions for future research. In particular, we discuss the hallucination issues arising during the encoding phase within encoder-decoder architectures, as well as the problem of data leakage in current PGPS benchmarks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications</title>
<link>https://arxiv.org/abs/2505.14354</link>
<guid>https://arxiv.org/abs/2505.14354</guid>
<content:encoded><![CDATA[
arXiv:2505.14354v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved impressive results across a broad array of tasks, yet their capacity for complex, domain-specific mathematical reasoning-particularly in wireless communications-remains underexplored. In this work, we introduce WirelessMathBench, a novel benchmark specifically designed to evaluate LLMs on mathematical modeling challenges to wireless communications engineering. Our benchmark consists of 587 meticulously curated questions sourced from 40 state-of-the-art research papers, encompassing a diverse spectrum of tasks ranging from basic multiple-choice questions to complex equation completion tasks, including both partial and full completions, all of which rigorously adhere to physical and dimensional constraints. Through extensive experimentation with leading LLMs, we observe that while many models excel in basic recall tasks, their performance degrades significantly when reconstructing partially or fully obscured equations, exposing fundamental limitations in current LLMs. Even DeepSeek-R1, the best performer on our benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83% success rate in full equation completion. By publicly releasing WirelessMathBench along with the evaluation toolkit, we aim to advance the development of more robust, domain-aware LLMs for wireless system analysis and broader engineering applications.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vid2World: Crafting Video Diffusion Models to Interactive World Models</title>
<link>https://arxiv.org/abs/2505.14357</link>
<guid>https://arxiv.org/abs/2505.14357</guid>
<content:encoded><![CDATA[
arXiv:2505.14357v1 Announce Type: cross 
Abstract: World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse predictions, limiting their applicability in complex environments. In contrast, video diffusion models trained on large, internet-scale datasets have demonstrated impressive capabilities in generating high-quality videos that capture diverse real-world dynamics. In this work, we present Vid2World, a general approach for leveraging and transferring pre-trained video diffusion models into interactive world models. To bridge the gap, Vid2World performs casualization of a pre-trained video diffusion model by crafting its architecture and training objective to enable autoregressive generation. Furthermore, it introduces a causal action guidance mechanism to enhance action controllability in the resulting interactive world model. Extensive experiments in robot manipulation and game simulation domains show that our method offers a scalable and effective approach for repurposing highly capable video diffusion models to interactive world models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds</title>
<link>https://arxiv.org/abs/2505.14396</link>
<guid>https://arxiv.org/abs/2505.14396</guid>
<content:encoded><![CDATA[
arXiv:2505.14396v1 Announce Type: cross 
Abstract: Causal world models are systems that can answer counterfactual questions about an environment of interest, i.e. predict how it would have evolved if an arbitrary subset of events had been realized differently. It requires understanding the underlying causes behind chains of events and conducting causal inference for arbitrary unseen distributions. So far, this task eludes foundation models, notably large language models (LLMs), which do not have demonstrated causal reasoning capabilities beyond the memorization of existing causal relationships. Furthermore, evaluating counterfactuals in real-world applications is challenging since only the factual world is observed, limiting evaluation to synthetic datasets. We address these problems by explicitly extracting and modeling causal relationships and propose the Causal Cartographer framework. First, we introduce a graph retrieval-augmented generation agent tasked to retrieve causal relationships from data. This approach allows us to construct a large network of real-world causal relationships that can serve as a repository of causal knowledge and build real-world counterfactuals. In addition, we create a counterfactual reasoning agent constrained by causal relationships to perform reliable step-by-step causal inference. We show that our approach can extract causal knowledge and improve the robustness of LLMs for causal reasoning tasks while reducing inference costs and spurious correlations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation</title>
<link>https://arxiv.org/abs/2505.14398</link>
<guid>https://arxiv.org/abs/2505.14398</guid>
<content:encoded><![CDATA[
arXiv:2505.14398v1 Announce Type: cross 
Abstract: While humans naturally learn and adapt from past experiences, large language models (LLMs) and their agentic counterparts struggle to retain reasoning from previous tasks and apply them in future contexts. To address this limitation, we propose a novel framework, log-augmented generation (LAG) that directly reuses prior computation and reasoning from past logs at test time to enhance model's ability to learn from previous tasks and perform better on new, unseen challenges, all while keeping the system efficient and scalable. Specifically, our system represents task logs using key-value (KV) caches, encoding the full reasoning context of prior tasks while storing KV caches for only a selected subset of tokens. When a new task arises, LAG retrieves the KV values from relevant logs to augment generation. Our approach differs from reflection-based memory mechanisms by directly reusing prior reasoning and computations without requiring additional steps for knowledge extraction or distillation. Our method also goes beyond existing KV caching techniques, which primarily target efficiency gains rather than improving accuracy. Experiments on knowledge- and reasoning-intensive datasets demonstrate that our method significantly outperforms standard agentic systems that do not utilize logs, as well as existing solutions based on reflection and KV cache techniques.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean Frameworks</title>
<link>https://arxiv.org/abs/2505.14417</link>
<guid>https://arxiv.org/abs/2505.14417</guid>
<content:encoded><![CDATA[
arXiv:2505.14417v1 Announce Type: cross 
Abstract: In the era of foundation models and Large Language Models (LLMs), Euclidean space is the de facto geometric setting of our machine learning architectures. However, recent literature has demonstrated that this choice comes with fundamental limitations. To that end, non-Euclidean learning is quickly gaining traction, particularly in web-related applications where complex relationships and structures are prevalent. Non-Euclidean spaces, such as hyperbolic, spherical, and mixed-curvature spaces, have been shown to provide more efficient and effective representations for data with intrinsic geometric properties, including web-related data like social network topology, query-document relationships, and user-item interactions. Integrating foundation models with non-Euclidean geometries has great potential to enhance their ability to capture and model the underlying structures, leading to better performance in search, recommendations, and content understanding. This workshop focuses on the intersection of Non-Euclidean Foundation Models and Geometric Learning (NEGEL), exploring its potential benefits, including the potential benefits for advancing web-related technologies, challenges, and future directions. Workshop page: [https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection</title>
<link>https://arxiv.org/abs/2505.14420</link>
<guid>https://arxiv.org/abs/2505.14420</guid>
<content:encoded><![CDATA[
arXiv:2505.14420v1 Announce Type: cross 
Abstract: Predicting earnings surprises through the analysis of earnings conference call transcripts has attracted increasing attention from the financial research community. Conference calls serve as critical communication channels between company executives, analysts, and shareholders, offering valuable forward-looking information. However, these transcripts present significant analytical challenges, typically containing over 5,000 words with substantial redundancy and industry-specific terminology that creates obstacles for language models. In this work, we propose the Sparse Autoencoder for Financial Representation Enhancement (SAE-FiRE) framework to address these limitations by extracting key information while eliminating redundancy. SAE-FiRE employs Sparse Autoencoders (SAEs) to efficiently identify patterns and filter out noises, and focusing specifically on capturing nuanced financial signals that have predictive power for earnings surprises. Experimental results indicate that the proposed method can significantly outperform comparing baselines.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A system identification approach to clustering vector autoregressive time series</title>
<link>https://arxiv.org/abs/2505.14421</link>
<guid>https://arxiv.org/abs/2505.14421</guid>
<content:encoded><![CDATA[
arXiv:2505.14421v1 Announce Type: cross 
Abstract: Clustering of time series based on their underlying dynamics is keeping attracting researchers due to its impacts on assisting complex system modelling. Most current time series clustering methods handle only scalar time series, treat them as white noise, or rely on domain knowledge for high-quality feature construction, where the autocorrelation pattern/feature is mostly ignored. Instead of relying on heuristic feature/metric construction, the system identification approach allows treating vector time series clustering by explicitly considering their underlying autoregressive dynamics. We first derive a clustering algorithm based on a mixture autoregressive model. Unfortunately it turns out to have significant computational problems. We then derive a `small-noise' limiting version of the algorithm, which we call k-LMVAR (Limiting Mixture Vector AutoRegression), that is computationally manageable. We develop an associated BIC criterion for choosing the number of clusters and model order. The algorithm performs very well in comparative simulations and also scales well computationally.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowTSE: Target Speaker Extraction with Flow Matching</title>
<link>https://arxiv.org/abs/2505.14465</link>
<guid>https://arxiv.org/abs/2505.14465</guid>
<content:encoded><![CDATA[
arXiv:2505.14465v1 Announce Type: cross 
Abstract: Target speaker extraction (TSE) aims to isolate a specific speaker's speech from a mixture using speaker enrollment as a reference. While most existing approaches are discriminative, recent generative methods for TSE achieve strong results. However, generative methods for TSE remain underexplored, with most existing approaches relying on complex pipelines and pretrained components, leading to computational overhead. In this work, we present FlowTSE, a simple yet effective TSE approach based on conditional flow matching. Our model receives an enrollment audio sample and a mixed speech signal, both represented as mel-spectrograms, with the objective of extracting the target speaker's clean speech. Furthermore, for tasks where phase reconstruction is crucial, we propose a novel vocoder conditioned on the complex STFT of the mixed signal, enabling improved phase estimation. Experimental results on standard TSE benchmarks show that FlowTSE matches or outperforms strong baselines.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAST: Phonetic-Acoustic Speech Tokenizer</title>
<link>https://arxiv.org/abs/2505.14470</link>
<guid>https://arxiv.org/abs/2505.14470</guid>
<content:encoded><![CDATA[
arXiv:2505.14470v1 Announce Type: cross 
Abstract: We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Interpretability of Sparse Latent Representations with Class Information</title>
<link>https://arxiv.org/abs/2505.14476</link>
<guid>https://arxiv.org/abs/2505.14476</guid>
<content:encoded><![CDATA[
arXiv:2505.14476v1 Announce Type: cross 
Abstract: Variational Autoencoders (VAEs) are powerful generative models for learning latent representations. Standard VAEs generate dispersed and unstructured latent spaces by utilizing all dimensions, which limits their interpretability, especially in high-dimensional spaces. To address this challenge, Variational Sparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting in sparse latent representations for each input. These sparse representations, characterized by a limited number of active dimensions, are inherently more interpretable. Despite this advantage, VSC falls short in providing structured interpretations across samples within the same class. Intuitively, samples from the same class are expected to share similar attributes while allowing for variations in those attributes. This expectation should manifest as consistent patterns of active dimensions in their latent representations, but VSC does not enforce such consistency.
  In this paper, we propose a novel approach to enhance the latent space interpretability by ensuring that the active dimensions in the latent space are consistent across samples within the same class. To achieve this, we introduce a new loss function that encourages samples from the same class to share similar active dimensions. This alignment creates a more structured and interpretable latent space, where each shared dimension corresponds to a high-level concept, or "factor." Unlike existing disentanglement-based methods that primarily focus on global factors shared across all classes, our method captures both global and class-specific factors, thereby enhancing the utility and interpretability of latent representations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated prediction for scalable and privacy-preserved knowledge-based planning in radiotherapy</title>
<link>https://arxiv.org/abs/2505.14507</link>
<guid>https://arxiv.org/abs/2505.14507</guid>
<content:encoded><![CDATA[
arXiv:2505.14507v1 Announce Type: cross 
Abstract: Background: Deep learning has potential to improve the efficiency and consistency of radiation therapy planning, but clinical adoption is hindered by the limited model generalizability due to data scarcity and heterogeneity among institutions. Although aggregating data from different institutions could alleviate this problem, data sharing is a practical challenge due to concerns about patient data privacy and other technical obstacles. Purpose: This work aims to address this dilemma by developing FedKBP+, a comprehensive federated learning (FL) platform for predictive tasks in real-world applications in radiotherapy treatment planning. Methods: We implemented a unified communication stack based on Google Remote Procedure Call (gRPC) to support communication between participants whether located on the same workstation or distributed across multiple workstations. In addition to supporting the centralized FL strategies commonly available in existing open-source frameworks, FedKBP+ also provides a fully decentralized FL model where participants directly exchange model weights to each other through Peer-to-Peer communication. We evaluated FedKBP+ on three predictive tasks using scale-attention network (SA-Net) as the predictive model. Conclusions: Our results demonstrate that FedKBP+ is highly effective, efficient and robust, showing great potential as a federated learning platform for radiation therapy.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BACON: A fully explainable AI model with graded logic for decision making problems</title>
<link>https://arxiv.org/abs/2505.14510</link>
<guid>https://arxiv.org/abs/2505.14510</guid>
<content:encoded><![CDATA[
arXiv:2505.14510v1 Announce Type: cross 
Abstract: As machine learning models and autonomous agents are increasingly deployed in high-stakes, real-world domains such as healthcare, security, finance, and robotics, the need for transparent and trustworthy explanations has become critical. To ensure end-to-end transparency of AI decisions, we need models that are not only accurate but also fully explainable and human-tunable. We introduce BACON, a novel framework for automatically training explainable AI models for decision making problems using graded logic. BACON achieves high predictive accuracy while offering full structural transparency and precise, logic-based symbolic explanations, enabling effective human-AI collaboration and expert-guided refinement. We evaluate BACON with a diverse set of scenarios: classic Boolean approximation, Iris flower classification, house purchasing decisions and breast cancer diagnosis. In each case, BACON provides high-performance models while producing compact, human-verifiable decision logic. These results demonstrate BACON's potential as a practical and principled approach for delivering crisp, trustworthy explainable AI.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Deep Non-Linear Spatially Selective Filters for Weakly Guided Extraction of Moving Speakers in Dynamic Scenarios</title>
<link>https://arxiv.org/abs/2505.14517</link>
<guid>https://arxiv.org/abs/2505.14517</guid>
<content:encoded><![CDATA[
arXiv:2505.14517v1 Announce Type: cross 
Abstract: Recent speaker extraction methods using deep non-linear spatial filtering perform exceptionally well when the target direction is known and stationary. However, spatially dynamic scenarios are considerably more challenging due to time-varying spatial features and arising ambiguities, e.g. when moving speakers cross. While in a static scenario it may be easy for a user to point to the target's direction, manually tracking a moving speaker is impractical. Instead of relying on accurate time-dependent directional cues, which we refer to as strong guidance, in this paper we propose a weakly guided extraction method solely depending on the target's initial position to cope with spatial dynamic scenarios. By incorporating our own deep tracking algorithm and developing a joint training strategy on a synthetic dataset, we demonstrate the proficiency of our approach in resolving spatial ambiguities and even outperform a mismatched, but strongly guided extraction method.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A simple estimator of the correlation kernel matrix of a determinantal point process</title>
<link>https://arxiv.org/abs/2505.14529</link>
<guid>https://arxiv.org/abs/2505.14529</guid>
<content:encoded><![CDATA[
arXiv:2505.14529v1 Announce Type: cross 
Abstract: The Determinantal Point Process (DPP) is a parameterized model for multivariate binary variables, characterized by a correlation kernel matrix. This paper proposes a closed form estimator of this kernel, which is particularly easy to implement and can also be used as a starting value of learning algorithms for maximum likelihood estimation. We prove the consistency and asymptotic normality of our estimator, as well as its large deviation properties.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs</title>
<link>https://arxiv.org/abs/2505.14530</link>
<guid>https://arxiv.org/abs/2505.14530</guid>
<content:encoded><![CDATA[
arXiv:2505.14530v1 Announce Type: cross 
Abstract: We show that large language models (LLMs) exhibit an $\textit{internal chain-of-thought}$: they sequentially decompose and execute composite tasks layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned at different network depths, and (ii) these subtasks are executed sequentially across layers. On a benchmark of 15 two-step composite tasks, we employ layer-from context-masking and propose a novel cross-task patching method, confirming (i). To examine claim (ii), we apply LogitLens to decode hidden states, revealing a consistent layerwise execution pattern. We further replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing the same stepwise dynamics. Together, our results enhance LLMs transparency by showing their capacity to internally plan and execute subtasks (or instructions), opening avenues for fine-grained, instruction-level activation steering.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons from Defending Gemini Against Indirect Prompt Injections</title>
<link>https://arxiv.org/abs/2505.14534</link>
<guid>https://arxiv.org/abs/2505.14534</guid>
<content:encoded><![CDATA[
arXiv:2505.14534v1 Announce Type: cross 
Abstract: Gemini is increasingly used to perform tasks on behalf of users, where function-calling and tool-use capabilities enable the model to access user data. Some tools, however, require access to untrusted data introducing risk. Adversaries can embed malicious instructions in untrusted data which cause the model to deviate from the user's expectations and mishandle their data or permissions. In this report, we set out Google DeepMind's approach to evaluating the adversarial robustness of Gemini models and describe the main lessons learned from the process. We test how Gemini performs against a sophisticated adversary through an adversarial evaluation framework, which deploys a suite of adaptive attack techniques to run continuously against past, current, and future versions of Gemini. We describe how these ongoing evaluations directly help make Gemini more resilient against manipulation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2505.14552</link>
<guid>https://arxiv.org/abs/2505.14552</guid>
<content:encoded><![CDATA[
arXiv:2505.14552v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM's general reasoning potential. To address this limitation, we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pivot Language for Low-Resource Machine Translation</title>
<link>https://arxiv.org/abs/2505.14553</link>
<guid>https://arxiv.org/abs/2505.14553</guid>
<content:encoded><![CDATA[
arXiv:2505.14553v1 Announce Type: cross 
Abstract: Certain pairs of languages suffer from lack of a parallel corpus which is large in size and diverse in domain. One of the ways this is overcome is via use of a pivot language. In this paper we use Hindi as a pivot language to translate Nepali into English. We describe what makes Hindi a good candidate for the pivot. We discuss ways in which a pivot language can be used, and use two such approaches - the Transfer Method (fully supervised) and Backtranslation (semi-supervised) - to translate Nepali into English. Using the former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which improves the baseline fully supervised score reported by (Guzman et al., 2019) by 6.6 points. While we are slightly below the semi-supervised baseline score of 15.1, we discuss what may have caused this under-performance, and suggest scope for future work.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification</title>
<link>https://arxiv.org/abs/2505.14561</link>
<guid>https://arxiv.org/abs/2505.14561</guid>
<content:encoded><![CDATA[
arXiv:2505.14561v1 Announce Type: cross 
Abstract: Self-Supervised Learning (SSL) has led to considerable progress in Speaker Verification (SV). The standard framework uses same-utterance positive sampling and data-augmentation to generate anchor-positive pairs of the same speaker. This is a major limitation, as this strategy primarily encodes channel information from the recording condition, shared by the anchor and positive. We propose a new positive sampling technique to address this bottleneck: Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find an appropriate positive, i.e., of the same speaker identity but a different recording condition, in the latent space using clustering assignments and a memory queue of positive embeddings. SSPS improves SV performance for both SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by lowering intra-speaker variance, providing comparable performance to DINO-SSPS.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Context Protocols Enhance Collective Inference</title>
<link>https://arxiv.org/abs/2505.14569</link>
<guid>https://arxiv.org/abs/2505.14569</guid>
<content:encoded><![CDATA[
arXiv:2505.14569v1 Announce Type: cross 
Abstract: AI agents have become increasingly adept at complex tasks such as coding, reasoning, and multimodal understanding. However, building generalist systems requires moving beyond individual agents to collective inference -- a paradigm where multi-agent systems with diverse, task-specialized agents complement one another through structured communication and collaboration. Today, coordination is usually handled with imprecise, ad-hoc natural language, which limits complex interaction and hinders interoperability with domain-specific agents. We introduce Agent context protocols (ACPs): a domain- and agent-agnostic family of structured protocols for agent-agent communication, coordination, and error handling. ACPs combine (i) persistent execution blueprints -- explicit dependency graphs that store intermediate agent outputs -- with (ii) standardized message schemas, enabling robust and fault-tolerant multi-agent collective inference. ACP-powered generalist systems reach state-of-the-art performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance and best-in-class multimodal technical reports, outperforming commercial AI systems in human evaluation. ACPs are highly modular and extensible, allowing practitioners to build top-tier generalist agents quickly.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Fetal Biometry Assessment with Deep Ensembles using Sparse-Sampling of 2D Intrapartum Ultrasound Images</title>
<link>https://arxiv.org/abs/2505.14572</link>
<guid>https://arxiv.org/abs/2505.14572</guid>
<content:encoded><![CDATA[
arXiv:2505.14572v1 Announce Type: cross 
Abstract: The International Society of Ultrasound advocates Intrapartum Ultrasound (US) Imaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression through changes in fetal head position. Two reliable ultrasound-derived parameters that are used to predict outcomes of instrumental vaginal delivery are the angle of progression (AoP) and head-symphysis distance (HSD). In this work, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we propose an automated fetal biometry measurement pipeline to reduce intra- and inter-observer variability and improve measurement reliability. Our pipeline consists of three key tasks: (i) classification of standard planes (SP) from US videos, (ii) segmentation of fetal head and pubic symphysis from the detected SPs, and (iii) computation of the AoP and HSD from the segmented regions. We perform sparse sampling to mitigate class imbalances and reduce spurious correlations in task (i), and utilize ensemble-based deep learning methods for task (i) and (ii) to enhance generalizability under different US acquisition settings. Finally, to promote robustness in task iii) with respect to the structural fidelity of measurements, we retain the largest connected components and apply ellipse fitting to the segmentations. Our solution achieved ACC: 0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71, $\Delta_{AoP}$: 8.90 and $\Delta_{HSD}$: 14.35 across an unseen hold-out set of 4 patients and 224 US frames. The results from the proposed automated pipeline can improve the understanding of labour arrest causes and guide the development of clinical risk stratification tools for efficient and effective prenatal care.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Optimization of Energy-Harvesting Underlay Cognitive Radio Networks Using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.14581</link>
<guid>https://arxiv.org/abs/2505.14581</guid>
<content:encoded><![CDATA[
arXiv:2505.14581v1 Announce Type: cross 
Abstract: In this paper, a reinforcement learning technique is employed to maximize the performance of a cognitive radio network (CRN). In the presence of primary users (PUs), it is presumed that two secondary users (SUs) access the licensed band within underlay mode. In addition, the SU transmitter is assumed to be an energy-constrained device that requires harvesting energy in order to transmit signals to their intended destination. Therefore, we propose that there are two main sources of energy; the interference of PUs' transmissions and ambient radio frequency (RF) sources. The SU will select whether to gather energy from PUs or only from ambient sources based on a predetermined threshold. The process of energy harvesting from the PUs' messages is accomplished via the time switching approach. In addition, based on a deep Q-network (DQN) approach, the SU transmitter determines whether to collect energy or transmit messages during each time slot as well as selects the suitable transmission power in order to maximize its average data rate. Our approach outperforms a baseline strategy and converges, as shown by our findings.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance Segmentation for Point Sets</title>
<link>https://arxiv.org/abs/2505.14583</link>
<guid>https://arxiv.org/abs/2505.14583</guid>
<content:encoded><![CDATA[
arXiv:2505.14583v1 Announce Type: cross 
Abstract: Recently proposed neural network architectures like PointNet [QSMG16] and PointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point sets. The feature representations of shapes learned by these two networks enabled training classifiers for Semantic Segmentation, and more recently for Instance Segmentation via the Similarity Group Proposal Network (SGPN) [WYHN17]. One area of improvement which has been highlighted by SGPN's authors, pertains to use of memory intensive similarity matrices which occupy memory quadratic in the number of points. In this report, we attempt to tackle this issue through use of two sampling based methods, which compute Instance Segmentation on a sub-sampled Point Set, and then extrapolate labels to the complete set using the nearest neigbhour approach. While both approaches perform equally well on large sub-samples, the random-based strategy gives the most improvements in terms of speed and memory usage.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Dimensional Analysis of Bootstrap Ensemble Classifiers</title>
<link>https://arxiv.org/abs/2505.14587</link>
<guid>https://arxiv.org/abs/2505.14587</guid>
<content:encoded><![CDATA[
arXiv:2505.14587v1 Announce Type: cross 
Abstract: Bootstrap methods have long been a cornerstone of ensemble learning in machine learning. This paper presents a theoretical analysis of bootstrap techniques applied to the Least Square Support Vector Machine (LSSVM) ensemble in the context of large and growing sample sizes and feature dimensionalities. Leveraging tools from Random Matrix Theory, we investigate the performance of this classifier that aggregates decision functions from multiple weak classifiers, each trained on different subsets of the data. We provide insights into the use of bootstrap methods in high-dimensional settings, enhancing our understanding of their impact. Based on these findings, we propose strategies to select the number of subsets and the regularization parameter that maximize the performance of the LSSVM. Empirical experiments on synthetic and real-world datasets validate our theoretical results.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Foundation Model for Communication Systems</title>
<link>https://arxiv.org/abs/2505.14603</link>
<guid>https://arxiv.org/abs/2505.14603</guid>
<content:encoded><![CDATA[
arXiv:2505.14603v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) has demonstrated unprecedented performance across various domains, and its application to communication systems is an active area of research. While current methods focus on task-specific solutions, the broader trend in AI is shifting toward large general models capable of supporting multiple applications. In this work, we take a step toward a foundation model for communication data--a transformer-based, multi-modal model designed to operate directly on communication data. We propose methodologies to address key challenges, including tokenization, positional embedding, multimodality, variable feature sizes, and normalization. Furthermore, we empirically demonstrate that such a model can successfully estimate multiple features, including transmission rank, selected precoder, Doppler spread, and delay profile.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)</title>
<link>https://arxiv.org/abs/2505.14608</link>
<guid>https://arxiv.org/abs/2505.14608</guid>
<content:encoded><![CDATA[
arXiv:2505.14608v1 Announce Type: cross 
Abstract: Despite considerable progress in the development of machine-text detectors, it has been suggested that the problem is inherently hard, and therefore, that stakeholders should proceed under the assumption that machine-generated text cannot be reliably detected as such. We examine a recent such claim by Nicks et al. (2024) regarding the ease with which language models can be optimized to degrade the performance of machine-text detectors, including detectors not specifically optimized against. We identify a feature space$\unicode{x2013}$the stylistic feature space$\unicode{x2013}$that is robust to such optimization, and show that it may be used to reliably detect samples from language models optimized to prevent detection. Furthermore, we show that even when models are explicitly optimized against stylistic detectors, detection performance remains surprisingly unaffected. We then seek to understand if stylistic detectors are inherently more robust. To study this question, we explore a new paraphrasing approach that simultaneously aims to close the gap between human writing and machine writing in stylistic feature space while avoiding detection using traditional features. We show that when only a single sample is available for detection, this attack is universally effective across all detectors considered, including those that use writing style. However, as the number of samples available for detection grows, the human and machine distributions become distinguishable. This observation encourages us to introduce AURA, a metric that estimates the overlap between human and machine-generated distributions by analyzing how detector performance improves as more samples become available. Overall, our findings underscore previous recommendations to avoid reliance on machine-text detection.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas</title>
<link>https://arxiv.org/abs/2505.14615</link>
<guid>https://arxiv.org/abs/2505.14615</guid>
<content:encoded><![CDATA[
arXiv:2505.14615v1 Announce Type: cross 
Abstract: We introduce SATBench, a benchmark for evaluating the logical reasoning capabilities of large language models (LLMs) through logical puzzles derived from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on inference rule-based reasoning, which often involves deducing conclusions from a set of premises, our approach leverages the search-based nature of SAT problems, where the objective is to find a solution that fulfills a specified set of logical constraints. Each instance in SATBench is generated from a SAT formula, then translated into a story context and conditions using LLMs. The generation process is fully automated and allows for adjustable difficulty by varying the number of clauses. All 2100 puzzles are validated through both LLM-assisted and solver-based consistency checks, with human validation on a subset. Experimental results show that even the strongest model, o4-mini, achieves only 65.0% accuracy on hard UNSAT problems, close to the random baseline of 50%. SATBench exposes fundamental limitations in the search-based logical reasoning abilities of current LLMs and provides a scalable testbed for future research in logical reasoning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Reconstruction from Sketches</title>
<link>https://arxiv.org/abs/2505.14621</link>
<guid>https://arxiv.org/abs/2505.14621</guid>
<content:encoded><![CDATA[
arXiv:2505.14621v1 Announce Type: cross 
Abstract: We consider the problem of reconstructing a 3D scene from multiple sketches. We propose a pipeline which involves (1) stitching together multiple sketches through use of correspondence points, (2) converting the stitched sketch into a realistic image using a CycleGAN, and (3) estimating that image's depth-map using a pre-trained convolutional neural network based architecture called MegaDepth. Our contribution includes constructing a dataset of image-sketch pairs, the images for which are from the Zurich Building Database, and sketches have been generated by us. We use this dataset to train a CycleGAN for our pipeline's second step. We end up with a stitching process that does not generalize well to real drawings, but the rest of the pipeline that creates a 3D reconstruction from a single sketch performs quite well on a wide variety of drawings.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas</title>
<link>https://arxiv.org/abs/2505.14633</link>
<guid>https://arxiv.org/abs/2505.14633</guid>
<content:encoded><![CDATA[
arXiv:2505.14633v1 Announce Type: cross 
Abstract: Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference</title>
<link>https://arxiv.org/abs/2505.14638</link>
<guid>https://arxiv.org/abs/2505.14638</guid>
<content:encoded><![CDATA[
arXiv:2505.14638v1 Announce Type: cross 
Abstract: Deep neural networks have achieved state-of-the-art results in a wide range of applications, from natural language processing and computer vision to speech recognition. However, as tasks become increasingly complex, model sizes continue to grow, posing challenges in latency and memory efficiency. To meet these constraints, post-training quantization has emerged as a promising solution. In this paper, we propose a novel hardware-efficient quantization and inference scheme that exploits hardware advantages with minimal accuracy degradation. Specifically, we introduce a W4A8 scheme, where weights are quantized and stored using 4-bit integer precision, and inference computations are performed using 8-bit floating-point arithmetic, demonstrating significant speedups and improved memory utilization compared to 16-bit operations, applicable on various modern accelerators. To mitigate accuracy loss, we develop a novel quantization algorithm, dubbed Dual Precision Quantization (DPQ), that leverages the unique structure of our scheme without introducing additional inference overhead. Experimental results demonstrate improved performance (i.e., increased throughput) while maintaining tolerable accuracy degradation relative to the full-precision model.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential QCQP for Bilevel Optimization with Line Search</title>
<link>https://arxiv.org/abs/2505.14647</link>
<guid>https://arxiv.org/abs/2505.14647</guid>
<content:encoded><![CDATA[
arXiv:2505.14647v1 Announce Type: cross 
Abstract: Bilevel optimization involves a hierarchical structure where one problem is nested within another, leading to complex interdependencies between levels. We propose a single-loop, tuning-free algorithm that guarantees anytime feasibility, i.e., approximate satisfaction of the lower-level optimality condition, while ensuring descent of the upper-level objective. At each iteration, a convex quadratically-constrained quadratic program (QCQP) with a closed-form solution yields the search direction, followed by a backtracking line search inspired by control barrier functions to ensure safe, uniformly positive step sizes. The resulting method is scalable, requires no hyperparameter tuning, and converges under mild local regularity assumptions. We establish an O(1/k) ergodic convergence rate and demonstrate the algorithm's effectiveness on representative bilevel tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings</title>
<link>https://arxiv.org/abs/2505.14664</link>
<guid>https://arxiv.org/abs/2505.14664</guid>
<content:encoded><![CDATA[
arXiv:2505.14664v1 Announce Type: cross 
Abstract: Cross-modal embeddings form the foundation for multi-modal models. However, visualization methods for interpreting cross-modal embeddings have been primarily confined to traditional dimensionality reduction (DR) techniques like PCA and t-SNE. These DR methods primarily focus on feature distributions within a single modality, whilst failing to incorporate metrics (e.g., CLIPScore) across multiple modalities.This paper introduces AKRMap, a new DR technique designed to visualize cross-modal embeddings metric with enhanced accuracy by learning kernel regression of the metric landscape in the projection space. Specifically, AKRMap constructs a supervised projection network guided by a post-projection kernel regression loss, and employs adaptive generalized kernels that can be jointly optimized with the projection. This approach enables AKRMap to efficiently generate visualizations that capture complex metric distributions, while also supporting interactive features such as zoom and overlay for deeper exploration. Quantitative experiments demonstrate that AKRMap outperforms existing DR methods in generating more accurate and trustworthy visualizations. We further showcase the effectiveness of AKRMap in visualizing and comparing cross-modal embeddings for text-to-image models. Code and demo are available at https://github.com/yilinye/AKRMap.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Optimization via Gradient-Based Hamiltonian Descent</title>
<link>https://arxiv.org/abs/2505.14670</link>
<guid>https://arxiv.org/abs/2505.14670</guid>
<content:encoded><![CDATA[
arXiv:2505.14670v1 Announce Type: cross 
Abstract: With rapid advancements in machine learning, first-order algorithms have emerged as the backbone of modern optimization techniques, owing to their computational efficiency and low memory requirements. Recently, the connection between accelerated gradient methods and damped heavy-ball motion, particularly within the framework of Hamiltonian dynamics, has inspired the development of innovative quantum algorithms for continuous optimization. One such algorithm, Quantum Hamiltonian Descent (QHD), leverages quantum tunneling to escape saddle points and local minima, facilitating the discovery of global solutions in complex optimization landscapes. However, QHD faces several challenges, including slower convergence rates compared to classical gradient methods and limited robustness in highly non-convex problems due to the non-local nature of quantum states. Furthermore, the original QHD formulation primarily relies on function value information, which limits its effectiveness. Inspired by insights from high-resolution differential equations that have elucidated the acceleration mechanisms in classical methods, we propose an enhancement to QHD by incorporating gradient information, leading to what we call gradient-based QHD. Gradient-based QHD achieves faster convergence and significantly increases the likelihood of identifying global solutions. Numerical simulations on challenging problem instances demonstrate that gradient-based QHD outperforms existing quantum and classical methods by at least an order of magnitude.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training</title>
<link>https://arxiv.org/abs/2505.14681</link>
<guid>https://arxiv.org/abs/2505.14681</guid>
<content:encoded><![CDATA[
arXiv:2505.14681v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit vs Unfolded Graph Neural Networks</title>
<link>https://arxiv.org/abs/2111.06592</link>
<guid>https://arxiv.org/abs/2111.06592</guid>
<content:encoded><![CDATA[
arXiv:2111.06592v3 Announce Type: replace 
Abstract: It has been observed that message-passing graph neural networks (GNN) sometimes struggle to maintain a healthy balance between the efficient/scalable modeling of long-range dependencies across nodes while avoiding unintended consequences such oversmoothed node representations, sensitivity to spurious edges, or inadequate model interpretability. To address these and other issues, two separate strategies have recently been proposed, namely implicit and unfolded GNNs (that we abbreviate to IGNN and UGNN respectively). The former treats node representations as the fixed points of a deep equilibrium model that can efficiently facilitate arbitrary implicit propagation across the graph with a fixed memory footprint. In contrast, the latter involves treating graph propagation as unfolded descent iterations as applied to some graph-regularized energy function. While motivated differently, in this paper we carefully quantify explicit situations where the solutions they produce are equivalent and others where their properties sharply diverge. This includes the analysis of convergence, representational capacity, and interpretability. In support of this analysis, we also provide empirical head-to-head comparisons across multiple synthetic and public real-world node classification benchmarks. These results indicate that while IGNN is substantially more memory-efficient, UGNN models support unique, integrated graph attention mechanisms and propagation rules that can achieve strong node classification accuracy across disparate regimes such as adversarially-perturbed graphs, graphs with heterophily, and graphs involving long-range dependencies.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gated Recurrent Neural Networks with Weighted Time-Delay Feedback</title>
<link>https://arxiv.org/abs/2212.00228</link>
<guid>https://arxiv.org/abs/2212.00228</guid>
<content:encoded><![CDATA[
arXiv:2212.00228v2 Announce Type: replace 
Abstract: In this paper, we present a novel approach to modeling long-term dependencies in sequential data by introducing a gated recurrent unit (GRU) with a weighted time-delay feedback mechanism. Our proposed model, named $\tau$-GRU, is a discretized version of a continuous-time formulation of a recurrent unit, where the dynamics are governed by delay differential equations (DDEs). We prove the existence and uniqueness of solutions for the continuous-time model and show that the proposed feedback mechanism can significantly improve the modeling of long-term dependencies. Our empirical results indicate that $\tau$-GRU outperforms state-of-the-art recurrent units and gated recurrent architectures on a range of tasks, achieving faster convergence and better generalization.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Model-Agnostic Federated Learning over Networks</title>
<link>https://arxiv.org/abs/2302.04363</link>
<guid>https://arxiv.org/abs/2302.04363</guid>
<content:encoded><![CDATA[
arXiv:2302.04363v3 Announce Type: replace 
Abstract: We present a model-agnostic federated learning method for networks of heterogeneous data and models. The network structure reflects similarities between the (statistics of the) local datasets and, in turn, their associated local (personal) models. Our method is an instance of empirical risk minimization, with a regularization term derived from the network structure of the data. In particular, we require well-connected local models, which form clusters, to yield similar predictions on shared public, unlabelled dataset(s). The proposed method allows for a wide range of local models. The only restriction is that these local models must allow for efficient implementation of regularized empirical risk minimization (training). For many models, such implementations are readily available in high-level programming libraries, including scikit-learn, Keras, and PyTorch.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>