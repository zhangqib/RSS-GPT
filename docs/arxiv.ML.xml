<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>AGRAG: Advanced Graph-based Retrieval-Augmented Generation for LLMs</title>
<link>https://arxiv.org/abs/2511.05549</link>
<guid>https://arxiv.org/abs/2511.05549</guid>
<content:encoded><![CDATA[
<div> Graph-based retrieval-augmented generation, structured knowledge, LLM, reasoning ability, MCMI<br>
Summary:<br>
The article introduces AGRAG, an advanced framework to address challenges in Graph-based RAG. AGRAG tackles issues of Inaccurate Graph Construction by replacing LLM entity extraction with a statistics-based method. It improves Reasoning Ability by formulating reasoning as the MCMI subgraph generation problem, optimizing for high influence nodes with low edge costs. This enhancement allows for explicit reasoning paths that clarify why certain chunks were selected. AGRAG also enhances Answering capability by providing more comprehensive reasoning paths, reducing noise impact, and helping LLM focus on query-related content. By allowing complex graph structures like cycles, AGRAG improves the comprehensiveness of reasoning paths, enhancing overall performance in comparison to NaiveRAG on certain tasks.<br> <br>Summary: <div>
arXiv:2511.05549v1 Announce Type: new 
Abstract: Graph-based retrieval-augmented generation (Graph-based RAG) has demonstrated significant potential in enhancing Large Language Models (LLMs) with structured knowledge. However, existing methods face three critical challenges: Inaccurate Graph Construction, caused by LLM hallucination; Poor Reasoning Ability, caused by failing to generate explicit reasons telling LLM why certain chunks were selected; and Inadequate Answering, which only partially answers the query due to the inadequate LLM reasoning, making their performance lag behind NaiveRAG on certain tasks. To address these issues, we propose AGRAG, an advanced graph-based retrieval-augmented generation framework. When constructing the graph, AGRAG substitutes the widely used LLM entity extraction method with a statistics-based method, avoiding hallucination and error propagation. When retrieval, AGRAG formulates the graph reasoning procedure as the Minimum Cost Maximum Influence (MCMI) subgraph generation problem, where we try to include more nodes with high influence score, but with less involving edge cost, to make the generated reasoning paths more comprehensive. We prove this problem to be NP-hard, and propose a greedy algorithm to solve it. The MCMI subgraph generated can serve as explicit reasoning paths to tell LLM why certain chunks were retrieved, thereby making the LLM better focus on the query-related part contents of the chunks, reducing the impact of noise, and improving AGRAG's reasoning ability. Furthermore, compared with the simple tree-structured reasoning paths, our MCMI subgraph can allow more complex graph structures, such as cycles, and improve the comprehensiveness of the generated reasoning paths.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep one-gate per layer networks with skip connections are universal classifiers</title>
<link>https://arxiv.org/abs/2511.05552</link>
<guid>https://arxiv.org/abs/2511.05552</guid>
<content:encoded><![CDATA[
<div> Keywords: multilayer perceptron, deep neural network, one-gate layers, skip connections, classification

Summary: 
This paper presents a method to convert a multilayer perceptron with two hidden layers designed for binary classification into a deep neural network with one-gate layers and skip connections. By implementing skip connections and reducing the number of gates in each layer, the network structure becomes more efficient and easier to interpret. This transformation allows for a more streamlined architecture while maintaining or improving classification performance. The use of skip connections helps alleviate the vanishing gradient problem and enables better information flow between layers. Additionally, the introduction of one-gate layers simplifies the network structure and reduces computational complexity. Overall, the proposed approach offers a novel way to enhance the performance and efficiency of neural networks for binary classification tasks. <br><br>Summary: <div>
arXiv:2511.05552v1 Announce Type: new 
Abstract: This paper shows how a multilayer perceptron with two hidden layers, which has been designed to classify two classes of data points, can easily be transformed into a deep neural network with one-gate layers and skip connections.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Daily Forecasting for Annual Time Series Datasets Using Similarity-Based Machine Learning Methods: A Case Study in the Energy Market</title>
<link>https://arxiv.org/abs/2511.05556</link>
<guid>https://arxiv.org/abs/2511.05556</guid>
<content:encoded><![CDATA[
<div> Keywords: Energy Security Index, daily proxy, time series similarity measures, XGBoost algorithm, high frequency forecasting

Summary:
The study introduces a daily proxy for the Energy Security Index to forecast energy security at a daily frequency, allowing for more timely response to fluctuations in policy environments. By utilizing time series similarity measures, the study identifies Volume Brent as the most suitable proxy and applies the XGBoost algorithm to generate 15-day ahead forecasts with strong performance. The forecasted Brent volume shows short-term fluctuations with prediction intervals, enabling real-time monitoring of energy security dynamics. This innovative framework integrates low-frequency macroeconomic indicators with high-frequency actionable signals, offering policymakers and analysts a scalable tool to respond rapidly to fast-changing policy and market conditions, particularly in data-scarce environments. <div>
arXiv:2511.05556v1 Announce Type: new 
Abstract: The policy environment of countries changes rapidly, influencing macro-level indicators such as the Energy Security Index. However, this index is only reported annually, limiting its responsiveness to short-term fluctuations. To address this gap, the present study introduces a daily proxy for the Energy Security Index and applies it to forecast energy security at a daily frequency.The study employs a two stage approach first, a suitable daily proxy for the annual Energy Security Index is identified by applying six time series similarity measures to key energy related variables. Second, the selected proxy is modeled using the XGBoost algorithm to generate 15 day ahead forecasts, enabling high frequency monitoring of energy security dynamics.As the result of proxy choosing, Volume Brent consistently emerged as the most suitable proxy across the majority of methods. The model demonstrated strong performance, with an R squared of 0.981 on the training set and 0.945 on the test set, and acceptable error metrics . The 15 day forecast of Brent volume indicates short term fluctuations, with a peak around day 4, a decline until day 8, a rise near day 10, and a downward trend toward day 15, accompanied by prediction intervals.By integrating time series similarity measures with machine learning based forecasting, this study provides a novel framework for converting low frequency macroeconomic indicators into high frequency, actionable signals. The approach enables real time monitoring of the Energy Security Index, offering policymakers and analysts a scalable and practical tool to respond more rapidly to fast changing policy and market conditions, especially in data scarce environments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diversified Flow Matching with Translation Identifiability</title>
<link>https://arxiv.org/abs/2511.05558</link>
<guid>https://arxiv.org/abs/2511.05558</guid>
<content:encoded><![CDATA[
<div> Keywords: Diversified distribution matching, unified translation function, flow matching, ODE-based framework, translation identifiability

Summary: 
Diversified distribution matching (DDM) aims to map conditional source distributions to their target counterparts, addressing content misalignment in unpaired domain translation and achieving translation identifiability. However, DDM has only been implemented using GANs, which are unstable and lack transport trajectory information. This work introduces diversified flow matching (DFM), an ODE-based framework for DDM. DFM adapts flow matching to enforce a unified translation function, overcoming challenges such as learning the translation function's velocity instead of the function itself. A custom bilevel optimization-based training loss, a nonlinear interpolant, and structural reformulation provide a tangible implementation. DFM is the first ODE-based approach to ensure translation identifiability. Experiments on synthetic and real-world datasets confirm the effectiveness of the proposed method. 

<br><br>Summary: <div>
arXiv:2511.05558v1 Announce Type: new 
Abstract: Diversified distribution matching (DDM) finds a unified translation function mapping a diverse collection of conditional source distributions to their target counterparts. DDM was proposed to resolve content misalignment issues in unpaired domain translation, achieving translation identifiability. However, DDM has only been implemented using GANs due to its constraints on the translation function. GANs are often unstable to train and do not provide the transport trajectory information -- yet such trajectories are useful in applications such as single-cell evolution analysis and robot route planning. This work introduces diversified flow matching (DFM), an ODE-based framework for DDM. Adapting flow matching (FM) to enforce a unified translation function as in DDM is challenging, as FM learns the translation function's velocity rather than the translation function itself. A custom bilevel optimization-based training loss, a nonlinear interpolant, and a structural reformulation are proposed to address these challenges, offering a tangible implementation. To our knowledge, DFM is the first ODE-based approach guaranteeing translation identifiability. Experiments on synthetic and real-world datasets validate the proposed method.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effective Test-Time Scaling of Discrete Diffusion through Iterative Refinement</title>
<link>https://arxiv.org/abs/2511.05562</link>
<guid>https://arxiv.org/abs/2511.05562</guid>
<content:encoded><![CDATA[
<div> Reward-Guided Generation, Discrete Diffusion Models, Iterative Reward-Guided Refinement, Multiple-Try Metropolis, Test-Time Scaling <br>
Summary: <br>
The article introduces Iterative Reward-Guided Refinement (IterRef) as a test-time scaling method for discrete diffusion models. It leverages reward-guided transitions to refine misaligned states, unlike previous methods that only guide subsequent transitions. IterRef is formalized within a Multiple-Try Metropolis framework, proving convergence to the reward-aligned distribution. It progressively improves intermediate states by steering them towards the optimal distribution. Evaluation on text and image domains shows consistent enhancements in reward-guided generation quality. The method achieves significant improvements under low compute budgets, surpassing prior baselines. <div>
arXiv:2511.05562v1 Announce Type: new 
Abstract: Test-time scaling through reward-guided generation remains largely unexplored for discrete diffusion models despite its potential as a promising alternative. In this work, we introduce Iterative Reward-Guided Refinement (IterRef), a novel test-time scaling method tailored to discrete diffusion that leverages reward- guided noising-denoising transitions to progressively refine misaligned intermediate states. We formalize this process within a Multiple-Try Metropolis (MTM) framework, proving convergence to the reward-aligned distribution. Unlike prior methods that assume the current state is already aligned with the reward distribution and only guide the subsequent transition, our approach explicitly refines each state in situ, progressively steering it toward the optimal intermediate distribution. Across both text and image domains, we evaluate IterRef on diverse discrete diffusion models and observe consistent improvements in reward-guided generation quality. In particular, IterRef achieves striking gains under low compute budgets, far surpassing prior state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lookahead Unmasking Elicits Accurate Decoding in Diffusion Language Models</title>
<link>https://arxiv.org/abs/2511.05563</link>
<guid>https://arxiv.org/abs/2511.05563</guid>
<content:encoded><![CDATA[
<div> Keywords: Masked Diffusion Models, Lookahead Unmasking, path selection, uncertainty, efficiency

Summary: 
Masked Diffusion Models (MDMs) use iteratively unmasking tokens for language models, but their performance is sensitive to the order of unmasking during inference. This study introduces Lookahead Unmasking (LookUM), which optimizes sampling by selecting paths from all possible unmasking orders without needing an external reward model. The framework consists of a path generator proposing paths and a verifier calculating path uncertainty for selection. LookUM leverages uncertainty to avoid error-prone trajectories and enhance performance across various benchmarks. Notably, LookUM achieves peak performance with only two to three paths, outperforming RL-tuned models and improving both base and post-trained versions of LLaDA. The framework's effectiveness in path selection showcases its efficiency and versatility in improving language modeling tasks. The code for LookUM will be made public. 

<br><br>Summary: <div>
arXiv:2511.05563v1 Announce Type: new 
Abstract: Masked Diffusion Models (MDMs) as language models generate by iteratively unmasking tokens, yet their performance crucially depends on the inference time order of unmasking. Prevailing heuristics, such as confidence based sampling, are myopic: they optimize locally, fail to leverage extra test-time compute, and let early decoding mistakes cascade. We propose Lookahead Unmasking (LookUM), which addresses these concerns by reformulating sampling as path selection over all possible unmasking orders without the need for an external reward model. Our framework couples (i) a path generator that proposes paths by sampling from pools of unmasking sets with (ii) a verifier that computes the uncertainty of the proposed paths and performs importance sampling to subsequently select the final paths. Empirically, erroneous unmasking measurably inflates sequence level uncertainty, and our method exploits this to avoid error-prone trajectories. We validate our framework across six benchmarks, such as mathematics, planning, and coding, and demonstrate consistent performance improvements. LookUM requires only two to three paths to achieve peak performance, demonstrating remarkably efficient path selection. The consistent improvements on both LLaDA and post-trained LLaDA 1.5 are particularly striking: base LLaDA with LookUM rivals the performance of RL-tuned LLaDA 1.5, while LookUM further enhances LLaDA 1.5 itself showing that uncertainty based verification provides orthogonal benefits to reinforcement learning and underscoring the versatility of our framework. Code will be publicly released.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Sample-Level Framework Motivated by Distributionally Robust Optimization with Variance-Based Radius Assignment for Enhanced Neural Network Generalization Under Distribution Shift</title>
<link>https://arxiv.org/abs/2511.05568</link>
<guid>https://arxiv.org/abs/2511.05568</guid>
<content:encoded><![CDATA[
<div> variance-driven, adaptive, sample-level DRO, high-risk training samples, personalized robustness budget, two-sided KL-divergence-style bounds, linear inner maximization problem

Summary:<br><br>The paper introduces a variance-driven, adaptive, sample-level Distributionally Robust Optimization (Var-DRO) framework to address distribution shifts and minority subpopulations in deep neural network training. This framework automatically identifies high-risk training samples and assigns personalized robustness budgets based on online loss variance. By constraining the ratio between adversarial and empirical weights for each sample, the method optimizes for worst-case risk while avoiding overly conservative models. To stabilize training, a warm-up phase, linear ramp schedule for global budget caps, and label smoothing are introduced. Var-DRO outperforms ERM and KL-DRO on CIFAR-10-C and Waterbirds datasets, demonstrating improved overall performance and competitive results on the original CIFAR-10 dataset. The proposed framework is unsupervised, easy to implement, theoretically grounded, and computationally efficient. <div>
arXiv:2511.05568v1 Announce Type: new 
Abstract: Distribution shifts and minority subpopulations frequently undermine the reliability of deep neural networks trained using Empirical Risk Minimization (ERM). Distributionally Robust Optimization (DRO) addresses this by optimizing for the worst-case risk within a neighborhood of the training distribution. However, conventional methods depend on a single, global robustness budget, which can lead to overly conservative models or a misallocation of robustness. We propose a variance-driven, adaptive, sample-level DRO (Var-DRO) framework that automatically identifies high-risk training samples and assigns a personalized robustness budget to each based on its online loss variance. Our formulation employs two-sided, KL-divergence-style bounds to constrain the ratio between adversarial and empirical weights for every sample. This results in a linear inner maximization problem over a convex polytope, which admits an efficient water-filling solution. To stabilize training, we introduce a warmup phase and a linear ramp schedule for the global cap on per-sample budgets, complemented by label smoothing for numerical robustness. Evaluated on CIFAR-10-C (corruptions), our method achieves the highest overall mean accuracy compared to ERM and KL-DRO. On Waterbirds, Var-DRO improves overall performance while matching or surpassing KL-DRO. On the original CIFAR-10 dataset, Var-DRO remains competitive, exhibiting the modest trade-off anticipated when prioritizing robustness. The proposed framework is unsupervised (requiring no group labels), straightforward to implement, theoretically sound, and computationally efficient.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven jet fuel demand forecasting: A case study of Copenhagen Airport</title>
<link>https://arxiv.org/abs/2511.05569</link>
<guid>https://arxiv.org/abs/2511.05569</guid>
<content:encoded><![CDATA[
<div> forecasting, jet fuel demand, machine learning models, data-driven approaches, predictive capabilities 

Summary:
This research addresses the importance of accurate forecasting of jet fuel demand for optimizing supply chain operations in the aviation market. The study evaluates the performance of data-driven approaches, including traditional time series models, Prophet, LSTM sequence-to-sequence neural networks, and hybrid models. The research utilizes data from a major aviation fuel distributor in the Danish market and analyzes three different datasets to provide valuable insights to practitioners. A key challenge is the forecasting horizon of 30 days to optimize sourcing strategies. The study aims to demonstrate the advantages of employing data-driven models in jet fuel demand forecasting and emphasizes the impact of incorporating additional variables in the predictive models. <div>
arXiv:2511.05569v1 Announce Type: new 
Abstract: Accurate forecasting of jet fuel demand is crucial for optimizing supply chain operations in the aviation market. Fuel distributors specifically require precise estimates to avoid inventory shortages or excesses. However, there is a lack of studies that analyze the jet fuel demand forecasting problem using machine learning models. Instead, many industry practitioners rely on deterministic or expertise-based models. In this research, we evaluate the performance of data-driven approaches using a substantial amount of data obtained from a major aviation fuel distributor in the Danish market. Our analysis compares the predictive capabilities of traditional time series models, Prophet, LSTM sequence-to-sequence neural networks, and hybrid models. A key challenge in developing these models is the required forecasting horizon, as fuel demand needs to be predicted for the next 30 days to optimize sourcing strategies. To ensure the reliability of the data-driven approaches and provide valuable insights to practitioners, we analyze three different datasets. The primary objective of this study is to present a comprehensive case study on jet fuel demand forecasting, demonstrating the advantages of employing data-driven models and highlighting the impact of incorporating additional variables in the predictive models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction</title>
<link>https://arxiv.org/abs/2511.05577</link>
<guid>https://arxiv.org/abs/2511.05577</guid>
<content:encoded><![CDATA[
<div> dataset, fine-tune, VLM, polymer property prediction, multimodal learning <br>
<br>Summary: 
This study focuses on the development of Vision-Language Models (VLMs) for materials science, specifically in the domain of polymer property prediction. The researchers introduce a new multimodal polymer dataset and propose the use of instruction-tuning pairs to fine-tune VLMs. By leveraging multimodal data, the fine-tuned models, implemented using LoRA, show improved performance compared to unimodal and baseline approaches. The study highlights the benefits of multimodal learning in enhancing prediction accuracy and reducing the need for separate models for different properties. This approach not only enhances prediction performance but also reduces deployment and maintenance costs, making it a promising strategy for scalable and efficient polymer property prediction in materials science.  <br> <div>
arXiv:2511.05577v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have shown strong performance in tasks like visual question answering and multimodal text generation, but their effectiveness in scientific domains such as materials science remains limited. While some machine learning methods have addressed specific challenges in this field, there is still a lack of foundation models designed for broad tasks like polymer property prediction using multimodal data. In this work, we present a multimodal polymer dataset to fine-tune VLMs through instruction-tuning pairs and assess the impact of multimodality on prediction performance. Our fine-tuned models, using LoRA, outperform unimodal and baseline approaches, demonstrating the benefits of multimodal learning. Additionally, this approach reduces the need to train separate models for different properties, lowering deployment and maintenance costs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distillation-Accelerated Uncertainty Modeling for Multi-Objective RTA Interception</title>
<link>https://arxiv.org/abs/2511.05582</link>
<guid>https://arxiv.org/abs/2511.05582</guid>
<content:encoded><![CDATA[
<div> Keywords: Real-Time Auction, Traffic Quality Estimation, Uncertainty Modeling, Multi-Objective Learning, Knowledge Distillation

Summary:
DAUM is proposed as a framework for Real-Time Auction Interception to address challenges in traffic quality estimation and uncertainty modeling. It integrates multi-objective learning with uncertainty modeling to provide accurate traffic quality predictions and reliable confidence estimates. The application of knowledge distillation reduces computational overhead while maintaining predictive accuracy and the benefits of uncertainty estimation. Experiments on the JD advertisement dataset show that DAUM consistently improves predictive performance, with the distilled model achieving a tenfold increase in inference speed. <div>
arXiv:2511.05582v1 Announce Type: new 
Abstract: Real-Time Auction (RTA) Interception aims to filter out invalid or irrelevant traffic to enhance the integrity and reliability of downstream data. However, two key challenges remain: (i) the need for accurate estimation of traffic quality together with sufficiently high confidence in the model's predictions, typically addressed through uncertainty modeling, and (ii) the efficiency bottlenecks that such uncertainty modeling introduces in real-time applications due to repeated inference. To address these challenges, we propose DAUM, a joint modeling framework that integrates multi-objective learning with uncertainty modeling, yielding both traffic quality predictions and reliable confidence estimates. Building on DAUM, we further apply knowledge distillation to reduce the computational overhead of uncertainty modeling, while largely preserving predictive accuracy and retaining the benefits of uncertainty estimation. Experiments on the JD advertisement dataset demonstrate that DAUM consistently improves predictive performance, with the distilled model delivering a tenfold increase in inference speed.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Depth-induced NTK: Bridging Over-parameterized Neural Networks and Deep Neural Kernels</title>
<link>https://arxiv.org/abs/2511.05585</link>
<guid>https://arxiv.org/abs/2511.05585</guid>
<content:encoded><![CDATA[
<div> deep learning, neural networks, kernel methods, representation learning, scaling law

Summary:
The article introduces deep neural kernels as a framework to interpret over-parameterized neural networks. Neural tangent kernels (NTKs) have connected infinitely wide neural networks to nonparametric Bayesian inference. However, the existing NTK paradigm has focused on the infinite-width regime, neglecting network depth. The proposed depth-induced NTK kernel, based on a shortcut-related architecture, converges to a Gaussian process as network depth approaches infinity. The kernel's training invariance and spectrum properties stabilize kernel dynamics and prevent degeneration. Experimental results demonstrate the effectiveness of the proposed method. This research significantly expands neural kernel theory, enhancing understanding of deep learning and the scaling law.<br><br>Summary: <div>
arXiv:2511.05585v1 Announce Type: new 
Abstract: While deep learning has achieved remarkable success across a wide range of applications, its theoretical understanding of representation learning remains limited. Deep neural kernels provide a principled framework to interpret over-parameterized neural networks by mapping hierarchical feature transformations into kernel spaces, thereby combining the expressive power of deep architectures with the analytical tractability of kernel methods. Recent advances, particularly neural tangent kernels (NTKs) derived by gradient inner products, have established connections between infinitely wide neural networks and nonparametric Bayesian inference. However, the existing NTK paradigm has been predominantly confined to the infinite-width regime, while overlooking the representational role of network depth. To address this gap, we propose a depth-induced NTK kernel based on a shortcut-related architecture, which converges to a Gaussian process as the network depth approaches infinity. We theoretically analyze the training invariance and spectrum properties of the proposed kernel, which stabilizes the kernel dynamics and mitigates degeneration. Experimental results further underscore the effectiveness of our proposed method. Our findings significantly extend the existing landscape of the neural kernel theory and provide an in-depth understanding of deep learning and the scaling law.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompting Neural-Guided Equation Discovery Based on Residuals</title>
<link>https://arxiv.org/abs/2511.05586</link>
<guid>https://arxiv.org/abs/2511.05586</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural-guided equation discovery, Residuals, Syntax tree, Equation improvement, Mathematical operations 

Summary: 
Residuals for Equation Discovery (RED) is introduced as a post-processing method to enhance given equations by focusing on their residuals. By utilizing node-based calculation rules on the syntax tree of the initial equation, RED computes residuals for each subequation. These residuals are then used as new target variables in the data set to generate a new prompt for equation suggestion. If the resulting equation from the new prompt outperforms the original on a validation set, it replaces the original subequation. RED is compatible with any equation discovery system, efficient in computation, and easily adaptable for additional mathematical operations. Experimental results on 53 equations from the Feynman benchmark demonstrate that RED effectively improves both neural-guided and classical genetic programming equation discovery systems.<br><br>Summary: <div>
arXiv:2511.05586v1 Announce Type: new 
Abstract: Neural-guided equation discovery systems use a data set as prompt and predict an equation that describes the data set without extensive search. However, if the equation does not meet the user's expectations, there are few options for getting other equation suggestions without intensive work with the system. To fill this gap, we propose Residuals for Equation Discovery (RED), a post-processing method that improves a given equation in a targeted manner, based on its residuals. By parsing the initial equation to a syntax tree, we can use node-based calculation rules to compute the residual for each subequation of the initial equation. It is then possible to use this residual as new target variable in the original data set and generate a new prompt. If, with the new prompt, the equation discovery system suggests a subequation better than the old subequation on a validation set, we replace the latter by the former. RED is usable with any equation discovery system, is fast to calculate, and is easy to extend for new mathematical operations. In experiments on 53 equations from the Feynman benchmark, we show that it not only helps to improve all tested neural-guided systems, but also all tested classical genetic programming systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoPRIS: Efficient and Stable Reinforcement Learning via Concurrency-Controlled Partial Rollout with Importance Sampling</title>
<link>https://arxiv.org/abs/2511.05589</link>
<guid>https://arxiv.org/abs/2511.05589</guid>
<content:encoded><![CDATA[
<div> RL, language models, Concurrency-Controlled Partial Rollout, Importance Sampling, off-policy<br>
<br>
Summary:<br>
The article introduces CoPRIS, a reinforcement learning system for enhancing large language models that addresses inefficiencies in fully synchronous RL systems. CoPRIS maintains a fixed number of concurrent rollouts, terminates early to avoid long-tail inefficiencies, and reuses unfinished trajectories. It also introduces Cross-stage Importance Sampling Correction to handle off-policy trajectories by incorporating buffered log probabilities from the previous policy. Experiments on mathematical reasoning benchmarks demonstrate that CoPRIS achieves up to 1.94x faster training without compromising performance. The code for CoPRIS is available on GitHub for further exploration and implementation. <div>
arXiv:2511.05589v1 Announce Type: new 
Abstract: Reinforcement learning (RL) post-training has become a trending paradigm for enhancing the capabilities of large language models (LLMs). Most existing RL systems for LLMs operate in a fully synchronous manner, where training must wait for the rollout of an entire batch to complete. This design leads to severe inefficiencies, as extremely long trajectories can stall the entire rollout process and leave many GPUs idle. To address this issue, we propose Concurrency- Controlled Partial Rollout with Importance Sampling (CoPRIS), which mitigates long-tail inefficiencies by maintaining a fixed number of concurrent rollouts, early-terminating once sufficient samples are collected, and reusing unfinished trajectories in subsequent rollouts. To mitigate the impact of off-policy trajectories, we introduce Cross-stage Importance Sampling Correction, which concatenates buffered log probabilities from the previous policy with those recomputed under the current policy for importance sampling correction. Experiments on challenging mathematical reasoning benchmarks show that CoPRIS achieves up to 1.94x faster training while maintaining comparable or superior performance to synchronous RL systems. The code of CoPRIS is available at https://github.com/777pomingzi/CoPRIS.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSparQ: Adaptive Sparse Quantization with Error Feedback for Robust &amp; Efficient Federated Learning</title>
<link>https://arxiv.org/abs/2511.05591</link>
<guid>https://arxiv.org/abs/2511.05591</guid>
<content:encoded><![CDATA[
<div> compression, federated learning, communication overhead, model accuracy, convergence robustness

Summary:
FedSparQ introduces a lightweight compression framework for Federated Learning (FL) that dynamically sparsifies gradients, quantizes entries, and integrates error feedback residuals. It reduces communication overhead by 90% compared to FedAvg, while preserving or enhancing model accuracy by up to 6%. The approach adapts seamlessly to data distributions, requires no manual tuning, and is agnostic to model architecture. Empirical evaluations on vision benchmarks show significant improvements in model accuracy and convergence robustness compared to other baselines. FedSparQ is a practical solution for bandwidth-constrained federated deployments, with potential for future extensions in adaptive precision and privacy-preserving protocols. <br><br>Summary: <div>
arXiv:2511.05591v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across decentralized clients while preserving data privacy by keeping raw data local. However, FL suffers from significant communication overhead due to the frequent exchange of high-dimensional model updates over constrained networks. In this paper, we present FedSparQ, a lightweight compression framework that dynamically sparsifies the gradient of each client through an adaptive threshold, applies half-precision quanti- zation to retained entries and integrates residuals from error feedback to prevent loss of information. FedSparQ requires no manual tuning of sparsity rates or quantization schedules, adapts seamlessly to both homogeneous and heterogeneous data distributions, and is agnostic to model architecture. Through extensive empirical evaluation on vision benchmarks under independent and identically distributed (IID) and non-IID data, we show that FedSparQ substantially reduces communication overhead (reducing by 90% of bytes sent compared to FedAvg) while preserving or improving model accuracy (improving by 6% compared to FedAvg non-compressed solution or to state-of-the- art compression models) and enhancing convergence robustness (by 50%, compared to the other baselines). Our approach provides a practical, easy-to-deploy solution for bandwidth- constrained federated deployments and lays the groundwork for future extensions in adaptive precision and privacy-preserving protocols.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAVER: Generative Graph Vocabularies for Robust Graph Foundation Models Fine-tuning</title>
<link>https://arxiv.org/abs/2511.05592</link>
<guid>https://arxiv.org/abs/2511.05592</guid>
<content:encoded><![CDATA[
<div> Generative augmentations, ego-graph disentanglement, graphon-based generative experts, MoE-CoE network, few-shot node and graph classification <br>
Summary:<br>
1. The paper introduces GRAVER, a framework for robust and efficient fine-tuning of Graph Foundation Models (GFMs) through generative augmentations.
2. GRAVER identifies transferable subgraph patterns through ego-graph disentanglement and constructs graph vocabularies based on ego-graph similarity.
3. It utilizes a universal task template and graphon-based generative experts for effective pre-training across diverse domains.
4. The framework graces support samples with in-context vocabularies to facilitate prompt fine-tuning, with knowledge routing through a MoE-CoE network.
5. Extensive experiments demonstrate GRAVER's superiority in effectiveness, robustness, and efficiency on few-shot node and graph classification tasks compared to 15 state-of-the-art baselines. <br> <div>
arXiv:2511.05592v1 Announce Type: new 
Abstract: Inspired by the remarkable success of foundation models in language and vision, Graph Foundation Models (GFMs) hold significant promise for broad applicability across diverse graph tasks and domains. However, existing GFMs struggle with unstable few-shot fine-tuning, where both performance and adaptation efficiency exhibit significant fluctuations caused by the randomness in the support sample selection and structural discrepancies between the pre-trained and target graphs. How to fine-tune GFMs robustly and efficiently to enable trustworthy knowledge transfer across domains and tasks is the major challenge. In this paper, we propose GRAVER, a novel Generative gRAph VocabulariEs for Robust GFM fine-tuning framework that tackles the aforementioned instability via generative augmentations. Specifically, to identify transferable units, we analyze and extract key class-specific subgraph patterns by ego-graph disentanglement and validate their transferability both theoretically and empirically. To enable effective pre-training across diverse domains, we leverage a universal task template based on ego-graph similarity and construct graph vocabularies via graphon-based generative experts. To facilitate robust and efficient prompt fine-tuning, we grave the support samples with in-context vocabularies, where the lightweight MoE-CoE network attentively routes knowledge from source domains. Extensive experiments demonstrate the superiority of GRAVER over effectiveness, robustness, and efficiency on downstream few-shot node and graph classification tasks compared with 15 state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient Projection onto Historical Descent Directions for Communication-Efficient Federated Learning</title>
<link>https://arxiv.org/abs/2511.05593</link>
<guid>https://arxiv.org/abs/2511.05593</guid>
<content:encoded><![CDATA[
<div> compressors, federated learning, ProjFL, ProjFL+EF, communication efficiency

Summary:
ProjFL and ProjFL+EF are algorithms introduced in this work for federated learning to improve communication efficiency. ProjFL works with unbiased compressors, while ProjFL+EF is tailored for biased compressors with an Error Feedback mechanism. Both algorithms project local gradients onto a shared client-server subspace using historical descent directions to exchange information efficiently with minimal communication overhead. Convergence guarantees are established for strongly convex, convex, and non-convex settings. Empirical evaluations on FL classification benchmarks with deep neural networks show that both methods achieve comparable accuracy to existing baselines while significantly reducing communication costs. <div>
arXiv:2511.05593v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized model training across multiple clients while optionally preserving data privacy. However, communication efficiency remains a critical bottleneck, particularly for large-scale models. In this work, we introduce two complementary algorithms: ProjFL, designed for unbiased compressors, and ProjFL+EF, tailored for biased compressors through an Error Feedback mechanism. Both methods rely on projecting local gradients onto a shared client-server subspace spanned by historical descent directions, enabling efficient information exchange with minimal communication overhead. We establish convergence guarantees for both algorithms under strongly convex, convex, and non-convex settings. Empirical evaluations on standard FL classification benchmarks with deep neural networks show that ProjFL and ProjFL+EF achieve accuracy comparable to existing baselines while substantially reducing communication costs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Predictive Maintenance in Intelligent Manufacturing: An Integrated FNO-DAE-GNN-PPO MDP Framework</title>
<link>https://arxiv.org/abs/2511.05594</link>
<guid>https://arxiv.org/abs/2511.05594</guid>
<content:encoded><![CDATA[
<div> Keywords: smart manufacturing, predictive maintenance, Markov Decision Process, deep learning, industrial potential

Summary: 
The paper introduces a novel framework for predictive maintenance in smart manufacturing systems. It combines advanced soft computing techniques including Fourier Neural Operator (FNO), Denoising Autoencoder (DAE), Graph Neural Network (GNN), and Proximal Policy Optimisation (PPO) to address the challenges of maintenance in complex systems. The framework uses FNOs for capturing temporal patterns, DAEs for noise-resistant state embedding, and GNNs for modeling inter-device dependencies. PPO is employed for stable optimization of long-term maintenance strategies. Experimental results show the framework outperforms deep learning baseline models with cost reductions of up to 13% and demonstrates strong convergence and inter-module synergy. The approach has significant potential in the industry to reduce downtime and operating expenses through data-driven maintenance strategies.

<br><br>Summary: <div>
arXiv:2511.05594v1 Announce Type: new 
Abstract: In the era of smart manufacturing, predictive maintenance (PdM) plays a pivotal role in improving equipment reliability and reducing operating costs. In this paper, we propose a novel Markov Decision Process (MDP) framework that integrates advanced soft computing techniques - Fourier Neural Operator (FNO), Denoising Autoencoder (DAE), Graph Neural Network (GNN), and Proximal Policy Optimisation (PPO) - to address the multidimensional challenges of predictive maintenance in complex manufacturing systems. Specifically, the proposed framework innovatively combines the powerful frequency-domain representation capability of FNOs to capture high-dimensional temporal patterns; DAEs to achieve robust, noise-resistant latent state embedding from complex non-Gaussian sensor data; and GNNs to accurately represent inter-device dependencies for coordinated system-wide maintenance decisions. Furthermore, by exploiting PPO, the framework ensures stable and efficient optimisation of long-term maintenance strategies to effectively handle uncertainty and non-stationary dynamics. Experimental validation demonstrates that the approach significantly outperforms multiple deep learning baseline models with up to 13% cost reduction, as well as strong convergence and inter-module synergy. The framework has considerable industrial potential to effectively reduce downtime and operating expenses through data-driven strategies.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowNet: Modeling Dynamic Spatio-Temporal Systems via Flow Propagation</title>
<link>https://arxiv.org/abs/2511.05595</link>
<guid>https://arxiv.org/abs/2511.05595</guid>
<content:encoded><![CDATA[
<div> FlowNet, dynamic node couplings, flow transfers, conservation principles, context-aware propagation
Summary:
FlowNet is a novel architecture based on the Spatio-Temporal Flow paradigm that models dynamic node couplings through flow transfers governed by conservation principles. It uses flow tokens to simulate source-to-destination transfers, ensuring state redistribution aligns with conservation laws. The architecture includes Flow Allocation Modules for efficient information transfer and an Adaptive Spatial Masking module to adjust the interaction radius. The cascaded architecture enhances scalability and nonlinear representation capacity. In experiments on real-world systems, FlowNet outperforms existing approaches on seven metrics, demonstrating its efficiency and physical interpretability. This methodology provides a principled approach for modeling complex systems through spatio-temporal flow interactions. 
Summary:  <div>
arXiv:2511.05595v1 Announce Type: new 
Abstract: Accurately modeling complex dynamic spatio-temporal systems requires capturing flow-mediated interdependencies and context-sensitive interaction dynamics. Existing methods, predominantly graph-based or attention-driven, rely on similarity-driven connectivity assumptions, neglecting asymmetric flow exchanges that govern system evolution. We propose Spatio-Temporal Flow, a physics-inspired paradigm that explicitly models dynamic node couplings through quantifiable flow transfers governed by conservation principles. Building on this, we design FlowNet, a novel architecture leveraging flow tokens as information carriers to simulate source-to-destination transfers via Flow Allocation Modules, ensuring state redistribution aligns with conservation laws. FlowNet dynamically adjusts the interaction radius through an Adaptive Spatial Masking module, suppressing irrelevant noise while enabling context-aware propagation. A cascaded architecture enhances scalability and nonlinear representation capacity. Experiments demonstrate that FlowNet significantly outperforms existing state-of-the-art approaches on seven metrics in the modeling of three real-world systems, validating its efficiency and physical interpretability. We establish a principled methodology for modeling complex systems through spatio-temporal flow interactions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoHood3D: A Multi-Modal Benchmark for Automotive Hood Design and Fluid-Structure Interaction</title>
<link>https://arxiv.org/abs/2511.05596</link>
<guid>https://arxiv.org/abs/2511.05596</guid>
<content:encoded><![CDATA[
<div> Dataset, automotive hoods, machine learning, multiphysics, fluid-solid coupling 
Summary: 
AutoHood3D presents a new high-fidelity multi-modal dataset consisting of 16000+ geometric variants of automotive hoods for machine learning applications in engineering component design and process optimization. The dataset focuses on hood deformation due to fluid entrapment and inertial loading during rotary-dip painting, modeled using a coupled Large-Eddy Simulation (LES)-Finite Element Analysis (FEA). It includes time-resolved physical fields, STL meshes, and natural language prompts for text-to-geometry synthesis. The dataset addresses limitations of existing datasets by offering 3D cases, diverse geometric variations, and multi-modal annotations. The study validates the numerical methodology, establishes neural architecture baselines, and highlights systematic errors in displacement and force predictions. New approaches and multiphysics loss functions are recommended to improve model training with fluid-solid coupling. AutoHood3D facilitates physics-aware ML development, accelerates generative-design iteration, and supports the creation of new fluid-structure interaction benchmarks. URLs for dataset and code access are provided in the appendix. 
Summary: <div>
arXiv:2511.05596v1 Announce Type: new 
Abstract: This study presents a new high-fidelity multi-modal dataset containing 16000+ geometric variants of automotive hoods useful for machine learning (ML) applications such as engineering component design and process optimization, and multiphysics system surrogates. The dataset is centered on a practical multiphysics problem-hood deformation from fluid entrapment and inertial loading during rotary-dip painting. Each hood is numerically modeled with a coupled Large-Eddy Simulation (LES)-Finite Element Analysis (FEA), using 1.2M cells in total to ensure spatial and temporal accuracy. The dataset provides time-resolved physical fields, along with STL meshes and structured natural language prompts for text-to-geometry synthesis. Existing datasets are either confined to 2D cases, exhibit limited geometric variations, or lack the multi-modal annotations and data structures - shortcomings we address with AutoHood3D. We validate our numerical methodology, establish quantitative baselines across five neural architectures, and demonstrate systematic surrogate errors in displacement and force predictions. These findings motivate the design of novel approaches and multiphysics loss functions that enforce fluid-solid coupling during model training. By providing fully reproducible workflows, AutoHood3D enables physics-aware ML development, accelerates generative-design iteration, and facilitates the creation of new FSI benchmarks. Dataset and code URLs in Appendix.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FiCABU: A Fisher-Based, Context-Adaptive Machine Unlearning Processor for Edge AI</title>
<link>https://arxiv.org/abs/2511.05605</link>
<guid>https://arxiv.org/abs/2511.05605</guid>
<content:encoded><![CDATA[
<div> Machine unlearning, edge AI processors, FiCABU, privacy regulations, right to be forgotten  
Summary:  
FiCABU, a software-hardware co-design, enables efficient machine unlearning at the edge for privacy compliance. It combines Context-Adaptive Unlearning and Balanced Dampening to achieve random-guess forget accuracy while maintaining retain accuracy. The approach starts edits from back-end layers and adjusts dampening strength by depth, reducing computation by up to 87.52% for ResNet-18 and 71.03% for ViT models. Validated on FPGA and synthesized in 45 nm, FiCABU surpasses the retraining-free SSD baseline in retain preservation and energy efficiency on an INT8 hardware prototype. This innovative method demonstrates that depth-aware unlearning can be practical and efficient for resource-constrained edge AI devices.  
<br><br>Summary: <div>
arXiv:2511.05605v1 Announce Type: new 
Abstract: Machine unlearning, driven by privacy regulations and the "right to be forgotten", is increasingly needed at the edge, yet server-centric or retraining-heavy methods are impractical under tight computation and energy budgets. We present FiCABU (Fisher-based Context-Adaptive Balanced Unlearning), a software-hardware co-design that brings unlearning to edge AI processors. FiCABU combines (i) Context-Adaptive Unlearning, which begins edits from back-end layers and halts once the target forgetting is reached, with (ii) Balanced Dampening, which scales dampening strength by depth to preserve retain accuracy. These methods are realized in a full RTL design of a RISC-V edge AI processor that integrates two lightweight IPs for Fisher estimation and dampening into a GEMM-centric streaming pipeline, validated on an FPGA prototype and synthesized in 45 nm for power analysis. Across CIFAR-20 and PinsFaceRecognition with ResNet-18 and ViT, FiCABU achieves random-guess forget accuracy while matching the retraining-free Selective Synaptic Dampening (SSD) baseline on retain accuracy, reducing computation by up to 87.52 percent (ResNet-18) and 71.03 percent (ViT). On the INT8 hardware prototype, FiCABU further improves retain preservation and reduces energy to 6.48 percent (CIFAR-20) and 0.13 percent (PinsFaceRecognition) of the SSD baseline. In sum, FiCABU demonstrates that back-end-first, depth-aware unlearning can be made both practical and efficient for resource-constrained edge AI devices.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Prediction-Driven Adaptive Sampling for Digital Twins of Water Distribution Networks</title>
<link>https://arxiv.org/abs/2511.05610</link>
<guid>https://arxiv.org/abs/2511.05610</guid>
<content:encoded><![CDATA[
<div> Adaptive Framework, LSTM Forecasting, Conformal Prediction, Water Distribution Networks, Digital Twins
Summary:
An adaptive framework combining LSTM forecasting and Conformal Prediction is proposed for accurate state estimation in Water Distribution Networks (WDNs) using Digital Twins. This framework aims to optimize sensor placement by focusing on nodes with the highest uncertainty, rather than using uniform sampling which can be wasteful. The use of Marginal Conformal Prediction allows for low computational cost, making it suitable for real-time implementation. Experimental results on three different network datasets demonstrate a significant reduction in demand error compared to uniform sampling, achieving 33-34% lower error rates at 40% coverage. The adaptive framework also maintains high empirical coverage (89.4-90.2%) with only a small increase in computational cost (5-10%). Overall, the proposed approach shows promising results in improving state estimation accuracy in WDNs while optimizing resource utilization through targeted sensor placement. 
<br><br>Summary: <div>
arXiv:2511.05610v1 Announce Type: new 
Abstract: Digital Twins (DTs) for Water Distribution Networks (WDNs) require accurate state estimation with limited sensors. Uniform sampling often wastes resources across nodes with different uncertainty. We propose an adaptive framework combining LSTM forecasting and Conformal Prediction (CP) to estimate node-wise uncertainty and focus sensing on the most uncertain points. Marginal CP is used for its low computational cost, suitable for real-time DTs. Experiments on Hanoi, Net3, and CTOWN show 33-34% lower demand error than uniform sampling at 40% coverage and maintain 89.4-90.2% empirical coverage with only 5-10% extra computation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An MLCommons Scientific Benchmarks Ontology</title>
<link>https://arxiv.org/abs/2511.05614</link>
<guid>https://arxiv.org/abs/2511.05614</guid>
<content:encoded><![CDATA[
<div> physics, chemistry, materials science, biology, climate science<br>
Summary:<br>
This paper introduces an ontology for scientific benchmarking developed through a unified, community-driven effort within the MLCommons ecosystem. The ontology covers diverse domains such as physics, chemistry, materials science, biology, and climate science. It consolidates various benchmarks and frameworks into a single taxonomy, allowing for standardized and cross-domain benchmarking in scientific machine learning. New benchmarks can be added through an open submission workflow and evaluated against a six-category rating rubric. The architecture is extensible, supporting future scientific and AI/ML motifs. The effort aims to provide a scalable foundation for reproducible, high-quality benchmarking in scientific machine learning. A companion webpage has been developed to track the evolution of the project: https://mlcommons-science.github.io/benchmark/ <div>
arXiv:2511.05614v1 Announce Type: new 
Abstract: Scientific machine learning research spans diverse domains and data modalities, yet existing benchmark efforts remain siloed and lack standardization. This makes novel and transformative applications of machine learning to critical scientific use-cases more fragmented and less clear in pathways to impact. This paper introduces an ontology for scientific benchmarking developed through a unified, community-driven effort that extends the MLCommons ecosystem to cover physics, chemistry, materials science, biology, climate science, and more. Building on prior initiatives such as XAI-BENCH, FastML Science Benchmarks, PDEBench, and the SciMLBench framework, our effort consolidates a large set of disparate benchmarks and frameworks into a single taxonomy of scientific, application, and system-level benchmarks. New benchmarks can be added through an open submission workflow coordinated by the MLCommons Science Working Group and evaluated against a six-category rating rubric that promotes and identifies high-quality benchmarks, enabling stakeholders to select benchmarks that meet their specific needs. The architecture is extensible, supporting future scientific and AI/ML motifs, and we discuss methods for identifying emerging computing patterns for unique scientific workloads. The MLCommons Science Benchmarks Ontology provides a standardized, scalable foundation for reproducible, cross-domain benchmarking in scientific machine learning. A companion webpage for this work has also been developed as the effort evolves: https://mlcommons-science.github.io/benchmark/
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>wa-hls4ml: A Benchmark and Surrogate Models for hls4ml Resource and Latency Estimation</title>
<link>https://arxiv.org/abs/2511.05615</link>
<guid>https://arxiv.org/abs/2511.05615</guid>
<content:encoded><![CDATA[
<div> benchmark, ML accelerator, resource estimation, latency estimation, surrogate model 

Summary:
The article discusses the increasing integration of machine learning (ML) into hardware for real-time scientific applications and the challenges it faces. Advanced toolchains have reduced design iteration time but have also highlighted new obstacles like hardware synthesis bottlenecks. To address these issues, efforts are being made to develop ML-based surrogate models for estimating resource usage of ML accelerator architectures. The article introduces wa-hls4ml, a benchmark tool for ML accelerator resource and latency estimation, along with a dataset of over 680,000 neural networks synthesized using hls4ml on Xilinx FPGAs. It evaluates resource and latency predictors on various ML model architectures and introduces GNN- and transformer-based surrogate models for predicting accelerator performance. The models show promise, accurately predicting latency and resources close to the synthesized values on a test dataset. <div>
arXiv:2511.05615v1 Announce Type: new 
Abstract: As machine learning (ML) is increasingly implemented in hardware to address real-time challenges in scientific applications, the development of advanced toolchains has significantly reduced the time required to iterate on various designs. These advancements have solved major obstacles, but also exposed new challenges. For example, processes that were not previously considered bottlenecks, such as hardware synthesis, are becoming limiting factors in the rapid iteration of designs. To mitigate these emerging constraints, multiple efforts have been undertaken to develop an ML-based surrogate model that estimates resource usage of ML accelerator architectures. We introduce wa-hls4ml, a benchmark for ML accelerator resource and latency estimation, and its corresponding initial dataset of over 680,000 fully connected and convolutional neural networks, all synthesized using hls4ml and targeting Xilinx FPGAs. The benchmark evaluates the performance of resource and latency predictors against several common ML model architectures, primarily originating from scientific domains, as exemplar models, and the average performance across a subset of the dataset. Additionally, we introduce GNN- and transformer-based surrogate models that predict latency and resources for ML accelerators. We present the architecture and performance of the models and find that the models generally predict latency and resources for the 75% percentile within several percent of the synthesized resources on the synthetic test dataset.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Frequency Matters: When Time Series Foundation Models Fail Under Spectral Shift</title>
<link>https://arxiv.org/abs/2511.05619</link>
<guid>https://arxiv.org/abs/2511.05619</guid>
<content:encoded><![CDATA[
<div> TSFMs, industrial settings, spectral shift, player engagement prediction, frequency awareness <br>
<br>
Time series foundation models (TSFMs) have shown strong performance on standard benchmarks, leading to comparisons with the "BERT moment" for time series analysis. However, their efficacy in real-world industrial applications is uncertain. Research indicates that TSFMs struggle to generalize due to spectral shift, where frequency components in downstream tasks differ from those during pretraining. An investigation in mobile gaming highlighted TSFMs' underperformance compared to domain-adapted models. Controlled synthetic experiments revealed systematic performance degradation under spectral mismatch, emphasizing the importance of frequency awareness in robust TSFM deployment. These findings suggest the need for new pretraining and evaluation approaches that consider spectral diversity explicitly. <br><br>Summary: <div>
arXiv:2511.05619v1 Announce Type: new 
Abstract: Time series foundation models (TSFMs) have shown strong results on public benchmarks, prompting comparisons to a "BERT moment" for time series. Their effectiveness in industrial settings, however, remains uncertain. We examine why TSFMs often struggle to generalize and highlight spectral shift (a mismatch between the dominant frequency components in downstream tasks and those represented during pretraining) as a key factor. We present evidence from an industrial-scale player engagement prediction task in mobile gaming, where TSFMs underperform domain-adapted baselines. To isolate the mechanism, we design controlled synthetic experiments contrasting signals with seen versus unseen frequency bands, observing systematic degradation under spectral mismatch. These findings position frequency awareness as critical for robust TSFM deployment and motivate new pretraining and evaluation protocols that explicitly account for spectral diversity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fooling Algorithms in Non-Stationary Bandits using Belief Inertia</title>
<link>https://arxiv.org/abs/2511.05620</link>
<guid>https://arxiv.org/abs/2511.05620</guid>
<content:encoded><![CDATA[
<div> Belief Inertia, Piecewise Stationary Bandits, Regret, Lower Bounds, Non-Stationarity <br>
Summary:
The study examines worst-case regret in piecewise stationary multi-armed bandits. Traditional lower bounds in time-varying settings rely on infrequent sampling, where lack of exploration leads to high regret. The paper introduces a novel approach based on belief inertia, demonstrating how algorithms' historical reward averages create momentum that resists new evidence post-change. This inertia can mislead classical algorithms like Explore Then Commit, epsilon-greedy, and UCB, causing substantial regret growth despite parameter tuning. Even with periodic restarts to handle non-stationarity, worst-case regret remains linear. The findings suggest that leveraging belief inertia is an effective method for deriving precise lower bounds in non-stationary bandit settings. <br> <div>
arXiv:2511.05620v1 Announce Type: new 
Abstract: We study the problem of worst case regret in piecewise stationary multi armed bandits. While the minimax theory for stationary bandits is well established, understanding analogous limits in time-varying settings is challenging. Existing lower bounds rely on what we refer to as infrequent sampling arguments, where long intervals without exploration allow adversarial reward changes that induce large regret.
  In this paper, we introduce a fundamentally different approach based on a belief inertia argument. Our analysis captures how an algorithm's empirical beliefs, encoded through historical reward averages, create momentum that resists new evidence after a change. We show how this inertia can be exploited to construct adversarial instances that mislead classical algorithms such as Explore Then Commit, epsilon greedy, and UCB, causing them to suffer regret that grows linearly with T and with a substantial constant factor, regardless of how their parameters are tuned, even with a single change point.
  We extend the analysis to algorithms that periodically restart to handle non stationarity and prove that, even then, the worst case regret remains linear in T. Our results indicate that utilizing belief inertia can be a powerful method for deriving sharp lower bounds in non stationary bandits.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling the Training Dynamics of ReLU Networks through a Linear Lens</title>
<link>https://arxiv.org/abs/2511.05628</link>
<guid>https://arxiv.org/abs/2511.05628</guid>
<content:encoded><![CDATA[
<div> learning mechanisms, ReLU networks, effective weights, representation learning, decision boundaries
<br>
<br>
Summary: 
This study introduces a novel analytical framework that simplifies multi-layer ReLU networks into a single-layer linear model with input-dependent effective weights. These effective weights, denoted as $W_{\text{eff}}(x)$, adjust based on the activation pattern of ReLU units for each input sample. As training progresses, the effective weights for samples of the same class converge, while those from different classes diverge. Tracking the trajectories of these sample-specific effective weights offers insights into the formation of class-specific decision boundaries and the development of semantic representations within the network. This approach provides a clearer understanding of the internal workings of deep neural networks and sheds light on the fundamental principles of representation learning in complex, high-dimensional systems. <div>
arXiv:2511.05628v1 Announce Type: new 
Abstract: Deep neural networks, particularly those employing Rectified Linear Units (ReLU), are often perceived as complex, high-dimensional, non-linear systems. This complexity poses a significant challenge to understanding their internal learning mechanisms. In this work, we propose a novel analytical framework that recasts a multi-layer ReLU network into an equivalent single-layer linear model with input-dependent "effective weights". For any given input sample, the activation pattern of ReLU units creates a unique computational path, effectively zeroing out a subset of weights in the network. By composing the active weights across all layers, we can derive an effective weight matrix, $W_{\text{eff}}(x)$, that maps the input directly to the output for that specific sample. We posit that the evolution of these effective weights reveals fundamental principles of representation learning. Our work demonstrates that as training progresses, the effective weights corresponding to samples from the same class converge, while those from different classes diverge. By tracking the trajectories of these sample-wise effective weights, we provide a new lens through which to interpret the formation of class-specific decision boundaries and the emergence of semantic representations within the network.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction</title>
<link>https://arxiv.org/abs/2511.05629</link>
<guid>https://arxiv.org/abs/2511.05629</guid>
<content:encoded><![CDATA[
<div> data-driven models, SST prediction, physics-informed neural networks, ocean-atmosphere dynamics, Neural Ordinary Differential Equations (Neural ODEs)<br>
Summary:<br>
Sea Surface Temperature (SST) prediction is crucial for understanding ocean dynamics and interactions. Existing data-driven models lack interpretability and overlook key physical processes. SSTODE, a physics-informed Neural ODE framework, addresses these challenges by incorporating fluid transport principles to model spatiotemporal dynamics. The model derives ODEs for ocean dynamics and integrates external SST drivers, providing insights into SST evolution. SSTODE outperforms existing models in global and regional SST forecasting benchmarks, highlighting its interpretability and physical consistency. The model visually reveals the impact of advection dynamics, thermal diffusion patterns, and diurnal heating-cooling cycles on SST evolution. SSTODE's innovative approach has the potential to enhance understanding of SST dynamics and improve ocean-atmosphere predictions.
 <div>
arXiv:2511.05629v1 Announce Type: new 
Abstract: Sea Surface Temperature (SST) is crucial for understanding upper-ocean thermal dynamics and ocean-atmosphere interactions, which have profound economic and social impacts. While data-driven models show promise in SST prediction, their black-box nature often limits interpretability and overlooks key physical processes. Recently, physics-informed neural networks have been gaining momentum but struggle with complex ocean-atmosphere dynamics due to 1) inadequate characterization of seawater movement (e.g., coastal upwelling) and 2) insufficient integration of external SST drivers (e.g., turbulent heat fluxes). To address these challenges, we propose SSTODE, a physics-informed Neural Ordinary Differential Equations (Neural ODEs) framework for SST prediction. First, we derive ODEs from fluid transport principles, incorporating both advection and diffusion to model ocean spatiotemporal dynamics. Through variational optimization, we recover a latent velocity field that explicitly governs the temporal dynamics of SST. Building upon ODE, we introduce an Energy Exchanges Integrator (EEI)-inspired by ocean heat budget equations-to account for external forcing factors. Thus, the variations in the components of these factors provide deeper insights into SST dynamics. Extensive experiments demonstrate that SSTODE achieves state-of-the-art performances in global and regional SST forecasting benchmarks. Furthermore, SSTODE visually reveals the impact of advection dynamics, thermal diffusion patterns, and diurnal heating-cooling cycles on SST evolution. These findings demonstrate the model's interpretability and physical consistency.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Guided Machine Learning for Uncertainty Quantification in Turbulence Models</title>
<link>https://arxiv.org/abs/2511.05633</link>
<guid>https://arxiv.org/abs/2511.05633</guid>
<content:encoded><![CDATA[
<div> Keywords: turbulent flows, turbulence models, Eigenspace Perturbation Method, convolutional neural network, calibration

Summary:
Predicting the evolution of turbulent flows is crucial in various fields, but the use of turbulence models introduces uncertainty. The Eigenspace Perturbation Method (EPM) is commonly used to assess model-form uncertainty, but it may overestimate uncertainty bounds due to its purely physics-based nature. In this study, a hybrid approach combining a convolutional neural network (CNN) with EPM is proposed to enhance the calibration of uncertainty estimates while maintaining physical consistency. By modulating EPM perturbation magnitudes with CNN, the hybrid ML-EPM framework demonstrates significant improvements in providing tighter and better-calibrated uncertainty estimates compared to using EPM alone. This innovative approach holds promise for more accurate predictions in turbulent flow simulations, bridging the gap between physics-based methods and machine learning techniques. 

<br><br>Summary: <div>
arXiv:2511.05633v1 Announce Type: new 
Abstract: Predicting the evolution of turbulent flows is central across science and engineering. Most studies rely on simulations with turbulence models, whose empirical simplifications introduce epistemic uncertainty. The Eigenspace Perturbation Method (EPM) is a widely used physics-based approach to quantify model-form uncertainty, but being purely physics-based it can overpredict uncertainty bounds. We propose a convolutional neural network (CNN)-based modulation of EPM perturbation magnitudes to improve calibration while preserving physical consistency. Across canonical cases, the hybrid ML-EPM framework yields substantially tighter, better-calibrated uncertainty estimates than baseline EPM alone.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games</title>
<link>https://arxiv.org/abs/2511.05640</link>
<guid>https://arxiv.org/abs/2511.05640</guid>
<content:encoded><![CDATA[
<div> entropy-regularized, Quantal Response Equilibrium, Inverse Game Theory, Blind-IGT, joint parameter recovery <br>
<br>
Summary: 
The article introduces Blind-IGT, a statistical framework for recovering both the rationality parameter and reward parameters in competitive settings where the rationality parameter is unknown. By imposing a normalization constraint to resolve scale ambiguity, the framework can uniquely identify these parameters. An efficient Normalized Least Squares estimator is proposed, achieving optimal convergence rates. In cases where unique identification is not possible, confidence sets are constructed for partial identification. The framework is extended to Markov games, demonstrating optimal convergence rates and strong empirical performance even when transition dynamics are unknown. <div>
arXiv:2511.05640v1 Announce Type: new 
Abstract: Inverse Game Theory (IGT) methods based on the entropy-regularized Quantal Response Equilibrium (QRE) offer a tractable approach for competitive settings, but critically assume the agents' rationality parameter (temperature $\tau$) is known a priori. When $\tau$ is unknown, a fundamental scale ambiguity emerges that couples $\tau$ with the reward parameters ($\theta$), making them statistically unidentifiable. We introduce Blind-IGT, the first statistical framework to jointly recover both $\theta$ and $\tau$ from observed behavior. We analyze this bilinear inverse problem and establish necessary and sufficient conditions for unique identification by introducing a normalization constraint that resolves the scale ambiguity. We propose an efficient Normalized Least Squares (NLS) estimator and prove it achieves the optimal $\mathcal{O}(N^{-1/2})$ convergence rate for joint parameter recovery. When strong identifiability conditions fail, we provide partial identification guarantees through confidence set construction. We extend our framework to Markov games and demonstrate optimal convergence rates with strong empirical performance even when transition dynamics are unknown.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KLASS: KL-Guided Fast Inference in Masked Diffusion Models</title>
<link>https://arxiv.org/abs/2511.05664</link>
<guid>https://arxiv.org/abs/2511.05664</guid>
<content:encoded><![CDATA[
<div> Keywords: Masked diffusion models, KL-Adaptive Stability Sampling, iteration refinement process, fast sampling method, diverse domains

Summary:
Masked diffusion models have shown promise in various tasks, including language generation. However, their iterative refinement process often leads to slow inference due to static sampling speed. To address this issue, the authors propose KL-Adaptive Stability Sampling (KLASS), a fast sampling method that leverages token-level KL divergence to identify stable predictions with high confidence. By unmasking multiple tokens in each iteration without additional training, KLASS significantly accelerates generation speed while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to 2.78 times faster wall-clock speedups compared to standard greedy decoding, outperforming other diffusion-based samplers. The effectiveness of KLASS is demonstrated across different domains such as text, image, and molecular generation, highlighting its versatility as a widely applicable sampler for diverse models.<br><br>Summary: <div>
arXiv:2511.05664v1 Announce Type: new 
Abstract: Masked diffusion models have demonstrated competitive results on various tasks including language generation. However, due to its iterative refinement process, the inference is often bottlenecked by slow and static sampling speed. To overcome this problem, we introduce `KL-Adaptive Stability Sampling' (KLASS), a fast yet effective sampling method that exploits token-level KL divergence to identify stable, high-confidence predictions. By unmasking multiple tokens in each iteration without any additional model training, our approach speeds up generation significantly while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to $2.78\times$ wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers. We further validate KLASS across diverse domains, including text, image, and molecular generation, showing its effectiveness as a broadly applicable sampler across different models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributionally Robust Self Paced Curriculum Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.05694</link>
<guid>https://arxiv.org/abs/2511.05694</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Distributionally Robust, Self-Paced Curriculum, Robustness, Performance

Summary: 
Distributionally Robust Self-Paced Curriculum Reinforcement Learning (DR-SPCRL) addresses the challenge in reinforcement learning of policies failing under distribution shifts in real-world environments. It treats the robustness budget as a continuous curriculum, adapting it based on the agent's progress. This adaptive scheduling allows for a balance between nominal and robust performance. Empirical results show that DR-SPCRL stabilizes training and achieves a superior trade-off between robustness and performance. Compared to fixed or heuristic scheduling strategies, DR-SPCRL demonstrates an average 11.8% increase in episodic return under varying perturbations and achieves approximately 1.9 times the performance of corresponding nominal RL algorithms. This method offers a promising solution to the trade-off challenge in reinforcement learning, providing improved robustness and performance in complex environments.<br><br>Summary: <div>
arXiv:2511.05694v1 Announce Type: new 
Abstract: A central challenge in reinforcement learning is that policies trained in controlled environments often fail under distribution shifts at deployment into real-world environments. Distributionally Robust Reinforcement Learning (DRRL) addresses this by optimizing for worst-case performance within an uncertainty set defined by a robustness budget $\epsilon$. However, fixing $\epsilon$ results in a tradeoff between performance and robustness: small values yield high nominal performance but weak robustness, while large values can result in instability and overly conservative policies. We propose Distributionally Robust Self-Paced Curriculum Reinforcement Learning (DR-SPCRL), a method that overcomes this limitation by treating $\epsilon$ as a continuous curriculum. DR-SPCRL adaptively schedules the robustness budget according to the agent's progress, enabling a balance between nominal and robust performance. Empirical results across multiple environments demonstrate that DR-SPCRL not only stabilizes training but also achieves a superior robustness-performance trade-off, yielding an average 11.8\% increase in episodic return under varying perturbations compared to fixed or heuristic scheduling strategies, and achieving approximately 1.9$\times$ the performance of the corresponding nominal RL algorithms.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-assisted workflow enables rapid, high-fidelity breast cancer clinical trial eligibility prescreening</title>
<link>https://arxiv.org/abs/2511.05696</link>
<guid>https://arxiv.org/abs/2511.05696</guid>
<content:encoded><![CDATA[
<div> AI, clinical trials, cancer care, eligibility screening, oncology

Summary: 
MSK-MATCH is an AI system developed for automated eligibility screening in cancer clinical trials. It integrates a large language model with an oncology trial knowledge base and retrieval-augmented architecture, providing explanations for all AI predictions based on source text. In a retrospective dataset of 88,518 clinical documents, MSK-MATCH achieved 98.6% accuracy in patient-level eligibility classification, resolving 61.9% of cases automatically and triaging the rest for human review. The AI-assisted workflow outperformed human-only and AI-only comparisons in sensitivity and specificity. For cases requiring manual review, prepopulating eligibility screens with AI-generated explanations reduced screening time significantly and cost only $0.96 per patient-trial pair. This innovative approach could improve participation rates in cancer clinical trials by streamlining eligibility screening processes. 

<br><br>Summary: <div>
arXiv:2511.05696v1 Announce Type: new 
Abstract: Clinical trials play an important role in cancer care and research, yet participation rates remain low. We developed MSK-MATCH (Memorial Sloan Kettering Multi-Agent Trial Coordination Hub), an AI system for automated eligibility screening from clinical text. MSK-MATCH integrates a large language model with a curated oncology trial knowledge base and retrieval-augmented architecture providing explanations for all AI predictions grounded in source text. In a retrospective dataset of 88,518 clinical documents from 731 patients across six breast cancer trials, MSK-MATCH automatically resolved 61.9% of cases and triaged 38.1% for human review. This AI-assisted workflow achieved 98.6% accuracy, 98.4% sensitivity, and 98.7% specificity for patient-level eligibility classification, matching or exceeding performance of the human-only and AI-only comparisons. For the triaged cases requiring manual review, prepopulating eligibility screens with AI-generated explanations reduced screening time from 20 minutes to 43 seconds at an average cost of $0.96 per patient-trial pair.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabDistill: Distilling Transformers into Neural Nets for Few-Shot Tabular Classification</title>
<link>https://arxiv.org/abs/2511.05704</link>
<guid>https://arxiv.org/abs/2511.05704</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer-based models, tabular data, few-shot regime, knowledge distillation, neural networks

Summary:
TabDistill introduces a strategy to distill the pre-trained knowledge from complex transformer-based models into simpler neural networks for classifying tabular data. This approach aims to achieve high performance with limited training data while remaining parameter-efficient. The distilled neural networks outperform classical baselines such as regular neural networks, XGBoost, and logistic regression with equal training data. In some cases, the distilled models even outperform the original transformer-based models they were derived from. This strategy bridges the gap between performance and complexity, providing a middle ground for effectively handling tabular data classification tasks. <div>
arXiv:2511.05704v1 Announce Type: new 
Abstract: Transformer-based models have shown promising performance on tabular data compared to their classical counterparts such as neural networks and Gradient Boosted Decision Trees (GBDTs) in scenarios with limited training data. They utilize their pre-trained knowledge to adapt to new domains, achieving commendable performance with only a few training examples, also called the few-shot regime. However, the performance gain in the few-shot regime comes at the expense of significantly increased complexity and number of parameters. To circumvent this trade-off, we introduce TabDistill, a new strategy to distill the pre-trained knowledge in complex transformer-based models into simpler neural networks for effectively classifying tabular data. Our framework yields the best of both worlds: being parameter-efficient while performing well with limited training data. The distilled neural networks surpass classical baselines such as regular neural networks, XGBoost and logistic regression under equal training data, and in some cases, even the original transformer-based models that they were distilled from.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributionally Robust Multimodal Machine Learning</title>
<link>https://arxiv.org/abs/2511.05716</link>
<guid>https://arxiv.org/abs/2511.05716</guid>
<content:encoded><![CDATA[
<div> Distributionally robust optimization, multimodal machine learning, uncertainty modeling, complexity analysis, performance guarantees
Summary:
1. The article addresses the problem of distributionally robust multimodal machine learning, proposing a novel framework that considers both theoretical and practical insights.
2. The framework justifies the setup and demonstrates the significance of the problem through complexity analysis.
3. Generalization upper bounds and minimax lower bounds are established to provide performance guarantees, which are further extended to settings involving encoder-specific error propogations.
4. Empirical results show that the proposed approach improves robustness in simulation settings and real-world datasets.
5. The findings offer a principled foundation for utilizing multimodal machine learning models in high-stakes applications where uncertainty is inevitable.
<br><br>Summary: <div>
arXiv:2511.05716v1 Announce Type: new 
Abstract: We consider the problem of distributionally robust multimodal machine learning. Existing approaches often rely on merging modalities on the feature level (early fusion) or heuristic uncertainty modeling, which downplays modality-aware ef- fects and provide limited insights. We propose a novel distributionally robust optimization (DRO) framework that aims to study both the theoretical and practical insights of multimodal machine learning. We first justify this setup and show the significance of this problem through complexity analysis. We then establish both generalization upper bounds and minimax lower bounds which provide perfor- mance guarantees. These results are further extended in settings where we consider encoder-specific error propogations. Empirically, we demonstrate that our approach improves robustness in both simulation settings and real-world datasets. Together, these findings provide a principled foundation for employing multimodal machine learning models in high-stakes applications where uncertainty is unavoidable.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GastroDL-Fusion: A Dual-Modal Deep Learning Framework Integrating Protein-Ligand Complexes and Gene Sequences for Gastrointestinal Disease Drug Discovery</title>
<link>https://arxiv.org/abs/2511.05726</link>
<guid>https://arxiv.org/abs/2511.05726</guid>
<content:encoded><![CDATA[
<div> Prediction, protein-ligand binding affinity, deep learning, gastrointestinal diseases, drug development

Summary:
- The article introduces GastroDL-Fusion, a dual-modal deep learning framework that combines protein-ligand complex data with disease-associated gene sequence information for drug discovery in gastrointestinal diseases.
- GastroDL-Fusion integrates molecular graph representations of protein-ligand complexes with gene sequence embeddings generated by a pre-trained Transformer model.
- The model utilizes a Graph Isomorphism Network (GIN) for the protein-ligand complex data and a multi-layer perceptron for cross-modal interaction learning.
- Evaluation on GI disease-related targets shows that GastroDL-Fusion outperforms traditional methods, achieving a mean absolute error (MAE) of 1.12 and a root mean square error (RMSE) of 1.75.
- Incorporating both structural and genetic features leads to more accurate predictions of binding affinities, making GastroDL-Fusion a valuable tool for designing targeted therapies and vaccines in the context of gastrointestinal diseases.

<br><br>Summary: <div>
arXiv:2511.05726v1 Announce Type: new 
Abstract: Accurate prediction of protein-ligand binding affinity plays a pivotal role in accelerating the discovery of novel drugs and vaccines, particularly for gastrointestinal (GI) diseases such as gastric ulcers, Crohn's disease, and ulcerative colitis. Traditional computational models often rely on structural information alone and thus fail to capture the genetic determinants that influence disease mechanisms and therapeutic responses. To address this gap, we propose GastroDL-Fusion, a dual-modal deep learning framework that integrates protein-ligand complex data with disease-associated gene sequence information for drug and vaccine development. In our approach, protein-ligand complexes are represented as molecular graphs and modeled using a Graph Isomorphism Network (GIN), while gene sequences are encoded into biologically meaningful embeddings via a pre-trained Transformer (ProtBERT/ESM). These complementary modalities are fused through a multi-layer perceptron to enable robust cross-modal interaction learning. We evaluate the model on benchmark datasets of GI disease-related targets, demonstrating that GastroDL-Fusion significantly improves predictive performance over conventional methods. Specifically, the model achieves a mean absolute error (MAE) of 1.12 and a root mean square error (RMSE) of 1.75, outperforming CNN, BiLSTM, GIN, and Transformer-only baselines. These results confirm that incorporating both structural and genetic features yields more accurate predictions of binding affinities, providing a reliable computational tool for accelerating the design of targeted therapies and vaccines in the context of gastrointestinal diseases.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compressing Chemistry Reveals Functional Groups</title>
<link>https://arxiv.org/abs/2511.05728</link>
<guid>https://arxiv.org/abs/2511.05728</guid>
<content:encoded><![CDATA[
<div> Keywords: chemical functional groups, unsupervised learning, Minimum Message Length, bioactivity prediction, fingerprint representations

Summary:
In this study, the utility of traditional chemical functional groups in chemical explanations was formally assessed using an unsupervised learning algorithm based on the Minimum Message Length principle. The algorithm searched for substructures that compress around three million biologically relevant molecules, discovering both human-curated functional groups and novel larger patterns with specific functions. The algorithm was then applied to 24 bioactivity prediction datasets to find dataset-specific functional groups. Fingerprints constructed from these dataset-specific functional groups outperformed other fingerprint representations in bioactivity regression tasks. This research showcases the effectiveness of using unsupervised learning algorithms and the Minimum Message Length principle in identifying functional groups and improving bioactivity prediction models. <div>
arXiv:2511.05728v1 Announce Type: new 
Abstract: We introduce the first formal large-scale assessment of the utility of traditional chemical functional groups as used in chemical explanations. Our assessment employs a fundamental principle from computational learning theory: a good explanation of data should also compress the data. We introduce an unsupervised learning algorithm based on the Minimum Message Length (MML) principle that searches for substructures that compress around three million biologically relevant molecules. We demonstrate that the discovered substructures contain most human-curated functional groups as well as novel larger patterns with more specific functions. We also run our algorithm on 24 specific bioactivity prediction datasets to discover dataset-specific functional groups. Fingerprints constructed from dataset-specific functional groups are shown to significantly outperform other fingerprint representations, including the MACCS and Morgan fingerprint, when training ridge regression models on bioactivity regression tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QiVC-Net: Quantum-Inspired Variational Convolutional Network, with Application to Biosignal Classification</title>
<link>https://arxiv.org/abs/2511.05730</link>
<guid>https://arxiv.org/abs/2511.05730</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantum-inspired variational convolution, probabilistic inference, ensemble rotation, uncertainty modeling, biosignal classification 

Summary:<br>
The Quantum-inspired Variational Convolution (QiVC) framework combines probabilistic inference and quantum-inspired transformations within convolutional architectures. The QiVC framework introduces the Quantum-inspired Rotated Ensemble (QiRE) mechanism, which performs differentiable subspace rotations of convolutional weights akin to quantum state evolution. This allows for structured uncertainty modeling while maintaining parameter space geometry, resulting in more expressive and stable representations. The proposed QiVC-Net, with a QiVC layer that does not add parameters, achieves state-of-the-art performance in biosignal classification tasks, particularly in challenging domains like phonocardiogram (PCG) recordings. The QiVC-Net demonstrates high accuracy on benchmark datasets, showcasing its promise in uncertainty-aware modeling for real-world biomedical signal analysis. Implementation of the QiVC layer is publicly available on GitHub. 

<br><br>Summary: <div>
arXiv:2511.05730v1 Announce Type: new 
Abstract: This work introduces the quantum-inspired variational convolution (QiVC) framework, a novel learning paradigm that integrates principles of probabilistic inference, variational optimization, and quantum-inspired transformations within convolutional architectures. The central innovation of QiVC lies in its quantum-inspired rotated ensemble (QiRE) mechanism. QiRE performs differentiable low-dimensional subspace rotations of convolutional weights, analogously to quantum state evolution. This approach enables structured uncertainty modeling while preserving the intrinsic geometry of the parameter space, resulting in more expressive, stable, and uncertainty-aware representations. To demonstrate its practical potential, the concept is instantiated in a QiVC-based convolutional network (QiVC-Net) and evaluated in the context of biosignal classification, focusing on phonocardiogram (PCG) recordings, a challenging domain characterized by high noise, inter-subject variability, and often imbalanced data. The proposed QiVC-Net integrates an architecture in which the QiVC layer does not introduce additional parameters, instead performing an ensemble rotation of the convolutional weights through a structured mechanism ensuring robustness without added highly computational burden. Experiments on two benchmark datasets, PhysioNet CinC 2016 and PhysioNet CirCor DigiScope 2022, show that QiVC-Net achieves state-of-the-art performance, reaching accuracies of 97.84% and 97.89%, respectively. These findings highlight the versatility of the QiVC framework and its promise for advancing uncertainty-aware modeling in real-world biomedical signal analysis. The implementation of the QiVConv layer is openly available in GitHub.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-Exponential Savings for Mean Estimation with Active Learning</title>
<link>https://arxiv.org/abs/2511.05736</link>
<guid>https://arxiv.org/abs/2511.05736</guid>
<content:encoded><![CDATA[
<div> Algorithm, Active Learning, Estimation, Covariates, PartiBandits <br>
<br>
The study presents an active learning algorithm called PartiBandits for efficient estimation of the mean of a k-class random variable with limited labels and access to informative covariates. The algorithm provides an estimate of the mean with an error that decreases with the number of labels N. It consists of two stages - learning a partition of unlabeled data to reduce conditional variance and using a UCB-style subroutine for label requests. The convergence rates of both the algorithm and subroutine are optimal. PartiBandits combines UCB and disagreement-based approaches in active learning, demonstrating versatility. Simulation using electronic health records showcases the algorithm's effectiveness. The PartiBandits package in R enables easy implementation of the proposed method. <br><br>Summary: <div>
arXiv:2511.05736v1 Announce Type: new 
Abstract: We study the problem of efficiently estimating the mean of a $k$-class random variable, $Y$, using a limited number of labels, $N$, in settings where the analyst has access to auxiliary information (i.e.: covariates) $X$ that may be informative about $Y$. We propose an active learning algorithm ("PartiBandits") to estimate $\mathbb{E}[Y]$. The algorithm yields an estimate, $\widehat{\mu}_{\text{PB}}$, such that $\left( \widehat{\mu}_{\text{PB}} - \mathbb{E}[Y]\right)^2$ is $\tilde{\mathcal{O}}\left( \frac{\nu + \exp(c \cdot (-N/\log(N))) }{N} \right)$, where $c > 0$ is a constant and $\nu$ is the risk of the Bayes-optimal classifier. PartiBandits is essentially a two-stage algorithm. In the first stage, it learns a partition of the unlabeled data that shrinks the average conditional variance of $Y$. In the second stage it uses a UCB-style subroutine ("WarmStart-UCB") to request labels from each stratum round-by-round. Both the main algorithm's and the subroutine's convergence rates are minimax optimal in classical settings. PartiBandits bridges the UCB and disagreement-based approaches to active learning despite these two approaches being designed to tackle very different tasks. We illustrate our methods through simulation using nationwide electronic health records. Our methods can be implemented using the PartiBandits package in R.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Redundancy: Diverse and Specialized Multi-Expert Sparse Autoencoder</title>
<link>https://arxiv.org/abs/2511.05745</link>
<guid>https://arxiv.org/abs/2511.05745</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse autoencoders, large language models, interpretability, Mixture of Experts, feature redundancy

Summary: 
Sparse autoencoders (SAEs) are important for interpreting large language models (LLMs) by breaking down token activations into understandable features. However, the high dimensionality of SAEs for better interpretability leads to costly training and inference. Mixture of Experts (MoE) techniques aim to address this by splitting SAEs into narrower expert networks with gated activation to reduce computation. Nonetheless, MoE-SAE faces a limitation where experts often do not specialize and learn overlapping features. To tackle this issue, the authors introduce Multiple Expert Activation to engage semantically weighted expert subsets simultaneously and Feature Scaling for diverse feature learning via adaptive scaling. Experimental results show improved performance with a 24% lower reconstruction error and 99% less feature redundancy compared to existing MoE-SAE methods. This work bridges the gap between interpretability and efficiency in LLM analysis by enabling transparent model investigation while maintaining computational feasibility.<br><br>Summary: <div>
arXiv:2511.05745v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting large language models (LLMs) by decomposing token activations into combinations of human-understandable features. While SAEs provide crucial insights into LLM explanations, their practical adoption faces a fundamental challenge: better interpretability demands that SAEs' hidden layers have high dimensionality to satisfy sparsity constraints, resulting in prohibitive training and inference costs. Recent Mixture of Experts (MoE) approaches attempt to address this by partitioning SAEs into narrower expert networks with gated activation, thereby reducing computation. In a well-designed MoE, each expert should focus on learning a distinct set of features. However, we identify a \textit{critical limitation} in MoE-SAE: Experts often fail to specialize, which means they frequently learn overlapping or identical features. To deal with it, we propose two key innovations: (1) Multiple Expert Activation that simultaneously engages semantically weighted expert subsets to encourage specialization, and (2) Feature Scaling that enhances diversity through adaptive high-frequency scaling. Experiments demonstrate a 24\% lower reconstruction error and a 99\% reduction in feature redundancy compared to existing MoE-SAE methods. This work bridges the interpretability-efficiency gap in LLM analysis, allowing transparent model inspection without compromising computational feasibility.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Primal-Only Actor Critic Algorithm for Robust Constrained Average Cost MDPs</title>
<link>https://arxiv.org/abs/2511.05758</link>
<guid>https://arxiv.org/abs/2511.05758</guid>
<content:encoded><![CDATA[
<div> Average-Cost RCMDPs, Robust policies, Actor-critic algorithm, Feasibility, Optimality <br>
<br>
Summary: 
In the study of Robust Constrained Average-Cost Markov Decision Processes (RCMDPs), the lack of strong duality and non-contractive properties of the Robust Bellman operator pose challenges for traditional methods. To tackle these obstacles, an actor-critic algorithm for Average-Cost RCMDPs is proposed in this work. The algorithm is shown to achieve both feasibility and optimality within certain bounds, with sample complexities comparable to the discounted setting. The method addresses the difficulties of finding robust and safe policies in a complex environment, showcasing promising results in overcoming the limitations of standard primal-dual approaches. This research provides a valuable contribution to the field of constrained reinforcement learning, offering a new perspective on tackling challenging problems in Markov decision processes. <br><br> <div>
arXiv:2511.05758v1 Announce Type: new 
Abstract: In this work, we study the problem of finding robust and safe policies in Robust Constrained Average-Cost Markov Decision Processes (RCMDPs). A key challenge in this setting is the lack of strong duality, which prevents the direct use of standard primal-dual methods for constrained RL. Additional difficulties arise from the average-cost setting, where the Robust Bellman operator is not a contraction under any norm. To address these challenges, we propose an actor-critic algorithm for Average-Cost RCMDPs. We show that our method achieves both \(\epsilon\)-feasibility and \(\epsilon\)-optimality, and we establish a sample complexities of \(\tilde{O}\left(\epsilon^{-4}\right)\) and \(\tilde{O}\left(\epsilon^{-6}\right)\) with and without slackness assumption, which is comparable to the discounted setting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Gradient-Aware Error-Bounded Lossy Compressor for Federated Learning</title>
<link>https://arxiv.org/abs/2511.05770</link>
<guid>https://arxiv.org/abs/2511.05770</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Error-Bounded Lossy Compression, Gradient Data, Communication Efficiency, Model Accuracy

Summary: 
An innovative error-bounded lossy compression framework tailored for Federated Learning (FL) gradient data is proposed to address communication efficiency in collaborative model training. The framework leverages temporal correlations across FL training rounds and structural regularities within convolutional kernels for high compression ratios. It includes a magnitude predictor based on a normalized exponential moving average and a sign predictor leveraging gradient oscillation and kernel-level consistency. Experimental results show up to 1.53x higher compression ratios compared to existing methods with lower accuracy loss. Integrated into a real-world FL framework, APPFL, the new framework reduces end-to-end communication time by 76.1%-96.2% under constrained-bandwidth scenarios, demonstrating scalability for practical FL deployments. <div>
arXiv:2511.05770v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training without exposing clients' private data, but its deployment is often constrained by the communication cost of transmitting gradients between clients and the central server, especially under system heterogeneity where low-bandwidth clients bottleneck overall performance. Lossy compression of gradient data can mitigate this overhead, and error-bounded lossy compression (EBLC) is particularly appealing for its fine-grained utility-compression tradeoff. However, existing EBLC methods (e.g., SZ), originally designed for smooth scientific data with strong spatial locality, rely on generic predictors such as Lorenzo and interpolation for entropy reduction to improve compression ratio. Gradient tensors, in contrast, exhibit low smoothness and weak spatial correlation, rendering these predictors ineffective and leading to poor compression ratios. To address this limitation, we propose an EBLC framework tailored for FL gradient data to achieve high compression ratios while preserving model accuracy. The core of it is an innovative prediction mechanism that exploits temporal correlations across FL training rounds and structural regularities within convolutional kernels to reduce residual entropy. The predictor is compatible with standard quantizers and entropy coders and comprises (1) a cross-round magnitude predictor based on a normalized exponential moving average, and (2) a sign predictor that leverages gradient oscillation and kernel-level sign consistency. Experiments show that this new EBLC yields up to 1.53x higher compression ratios than SZ3 with lower accuracy loss. Integrated into a real-world FL framework, APPFL, it reduces end-to-end communication time by 76.1%-96.2% under various constrained-bandwidth scenarios, demonstrating strong scalability for real-world FL deployments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARAuder's Map: Motion-Aware Real-time Activity Recognition with Layout-Based Trajectories</title>
<link>https://arxiv.org/abs/2511.05773</link>
<guid>https://arxiv.org/abs/2511.05773</guid>
<content:encoded><![CDATA[
<div> framework, real-time, activity recognition, sensor streams, deep learning 
Summary:
The paper introduces MARAuder's Map, a framework for real-time human activity recognition in smart homes using raw sensor data. The method projects sensor activations onto a physical floorplan to create trajectory-aware image sequences, capturing the spatial flow of human movement. A hybrid deep learning model is employed to process these representations, incorporating spatial structure and temporal dependencies. A learnable time embedding module encodes contextual cues like hour-of-day and day-of-week to enhance temporal awareness. An attention-based encoder focuses on informative segments within observation windows, enabling accurate recognition even during cross-activity transitions and temporal ambiguity. Extensive experiments on real-world smart home datasets demonstrate the superiority of the proposed method over existing baselines, offering a practical solution for real-time human activity recognition in ambient sensor environments. 
<br><br>Summary: <div>
arXiv:2511.05773v1 Announce Type: new 
Abstract: Ambient sensor-based human activity recognition (HAR) in smart homes remains challenging due to the need for real-time inference, spatially grounded reasoning, and context-aware temporal modeling. Existing approaches often rely on pre-segmented, within-activity data and overlook the physical layout of the environment, limiting their robustness in continuous, real-world deployments. In this paper, we propose MARAuder's Map, a novel framework for real-time activity recognition from raw, unsegmented sensor streams. Our method projects sensor activations onto the physical floorplan to generate trajectory-aware, image-like sequences that capture the spatial flow of human movement. These representations are processed by a hybrid deep learning model that jointly captures spatial structure and temporal dependencies. To enhance temporal awareness, we introduce a learnable time embedding module that encodes contextual cues such as hour-of-day and day-of-week. Additionally, an attention-based encoder selectively focuses on informative segments within each observation window, enabling accurate recognition even under cross-activity transitions and temporal ambiguity. Extensive experiments on multiple real-world smart home datasets demonstrate that our method outperforms strong baselines, offering a practical solution for real-time HAR in ambient sensor environments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SymLight: Exploring Interpretable and Deployable Symbolic Policies for Traffic Signal Control</title>
<link>https://arxiv.org/abs/2511.05790</link>
<guid>https://arxiv.org/abs/2511.05790</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Reinforcement Learning, Traffic Signal Control, Monte Carlo Tree Search, Interpretable Policies, Symbolic Priority Functions

Summary:
SymLight introduces a framework based on Monte Carlo Tree Search (MCTS) to discover interpretable symbolic priority functions for traffic signal control policies. These priority functions take traffic features as input and output priorities for each traffic signal phase, allowing for effective phase transition. The framework includes a concise representation for the priority function to handle the complexity of the action space in MCTS. Additionally, a probabilistic structural rollout strategy uses patterns from high-quality priority functions to guide the search process. Experimental results show SymLight outperforms baselines on real-world datasets, producing interpretable policies that can be deployed effectively while maintaining high performance. Overall, SymLight offers a promising approach for generating efficient and understandable traffic signal control policies using deep reinforcement learning. 

<br><br>Summary: <div>
arXiv:2511.05790v1 Announce Type: new 
Abstract: Deep Reinforcement Learning have achieved significant success in automatically devising effective traffic signal control (TSC) policies. Neural policies, however, tend to be over-parameterized and non-transparent, hindering their interpretability and deployability on resource-limited edge devices. This work presents SymLight, a priority function search framework based on Monte Carlo Tree Search (MCTS) for discovering inherently interpretable and deployable symbolic priority functions to serve as the TSC policies. The priority function, in particular, accepts traffic features as input and then outputs a priority for each traffic signal phase, which subsequently directs the phase transition. For effective search, we propose a concise yet expressive priority function representation. This helps mitigate the combinatorial explosion of the action space in MCTS. Additionally, a probabilistic structural rollout strategy is introduced to leverage structural patterns from previously discovered high-quality priority functions, guiding the rollout process. Our experiments on real-world datasets demonstrate SymLight's superior performance across a range of baselines. A key advantage is SymLight's ability to produce interpretable and deployable TSC policies while maintaining excellent performance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Lower Bound: Bridging Regret Minimization and Best Arm Identification in Lexicographic Bandits</title>
<link>https://arxiv.org/abs/2511.05802</link>
<guid>https://arxiv.org/abs/2511.05802</guid>
<content:encoded><![CDATA[
<div> bandit algorithms, multi-objective decision-making, lexicographic preferences, regret minimization, best arm identification

Summary: In the context of multi-objective decision-making with hierarchical preferences, this study introduces two elimination-based algorithms that balance regret minimization and best arm identification under lexicographic preferences. The first algorithm sequentially eliminates suboptimal arms based on priority objectives, achieving competitive sample complexity and regret bounds. The second algorithm leverages cross-objective dependencies to outperform the lower bound for single-objective bandit problems. Empirical results confirm the superior performance of these algorithms over baseline methods, highlighting the effectiveness of incorporating cross-objective information sharing in the multi-objective setting. <div>
arXiv:2511.05802v1 Announce Type: new 
Abstract: In multi-objective decision-making with hierarchical preferences, lexicographic bandits provide a natural framework for optimizing multiple objectives in a prioritized order. In this setting, a learner repeatedly selects arms and observes reward vectors, aiming to maximize the reward for the highest-priority objective, then the next, and so on. While previous studies have primarily focused on regret minimization, this work bridges the gap between \textit{regret minimization} and \textit{best arm identification} under lexicographic preferences. We propose two elimination-based algorithms to address this joint objective. The first algorithm eliminates suboptimal arms sequentially, layer by layer, in accordance with the objective priorities, and achieves sample complexity and regret bounds comparable to those of the best single-objective algorithms. The second algorithm simultaneously leverages reward information from all objectives in each round, effectively exploiting cross-objective dependencies. Remarkably, it outperforms the known lower bound for the single-objective bandit problem, highlighting the benefit of cross-objective information sharing in the multi-objective setting. Empirical results further validate their superior performance over baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Catching Contamination Before Generation: Spectral Kill Switches for Agents</title>
<link>https://arxiv.org/abs/2511.05804</link>
<guid>https://arxiv.org/abs/2511.05804</guid>
<content:encoded><![CDATA[
<div> detect, agentic language models, attention, spectral statistics, context verification  
Summary:  
The article introduces a diagnostic method for agentic language models that can detect errors in reasoning chains caused by inconsistent context, retrieval errors, or adversarial inputs. The method analyzes token graphs induced by attention in early layers and computes spectral statistics, specifically the high frequency energy ratio and spectral entropy. Using a two-regime mixture assumption, the method shows that a single threshold on the high frequency energy ratio is optimal for detecting context inconsistency. Empirical results demonstrate the method's robustness in context verification across different model families, with low processing overhead. The approach can be integrated into retrieval augmented agent pipelines and used as an inline safety monitor to detect contamination in the text while the model is still processing it, preventing errors from propagating in the reasoning chain.<br><br>Summary: <div>
arXiv:2511.05804v1 Announce Type: new 
Abstract: Agentic language models compose multi step reasoning chains, yet intermediate steps can be corrupted by inconsistent context, retrieval errors, or adversarial inputs, which makes post hoc evaluation too late because errors propagate before detection. We introduce a diagnostic that requires no additional training and uses only the forward pass to emit a binary accept or reject signal during agent execution. The method analyzes token graphs induced by attention and computes two spectral statistics in early layers, namely the high frequency energy ratio and spectral entropy. We formalize these signals, establish invariances, and provide finite sample estimators with uncertainty quantification. Under a two regime mixture assumption with a monotone likelihood ratio property, we show that a single threshold on the high frequency energy ratio is optimal in the Bayes sense for detecting context inconsistency. Empirically, the high frequency energy ratio exhibits robust bimodality during context verification across multiple model families, which enables gating decisions with overhead below one millisecond on our hardware and configurations. We demonstrate integration into retrieval augmented agent pipelines and discuss deployment as an inline safety monitor. The approach detects contamination while the model is still processing the text, before errors commit to the reasoning chain.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Model Performance in the Presence of an Intervention</title>
<link>https://arxiv.org/abs/2511.05805</link>
<guid>https://arxiv.org/abs/2511.05805</guid>
<content:encoded><![CDATA[
<div> Randomized controlled trials, model evaluation, bias, intervention, nuisance parameter weighting

Summary:
- AI models are often evaluated based on their predictive ability, but interventions in social impact applications can bias evaluations.
- RCTs randomize interventions, but traditional approaches ignore data from the treatment group, leading to inefficiencies.
- The study quantifies bias from aggregating performance estimates from treatment and control groups and proposes nuisance parameter weighting (NPW) for unbiased model evaluation.
- NPW reweights treatment group data to mimic distributions under no intervention, improving model selection compared to standard approaches.
- Synthetic and real-world datasets demonstrate the effectiveness of NPW in various intervention effect and sample size scenarios. 

<br><br>Summary: <div>
arXiv:2511.05805v1 Announce Type: new 
Abstract: AI models are often evaluated based on their ability to predict the outcome of interest. However, in many AI for social impact applications, the presence of an intervention that affects the outcome can bias the evaluation. Randomized controlled trials (RCTs) randomly assign interventions, allowing data from the control group to be used for unbiased model evaluation. However, this approach is inefficient because it ignores data from the treatment group. Given the complexity and cost often associated with RCTs, making the most use of the data is essential. Thus, we investigate model evaluation strategies that leverage all data from an RCT. First, we theoretically quantify the estimation bias that arises from na\"ively aggregating performance estimates from treatment and control groups, and derive the condition under which this bias leads to incorrect model selection. Leveraging these theoretical insights, we propose nuisance parameter weighting (NPW), an unbiased model evaluation approach that reweights data from the treatment group to mimic the distributions of samples that would or would not experience the outcome under no intervention. Using synthetic and real-world datasets, we demonstrate that our proposed evaluation approach consistently yields better model selection than the standard approach, which ignores data from the treatment group, across various intervention effect and sample size settings. Our contribution represents a meaningful step towards more efficient model evaluation in real-world contexts.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling</title>
<link>https://arxiv.org/abs/2511.05811</link>
<guid>https://arxiv.org/abs/2511.05811</guid>
<content:encoded><![CDATA[
<div> Keywords: FP8 training, numerical stability, two-level microscaling, automatic scaling, training efficiency

Summary: 
MOSS is a new FP8 training framework designed to address challenges in efficiency and numerical stability. It introduces a two-level microscaling strategy for quantizing sensitive activations, combining global and local scales to balance precision and dequantization cost effectively. Additionally, MOSS implements automatic scaling for weights in linear layers, eliminating costly max-reduction operations by predicting and adjusting scaling factors during training. These innovations allow for efficient FP8 training of large language models, achieving performance comparable to BF16 baseline models while achieving up to 34% higher training throughput.MOSS improves efficiency and stability in FP8 training by optimizing quantization strategies for activations and weights, leading to significant performance gains in training large language models. 

<br><br>Summary: <div>
arXiv:2511.05811v1 Announce Type: new 
Abstract: Training large language models with FP8 formats offers significant efficiency gains. However, the reduced numerical precision of FP8 poses challenges for stable and accurate training. Current frameworks preserve training performance using mixed-granularity quantization, i.e., applying per-group quantization for activations and per-tensor/block quantization for weights. While effective, per-group quantization requires scaling along the inner dimension of matrix multiplication, introducing additional dequantization overhead. Moreover, these frameworks often rely on just-in-time scaling to dynamically adjust scaling factors based on the current data distribution. However, this online quantization is inefficient for FP8 training, as it involves multiple memory reads and writes that negate the performance benefits of FP8. To overcome these limitations, we propose MOSS, a novel FP8 training framework that ensures both efficiency and numerical stability. MOSS introduces two key innovations: (1) a two-level microscaling strategy for quantizing sensitive activations, which balances precision and dequantization cost by combining a high-precision global scale with compact, power-of-two local scales; and (2) automatic scaling for weights in linear layers, which eliminates the need for costly max-reduction operations by predicting and adjusting scaling factors during training. Leveraging these techniques, MOSS enables efficient FP8 training of a 7B parameter model, achieving performance comparable to the BF16 baseline while achieving up to 34% higher training throughput.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-depth Analysis on Caching and Pre-fetching in Mixture of Experts Offloading</title>
<link>https://arxiv.org/abs/2511.05814</link>
<guid>https://arxiv.org/abs/2511.05814</guid>
<content:encoded><![CDATA[
<div> Memory Optimization, Mixture of Experts, Offloading, Caching, Pre-fetching
Summary:
This study delves into the challenges of deploying Mixture of Experts (MoE) models due to their high memory requirements. The authors analyze expert activation and caching behavior, proposing a Least Frequently Used (LFU) caching optimization that outperforms the traditional Least Recently Used (LRU) approach. Furthermore, they explore the implementation and potential benefits of speculative expert pre-fetching. The study provides insights into the behavior of the MoE architecture, shedding light on the characteristics of the gating network and experts. These findings not only enhance our understanding of MoE models but also lay the groundwork for future research on interpreting MoE models and developing pruning techniques with minimal performance impact. 
<br><br>Summary: <div>
arXiv:2511.05814v1 Announce Type: new 
Abstract: In today's landscape, Mixture of Experts (MoE) is a crucial architecture that has been used by many of the most advanced models. One of the major challenges of MoE models is that they usually require much more memory than their dense counterparts due to their unique architecture, and hence are harder to deploy in environments with limited GPU memory, such as edge devices. MoE offloading is a promising technique proposed to overcome this challenge, especially if it is enhanced with caching and pre-fetching, but prior work stopped at suboptimal caching algorithm and offered limited insights. In this work, we study MoE offloading in depth and make the following contributions: 1. We analyze the expert activation and LRU caching behavior in detail and provide traces. 2. We propose LFU caching optimization based on our analysis and obtain strong improvements from LRU. 3. We implement and experiment speculative expert pre-fetching, providing detailed trace showing its huge potential . 4. In addition, our study extensively covers the behavior of the MoE architecture itself, offering information on the characteristic of the gating network and experts. This can inspire future work on the interpretation of MoE models and the development of pruning techniques for MoE architecture with minimal performance loss.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AiEDA: An Open-Source AI-Aided Design Library for Design-to-Vector</title>
<link>https://arxiv.org/abs/2511.05823</link>
<guid>https://arxiv.org/abs/2511.05823</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, electronic design automation, chip design, unified library, data pipeline

Summary:
AiEDA is a new open-source library for electronic design automation (EDA) that addresses the fragmented nature of current AI-EDA infrastructures. It integrates various design-to-vector data representation techniques to create universal multi-level vector representations, optimizing the AI-aided design (AAD) workflow. The library provides complete physical design flows with standardized Python interfaces for seamless integration of EDA datasets and AI frameworks. AiEDA has been used to generate a large dataset called iDATA, derived from 50 real chip designs at 28nm, and has been validated through various AAD tasks such as prediction, generation, optimization, and analysis. The code for AiEDA is available on GitHub, and the iDATA dataset is in the process of being released publicly. This development lays the foundation for future research in AI-EDA. 

<br><br>Summary: AiEDA is a unified open-source library for electronic design automation that addresses current infrastructure challenges, providing multi-level vector representations and standardized Python interfaces. It has been used to generate and validate the iDATA dataset for real chip designs, facilitating various AAD tasks and laying the groundwork for future AI-EDA research. <div>
arXiv:2511.05823v1 Announce Type: new 
Abstract: Recent research has demonstrated that artificial intelligence (AI) can assist electronic design automation (EDA) in improving both the quality and efficiency of chip design. But current AI for EDA (AI-EDA) infrastructures remain fragmented, lacking comprehensive solutions for the entire data pipeline from design execution to AI integration. Key challenges include fragmented flow engines that generate raw data, heterogeneous file formats for data exchange, non-standardized data extraction methods, and poorly organized data storage. This work introduces a unified open-source library for EDA (AiEDA) that addresses these issues. AiEDA integrates multiple design-to-vector data representation techniques that transform diverse chip design data into universal multi-level vector representations, establishing an AI-aided design (AAD) paradigm optimized for AI-EDA workflows. AiEDA provides complete physical design flows with programmatic data extraction and standardized Python interfaces bridging EDA datasets and AI frameworks. Leveraging the AiEDA library, we generate iDATA, a 600GB dataset of structured data derived from 50 real chip designs (28nm), and validate its effectiveness through seven representative AAD tasks spanning prediction, generation, optimization and analysis. The code is publicly available at https://github.com/OSCC-Project/AiEDA, while the full iDATA dataset is being prepared for public release, providing a foundation for future AI-EDA research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADM: Cluster-customized Adaptive Distance Metric for Categorical Data Clustering</title>
<link>https://arxiv.org/abs/2511.05826</link>
<guid>https://arxiv.org/abs/2511.05826</guid>
<content:encoded><![CDATA[
<div> distance metric, categorical data clustering, cluster-customized, mixed data, experiments

Summary:
The article introduces a new approach for clustering categorical data by proposing a cluster-customized distance metric. Traditional distance metrics do not consider the varying distances between attribute values in different clusters, leading to inaccurate measurements. The proposed method updates distances based on the distributions of attributes within each cluster, resulting in more precise clustering. Additionally, the distance metric is extended to handle mixed data with numerical and categorical attributes. Experimental results demonstrate the effectiveness of the proposed approach, consistently achieving top rankings across multiple datasets. The source code for the method is also made available for further research and implementation. <div>
arXiv:2511.05826v1 Announce Type: new 
Abstract: An appropriate distance metric is crucial for categorical data clustering, as the distance between categorical data cannot be directly calculated. However, the distances between attribute values usually vary in different clusters induced by their different distributions, which has not been taken into account, thus leading to unreasonable distance measurement. Therefore, we propose a cluster-customized distance metric for categorical data clustering, which can competitively update distances based on different distributions of attributes in each cluster. In addition, we extend the proposed distance metric to the mixed data that contains both numerical and categorical attributes. Experiments demonstrate the efficacy of the proposed method, i.e., achieving an average ranking of around first in fourteen datasets. The source code is available at https://anonymous.4open.science/r/CADM-47D8
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting the Future by Retrieving the Past</title>
<link>https://arxiv.org/abs/2511.05859</link>
<guid>https://arxiv.org/abs/2511.05859</guid>
<content:encoded><![CDATA[

arXiv:2511.05859v1 Announce Type: new 
Abstract: Deep learning models such as MLP, Transformer, and TCN have achieved remarkable success in univariate time series forecasting, typically relying on sliding window samples from historical data for training. However, while these models implicitly compress historical information into their parameters during training, they are unable to explicitly and dynamically access this global knowledge during inference, relying only on the local context within the lookback window. This results in an underutilization of rich patterns from the global history. To bridge this gap, we propose Predicting the Future by Retrieving the Past (PFRP), a novel approach that explicitly integrates global historical data to enhance forecasting accuracy. Specifically, we construct a Global Memory Bank (GMB) to effectively store and manage global historical patterns. A retrieval mechanism is then employed to extract similar patterns from the GMB, enabling the generation of global predictions. By adaptively combining these global predictions with the outputs of any local prediction model, PFRP produces more accurate and interpretable forecasts. Extensive experiments conducted on seven real-world datasets demonstrate that PFRP significantly enhances the average performance of advanced univariate forecasting models by 8.4\%. Codes can be found in https://github.com/ddz16/PFRP.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMOD: A Unified EEG Emotion Representation Framework Leveraging V-A Guided Contrastive Learning</title>
<link>https://arxiv.org/abs/2511.05863</link>
<guid>https://arxiv.org/abs/2511.05863</guid>
<content:encoded><![CDATA[

arXiv:2511.05863v1 Announce Type: new 
Abstract: Emotion recognition from EEG signals is essential for affective computing and has been widely explored using deep learning. While recent deep learning approaches have achieved strong performance on single EEG emotion datasets, their generalization across datasets remains limited due to the heterogeneity in annotation schemes and data formats. Existing models typically require dataset-specific architectures tailored to input structure and lack semantic alignment across diverse emotion labels. To address these challenges, we propose EMOD: A Unified EEG Emotion Representation Framework Leveraging Valence-Arousal (V-A) Guided Contrastive Learning. EMOD learns transferable and emotion-aware representations from heterogeneous datasets by bridging both semantic and structural gaps. Specifically, we project discrete and continuous emotion labels into a unified V-A space and formulate a soft-weighted supervised contrastive loss that encourages emotionally similar samples to cluster in the latent space. To accommodate variable EEG formats, EMOD employs a flexible backbone comprising a Triple-Domain Encoder followed by a Spatial-Temporal Transformer, enabling robust extraction and integration of temporal, spectral, and spatial features. We pretrain EMOD on eight public EEG datasets and evaluate its performance on three benchmark datasets. Experimental results show that EMOD achieves state-of-the-art performance, demonstrating strong adaptability and generalization across diverse EEG-based emotion recognition scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptation and Fine-tuning with TabPFN for Travelling Salesman Problem</title>
<link>https://arxiv.org/abs/2511.05872</link>
<guid>https://arxiv.org/abs/2511.05872</guid>
<content:encoded><![CDATA[

arXiv:2511.05872v1 Announce Type: new 
Abstract: Tabular Prior-Data Fitted Network (TabPFN) is a foundation model designed for small to medium-sized tabular data, which has attracted much attention recently. This paper investigates the application of TabPFN in Combinatorial Optimization (CO) problems. The aim is to lessen challenges in time and data-intensive training requirements often observed in using traditional methods including exact and heuristic algorithms, Machine Learning (ML)-based models, to solve CO problems. Proposing possibly the first ever application of TabPFN for such a purpose, we adapt and fine-tune the TabPFN model to solve the Travelling Salesman Problem (TSP), one of the most well-known CO problems. Specifically, we adopt the node-based approach and the node-predicting adaptation strategy to construct the entire TSP route. Our evaluation with varying instance sizes confirms that TabPFN requires minimal training, adapts to TSP using a single sample, performs better generalization across varying TSP instance sizes, and reduces performance degradation. Furthermore, the training process with adaptation and fine-tuning is completed within minutes. The methodology leads to strong solution quality even without post-processing and achieves performance comparable to other models with post-processing refinement. Our findings suggest that the TabPFN model is a promising approach to solve structured and CO problems efficiently under training resource constraints and rapid deployment requirements.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FusionLog: Cross-System Log-based Anomaly Detection via Fusion of General and Proprietary Knowledge</title>
<link>https://arxiv.org/abs/2511.05878</link>
<guid>https://arxiv.org/abs/2511.05878</guid>
<content:encoded><![CDATA[

arXiv:2511.05878v1 Announce Type: new 
Abstract: Log-based anomaly detection is critical for ensuring the stability and reliability of web systems. One of the key problems in this task is the lack of sufficient labeled logs, which limits the rapid deployment in new systems. Existing works usually leverage large-scale labeled logs from a mature web system and a small amount of labeled logs from a new system, using transfer learning to extract and generalize general knowledge across both domains. However, these methods focus solely on the transfer of general knowledge and neglect the disparity and potential mismatch between such knowledge and the proprietary knowledge of target system, thus constraining performance. To address this limitation, we propose FusionLog, a novel zero-label cross-system log-based anomaly detection method that effectively achieves the fusion of general and proprietary knowledge, enabling cross-system generalization without any labeled target logs. Specifically, we first design a training-free router based on semantic similarity that dynamically partitions unlabeled target logs into 'general logs' and 'proprietary logs.' For general logs, FusionLog employs a small model based on system-agnostic representation meta-learning for direct training and inference, inheriting the general anomaly patterns shared between the source and target systems. For proprietary logs, we iteratively generate pseudo-labels and fine-tune the small model using multi-round collaborative knowledge distillation and fusion based on large language model (LLM) and small model (SM) to enhance its capability to recognize anomaly patterns specific to the target system. Experimental results on three public log datasets from different systems show that FusionLog achieves over 90% F1-score under a fully zero-label setting, significantly outperforming state-of-the-art cross-system log-based anomaly detection methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Real-Time Gas Crossover Prediction in PEM Electrolyzers: First Application with Multi-Membrane Validation</title>
<link>https://arxiv.org/abs/2511.05879</link>
<guid>https://arxiv.org/abs/2511.05879</guid>
<content:encoded><![CDATA[

arXiv:2511.05879v1 Announce Type: new 
Abstract: Green hydrogen production via polymer electrolyte membrane (PEM) water electrolysis is pivotal for energy transition, yet hydrogen crossover through membranes threatens safety and economic viability-approaching explosive limits (4 mol% H$_2$ in O$_2$) while reducing Faradaic efficiency by 2.5%. Current physics-based models require extensive calibration and computational resources that preclude real-time implementation, while purely data-driven approaches fail to extrapolate beyond training conditions-critical for dynamic electrolyzer operation. Here we present the first application of physics-informed neural networks (PINNs) for hydrogen crossover prediction, integrating mass conservation, Fick's diffusion law, and Henry's solubility law within a compact architecture (17,793 parameters). Validated across six membranes under industrially relevant conditions (0.05-5.0 A/cm$^2$, 1-200 bar, 25-85{\deg}C), our PINN achieves exceptional accuracy (R$^2$ = 99.84%, RMSE = 0.0348%) with sub-millisecond inference times suitable for real-time control. Remarkably, the model maintains R$^2$ > 86% when predicting crossover at pressures 2.5x beyond training range-substantially outperforming pure neural networks (R$^2$ = 43.4%). The hardware-agnostic deployment, from desktop CPUs to edge devices (Raspberry Pi 4), enables distributed safety monitoring essential for gigawatt-scale installations. By bridging physical rigor and computational efficiency, this work establishes a new paradigm for real-time electrolyzer monitoring, accelerating deployment of safe, efficient green hydrogen infrastructure crucial for net-zero emissions targets.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Kernels to Attention: A Transformer Framework for Density and Score Estimation</title>
<link>https://arxiv.org/abs/2511.05924</link>
<guid>https://arxiv.org/abs/2511.05924</guid>
<content:encoded><![CDATA[

arXiv:2511.05924v1 Announce Type: new 
Abstract: We introduce a unified attention-based framework for joint score and density estimation. Framing the problem as a sequence-to-sequence task, we develop a permutation- and affine-equivariant transformer that estimates both the probability density $f(x)$ and its score $\nabla_x \log f(x)$ directly from i.i.d. samples. Unlike traditional score-matching methods that require training a separate model for each distribution, our approach learns a single distribution-agnostic operator that generalizes across densities and sample sizes. The architecture employs cross-attention to connect observed samples with arbitrary query points, enabling generalization beyond the training data, while built-in symmetry constraints ensure equivariance to permutation and affine transformations. Analytically, we show that the attention weights can recover classical kernel density estimation (KDE), and verify it empirically, establishing a principled link between classical KDE and the transformer architecture. Empirically, the model achieves substantially lower error and better scaling than KDE and score-debiased KDE (SD-KDE), while exhibiting better runtime scaling. Together, these results establish transformers as general-purpose, data-adaptive operators for nonparametric density and score estimation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Survival Analysis of Longitudinal EHR Data for Joint Prediction of Hospitalization and Death in COPD Patients</title>
<link>https://arxiv.org/abs/2511.05960</link>
<guid>https://arxiv.org/abs/2511.05960</guid>
<content:encoded><![CDATA[

arXiv:2511.05960v1 Announce Type: new 
Abstract: Patients with chronic obstructive pulmonary disease (COPD) have an increased risk of hospitalizations, strongly associated with decreased survival, yet predicting the timing of these events remains challenging and has received limited attention in the literature. In this study, we performed survival analysis to predict hospitalization and death in COPD patients using longitudinal electronic health records (EHRs), comparing statistical models, machine learning (ML), and deep learning (DL) approaches. We analyzed data from more than 150k patients from the SIDIAP database in Catalonia, Spain, from 2013 to 2017, modeling hospitalization as a first event and death as a semi-competing terminal event. Multiple models were evaluated, including Cox proportional hazards, SurvivalBoost, DeepPseudo, SurvTRACE, Dynamic Deep-Hit, and Deep Recurrent Survival Machine. Results showed that DL models utilizing recurrent architectures outperformed both ML and linear approaches in concordance and time-dependent AUC, especially for hospitalization, which proved to be the harder event to predict. This study is, to our knowledge, the first to apply deep survival analysis on longitudinal EHR data to jointly predict multiple time-to-event outcomes in COPD patients, highlighting the potential of DL approaches to capture temporal patterns and improve risk stratification.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next-Latent Prediction Transformers Learn Compact World Models</title>
<link>https://arxiv.org/abs/2511.05963</link>
<guid>https://arxiv.org/abs/2511.05963</guid>
<content:encoded><![CDATA[

arXiv:2511.05963v1 Announce Type: new 
Abstract: Transformers replace recurrence with a memory that grows with sequence length and self-attention that enables ad-hoc look ups over past tokens. Consequently, they lack an inherent incentive to compress history into compact latent states with consistent transition rules. This often leads to learning solutions that generalize poorly. We introduce Next-Latent Prediction (NextLat), which extends standard next-token training with self-supervised predictions in the latent space. Specifically, NextLat trains a transformer to learn latent representations that are predictive of its next latent state given the next output token. Theoretically, we show that these latents provably converge to belief states, compressed information of the history necessary to predict the future. This simple auxiliary objective also injects a recurrent inductive bias into transformers, while leaving their architecture, parallel training, and inference unchanged. NextLat effectively encourages the transformer to form compact internal world models with its own belief states and transition dynamics -- a crucial property absent in standard next-token prediction transformers. Empirically, across benchmarks targeting core sequence modeling competencies -- world modeling, reasoning, planning, and language modeling -- NextLat demonstrates significant gains over standard next-token training in downstream accuracy, representation compression, and lookahead planning. NextLat stands as a simple and efficient paradigm for shaping transformer representations toward stronger generalization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Deep Learning-based Classification of Wolff-Parkinson-White Electrocardiographic Signals</title>
<link>https://arxiv.org/abs/2511.05973</link>
<guid>https://arxiv.org/abs/2511.05973</guid>
<content:encoded><![CDATA[

arXiv:2511.05973v1 Announce Type: new 
Abstract: Wolff-Parkinson-White (WPW) syndrome is a cardiac electrophysiology (EP) disorder caused by the presence of an accessory pathway (AP) that bypasses the atrioventricular node, faster ventricular activation rate, and provides a substrate for atrio-ventricular reentrant tachycardia (AVRT). Accurate localization of the AP is critical for planning and guiding catheter ablation procedures. While traditional diagnostic tree (DT) methods and more recent machine learning (ML) approaches have been proposed to predict AP location from surface electrocardiogram (ECG), they are often constrained by limited anatomical localization resolution, poor interpretability, and the use of small clinical datasets. In this study, we present a Deep Learning (DL) model for the localization of single manifest APs across 24 cardiac regions, trained on a large, physiologically realistic database of synthetic ECGs generated using a personalized virtual heart model. We also integrate eXplainable Artificial Intelligence (XAI) methods, Guided Backpropagation, Grad-CAM, and Guided Grad-CAM, into the pipeline. This enables interpretation of DL decision-making and addresses one of the main barriers to clinical adoption: lack of transparency in ML predictions. Our model achieves localization accuracy above 95%, with a sensitivity of 94.32% and specificity of 99.78%. XAI outputs are physiologically validated against known depolarization patterns, and a novel index is introduced to identify the most informative ECG leads for AP localization. Results highlight lead V2 as the most critical, followed by aVF, V1, and aVL. This work demonstrates the potential of combining cardiac digital twins with explainable DL to enable accurate, transparent, and non-invasive AP localization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kunlun Anomaly Troubleshooter: Enabling Kernel-Level Anomaly Detection and Causal Reasoning for Large Model Distributed Inference</title>
<link>https://arxiv.org/abs/2511.05978</link>
<guid>https://arxiv.org/abs/2511.05978</guid>
<content:encoded><![CDATA[

arXiv:2511.05978v1 Announce Type: new 
Abstract: Anomaly troubleshooting for large model distributed inference (LMDI) remains a critical challenge. Resolving anomalies such as inference performance degradation or latency jitter in distributed system demands significant manual efforts from domain experts, resulting in extremely time-consuming diagnosis processes with relatively low accuracy. In this paper, we introduce Kunlun Anomaly Troubleshooter (KAT), the first anomaly troubleshooting framework tailored for LMDI. KAT addresses this problem through two core innovations. First, KAT exploits the synchronicity and consistency of GPU workers, innovatively leverages function trace data to precisely detect kernel-level anomalies and associated hardware components at nanosecond resolution. Second, KAT integrates these detection results into a domain-adapted LLM, delivering systematic causal reasoning and natural language interpretation of complex anomaly symptoms. Evaluations conducted in Alibaba Cloud Service production environment indicate that KAT achieves over 0.884 precision and 0.936 recall in anomaly detection, providing detail anomaly insights that significantly narrow down the diagnostic scope and improve both the efficiency and success rate of troubleshooting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Time-Indexed Foundation Models the Future of Time Series Imputation?</title>
<link>https://arxiv.org/abs/2511.05980</link>
<guid>https://arxiv.org/abs/2511.05980</guid>
<content:encoded><![CDATA[

arXiv:2511.05980v1 Announce Type: new 
Abstract: Foundation models for time series imputation remain largely unexplored. Recently, two such models, TabPFN-TS and MoTM, have emerged. These models share a common philosophy that places them within the family of time-indexed foundation models. This paper presents the first large-scale empirical study of these models for zero-shot imputation, which enables missing value recovery without retraining across a wide range of scenarios. We conduct extensive univariate experiments across 33 out-of-domain datasets (approximately 1.3M imputation windows) and evaluate their ability to integrate covariates at inference time to improve accuracy without fine-tuning. Our results demonstrate that time-indexed foundation models are a powerful and practical step toward achieving general-purpose, zero-shot imputation for real-world time series.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bespoke Co-processor for Energy-Efficient Health Monitoring on RISC-V-based Flexible Wearables</title>
<link>https://arxiv.org/abs/2511.05985</link>
<guid>https://arxiv.org/abs/2511.05985</guid>
<content:encoded><![CDATA[

arXiv:2511.05985v1 Announce Type: new 
Abstract: Flexible electronics offer unique advantages for conformable, lightweight, and disposable healthcare wearables. However, their limited gate count, large feature sizes, and high static power consumption make on-body machine learning classification highly challenging. While existing bendable RISC-V systems provide compact solutions, they lack the energy efficiency required. We present a mechanically flexible RISC-V that integrates a bespoke multiply-accumulate co-processor with fixed coefficients to maximize energy efficiency and minimize latency. Our approach formulates a constrained programming problem to jointly determine co-processor constants and optimally map Multi-Layer Perceptron (MLP) inference operations, enabling compact, model-specific hardware by leveraging the low fabrication and non-recurring engineering costs of flexible technologies. Post-layout results demonstrate near-real-time performance across several healthcare datasets, with our circuits operating within the power budget of existing flexible batteries and occupying only 2.42 mm^2, offering a promising path toward accessible, sustainable, and conformable healthcare wearables. Our microprocessors achieve an average 2.35x speedup and 2.15x lower energy consumption compared to the state of the art.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference</title>
<link>https://arxiv.org/abs/2511.06010</link>
<guid>https://arxiv.org/abs/2511.06010</guid>
<content:encoded><![CDATA[

arXiv:2511.06010v1 Announce Type: new 
Abstract: The escalating context length in Large Language Models (LLMs) creates a severe performance bottleneck around the Key-Value (KV) cache, whose memory-bound nature leads to significant GPU under-utilization. This paper introduces Mixture of Shared KV Attention (MoSKA), an architecture that addresses this challenge by exploiting the heterogeneity of context data. It differentiates between per-request unique and massively reused shared sequences. The core of MoSKA is a novel Shared KV Attention mechanism that transforms the attention on shared data from a series of memory-bound GEMV operations into a single, compute-bound GEMM by batching concurrent requests. This is supported by an MoE-inspired sparse attention strategy that prunes the search space and a tailored Disaggregated Infrastructure that specializes hardware for unique and shared data. This comprehensive approach demonstrates a throughput increase of up to 538.7x over baselines in workloads with high context sharing, offering a clear architectural path toward scalable LLM inference.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving</title>
<link>https://arxiv.org/abs/2511.06029</link>
<guid>https://arxiv.org/abs/2511.06029</guid>
<content:encoded><![CDATA[

arXiv:2511.06029v1 Announce Type: new 
Abstract: Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ITPP: Learning Disentangled Event Dynamics in Marked Temporal Point Processes</title>
<link>https://arxiv.org/abs/2511.06032</link>
<guid>https://arxiv.org/abs/2511.06032</guid>
<content:encoded><![CDATA[

arXiv:2511.06032v1 Announce Type: new 
Abstract: Marked Temporal Point Processes (MTPPs) provide a principled framework for modeling asynchronous event sequences by conditioning on the history of past events. However, most existing MTPP models rely on channel-mixing strategies that encode information from different event types into a single, fixed-size latent representation. This entanglement can obscure type-specific dynamics, leading to performance degradation and increased risk of overfitting. In this work, we introduce ITPP, a novel channel-independent architecture for MTPP modeling that decouples event type information using an encoder-decoder framework with an ODE-based backbone. Central to ITPP is a type-aware inverted self-attention mechanism, designed to explicitly model inter-channel correlations among heterogeneous event types. This architecture enhances effectiveness and robustness while reducing overfitting. Comprehensive experiments on multiple real-world and synthetic datasets demonstrate that ITPP consistently outperforms state-of-the-art MTPP models in both predictive accuracy and generalization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Ocean State Estimation with efficient and scalable AI</title>
<link>https://arxiv.org/abs/2511.06041</link>
<guid>https://arxiv.org/abs/2511.06041</guid>
<content:encoded><![CDATA[

arXiv:2511.06041v1 Announce Type: new 
Abstract: Accurate and efficient global ocean state estimation remains a grand challenge for Earth system science, hindered by the dual bottlenecks of computational scalability and degraded data fidelity in traditional data assimilation (DA) and deep learning (DL) approaches. Here we present an AI-driven Data Assimilation Framework for Ocean (ADAF-Ocean) that directly assimilates multi-source and multi-scale observations, ranging from sparse in-situ measurements to 4 km satellite swaths, without any interpolation or data thinning. Inspired by Neural Processes, ADAF-Ocean learns a continuous mapping from heterogeneous inputs to ocean states, preserving native data fidelity. Through AI-driven super-resolution, it reconstructs 0.25$^\circ$ mesoscale dynamics from coarse 1$^\circ$ fields, which ensures both efficiency and scalability, with just 3.7\% more parameters than the 1$^\circ$ configuration. When coupled with a DL forecasting system, ADAF-Ocean extends global forecast skill by up to 20 days compared to baselines without assimilation. This framework establishes a computationally viable and scientifically rigorous pathway toward real-time, high-resolution Earth system monitoring.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Design of Input Convex Neural Networks for Consistency Optimal Transport Flow Matching</title>
<link>https://arxiv.org/abs/2511.06042</link>
<guid>https://arxiv.org/abs/2511.06042</guid>
<content:encoded><![CDATA[

arXiv:2511.06042v1 Announce Type: new 
Abstract: We propose a consistency model based on the optimal-transport flow. A physics-informed design of partially input-convex neural networks (PICNN) plays a central role in constructing the flow field that emulates the displacement interpolation. During the training stage, we couple the Hamilton-Jacobi (HJ) residual in the OT formulation with the original flow matching loss function. Our approach avoids inner optimization subproblems that are present in previous one-step OFM approaches. During the prediction stage, our approach supports both one-step (Brenier-map) and multi-step ODE sampling from the same learned potential, leveraging the straightness of the OT flow. We validate scalability and performance on standard OT benchmarks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Particle-System Random Batch Methods Enhance Graph Transformer: Memory Efficiency and Parallel Computing Strategy</title>
<link>https://arxiv.org/abs/2511.06044</link>
<guid>https://arxiv.org/abs/2511.06044</guid>
<content:encoded><![CDATA[

arXiv:2511.06044v1 Announce Type: new 
Abstract: Attention mechanism is a significant part of Transformer models. It helps extract features from embedded vectors by adding global information and its expressivity has been proved to be powerful. Nevertheless, the quadratic complexity restricts its practicability. Although several researches have provided attention mechanism in sparse form, they are lack of theoretical analysis about the expressivity of their mechanism while reducing complexity. In this paper, we put forward Random Batch Attention (RBA), a linear self-attention mechanism, which has theoretical support of the ability to maintain its expressivity. Random Batch Attention has several significant strengths as follows: (1) Random Batch Attention has linear time complexity. Other than this, it can be implemented in parallel on a new dimension, which contributes to much memory saving. (2) Random Batch Attention mechanism can improve most of the existing models by replacing their attention mechanisms, even many previously improved attention mechanisms. (3) Random Batch Attention mechanism has theoretical explanation in convergence, as it comes from Random Batch Methods on computation mathematics. Experiments on large graphs have proved advantages mentioned above. Also, the theoretical modeling of self-attention mechanism is a new tool for future research on attention-mechanism analysis.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Function Based Isolation Forest (FuBIF): A Unifying Framework for Interpretable Isolation-Based Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.06054</link>
<guid>https://arxiv.org/abs/2511.06054</guid>
<content:encoded><![CDATA[

arXiv:2511.06054v1 Announce Type: new 
Abstract: Anomaly Detection (AD) is evolving through algorithms capable of identifying outliers in complex datasets. The Isolation Forest (IF), a pivotal AD technique, exhibits adaptability limitations and biases. This paper introduces the Function-based Isolation Forest (FuBIF), a generalization of IF that enables the use of real-valued functions for dataset branching, significantly enhancing the flexibility of evaluation tree construction. Complementing this, the FuBIF Feature Importance (FuBIFFI) algorithm extends the interpretability in IF-based approaches by providing feature importance scores across possible FuBIF models. This paper details the operational framework of FuBIF, evaluates its performance against established methods, and explores its theoretical contributions. An open-source implementation is provided to encourage further research and ensure reproducibility.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CatBack: Universal Backdoor Attacks on Tabular Data via Categorical Encoding</title>
<link>https://arxiv.org/abs/2511.06072</link>
<guid>https://arxiv.org/abs/2511.06072</guid>
<content:encoded><![CDATA[

arXiv:2511.06072v1 Announce Type: new 
Abstract: Backdoor attacks in machine learning have drawn significant attention for their potential to compromise models stealthily, yet most research has focused on homogeneous data such as images. In this work, we propose a novel backdoor attack on tabular data, which is particularly challenging due to the presence of both numerical and categorical features. Our key idea is a novel technique to convert categorical values into floating-point representations. This approach preserves enough information to maintain clean-model accuracy compared to traditional methods like one-hot or ordinal encoding. By doing this, we create a gradient-based universal perturbation that applies to all features, including categorical ones.
  We evaluate our method on five datasets and four popular models. Our results show up to a 100% attack success rate in both white-box and black-box settings (including real-world applications like Vertex AI), revealing a severe vulnerability for tabular data. Our method is shown to surpass the previous works like Tabdoor in terms of performance, while remaining stealthy against state-of-the-art defense mechanisms. We evaluate our attack against Spectral Signatures, Neural Cleanse, Beatrix, and Fine-Pruning, all of which fail to defend successfully against it. We also verify that our attack successfully bypasses popular outlier detection mechanisms.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Make It Long, Keep It Fast: End-to-End 10k-Sequence Modeling at Billion Scale on Douyin</title>
<link>https://arxiv.org/abs/2511.06077</link>
<guid>https://arxiv.org/abs/2511.06077</guid>
<content:encoded><![CDATA[

arXiv:2511.06077v1 Announce Type: new 
Abstract: Short-video recommenders such as Douyin must exploit extremely long user histories without breaking latency or cost budgets. We present an end-to-end system that scales long-sequence modeling to 10k-length histories in production. First, we introduce Stacked Target-to-History Cross Attention (STCA), which replaces history self-attention with stacked cross-attention from the target to the history, reducing complexity from quadratic to linear in sequence length and enabling efficient end-to-end training. Second, we propose Request Level Batching (RLB), a user-centric batching scheme that aggregates multiple targets for the same user/request to share the user-side encoding, substantially lowering sequence-related storage, communication, and compute without changing the learning objective. Third, we design a length-extrapolative training strategy -- train on shorter windows, infer on much longer ones -- so the model generalizes to 10k histories without additional training cost. Across offline and online experiments, we observe predictable, monotonic gains as we scale history length and model capacity, mirroring the scaling law behavior observed in large language models. Deployed at full traffic on Douyin, our system delivers significant improvements on key engagement metrics while meeting production latency, demonstrating a practical path to scaling end-to-end long-sequence recommendation to the 10k regime.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event-driven physics-informed operator learning for reliability analysis</title>
<link>https://arxiv.org/abs/2511.06083</link>
<guid>https://arxiv.org/abs/2511.06083</guid>
<content:encoded><![CDATA[

arXiv:2511.06083v1 Announce Type: new 
Abstract: Reliability analysis of engineering systems under uncertainty poses significant computational challenges, particularly for problems involving high-dimensional stochastic inputs, nonlinear system responses, and multiphysics couplings. Traditional surrogate modeling approaches often incur high energy consumption, which severely limits their scalability and deployability in resource-constrained environments. We introduce NeuroPOL, \textit{the first neuroscience-inspired physics-informed operator learning framework} for reliability analysis. NeuroPOL incorporates Variable Spiking Neurons into a physics-informed operator architecture, replacing continuous activations with event-driven spiking dynamics. This innovation promotes sparse communication, significantly reduces computational load, and enables an energy-efficient surrogate model. The proposed framework lowers both computational and power demands, supporting real-time reliability assessment and deployment on edge devices and digital twins. By embedding governing physical laws into operator learning, NeuroPOL builds physics-consistent surrogates capable of accurate uncertainty propagation and efficient failure probability estimation, even for high-dimensional problems. We evaluate NeuroPOL on five canonical benchmarks, the Burgers equation, Nagumo equation, two-dimensional Poisson equation, two-dimensional Darcy equation, and incompressible Navier-Stokes equation with energy coupling. Results show that NeuroPOL achieves reliability measures comparable to standard physics-informed operators, while introducing significant communication sparsity, enabling scalable, distributed, and energy-efficient deployment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximating Shapley Explanations in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.06094</link>
<guid>https://arxiv.org/abs/2511.06094</guid>
<content:encoded><![CDATA[

arXiv:2511.06094v1 Announce Type: new 
Abstract: Reinforcement learning has achieved remarkable success in complex decision-making environments, yet its lack of transparency limits its deployment in practice, especially in safety-critical settings. Shapley values from cooperative game theory provide a principled framework for explaining reinforcement learning; however, the computational cost of Shapley explanations is an obstacle to their use. We introduce FastSVERL, a scalable method for explaining reinforcement learning by approximating Shapley values. FastSVERL is designed to handle the unique challenges of reinforcement learning, including temporal dependencies across multi-step trajectories, learning from off-policy data, and adapting to evolving agent behaviours in real time. FastSVERL introduces a practical, scalable approach for principled and rigorous interpretability in reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Web Agents with Synthetic Supervision</title>
<link>https://arxiv.org/abs/2511.06101</link>
<guid>https://arxiv.org/abs/2511.06101</guid>
<content:encoded><![CDATA[

arXiv:2511.06101v1 Announce Type: new 
Abstract: Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guardian-regularized Safe Offline Reinforcement Learning for Smart Weaning of Mechanical Circulatory Devices</title>
<link>https://arxiv.org/abs/2511.06111</link>
<guid>https://arxiv.org/abs/2511.06111</guid>
<content:encoded><![CDATA[

arXiv:2511.06111v1 Announce Type: new 
Abstract: We study the sequential decision-making problem for automated weaning of mechanical circulatory support (MCS) devices in cardiogenic shock patients. MCS devices are percutaneous micro-axial flow pumps that provide left ventricular unloading and forward blood flow, but current weaning strategies vary significantly across care teams and lack data-driven approaches. Offline reinforcement learning (RL) has proven to be successful in sequential decision-making tasks, but our setting presents challenges for training and evaluating traditional offline RL methods: prohibition of online patient interaction, highly uncertain circulatory dynamics due to concurrent treatments, and limited data availability. We developed an end-to-end machine learning framework with two key contributions (1) Clinically-aware OOD-regularized Model-based Policy Optimization (CORMPO), a density-regularized offline RL algorithm for out-of-distribution suppression that also incorporates clinically-informed reward shaping and (2) a Transformer-based probabilistic digital twin that models MCS circulatory dynamics for policy evaluation with rich physiological and clinical metrics. We prove that \textsf{CORMPO} achieves theoretical performance guarantees under mild assumptions. CORMPO attains a higher reward than the offline RL baselines by 28% and higher scores in clinical metrics by 82.6% on real and synthetic datasets. Our approach offers a principled framework for safe offline policy learning in high-stakes medical applications where domain expertise and safety constraints are essential.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Convergence and Stability of Distributed Sub-model Training</title>
<link>https://arxiv.org/abs/2511.06132</link>
<guid>https://arxiv.org/abs/2511.06132</guid>
<content:encoded><![CDATA[

arXiv:2511.06132v1 Announce Type: new 
Abstract: As learning models continue to grow in size, enabling on-device local training of these models has emerged as a critical challenge in federated learning. A popular solution is sub-model training, where the server only distributes randomly sampled sub-models to the edge clients, and clients only update these small models. However, those random sampling of sub-models may not give satisfying convergence performance. In this paper, observing the success of SGD with shuffling, we propose a distributed shuffled sub-model training, where the full model is partitioned into several sub-models in advance, and the server shuffles those sub-models, sends each of them to clients at each round, and by the end of local updating period, clients send back the updated sub-models, and server averages them. We establish the convergence rate of this algorithm. We also study the generalization of distributed sub-model training via stability analysis, and find that the sub-model training can improve the generalization via amplifying the stability of training process. The extensive experiments also validate our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Robustness of Graph Neural Networks through p-Laplacian</title>
<link>https://arxiv.org/abs/2511.06143</link>
<guid>https://arxiv.org/abs/2511.06143</guid>
<content:encoded><![CDATA[

arXiv:2511.06143v1 Announce Type: new 
Abstract: With the increase of data in day-to-day life, businesses and different stakeholders need to analyze the data for better pre- dictions. Traditionally, relational data has been a source of various insights, but with the increase in computational power and the need to understand deeper relationships between en- tities, the need to design new techniques has arisen. For this graph data analysis has become an extraordinary tool for un- derstanding the data, which reveals more realistic and flexible modelling of complex relationships. Recently, Graph Neural Networks (GNNs) have shown great promise in various ap- plications, such as social network analysis, recommendation systems, drug discovery, and more. However, many adversar- ial attacks can happen over the data, whether during training (poisoning attack) or during testing (evasion attack), which can adversely manipulate the desired outcome from the GNN model. Therefore, it is crucial to make the GNNs robust to such attacks. The existing robustness methods are computa- tionally demanding and perform poorly when the intensity of attack increases. This paper presents a computationally ef- ficient framework, namely, pLAPGNN, based on weighted p-Laplacian for making GNNs robust. Empirical evaluation on real datasets establishes the efficacy and efficiency of the proposed method.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Models Got Talent: Identifying High Performing Wearable Human Activity Recognition Models Without Training</title>
<link>https://arxiv.org/abs/2511.06157</link>
<guid>https://arxiv.org/abs/2511.06157</guid>
<content:encoded><![CDATA[

arXiv:2511.06157v1 Announce Type: new 
Abstract: A promising alternative to the computationally expensive Neural Architecture Search (NAS) involves the development of \textit{Zero Cost Proxies (ZCPs)}, which correlate well to trained performance, but can be computed through a single forward/backward pass on a randomly sampled batch of data. In this paper, we investigate the effectiveness of ZCPs for HAR on six benchmark datasets, and demonstrate that they discover network architectures that obtain within 5\% of performance attained by full scale training involving 1500 randomly sampled architectures. This results in substantial computational savings as high performing architectures can be discovered with minimal training. Our experiments not only introduce ZCPs to sensor-based HAR, but also demonstrate that they are robust to data noise, further showcasing their suitability for practical scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Attention Transplant for Transfer Learning of Tabular Data Across Disparate Domains</title>
<link>https://arxiv.org/abs/2511.06161</link>
<guid>https://arxiv.org/abs/2511.06161</guid>
<content:encoded><![CDATA[

arXiv:2511.06161v1 Announce Type: new 
Abstract: Transfer learning of tabular data is non-trivial due to heterogeneity in the feature space across disparate domains. The limited success of traditional deep learning in tabular knowledge transfer can be advanced by leveraging large language models (LLMs). However, the efficacy of LLMs often stagnates for mixed data types structured in tables due to the limitations of text prompts and in-context learning. We propose a lightweight transfer learning framework that fine-tunes an LLM using source tabular data and transplants the LLM's selective $key$ and $value$ projection weights into a gated feature tokenized transformer (gFTT) built for tabular data. The gFTT model with cross-domain attention is fine-tuned using target tabular data for transfer learning, eliminating the need for shared features, LLM prompt engineering, and large-scale pretrained models. Our experiments using ten pairs of source-target data sets and 12 baselines demonstrate the superiority of the proposed LLM-attention transplant for transfer learning (LATTLE) method over traditional ML models, state-of-the-art deep tabular architectures, and transfer learning models trained on thousands to billions of tabular samples. The proposed attention transfer demonstrates an effective solution to learning relationships between data tables using an LLM in a low-resource learning environment. The source code for the proposed method is publicly available.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Gaussian DAG Models without Condition Number Bounds</title>
<link>https://arxiv.org/abs/2511.06164</link>
<guid>https://arxiv.org/abs/2511.06164</guid>
<content:encoded><![CDATA[

arXiv:2511.06164v1 Announce Type: new 
Abstract: We study the problem of learning the topology of a directed Gaussian Graphical Model under the equal-variance assumption, where the graph has $n$ nodes and maximum in-degree $d$. Prior work has established that $O(d \log n)$ samples are sufficient for this task. However, an important factor that is often overlooked in these analyses is the dependence on the condition number of the covariance matrix of the model. Indeed, all algorithms from prior work require a number of samples that grows polynomially with this condition number. In many cases this is unsatisfactory, since the condition number could grow polynomially with $n$, rendering these prior approaches impractical in high-dimensional settings. In this work, we provide an algorithm that recovers the underlying graph and prove that the number of samples required is independent of the condition number. Furthermore, we establish lower bounds that nearly match the upper bound up to a $d$-factor, thus providing an almost tight characterization of the true sample complexity of the problem. Moreover, under a further assumption that all the variances of the variables are bounded, we design a polynomial-time algorithm that recovers the underlying graph, at the cost of an additional polynomial dependence of the sample complexity on $d$. We complement our theoretical findings with simulations on synthetic datasets that confirm our predictions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local K-Similarity Constraint for Federated Learning with Label Noise</title>
<link>https://arxiv.org/abs/2511.06169</link>
<guid>https://arxiv.org/abs/2511.06169</guid>
<content:encoded><![CDATA[

arXiv:2511.06169v1 Announce Type: new 
Abstract: Federated learning on clients with noisy labels is a challenging problem, as such clients can infiltrate the global model, impacting the overall generalizability of the system. Existing methods proposed to handle noisy clients assume that a sufficient number of clients with clean labels are available, which can be leveraged to learn a robust global model while dampening the impact of noisy clients. This assumption fails when a high number of heterogeneous clients contain noisy labels, making the existing approaches ineffective. In such scenarios, it is important to locally regularize the clients before communication with the global model, to ensure the global model isn't corrupted by noisy clients. While pre-trained self-supervised models can be effective for local regularization, existing centralized approaches relying on pretrained initialization are impractical in a federated setting due to the potentially large size of these models, which increases communication costs. In that line, we propose a regularization objective for client models that decouples the pre-trained and classification models by enforcing similarity between close data points within the client. We leverage the representation space of a self-supervised pretrained model to evaluate the closeness among examples. This regularization, when applied with the standard objective function for the downstream task in standard noisy federated settings, significantly improves performance, outperforming existing state-of-the-art federated methods in multiple computer vision and medical image classification benchmarks. Unlike other techniques that rely on self-supervised pretrained initialization, our method does not require the pretrained model and classifier backbone to share the same architecture, making it architecture-agnostic.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resilience Inference for Supply Chains with Hypergraph Neural Network</title>
<link>https://arxiv.org/abs/2511.06208</link>
<guid>https://arxiv.org/abs/2511.06208</guid>
<content:encoded><![CDATA[

arXiv:2511.06208v1 Announce Type: new 
Abstract: Supply chains are integral to global economic stability, yet disruptions can swiftly propagate through interconnected networks, resulting in substantial economic impacts. Accurate and timely inference of supply chain resilience the capability to maintain core functions during disruptions is crucial for proactive risk mitigation and robust network design. However, existing approaches lack effective mechanisms to infer supply chain resilience without explicit system dynamics and struggle to represent the higher-order, multi-entity dependencies inherent in supply chain networks. These limitations motivate the definition of a novel problem and the development of targeted modeling solutions. To address these challenges, we formalize a novel problem: Supply Chain Resilience Inference (SCRI), defined as predicting supply chain resilience using hypergraph topology and observed inventory trajectories without explicit dynamic equations. To solve this problem, we propose the Supply Chain Resilience Inference Hypergraph Network (SC-RIHN), a novel hypergraph-based model leveraging set-based encoding and hypergraph message passing to capture multi-party firm-product interactions. Comprehensive experiments demonstrate that SC-RIHN significantly outperforms traditional MLP, representative graph neural network variants, and ResInf baselines across synthetic benchmarks, underscoring its potential for practical, early-warning risk assessment in complex supply chain systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Linear Regression is Easy on Random Supports</title>
<link>https://arxiv.org/abs/2511.06211</link>
<guid>https://arxiv.org/abs/2511.06211</guid>
<content:encoded><![CDATA[

arXiv:2511.06211v1 Announce Type: new 
Abstract: Sparse linear regression is one of the most basic questions in machine learning and statistics. Here, we are given as input a design matrix $X \in \mathbb{R}^{N \times d}$ and measurements or labels ${y} \in \mathbb{R}^N$ where ${y} = {X} {w}^* + {\xi}$, and ${\xi}$ is the noise in the measurements. Importantly, we have the additional constraint that the unknown signal vector ${w}^*$ is sparse: it has $k$ non-zero entries where $k$ is much smaller than the ambient dimension. Our goal is to output a prediction vector $\widehat{{w}}$ that has small prediction error: $\frac{1}{N}\cdot \|{X} {w}^* - {X} \widehat{{w}}\|^2_2$.
  Information-theoretically, we know what is best possible in terms of measurements: under most natural noise distributions, we can get prediction error at most $\epsilon$ with roughly $N = O(k \log d/\epsilon)$ samples. Computationally, this currently needs $d^{\Omega(k)}$ run-time. Alternately, with $N = O(d)$, we can get polynomial-time. Thus, there is an exponential gap (in the dependence on $d$) between the two and we do not know if it is possible to get $d^{o(k)}$ run-time and $o(d)$ samples.
  We give the first generic positive result for worst-case design matrices ${X}$: For any ${X}$, we show that if the support of ${w}^*$ is chosen at random, we can get prediction error $\epsilon$ with $N = \text{poly}(k, \log d, 1/\epsilon)$ samples and run-time $\text{poly}(d,N)$. This run-time holds for any design matrix ${X}$ with condition number up to $2^{\text{poly}(d)}$.
  Previously, such results were known for worst-case ${w}^*$, but only for random design matrices from well-behaved families, matrices that have a very low condition number ($\text{poly}(\log d)$; e.g., as studied in compressed sensing), or those with special structural properties.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Multi-view Graph Contrastive Learning via Fractional-order Neural Diffusion Networks</title>
<link>https://arxiv.org/abs/2511.06216</link>
<guid>https://arxiv.org/abs/2511.06216</guid>
<content:encoded><![CDATA[

arXiv:2511.06216v1 Announce Type: new 
Abstract: Graph contrastive learning (GCL) learns node and graph representations by contrasting multiple views of the same graph. Existing methods typically rely on fixed, handcrafted views-usually a local and a global perspective, which limits their ability to capture multi-scale structural patterns. We present an augmentation-free, multi-view GCL framework grounded in fractional-order continuous dynamics. By varying the fractional derivative order $\alpha \in (0,1]$, our encoders produce a continuous spectrum of views: small $\alpha$ yields localized features, while large $\alpha$ induces broader, global aggregation. We treat $\alpha$ as a learnable parameter so the model can adapt diffusion scales to the data and automatically discover informative views. This principled approach generates diverse, complementary representations without manual augmentations. Extensive experiments on standard benchmarks demonstrate that our method produces more robust and expressive embeddings and outperforms state-of-the-art GCL baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Dynamic Origin-Destination Matrix Estimation in Microscopic Traffic Simulations Considering Credit Assignment</title>
<link>https://arxiv.org/abs/2511.06229</link>
<guid>https://arxiv.org/abs/2511.06229</guid>
<content:encoded><![CDATA[

arXiv:2511.06229v1 Announce Type: new 
Abstract: This paper focuses on dynamic origin-destination matrix estimation (DODE), a crucial calibration process necessary for the effective application of microscopic traffic simulations. The fundamental challenge of the DODE problem in microscopic simulations stems from the complex temporal dynamics and inherent uncertainty of individual vehicle dynamics. This makes it highly challenging to precisely determine which vehicle traverses which link at any given moment, resulting in intricate and often ambiguous relationships between origin-destination (OD) matrices and their contributions to resultant link flows. This phenomenon constitutes the credit assignment problem, a central challenge addressed in this study. We formulate the DODE problem as a Markov Decision Process (MDP) and propose a novel framework that applies model-free deep reinforcement learning (DRL). Within our proposed framework, the agent learns an optimal policy to sequentially generate OD matrices, refining its strategy through direct interaction with the simulation environment. The proposed method is validated on the Nguyen-Dupuis network using SUMO, where its performance is evaluated against ground-truth link flows aggregated at 5-minute intervals over a 30-minute horizon. Experimental results demonstrate that our approach achieves a 43.2% reduction in mean squared error (MSE) compared to the best-performing conventional baseline. By reframing DODE as a sequential decision-making problem, our approach addresses the credit assignment challenge through its learned policy, thereby overcoming the limitations of conventional methods and proposing a novel framework for calibration of microscopic traffic simulations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synheart Emotion: Privacy-Preserving On-Device Emotion Recognition from Biosignals</title>
<link>https://arxiv.org/abs/2511.06231</link>
<guid>https://arxiv.org/abs/2511.06231</guid>
<content:encoded><![CDATA[

arXiv:2511.06231v1 Announce Type: new 
Abstract: Human-computer interaction increasingly demands systems that recognize not only explicit user inputs but also implicit emotional states. While substantial progress has been made in affective computing, most emotion recognition systems rely on cloud-based inference, introducing privacy vulnerabilities and latency constraints unsuitable for real-time applications. This work presents a comprehensive evaluation of machine learning architectures for on-device emotion recognition from wrist-based photoplethysmography (PPG), systematically comparing different models spanning classical ensemble methods, deep neural networks, and transformers on the WESAD stress detection dataset. Results demonstrate that classical ensemble methods substantially outperform deep learning on small physiological datasets, with ExtraTrees achieving F1 = 0.826 on combined features and F1 = 0.623 on wrist-only features, compared to transformers achieving only F1 = 0.509-0.577. We deploy the wrist-only ExtraTrees model optimized via ONNX conversion, achieving a 4.08 MB footprint, 0.05 ms inference latency, and 152x speedup over the original implementation. Furthermore, ONNX optimization yields a 30.5% average storage reduction and 40.1x inference speedup, highlighting the feasibility of privacy-preserving on-device emotion recognition for real-world wearables.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Laws and In-Context Learning: A Unified Theoretical Framework</title>
<link>https://arxiv.org/abs/2511.06232</link>
<guid>https://arxiv.org/abs/2511.06232</guid>
<content:encoded><![CDATA[

arXiv:2511.06232v1 Announce Type: new 
Abstract: In-context learning (ICL) enables large language models to adapt to new tasks from demonstrations without parameter updates. Despite extensive empirical studies, a principled understanding of ICL emergence at scale remains more elusive. We present a unified theoretical framework connecting scaling laws to ICL emergence in transformers. Our analysis establishes that ICL performance follows power-law relationships with model depth $L$, width $d$, context length $k$, and training data $D$, with exponents determined by task structure. We show that under specific conditions, transformers implement gradient-based metalearning in their forward pass, with an effective learning rate $\eta_{\text{eff}} = \Theta(1/\sqrt{Ld})$. We demonstrate sharp phase transitions at critical scales and derive optimal depth-width allocations favoring $L^* \propto N^{2/3}$, $d^* \propto N^{1/3}$ for the fixed parameter budget $N = Ld$. Systematic experiments on synthetic tasks validate our predictions, with measured scaling exponents closely matching theory. This work provides both necessary and sufficient conditions for the emergence of ICLs and establishes fundamental computational limits on what transformers can learn in-context.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixtures of SubExperts for Large Language Continual Learning</title>
<link>https://arxiv.org/abs/2511.06237</link>
<guid>https://arxiv.org/abs/2511.06237</guid>
<content:encoded><![CDATA[

arXiv:2511.06237v1 Announce Type: new 
Abstract: Adapting Large Language Models (LLMs) to a continuous stream of tasks is a critical yet challenging endeavor. While Parameter-Efficient Fine-Tuning (PEFT) methods have become a standard for this, they face a fundamental dilemma in continual learning. Reusing a single set of PEFT parameters for new tasks often leads to catastrophic forgetting of prior knowledge. Conversely, allocating distinct parameters for each task prevents forgetting but results in a linear growth of the model's size and fails to facilitate knowledge transfer between related tasks. To overcome these limitations, we propose a novel adaptive PEFT method referred to as \textit{Mixtures of SubExperts (MoSEs)}, a novel continual learning framework designed for minimal forgetting and efficient scalability. MoSEs integrate a sparse Mixture of SubExperts into the transformer layers, governed by a task-specific routing mechanism. This architecture allows the model to isolate and protect knowledge within dedicated SubExperts, thereby minimizing parameter interference and catastrophic forgetting. Crucially, the router can adaptively select and combine previously learned sparse parameters for new tasks, enabling effective knowledge transfer while ensuring that the model's capacity grows sublinearly. We evaluate MoSEs on the comprehensive TRACE benchmark datasets. Our experiments demonstrate that MoSEs significantly outperform conventional continual learning approaches in both knowledge retention and scalability to new tasks, achieving state-of-the-art performance with substantial memory and computational savings.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constraint-Informed Active Learning for End-to-End ACOPF Optimization Proxies</title>
<link>https://arxiv.org/abs/2511.06248</link>
<guid>https://arxiv.org/abs/2511.06248</guid>
<content:encoded><![CDATA[

arXiv:2511.06248v1 Announce Type: new 
Abstract: This paper studies optimization proxies, machine learning (ML) models trained to efficiently predict optimal solutions for AC Optimal Power Flow (ACOPF) problems. While promising, optimization proxy performance heavily depends on training data quality. To address this limitation, this paper introduces a novel active sampling framework for ACOPF optimization proxies designed to generate realistic and diverse training data. The framework actively explores varied, flexible problem specifications reflecting plausible operational realities. More importantly, the approach uses optimization-specific quantities (active constraint sets) that better capture the salient features of an ACOPF that lead to the optimal solution. Numerical results show superior generalization over existing sampling methods with an equivalent training budget, significantly advancing the state-of-practice for trustworthy ACOPF optimization proxies.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Iterative Error Correction for Efficient Diffusion Models</title>
<link>https://arxiv.org/abs/2511.06250</link>
<guid>https://arxiv.org/abs/2511.06250</guid>
<content:encoded><![CDATA[

arXiv:2511.06250v1 Announce Type: new 
Abstract: With the growing demand for high-quality image generation on resource-constrained devices, efficient diffusion models have received increasing attention. However, such models suffer from approximation errors introduced by efficiency techniques, which significantly degrade generation quality. Once deployed, these errors are difficult to correct, as modifying the model is typically infeasible in deployment environments. Through an analysis of error propagation across diffusion timesteps, we reveal that these approximation errors can accumulate exponentially, severely impairing output quality. Motivated by this insight, we propose Iterative Error Correction (IEC), a novel test-time method that mitigates inference-time errors by iteratively refining the model's output. IEC is theoretically proven to reduce error propagation from exponential to linear growth, without requiring any retraining or architectural changes. IEC can seamlessly integrate into the inference process of existing diffusion models, enabling a flexible trade-off between performance and efficiency. Extensive experiments show that IEC consistently improves generation quality across various datasets, efficiency techniques, and model architectures, establishing it as a practical and generalizable solution for test-time enhancement of efficient diffusion models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MrCoM: A Meta-Regularized World-Model Generalizing Across Multi-Scenarios</title>
<link>https://arxiv.org/abs/2511.06252</link>
<guid>https://arxiv.org/abs/2511.06252</guid>
<content:encoded><![CDATA[

arXiv:2511.06252v1 Announce Type: new 
Abstract: Model-based reinforcement learning (MBRL) is a crucial approach to enhance the generalization capabilities and improve the sample efficiency of RL algorithms. However, current MBRL methods focus primarily on building world models for single tasks and rarely address generalization across different scenarios. Building on the insight that dynamics within the same simulation engine share inherent properties, we attempt to construct a unified world model capable of generalizing across different scenarios, named Meta-Regularized Contextual World-Model (MrCoM). This method first decomposes the latent state space into various components based on the dynamic characteristics, thereby enhancing the accuracy of world-model prediction. Further, MrCoM adopts meta-state regularization to extract unified representation of scenario-relevant information, and meta-value regularization to align world-model optimization with policy learning across diverse scenario objectives. We theoretically analyze the generalization error upper bound of MrCoM in multi-scenario settings. We systematically evaluate our algorithm's generalization ability across diverse scenarios, demonstrating significantly better performance than previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Modality Barrier: Generative Modeling for Accurate Molecule Retrieval from Mass Spectra</title>
<link>https://arxiv.org/abs/2511.06259</link>
<guid>https://arxiv.org/abs/2511.06259</guid>
<content:encoded><![CDATA[

arXiv:2511.06259v1 Announce Type: new 
Abstract: Retrieving molecular structures from tandem mass spectra is a crucial step in rapid compound identification. Existing retrieval methods, such as traditional mass spectral library matching, suffer from limited spectral library coverage, while recent cross-modal representation learning frameworks often encounter modality misalignment, resulting in suboptimal retrieval accuracy and generalization. To address these limitations, we propose GLMR, a Generative Language Model-based Retrieval framework that mitigates the cross-modal misalignment through a two-stage process. In the pre-retrieval stage, a contrastive learning-based model identifies top candidate molecules as contextual priors for the input mass spectrum. In the generative retrieval stage, these candidate molecules are integrated with the input mass spectrum to guide a generative model in producing refined molecular structures, which are then used to re-rank the candidates based on molecular similarity. Experiments on both MassSpecGym and the proposed MassRET-20k dataset demonstrate that GLMR significantly outperforms existing methods, achieving over 40% improvement in top-1 accuracy and exhibiting strong generalizability.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAMP-HiVe: Cyclic Pair Merging based Efficient DNN Pruning with Hessian-Vector Approximation for Resource-Constrained Systems</title>
<link>https://arxiv.org/abs/2511.06265</link>
<guid>https://arxiv.org/abs/2511.06265</guid>
<content:encoded><![CDATA[

arXiv:2511.06265v1 Announce Type: new 
Abstract: Deep learning algorithms are becoming an essential component of many artificial intelligence (AI) driven applications, many of which run on resource-constrained and energy-constrained systems. For efficient deployment of these algorithms, although different techniques for the compression of neural network models are proposed, neural pruning is one of the fastest and effective methods, which can provide a high compression gain with minimal cost. To harness enhanced performance gain with respect to model complexity, we propose a novel neural network pruning approach utilizing Hessian-vector products that approximate crucial curvature information in the loss function, which significantly reduces the computation demands. By employing a power iteration method, our algorithm effectively identifies and preserves the essential information, ensuring a balanced trade-off between model accuracy and computational efficiency. Herein, we introduce CAMP-HiVe, a cyclic pair merging-based pruning with Hessian Vector approximation by iteratively consolidating weight pairs, combining significant and less significant weights, thus effectively streamlining the model while preserving its performance. This dynamic, adaptive framework allows for real-time adjustment of weight significance, ensuring that only the most critical parameters are retained. Our experimental results demonstrate that our proposed method achieves significant reductions in computational requirements while maintaining high performance across different neural network architectures, e.g., ResNet18, ResNet56, and MobileNetv2, on standard benchmark datasets, e.g., CIFAR10, CIFAR-100, and ImageNet, and it outperforms the existing state-of-the-art neural pruning methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM$^3$-DTI: A Large Language Model and Multi-modal data co-powered framework for Drug-Target Interaction prediction</title>
<link>https://arxiv.org/abs/2511.06269</link>
<guid>https://arxiv.org/abs/2511.06269</guid>
<content:encoded><![CDATA[

arXiv:2511.06269v1 Announce Type: new 
Abstract: Drug-target interaction (DTI) prediction is of great significance for drug discovery and drug repurposing. With the accumulation of a large volume of valuable data, data-driven methods have been increasingly harnessed to predict DTIs, reducing costs across various dimensions. Therefore, this paper proposes a $\textbf{L}$arge $\textbf{L}$anguage $\textbf{M}$odel and $\textbf{M}$ulti-$\textbf{M}$odel data co-powered $\textbf{D}$rug $\textbf{T}$arget $\textbf{I}$nteraction prediction framework, named LLM$^3$-DTI. LLM$^3$-DTI constructs multi-modal data embedding to enhance DTI prediction performance. In this framework, the text semantic embeddings of drugs and targets are encoded by a domain-specific LLM. To effectively align and fuse multi-modal embedding. We propose the dual cross-attention mechanism and the TSFusion module. Finally, these multi-modal data are utilized for the DTI task through an output network. The experimental results indicate that LLM$^3$-DTI can proficiently identify validated DTIs, surpassing the performance of the models employed for comparison across diverse scenarios. Consequently, LLM$^3$-DTI is adept at fulfilling the task of DTI prediction with excellence. The data and code are available at https://github.com/chaser-gua/LLM3DTI.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COTN: A Chaotic Oscillatory Transformer Network for Complex Volatile Systems under Extreme Conditions</title>
<link>https://arxiv.org/abs/2511.06273</link>
<guid>https://arxiv.org/abs/2511.06273</guid>
<content:encoded><![CDATA[

arXiv:2511.06273v1 Announce Type: new 
Abstract: Accurate prediction of financial and electricity markets, especially under extreme conditions, remains a significant challenge due to their intrinsic nonlinearity, rapid fluctuations, and chaotic patterns. To address these limitations, we propose the Chaotic Oscillatory Transformer Network (COTN). COTN innovatively combines a Transformer architecture with a novel Lee Oscillator activation function, processed through Max-over-Time pooling and a lambda-gating mechanism. This design is specifically tailored to effectively capture chaotic dynamics and improve responsiveness during periods of heightened volatility, where conventional activation functions (e.g., ReLU, GELU) tend to saturate. Furthermore, COTN incorporates an Autoencoder Self-Regressive (ASR) module to detect and isolate abnormal market patterns, such as sudden price spikes or crashes, thereby preventing corruption of the core prediction process and enhancing robustness. Extensive experiments across electricity spot markets and financial markets demonstrate the practical applicability and resilience of COTN. Our approach outperforms state-of-the-art deep learning models like Informer by up to 17% and traditional statistical methods like GARCH by as much as 40%. These results underscore COTN's effectiveness in navigating real-world market uncertainty and complexity, offering a powerful tool for forecasting highly volatile systems under duress.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Fairness Without Harm via Selective Demographic Experts</title>
<link>https://arxiv.org/abs/2511.06293</link>
<guid>https://arxiv.org/abs/2511.06293</guid>
<content:encoded><![CDATA[

arXiv:2511.06293v1 Announce Type: new 
Abstract: As machine learning systems become increasingly integrated into human-centered domains such as healthcare, ensuring fairness while maintaining high predictive performance is critical. Existing bias mitigation techniques often impose a trade-off between fairness and accuracy, inadvertently degrading performance for certain demographic groups. In high-stakes domains like clinical diagnosis, such trade-offs are ethically and practically unacceptable. In this study, we propose a fairness-without-harm approach by learning distinct representations for different demographic groups and selectively applying demographic experts consisting of group-specific representations and personalized classifiers through a no-harm constrained selection. We evaluate our approach on three real-world medical datasets -- covering eye disease, skin cancer, and X-ray diagnosis -- as well as two face datasets. Extensive empirical results demonstrate the effectiveness of our approach in achieving fairness without harm.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transolver is a Linear Transformer: Revisiting Physics-Attention through the Lens of Linear Attention</title>
<link>https://arxiv.org/abs/2511.06294</link>
<guid>https://arxiv.org/abs/2511.06294</guid>
<content:encoded><![CDATA[

arXiv:2511.06294v1 Announce Type: new 
Abstract: Recent advances in Transformer-based Neural Operators have enabled significant progress in data-driven solvers for Partial Differential Equations (PDEs). Most current research has focused on reducing the quadratic complexity of attention to address the resulting low training and inference efficiency. Among these works, Transolver stands out as a representative method that introduces Physics-Attention to reduce computational costs. Physics-Attention projects grid points into slices for slice attention, then maps them back through deslicing. However, we observe that Physics-Attention can be reformulated as a special case of linear attention, and that the slice attention may even hurt the model performance. Based on these observations, we argue that its effectiveness primarily arises from the slice and deslice operations rather than interactions between slices. Building on this insight, we propose a two-step transformation to redesign Physics-Attention into a canonical linear attention, which we call Linear Attention Neural Operator (LinearNO). Our method achieves state-of-the-art performance on six standard PDE benchmarks, while reducing the number of parameters by an average of 40.0% and computational cost by 36.2%. Additionally, it delivers superior performance on two challenging, industrial-level datasets: AirfRANS and Shape-Net Car.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3dSAGER: Geospatial Entity Resolution over 3D Objects (Technical Report)</title>
<link>https://arxiv.org/abs/2511.06300</link>
<guid>https://arxiv.org/abs/2511.06300</guid>
<content:encoded><![CDATA[

arXiv:2511.06300v1 Announce Type: new 
Abstract: Urban environments are continuously mapped and modeled by various data collection platforms, including satellites, unmanned aerial vehicles and street cameras. The growing availability of 3D geospatial data from multiple modalities has introduced new opportunities and challenges for integrating spatial knowledge at scale, particularly in high-impact domains such as urban planning and rapid disaster management. Geospatial entity resolution is the task of identifying matching spatial objects across different datasets, often collected independently under varying conditions. Existing approaches typically rely on spatial proximity, textual metadata, or external identifiers to determine correspondence. While useful, these signals are often unavailable, unreliable, or misaligned, especially in cross-source scenarios. To address these limitations, we shift the focus to the intrinsic geometry of 3D spatial objects and present 3dSAGER (3D Spatial-Aware Geospatial Entity Resolution), an end-to-end pipeline for geospatial entity resolution over 3D objects. 3dSAGER introduces a novel, spatial-reference-independent featurization mechanism that captures intricate geometric characteristics of matching pairs, enabling robust comparison even across datasets with incompatible coordinate systems where traditional spatial methods fail. As a key component of 3dSAGER, we also propose a new lightweight and interpretable blocking method, BKAFI, that leverages a trained model to efficiently generate high-recall candidate sets. We validate 3dSAGER through extensive experiments on real-world urban datasets, demonstrating significant gains in both accuracy and efficiency over strong baselines. Our empirical study further dissects the contributions of each component, providing insights into their impact and the overall design choices.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kaggle Chronicles: 15 Years of Competitions, Community and Data Science Innovation</title>
<link>https://arxiv.org/abs/2511.06304</link>
<guid>https://arxiv.org/abs/2511.06304</guid>
<content:encoded><![CDATA[

arXiv:2511.06304v1 Announce Type: new 
Abstract: Since 2010, Kaggle has been a platform where data scientists from around the world come together to compete, collaborate, and push the boundaries of Data Science. Over these 15 years, it has grown from a purely competition-focused site into a broader ecosystem with forums, notebooks, models, datasets, and more. With the release of the Kaggle Meta Code and Kaggle Meta Datasets, we now have a unique opportunity to explore these competitions, technologies, and real-world applications of Machine Learning and AI. And so in this study, we take a closer look at 15 years of data science on Kaggle - through metadata, shared code, community discussions, and the competitions themselves. We explore Kaggle's growth, its impact on the data science community, uncover hidden technological trends, analyze competition winners, how Kagglers approach problems in general, and more. We do this by analyzing millions of kernels and discussion threads to perform both longitudinal trend analysis and standard exploratory data analysis. Our findings show that Kaggle is a steadily growing platform with increasingly diverse use cases, and that Kagglers are quick to adapt to new trends and apply them to real-world challenges, while producing - on average - models with solid generalization capabilities. We also offer a snapshot of the platform as a whole, highlighting its history and technological evolution. Finally, this study is accompanied by a video (https://www.youtube.com/watch?v=YVOV9bIUNrM) and a Kaggle write-up (https://kaggle.com/competitions/meta-kaggle-hackathon/writeups/kaggle-chronicles-15-years-of-competitions-communi) for your convenience.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation</title>
<link>https://arxiv.org/abs/2511.06307</link>
<guid>https://arxiv.org/abs/2511.06307</guid>
<content:encoded><![CDATA[

arXiv:2511.06307v1 Announce Type: new 
Abstract: Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a resurgence of interest in RLVR. Nevertheless, advances are dominated by mathematics (e.g., AIME), with competitive-programming code generation underexplored and data curation receiving less attention than RL algorithm design. We investigate how to construct RLVR datasets (i.e., RL prompts) and present practical training techniques that yield strong performance on competitive-programming code generation. Our pipeline begins with supervised fine-tuning (SFT) distilled from strong open-source models, augmented with general-purpose and reasoning-intensive data. RL then follows a two-stage process with executable, testcase-driven rewards: first, training on a large, uniformly distributed set of competitive-programming problems using Group Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively short response-generation window (e.g., 32k during SFT and 24k in this stage) to expand entropy and mitigate repetition and truncation; second, we perform \textbf{Pre-GRPO}: updating on a small, high-quality set of challenging problems with a large rollout budget (64 rollouts per prompt) under a hard-focus curriculum that continuously retains the most difficult instances throughout training. We implement our method on Qwen2.5-32B and evaluate on LeetCode and Codeforces weekly contests to avoid data leakage. The resulting model achieves state-of-the-art performance among models of similar scale and is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking. We also examine scaling trends and observe strong RL scaling on an internal large-scale MoE model. Our study distills concise best practices for data curation, entropy expansion, and curriculum design in RLVR for competitive-programming code generation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Verification of Neural Control Barrier Functions Using Linear Bound Propagation</title>
<link>https://arxiv.org/abs/2511.06341</link>
<guid>https://arxiv.org/abs/2511.06341</guid>
<content:encoded><![CDATA[

arXiv:2511.06341v1 Announce Type: new 
Abstract: Control barrier functions (CBFs) are a popular tool for safety certification of nonlinear dynamical control systems. Recently, CBFs represented as neural networks have shown great promise due to their expressiveness and applicability to a broad class of dynamics and safety constraints. However, verifying that a trained neural network is indeed a valid CBF is a computational bottleneck that limits the size of the networks that can be used. To overcome this limitation, we present a novel framework for verifying neural CBFs based on piecewise linear upper and lower bounds on the conditions required for a neural network to be a CBF. Our approach is rooted in linear bound propagation (LBP) for neural networks, which we extend to compute bounds on the gradients of the network. Combined with McCormick relaxation, we derive linear upper and lower bounds on the CBF conditions, thereby eliminating the need for computationally expensive verification procedures. Our approach applies to arbitrary control-affine systems and a broad range of nonlinear activation functions. To reduce conservatism, we develop a parallelizable refinement strategy that adaptively refines the regions over which these bounds are computed. Our approach scales to larger neural networks than state-of-the-art verification procedures for CBFs, as demonstrated by our numerical experiments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets</title>
<link>https://arxiv.org/abs/2511.06356</link>
<guid>https://arxiv.org/abs/2511.06356</guid>
<content:encoded><![CDATA[

arXiv:2511.06356v1 Announce Type: new 
Abstract: Chemical reaction prediction remains a fundamental challenge in organic chemistry, where existing machine learning models face two critical limitations: sensitivity to input permutations (molecule/atom orderings) and inadequate modeling of substructural interactions governing reactivity. These shortcomings lead to inconsistent predictions and poor generalization to real-world scenarios. To address these challenges, we propose ReaDISH, a novel reaction prediction model that learns permutation-invariant representations while incorporating interaction-aware features. It introduces two innovations: (1) symmetric difference shingle encoding, which computes molecular shingle differences to capture reaction-specific structural changes while eliminating order sensitivity; and (2) geometry-structure interaction attention, a mechanism that models intra- and inter-molecular interactions at the shingle level. Extensive experiments demonstrate that ReaDISH improves reaction prediction performance across diverse benchmarks. It shows enhanced robustness with an average improvement of 8.76% on R$^2$ under permutation perturbations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Federated Learning for Fair and Efficient Urban Traffic Optimization</title>
<link>https://arxiv.org/abs/2511.06363</link>
<guid>https://arxiv.org/abs/2511.06363</guid>
<content:encoded><![CDATA[

arXiv:2511.06363v1 Announce Type: new 
Abstract: The optimization of urban traffic is threatened by the complexity of achieving a balance between transport efficiency and the maintenance of privacy, as well as the equitable distribution of traffic based on socioeconomically diverse neighborhoods. Current centralized traffic management schemes invade user location privacy and further entrench traffic disparity by offering disadvantaged route suggestions, whereas current federated learning frameworks do not consider fairness constraints in multi-objective traffic settings. This study presents a privacy-preserving federated learning framework, termed FedFair-Traffic, that jointly and simultaneously optimizes travel efficiency, traffic fairness, and differential privacy protection. This is the first attempt to integrate three conflicting objectives to improve urban transportation systems. The proposed methodology enables collaborative learning between related vehicles with data locality by integrating Graph Neural Networks with differential privacy mechanisms ($\epsilon$-privacy guarantees) and Gini coefficient-based fair constraints using multi-objective optimization. The framework uses federated aggregation methods of gradient clipping and noise injection to provide differential privacy and optimize Pareto-efficient solutions for the efficiency-fairness tradeoff. Real-world comprehensive experiments on the METR-LA traffic dataset showed that FedFair-Traffic can reduce the average travel time by 7\% (14.2 minutes) compared with their centralized baselines, promote traffic fairness by 73\% (Gini coefficient, 0.78), and offer high privacy protection (privacy score, 0.8) with an 89\% reduction in communication overhead. These outcomes demonstrate that FedFair-Traffic is a scalable privacy-aware smart city infrastructure with possible use-cases in metropolitan traffic flow control and federated transportation networks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Regularization for Large-Scale Sparse Feature Embedding Models</title>
<link>https://arxiv.org/abs/2511.06374</link>
<guid>https://arxiv.org/abs/2511.06374</guid>
<content:encoded><![CDATA[

arXiv:2511.06374v1 Announce Type: new 
Abstract: The one-epoch overfitting problem has drawn widespread attention, especially in CTR and CVR estimation models in search, advertising, and recommendation domains. These models which rely heavily on large-scale sparse categorical features, often suffer a significant decline in performance when trained for multiple epochs. Although recent studies have proposed heuristic solutions, they have not clearly identified the fundamental cause of this phenomenon. In this work, we provide a theoretical analysis that explains why overfitting occurs in models that use large-scale sparse categorical features. Based on this analysis, we propose an adaptive regularization method to address it. Our approach not only prevents the severe performance degradation observed during multi-epoch training, but also improves model performance within a single epoch. This method has already been deployed in online production systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vocabulary In-Context Learning in Transformers: Benefits of Positional Encoding</title>
<link>https://arxiv.org/abs/2511.06376</link>
<guid>https://arxiv.org/abs/2511.06376</guid>
<content:encoded><![CDATA[

arXiv:2511.06376v1 Announce Type: new 
Abstract: Numerous studies have demonstrated that the Transformer architecture possesses the capability for in-context learning (ICL). In scenarios involving function approximation, context can serve as a control parameter for the model, endowing it with the universal approximation property (UAP). In practice, context is represented by tokens from a finite set, referred to as a vocabulary, which is the case considered in this paper, \emph{i.e.}, vocabulary in-context learning (VICL). We demonstrate that VICL in single-layer Transformers, without positional encoding, does not possess the UAP; however, it is possible to achieve the UAP when positional encoding is included. Several sufficient conditions for the positional encoding are provided. Our findings reveal the benefits of positional encoding from an approximation theory perspective in the context of ICL.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CG-TTRL: Context-Guided Test-Time Reinforcement Learning for On-Device Large Language Models</title>
<link>https://arxiv.org/abs/2511.06430</link>
<guid>https://arxiv.org/abs/2511.06430</guid>
<content:encoded><![CDATA[

arXiv:2511.06430v1 Announce Type: new 
Abstract: Test-time Reinforcement Learning (TTRL) has shown promise in adapting foundation models for complex tasks at test-time, resulting in large performance improvements. TTRL leverages an elegant two-phase sampling strategy: first, multi-sampling derives a pseudo-label via majority voting, while subsequent downsampling and reward-based fine-tuning encourages the model to explore and learn diverse valid solutions, with the pseudo-label modulating the reward signal. Meanwhile, in-context learning has been widely explored at inference time and demonstrated the ability to enhance model performance without weight updates. However, TTRL's two-phase sampling strategy under-utilizes contextual guidance, which can potentially improve pseudo-label accuracy in the initial exploitation phase while regulating exploration in the second. To address this, we propose context-guided TTRL (CG-TTRL), integrating context dynamically into both sampling phases and propose a method for efficient context selection for on-device applications. Our evaluations on mathematical and scientific QA benchmarks show CG-TTRL outperforms TTRL (e.g. additional 7% relative accuracy improvement over TTRL), while boosting efficiency by obtaining strong performance after only a few steps of test-time training (e.g. 8% relative improvement rather than 1% over TTRL after 3 steps).
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Wide and How Deep? Mitigating Over-Squashing of GNNs via Channel Capacity Constrained Estimation</title>
<link>https://arxiv.org/abs/2511.06443</link>
<guid>https://arxiv.org/abs/2511.06443</guid>
<content:encoded><![CDATA[

arXiv:2511.06443v1 Announce Type: new 
Abstract: Existing graph neural networks typically rely on heuristic choices for hidden dimensions and propagation depths, which often lead to severe information loss during propagation, known as over-squashing. To address this issue, we propose Channel Capacity Constrained Estimation (C3E), a novel framework that formulates the selection of hidden dimensions and depth as a nonlinear programming problem grounded in information theory. Through modeling spectral graph neural networks as communication channels, our approach directly connects channel capacity to hidden dimensions, propagation depth, propagation mechanism, and graph structure. Extensive experiments on nine public datasets demonstrate that hidden dimensions and depths estimated by C3E can mitigate over-squashing and consistently improve representation learning. Experimental results show that over-squashing occurs due to the cumulative compression of information in representation matrices. Furthermore, our findings show that increasing hidden dimensions indeed mitigate information compression, while the role of propagation depth is more nuanced, uncovering a fundamental balance between information compression and representation complexity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLEX: Continuous Agent Evolution via Forward Learning from Experience</title>
<link>https://arxiv.org/abs/2511.06449</link>
<guid>https://arxiv.org/abs/2511.06449</guid>
<content:encoded><![CDATA[

arXiv:2511.06449v1 Announce Type: new 
Abstract: Autonomous agents driven by Large Language Models (LLMs) have revolutionized reasoning and problem-solving but remain static after training, unable to grow with experience as intelligent beings do during deployment. We introduce Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that enables LLM agents to continuously evolve through accumulated experience. Specifically, FLEX cultivates scalable and inheritable evolution by constructing a structured experience library through continual reflection on successes and failures during interaction with the environment. FLEX delivers substantial improvements on mathematical reasoning, chemical retrosynthesis, and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14% on ProteinGym). We further identify a clear scaling law of experiential growth and the phenomenon of experience inheritance across agents, marking a step toward scalable and inheritable continuous agent evolution. Project Page: https://flex-gensi-thuair.github.io.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Risk-Neutral Neural Operator for Arbitrage-Free SPX-VIX Term Structures</title>
<link>https://arxiv.org/abs/2511.06451</link>
<guid>https://arxiv.org/abs/2511.06451</guid>
<content:encoded><![CDATA[

arXiv:2511.06451v1 Announce Type: new 
Abstract: We propose ARBITER, a risk-neutral neural operator for learning joint SPX-VIX term structures under no-arbitrage constraints. ARBITER maps market states to an operator that outputs implied volatility and variance curves while enforcing static arbitrage (calendar, vertical, butterfly), Lipschitz bounds, and monotonicity. The model couples operator learning with constrained decoders and is trained with extragradient-style updates plus projection. We introduce evaluation metrics for derivatives term structures (NAS, CNAS, NI, Dual-Gap, Stability Rate) and show gains over Fourier Neural Operator, DeepONet, and state-space sequence models on historical SPX and VIX data. Ablation studies indicate that tying the SPX and VIX legs reduces Dual-Gap and improves NI, Lipschitz projection stabilizes calibration, and selective state updates improve long-horizon generalization. We provide identifiability and approximation results and describe practical recipes for arbitrage-free interpolation and extrapolation across maturities and strikes.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MULTIBENCH++: A Unified and Comprehensive Multimodal Fusion Benchmarking Across Specialized Domains</title>
<link>https://arxiv.org/abs/2511.06452</link>
<guid>https://arxiv.org/abs/2511.06452</guid>
<content:encoded><![CDATA[

arXiv:2511.06452v1 Announce Type: new 
Abstract: Although multimodal fusion has made significant progress, its advancement is severely hindered by the lack of adequate evaluation benchmarks. Current fusion methods are typically evaluated on a small selection of public datasets, a limited scope that inadequately represents the complexity and diversity of real-world scenarios, potentially leading to biased evaluations. This issue presents a twofold challenge. On one hand, models may overfit to the biases of specific datasets, hindering their generalization to broader practical applications. On the other hand, the absence of a unified evaluation standard makes fair and objective comparisons between different fusion methods difficult. Consequently, a truly universal and high-performance fusion model has yet to emerge. To address these challenges, we have developed a large-scale, domain-adaptive benchmark for multimodal evaluation. This benchmark integrates over 30 datasets, encompassing 15 modalities and 20 predictive tasks across key application domains. To complement this, we have also developed an open-source, unified, and automated evaluation pipeline that includes standardized implementations of state-of-the-art models and diverse fusion paradigms. Leveraging this platform, we have conducted large-scale experiments, successfully establishing new performance baselines across multiple tasks. This work provides the academic community with a crucial platform for rigorous and reproducible assessment of multimodal models, aiming to propel the field of multimodal artificial intelligence to new heights.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstruction and Secrecy under Approximate Distance Queries</title>
<link>https://arxiv.org/abs/2511.06461</link>
<guid>https://arxiv.org/abs/2511.06461</guid>
<content:encoded><![CDATA[

arXiv:2511.06461v1 Announce Type: new 
Abstract: Consider the task of locating an unknown target point using approximate distance queries: in each round, a reconstructor selects a query point and receives a noisy version of its distance to the target. This problem arises naturally in various contexts ranging from localization in GPS and sensor networks to privacy-aware data access, and spans a wide variety of metric spaces. It is relevant from the perspective of both the reconstructor (seeking accurate recovery) and the responder (aiming to limit information disclosure, e.g., for privacy or security reasons). We study this reconstruction game through a learning-theoretic lens, focusing on the rate and limits of the best possible reconstruction error. Our first result provides a tight geometric characterization of the optimal error in terms of the Chebyshev radius, a classical concept from geometry. This characterization applies to all compact metric spaces (in fact, even to all totally bounded spaces) and yields explicit formulas for natural metric spaces. Our second result addresses the asymptotic behavior of reconstruction, distinguishing between pseudo-finite spaces -- where the optimal error is attained after finitely many queries -- and spaces where the approximation curve exhibits nontrivial decay. We characterize pseudo-finiteness for convex Euclidean spaces.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Estimate and Convergence Analysis for Data Valuation</title>
<link>https://arxiv.org/abs/2511.06463</link>
<guid>https://arxiv.org/abs/2511.06463</guid>
<content:encoded><![CDATA[

arXiv:2511.06463v1 Announce Type: new 
Abstract: Data valuation quantifies data importance, but existing methods cannot ensure validity in a single training process. The neural dynamic data valuation (NDDV) method [3] addresses this limitation. Based on NDDV, we are the first to explore error estimation and convergence analysis in data valuation. Under Lipschitz and smoothness assumptions, we derive quadratic error bounds for loss differences that scale inversely with time steps and quadratically with control variations, ensuring stability. We also prove that the expected squared gradient norm for the training loss vanishes asymptotically, and that the meta loss converges sublinearly over iterations. In particular, NDDV achieves sublinear convergence.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DyKAF: Dynamical Kronecker Approximation of the Fisher Information Matrix for Gradient Preconditioning</title>
<link>https://arxiv.org/abs/2511.06477</link>
<guid>https://arxiv.org/abs/2511.06477</guid>
<content:encoded><![CDATA[

arXiv:2511.06477v1 Announce Type: new 
Abstract: Recently, optimizers that explicitly treat weights as matrices, rather than flattened vectors, have demonstrated their effectiveness. This perspective naturally leads to structured approximations of the Fisher matrix as preconditioners, where the matrix view induces a Kronecker-factorized form that enables memory-efficient representation. However, constructing such approximations both efficiently and accurately remains an open challenge, since obtaining the optimal factorization is resource-intensive and practical methods therefore rely on heuristic design choices. In this work, we introduce a novel approach that leverages projector-splitting integrators to construct effective preconditioners. Our optimizer, DyKAF (Dynamical Kronecker Approximation of the Fisher Matrix), consistently improves the Fisher matrix approximation quality. Experiments on large language model pre-training and fine-tuning demonstrate that DyKAF outperforms existing optimizers across a range of evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI For Early Detection Of Sepsis</title>
<link>https://arxiv.org/abs/2511.06492</link>
<guid>https://arxiv.org/abs/2511.06492</guid>
<content:encoded><![CDATA[

arXiv:2511.06492v1 Announce Type: new 
Abstract: Sepsis is a life-threatening condition that requires rapid detection and treatment to prevent progression to severe sepsis, septic shock, or multi-organ failure. Despite advances in medical technology, it remains a major challenge for clinicians. While recent machine learning models have shown promise in predicting sepsis onset, their black-box nature limits interpretability and clinical trust. In this study, we present an interpretable AI approach for sepsis analysis that integrates machine learning with clinical knowledge. Our method not only delivers accurate predictions of sepsis onset but also enables clinicians to understand, validate, and align model outputs with established medical expertise.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Time-Varying Graph Signals via Koopman</title>
<link>https://arxiv.org/abs/2511.06493</link>
<guid>https://arxiv.org/abs/2511.06493</guid>
<content:encoded><![CDATA[

arXiv:2511.06493v1 Announce Type: new 
Abstract: A wide variety of real-world data, such as sea measurements, e.g., temperatures collected by distributed sensors and multiple unmanned aerial vehicles (UAV) trajectories, can be naturally represented as graphs, often exhibiting non-Euclidean structures. These graph representations may evolve over time, forming time-varying graphs. Effectively modeling and analyzing such dynamic graph data is critical for tasks like predicting graph evolution and reconstructing missing graph data. In this paper, we propose a framework based on the Koopman autoencoder (KAE) to handle time-varying graph data. Specifically, we assume the existence of a hidden non-linear dynamical system, where the state vector corresponds to the graph embedding of the time-varying graph signals. To capture the evolving graph structures, the graph data is first converted into a vector time series through graph embedding, representing the structural information in a finite-dimensional latent space. In this latent space, the KAE is applied to learn the underlying non-linear dynamics governing the temporal evolution of graph features, enabling both prediction and reconstruction tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Route Experts by Sequence, not by Token</title>
<link>https://arxiv.org/abs/2511.06494</link>
<guid>https://arxiv.org/abs/2511.06494</guid>
<content:encoded><![CDATA[

arXiv:2511.06494v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) architectures scale large language models (LLMs) by activating only a subset of experts per token, but the standard TopK routing assigns the same fixed number of experts to all tokens, ignoring their varying complexity. Prior adaptive routing methods introduce additional modules and hyperparameters, often requiring costly retraining from scratch. We propose Sequence-level TopK (SeqTopK), a minimal modification that shifts the expert budget from the token level to the sequence level. By selecting the top $T \cdot K$ experts across all $T$ tokens, SeqTopK enables end-to-end learned dynamic allocation -- assigning more experts to difficult tokens and fewer to easy ones -- while preserving the same overall budget. SeqTopK requires only a few lines of code, adds less than 1% overhead, and remains fully compatible with pretrained MoE models. Experiments across math, coding, law, and writing show consistent improvements over TopK and prior parameter-free adaptive methods, with gains that become substantially larger under higher sparsity (up to 16.9%). These results highlight SeqTopK as a simple, efficient, and scalable routing strategy, particularly well-suited for the extreme sparsity regimes of next-generation LLMs. Code is available at https://github.com/Y-Research-SBU/SeqTopK.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probably Approximately Global Robustness Certification</title>
<link>https://arxiv.org/abs/2511.06495</link>
<guid>https://arxiv.org/abs/2511.06495</guid>
<content:encoded><![CDATA[

arXiv:2511.06495v1 Announce Type: new 
Abstract: We propose and investigate probabilistic guarantees for the adversarial robustness of classification algorithms. While traditional formal verification approaches for robustness are intractable and sampling-based approaches do not provide formal guarantees, our approach is able to efficiently certify a probabilistic relaxation of robustness. The key idea is to sample an $\epsilon$-net and invoke a local robustness oracle on the sample. Remarkably, the size of the sample needed to achieve probably approximately global robustness guarantees is independent of the input dimensionality, the number of classes, and the learning algorithm itself. Our approach can, therefore, be applied even to large neural networks that are beyond the scope of traditional formal verification. Experiments empirically confirm that it characterizes robustness better than state-of-the-art sampling-based approaches and scales better than formal methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Approximation of Volterra Series for High-Dimensional Systems</title>
<link>https://arxiv.org/abs/2511.06527</link>
<guid>https://arxiv.org/abs/2511.06527</guid>
<content:encoded><![CDATA[

arXiv:2511.06527v1 Announce Type: new 
Abstract: The identification of high-dimensional nonlinear dynamical systems via the Volterra series has significant potential, but has been severely hindered by the curse of dimensionality. Tensor Network (TN) methods such as the Modified Alternating Linear Scheme (MVMALS) have been a breakthrough for the field, offering a tractable approach by exploiting the low-rank structure in Volterra kernels. However, these techniques still encounter prohibitive computational and memory bottlenecks due to high-order polynomial scaling with respect to input dimension. To overcome this barrier, we introduce the Tensor Head Averaging (THA) algorithm, which significantly reduces complexity by constructing an ensemble of localized MVMALS models trained on small subsets of the input space. In this paper, we present a theoretical foundation for the THA algorithm. We establish observable, finite-sample bounds on the error between the THA ensemble and a full MVMALS model, and we derive an exact decomposition of the squared error. This decomposition is used to analyze the manner in which subset models implicitly compensate for omitted dynamics. We quantify this effect, and prove that correlation between the included and omitted dynamics creates an optimization incentive which drives THA's performance toward accuracy superior to a simple truncation of a full MVMALS model. THA thus offers a scalable and theoretically grounded approach for identifying previously intractable high-dimensional systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriShGAN: Enhancing Sparsity and Robustness in Multivariate Time Series Counterfactuals Explanation</title>
<link>https://arxiv.org/abs/2511.06529</link>
<guid>https://arxiv.org/abs/2511.06529</guid>
<content:encoded><![CDATA[

arXiv:2511.06529v1 Announce Type: new 
Abstract: In decision-making processes, stakeholders often rely on counterfactual explanations, which provide suggestions about what should be changed in the queried instance to alter the outcome of an AI system. However, generating these explanations for multivariate time series presents challenges due to their complex, multi-dimensional nature. Traditional Nearest Unlike Neighbor-based methods typically substitute subsequences in a queried time series with influential subsequences from an NUN, which is not always realistic in real-world scenarios due to the rigid direct substitution. Counterfactual with Residual Generative Adversarial Networks-based methods aim to address this by learning from the distribution of observed data to generate synthetic counterfactual explanations. However, these methods primarily focus on minimizing the cost from the queried time series to the counterfactual explanations and often neglect the importance of distancing the counterfactual explanation from the decision boundary. This oversight can result in explanations that no longer qualify as counterfactual if minor changes occur within the model. To generate a more robust counterfactual explanation, we introduce TriShGAN, under the CounteRGAN framework enhanced by the incorporation of triplet loss. This unsupervised learning approach uses distance metric learning to encourage the counterfactual explanations not only to remain close to the queried time series but also to capture the feature distribution of the instance with the desired outcome, thereby achieving a better balance between minimal cost and robustness. Additionally, we integrate a Shapelet Extractor that strategically selects the most discriminative parts of the high-dimensional queried time series to enhance the sparsity of counterfactual explanation and efficiency of the training process.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Uncertainty Quantification with Anchored Ensembles for Robust EV Power Consumption Prediction</title>
<link>https://arxiv.org/abs/2511.06538</link>
<guid>https://arxiv.org/abs/2511.06538</guid>
<content:encoded><![CDATA[

arXiv:2511.06538v1 Announce Type: new 
Abstract: Accurate EV power estimation underpins range prediction and energy management, yet practitioners need both point accuracy and trustworthy uncertainty. We propose an anchored-ensemble Long Short-Term Memory (LSTM) with a Student-t likelihood that jointly captures epistemic (model) and aleatoric (data) uncertainty. Anchoring imposes a Gaussian weight prior (MAP training), yielding posterior-like diversity without test-time sampling, while the t-head provides heavy-tailed robustness and closed-form prediction intervals. Using vehicle-kinematic time series (e.g., speed, motor RPM), our model attains strong accuracy: RMSE 3.36 +/- 1.10, MAE 2.21 +/- 0.89, R-squared = 0.93 +/- 0.02, explained variance 0.93 +/- 0.02, and delivers well-calibrated uncertainty bands with near-nominal coverage. Against competitive baselines (Student-t MC dropout; quantile regression with/without anchoring), our method matches or improves log-scores while producing sharper intervals at the same coverage. Crucially for real-time deployment, inference is a single deterministic pass per ensemble member (or a weight-averaged collapse), eliminating Monte Carlo latency. The result is a compact, theoretically grounded estimator that couples accuracy, calibration, and systems efficiency, enabling reliable range estimation and decision-making for production EV energy management.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Policy Distillation for Reinforcement Learning in Radio Access Networks</title>
<link>https://arxiv.org/abs/2511.06563</link>
<guid>https://arxiv.org/abs/2511.06563</guid>
<content:encoded><![CDATA[

arXiv:2511.06563v1 Announce Type: new 
Abstract: Adopting artificial intelligence (AI) in radio access networks (RANs) presents several challenges, including limited availability of link-level measurements (e.g., CQI reports), stringent real-time processing constraints (e.g., sub-1 ms per TTI), and network heterogeneity (different spectrum bands, cell types, and vendor equipment). A critical yet often overlooked barrier lies in the computational and memory limitations of RAN baseband hardware, particularly in legacy 4th Generation (4G) systems, which typically lack on-chip neural accelerators. As a result, only lightweight AI models (under 1 Mb and sub-100~\mu s inference time) can be effectively deployed, limiting both their performance and applicability. However, achieving strong generalization across diverse network conditions often requires large-scale models with substantial resource demands. To address this trade-off, this paper investigates policy distillation in the context of a reinforcement learning-based link adaptation task. We explore two strategies: single-policy distillation, where a scenario-agnostic teacher model is compressed into one generalized student model; and multi-policy distillation, where multiple scenario-specific teachers are consolidated into a single generalist student. Experimental evaluations in a high-fidelity, 5th Generation (5G)-compliant simulator demonstrate that both strategies produce compact student models that preserve the teachers' generalization capabilities while complying with the computational and memory limitations of existing RAN hardware.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Dyadic Barrier: Rethinking Fairness in Link Prediction Beyond Demographic Parity</title>
<link>https://arxiv.org/abs/2511.06568</link>
<guid>https://arxiv.org/abs/2511.06568</guid>
<content:encoded><![CDATA[

arXiv:2511.06568v1 Announce Type: new 
Abstract: Link prediction is a fundamental task in graph machine learning with applications, ranging from social recommendation to knowledge graph completion. Fairness in this setting is critical, as biased predictions can exacerbate societal inequalities. Prior work adopts a dyadic definition of fairness, enforcing fairness through demographic parity between intra-group and inter-group link predictions. However, we show that this dyadic framing can obscure underlying disparities across subgroups, allowing systemic biases to go undetected. Moreover, we argue that demographic parity does not meet desired properties for fairness assessment in ranking-based tasks such as link prediction. We formalize the limitations of existing fairness evaluations and propose a framework that enables a more expressive assessment. Additionally, we propose a lightweight post-processing method combined with decoupled link predictors that effectively mitigates bias and achieves state-of-the-art fairness-utility trade-offs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimistic Online-to-Batch Conversions for Accelerated Convergence and Universality</title>
<link>https://arxiv.org/abs/2511.06597</link>
<guid>https://arxiv.org/abs/2511.06597</guid>
<content:encoded><![CDATA[

arXiv:2511.06597v1 Announce Type: new 
Abstract: In this work, we study offline convex optimization with smooth objectives, where the classical Nesterov's Accelerated Gradient (NAG) method achieves the optimal accelerated convergence. Extensive research has aimed to understand NAG from various perspectives, and a recent line of work approaches this from the viewpoint of online learning and online-to-batch conversion, emphasizing the role of optimistic online algorithms for acceleration. In this work, we contribute to this perspective by proposing novel optimistic online-to-batch conversions that incorporate optimism theoretically into the analysis, thereby significantly simplifying the online algorithm design while preserving the optimal convergence rates. Specifically, we demonstrate the effectiveness of our conversions through the following results: (i) when combined with simple online gradient descent, our optimistic conversion achieves the optimal accelerated convergence; (ii) our conversion also applies to strongly convex objectives, and by leveraging both optimistic online-to-batch conversion and optimistic online algorithms, we achieve the optimal accelerated convergence rate for strongly convex and smooth objectives, for the first time through the lens of online-to-batch conversion; (iii) our optimistic conversion can achieve universality to smoothness -- applicable to both smooth and non-smooth objectives without requiring knowledge of the smoothness coefficient -- and remains efficient as non-universal methods by using only one gradient query in each iteration. Finally, we highlight the effectiveness of our optimistic online-to-batch conversions by a precise correspondence with NAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Initial Residual Connections for GNNs with Theoretical Guarantees</title>
<link>https://arxiv.org/abs/2511.06598</link>
<guid>https://arxiv.org/abs/2511.06598</guid>
<content:encoded><![CDATA[

arXiv:2511.06598v1 Announce Type: new 
Abstract: Message passing is the core operation in graph neural networks, where each node updates its embeddings by aggregating information from its neighbors. However, in deep architectures, this process often leads to diminished expressiveness. A popular solution is to use residual connections, where the input from the current (or initial) layer is added to aggregated neighbor information to preserve embeddings across layers. Following a recent line of research, we investigate an adaptive residual scheme in which different nodes have varying residual strengths. We prove that this approach prevents oversmoothing; particularly, we show that the Dirichlet energy of the embeddings remains bounded away from zero. This is the first theoretical guarantee not only for the adaptive setting, but also for static residual connections (where residual strengths are shared across nodes) with activation functions. Furthermore, extensive experiments show that this adaptive approach outperforms standard and state-of-the-art message passing mechanisms, especially on heterophilic graphs. To improve the time complexity of our approach, we introduce a variant in which residual strengths are not learned but instead set heuristically, a choice that performs as well as the learnable version.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Probabilistic Machine Learning for Predicting Drilling Fluid Loss of Circulation in Marun Oil Field</title>
<link>https://arxiv.org/abs/2511.06607</link>
<guid>https://arxiv.org/abs/2511.06607</guid>
<content:encoded><![CDATA[

arXiv:2511.06607v1 Announce Type: new 
Abstract: Lost circulation remains a major and costly challenge in drilling operations, often resulting in wellbore instability, stuck pipe, and extended non-productive time. Accurate prediction of fluid loss is therefore essential for improving drilling safety and efficiency. This study presents a probabilistic machine learning framework based on Gaussian Process Regression (GPR) for predicting drilling fluid loss in complex formations. The GPR model captures nonlinear dependencies among drilling parameters while quantifying predictive uncertainty, offering enhanced reliability for high-risk decision-making. Model hyperparameters are optimized using the Limited memory Broyden Fletcher Goldfarb Shanno (LBFGS) algorithm to ensure numerical stability and robust generalization. To improve interpretability, Local Interpretable Model agnostic Explanations (LIME) are employed to elucidate how individual features influence model predictions. The results highlight the potential of explainable probabilistic learning for proactive identification of lost-circulation risks, optimized design of lost circulation materials (LCM), and reduction of operational uncertainties in drilling applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Fixed Depth: Adaptive Graph Neural Networks for Node Classification Under Varying Homophily</title>
<link>https://arxiv.org/abs/2511.06608</link>
<guid>https://arxiv.org/abs/2511.06608</guid>
<content:encoded><![CDATA[

arXiv:2511.06608v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have achieved significant success in addressing node classification tasks. However, the effectiveness of traditional GNNs degrades on heterophilic graphs, where connected nodes often belong to different labels or properties. While recent work has introduced mechanisms to improve GNN performance under heterophily, certain key limitations still exist. Most existing models apply a fixed aggregation depth across all nodes, overlooking the fact that nodes may require different propagation depths based on their local homophily levels and neighborhood structures. Moreover, many methods are tailored to either homophilic or heterophilic settings, lacking the flexibility to generalize across both regimes. To address these challenges, we develop a theoretical framework that links local structural and label characteristics to information propagation dynamics at the node level. Our analysis shows that optimal aggregation depth varies across nodes and is critical for preserving class-discriminative information. Guided by this insight, we propose a novel adaptive-depth GNN architecture that dynamically selects node-specific aggregation depths using theoretically grounded metrics. Our method seamlessly adapts to both homophilic and heterophilic patterns within a unified model. Extensive experiments demonstrate that our approach consistently enhances the performance of standard GNN backbones across diverse benchmarks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Weak Penalty Neural ODE for Learning Chaotic Dynamics from Noisy Time Series</title>
<link>https://arxiv.org/abs/2511.06609</link>
<guid>https://arxiv.org/abs/2511.06609</guid>
<content:encoded><![CDATA[

arXiv:2511.06609v1 Announce Type: new 
Abstract: Accurate forecasting of complex high-dimensional dynamical systems from observational data is essential for several applications across science and engineering. A key challenge, however, is that real-world measurements are often corrupted by noise, which severely degrades the performance of data-driven models. Particularly, in chaotic dynamical systems, where small errors amplify rapidly, it is challenging to identify a data-driven model from noisy data that achieves short-term accuracy while preserving long-term invariant properties. In this paper, we propose the use of the weak formulation as a complementary approach to the classical strong formulation of data-driven time-series forecasting models. Specifically, we focus on the neural ordinary differential equation (NODE) architecture. Unlike the standard strong formulation, which relies on the discretization of the NODE followed by optimization, the weak formulation constrains the model using a set of integrated residuals over temporal subdomains. While such a formulation yields an effective NODE model, we discover that the performance of a NODE can be further enhanced by employing this weak formulation as a penalty alongside the classical strong formulation-based learning. Through numerical demonstrations, we illustrate that our proposed training strategy, which we coined as the Weak-Penalty NODE (WP-NODE), achieves state-of-the-art forecasting accuracy and exceptional robustness across benchmark chaotic dynamical systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Rival Data as Rival Products: An Encapsulation-Forging Approach for Data Synthesis</title>
<link>https://arxiv.org/abs/2511.06610</link>
<guid>https://arxiv.org/abs/2511.06610</guid>
<content:encoded><![CDATA[

arXiv:2511.06610v1 Announce Type: new 
Abstract: The non-rival nature of data creates a dilemma for firms: sharing data unlocks value but risks eroding competitive advantage. Existing data synthesis methods often exacerbate this problem by creating data with symmetric utility, allowing any party to extract its value. This paper introduces the Encapsulation-Forging (EnFo) framework, a novel approach to generate rival synthetic data with asymmetric utility. EnFo operates in two stages: it first encapsulates predictive knowledge from the original data into a designated ``key'' model, and then forges a synthetic dataset by optimizing the data to intentionally overfit this key model. This process transforms non-rival data into a rival product, ensuring its value is accessible only to the intended model, thereby preventing unauthorized use and preserving the data owner's competitive edge. Our framework demonstrates remarkable sample efficiency, matching the original data's performance with a fraction of its size, while providing robust privacy protection and resistance to misuse. EnFo offers a practical solution for firms to collaborate strategically without compromising their core analytical advantage.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-branch Spatial-Temporal Self-supervised Representation for Enhanced Road Network Learning</title>
<link>https://arxiv.org/abs/2511.06633</link>
<guid>https://arxiv.org/abs/2511.06633</guid>
<content:encoded><![CDATA[

arXiv:2511.06633v1 Announce Type: new 
Abstract: Road network representation learning (RNRL) has attracted increasing attention from both researchers and practitioners as various spatiotemporal tasks are emerging. Recent advanced methods leverage Graph Neural Networks (GNNs) and contrastive learning to characterize the spatial structure of road segments in a self-supervised paradigm. However, spatial heterogeneity and temporal dynamics of road networks raise severe challenges to the neighborhood smoothing mechanism of self-supervised GNNs. To address these issues, we propose a $\textbf{D}$ual-branch $\textbf{S}$patial-$\textbf{T}$emporal self-supervised representation framework for enhanced road representations, termed as DST. On one hand, DST designs a mix-hop transition matrix for graph convolution to incorporate dynamic relations of roads from trajectories. Besides, DST contrasts road representations of the vanilla road network against that of the hypergraph in a spatial self-supervised way. The hypergraph is newly built based on three types of hyperedges to capture long-range relations. On the other hand, DST performs next token prediction as the temporal self-supervised task on the sequences of traffic dynamics based on a causal Transformer, which is further regularized by differentiating traffic modes of weekdays from those of weekends. Extensive experiments against state-of-the-art methods verify the superiority of our proposed framework. Moreover, the comprehensive spatiotemporal modeling facilitates DST to excel in zero-shot learning scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaberNet: Causal Representation Learning for Cross-Domain HVAC Energy Prediction</title>
<link>https://arxiv.org/abs/2511.06634</link>
<guid>https://arxiv.org/abs/2511.06634</guid>
<content:encoded><![CDATA[

arXiv:2511.06634v1 Announce Type: new 
Abstract: Cross-domain HVAC energy prediction is essential for scalable building energy management, particularly because collecting extensive labeled data for every new building is both costly and impractical. Yet, this task remains highly challenging due to the scarcity and heterogeneity of data across different buildings, climate zones, and seasonal patterns. In particular, buildings situated in distinct climatic regions introduce variability that often leads existing methods to overfit to spurious correlations, rely heavily on expert intervention, or compromise on data diversity. To address these limitations, we propose CaberNet, a causal and interpretable deep sequence model that learns invariant (Markov blanket) representations for robust cross-domain prediction. In a purely data-driven fashion and without requiring any prior knowledge, CaberNet integrates i) a global feature gate trained with a self-supervised Bernoulli regularization to distinguish superior causal features from inferior ones, and ii) a domain-wise training scheme that balances domain contributions, minimizes cross-domain loss variance, and promotes latent factor independence. We evaluate CaberNet on real-world datasets collected from three buildings located in three climatically diverse cities, and it consistently outperforms all baselines, achieving a 22.9\% reduction in normalized mean squared error (NMSE) compared to the best benchmark. Our code is available at https://github.com/rickzky1001/CaberNet-CRL.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neyman-Pearson Classification under Both Null and Alternative Distributions Shift</title>
<link>https://arxiv.org/abs/2511.06641</link>
<guid>https://arxiv.org/abs/2511.06641</guid>
<content:encoded><![CDATA[

arXiv:2511.06641v1 Announce Type: new 
Abstract: We consider the problem of transfer learning in Neyman-Pearson classification, where the objective is to minimize the error w.r.t. a distribution $\mu_1$, subject to the constraint that the error w.r.t. a distribution $\mu_0$ remains below a prescribed threshold. While transfer learning has been extensively studied in traditional classification, transfer learning in imbalanced classification such as Neyman-Pearson classification has received much less attention. This setting poses unique challenges, as both types of errors must be simultaneously controlled. Existing works address only the case of distribution shift in $\mu_1$, whereas in many practical scenarios shifts may occur in both $\mu_0$ and $\mu_1$. We derive an adaptive procedure that not only guarantees improved Type-I and Type-II errors when the source is informative, but also automatically adapt to situations where the source is uninformative, thereby avoiding negative transfer. In addition to such statistical guarantees, the procedures is efficient, as shown via complementary computational guarantees.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Asset Allocation in a Fast Moving Consumer Goods B2B Company: An Interpretable Machine Learning Framework for Commercial Cooler Assignment Based on Multi-Tier Growth Targets</title>
<link>https://arxiv.org/abs/2511.06642</link>
<guid>https://arxiv.org/abs/2511.06642</guid>
<content:encoded><![CDATA[

arXiv:2511.06642v1 Announce Type: new 
Abstract: In the fast-moving consumer goods (FMCG) industry, deciding where to place physical assets, such as commercial beverage coolers, can directly impact revenue growth and execution efficiency. Although churn prediction and demand forecasting have been widely studied in B2B contexts, the use of machine learning to guide asset allocation remains relatively unexplored. This paper presents a framework focused on predicting which beverage clients are most likely to deliver strong returns in volume after receiving a cooler. Using a private dataset from a well-known Central American brewing and beverage company of 3,119 B2B traditional trade channel clients that received a cooler from 2022-01 to 2024-07, and tracking 12 months of sales transactions before and after cooler installation, three growth thresholds were defined: 10%, 30% and 50% growth in sales volume year over year. The analysis compares results of machine learning models such as XGBoost, LightGBM, and CatBoost combined with SHAP for interpretable feature analysis in order to have insights into improving business operations related to cooler allocation; the results show that the best model has AUC scores of 0.857, 0.877, and 0.898 across the thresholds on the validation set. Simulations suggest that this approach can improve ROI because it better selects potential clients to grow at the expected level and increases cost savings by not assigning clients that will not grow, compared to traditional volume-based approaches with substantial business management recommendations
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Pathway Fusion of EHRs and Knowledge Graphs for Predicting Unseen Drug-Drug Interactions</title>
<link>https://arxiv.org/abs/2511.06662</link>
<guid>https://arxiv.org/abs/2511.06662</guid>
<content:encoded><![CDATA[

arXiv:2511.06662v1 Announce Type: new 
Abstract: Drug-drug interactions (DDIs) remain a major source of preventable harm, and many clinically important mechanisms are still unknown. Existing models either rely on pharmacologic knowledge graphs (KGs), which fail on unseen drugs, or on electronic health records (EHRs), which are noisy, temporal, and site-dependent. We introduce, to our knowledge, the first system that conditions KG relation scoring on patient-level EHR context and distills that reasoning into an EHR-only model for zero-shot inference. A fusion "Teacher" learns mechanism-specific relations for drug pairs represented in both sources, while a distilled "Student" generalizes to new or rarely used drugs without KG access at inference. Both operate under a shared ontology (set) of pharmacologic mechanisms (drug relations) to produce interpretable, auditable alerts rather than opaque risk scores. Trained on a multi-institution EHR corpus paired with a curated DrugBank DDI graph, and evaluated using a clinically aligned, decision-focused protocol with leakage-safe negatives that avoid artificially easy pairs, the system maintains precision across multi-institutuion test data, produces mechanism-specific, clinically consistent predictions, reduces false alerts (higher precision) at comparable overall detection performance (F1), and misses fewer true interactions compared to prior methods. Case studies further show zero-shot identification of clinically recognized CYP-mediated and pharmacodynamic mechanisms for drugs absent from the KG, supporting real-world use in clinical decision support and pharmacovigilance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Adaptive Machine Learning Triage Framework for Predicting Alzheimer's Disease Progression</title>
<link>https://arxiv.org/abs/2511.06681</link>
<guid>https://arxiv.org/abs/2511.06681</guid>
<content:encoded><![CDATA[

arXiv:2511.06681v1 Announce Type: new 
Abstract: Accurate predictions of conversion from mild cognitive impairment (MCI) to Alzheimer's disease (AD) can enable effective personalized therapy. While cognitive tests and clinical data are routinely collected, they lack the predictive power of PET scans and CSF biomarker analysis, which are prohibitively expensive to obtain for every patient. To address this cost-accuracy dilemma, we design a two-stage machine learning framework that selectively obtains advanced, costly features based on their predicted "value of information". We apply our framework to predict AD progression for MCI patients using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our framework reduces the need for advanced testing by 20% while achieving a test AUROC of 0.929, comparable to the model that uses both basic and advanced features (AUROC=0.915, p=0.1010). We also provide an example interpretability analysis showing how one may explain the triage decision. Our work presents an interpretable, data-driven framework that optimizes AD diagnostic pathways and balances accuracy with cost, representing a step towards making early, reliable AD prediction more accessible in real-world practice. Future work should consider multiple categories of advanced features and larger-scale validation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Modality Imbalance in Multi-modal Learning via Multi-objective Optimization</title>
<link>https://arxiv.org/abs/2511.06686</link>
<guid>https://arxiv.org/abs/2511.06686</guid>
<content:encoded><![CDATA[

arXiv:2511.06686v1 Announce Type: new 
Abstract: Multi-modal learning (MML) aims to integrate information from multiple modalities, which is expected to lead to superior performance over single-modality learning. However, recent studies have shown that MML can underperform, even compared to single-modality approaches, due to imbalanced learning across modalities. Methods have been proposed to alleviate this imbalance issue using different heuristics, which often lead to computationally intensive subroutines. In this paper, we reformulate the MML problem as a multi-objective optimization (MOO) problem that overcomes the imbalanced learning issue among modalities and propose a gradient-based algorithm to solve the modified MML problem. We provide convergence guarantees for the proposed method, and empirical evaluations on popular MML benchmarks showcasing the improved performance of the proposed method over existing balanced MML and MOO baselines, with up to ~20x reduction in subroutine computation time. Our code is available at https://github.com/heshandevaka/MIMO.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Peeling Context from Cause for Multimodal Molecular Property Prediction</title>
<link>https://arxiv.org/abs/2511.06692</link>
<guid>https://arxiv.org/abs/2511.06692</guid>
<content:encoded><![CDATA[

arXiv:2511.06692v1 Announce Type: new 
Abstract: Deep models are used for molecular property prediction, yet they are often difficult to interpret and may rely on spurious context rather than causal structure, which reduces reliability under distribution shift and harms predictive performance. We introduce CLaP (Causal Layerwise Peeling), a framework that separates causal signal from context in a layerwise manner and integrates diverse graph representations of molecules. At each layer, a causal block performs a soft split into causal and non-causal branches, fuses causal evidence across modalities, and progressively removes batch-coupled context to focus on label-relevant structure, thereby limiting shortcut signals and stabilizing layerwise refinement. Across four molecular benchmarks, CLaP consistently improves MAE, MSE, and $R^2$ over competitive baselines. The model also produces atom-level causal saliency maps that highlight substructures responsible for predictions, providing actionable guidance for targeted molecular edits. Case studies confirm the accuracy of these maps and their alignment with chemical intuition. By peeling context from cause at every layer, the model yields predictors that are both accurate and interpretable for molecular design.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ML-EcoLyzer: Quantifying the Environmental Cost of Machine Learning Inference Across Frameworks and Hardware</title>
<link>https://arxiv.org/abs/2511.06694</link>
<guid>https://arxiv.org/abs/2511.06694</guid>
<content:encoded><![CDATA[

arXiv:2511.06694v1 Announce Type: new 
Abstract: Machine learning inference occurs at a massive scale, yet its environmental impact remains poorly quantified, especially on low-resource hardware. We present ML-EcoLyzer, a cross-framework tool for measuring the carbon, energy, thermal, and water costs of inference across CPUs, consumer GPUs, and datacenter accelerators. The tool supports both classical and modern models, applying adaptive monitoring and hardware-aware evaluation.
  We introduce the Environmental Sustainability Score (ESS), which quantifies the number of effective parameters served per gram of CO$_2$ emitted. Our evaluation covers over 1,900 inference configurations, spanning diverse model architectures, task modalities (text, vision, audio, tabular), hardware types, and precision levels. These rigorous and reliable measurements demonstrate that quantization enhances ESS, huge accelerators can be inefficient for lightweight applications, and even small models may incur significant costs when implemented suboptimally. ML-EcoLyzer sets a standard for sustainability-conscious model selection and offers an extensive empirical evaluation of environmental costs during inference.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magnitude-Modulated Equivariant Adapter for Parameter-Efficient Fine-Tuning of Equivariant Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.06696</link>
<guid>https://arxiv.org/abs/2511.06696</guid>
<content:encoded><![CDATA[

arXiv:2511.06696v1 Announce Type: new 
Abstract: Pretrained equivariant graph neural networks based on spherical harmonics offer efficient and accurate alternatives to computationally expensive ab-initio methods, yet adapting them to new tasks and chemical environments still requires fine-tuning. Conventional parameter-efficient fine-tuning (PEFT) techniques, such as Adapters and LoRA, typically break symmetry, making them incompatible with those equivariant architectures. ELoRA, recently proposed, is the first equivariant PEFT method. It achieves improved parameter efficiency and performance on many benchmarks. However, the relatively high degrees of freedom it retains within each tensor order can still perturb pretrained feature distributions and ultimately degrade performance. To address this, we present Magnitude-Modulated Equivariant Adapter (MMEA), a novel equivariant fine-tuning method which employs lightweight scalar gating to modulate feature magnitudes on a per-order and per-multiplicity basis. We demonstrate that MMEA preserves strict equivariance and, across multiple benchmarks, consistently improves energy and force predictions to state-of-the-art levels while training fewer parameters than competing approaches. These results suggest that, in many practical scenarios, modulating channel magnitudes is sufficient to adapt equivariant models to new chemical environments without breaking symmetry, pointing toward a new paradigm for equivariant PEFT design.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sensor Calibration Model Balancing Accuracy, Real-time, and Efficiency</title>
<link>https://arxiv.org/abs/2511.06715</link>
<guid>https://arxiv.org/abs/2511.06715</guid>
<content:encoded><![CDATA[

arXiv:2511.06715v1 Announce Type: new 
Abstract: Most on-device sensor calibration studies benchmark models only against three macroscopic requirements (i.e., accuracy, real-time, and resource efficiency), thereby hiding deployment bottlenecks such as instantaneous error and worst-case latency. We therefore decompose this triad into eight microscopic requirements and introduce Scare (Sensor Calibration model balancing Accuracy, Real-time, and Efficiency), an ultra-compressed transformer that fulfills them all. SCARE comprises three core components: (1) Sequence Lens Projector (SLP) that logarithmically compresses time-series data while preserving boundary information across bins, (2) Efficient Bitwise Attention (EBA) module that replaces costly multiplications with bitwise operations via binary hash codes, and (3) Hash optimization strategy that ensures stable training without auxiliary loss terms. Together, these components minimize computational overhead while maintaining high accuracy and compatibility with microcontroller units (MCUs). Extensive experiments on large-scale air-quality datasets and real microcontroller deployments demonstrate that Scare outperforms existing linear, hybrid, and deep-learning baselines, making Scare, to the best of our knowledge, the first model to meet all eight microscopic requirements simultaneously.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MobileLLM-Pro Technical Report</title>
<link>https://arxiv.org/abs/2511.06719</link>
<guid>https://arxiv.org/abs/2511.06719</guid>
<content:encoded><![CDATA[

arXiv:2511.06719v1 Announce Type: new 
Abstract: Efficient on-device language models around 1 billion parameters are essential for powering low-latency AI applications on mobile and wearable devices. However, achieving strong performance in this model class, while supporting long context windows and practical deployment remains a significant challenge. We introduce MobileLLM-Pro, a 1-billion-parameter language model optimized for on-device deployment. MobileLLM-Pro achieves state-of-the-art results across 11 standard benchmarks, significantly outperforming both Gemma 3-1B and Llama 3.2-1B, while supporting context windows of up to 128,000 tokens and showing only minor performance regressions at 4-bit quantization. These improvements are enabled by four core innovations: (1) implicit positional distillation, a novel technique that effectively instills long-context capabilities through knowledge distillation; (2) a specialist model merging framework that fuses multiple domain experts into a compact model without parameter growth; (3) simulation-driven data mixing using utility estimation; and (4) 4-bit quantization-aware training with self-distillation. We release our model weights and code to support future research in efficient on-device language models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Continual Learning via Cross-Modality Adapters and Representation Alignment with Knowledge Preservation</title>
<link>https://arxiv.org/abs/2511.06723</link>
<guid>https://arxiv.org/abs/2511.06723</guid>
<content:encoded><![CDATA[

arXiv:2511.06723v1 Announce Type: new 
Abstract: Continual learning is essential for adapting models to new tasks while retaining previously acquired knowledge. While existing approaches predominantly focus on uni-modal data, multi-modal learning offers substantial benefits by utilizing diverse sensory inputs, akin to human perception. However, multi-modal continual learning presents additional challenges, as the model must effectively integrate new information from various modalities while preventing catastrophic forgetting. In this work, we propose a pre-trained model-based framework for multi-modal continual learning. Our framework includes a novel cross-modality adapter with a mixture-of-experts structure to facilitate effective integration of multi-modal information across tasks. We also introduce a representation alignment loss that fosters learning of robust multi-modal representations, and regularize relationships between learned representations to preserve knowledge from previous tasks. Experiments on several multi-modal datasets demonstrate that our approach consistently outperforms baselines in both class-incremental and domain-incremental learning, achieving higher accuracy and reduced forgetting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rank-1 LoRAs Encode Interpretable Reasoning Signals</title>
<link>https://arxiv.org/abs/2511.06739</link>
<guid>https://arxiv.org/abs/2511.06739</guid>
<content:encoded><![CDATA[

arXiv:2511.06739v1 Announce Type: new 
Abstract: Reasoning models leverage inference-time compute to significantly enhance the performance of language models on difficult logical tasks, and have become a dominating paradigm in frontier LLMs. Despite their wide adoption, the mechanisms underpinning the enhanced performance of these reasoning models are not well understood. In this work, we show that the majority of new capabilities in reasoning models can be elicited by small, single-rank changes to base model parameters, with many of these changes being interpretable. Specifically, we use a rank-1 LoRA to create a minimal parameter adapter for Qwen-2.5-32B-Instruct which recovers 73-90% of reasoning-benchmark performance compared to a full parameter finetune. We find that the activations of this LoRA are as interpretable as MLP neurons, and fire for reasoning-specific behaviors. Finally, we train a sparse autoencoder on the entire activation state of this LoRA and identify fine-grained and monosemantic features. Our findings highlight that reasoning performance can arise largely from minimal changes to base model parameters, and explore what these changes affect. More broadly, our work shows that parameter-efficient training methods can be used as a targeted lens for uncovering fundamental insights about language model behavior and dynamics.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Mamba for Node-Specific Representation Learning: Tackling Over-Smoothing with Selective State Space Modeling</title>
<link>https://arxiv.org/abs/2511.06756</link>
<guid>https://arxiv.org/abs/2511.06756</guid>
<content:encoded><![CDATA[

arXiv:2511.06756v1 Announce Type: new 
Abstract: Over-smoothing remains a fundamental challenge in deep Graph Neural Networks (GNNs), where repeated message passing causes node representations to become indistinguishable. While existing solutions, such as residual connections and skip layers, alleviate this issue to some extent, they fail to explicitly model how node representations evolve in a node-specific and progressive manner across layers. Moreover, these methods do not take global information into account, which is also crucial for mitigating the over-smoothing problem. To address the aforementioned issues, in this work, we propose a Dual Mamba-enhanced Graph Convolutional Network (DMbaGCN), which is a novel framework that integrates Mamba into GNNs to address over-smoothing from both local and global perspectives. DMbaGCN consists of two modules: the Local State-Evolution Mamba (LSEMba) for local neighborhood aggregation and utilizing Mamba's selective state space modeling to capture node-specific representation dynamics across layers, and the Global Context-Aware Mamba (GCAMba) that leverages Mamba's global attention capabilities to incorporate global context for each node. By combining these components, DMbaGCN enhances node discriminability in deep GNNs, thereby mitigating over-smoothing. Extensive experiments on multiple benchmarks demonstrate the effectiveness and efficiency of our method.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implicit Federated In-context Learning For Task-Specific LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.06757</link>
<guid>https://arxiv.org/abs/2511.06757</guid>
<content:encoded><![CDATA[

arXiv:2511.06757v1 Announce Type: new 
Abstract: As large language models continue to develop and expand, the extensive public data they rely on faces the risk of depletion. Consequently, leveraging private data within organizations to enhance the performance of large models has emerged as a key challenge. The federated learning paradigm, combined with model fine-tuning techniques, effectively reduces the number of trainable parameters. However,the necessity to process high-dimensional feature spaces results in substantial overall computational overhead. To address this issue, we propose the Implicit Federated In-Context Learning (IFed-ICL) framework. IFed-ICL draws inspiration from federated learning to establish a novel distributed collaborative paradigm, by converting client local context examples into implicit vector representations, it enables distributed collaborative computation during the inference phase and injects model residual streams to enhance model performance. Experiments demonstrate that our proposed method achieves outstanding performance across multiple text classification tasks. Compared to traditional methods, IFed-ICL avoids the extensive parameter updates required by conventional fine-tuning methods while reducing data transmission and local computation at the client level in federated learning. This enables efficient distributed context learning using local private-domain data, significantly improving model performance on specific tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QUARK: Quantization-Enabled Circuit Sharing for Transformer Acceleration by Exploiting Common Patterns in Nonlinear Operations</title>
<link>https://arxiv.org/abs/2511.06767</link>
<guid>https://arxiv.org/abs/2511.06767</guid>
<content:encoded><![CDATA[

arXiv:2511.06767v1 Announce Type: new 
Abstract: Transformer-based models have revolutionized computer vision (CV) and natural language processing (NLP) by achieving state-of-the-art performance across a range of benchmarks. However, nonlinear operations in models significantly contribute to inference latency, presenting unique challenges for efficient hardware acceleration. To this end, we propose QUARK, a quantization-enabled FPGA acceleration framework that leverages common patterns in nonlinear operations to enable efficient circuit sharing, thereby reducing hardware resource requirements. QUARK targets all nonlinear operations within Transformer-based models, achieving high-performance approximation through a novel circuit-sharing design tailored to accelerate these operations. Our evaluation demonstrates that QUARK significantly reduces the computational overhead of nonlinear operators in mainstream Transformer architectures, achieving up to a 1.96 times end-to-end speedup over GPU implementations. Moreover, QUARK lowers the hardware overhead of nonlinear modules by more than 50% compared to prior approaches, all while maintaining high model accuracy -- and even substantially boosting accuracy under ultra-low-bit quantization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Trajectory Alignment for LLM Domain Adaptation: A Two-Phase Synthesis Framework for Telecommunications Mathematics</title>
<link>https://arxiv.org/abs/2511.06776</link>
<guid>https://arxiv.org/abs/2511.06776</guid>
<content:encoded><![CDATA[

arXiv:2511.06776v1 Announce Type: new 
Abstract: General-purpose large language models (LLMs) are increasingly deployed in verticals such as telecommunications, where adaptation is hindered by scarce, low-information-density corpora and tight mobile/edge constraints. We propose Data Trajectory Alignment (DTA), a two-phase, model-agnostic data curation framework that treats solution processes - not only final answers - as first-class supervision. Phase I (Initializing) synthesizes diverse, high-coverage candidates using an ensemble of strong teachers. Phase II (DTA) rewrites teacher solutions to align intermediate steps and presentation style with the target student's inductive biases and then performs signal-aware exemplar selection via agreement checks and reflection-based judging. Instantiated on telecommunications mathematics (e.g., link budgets, SNR/AMC selection, and power-control feasibility), DTA yields state-of-the-art (SOTA) accuracy on TELEMATH without enabling explicit "thinking" modes: 72.45% pass@1, surpassing distilled-only training by +17.65 points and outperforming a strong baseline (Qwen3-32B with thinking enabled) by +2.94 points. Token-shift analyses indicate that DTA concentrates gains on logical-structural discourse markers rather than merely amplifying domain nouns, indicating improved reasoning scaffolding. Under edge-like inference settings, DTA improves efficiency by reducing reliance on multi-sample voting and disabling expensive reasoning heuristics, cutting energy per output token by ~42% versus Qwen3-32B (thinking mode enabled) and end-to-end latency by ~60% versus Qwen3-32B (thinking mode disabled). These results demonstrate that aligning how solutions are produced enables compact, high-yield supervision that is effective for both accuracy and efficiency, offering a practical recipe for domain adaptation in low-resource verticals beyond telecom.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Mechanisms of Collaborative Learning in VAE Recommenders</title>
<link>https://arxiv.org/abs/2511.06781</link>
<guid>https://arxiv.org/abs/2511.06781</guid>
<content:encoded><![CDATA[

arXiv:2511.06781v1 Announce Type: new 
Abstract: Variational Autoencoders (VAEs) are a powerful alternative to matrix factorization for recommendation. A common technique in VAE-based collaborative filtering (CF) consists in applying binary input masking to user interaction vectors, which improves performance but remains underexplored theoretically. In this work, we analyze how collaboration arises in VAE-based CF and show it is governed by latent proximity: we derive a latent sharing radius that informs when an SGD update on one user strictly reduces the loss on another user, with influence decaying as the latent Wasserstein distance increases. We further study the induced geometry: with clean inputs, VAE-based CF primarily exploits \emph{local} collaboration between input-similar users and under-utilizes global collaboration between far-but-related users. We compare two mechanisms that encourage \emph{global} mixing and characterize their trade-offs: (1) $\beta$-KL regularization directly tightens the information bottleneck, promoting posterior overlap but risking representational collapse if too large; (2) input masking induces stochastic geometric contractions and expansions, which can bring distant users onto the same latent neighborhood but also introduce neighborhood drift. To preserve user identity while enabling global consistency, we propose an anchor regularizer that aligns user posteriors with item embeddings, stabilizing users under masking and facilitating signal sharing across related items. Our analyses are validated on the Netflix, MovieLens-20M, and Million Song datasets. We also successfully deployed our proposed algorithm on an Amazon streaming platform following a successful online experiment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource Efficient Sleep Staging via Multi-Level Masking and Prompt Learning</title>
<link>https://arxiv.org/abs/2511.06785</link>
<guid>https://arxiv.org/abs/2511.06785</guid>
<content:encoded><![CDATA[

arXiv:2511.06785v1 Announce Type: new 
Abstract: Automatic sleep staging plays a vital role in assessing sleep quality and diagnosing sleep disorders. Most existing methods rely heavily on long and continuous EEG recordings, which poses significant challenges for data acquisition in resource-constrained systems, such as wearable or home-based monitoring systems. In this paper, we propose the task of resource-efficient sleep staging, which aims to reduce the amount of signal collected per sleep epoch while maintaining reliable classification performance. To solve this task, we adopt the masking and prompt learning strategy and propose a novel framework called Mask-Aware Sleep Staging (MASS). Specifically, we design a multi-level masking strategy to promote effective feature modeling under partial and irregular observations. To mitigate the loss of contextual information introduced by masking, we further propose a hierarchical prompt learning mechanism that aggregates unmasked data into a global prompt, serving as a semantic anchor for guiding both patch-level and epoch-level feature modeling. MASS is evaluated on four datasets, demonstrating state-of-the-art performance, especially when the amount of data is very limited. This result highlights its potential for efficient and scalable deployment in real-world low-resource sleep monitoring environments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Parameter Sharing as Graph Coloring for Structured Compression</title>
<link>https://arxiv.org/abs/2511.06786</link>
<guid>https://arxiv.org/abs/2511.06786</guid>
<content:encoded><![CDATA[

arXiv:2511.06786v1 Announce Type: new 
Abstract: Modern deep models have massive parameter sizes, leading to high inference-time memory usage that limits practical deployment. Parameter sharing, a form of structured compression, effectively reduces redundancy, but existing approaches remain heuristic-restricted to adjacent layers and lacking a systematic analysis for cross-layer sharing. However, extending sharing across multiple layers leads to an exponentially expanding configuration space, making exhaustive search computationally infeasible and forming a critical bottleneck for parameter sharing. We recast parameter sharing from a group-theoretic perspective as introducing structural symmetries in the model's parameter space. A sharing configuration can be described by a coloring function $\alpha:L\rightarrow C$ (L: layer indices and C: sharing classes), which determines inter-layer sharing groups while preserving structural symmetry. To determine the coloring function, we propose a second-order geometric criterion based on Taylor expansion and the Hessian spectrum. By projecting perturbations onto the Hessian's low-curvature eigensubspace, the criterion provides an analytic rule for selecting sharing groups that minimize performance impact, yielding a principled and scalable configuration procedure. Across diverse architectures and tasks, Geo-Sharing consistently outperforms state-of-the-art heuristic sharing strategies, achieving higher compression ratios with smaller accuracy degradation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Causal Discovery under Imperfect Structural Constraints</title>
<link>https://arxiv.org/abs/2511.06790</link>
<guid>https://arxiv.org/abs/2511.06790</guid>
<content:encoded><![CDATA[

arXiv:2511.06790v1 Announce Type: new 
Abstract: Robust causal discovery from observational data under imperfect prior knowledge remains a significant and largely unresolved challenge. Existing methods typically presuppose perfect priors or can only handle specific, pre-identified error types. And their performance degrades substantially when confronted with flawed constraints of unknown location and type. This decline arises because most of them rely on inflexible and biased thresholding strategies that may conflict with the data distribution. To overcome these limitations, we propose to harmonizes knowledge and data through prior alignment and conflict resolution. First, we assess the credibility of imperfect structural constraints through a surrogate model, which then guides a sparse penalization term measuring the loss between the learned and constrained adjacency matrices. We theoretically prove that, under ideal assumption, the knowledge-driven objective aligns with the data-driven objective. Furthermore, to resolve conflicts when this assumption is violated, we introduce a multi-task learning framework optimized via multi-gradient descent, jointly minimizing both objectives. Our proposed method is robust to both linear and nonlinear settings. Extensive experiments, conducted under diverse noise conditions and structural equation model types, demonstrate the effectiveness and efficiency of our method under imperfect structural constraints.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coupling Agent-based Modeling and Life Cycle Assessment to Analyze Trade-offs in Resilient Energy Transitions</title>
<link>https://arxiv.org/abs/2511.06791</link>
<guid>https://arxiv.org/abs/2511.06791</guid>
<content:encoded><![CDATA[

arXiv:2511.06791v1 Announce Type: new 
Abstract: Transitioning to sustainable and resilient energy systems requires navigating complex and interdependent trade-offs across environmental, social, and resource dimensions. Neglecting these trade-offs can lead to unintended consequences across sectors. However, existing assessments often evaluate emerging energy pathways and their impacts in silos, overlooking critical interactions such as regional resource competition and cumulative impacts. We present an integrated modeling framework that couples agent-based modeling and Life Cycle Assessment (LCA) to simulate how energy transition pathways interact with regional resource competition, ecological constraints, and community-level burdens. We apply the model to a case study in Southern California. The results demonstrate how integrated and multiscale decision making can shape energy pathway deployment and reveal spatially explicit trade-offs under scenario-driven constraints. This modeling framework can further support more adaptive and resilient energy transition planning on spatial and institutional scales.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Modal Unlearning via Influential Neuron Path Editing in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.06793</link>
<guid>https://arxiv.org/abs/2511.06793</guid>
<content:encoded><![CDATA[

arXiv:2511.06793v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) extend foundation models to real-world applications by integrating inputs such as text and vision. However, their broad knowledge capacity raises growing concerns about privacy leakage, toxicity mitigation, and intellectual property violations. Machine Unlearning (MU) offers a practical solution by selectively forgetting targeted knowledge while preserving overall model utility. When applied to MLLMs, existing neuron-editing-based MU approaches face two fundamental challenges: (1) forgetting becomes inconsistent across modalities because existing point-wise attribution methods fail to capture the structured, layer-by-layer information flow that connects different modalities; and (2) general knowledge performance declines when sensitive neurons that also support important reasoning paths are pruned, as this disrupts the model's ability to generalize. To alleviate these limitations, we propose a multimodal influential neuron path editor (MIP-Editor) for MU. Our approach introduces modality-specific attribution scores to identify influential neuron paths responsible for encoding forget-set knowledge and applies influential-path-aware neuron-editing via representation misdirection. This strategy also enables effective and coordinated forgetting across modalities while preserving the model's general capabilities. Experimental results demonstrate that MIP-Editor achieves a superior unlearning performance on multimodal tasks, with a maximum forgetting rate of 87.75% and up to 54.26% improvement in general knowledge retention. On textual tasks, MIP-Editor achieves up to 80.65% forgetting and preserves 77.9% of general performance. Codes are available at https://github.com/PreckLi/MIP-Editor.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Uniform Deletion: A Data Value-Weighted Framework for Certified Machine Unlearning</title>
<link>https://arxiv.org/abs/2511.06794</link>
<guid>https://arxiv.org/abs/2511.06794</guid>
<content:encoded><![CDATA[

arXiv:2511.06794v1 Announce Type: new 
Abstract: As the right to be forgotten becomes legislated worldwide, machine unlearning mechanisms have emerged to efficiently update models for data deletion and enhance user privacy protection. However, existing machine unlearning algorithms frequently neglect the fact that different data points may contribute unequally to model performance (i.e., heterogeneous data values). Treat them equally in machine unlearning procedure can potentially degrading the performance of updated models. To address this limitation, we propose Data Value-Weighted Unlearning (DVWU), a general unlearning framework that accounts for data value heterogeneity into the unlearning process. Specifically, we design a weighting strategy based on data values, which are then integrated into the unlearning procedure to enable differentiated unlearning for data points with varying utility to the model. The DVWU framework can be broadly adapted to various existing machine unlearning methods. We use the one-step Newton update as an example for implementation, developing both output and objective perturbation algorithms to achieve certified unlearning. Experiments on both synthetic and real-world datasets demonstrate that our methods achieve superior predictive performance and robustness compared to conventional unlearning approaches. We further show the extensibility of our framework on gradient ascent method by incorporating the proposed weighting strategy into the gradient terms, highlighting the adaptability of DVWU for broader gradient-based deep unlearning methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedNET: Federated Learning for Proactive Traffic Management and Network Capacity Planning</title>
<link>https://arxiv.org/abs/2511.06797</link>
<guid>https://arxiv.org/abs/2511.06797</guid>
<content:encoded><![CDATA[

arXiv:2511.06797v1 Announce Type: new 
Abstract: We propose FedNET, a proactive and privacy-preserving framework for early identification of high-risk links in large-scale communication networks, that leverages a distributed multi-step traffic forecasting method. FedNET employs Federated Learning (FL) to model the temporal evolution of node-level traffic in a distributed manner, enabling accurate multi-step-ahead predictions (e.g., several hours to days) without exposing sensitive network data. Using these node-level forecasts and known routing information, FedNET estimates the future link-level utilization by aggregating traffic contributions across all source-destination pairs. The links are then ranked according to the predicted load intensity and temporal variability, providing an early warning signal for potential high-risk links. We compare the federated traffic prediction of FedNET against a centralized multi-step learning baseline and then systematically analyze the impact of history and prediction window sizes on forecast accuracy using the $R^2$ score. Results indicate that FL achieves accuracy close to centralized training, with shorter prediction horizons consistently yielding the highest accuracy ($R^2 >0.92$), while longer horizons providing meaningful forecasts ($R^2 \approx 0.45\text{--}0.55$). We further validate the efficacy of the FedNET framework in predicting network utilization on a realistic network topology and demonstrate that it consistently identifies high-risk links well in advance (i.e., three days ahead) of the critical stress states emerging, making it a practical tool for anticipatory traffic engineering and capacity planning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recursive Dynamics in Fast-Weights Homeostatic Reentry Networks: Toward Reflective Intelligence</title>
<link>https://arxiv.org/abs/2511.06798</link>
<guid>https://arxiv.org/abs/2511.06798</guid>
<content:encoded><![CDATA[

arXiv:2511.06798v1 Announce Type: new 
Abstract: This study introduces the Fast-Weights Homeostatic Reentry Layer (FH-RL), a neural mechanism that integrates fast-weight associative memory, homeostatic regularization, and learned reentrant feedback to approximate self-referential computation in neural networks. Unlike standard transformer architectures that operate in a purely feedforward manner during inference, FH-RL enables internal recurrence without external looping, allowing prior latent states to be dynamically re-entered into the ongoing computation stream. We conduct controlled experiments sweeping the reentry gain $\gamma$ and evaluate emergent internal dynamics using three novel metrics: the Information Reentry Ratio (IRR), Eigen-Spectrum Recursion Index (ESRI), and Representational Drift Periodicity (RDP). Results show that reentry quantity increases proportionally with~$\gamma$, while the learned feedback matrix $W_r$ remains bounded and becomes more structured at moderate gains. Critically, a stable reflective band emerges around $\gamma \approx 0.10-0.20$, where internal feedback is maximally expressive yet spectrally stable: IRR rises smoothly, ESRI remains near zero, and RDP exhibits consistent low-frequency cycles. These findings provide quantitative evidence that reflective, thought-like internal processing can arise from a principled balance between feedback amplification and homeostatic regulation, linking modern fast-weight architectures to theories of cortical reentry and recursive cognition.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural-Initialized Newton: Accelerating Nonlinear Finite Elements via Operator Learning</title>
<link>https://arxiv.org/abs/2511.06802</link>
<guid>https://arxiv.org/abs/2511.06802</guid>
<content:encoded><![CDATA[

arXiv:2511.06802v1 Announce Type: new 
Abstract: We propose a Newton-based scheme, initialized by neural operator predictions, to accelerate the parametric solution of nonlinear problems in computational solid mechanics. First, a physics informed conditional neural field is trained to approximate the nonlinear parametric solutionof the governing equations. This establishes a continuous mapping between the parameter and solution spaces, which can then be evaluated for a given parameter at any spatial resolution. Second, since the neural approximation may not be exact, it is subsequently refined using a Newton-based correction initialized by the neural output. To evaluate the effectiveness of this hybrid approach, we compare three solution strategies: (i) the standard Newton-Raphson solver used in NFEM, which is robust and accurate but computationally demanding; (ii) physics-informed neural operators, which provide rapid inference but may lose accuracy outside the training distribution and resolution; and (iii) the neural-initialized Newton (NiN) strategy, which combines the efficiency of neural operators with the robustness of NFEM. The results demonstrate that the proposed hybrid approach reduces computational cost while preserving accuracy, highlighting its potential to accelerate large-scale nonlinear simulations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllable Flow Matching for Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.06816</link>
<guid>https://arxiv.org/abs/2511.06816</guid>
<content:encoded><![CDATA[

arXiv:2511.06816v1 Announce Type: new 
Abstract: Model-based reinforcement learning (MBRL) typically relies on modeling environment dynamics for data efficiency. However, due to the accumulation of model errors over long-horizon rollouts, such methods often face challenges in maintaining modeling stability. To address this, we propose CtrlFlow, a trajectory-level synthetic method using conditional flow matching (CFM), which directly modeling the distribution of trajectories from initial states to high-return terminal states without explicitly modeling the environment transition function. Our method ensures optimal trajectory sampling by minimizing the control energy governed by the non-linear Controllability Gramian Matrix, while the generated diverse trajectory data significantly enhances the robustness and cross-task generalization of policy learning. In online settings, CtrlFlow demonstrates the better performance on common MuJoCo benchmark tasks than dynamics models and achieves superior sample efficiency compared to standard MBRL methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepRWCap: Neural-Guided Random-Walk Capacitance Solver for IC Design</title>
<link>https://arxiv.org/abs/2511.06831</link>
<guid>https://arxiv.org/abs/2511.06831</guid>
<content:encoded><![CDATA[

arXiv:2511.06831v1 Announce Type: new 
Abstract: Monte Carlo random walk methods are widely used in capacitance extraction for their mesh-free formulation and inherent parallelism. However, modern semiconductor technologies with densely packed structures present significant challenges in unbiasedly sampling transition domains in walk steps with multiple high-contrast dielectric materials. We present DeepRWCap, a machine learning-guided random walk solver that predicts the transition quantities required to guide each step of the walk. These include Poisson kernels, gradient kernels, signs and magnitudes of weights. DeepRWCap employs a two-stage neural architecture that decomposes structured outputs into face-wise distributions and spatial kernels on cube faces. It uses 3D convolutional networks to capture volumetric dielectric interactions and 2D depthwise separable convolutions to model localized kernel behavior. The design incorporates grid-based positional encodings and structural design choices informed by cube symmetries to reduce learning redundancy and improve generalization. Trained on 100,000 procedurally generated dielectric configurations, DeepRWCap achieves a mean relative error of $1.24\pm0.53$\% when benchmarked against the commercial Raphael solver on the self-capacitance estimation of 10 industrial designs spanning 12 to 55 nm nodes. Compared to the state-of-the-art stochastic difference method Microwalk, DeepRWCap achieves an average 23\% speedup. On complex designs with runtimes over 10 s, it reaches an average 49\% acceleration.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimum Width of Deep Narrow Networks for Universal Approximation</title>
<link>https://arxiv.org/abs/2511.06837</link>
<guid>https://arxiv.org/abs/2511.06837</guid>
<content:encoded><![CDATA[

arXiv:2511.06837v1 Announce Type: new 
Abstract: Determining the minimum width of fully connected neural networks has become a fundamental problem in recent theoretical studies of deep neural networks. In this paper, we study the lower bounds and upper bounds of the minimum width required for fully connected neural networks in order to have universal approximation capability, which is important in network design and training. We show that $w_{min}\leq\max(2d_x+1, d_y)$ for networks with ELU, SELU, and the upper bound of this inequality is attained when $d_y=2d_x$, where $d_x$, $d_y$ denote the input and output dimensions, respectively. Besides, we show that $d_x+1\leq w_{min}\leq d_x+d_y$ for networks with LeakyReLU, ELU, CELU, SELU, Softplus, by proving that ReLU can be approximated by these activation functions. In addition, in the case that the activation function is injective or can be uniformly approximated by a sequence of injective functions (e.g., ReLU), we present a new proof of the inequality $w_{min}\ge d_y+\mathbf{1}_{d_x<d_y\leq2d_x}$ by constructing a more intuitive example via a new geometric approach based on Poincar$\acute{\text{e}}$-Miranda Theorem.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MI-to-Mid Distilled Compression (M2M-DC): An Hybrid-Information-Guided-Block Pruning with Progressive Inner Slicing Approach to Model Compression</title>
<link>https://arxiv.org/abs/2511.06842</link>
<guid>https://arxiv.org/abs/2511.06842</guid>
<content:encoded><![CDATA[

arXiv:2511.06842v1 Announce Type: new 
Abstract: We introduce MI-to-Mid Distilled Compression (M2M-DC), a two-scale, shape-safe compression framework that interleaves information-guided block pruning with progressive inner slicing and staged knowledge distillation (KD). First, M2M-DC ranks residual (or inverted-residual) blocks by a label-aware mutual information (MI) signal and removes the least informative units (structured prune-after-training). It then alternates short KD phases with stage-coherent, residual-safe channel slicing: (i) stage "planes" (co-slicing conv2 out-channels with the downsample path and next-stage inputs), and (ii) an optional mid-channel trim (conv1 out / bn1 / conv2 in). This targets complementary redundancy, whole computational motifs and within-stage width while preserving residual shape invariants. On CIFAR-100, M2M-DC yields a clean accuracy-compute frontier. For ResNet-18, we obtain 85.46% Top-1 with 3.09M parameters and 0.0139 GMacs (72% params, 63% GMacs vs. teacher; mean final 85.29% over three seeds). For ResNet-34, we reach 85.02% Top-1 with 5.46M params and 0.0195 GMacs (74% / 74% vs. teacher; mean final 84.62%). Extending to inverted-residuals, MobileNetV2 achieves a mean final 68.54% Top-1 at 1.71M params (27%) and 0.0186 conv GMacs (24%), improving over the teacher's 66.03% by +2.5 points across three seeds. Because M2M-DC exposes only a thin, architecture-aware interface (blocks, stages, and down sample/skip wiring), it generalizes across residual CNNs and extends to inverted-residual families with minor legalization rules. The result is a compact, practical recipe for deployment-ready models that match or surpass teacher accuracy at a fraction of the compute.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning</title>
<link>https://arxiv.org/abs/2511.06854</link>
<guid>https://arxiv.org/abs/2511.06854</guid>
<content:encoded><![CDATA[

arXiv:2511.06854v1 Announce Type: new 
Abstract: Irregularly sampled time series (ISTS), characterized by non-uniform time intervals with natural missingness, are prevalent in real-world applications. Existing approaches for ISTS modeling primarily rely on observed values to impute unobserved ones or infer latent dynamics. However, these methods overlook a critical source of learning signal: the reconstruction error inherently produced during model training. Such error implicitly reflects how well a model captures the underlying data structure and can serve as an informative proxy for unobserved values. To exploit this insight, we propose iTimER, a simple yet effective self-supervised pre-training framework for ISTS representation learning. iTimER models the distribution of reconstruction errors over observed values and generates pseudo-observations for unobserved timestamps through a mixup strategy between sampled errors and the last available observations. This transforms unobserved timestamps into noise-aware training targets, enabling meaningful reconstruction signals. A Wasserstein metric aligns reconstruction error distributions between observed and pseudo-observed regions, while a contrastive learning objective enhances the discriminability of learned representations. Extensive experiments on classification, interpolation, and forecasting tasks demonstrate that iTimER consistently outperforms state-of-the-art methods under the ISTS setting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contact Wasserstein Geodesics for Non-Conservative Schrodinger Bridges</title>
<link>https://arxiv.org/abs/2511.06856</link>
<guid>https://arxiv.org/abs/2511.06856</guid>
<content:encoded><![CDATA[

arXiv:2511.06856v1 Announce Type: new 
Abstract: The Schr\"odinger Bridge provides a principled framework for modeling stochastic processes between distributions; however, existing methods are limited by energy-conservation assumptions, which constrains the bridge's shape preventing it from model varying-energy phenomena. To overcome this, we introduce the non-conservative generalized Schr\"odinger bridge (NCGSB), a novel, energy-varying reformulation based on contact Hamiltonian mechanics. By allowing energy to change over time, the NCGSB provides a broader class of real-world stochastic processes, capturing richer and more faithful intermediate dynamics. By parameterizing the Wasserstein manifold, we lift the bridge problem to a tractable geodesic computation in a finite-dimensional space. Unlike computationally expensive iterative solutions, our contact Wasserstein geodesic (CWG) is naturally implemented via a ResNet architecture and relies on a non-iterative solver with near-linear complexity. Furthermore, CWG supports guided generation by modulating a task-specific distance metric. We validate our framework on tasks including manifold navigation, molecular dynamics predictions, and image generation, demonstrating its practical benefits and versatility.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TuckA: Hierarchical Compact Tensor Experts for Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.06859</link>
<guid>https://arxiv.org/abs/2511.06859</guid>
<content:encoded><![CDATA[

arXiv:2511.06859v1 Announce Type: new 
Abstract: Efficiently fine-tuning pre-trained models for downstream tasks is a key challenge in the era of foundation models. Parameter-efficient fine-tuning (PEFT) presents a promising solution, achieving performance comparable to full fine-tuning by updating only a small number of adaptation weights per layer. Traditional PEFT methods typically rely on a single expert, where the adaptation weight is a low-rank matrix. However, for complex tasks, the data's inherent diversity poses a significant challenge for such models, as a single adaptation weight cannot adequately capture the features of all samples. To address this limitation, we explore how to integrate multiple small adaptation experts into a compact structure to defeat a large adapter. Specifically, we propose Tucker Adaptation (TuckA), a method with four key properties: (i) We use Tucker decomposition to create a compact 3D tensor where each slice naturally serves as an expert. The low-rank nature of this decomposition ensures that the number of parameters scales efficiently as more experts are added. (ii) We introduce a hierarchical strategy that organizes these experts into groups at different granularities, allowing the model to capture both local and global data patterns. (iii) We develop an efficient batch-level routing mechanism, which reduces the router's parameter size by a factor of $L$ compared to routing at every adapted layer (where $L$ is the number of adapted layers) (iv) We propose data-aware initialization to achieve loss-free expert load balancing based on theoretical analysis. Extensive experiments on benchmarks in natural language understanding, image classification, and mathematical reasoning speak to the efficacy of TuckA, offering a new and effective solution to the PEFT problem.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepBooTS: Dual-Stream Residual Boosting for Drift-Resilient Time-Series Forecasting</title>
<link>https://arxiv.org/abs/2511.06893</link>
<guid>https://arxiv.org/abs/2511.06893</guid>
<content:encoded><![CDATA[

arXiv:2511.06893v1 Announce Type: new 
Abstract: Time-Series (TS) exhibits pronounced non-stationarity. Consequently, most forecasting methods display compromised robustness to concept drift, despite the prevalent application of instance normalization. We tackle this challenge by first analysing concept drift through a bias-variance lens and proving that weighted ensemble reduces variance without increasing bias. These insights motivate DeepBooTS, a novel end-to-end dual-stream residual-decreasing boosting method that progressively reconstructs the intrinsic signal. In our design, each block of a deep model becomes an ensemble of learners with an auxiliary output branch forming a highway to the final prediction. The block-wise outputs correct the residuals of previous blocks, leading to a learning-driven decomposition of both inputs and targets. This method enhances versatility and interpretability while substantially improving robustness to concept drift. Extensive experiments, including those on large-scale datasets, show that the proposed method outperforms existing methods by a large margin, yielding an average performance improvement of 15.8% across various datasets, establishing a new benchmark for TS forecasting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COGNOS: Universal Enhancement for Time Series Anomaly Detection via Constrained Gaussian-Noise Optimization and Smoothing</title>
<link>https://arxiv.org/abs/2511.06894</link>
<guid>https://arxiv.org/abs/2511.06894</guid>
<content:encoded><![CDATA[

arXiv:2511.06894v1 Announce Type: new 
Abstract: Reconstruction-based methods are a dominant paradigm in time series anomaly detection (TSAD), however, their near-universal reliance on Mean Squared Error (MSE) loss results in statistically flawed reconstruction residuals. This fundamental weakness leads to noisy, unstable anomaly scores with a poor signal-to-noise ratio, hindering reliable detection. To address this, we propose Constrained Gaussian-Noise Optimization and Smoothing (COGNOS), a universal, model-agnostic enhancement framework that tackles this issue at its source. COGNOS introduces a novel Gaussian-White Noise Regularization strategy during training, which directly constrains the model's output residuals to conform to a Gaussian white noise distribution. This engineered statistical property creates the ideal precondition for our second contribution: a Kalman Smoothing Post-processor that provably operates as a statistically optimal estimator to denoise the raw anomaly scores. The synergy between these two components allows COGNOS to robustly separate the true anomaly signal from random fluctuations. Extensive experiments demonstrate that COGNOS is highly effective, delivering an average F-score uplift of 57.9% when applied to 12 diverse backbone models across multiple real-world benchmark datasets. Our work reveals that directly regularizing output statistics is a powerful and generalizable strategy for significantly improving anomaly detection systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On The Presence of Double-Descent in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.06895</link>
<guid>https://arxiv.org/abs/2511.06895</guid>
<content:encoded><![CDATA[

arXiv:2511.06895v1 Announce Type: new 
Abstract: The double descent (DD) paradox, where over-parameterized models see generalization improve past the interpolation point, remains largely unexplored in the non-stationary domain of Deep Reinforcement Learning (DRL). We present preliminary evidence that DD exists in model-free DRL, investigating it systematically across varying model capacity using the Actor-Critic framework. We rely on an information-theoretic metric, Policy Entropy, to measure policy uncertainty throughout training. Preliminary results show a clear epoch-wise DD curve; the policy's entrance into the second descent region correlates with a sustained, significant reduction in Policy Entropy. This entropic decay suggests that over-parameterization acts as an implicit regularizer, guiding the policy towards robust, flatter minima in the loss landscape. These findings establish DD as a factor in DRL and provide an information-based mechanism for designing agents that are more general, transferable, and robust.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Autoencoder-Transformer Model for Robust Day-Ahead Electricity Price Forecasting under Extreme Conditions</title>
<link>https://arxiv.org/abs/2511.06898</link>
<guid>https://arxiv.org/abs/2511.06898</guid>
<content:encoded><![CDATA[

arXiv:2511.06898v1 Announce Type: new 
Abstract: Accurate day-ahead electricity price forecasting (DAEPF) is critical for the efficient operation of power systems, but extreme condition and market anomalies pose significant challenges to existing forecasting methods. To overcome these challenges, this paper proposes a novel hybrid deep learning framework that integrates a Distilled Attention Transformer (DAT) model and an Autoencoder Self-regression Model (ASM). The DAT leverages a self-attention mechanism to dynamically assign higher weights to critical segments of historical data, effectively capturing both long-term trends and short-term fluctuations. Concurrently, the ASM employs unsupervised learning to detect and isolate anomalous patterns induced by extreme conditions, such as heavy rain, heat waves, or human festivals. Experiments on datasets sampled from California and Shandong Province demonstrate that our framework significantly outperforms state-of-the-art methods in prediction accuracy, robustness, and computational efficiency. Our framework thus holds promise for enhancing grid resilience and optimizing market operations in future power systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Closer Look at Knowledge Distillation in Spiking Neural Network Training</title>
<link>https://arxiv.org/abs/2511.06902</link>
<guid>https://arxiv.org/abs/2511.06902</guid>
<content:encoded><![CDATA[

arXiv:2511.06902v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) become popular due to excellent energy efficiency, yet facing challenges for effective model training. Recent works improve this by introducing knowledge distillation (KD) techniques, with the pre-trained artificial neural networks (ANNs) used as teachers and the target SNNs as students. This is commonly accomplished through a straightforward element-wise alignment of intermediate features and prediction logits from ANNs and SNNs, often neglecting the intrinsic differences between their architectures. Specifically, ANN's outputs exhibit a continuous distribution, whereas SNN's outputs are characterized by sparsity and discreteness. To mitigate this issue, we introduce two innovative KD strategies. Firstly, we propose the Saliency-scaled Activation Map Distillation (SAMD), which aligns the spike activation map of the student SNN with the class-aware activation map of the teacher ANN. Rather than performing KD directly on the raw %and distinct features of ANN and SNN, our SAMD directs the student to learn from saliency activation maps that exhibit greater semantic and distribution consistency. Additionally, we propose a Noise-smoothed Logits Distillation (NLD), which utilizes Gaussian noise to smooth the sparse logits of student SNN, facilitating the alignment with continuous logits from teacher ANN. Extensive experiments on multiple datasets demonstrate the effectiveness of our methods. Code is available~\footnote{https://github.com/SinoLeu/CKDSNN.git}.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Explanation for Multivariate Time Series Forecasting with Exogenous Variables</title>
<link>https://arxiv.org/abs/2511.06906</link>
<guid>https://arxiv.org/abs/2511.06906</guid>
<content:encoded><![CDATA[

arXiv:2511.06906v1 Announce Type: new 
Abstract: Currently, machine learning is widely used across various domains, including time series data analysis. However, some machine learning models function as black boxes, making interpretability a critical concern. One approach to address this issue is counterfactual explanation (CE), which aims to provide insights into model predictions. This study focuses on the relatively underexplored problem of generating counterfactual explanations for time series forecasting. We propose a method for extracting CEs in time series forecasting using exogenous variables, which are frequently encountered in fields such as business and marketing. In addition, we present methods for analyzing the influence of each variable over an entire time series, generating CEs by altering only specific variables, and evaluating the quality of the resulting CEs. We validate the proposed method through theoretical analysis and empirical experiments, showcasing its accuracy and practical applicability. These contributions are expected to support real-world decision-making based on time series data analysis.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling and Loss Weights in Multi-Domain Training</title>
<link>https://arxiv.org/abs/2511.06913</link>
<guid>https://arxiv.org/abs/2511.06913</guid>
<content:encoded><![CDATA[

arXiv:2511.06913v1 Announce Type: new 
Abstract: In the training of large deep neural networks, there is a need for vast amounts of training data. To meet this need, data is collected from multiple domains, such as Wikipedia and GitHub. These domains are heterogeneous in both data quality and the diversity of information they provide. This raises the question of how much we should rely on each domain. Several methods have attempted to address this issue by assigning sampling weights to each data domain using heuristics or approximations. As a first step toward a deeper understanding of the role of data mixing, this work revisits the problem by studying two kinds of weights: sampling weights, which control how much each domain contributes in a batch, and loss weights, which scale the loss from each domain during training. Through a rigorous study of linear regression, we show that these two weights play complementary roles. First, they can reduce the variance of gradient estimates in iterative methods such as stochastic gradient descent (SGD). Second, they can improve generalization performance by reducing the generalization gap. We provide both theoretical and empirical support for these claims. We further study the joint dynamics of sampling weights and loss weights, examining how they can be combined to capture both contributions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Focus: Prioritizing Informative Histories with Structured Attention Mechanisms in Partially Observable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.06946</link>
<guid>https://arxiv.org/abs/2511.06946</guid>
<content:encoded><![CDATA[

arXiv:2511.06946v1 Announce Type: new 
Abstract: Transformers have shown strong ability to model long-term dependencies and are increasingly adopted as world models in model-based reinforcement learning (RL) under partial observability. However, unlike natural language corpora, RL trajectories are sparse and reward-driven, making standard self-attention inefficient because it distributes weight uniformly across all past tokens rather than emphasizing the few transitions critical for control. To address this, we introduce structured inductive priors into the self-attention mechanism of the dynamics head: (i) per-head memory-length priors that constrain attention to task-specific windows, and (ii) distributional priors that learn smooth Gaussian weightings over past state-action pairs. We integrate these mechanisms into UniZero, a model-based RL agent with a Transformer-based world model that supports planning under partial observability. Experiments on the Atari 100k benchmark show that most efficiency gains arise from the Gaussian prior, which smoothly allocates attention to informative transitions, while memory-length priors often truncate useful signals with overly restrictive cut-offs. In particular, Gaussian Attention achieves a 77% relative improvement in mean human-normalized scores over UniZero. These findings suggest that in partially observable RL domains with non-stationary temporal dependencies, discrete memory windows are difficult to learn reliably, whereas smooth distributional priors flexibly adapt across horizons and yield more robust data efficiency. Overall, our results demonstrate that encoding structured temporal priors directly into self-attention improves the prioritization of informative histories for dynamics modeling under partial observability.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Autoencoders for Tabular Data: Leveraging Model-Based Augmentation in Low-Label Settings</title>
<link>https://arxiv.org/abs/2511.06961</link>
<guid>https://arxiv.org/abs/2511.06961</guid>
<content:encoded><![CDATA[

arXiv:2511.06961v1 Announce Type: new 
Abstract: Deep neural networks often under-perform on tabular data due to their sensitivity to irrelevant features and a spectral bias toward smooth, low-frequency functions. These limitations hinder their ability to capture the sharp, high-frequency signals that often define tabular structure, especially under limited labeled samples. While self-supervised learning (SSL) offers promise in such settings, it remains challenging in tabular domains due to the lack of effective data augmentations. We propose a hybrid autoencoder that combines a neural encoder with an oblivious soft decision tree (OSDT) encoder, each guided by its own stochastic gating network that performs sample-specific feature selection. Together, these structurally different encoders and model-specific gating networks implement model-based augmentation, producing complementary input views tailored to each architecture. The two encoders, trained with a shared decoder and cross-reconstruction loss, learn distinct yet aligned representations that reflect their respective inductive biases. During training, the OSDT encoder (robust to noise and effective at modeling localized, high-frequency structure) guides the neural encoder toward representations more aligned with tabular data. At inference, only the neural encoder is used, preserving flexibility and SSL compatibility. Spectral analysis highlights the distinct inductive biases of each encoder. Our method achieves consistent gains in low-label classification and regression across diverse tabular datasets, outperforming deep and tree-based supervised baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery</title>
<link>https://arxiv.org/abs/2511.06973</link>
<guid>https://arxiv.org/abs/2511.06973</guid>
<content:encoded><![CDATA[

arXiv:2511.06973v1 Announce Type: new 
Abstract: Traditional methods for identifying structurally similar spreadsheets fail to capture the spatial layouts and type patterns defining templates. To quantify spreadsheet similarity, we introduce a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning. In order to calculate spreadsheet similarity, our method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances. Experiments across template families demonstrate superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction (Adjusted Rand Index of 1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Crystal Symmetry Prediction: A Decoupled Perspective</title>
<link>https://arxiv.org/abs/2511.06976</link>
<guid>https://arxiv.org/abs/2511.06976</guid>
<content:encoded><![CDATA[

arXiv:2511.06976v1 Announce Type: new 
Abstract: Efficiently and accurately determining the symmetry is a crucial step in the structural analysis of crystalline materials. Existing methods usually mindlessly apply deep learning models while ignoring the underlying chemical rules. More importantly, experiments show that they face a serious sub-property confusion SPC problem. To address the above challenges, from a decoupled perspective, we introduce the XRDecoupler framework, a problem-solving arsenal specifically designed to tackle the SPC problem. Imitating the thinking process of chemists, we innovatively incorporate multidimensional crystal symmetry information as superclass guidance to ensure that the model's prediction process aligns with chemical intuition. We further design a hierarchical PXRD pattern learning model and a multi-objective optimization approach to achieve high-quality representation and balanced optimization. Comprehensive evaluations on three mainstream databases (e.g., CCDC, CoREMOF, and InorganicData) demonstrate that XRDecoupler excels in performance, interpretability, and generalization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Bayesian Updates via Harmonic Representations</title>
<link>https://arxiv.org/abs/2511.06978</link>
<guid>https://arxiv.org/abs/2511.06978</guid>
<content:encoded><![CDATA[

arXiv:2511.06978v1 Announce Type: new 
Abstract: Bayesian inference, while foundational to probabilistic reasoning, is often hampered by the computational intractability of posterior distributions, particularly through the challenging evidence integral. Conventional approaches like Markov Chain Monte Carlo (MCMC) and Variational Inference (VI) face significant scalability and efficiency limitations. This paper introduces a novel, unifying framework for fast Bayesian updates by leveraging harmonic analysis. We demonstrate that representing the prior and likelihood in a suitable orthogonal basis transforms the Bayesian update rule into a spectral convolution. Specifically, the Fourier coefficients of the posterior are shown to be the normalized convolution of the prior and likelihood coefficients. To achieve computational feasibility, we introduce a spectral truncation scheme, which, for smooth functions, yields an exceptionally accurate finite-dimensional approximation and reduces the update to a circular convolution. This formulation allows us to exploit the Fast Fourier Transform (FFT), resulting in a deterministic algorithm with O(N log N) complexity -- a substantial improvement over the O(N^2) cost of naive methods. We establish rigorous mathematical criteria for the applicability of our method, linking its efficiency to the smoothness and spectral decay of the involved distributions. The presented work offers a paradigm shift, connecting Bayesian computation to signal processing and opening avenues for real-time, sequential inference in a wide class of problems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Gradient Barrier: Unveiling Large Language Models for Strategic Classification</title>
<link>https://arxiv.org/abs/2511.06979</link>
<guid>https://arxiv.org/abs/2511.06979</guid>
<content:encoded><![CDATA[

arXiv:2511.06979v1 Announce Type: new 
Abstract: Strategic classification~(SC) explores how individuals or entities modify their features strategically to achieve favorable classification outcomes. However, existing SC methods, which are largely based on linear models or shallow neural networks, face significant limitations in terms of scalability and capacity when applied to real-world datasets with significantly increasing scale, especially in financial services and the internet sector. In this paper, we investigate how to leverage large language models to design a more scalable and efficient SC framework, especially in the case of growing individuals engaged with decision-making processes. Specifically, we introduce GLIM, a gradient-free SC method grounded in in-context learning. During the feed-forward process of self-attention, GLIM implicitly simulates the typical bi-level optimization process of SC, including both the feature manipulation and decision rule optimization. Without fine-tuning the LLMs, our proposed GLIM enjoys the advantage of cost-effective adaptation in dynamic strategic environments. Theoretically, we prove GLIM can support pre-trained LLMs to adapt to a broad range of strategic manipulations. We validate our approach through experiments with a collection of pre-trained LLMs on real-world and synthetic datasets in financial and internet domains, demonstrating that our GLIM exhibits both robustness and efficiency, and offering an effective solution for large-scale SC tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HCFSLN: Adaptive Hyperbolic Few-Shot Learning for Multimodal Anxiety Detection</title>
<link>https://arxiv.org/abs/2511.06988</link>
<guid>https://arxiv.org/abs/2511.06988</guid>
<content:encoded><![CDATA[

arXiv:2511.06988v1 Announce Type: new 
Abstract: Anxiety disorders impact millions globally, yet traditional diagnosis relies on clinical interviews, while machine learning models struggle with overfitting due to limited data. Large-scale data collection remains costly and time-consuming, restricting accessibility. To address this, we introduce the Hyperbolic Curvature Few-Shot Learning Network (HCFSLN), a novel Few-Shot Learning (FSL) framework for multimodal anxiety detection, integrating speech, physiological signals, and video data. HCFSLN enhances feature separability through hyperbolic embeddings, cross-modal attention, and an adaptive gating network, enabling robust classification with minimal data. We collected a multimodal anxiety dataset from 108 participants and benchmarked HCFSLN against six FSL baselines, achieving 88% accuracy, outperforming the best baseline by 14%. These results highlight the effectiveness of hyperbolic space for modeling anxiety-related speech patterns and demonstrate FSL's potential for anxiety classification.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoLM: Collaborative Large Models via A Client-Server Paradigm</title>
<link>https://arxiv.org/abs/2511.06991</link>
<guid>https://arxiv.org/abs/2511.06991</guid>
<content:encoded><![CDATA[

arXiv:2511.06991v1 Announce Type: new 
Abstract: Large models have achieved remarkable performance across a range of reasoning and understanding tasks. Prior work often utilizes model ensembles or multi-agent systems to collaboratively generate responses, effectively operating in a server-to-server paradigm. However, such approaches do not align well with practical deployment settings, where a limited number of server-side models are shared by many clients under modern internet architectures. In this paper, we introduce \textbf{CoLM} (\textbf{Co}llaboration in \textbf{L}arge-\textbf{M}odels), a novel framework for collaborative reasoning that redefines cooperation among large models from a client-server perspective. Unlike traditional ensemble methods that rely on simultaneous inference from multiple models to produce a single output, CoLM allows the outputs of multiple models to be aggregated or shared, enabling each client model to independently refine and update its own generation based on these high-quality outputs. This design enables collaborative benefits by fully leveraging both client-side and shared server-side models. We further extend CoLM to vision-language models (VLMs), demonstrating its applicability beyond language tasks. Experimental results across multiple benchmarks show that CoLM consistently improves model performance on previously failed queries, highlighting the effectiveness of collaborative guidance in enhancing single-model capabilities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S$^2$Drug: Bridging Protein Sequence and 3D Structure in Contrastive Representation Learning for Virtual Screening</title>
<link>https://arxiv.org/abs/2511.07006</link>
<guid>https://arxiv.org/abs/2511.07006</guid>
<content:encoded><![CDATA[

arXiv:2511.07006v1 Announce Type: new 
Abstract: Virtual screening (VS) is an essential task in drug discovery, focusing on the identification of small-molecule ligands that bind to specific protein pockets. Existing deep learning methods, from early regression models to recent contrastive learning approaches, primarily rely on structural data while overlooking protein sequences, which are more accessible and can enhance generalizability. However, directly integrating protein sequences poses challenges due to the redundancy and noise in large-scale protein-ligand datasets. To address these limitations, we propose \textbf{S$^2$Drug}, a two-stage framework that explicitly incorporates protein \textbf{S}equence information and 3D \textbf{S}tructure context in protein-ligand contrastive representation learning. In the first stage, we perform protein sequence pretraining on ChemBL using an ESM2-based backbone, combined with a tailored data sampling strategy to reduce redundancy and noise on both protein and ligand sides. In the second stage, we fine-tune on PDBBind by fusing sequence and structure information through a residue-level gating module, while introducing an auxiliary binding site prediction task. This auxiliary task guides the model to accurately localize binding residues within the protein sequence and capture their 3D spatial arrangement, thereby refining protein-ligand matching. Across multiple benchmarks, S$^2$Drug consistently improves virtual screening performance and achieves strong results on binding site prediction, demonstrating the value of bridging sequence and structure in contrastive learning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Correcting False Alarms from Unseen: Adapting Graph Anomaly Detectors at Test Time</title>
<link>https://arxiv.org/abs/2511.07023</link>
<guid>https://arxiv.org/abs/2511.07023</guid>
<content:encoded><![CDATA[

arXiv:2511.07023v1 Announce Type: new 
Abstract: Graph anomaly detection (GAD), which aims to detect outliers in graph-structured data, has received increasing research attention recently. However, existing GAD methods assume identical training and testing distributions, which is rarely valid in practice. In real-world scenarios, unseen but normal samples may emerge during deployment, leading to a normality shift that degrades the performance of GAD models trained on the original data. Through empirical analysis, we reveal that the degradation arises from (1) semantic confusion, where unseen normal samples are misinterpreted as anomalies due to their novel patterns, and (2) aggregation contamination, where the representations of seen normal nodes are distorted by unseen normals through message aggregation. While retraining or fine-tuning GAD models could be a potential solution to the above challenges, the high cost of model retraining and the difficulty of obtaining labeled data often render this approach impractical in real-world applications. To bridge the gap, we proposed a lightweight and plug-and-play Test-time adaptation framework for correcting Unseen Normal pattErns (TUNE) in GAD. To address semantic confusion, a graph aligner is employed to align the shifted data to the original one at the graph attribute level. Moreover, we utilize the minimization of representation-level shift as a supervision signal to train the aligner, which leverages the estimated aggregation contamination as a key indicator of normality shift. Extensive experiments on 10 real-world datasets demonstrate that TUNE significantly enhances the generalizability of pre-trained GAD models to both synthetic and real unseen normal patterns.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair Bayesian Data Selection via Generalized Discrepancy Measures</title>
<link>https://arxiv.org/abs/2511.07032</link>
<guid>https://arxiv.org/abs/2511.07032</guid>
<content:encoded><![CDATA[

arXiv:2511.07032v1 Announce Type: new 
Abstract: Fairness concerns are increasingly critical as machine learning models are deployed in high-stakes applications. While existing fairness-aware methods typically intervene at the model level, they often suffer from high computational costs, limited scalability, and poor generalization. To address these challenges, we propose a Bayesian data selection framework that ensures fairness by aligning group-specific posterior distributions of model parameters and sample weights with a shared central distribution. Our framework supports flexible alignment via various distributional discrepancy measures, including Wasserstein distance, maximum mean discrepancy, and $f$-divergence, allowing geometry-aware control without imposing explicit fairness constraints. This data-centric approach mitigates group-specific biases in training data and improves fairness in downstream tasks, with theoretical guarantees. Experiments on benchmark datasets show that our method consistently outperforms existing data selection and model-based fairness methods in both fairness and accuracy.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Quantized Continuous Controllers for Integer Hardware</title>
<link>https://arxiv.org/abs/2511.07046</link>
<guid>https://arxiv.org/abs/2511.07046</guid>
<content:encoded><![CDATA[

arXiv:2511.07046v1 Announce Type: new 
Abstract: Deploying continuous-control reinforcement learning policies on embedded hardware requires meeting tight latency and power budgets. Small FPGAs can deliver these, but only if costly floating point pipelines are avoided. We study quantization-aware training (QAT) of policies for integer inference and we present a learning-to-hardware pipeline that automatically selects low-bit policies and synthesizes them to an Artix-7 FPGA. Across five MuJoCo tasks, we obtain policy networks that are competitive with full precision (FP32) policies but require as few as 3 or even only 2 bits per weight, and per internal activation value, as long as input precision is chosen carefully. On the target hardware, the selected policies achieve inference latencies on the order of microseconds and consume microjoules per action, favorably comparing to a quantized reference. Last, we observe that the quantized policies exhibit increased input noise robustness compared to the floating-point baseline.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Privacy in Federated Clustering: Perfect Input Reconstruction via Temporal Correlations</title>
<link>https://arxiv.org/abs/2511.07073</link>
<guid>https://arxiv.org/abs/2511.07073</guid>
<content:encoded><![CDATA[

arXiv:2511.07073v1 Announce Type: new 
Abstract: Federated clustering allows multiple parties to discover patterns in distributed data without sharing raw samples. To reduce overhead, many protocols disclose intermediate centroids during training. While often treated as harmless for efficiency, whether such disclosure compromises privacy remains an open question. Prior analyses modeled the problem as a so-called Hidden Subset Sum Problem (HSSP) and argued that centroid release may be safe, since classical HSSP attacks fail to recover inputs.
  We revisit this question and uncover a new leakage mechanism: temporal regularities in $k$-means iterations create exploitable structure that enables perfect input reconstruction. Building on this insight, we propose Trajectory-Aware Reconstruction (TAR), an attack that combines temporal assignment information with algebraic analysis to recover exact original inputs. Our findings provide the first rigorous evidence, supported by a practical attack, that centroid disclosure in federated clustering significantly compromises privacy, exposing a fundamental tension between privacy and efficiency.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Direct Molecular Polarizability Prediction with SO(3) Equivariant Local Frame GNNs</title>
<link>https://arxiv.org/abs/2511.07087</link>
<guid>https://arxiv.org/abs/2511.07087</guid>
<content:encoded><![CDATA[

arXiv:2511.07087v1 Announce Type: new 
Abstract: We introduce a novel equivariant graph neural network (GNN) architecture designed to predict the tensorial response properties of molecules. Unlike traditional frameworks that focus on regressing scalar quantities and derive tensorial properties from their derivatives, our approach maintains $SO(3)$-equivariance through the use of local coordinate frames. Our GNN effectively captures geometric information by integrating scalar, vector, and tensor channels within a local message-passing framework. To assess the accuracy of our model, we apply it to predict the polarizabilities of molecules in the QM7-X dataset and show that tensorial message passing outperforms scalar message passing models. This work marks an advancement towards developing structured, geometry-aware neural models for molecular property prediction.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Joint Minimization of Regularization Loss Functions in Deep Variational Bayesian Methods for Attribute-Controlled Symbolic Music Generation</title>
<link>https://arxiv.org/abs/2511.07118</link>
<guid>https://arxiv.org/abs/2511.07118</guid>
<content:encoded><![CDATA[

arXiv:2511.07118v1 Announce Type: new 
Abstract: Explicit latent variable models provide a flexible yet powerful framework for data synthesis, enabling controlled manipulation of generative factors. With latent variables drawn from a tractable probability density function that can be further constrained, these models enable continuous and semantically rich exploration of the output space by navigating their latent spaces. Structured latent representations are typically obtained through the joint minimization of regularization loss functions. In variational information bottleneck models, reconstruction loss and Kullback-Leibler Divergence (KLD) are often linearly combined with an auxiliary Attribute-Regularization (AR) loss. However, balancing KLD and AR turns out to be a very delicate matter. When KLD dominates over AR, generative models tend to lack controllability; when AR dominates over KLD, the stochastic encoder is encouraged to violate the standard normal prior. We explore this trade-off in the context of symbolic music generation with explicit control over continuous musical attributes. We show that existing approaches struggle to jointly minimize both regularization objectives, whereas suitable attribute transformations can help achieve both controllability and regularization of the target latent dimensions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks</title>
<link>https://arxiv.org/abs/2511.07127</link>
<guid>https://arxiv.org/abs/2511.07127</guid>
<content:encoded><![CDATA[

arXiv:2511.07127v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and causal learning each hold strong potential for clinical decision making (CDM). However, their synergy remains poorly understood, largely due to the lack of systematic benchmarks evaluating their integration in clinical risk prediction. In real-world healthcare, identifying features with causal influence on outcomes is crucial for actionable and trustworthy predictions. While recent work highlights LLMs' emerging causal reasoning abilities, there lacks comprehensive benchmarks to assess their causal learning and performance informed by causal features in clinical risk prediction. To address this, we introduce REACT-LLM, a benchmark designed to evaluate whether combining LLMs with causal features can enhance clinical prognostic performance and potentially outperform traditional machine learning (ML) methods. Unlike existing LLM-clinical benchmarks that often focus on a limited set of outcomes, REACT-LLM evaluates 7 clinical outcomes across 2 real-world datasets, comparing 15 prominent LLMs, 6 traditional ML models, and 3 causal discovery (CD) algorithms. Our findings indicate that while LLMs perform reasonably in clinical prognostics, they have not yet outperformed traditional ML models. Integrating causal features derived from CD algorithms into LLMs offers limited performance gains, primarily due to the strict assumptions of many CD methods, which are often violated in complex clinical data. While the direct integration yields limited improvement, our benchmark reveals a more promising synergy.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Diffusion as Latent Constraints for Controllable Symbolic Music Generation</title>
<link>https://arxiv.org/abs/2511.07156</link>
<guid>https://arxiv.org/abs/2511.07156</guid>
<content:encoded><![CDATA[

arXiv:2511.07156v1 Announce Type: new 
Abstract: Recent advances in latent diffusion models have demonstrated state-of-the-art performance in high-dimensional time-series data synthesis while providing flexible control through conditioning and guidance. However, existing methodologies primarily rely on musical context or natural language as the main modality of interacting with the generative process, which may not be ideal for expert users who seek precise fader-like control over specific musical attributes. In this work, we explore the application of denoising diffusion processes as plug-and-play latent constraints for unconditional symbolic music generation models. We focus on a framework that leverages a library of small conditional diffusion models operating as implicit probabilistic priors on the latents of a frozen unconditional backbone. While previous studies have explored domain-specific use cases, this work, to the best of our knowledge, is the first to demonstrate the versatility of such an approach across a diverse array of musical attributes, such as note density, pitch range, contour, and rhythm complexity. Our experiments show that diffusion-driven constraints outperform traditional attribute regularization and other latent constraints architectures, achieving significantly stronger correlations between target and generated attributes while maintaining high perceptual quality and diversity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding Generative Models to Uncover Diverse and Novel Crystals via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.07158</link>
<guid>https://arxiv.org/abs/2511.07158</guid>
<content:encoded><![CDATA[

arXiv:2511.07158v1 Announce Type: new 
Abstract: Discovering functional crystalline materials entails navigating an immense combinatorial design space. While recent advances in generative artificial intelligence have enabled the sampling of chemically plausible compositions and structures, a fundamental challenge remains: the objective misalignment between likelihood-based sampling in generative modelling and targeted focus on underexplored regions where novel compounds reside. Here, we introduce a reinforcement learning framework that guides latent denoising diffusion models toward diverse and novel, yet thermodynamically viable crystalline compounds. Our approach integrates group relative policy optimisation with verifiable, multi-objective rewards that jointly balance creativity, stability, and diversity. Beyond de novo generation, we demonstrate enhanced property-guided design that preserves chemical validity, while targeting desired functional properties. This approach establishes a modular foundation for controllable AI-driven inverse design that addresses the novelty-validity trade-off across scientific discovery applications of generative models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMscape</title>
<link>https://arxiv.org/abs/2511.07161</link>
<guid>https://arxiv.org/abs/2511.07161</guid>
<content:encoded><![CDATA[

arXiv:2511.07161v1 Announce Type: new 
Abstract: LLMscape is an interactive installation that investigates how humans and AI construct meaning under shared conditions of uncertainty. Within a mutable, projection-mapped landscape, human participants reshape the world and engage with multiple AI agents, each developing incomplete and provisional accounts of their environment. Exhibited in Shanghai and continually evolving, the work positions AI not as deterministic tools but as embodied co-witnesses to an unstable world, examining the parallels between human and artificial meaning-making and inviting reflection on our shared epistemic limits.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Combining digital data streams and epidemic networks for real time outbreak detection</title>
<link>https://arxiv.org/abs/2511.07163</link>
<guid>https://arxiv.org/abs/2511.07163</guid>
<content:encoded><![CDATA[

arXiv:2511.07163v1 Announce Type: new 
Abstract: Responding to disease outbreaks requires close surveillance of their trajectories, but outbreak detection is hindered by the high noise in epidemic time series. Aggregating information across data sources has shown great denoising ability in other fields, but remains underexplored in epidemiology. Here, we present LRTrend, an interpretable machine learning framework to identify outbreaks in real time. LRTrend effectively aggregates diverse health and behavioral data streams within one region and learns disease-specific epidemic networks to aggregate information across regions. We reveal diverse epidemic clusters and connections across the United States that are not well explained by commonly used human mobility networks and may be informative for future public health coordination. We apply LRTrend to 2 years of COVID-19 data in 305 hospital referral regions and frequently detect regional Delta and Omicron waves within 2 weeks of the outbreak's start, when case counts are a small fraction of the wave's resulting peak.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fuzzy Label: From Concept to Its Application in Label Learning</title>
<link>https://arxiv.org/abs/2511.07165</link>
<guid>https://arxiv.org/abs/2511.07165</guid>
<content:encoded><![CDATA[

arXiv:2511.07165v1 Announce Type: new 
Abstract: Label learning is a fundamental task in machine learning that aims to construct intelligent models using labeled data, encompassing traditional single-label and multi-label classification models. Traditional methods typically rely on logical labels, such as binary indicators (e.g., "yes/no") that specify whether an instance belongs to a given category. However, in practical applications, label annotations often involve significant uncertainty due to factors such as data noise, inherent ambiguity in the observed entities, and the subjectivity of human annotators. Therefore, representing labels using simplistic binary logic can obscure valuable information and limit the expressiveness of label learning models. To overcome this limitation, this paper introduces the concept of fuzzy labels, grounded in fuzzy set theory, to better capture and represent label uncertainty. We further propose an efficient fuzzy labeling method that mines and generates fuzzy labels from the original data, thereby enriching the label space with more informative and nuanced representations. Based on this foundation, we present fuzzy-label-enhanced algorithms for both single-label and multi-label learning, using the classical K-Nearest Neighbors (KNN) and multi-label KNN algorithms as illustrative examples. Experimental results indicate that fuzzy labels can more effectively characterize the real-world labeling information and significantly enhance the performance of label learning models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Stealing Graph Neural Network Models</title>
<link>https://arxiv.org/abs/2511.07170</link>
<guid>https://arxiv.org/abs/2511.07170</guid>
<content:encoded><![CDATA[

arXiv:2511.07170v1 Announce Type: new 
Abstract: Current graph neural network (GNN) model-stealing methods rely heavily on queries to the victim model, assuming no hard query limits. However, in reality, the number of allowed queries can be severely limited. In this paper, we demonstrate how an adversary can extract the GNN with very limited interactions with the model. Our approach first enables the adversary to obtain the model backbone without making direct queries to the victim model and then to strategically utilize a fixed query limit to extract the most informative data. The experiments on eight real-world datasets demonstrate the effectiveness of the attack, even under a very restricted query limit and under defense against model extraction in place. Our findings underscore the need for robust defenses against GNN model extraction threats.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synergy over Discrepancy: A Partition-Based Approach to Multi-Domain LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.07198</link>
<guid>https://arxiv.org/abs/2511.07198</guid>
<content:encoded><![CDATA[

arXiv:2511.07198v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate impressive generalization abilities, yet adapting them effectively across multiple heterogeneous domains remains challenging due to inter-domain interference. To overcome this challenge, we propose a partition-based multi-stage fine-tuning framework designed to exploit inter-domain synergies while minimizing negative transfer. Our approach strategically partitions domains into subsets (stages) by balancing domain discrepancy, synergy, and model capacity constraints. We theoretically analyze the proposed framework and derive novel generalization bounds that justify our partitioning strategy. Extensive empirical evaluations on various language understanding tasks show that our method consistently outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMiLE: Provably Enforcing Global Relational Properties in Neural Networks</title>
<link>https://arxiv.org/abs/2511.07208</link>
<guid>https://arxiv.org/abs/2511.07208</guid>
<content:encoded><![CDATA[

arXiv:2511.07208v1 Announce Type: new 
Abstract: Artificial Intelligence systems are increasingly deployed in settings where ensuring robustness, fairness, or domain-specific properties is essential for regulation compliance and alignment with human values. However, especially on Neural Networks, property enforcement is very challenging, and existing methods are limited to specific constraints or local properties (defined around datapoints), or fail to provide full guarantees. We tackle these limitations by extending SMiLE, a recently proposed enforcement framework for NNs, to support global relational properties (defined over the entire input space). The proposed approach scales well with model complexity, accommodates general properties and backbones, and provides full satisfaction guarantees. We evaluate SMiLE on monotonicity, global robustness, and individual fairness, on synthetic and real data, for regression and classification tasks. Our approach is competitive with property-specific baselines in terms of accuracy and runtime, and strictly superior in terms of generality and level of guarantees. Overall, our results emphasize the potential of the SMiLE framework as a platform for future research and applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DETECT: Data-Driven Evaluation of Treatments Enabled by Classification Transformers</title>
<link>https://arxiv.org/abs/2511.07213</link>
<guid>https://arxiv.org/abs/2511.07213</guid>
<content:encoded><![CDATA[

arXiv:2511.07213v1 Announce Type: new 
Abstract: Chronic pain is a global health challenge affecting millions of individuals, making it essential for physicians to have reliable and objective methods to measure the functional impact of clinical treatments. Traditionally used methods, like the numeric rating scale, while personalized and easy to use, are subjective due to their self-reported nature. Thus, this paper proposes DETECT (Data-Driven Evaluation of Treatments Enabled by Classification Transformers), a data-driven framework that assesses treatment success by comparing patient activities of daily life before and after treatment. We use DETECT on public benchmark datasets and simulated patient data from smartphone sensors. Our results demonstrate that DETECT is objective yet lightweight, making it a significant and novel contribution to clinical decision-making. By using DETECT, independently or together with other self-reported metrics, physicians can improve their understanding of their treatment impacts, ultimately leading to more personalized and responsive patient care.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Neural Operator Learning for Probabilistic Models</title>
<link>https://arxiv.org/abs/2511.07235</link>
<guid>https://arxiv.org/abs/2511.07235</guid>
<content:encoded><![CDATA[

arXiv:2511.07235v1 Announce Type: new 
Abstract: We propose a deep neural-operator framework for a general class of probability models. Under global Lipschitz conditions on the operator over the entire Euclidean space-and for a broad class of probabilistic models-we establish a universal approximation theorem with explicit network-size bounds for the proposed architecture. The underlying stochastic processes are required only to satisfy integrability and general tail-probability conditions. We verify these assumptions for both European and American option-pricing problems within the forward-backward SDE (FBSDE) framework, which in turn covers a broad class of operators arising from parabolic PDEs, with or without free boundaries. Finally, we present a numerical example for a basket of American options, demonstrating that the learned model produces optimal stopping boundaries for new strike prices without retraining.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does TabPFN Understand Causal Structures?</title>
<link>https://arxiv.org/abs/2511.07236</link>
<guid>https://arxiv.org/abs/2511.07236</guid>
<content:encoded><![CDATA[

arXiv:2511.07236v1 Announce Type: new 
Abstract: Causal discovery is fundamental for multiple scientific domains, yet extracting causal information from real world data remains a significant challenge. Given the recent success on real data, we investigate whether TabPFN, a transformer-based tabular foundation model pre-trained on synthetic datasets generated from structural causal models, encodes causal information in its internal representations. We develop an adapter framework using a learnable decoder and causal tokens that extract causal signals from TabPFN's frozen embeddings and decode them into adjacency matrices for causal discovery. Our evaluations demonstrate that TabPFN's embeddings contain causal information, outperforming several traditional causal discovery algorithms, with such causal information being concentrated in mid-range layers. These findings establish a new direction for interpretable and adaptable foundation models and demonstrate the potential for leveraging pre-trained tabular models for causal discovery.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Few Govern the Many:Unveiling Few-Layer Dominance for Time Series Models</title>
<link>https://arxiv.org/abs/2511.07237</link>
<guid>https://arxiv.org/abs/2511.07237</guid>
<content:encoded><![CDATA[

arXiv:2511.07237v1 Announce Type: new 
Abstract: Large-scale models are at the forefront of time series (TS) forecasting, dominated by two paradigms: fine-tuning text-based Large Language Models (LLM4TS) and training Time Series Foundation Models (TSFMs) from scratch. Both approaches share a foundational assumption that scaling up model capacity and data volume leads to improved performance. However, we observe a \textit{\textbf{scaling paradox}} in TS models, revealing a puzzling phenomenon that larger models do \emph{NOT} achieve better performance. Through extensive experiments on two model families across four scales (100M to 1.7B parameters) and diverse data (up to 6B observations), we rigorously confirm that the scaling paradox is a pervasive issue. We then diagnose its root cause by analyzing internal representations, identifying a phenomenon we call \textit{few-layer dominance}: only a small subset of layers are functionally important, while the majority are redundant, under-utilized, and can even distract training. Based on this discovery, we propose a practical method to automatically identify and retain only these dominant layers. In our models, retaining only 21\% of the parameters achieves up to a 12\% accuracy improvement and a 2.7$\times$ inference speedup. We validate the universality of our method on 8 prominent SOTA models (LLM4TS and TSFMs, 90M to 6B), showing that retaining less than 30\% of layers achieves comparable or superior accuracy in over 95\% of tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding the role of depth in the neural tangent kernel for overparameterized neural networks</title>
<link>https://arxiv.org/abs/2511.07272</link>
<guid>https://arxiv.org/abs/2511.07272</guid>
<content:encoded><![CDATA[

arXiv:2511.07272v1 Announce Type: new 
Abstract: Overparameterized fully-connected neural networks have been shown to behave like kernel models when trained with gradient descent, under mild conditions on the width, the learning rate, and the parameter initialization. In the limit of infinitely large widths and small learning rate, the kernel that is obtained allows to represent the output of the learned model with a closed-form solution. This closed-form solution hinges on the invertibility of the limiting kernel, a property that often holds on real-world datasets. In this work, we analyze the sensitivity of large ReLU networks to increasing depths by characterizing the corresponding limiting kernel. Our theoretical results demonstrate that the normalized limiting kernel approaches the matrix of ones. In contrast, they show the corresponding closed-form solution approaches a fixed limit on the sphere. We empirically evaluate the order of magnitude in network depth required to observe this convergent behavior, and we describe the essential properties that enable the generalization of our results to other kernels.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-modal Dynamic Proxy Learning for Personalized Multiple Clustering</title>
<link>https://arxiv.org/abs/2511.07274</link>
<guid>https://arxiv.org/abs/2511.07274</guid>
<content:encoded><![CDATA[

arXiv:2511.07274v1 Announce Type: new 
Abstract: Multiple clustering aims to discover diverse latent structures from different perspectives, yet existing methods generate exhaustive clusterings without discerning user interest, necessitating laborious manual screening. Current multi-modal solutions suffer from static semantic rigidity: predefined candidate words fail to adapt to dataset-specific concepts, and fixed fusion strategies ignore evolving feature interactions. To overcome these limitations, we propose Multi-DProxy, a novel multi-modal dynamic proxy learning framework that leverages cross-modal alignment through learnable textual proxies. Multi-DProxy introduces 1) gated cross-modal fusion that synthesizes discriminative joint representations by adaptively modeling feature interactions. 2) dual-constraint proxy optimization where user interest constraints enforce semantic consistency with domain concepts while concept constraints employ hard example mining to enhance cluster discrimination. 3) dynamic candidate management that refines textual proxies through iterative clustering feedback. Therefore, Multi-DProxy not only effectively captures a user's interest through proxies but also enables the identification of relevant clusterings with greater precision. Extensive experiments demonstrate state-of-the-art performance with significant improvements over existing methods across a broad set of multi-clustering benchmarks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustA: Robust Anomaly Detection in Multimodal Data</title>
<link>https://arxiv.org/abs/2511.07276</link>
<guid>https://arxiv.org/abs/2511.07276</guid>
<content:encoded><![CDATA[

arXiv:2511.07276v1 Announce Type: new 
Abstract: In recent years, multimodal anomaly detection methods have demonstrated remarkable performance improvements over video-only models. However, real-world multimodal data is often corrupted due to unforeseen environmental distortions. In this paper, we present the first-of-its-kind work that comprehensively investigates the adverse effects of corrupted modalities on multimodal anomaly detection task. To streamline this work, we propose RobustA, a carefully curated evaluation dataset to systematically observe the impacts of audio and visual corruptions on the overall effectiveness of anomaly detection systems. Furthermore, we propose a multimodal anomaly detection method, which shows notable resilience against corrupted modalities. The proposed method learns a shared representation space for different modalities and employs a dynamic weighting scheme during inference based on the estimated level of corruption. Our work represents a significant step forward in enabling the real-world application of multimodal anomaly detection, addressing situations where the likely events of modality corruptions occur. The proposed evaluation dataset with corrupted modalities and respective extracted features will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MG-HGNN: A Heterogeneous GNN Framework for Indoor Wi-Fi Fingerprint-Based Localization</title>
<link>https://arxiv.org/abs/2511.07282</link>
<guid>https://arxiv.org/abs/2511.07282</guid>
<content:encoded><![CDATA[

arXiv:2511.07282v1 Announce Type: new 
Abstract: Received signal strength indicator (RSSI) is the primary representation of Wi-Fi fingerprints and serves as a crucial tool for indoor localization. However, existing RSSI-based positioning methods often suffer from reduced accuracy due to environmental complexity and challenges in processing multi-source information. To address these issues, we propose a novel multi-graph heterogeneous GNN framework (MG-HGNN) to enhance spatial awareness and improve positioning performance. In this framework, two graph construction branches perform node and edge embedding, respectively, to generate informative graphs. Subsequently, a heterogeneous graph neural network is employed for graph representation learning, enabling accurate positioning. The MG-HGNN framework introduces the following key innovations: 1) multi-type task-directed graph construction that combines label estimation and feature encoding for richer graph information; 2) a heterogeneous GNN structure that enhances the performance of conventional GNN models. Evaluations on the UJIIndoorLoc and UTSIndoorLoc public datasets demonstrate that MG-HGNN not only achieves superior performance compared to several state-of-the-art methods, but also provides a novel perspective for enhancing GNN-based localization methods. Ablation studies further confirm the rationality and effectiveness of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Off-Policy Imitation Learning with Deep Actor Critic Stabilization</title>
<link>https://arxiv.org/abs/2511.07288</link>
<guid>https://arxiv.org/abs/2511.07288</guid>
<content:encoded><![CDATA[

arXiv:2511.07288v1 Announce Type: new 
Abstract: Learning complex policies with Reinforcement Learning (RL) is often hindered by instability and slow convergence, a problem exacerbated by the difficulty of reward engineering. Imitation Learning (IL) from expert demonstrations bypasses this reliance on rewards. However, state-of-the-art IL methods, exemplified by Generative Adversarial Imitation Learning (GAIL)Ho et. al, suffer from severe sample inefficiency. This is a direct consequence of their foundational on-policy algorithms, such as TRPO Schulman et.al. In this work, we introduce an adversarial imitation learning algorithm that incorporates off-policy learning to improve sample efficiency. By combining an off-policy framework with auxiliary techniques specifically, double Q network based stabilization and value learning without reward function inference we demonstrate a reduction in the samples required to robustly match expert behavior.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Training Dynamics of Scale-Invariant Neural Networks Be Explained by the Thermodynamics of an Ideal Gas?</title>
<link>https://arxiv.org/abs/2511.07308</link>
<guid>https://arxiv.org/abs/2511.07308</guid>
<content:encoded><![CDATA[

arXiv:2511.07308v1 Announce Type: new 
Abstract: Understanding the training dynamics of deep neural networks remains a major open problem, with physics-inspired approaches offering promising insights. Building on this perspective, we develop a thermodynamic framework to describe the stationary distributions of stochastic gradient descent (SGD) with weight decay for scale-invariant neural networks, a setting that both reflects practical architectures with normalization layers and permits theoretical analysis. We establish analogies between training hyperparameters (e.g., learning rate, weight decay) and thermodynamic variables such as temperature, pressure, and volume. Starting with a simplified isotropic noise model, we uncover a close correspondence between SGD dynamics and ideal gas behavior, validated through theory and simulation. Extending to training of neural networks, we show that key predictions of the framework, including the behavior of stationary entropy, align closely with experimental observations. This framework provides a principled foundation for interpreting training dynamics and may guide future work on hyperparameter tuning and the design of learning rate schedulers.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Superhuman AI for Stratego Using Self-Play Reinforcement Learning and Test-Time Search</title>
<link>https://arxiv.org/abs/2511.07312</link>
<guid>https://arxiv.org/abs/2511.07312</guid>
<content:encoded><![CDATA[

arXiv:2511.07312v1 Announce Type: new 
Abstract: Few classical games have been regarded as such significant benchmarks of artificial intelligence as to have justified training costs in the millions of dollars. Among these, Stratego -- a board wargame exemplifying the challenge of strategic decision making under massive amounts of hidden information -- stands apart as a case where such efforts failed to produce performance at the level of top humans. This work establishes a step change in both performance and cost for Stratego, showing that it is now possible not only to reach the level of top humans, but to achieve vastly superhuman level -- and that doing so requires not an industrial budget, but merely a few thousand dollars. We achieved this result by developing general approaches for self-play reinforcement learning and test-time search under imperfect information.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training</title>
<link>https://arxiv.org/abs/2511.07328</link>
<guid>https://arxiv.org/abs/2511.07328</guid>
<content:encoded><![CDATA[

arXiv:2511.07328v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) methods enhance LLM performance by efficiently filtering relevant context for LLMs, reducing hallucinations and inference cost. However, most existing RAG methods focus on single-step retrieval, which is often insufficient for answering complex questions that require multi-step search. Recently, multi-step retrieval approaches have emerged, typically involving the fine-tuning of small LLMs to perform multi-step retrieval. This type of fine-tuning is highly resource-intensive and does not enable the use of larger LLMs. In this work, we propose Q-RAG, a novel approach that fine-tunes the Embedder model for multi-step retrieval using reinforcement learning (RL). Q-RAG offers a competitive, resource-efficient alternative to existing multi-step retrieval methods for open-domain question answering and achieves state-of-the-art results on the popular long-context benchmarks Babilong and RULER for contexts up to 10M tokens.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preparation of Fractal-Inspired Computational Architectures for Advanced Large Language Model Analysis</title>
<link>https://arxiv.org/abs/2511.07329</link>
<guid>https://arxiv.org/abs/2511.07329</guid>
<content:encoded><![CDATA[

arXiv:2511.07329v1 Announce Type: new 
Abstract: It introduces FractalNet, a fractal-inspired computational architectures for advanced large language model analysis that mainly challenges model diversity on a large scale in an efficient manner. The new set-up involves a template-driven generator, runner, and evaluation framework that, through systematic permutations of convolutional, normalization, activation, and dropout layers, can create more than 1,200 variants of neural networks. Fractal templates allow for structural recursion and multi-column pathways, thus, models become deeper and wider in a balanced way. Training utilizes PyTorch, Automatic Mixed Precision (AMP), and gradient checkpointing and is carried out on the CIFAR-10 dataset for five epochs. The outcomes show that fractal-based architectures are capable of strong performance and are computationally efficient. The paper positions fractal design as a feasible and resource-efficient method of automated architecture exploration.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounding Computer Use Agents on Human Demonstrations</title>
<link>https://arxiv.org/abs/2511.07332</link>
<guid>https://arxiv.org/abs/2511.07332</guid>
<content:encoded><![CDATA[

arXiv:2511.07332v1 Announce Type: new 
Abstract: Building reliable computer-use agents requires grounding: accurately connecting natural language instructions to the correct on-screen elements. While large datasets exist for web and mobile interactions, high-quality resources for desktop environments are limited. To address this gap, we introduce GroundCUA, a large-scale desktop grounding dataset built from expert human demonstrations. It covers 87 applications across 12 categories and includes 56K screenshots, with every on-screen element carefully annotated for a total of over 3.56M human-verified annotations. From these demonstrations, we generate diverse instructions that capture a wide range of real-world tasks, providing high-quality data for model training. Using GroundCUA, we develop the GroundNext family of models that map instructions to their target UI elements. At both 3B and 7B scales, GroundNext achieves state-of-the-art results across five benchmarks using supervised fine-tuning, while requiring less than one-tenth the training data of prior work. Reinforcement learning post-training further improves performance, and when evaluated in an agentic setting on the OSWorld benchmark using o3 as planner, GroundNext attains comparable or superior results to models trained with substantially more data,. These results demonstrate the critical role of high-quality, expert-driven datasets in advancing general-purpose computer-use agents.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TNT: Improving Chunkwise Training for Test-Time Memorization</title>
<link>https://arxiv.org/abs/2511.07343</link>
<guid>https://arxiv.org/abs/2511.07343</guid>
<content:encoded><![CDATA[

arXiv:2511.07343v1 Announce Type: new 
Abstract: Recurrent neural networks (RNNs) with deep test-time memorization modules, such as Titans and TTT, represent a promising, linearly-scaling paradigm distinct from Transformers. While these expressive models do not yet match the peak performance of state-of-the-art Transformers, their potential has been largely untapped due to prohibitively slow training and low hardware utilization. Existing parallelization methods force a fundamental conflict governed by the chunksize hyperparameter: large chunks boost speed but degrade performance, necessitating a fixed, suboptimal compromise. To solve this challenge, we introduce TNT, a novel training paradigm that decouples training efficiency from inference performance through a two-stage process. Stage one is an efficiency-focused pre-training phase utilizing a hierarchical memory. A global module processes large, hardware-friendly chunks for long-range context, while multiple parallel local modules handle fine-grained details. Crucially, by periodically resetting local memory states, we break sequential dependencies to enable massive context parallelization. Stage two is a brief fine-tuning phase where only the local memory modules are adapted to a smaller, high-resolution chunksize, maximizing accuracy with minimal overhead. Evaluated on Titans and TTT models, TNT achieves a substantial acceleration in training speed-up to 17 times faster than the most accurate baseline configuration - while simultaneously improving model accuracy. This improvement removes a critical scalability barrier, establishing a practical foundation for developing expressive RNNs and facilitating future work to close the performance gap with Transformers.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection</title>
<link>https://arxiv.org/abs/2511.07364</link>
<guid>https://arxiv.org/abs/2511.07364</guid>
<content:encoded><![CDATA[

arXiv:2511.07364v1 Announce Type: new 
Abstract: Reliability and failure detection of large language models (LLMs) is critical for their deployment in high-stakes, multi-step reasoning tasks. Prior work explores confidence estimation for self-evaluating LLM-scorer systems, with confidence scorers estimating the likelihood of errors in LLM responses. However, most methods focus on single-step outputs and overlook the challenges of multi-step reasoning. In this work, we extend self-evaluation techniques to multi-step tasks, testing two intuitive approaches: holistic scoring and step-by-step scoring. Using two multi-step benchmark datasets, we show that stepwise evaluation generally outperforms holistic scoring in detecting potential errors, with up to 15% relative increase in AUC-ROC. Our findings demonstrate that self-evaluating LLM systems provide meaningful confidence estimates in complex reasoning, improving their trustworthiness and providing a practical framework for failure detection.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Private Sketches for Linear Regression</title>
<link>https://arxiv.org/abs/2511.07365</link>
<guid>https://arxiv.org/abs/2511.07365</guid>
<content:encoded><![CDATA[

arXiv:2511.07365v1 Announce Type: new 
Abstract: Linear regression is frequently applied in a variety of domains. In order to improve the efficiency of these methods, various methods have been developed that compute summaries or \emph{sketches} of the datasets. Certain domains, however, contain sensitive data which necessitates that the application of these statistical methods does not reveal private information. Differentially private (DP) linear regression methods have been developed for mitigating this problem. These techniques typically involve estimating a noisy version of the parameter vector. Instead, we propose releasing private sketches of the datasets. We present differentially private sketches for the problems of least squares regression, as well as least absolute deviations regression. The availability of these private sketches facilitates the application of commonly available solvers for regression, without the risk of privacy leakage.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistency Is Not Always Correct: Towards Understanding the Role of Exploration in Post-Training Reasoning</title>
<link>https://arxiv.org/abs/2511.07368</link>
<guid>https://arxiv.org/abs/2511.07368</guid>
<content:encoded><![CDATA[

arXiv:2511.07368v1 Announce Type: new 
Abstract: Foundation models exhibit broad knowledge but limited task-specific reasoning, motivating post-training strategies such as RLVR and inference scaling with outcome or process reward models (ORM/PRM). While recent work highlights the role of exploration and entropy stability in improving pass@K, empirical evidence points to a paradox: RLVR and ORM/PRM typically reinforce existing tree-like reasoning paths rather than expanding the reasoning scope, raising the question of why exploration helps at all if no new patterns emerge.
  To reconcile this paradox, we adopt the perspective of Kim et al. (2025), viewing easy (e.g., simplifying a fraction) versus hard (e.g., discovering a symmetry) reasoning steps as low- versus high-probability Markov transitions, and formalize post-training dynamics through Multi-task Tree-structured Markov Chains (TMC). In this tractable model, pretraining corresponds to tree expansion, while post-training corresponds to chain-of-thought reweighting. We show that several phenomena recently observed in empirical studies arise naturally in this setting: (1) RLVR induces a squeezing effect, reducing reasoning entropy and forgetting some correct paths; (2) population rewards of ORM/PRM encourage consistency rather than accuracy, thereby favoring common patterns; and (3) certain rare, high-uncertainty reasoning paths by the base model are responsible for solving hard problem instances.
  Together, these explain why exploration -- even when confined to the base model's reasoning scope -- remains essential: it preserves access to rare but crucial reasoning traces needed for difficult cases, which are squeezed out by RLVR or unfavored by inference scaling. Building on this, we further show that exploration strategies such as rejecting easy instances and KL regularization help preserve rare reasoning traces. Empirical simulations corroborate our theoretical results.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Benefit of Curriculum in Transformer Tree-Reasoning Post-Training</title>
<link>https://arxiv.org/abs/2511.07372</link>
<guid>https://arxiv.org/abs/2511.07372</guid>
<content:encoded><![CDATA[

arXiv:2511.07372v1 Announce Type: new 
Abstract: Recent curriculum techniques in the post-training stage of LLMs have been widely observed to outperform non-curriculum approaches in enhancing reasoning performance, yet a principled understanding of why and to what extent they work remains elusive. To address this gap, we develop a theoretical framework grounded in the intuition that progressively learning through manageable steps is more efficient than directly tackling a hard reasoning task, provided each stage stays within the model's effective competence. Under mild complexity conditions linking consecutive curriculum stages, we show that curriculum post-training avoids the exponential complexity bottleneck.
  To substantiate this result, drawing insights from the Chain-of-Thoughts (CoTs) solving mathematical problems such as Countdown and parity, we model CoT generation as a states-conditioned autoregressive reasoning tree, define a uniform-branching base model to capture pretrained behavior, and formalize curriculum stages as either depth-increasing (longer reasoning chains) or hint-decreasing (shorter prefixes) subtasks. Our analysis shows that, under outcome-only reward signals, reinforcement learning finetuning achieves high accuracy with polynomial sample complexity, whereas direct learning suffers from an exponential bottleneck. We further establish analogous guarantees for test-time scaling, where curriculum-aware querying reduces both reward oracle calls and sampling cost from exponential to polynomial order.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformers Provably Learn Chain-of-Thought Reasoning with Length Generalization</title>
<link>https://arxiv.org/abs/2511.07378</link>
<guid>https://arxiv.org/abs/2511.07378</guid>
<content:encoded><![CDATA[

arXiv:2511.07378v1 Announce Type: new 
Abstract: The ability to reason lies at the core of artificial intelligence (AI), and challenging problems usually call for deeper and longer reasoning to tackle. A crucial question about AI reasoning is whether models can extrapolate learned reasoning patterns to solve harder tasks with longer chain-of-thought (CoT). In this work, we present a theoretical analysis of transformers learning on synthetic state-tracking tasks with gradient descent. We mathematically prove how the algebraic structure of state-tracking problems governs the degree of extrapolation of the learned CoT. Specifically, our theory characterizes the length generalization of transformers through the mechanism of attention concentration, linking the retrieval robustness of the attention layer to the state-tracking task structure of long-context reasoning. Moreover, for transformers with limited reasoning length, we prove that a recursive self-training scheme can progressively extend the range of solvable problem lengths. To our knowledge, we provide the first optimization guarantee that constant-depth transformers provably learn $\mathsf{NC}^1$-complete problems with CoT, significantly going beyond prior art confined in $\mathsf{TC}^0$, unless the widely held conjecture $\mathsf{TC}^0 \neq \mathsf{NC}^1$ fails. Finally, we present a broad set of experiments supporting our theoretical results, confirming the length generalization behaviors and the mechanism of attention concentration.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoReTTA: A Low Resource Framework To Poison Continuous Time Dynamic Graphs</title>
<link>https://arxiv.org/abs/2511.07379</link>
<guid>https://arxiv.org/abs/2511.07379</guid>
<content:encoded><![CDATA[

arXiv:2511.07379v1 Announce Type: new 
Abstract: Temporal Graph Neural Networks (TGNNs) are increasingly used in high-stakes domains, such as financial forecasting, recommendation systems, and fraud detection. However, their susceptibility to poisoning attacks poses a critical security risk. We introduce LoReTTA (Low Resource Two-phase Temporal Attack), a novel adversarial framework on Continuous-Time Dynamic Graphs, which degrades TGNN performance by an average of 29.47% across 4 widely benchmark datasets and 4 State-of-the-Art (SotA) models. LoReTTA operates through a two-stage approach: (1) sparsify the graph by removing high-impact edges using any of the 16 tested temporal importance metrics, (2) strategically replace removed edges with adversarial negatives via LoReTTA's novel degree-preserving negative sampling algorithm. Our plug-and-play design eliminates the need for expensive surrogate models while adhering to realistic unnoticeability constraints. LoReTTA degrades performance by upto 42.0% on MOOC, 31.5% on Wikipedia, 28.8% on UCI, and 15.6% on Enron. LoReTTA outperforms 11 attack baselines, remains undetectable to 4 leading anomaly detection systems, and is robust to 4 SotA adversarial defense training methods, establishing its effectiveness, unnoticeability, and robustness.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Diffusion Model to Shrink Proteins While Maintaining Their Function</title>
<link>https://arxiv.org/abs/2511.07390</link>
<guid>https://arxiv.org/abs/2511.07390</guid>
<content:encoded><![CDATA[

arXiv:2511.07390v1 Announce Type: new 
Abstract: Many proteins useful in modern medicine or bioengineering are challenging to make in the lab, fuse with other proteins in cells, or deliver to tissues in the body, because their sequences are too long. Shortening these sequences typically involves costly, time-consuming experimental campaigns. Ideally, we could instead use modern models of massive databases of sequences from nature to learn how to propose shrunken proteins that resemble sequences found in nature. Unfortunately, these models struggle to efficiently search the combinatorial space of all deletions, and are not trained with inductive biases to learn how to delete. To address this gap, we propose SCISOR, a novel discrete diffusion model that deletes letters from sequences to generate protein samples that resemble those found in nature. To do so, SCISOR trains a de-noiser to reverse a forward noising process that adds random insertions to natural sequences. As a generative model, SCISOR fits evolutionary sequence data competitively with previous large models. In evaluation, SCISOR achieves state-of-the-art predictions of the functional effects of deletions on ProteinGym. Finally, we use the SCISOR de-noiser to shrink long protein sequences, and show that its suggested deletions result in significantly more realistic proteins and more often preserve functional motifs than previous models of evolutionary sequences.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C3PO: Optimized Large Language Model Cascades with Probabilistic Cost Constraints for Reasoning</title>
<link>https://arxiv.org/abs/2511.07396</link>
<guid>https://arxiv.org/abs/2511.07396</guid>
<content:encoded><![CDATA[

arXiv:2511.07396v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive results on complex reasoning tasks, but their high inference cost remains a major barrier to real-world deployment. A promising solution is to use cascaded inference, where small, cheap models handle easy queries, and only the hardest examples are escalated to more powerful models. However, existing cascade methods typically rely on supervised training with labeled data, offer no theoretical generalization guarantees, and provide limited control over test-time computational cost. We introduce C3PO (Cost Controlled Cascaded Prediction Optimization), a self-supervised framework for optimizing LLM cascades under probabilistic cost constraints. By focusing on minimizing regret with respect to the most powerful model (MPM), C3PO avoids the need for labeled data by constructing a cascade using only unlabeled model outputs. It leverages conformal prediction to bound the probability that inference cost exceeds a user-specified budget. We provide theoretical guarantees on both cost control and generalization error, and show that our optimization procedure is effective even with small calibration sets. Empirically, C3PO achieves state-of-the-art performance across a diverse set of reasoning benchmarks including GSM8K, MATH-500, BigBench-Hard and AIME, outperforming strong LLM cascading baselines in both accuracy and cost-efficiency. Our results demonstrate that principled, label-free cascade optimization can enable scalable LLM deployment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entangled Schr\"odinger Bridge Matching</title>
<link>https://arxiv.org/abs/2511.07406</link>
<guid>https://arxiv.org/abs/2511.07406</guid>
<content:encoded><![CDATA[

arXiv:2511.07406v1 Announce Type: new 
Abstract: Simulating trajectories of multi-particle systems on complex energy landscapes is a central task in molecular dynamics (MD) and drug discovery, but remains challenging at scale due to computationally expensive and long simulations. Previous approaches leverage techniques such as flow or Schr\"odinger bridge matching to implicitly learn joint trajectories through data snapshots. However, many systems, including biomolecular systems and heterogeneous cell populations, undergo dynamic interactions that evolve over their trajectory and cannot be captured through static snapshots. To close this gap, we introduce Entangled Schr\"odinger Bridge Matching (EntangledSBM), a framework that learns the first- and second-order stochastic dynamics of interacting, multi-particle systems where the direction and magnitude of each particle's path depend dynamically on the paths of the other particles. We define the Entangled Schr\"odinger Bridge (EntangledSB) problem as solving a coupled system of bias forces that entangle particle velocities. We show that our framework accurately simulates heterogeneous cell populations under perturbations and rare transitions in high-dimensional biomolecular systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Routing Manifold Alignment Improves Generalization of Mixture-of-Experts LLMs</title>
<link>https://arxiv.org/abs/2511.07419</link>
<guid>https://arxiv.org/abs/2511.07419</guid>
<content:encoded><![CDATA[

arXiv:2511.07419v1 Announce Type: new 
Abstract: Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large language models since it can efficiently scale up the model capability without increasing the inference cost. However, evaluations on broad downstream tasks reveal a consistent suboptimality of the routers in existing MoE LLMs, which results in a severe performance gap (e.g., 10-20% in accuracy) to the optimal routing. In this paper, we show that aligning the manifold of routing weights with that of task embedding can effectively reduce the gap and improve MoE LLMs' generalization performance. Our method, "Routing Manifold Alignment (RoMA)", introduces an additional manifold regularization term in the post-training objective and only requires lightweight finetuning of routers (with other parameters frozen). Specifically, the regularization encourages the routing weights of each sample to be close to those of its successful neighbors (whose routing weights lead to correct answers) in a task embedding space. Consequently, samples targeting similar tasks will share similar expert choices across layers. Building such bindings between tasks and experts over different samples is essential to achieve better generalization. Moreover, RoMA demonstrates the advantage of unifying the task understanding (by embedding models) with solution generation (by MoE LLMs). In experiments, we finetune routers in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse benchmarks and extensive comparisons with baselines show the substantial improvement brought by RoMA.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Socially Aware Music Recommendation: A Multi-Modal Graph Neural Networks for Collaborative Music Consumption and Community-Based Engagement</title>
<link>https://arxiv.org/abs/2511.05497</link>
<guid>https://arxiv.org/abs/2511.05497</guid>
<content:encoded><![CDATA[

arXiv:2511.05497v1 Announce Type: cross 
Abstract: This study presents a novel Multi-Modal Graph Neural Network (MM-GNN) framework for socially aware music recommendation, designed to enhance personalization and foster community-based engagement. The proposed model introduces a fusion-free deep mutual learning strategy that aligns modality-specific representations from lyrics, audio, and visual data while maintaining robustness against missing modalities. A heterogeneous graph structure is constructed to capture both user-song interactions and user-user social relationships, enabling the integration of individual preferences with social influence. Furthermore, emotion-aware embeddings derived from acoustic and textual signals contribute to emotionally aligned recommendations. Experimental evaluations on benchmark datasets demonstrate that MM-GNN significantly outperforms existing state-of-the-art methods across various performance metrics. Ablation studies further validate the critical impact of each model component, confirming the effectiveness of the framework in delivering accurate and socially contextualized music recommendations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weightless Neural Networks for Continuously Trainable Personalized Recommendation Systems</title>
<link>https://arxiv.org/abs/2511.05499</link>
<guid>https://arxiv.org/abs/2511.05499</guid>
<content:encoded><![CDATA[

arXiv:2511.05499v1 Announce Type: cross 
Abstract: Given that conventional recommenders, while deeply effective, rely on large distributed systems pre-trained on aggregate user data, incorporating new data necessitates large training cycles, making them slow to adapt to real-time user feedback and often lacking transparency in recommendation rationale. We explore the performance of smaller personal models trained on per-user data using weightless neural networks (WNNs), an alternative to neural backpropagation that enable continuous learning by using neural networks as a state machine rather than a system with pretrained weights. We contrast our approach against a classic weighted system, also on a per-user level, and standard collaborative filtering, achieving competitive levels of accuracy on a subset of the MovieLens dataset. We close with a discussion of how weightless systems can be developed to augment centralized systems to achieve higher subjective accuracy through recommenders more directly tunable by end-users.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iEEG Seizure Detection with a Sparse Hyperdimensional Computing Accelerator</title>
<link>https://arxiv.org/abs/2511.05503</link>
<guid>https://arxiv.org/abs/2511.05503</guid>
<content:encoded><![CDATA[

arXiv:2511.05503v1 Announce Type: cross 
Abstract: Implantable devices for reliable intracranial electroencephalography (iEEG) require efficient, accurate, and real-time detection of seizures. Dense hyperdimensional computing (HDC) proves to be efficient over neural networks; however, it still consumes considerable switching power for an ultra-low energy application. Sparse HDC, on the other hand, has the potential of further reducing the energy consumption, yet at the expense of having to support more complex operations and introducing an extra hyperparameter, the maximum hypervector density. To improve the energy and area efficiency of the sparse HDC operations, this work introduces the compressed item memory (CompIM) and simplifies the spatial bundling. We also analyze how a proper hyperparameter choice improves the detection delay compared to dense HDC. Ultimately, our optimizations achieve a 1.73x more energy- and 2.20x more area-efficient hardware design than the naive sparse implementation. We are also 7.50x more energy- and 3.24x more area-efficient than the dense HDC implementation. This work highlights the hardware advantages of sparse HDC, demonstrating its potential to enable smaller brain implants with a substantially extended battery life compared to the current state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gravity-Awareness: Deep Learning Models and LLM Simulation of Human Awareness in Altered Gravity</title>
<link>https://arxiv.org/abs/2511.05536</link>
<guid>https://arxiv.org/abs/2511.05536</guid>
<content:encoded><![CDATA[

arXiv:2511.05536v1 Announce Type: cross 
Abstract: Earth's gravity has fundamentally shaped human development by guiding the brain's integration of vestibular, visual, and proprioceptive inputs into an internal model of gravity: a dynamic neural representation enabling prediction and interpretation of gravitational forces. This work presents a dual computational framework to quantitatively model these adaptations. The first component is a lightweight Multi-Layer Perceptron (MLP) that predicts g-load-dependent changes in key electroencephalographic (EEG) frequency bands, representing the brain's cortical state. The second component utilizes a suite of independent Gaussian Processes (GPs) to model the body's broader physiological state, including Heart Rate Variability (HRV), Electrodermal Activity (EDA), and motor behavior. Both models were trained on data derived from a comprehensive review of parabolic flight literature, using published findings as anchor points to construct robust, continuous functions. To complement this quantitative analysis, we simulated subjective human experience under different gravitational loads, ranging from microgravity (0g) and partial gravity (Moon 0.17g, Mars 0.38g) to hypergravity associated with spacecraft launch and re-entry (1.8g), using a large language model (Claude 3.5 Sonnet). The model was prompted with physiological parameters to generate introspective narratives of alertness and self-awareness, which closely aligned with the quantitative findings from both the EEG and physiological models. This combined framework integrates quantitative physiological modeling with generative cognitive simulation, offering a novel approach to understanding and predicting human performance in altered gravity
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection</title>
<link>https://arxiv.org/abs/2511.05537</link>
<guid>https://arxiv.org/abs/2511.05537</guid>
<content:encoded><![CDATA[

arXiv:2511.05537v1 Announce Type: cross 
Abstract: Depression is a major cause of global mental illness and significantly influences suicide rates. Timely and accurate diagnosis is essential for effective intervention. Electroencephalography (EEG) provides a non-invasive and accessible method for examining cerebral activity and identifying disease-associated patterns. We propose a novel graph-based deep learning framework, named Edge-gated, axis-mixed Pooling Attention Network (ExPANet), for differentiating major depressive disorder (MDD) patients from healthy controls (HC). EEG recordings undergo preprocessing to eliminate artifacts and are segmented into short periods of activity. We extract 14 features from each segment, which include time, frequency, fractal, and complexity domains. Electrodes are represented as nodes, whereas edges are determined by the phase-locking value (PLV) to represent functional connectivity. The generated brain graphs are examined utilizing an adapted graph attention network. This architecture acquires both localized electrode characteristics and comprehensive functional connectivity patterns. The proposed framework attains superior performance relative to current EEG-based approaches across two different datasets. A fundamental advantage of our methodology is its explainability. We evaluated the significance of features, channels, and edges, in addition to intrinsic attention weights. These studies highlight features, cerebral areas, and connectivity associations that are especially relevant to MDD, many of which correspond with clinical data. Our findings demonstrate a reliable and transparent method for EEG-based screening of MDD, using deep learning with clinically relevant results.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token Is All You Need: Cognitive Planning through Sparse Intent Alignment</title>
<link>https://arxiv.org/abs/2511.05540</link>
<guid>https://arxiv.org/abs/2511.05540</guid>
<content:encoded><![CDATA[

arXiv:2511.05540v1 Announce Type: cross 
Abstract: We challenge the long-standing assumption that exhaustive scene modeling is required for high-performance end-to-end autonomous driving (E2EAD). Unlike world-model approaches that rely on computationally intensive future scene generation or vision-language-action (VLA) systems constrained by Markov assumptions, we show that a minimal set of semantically rich tokens is sufficient for effective planning. Experiments on the nuPlan benchmark (720 scenarios, over 11,000 samples) using perception-informed BEV representations yield three key findings: (1) even without future prediction, our sparse representation achieves 0.548 m ADE, comparable to or surpassing prior methods reporting around 0.75 m on nuScenes; (2) conditioning trajectory decoding on predicted future tokens reduces ADE to 0.479 m, a 12.6% improvement over current-state baselines; and (3) explicit reconstruction loss offers no benefit and may degrade performance under reliable perception inputs. Notably, we observe the emergence of temporal fuzziness, where the model adaptively attends to task-relevant semantics rather than aligning rigidly to fixed timestamps, providing a cognitive advantage for planning under uncertainty. Our "token is all you need" principle marks a paradigm shift from reconstructing the world to understanding it, laying a foundation for cognitively inspired systems that plan through imagination rather than reaction.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability</title>
<link>https://arxiv.org/abs/2511.05541</link>
<guid>https://arxiv.org/abs/2511.05541</guid>
<content:encoded><![CDATA[

arXiv:2511.05541v1 Announce Type: cross 
Abstract: Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they suffer from a variety of problems, including a systematic failure to capture the rich conceptual information that drives linguistic understanding. Instead, they exhibit a bias towards shallow, token-specific, or noisy features, such as "the phrase 'The' at the start of sentences". In this work, we propose that this is due to a fundamental issue with how dictionary learning methods for LLMs are trained. Language itself has a rich, well-studied structure spanning syntax, semantics, and pragmatics; however, current unsupervised methods largely ignore this linguistic knowledge, leading to poor feature discovery that favors superficial patterns over meaningful concepts. We focus on a simple but important aspect of language: semantic content has long-range dependencies and tends to be smooth over a sequence, whereas syntactic information is much more local. Building on this insight, we introduce Temporal Sparse Autoencoders (T-SAEs), which incorporate a novel contrastive loss encouraging consistent activations of high-level features over adjacent tokens. This simple yet powerful modification enables SAEs to disentangle semantic from syntactic features in a self-supervised manner. Across multiple datasets and models, T-SAEs recover smoother, more coherent semantic concepts without sacrificing reconstruction quality. Strikingly, they exhibit clear semantic structure despite being trained without explicit semantic signal, offering a new pathway for unsupervised interpretability in language models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConnectomeBench: Can LLMs Proofread the Connectome?</title>
<link>https://arxiv.org/abs/2511.05542</link>
<guid>https://arxiv.org/abs/2511.05542</guid>
<content:encoded><![CDATA[

arXiv:2511.05542v1 Announce Type: cross 
Abstract: Connectomics - the mapping of neural connections in an organism's brain - currently requires extraordinary human effort to proofread the data collected from imaging and machine-learning assisted segmentation. With the growing excitement around using AI agents to automate important scientific tasks, we explore whether current AI systems can perform multiple tasks necessary for data proofreading. We introduce ConnectomeBench, a multimodal benchmark evaluating large language model (LLM) capabilities in three critical proofreading tasks: segment type identification, split error correction, and merge error detection. Using expert annotated data from two large open-source datasets - a cubic millimeter of mouse visual cortex and the complete Drosophila brain - we evaluate proprietary multimodal LLMs including Claude 3.7/4 Sonnet, o4-mini, GPT-4.1, GPT-4o, as well as open source models like InternVL-3 and NVLM. Our results demonstrate that current models achieve surprisingly high performance in segment identification (52-82% balanced accuracy vs. 20-25% chance) and binary/multiple choice split error correction (75-85% accuracy vs. 50% chance) while generally struggling on merge error identification tasks. Overall, while the best models still lag behind expert performance, they demonstrate promising capabilities that could eventually enable them to augment and potentially replace human proofreading in connectomics. Project page: https://github.com/jffbrwn2/ConnectomeBench and Dataset https://huggingface.co/datasets/jeffbbrown2/ConnectomeBench/tree/main
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Factual and Musical Evaluation Metrics for Music Language Models</title>
<link>https://arxiv.org/abs/2511.05550</link>
<guid>https://arxiv.org/abs/2511.05550</guid>
<content:encoded><![CDATA[

arXiv:2511.05550v1 Announce Type: cross 
Abstract: Music language models (Music LMs), like vision language models, leverage multimodal representations to answer natural language queries about musical audio recordings. Although Music LMs are reportedly improving, we find that current evaluations fail to capture whether their answers are correct. Specifically, for all Music LMs that we examine, widely-used evaluation metrics such as BLEU, METEOR, and BERTScore fail to measure anything beyond linguistic fluency of the model's responses. To measure the true performance of Music LMs, we propose (1) a better general-purpose evaluation metric for Music LMs adapted to the music domain and (2) a factual evaluation framework to quantify the correctness of a Music LM's responses. Our framework is agnostic to the modality of the question-answering model and could be generalized to quantify performance in other open-ended question-answering domains. We use open datasets in our experiments and will release all code on publication.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCFCN: Multi-View Clustering via a Fusion-Consensus Graph Convolutional Network</title>
<link>https://arxiv.org/abs/2511.05554</link>
<guid>https://arxiv.org/abs/2511.05554</guid>
<content:encoded><![CDATA[

arXiv:2511.05554v1 Announce Type: cross 
Abstract: Existing Multi-view Clustering (MVC) methods based on subspace learning focus on consensus representation learning while neglecting the inherent topological structure of data. Despite the integration of Graph Neural Networks (GNNs) into MVC, their input graph structures remain susceptible to noise interference. Methods based on Multi-view Graph Refinement (MGRC) also have limitations such as insufficient consideration of cross-view consistency, difficulty in handling hard-to-distinguish samples in the feature space, and disjointed optimization processes caused by graph construction algorithms. To address these issues, a Multi-View Clustering method via a Fusion-Consensus Graph Convolutional Network (MCFCN) is proposed. The network learns the consensus graph of multi-view data in an end-to-end manner and learns effective consensus representations through a view feature fusion model and a Unified Graph Structure Adapter (UGA). It designs Similarity Matrix Alignment Loss (SMAL) and Feature Representation Alignment Loss (FRAL). With the guidance of consensus, it optimizes view-specific graphs, preserves cross-view topological consistency, promotes the construction of intra-class edges, and realizes effective consensus representation learning with the help of GCN to improve clustering performance. MCFCN demonstrates state-of-the-art performance on eight multi-view benchmark datasets, and its effectiveness is verified by extensive qualitative and quantitative implementations. The code will be provided at https://github.com/texttao/MCFCN.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Extraction of Road Networks by using Teacher-Student Adaptive Structural Deep Belief Network and Its Application to Landslide Disaster</title>
<link>https://arxiv.org/abs/2511.05567</link>
<guid>https://arxiv.org/abs/2511.05567</guid>
<content:encoded><![CDATA[

arXiv:2511.05567v1 Announce Type: cross 
Abstract: An adaptive structural learning method of Restricted Boltzmann Machine (RBM) and Deep Belief Network (DBN) has been developed as one of prominent deep learning models. The neuron generation-annihilation algorithm in RBM and layer generation algorithm in DBN make an optimal network structure for given input during the learning. In this paper, our model is applied to an automatic recognition method of road network system, called RoadTracer. RoadTracer can generate a road map on the ground surface from aerial photograph data. A novel method of RoadTracer using the Teacher-Student based ensemble learning model of Adaptive DBN is proposed, since the road maps contain many complicated features so that a model with high representation power to detect should be required. The experimental results showed the detection accuracy of the proposed model was improved from 40.0\% to 89.0\% on average in the seven major cities among the test dataset. In addition, we challenged to apply our method to the detection of available roads when landslide by natural disaster is occurred, in order to rapidly obtain a way of transportation. For fast inference, a small size of the trained model was implemented on a small embedded edge device as lightweight deep learning. We reported the detection results for the satellite image before and after the rainfall disaster in Japan.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Street View Imagery and Public Participation GIS align: Comparative Analysis of Urban Attractiveness</title>
<link>https://arxiv.org/abs/2511.05570</link>
<guid>https://arxiv.org/abs/2511.05570</guid>
<content:encoded><![CDATA[

arXiv:2511.05570v1 Announce Type: cross 
Abstract: As digital tools increasingly shape spatial planning practices, understanding how different data sources reflect human experiences of urban environments is essential. Street View Imagery (SVI) and Public Participation GIS (PPGIS) represent two prominent approaches for capturing place-based perceptions that can support urban planning decisions, yet their comparability remains underexplored. This study investigates the alignment between SVI-based perceived attractiveness and residents' reported experiences gathered via a city-wide PPGIS survey in Helsinki, Finland. Using participant-rated SVI data and semantic image segmentation, we trained a machine learning model to predict perceived attractiveness based on visual features. We compared these predictions to PPGIS-identified locations marked as attractive or unattractive, calculating agreement using two sets of strict and moderate criteria. Our findings reveal only partial alignment between the two datasets. While agreement (with a moderate threshold) reached 67% for attractive and 77% for unattractive places, agreement (with a strict threshold) dropped to 27% and 29%, respectively. By analysing a range of contextual variables, including noise, traffic, population presence, and land use, we found that non-visual cues significantly contributed to mismatches. The model failed to account for experiential dimensions such as activity levels and environmental stressors that shape perceptions but are not visible in images. These results suggest that while SVI offers a scalable and visual proxy for urban perception, it cannot fully substitute the experiential richness captured through PPGIS. We argue that both methods are valuable but serve different purposes; therefore, a more integrated approach is needed to holistically capture how people perceive urban environments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Resolution: Multi - Scale Weather and Climate Data for Alpine Renewable Energy in the Digital Twin Era - First Evaluations and Recommendations</title>
<link>https://arxiv.org/abs/2511.05584</link>
<guid>https://arxiv.org/abs/2511.05584</guid>
<content:encoded><![CDATA[

arXiv:2511.05584v1 Announce Type: cross 
Abstract: When Austrian hydropower production plummeted by 44% in early 2025 due to reduced snowpack, it exposed a critical vulnerability: standard meteorological and climatological datasets systematically fail in mountain regions that hold untapped renewable potential. This perspectives paper evaluates emerging solutions to the Alpine energy-climate data gap, analyzing datasets from global reanalyses (ERA5, 31 km) to kilometre-scale Digital Twins (Climate DT, Extremes DT, 4.4 km), regional reanalyses (ARA, 2.5 km), and next-generation AI weather prediction models (AIFS, 31 km). The multi-resolution assessment reveals that no single dataset excels universally: coarse reanalyses provide essential climatologies but miss valley-scale processes, while Digital Twins resolve Alpine dynamics yet remain computationally demanding. Effective energy planning therefore requires strategic dataset combinations validated against energy-relevant indices such as population-weighted extremes, wind-gust return periods, and Alpine-adjusted storm thresholds. A key frontier is sub-hourly (10-15 min) temporal resolution to match grid-operation needs. Six evidence-based recommendations outline pathways for bridging spatial and temporal scales. As renewable deployment expands globally into complex terrain, the Alpine region offers transferable perspectives for tackling identical forecasting and climate analysis challenges in mountainous regions worldwide.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps</title>
<link>https://arxiv.org/abs/2511.05590</link>
<guid>https://arxiv.org/abs/2511.05590</guid>
<content:encoded><![CDATA[

arXiv:2511.05590v1 Announce Type: cross 
Abstract: Class Activation Mapping (CAM) and its extensions have become indispensable tools for visualizing the evidence behind deep network predictions. However, by relying on a final softmax classifier, these methods suffer from two fundamental distortions: additive logit shifts that arbitrarily bias importance scores, and sign collapse that conflates excitatory and inhibitory features. We propose a simple, architecture-agnostic dual-branch sigmoid head that decouples localization from classification. Given any pretrained model, we clone its classification head into a parallel branch ending in per-class sigmoid outputs, freeze the original softmax head, and fine-tune only the sigmoid branch with class-balanced binary supervision. At inference, softmax retains recognition accuracy, while class evidence maps are generated from the sigmoid branch -- preserving both magnitude and sign of feature contributions. Our method integrates seamlessly with most CAM variants and incurs negligible overhead. Extensive evaluations on fine-grained tasks (CUB-200-2011, Stanford Cars) and WSOL benchmarks (ImageNet-1K, OpenImages30K) show improved explanation fidelity and consistent Top-1 Localization gains -- without any drop in classification accuracy. Code is available at https://github.com/finallyupper/beyond-softmax.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Prompts to Power: Measuring the Energy Footprint of LLM Inference</title>
<link>https://arxiv.org/abs/2511.05597</link>
<guid>https://arxiv.org/abs/2511.05597</guid>
<content:encoded><![CDATA[

arXiv:2511.05597v1 Announce Type: cross 
Abstract: The rapid expansion of Large Language Models (LLMs) has introduced unprecedented energy demands, extending beyond training to large-scale inference workloads that often dominate total lifecycle consumption. Deploying these models requires energy-intensive GPU infrastructure, and in some cases has even prompted plans to power data centers with nuclear energy. Despite this growing relevance, systematic analyses of inference energy consumption remain limited. In this work, we present a large-scale measurement-based study comprising over 32,500 measurements across 21 GPU configurations and 155 model architectures, from small open-source models to frontier systems. Using the vLLM inference engine, we quantify energy usage at the prompt level and identify how architectural and operational factors shape energy demand. Building on these insights, we develop a predictive model that accurately estimates inference energy consumption across unseen architectures and hardware, and implement it as a browser extension to raise awareness of the environmental impact of generative AI.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Evaluates AI's Social Impacts? Mapping Coverage and Gaps in First and Third Party Evaluations</title>
<link>https://arxiv.org/abs/2511.05613</link>
<guid>https://arxiv.org/abs/2511.05613</guid>
<content:encoded><![CDATA[

arXiv:2511.05613v1 Announce Type: cross 
Abstract: Foundation models are increasingly central to high-stakes AI systems, and governance frameworks now depend on evaluations to assess their risks and capabilities. Although general capability evaluations are widespread, social impact assessments covering bias, fairness, privacy, environmental costs, and labor practices remain uneven across the AI ecosystem. To characterize this landscape, we conduct the first comprehensive analysis of both first-party and third-party social impact evaluation reporting across a wide range of model developers. Our study examines 186 first-party release reports and 183 post-release evaluation sources, and complements this quantitative analysis with interviews of model developers. We find a clear division of evaluation labor: first-party reporting is sparse, often superficial, and has declined over time in key areas such as environmental impact and bias, while third-party evaluators including academic researchers, nonprofits, and independent organizations provide broader and more rigorous coverage of bias, harmful content, and performance disparities. However, this complementarity has limits. Only model developers can authoritatively report on data provenance, content moderation labor, financial costs, and training infrastructure, yet interviews reveal that these disclosures are often deprioritized unless tied to product adoption or regulatory compliance. Our findings indicate that current evaluation practices leave major gaps in assessing AI's societal impacts, highlighting the urgent need for policies that promote developer transparency, strengthen independent evaluation ecosystems, and create shared infrastructure to aggregate and compare third-party evaluations in a consistent and accessible way.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounding Foundational Vision Models with 3D Human Poses for Robust Action Recognition</title>
<link>https://arxiv.org/abs/2511.05622</link>
<guid>https://arxiv.org/abs/2511.05622</guid>
<content:encoded><![CDATA[

arXiv:2511.05622v1 Announce Type: cross 
Abstract: For embodied agents to effectively understand and interact within the world around them, they require a nuanced comprehension of human actions grounded in physical space. Current action recognition models, often relying on RGB video, learn superficial correlations between patterns and action labels, so they struggle to capture underlying physical interaction dynamics and human poses in complex scenes. We propose a model architecture that grounds action recognition in physical space by fusing two powerful, complementary representations: V-JEPA 2's contextual, predictive world dynamics and CoMotion's explicit, occlusion-tolerant human pose data. Our model is validated on both the InHARD and UCF-19-Y-OCC benchmarks for general action recognition and high-occlusion action recognition, respectively. Our model outperforms three other baselines, especially within complex, occlusive scenes. Our findings emphasize a need for action recognition to be supported by spatial understanding instead of statistical pattern recognition.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Registration-Free Monitoring of Unstructured Point Cloud Data via Intrinsic Geometrical Properties</title>
<link>https://arxiv.org/abs/2511.05623</link>
<guid>https://arxiv.org/abs/2511.05623</guid>
<content:encoded><![CDATA[

arXiv:2511.05623v1 Announce Type: cross 
Abstract: Modern sensing technologies have enabled the collection of unstructured point cloud data (PCD) of varying sizes, which are used to monitor the geometric accuracy of 3D objects. PCD are widely applied in advanced manufacturing processes, including additive, subtractive, and hybrid manufacturing. To ensure the consistency of analysis and avoid false alarms, preprocessing steps such as registration and mesh reconstruction are commonly applied prior to monitoring. However, these steps are error-prone, time-consuming and may introduce artifacts, potentially affecting monitoring outcomes. In this paper, we present a novel registration-free approach for monitoring PCD of complex shapes, eliminating the need for both registration and mesh reconstruction. Our proposal consists of two alternative feature learning methods and a common monitoring scheme. Feature learning methods leverage intrinsic geometric properties of the shape, captured via the Laplacian and geodesic distances. In the monitoring scheme, thresholding techniques are used to further select intrinsic features most indicative of potential out-of-control conditions. Numerical experiments and case studies highlight the effectiveness of the proposed approach in identifying different types of defects.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Diversity and Quality through Base-Aligned Model Collaboration</title>
<link>https://arxiv.org/abs/2511.05650</link>
<guid>https://arxiv.org/abs/2511.05650</guid>
<content:encoded><![CDATA[

arXiv:2511.05650v1 Announce Type: cross 
Abstract: Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VMDT: Decoding the Trustworthiness of Video Foundation Models</title>
<link>https://arxiv.org/abs/2511.05682</link>
<guid>https://arxiv.org/abs/2511.05682</guid>
<content:encoded><![CDATA[

arXiv:2511.05682v1 Announce Type: cross 
Abstract: As foundation models become more sophisticated, ensuring their trustworthiness becomes increasingly critical; yet, unlike text and image, the video modality still lacks comprehensive trustworthiness benchmarks. We introduce VMDT (Video-Modal DecodingTrust), the first unified platform for evaluating text-to-video (T2V) and video-to-text (V2T) models across five key trustworthiness dimensions: safety, hallucination, fairness, privacy, and adversarial robustness. Through our extensive evaluation of 7 T2V models and 19 V2T models using VMDT, we uncover several significant insights. For instance, all open-source T2V models evaluated fail to recognize harmful queries and often generate harmful videos, while exhibiting higher levels of unfairness compared to image modality models. In V2T models, unfairness and privacy risks rise with scale, whereas hallucination and adversarial robustness improve -- though overall performance remains low. Uniquely, safety shows no correlation with model size, implying that factors other than scale govern current safety levels. Our findings highlight the urgent need for developing more robust and trustworthy video foundation models, and VMDT provides a systematic framework for measuring and tracking progress toward this goal. The code is available at https://sunblaze-ucb.github.io/VMDT-page/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Function Encoder-Based Differentiable Predictive Control</title>
<link>https://arxiv.org/abs/2511.05757</link>
<guid>https://arxiv.org/abs/2511.05757</guid>
<content:encoded><![CDATA[

arXiv:2511.05757v1 Announce Type: cross 
Abstract: We introduce a differentiable framework for zero-shot adaptive control over parametric families of nonlinear dynamical systems. Our approach integrates a function encoder-based neural ODE (FE-NODE) for modeling system dynamics with a differentiable predictive control (DPC) for offline self-supervised learning of explicit control policies. The FE-NODE captures nonlinear behaviors in state transitions and enables zero-shot adaptation to new systems without retraining, while the DPC efficiently learns control policies across system parameterizations, thus eliminating costly online optimization common in classical model predictive control. We demonstrate the efficiency, accuracy, and online adaptability of the proposed method across a range of nonlinear systems with varying parametric scenarios, highlighting its potential as a general-purpose tool for fast zero-shot adaptive control.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Generation: Complexity Barriers and Implications for Learning</title>
<link>https://arxiv.org/abs/2511.05759</link>
<guid>https://arxiv.org/abs/2511.05759</guid>
<content:encoded><![CDATA[

arXiv:2511.05759v1 Announce Type: cross 
Abstract: Kleinberg and Mullainathan showed that, in principle, language generation is always possible: with sufficiently many positive examples, a learner can eventually produce sentences indistinguishable from those of a target language. However, the existence of such a guarantee does not speak to its practical feasibility. In this work, we show that even for simple and well-studied language families -- such as regular and context-free languages -- the number of examples required for successful generation can be extraordinarily large, and in some cases not bounded by any computable function. These results reveal a substantial gap between theoretical possibility and efficient learnability. They suggest that explaining the empirical success of modern language models requires a refined perspective -- one that takes into account structural properties of natural language that make effective generation possible in practice.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sign language recognition from skeletal data using graph and recurrent neural networks</title>
<link>https://arxiv.org/abs/2511.05772</link>
<guid>https://arxiv.org/abs/2511.05772</guid>
<content:encoded><![CDATA[

arXiv:2511.05772v1 Announce Type: cross 
Abstract: This work presents an approach for recognizing isolated sign language gestures using skeleton-based pose data extracted from video sequences. A Graph-GRU temporal network is proposed to model both spatial and temporal dependencies between frames, enabling accurate classification. The model is trained and evaluated on the AUTSL (Ankara university Turkish sign language) dataset, achieving high accuracy. Experimental results demonstrate the effectiveness of integrating graph-based spatial representations with temporal modeling, providing a scalable framework for sign language recognition. The results of this approach highlight the potential of pose-driven methods for sign language understanding.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning</title>
<link>https://arxiv.org/abs/2511.05784</link>
<guid>https://arxiv.org/abs/2511.05784</guid>
<content:encoded><![CDATA[

arXiv:2511.05784v1 Announce Type: cross 
Abstract: Unlearning in Large Language Models (LLMs) is crucial for protecting private data and removing harmful knowledge. Most existing approaches rely on fine-tuning to balance unlearning efficiency with general language capabilities. However, these methods typically require training or access to retain data, which is often unavailable in real world scenarios. Although these methods can perform well when both forget and retain data are available, few works have demonstrated equivalent capability in more practical, data-limited scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes in-context chain-of-thought (CoT) instructions to guard deployed LLMs before inference. Instead of modifying the base model, DRAGON leverages the inherent instruction-following ability of LLMs and introduces a lightweight detection module to identify forget-worthy prompts without any retain data. These are then routed through a dedicated CoT guard model to enforce safe and accurate in-context intervention. To robustly evaluate unlearning performance, we introduce novel metrics for unlearning performance and the continual unlearning setting. Extensive experiments across three representative unlearning tasks validate the effectiveness of DRAGON, demonstrating its strong unlearning capability, scalability, and applicability in practical scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.05791</link>
<guid>https://arxiv.org/abs/2511.05791</guid>
<content:encoded><![CDATA[

arXiv:2511.05791v1 Announce Type: cross 
Abstract: Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large vision-language model to generate a goal image where a straight rod "impales" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting vision-language foundation models as powerful priors for robotic manipulation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis</title>
<link>https://arxiv.org/abs/2511.05810</link>
<guid>https://arxiv.org/abs/2511.05810</guid>
<content:encoded><![CDATA[

arXiv:2511.05810v1 Announce Type: cross 
Abstract: Building trustworthy clinical AI systems requires not only accurate predictions but also transparent, biologically grounded explanations. We present \texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian Process-based hierarchical model that infers cell-type-specific gene expression profiles from bulk and single-cell RNA-seq data while modeling biological uncertainty. These features, combined with regulatory priors from eQTL analysis, power a neural classifier that achieves high predictive performance in Alzheimer's Disease (AD) detection (88.0\% accuracy). To support human understanding and trust, we introduce an LLM-based reasoning module that translates model outputs into audience-specific diagnostic reports, grounded in clinical features, attribution signals, and domain knowledge. Human evaluations confirm that these reports are accurate, actionable, and appropriately tailored for both physicians and patients. Our findings show that LLMs, when deployed as post-hoc reasoners rather than end-to-end predictors, can serve as effective communicators within hybrid diagnostic pipelines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Diffusion Model Guidance through Calibration and Regularization</title>
<link>https://arxiv.org/abs/2511.05844</link>
<guid>https://arxiv.org/abs/2511.05844</guid>
<content:encoded><![CDATA[

arXiv:2511.05844v1 Announce Type: cross 
Abstract: Classifier-guided diffusion models have emerged as a powerful approach for conditional image generation, but they suffer from overconfident predictions during early denoising steps, causing the guidance gradient to vanish. This paper introduces two complementary contributions to address this issue. First, we propose a differentiable calibration objective based on the Smooth Expected Calibration Error (Smooth ECE), which improves classifier calibration with minimal fine-tuning and yields measurable improvements in Frechet Inception Distance (FID). Second, we develop enhanced sampling guidance methods that operate on off-the-shelf classifiers without requiring retraining. These include tilted sampling with batch-level reweighting, adaptive entropy-regularized sampling to preserve diversity, and a novel f-divergence-based sampling strategy that strengthens class-consistent guidance while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate that our divergence-regularized guidance achieves an FID of 2.13 using a ResNet-101 classifier, improving upon existing classifier-guided diffusion methods while requiring no diffusion model retraining. The results show that principled calibration and divergence-aware sampling provide practical and effective improvements for classifier-guided diffusion.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EGG-SR: Embedding Symbolic Equivalence into Symbolic Regression via Equality Graph</title>
<link>https://arxiv.org/abs/2511.05849</link>
<guid>https://arxiv.org/abs/2511.05849</guid>
<content:encoded><![CDATA[

arXiv:2511.05849v1 Announce Type: cross 
Abstract: Symbolic regression seeks to uncover physical laws from experimental data by searching for closed-form expressions, which is an important task in AI-driven scientific discovery. Yet the exponential growth of the search space of expression renders the task computationally challenging. A promising yet underexplored direction for reducing the effective search space and accelerating training lies in symbolic equivalence: many expressions, although syntactically different, define the same function -- for example, $\log(x_1^2x_2^3)$, $\log(x_1^2)+\log(x_2^3)$, and $2\log(x_1)+3\log(x_2)$. Existing algorithms treat such variants as distinct outputs, leading to redundant exploration and slow learning. We introduce EGG-SR, a unified framework that integrates equality graphs (e-graphs) into diverse symbolic regression algorithms, including Monte Carlo Tree Search (MCTS), deep reinforcement learning (DRL), and large language models (LLMs). EGG-SR compactly represents equivalent expressions through the proposed EGG module, enabling more efficient learning by: (1) pruning redundant subtree exploration in EGG-MCTS, (2) aggregating rewards across equivalence classes in EGG-DRL, and (3) enriching feedback prompts in EGG-LLM. Under mild assumptions, we show that embedding e-graphs tightens the regret bound of MCTS and reduces the variance of the DRL gradient estimator. Empirically, EGG-SR consistently enhances multiple baselines across challenging benchmarks, discovering equations with lower normalized mean squared error than state-of-the-art methods. Code implementation is available at: https://www.github.com/jiangnanhugo/egg-sr.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering</title>
<link>https://arxiv.org/abs/2511.05876</link>
<guid>https://arxiv.org/abs/2511.05876</guid>
<content:encoded><![CDATA[

arXiv:2511.05876v1 Announce Type: cross 
Abstract: In recent years, the advancement of Graph Neural Networks (GNNs) has significantly propelled progress in Multi-View Clustering (MVC). However, existing methods face the problem of coarse-grained graph fusion. Specifically, current approaches typically generate a separate graph structure for each view and then perform weighted fusion of graph structures at the view level, which is a relatively rough strategy. To address this limitation, we present a novel Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly consists of two modules. In particular, we propose an innovative Mixture of Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a Mixture-of-Experts network to implement fine-grained fusion of ego graphs at the sample level, rather than the conventional view-level fusion. Additionally, we present the Ego Graph Contrastive Learning (EGCL) module to align the fused representation with the view-specific representation. The EGCL module enhances the representation similarity of samples from the same cluster, not merely from the same sample, further boosting fine-grained graph representation. Extensive experiments demonstrate that MoEGCL achieves state-of-the-art results in deep multi-view clustering tasks. The source code is publicly available at https://github.com/HackerHyper/MoEGCL.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSGaze: Context-aware Social Gaze Prediction</title>
<link>https://arxiv.org/abs/2511.05955</link>
<guid>https://arxiv.org/abs/2511.05955</guid>
<content:encoded><![CDATA[

arXiv:2511.05955v1 Announce Type: cross 
Abstract: A person's gaze offers valuable insights into their focus of attention, level of social engagement, and confidence. In this work, we investigate how contextual cues combined with visual scene and facial information can be effectively utilized to predict and interpret social gaze patterns during conversational interactions. We introduce CSGaze, a context aware multimodal approach that leverages facial, scene information as complementary inputs to enhance social gaze pattern prediction from multi-person images. The model also incorporates a fine-grained attention mechanism centered on the principal speaker, which helps in better modeling social gaze dynamics. Experimental results show that CSGaze performs competitively with state-of-the-art methods on GP-Static, UCO-LAEO and AVA-LAEO. Our findings highlight the role of contextual cues in improving social gaze prediction. Additionally, we provide initial explainability through generated attention scores, offering insights into the model's decision-making process. We also demonstrate our model's generalizability by testing our model on open set datasets that demonstrating its robustness across diverse scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities</title>
<link>https://arxiv.org/abs/2511.05968</link>
<guid>https://arxiv.org/abs/2511.05968</guid>
<content:encoded><![CDATA[

arXiv:2511.05968v1 Announce Type: cross 
Abstract: The integration of medical images with clinical context is essential for generating accurate and clinically interpretable radiology reports. However, current automated methods often rely on resource-heavy Large Language Models (LLMs) or static knowledge graphs and struggle with two fundamental challenges in real-world clinical data: (1) missing modalities, such as incomplete clinical context , and (2) feature entanglement, where mixed modality-specific and shared information leads to suboptimal fusion and clinically unfaithful hallucinated findings. To address these challenges, we propose the DiA-gnostic VLVAE, which achieves robust radiology reporting through Disentangled Alignment. Our framework is designed to be resilient to missing modalities by disentangling shared and modality-specific features using a Mixture-of-Experts (MoE) based Vision-Language Variational Autoencoder (VLVAE). A constrained optimization objective enforces orthogonality and alignment between these latent representations to prevent suboptimal fusion. A compact LLaMA-X decoder then uses these disentangled representations to generate reports efficiently. On the IU X-Ray and MIMIC-CXR datasets, DiA has achieved competetive BLEU@4 scores of 0.266 and 0.134, respectively. Experimental results show that the proposed method significantly outperforms state-of-the-art models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Recognition of Cognitive Distortions in Natural Language Texts</title>
<link>https://arxiv.org/abs/2511.05969</link>
<guid>https://arxiv.org/abs/2511.05969</guid>
<content:encoded><![CDATA[

arXiv:2511.05969v1 Announce Type: cross 
Abstract: We propose a new approach to multi-factor classification of natural language texts based on weighted structured patterns such as N-grams, taking into account the heterarchical relationships between them, applied to solve such a socially impactful problem as the automation of detection of specific cognitive distortions in psychological care, relying on an interpretable, robust and transparent artificial intelligence model. The proposed recognition and learning algorithms improve the current state of the art in this field. The improvement is tested on two publicly available datasets, with significant improvements over literature-known F1 scores for the task, with optimal hyper-parameters determined, having code and models available for future use by the community.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey</title>
<link>https://arxiv.org/abs/2511.05982</link>
<guid>https://arxiv.org/abs/2511.05982</guid>
<content:encoded><![CDATA[

arXiv:2511.05982v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) are widely used in perception systems for safety-critical applications, such as autonomous driving and robotics. However, DNNs remain vulnerable to various safety concerns, including generalization errors, out-of-distribution (OOD) inputs, and adversarial attacks, which can lead to hazardous failures. This survey provides a comprehensive overview of runtime safety monitoring approaches, which operate in parallel to DNNs during inference to detect these safety concerns without modifying the DNN itself. We categorize existing methods into three main groups: Monitoring inputs, internal representations, and outputs. We analyze the state-of-the-art for each category, identify strengths and limitations, and map methods to the safety concerns they address. In addition, we highlight open challenges and future research directions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking of Clustering Validity Measures Revisited</title>
<link>https://arxiv.org/abs/2511.05983</link>
<guid>https://arxiv.org/abs/2511.05983</guid>
<content:encoded><![CDATA[

arXiv:2511.05983v1 Announce Type: cross 
Abstract: Validation plays a crucial role in the clustering process. Many different internal validity indexes exist for the purpose of determining the best clustering solution(s) from a given collection of candidates, e.g., as produced by different algorithms or different algorithm hyper-parameters. In this study, we present a comprehensive benchmark study of 26 internal validity indexes, which includes highly popular classic indexes as well as more recently developed ones. We adopted an enhanced revision of the methodology presented in Vendramin et al. (2010), developed here to address several shortcomings of this previous work. This overall new approach consists of three complementary custom-tailored evaluation sub-methodologies, each of which has been designed to assess specific aspects of an index's behaviour while preventing potential biases of the other sub-methodologies. Each sub-methodology features two complementary measures of performance, alongside mechanisms that allow for an in-depth investigation of more complex behaviours of the internal validity indexes under study. Additionally, a new collection of 16177 datasets has been produced, paired with eight widely-used clustering algorithms, for a wider applicability scope and representation of more diverse clustering scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning solutions of parameterized stiff ODEs using Gaussian processes</title>
<link>https://arxiv.org/abs/2511.05990</link>
<guid>https://arxiv.org/abs/2511.05990</guid>
<content:encoded><![CDATA[

arXiv:2511.05990v1 Announce Type: cross 
Abstract: Stiff ordinary differential equations (ODEs) play an important role in many scientific and engineering applications. Often, the dependence of the solution of the ODE on additional parameters is of interest, e.g.\ when dealing with uncertainty quantification or design optimization. Directly studying this dependence can quickly become too computationally expensive, such that cheaper surrogate models approximating the solution are of interest. One popular class of surrogate models are Gaussian processes (GPs). They perform well when approximating stationary functions, functions which have a similar level of variation along any given parameter direction, however solutions to stiff ODEs are often characterized by a mixture of regions of rapid and slow variation along the time axis and when dealing with such nonstationary functions, GP performance frequently degrades drastically. We therefore aim to reparameterize stiff ODE solutions based on the available data, to make them appear more stationary and hence recover good GP performance. This approach comes with minimal computational overhead and requires no internal changes to the GP implementation, as it can be seen as a separate preprocessing step. We illustrate the achieved benefits using multiple examples.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Entropy in Reinforcement Learning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.05993</link>
<guid>https://arxiv.org/abs/2511.05993</guid>
<content:encoded><![CDATA[

arXiv:2511.05993v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a predominant approach for enhancing the reasoning capabilities of large language models (LLMs). However, the entropy of LLMs usually collapses during RLVR training, causing premature convergence to suboptimal local minima and hinder further performance improvement. Although various approaches have been proposed to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains lacking. To address this gap, we conduct extensive experiments to investigate the entropy dynamics of LLMs trained with RLVR and analyze how model entropy correlates with response diversity, calibration, and performance across various benchmarks. Our findings reveal that the number of off-policy updates, the diversity of training data, and the clipping thresholds in the optimization objective are critical factors influencing the entropy of LLMs trained with RLVR. Moreover, we theoretically and empirically demonstrate that tokens with positive advantages are the primary contributors to entropy collapse, and that model entropy can be effectively regulated by adjusting the relative loss weights of tokens with positive and negative advantages during training.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Algorithmic Phase Transition in Symmetric Correlated Spiked Wigner Model</title>
<link>https://arxiv.org/abs/2511.06040</link>
<guid>https://arxiv.org/abs/2511.06040</guid>
<content:encoded><![CDATA[

arXiv:2511.06040v1 Announce Type: cross 
Abstract: We study the computational task of detecting and estimating correlated signals in a pair of spiked Wigner matrices. Our model consists of observations
  $$
  X = \tfrac{\lambda}{\sqrt{n}} xx^{\top} + W \,, \quad Y = \tfrac{\mu}{\sqrt{n}} yy^{\top} + Z \,.
  $$
  where $x,y \in \mathbb R^n$ are signal vectors with norm $\|x\|,\|y\| \approx\sqrt{n}$ and correlation $\langle x,y \rangle \approx \rho\|x\|\|y\|$, while $W,Z$ are independent Gaussian noise matrices. We propose an efficient algorithm that succeeds whenever $F(\lambda,\mu,\rho)>1$, where
  $$
  F(\lambda,\mu,\rho)=\max\Big\{ \lambda,\mu, \frac{ \lambda^2 \rho^2 }{ 1-\lambda^2+\lambda^2 \rho^2 } + \frac{ \mu^2 \rho^2 }{ 1-\mu^2+\mu^2 \rho^2 } \Big\} \,.
  $$
  Our result shows that an algorithm can leverage the correlation between the spikes to detect and estimate the signals even in regimes where efficiently recovering either $x$ from $X$ alone or $y$ from $Y$ alone is believed to be computationally infeasible.
  We complement our algorithmic result with evidence for a matching computational lower bound. In particular, we prove that when $F(\lambda,\mu,\rho)<1$, all algorithms based on {\em low-degree polynomials} fails to distinguish $(X,Y)$ with two independent Wigner matrices. This low-degree analysis strongly suggests that $F(\lambda,\mu,\rho)=1$ is the precise computation threshold for this problem.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts</title>
<link>https://arxiv.org/abs/2511.06048</link>
<guid>https://arxiv.org/abs/2511.06048</guid>
<content:encoded><![CDATA[

arXiv:2511.06048v1 Announce Type: cross 
Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering interpretable features in large language models (LLMs) through the sparse directions they learn. However, the sheer number of extracted directions makes comprehensive exploration intractable. While conventional embedding techniques such as UMAP can reveal global structure, they suffer from limitations including high-dimensional compression artifacts, overplotting, and misleading neighborhood distortions. In this work, we propose a focused exploration framework that prioritizes curated concepts and their corresponding SAE features over attempts to visualize all available features simultaneously. We present an interactive visualization system that combines topology-based visual encoding with dimensionality reduction to faithfully represent both local and global relationships among selected features. This hybrid approach enables users to investigate SAE behavior through targeted, interpretable subsets, facilitating deeper and more nuanced analysis of concept representation in latent space.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stemming Hallucination in Language Models Using a Licensing Oracle</title>
<link>https://arxiv.org/abs/2511.06073</link>
<guid>https://arxiv.org/abs/2511.06073</guid>
<content:encoded><![CDATA[

arXiv:2511.06073v1 Announce Type: cross 
Abstract: Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuonAll: Muon Variant for Efficient Finetuning of Large Language Models</title>
<link>https://arxiv.org/abs/2511.06086</link>
<guid>https://arxiv.org/abs/2511.06086</guid>
<content:encoded><![CDATA[

arXiv:2511.06086v1 Announce Type: cross 
Abstract: Muon optimizer has demonstrated robust results in pretraining of language models but its performance in finetuning of existing public pretrained models is not yet explored. Currently, Muon is used along with AdamW introducing a scope of improvement for adopting all parameters inside Muon. We introduce MuonAll, which incorporates all the parameters inside Muon by transforming into 2D matrices. We conduct extensive finetuning experiments across publicly available language models with model sizes upto half billion parameters. Muon and MuonAll perform at par with AdamW across major benchmarks, highlighting their effectiveness as alternative optimizers. We open-source the distributed implementations of Muon and MuonAll, available at https://github.com/Saurabh750/optimizer
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting Thermospheric Density with Transformers for Multi-Satellite Orbit Management</title>
<link>https://arxiv.org/abs/2511.06105</link>
<guid>https://arxiv.org/abs/2511.06105</guid>
<content:encoded><![CDATA[

arXiv:2511.06105v1 Announce Type: cross 
Abstract: Accurate thermospheric density prediction is crucial for reliable satellite operations in Low Earth Orbits, especially at high solar and geomagnetic activity. Physics-based models such as TIE-GCM offer high fidelity but are computationally expensive, while empirical models like NRLMSIS are efficient yet lack predictive power. This work presents a transformer-based model that forecasts densities up to three days ahead and is intended as a drop-in replacement for an empirical baseline. Unlike recent approaches, it avoids spatial reduction and complex input pipelines, operating directly on a compact input set. Validated on real-world data, the model improves key prediction metrics and shows potential to support mission planning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Deep Learning Model for Predicting Transformation Legality</title>
<link>https://arxiv.org/abs/2511.06120</link>
<guid>https://arxiv.org/abs/2511.06120</guid>
<content:encoded><![CDATA[

arXiv:2511.06120v1 Announce Type: cross 
Abstract: Compilers must check the legality of code transformations to guarantee the correctness of applying a sequence of code transformations to a given code. While such a legality check needs to be precisely computed in general, we can use an approximate legality prediction model in certain cases, such as training a reinforcement learning (RL) agent for schedule prediction. In this paper, we propose an approximate method for legality checks. We propose a novel DL model for predicting the legality of transformations. The model takes the code representation and a list of transformations as input and predicts whether applying those transformations to the code is legal. We implement and evaluate the proposed model, demonstrating its effectiveness. Our evaluation shows an F1 score of 0.91 on a test set of randomly generated programs. To further evaluate the model in a practical scenario, we used the model to replace the legality check used during the training of an RL agent designed for automatic code optimization. We demonstrate that such a replacement enables the agent to train on twice as many steps, resulting in faster training and reducing resource usage by approximately 80\% for CPU and 35\% for RAM. The agent trained using this approach maintains comparable performance, with only a 4\% reduction on benchmarks from the Polybench suite compared to the traditional method.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Modal Fine-Tuning of 3D Convolutional Foundation Models for ADHD Classification with Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2511.06163</link>
<guid>https://arxiv.org/abs/2511.06163</guid>
<content:encoded><![CDATA[

arXiv:2511.06163v1 Announce Type: cross 
Abstract: Early diagnosis of attention-deficit/hyperactivity disorder (ADHD) in children plays a crucial role in improving outcomes in education and mental health. Diagnosing ADHD using neuroimaging data, however, remains challenging due to heterogeneous presentations and overlapping symptoms with other conditions. To address this, we propose a novel parameter-efficient transfer learning approach that adapts a large-scale 3D convolutional foundation model, pre-trained on CT images, to an MRI-based ADHD classification task. Our method introduces Low-Rank Adaptation (LoRA) in 3D by factorizing 3D convolutional kernels into 2D low-rank updates, dramatically reducing trainable parameters while achieving superior performance. In a five-fold cross-validated evaluation on a public diffusion MRI database, our 3D LoRA fine-tuning strategy achieved state-of-the-art results, with one model variant reaching 71.9% accuracy and another attaining an AUC of 0.716. Both variants use only 1.64 million trainable parameters (over 113x fewer than a fully fine-tuned foundation model). Our results represent one of the first successful cross-modal (CT-to-MRI) adaptations of a foundation model in neuroimaging, establishing a new benchmark for ADHD classification while greatly improving efficiency.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based Attribution Fingerprinting</title>
<link>https://arxiv.org/abs/2511.06197</link>
<guid>https://arxiv.org/abs/2511.06197</guid>
<content:encoded><![CDATA[

arXiv:2511.06197v1 Announce Type: cross 
Abstract: The rapid proliferation of Internet of Things (IoT) devices has transformed numerous industries by enabling seamless connectivity and data-driven automation. However, this expansion has also exposed IoT networks to increasingly sophisticated security threats, including adversarial attacks targeting artificial intelligence (AI) and machine learning (ML)-based intrusion detection systems (IDS) to deliberately evade detection, induce misclassification, and systematically undermine the reliability and integrity of security defenses. To address these challenges, we propose a novel adversarial detection model that enhances the robustness of IoT IDS against adversarial attacks through SHapley Additive exPlanations (SHAP)-based fingerprinting. Using SHAP's DeepExplainer, we extract attribution fingerprints from network traffic features, enabling the IDS to reliably distinguish between clean and adversarially perturbed inputs. By capturing subtle attribution patterns, the model becomes more resilient to evasion attempts and adversarial manipulations. We evaluated the model on a standard IoT benchmark dataset, where it significantly outperformed a state-of-the-art method in detecting adversarial attacks. In addition to enhanced robustness, this approach improves model transparency and interpretability, thereby increasing trust in the IDS through explainable AI.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time Matters: A Novel Real-Time Long- and Short-term User Interest Model for Click-Through Rate Prediction</title>
<link>https://arxiv.org/abs/2511.06213</link>
<guid>https://arxiv.org/abs/2511.06213</guid>
<content:encoded><![CDATA[

arXiv:2511.06213v1 Announce Type: cross 
Abstract: Click-Through Rate (CTR) prediction is a core task in online personalization platform. A key step for CTR prediction is to learn accurate user representation to capture their interests. Generally, the interest expressed by a user is time-variant, i.e., a user activates different interests at different time. However, most previous CTR prediction methods overlook the correlation between the activated interest and the occurrence time, resulting in what they actually learn is the mixture of the interests expressed by the user at all time, rather than the real-time interest at the certain prediction time. To capture the correlation between the activated interest and the occurrence time, in this paper we investigate users' interest evolution from the perspective of the whole time line and develop two regular patterns: periodic pattern and time-point pattern. Based on the two patterns, we propose a novel time-aware long- and short-term user interest modeling method to model users' dynamic interests at different time. Extensive experiments on public datasets as well as an industrial dataset verify the effectiveness of exploiting the two patterns and demonstrate the superiority of our proposed method compared with other state-of-the-art ones.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparsity via Hyperpriors: A Theoretical and Algorithmic Study under Empirical Bayes Framework</title>
<link>https://arxiv.org/abs/2511.06235</link>
<guid>https://arxiv.org/abs/2511.06235</guid>
<content:encoded><![CDATA[

arXiv:2511.06235v1 Announce Type: cross 
Abstract: This paper presents a comprehensive analysis of hyperparameter estimation within the empirical Bayes framework (EBF) for sparse learning. By studying the influence of hyperpriors on the solution of EBF, we establish a theoretical connection between the choice of the hyperprior and the sparsity as well as the local optimality of the resulting solutions. We show that some strictly increasing hyperpriors, such as half-Laplace and half-generalized Gaussian with the power in $(0,1)$, effectively promote sparsity and improve solution stability with respect to measurement noise. Based on this analysis, we adopt a proximal alternating linearized minimization (PALM) algorithm with convergence guaranties for both convex and concave hyperpriors. Extensive numerical tests on two-dimensional image deblurring problems demonstrate that introducing appropriate hyperpriors significantly promotes the sparsity of the solution and enhances restoration accuracy. Furthermore, we illustrate the influence of the noise level and the ill-posedness of inverse problems to EBF solutions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces</title>
<link>https://arxiv.org/abs/2511.06239</link>
<guid>https://arxiv.org/abs/2511.06239</guid>
<content:encoded><![CDATA[

arXiv:2511.06239v1 Announce Type: cross 
Abstract: Learning-based methods for sampling from the Gibbs distribution in finite-dimensional spaces have progressed quickly, yet theory and algorithmic design for infinite-dimensional function spaces remain limited. This gap persists despite their strong potential for sampling the paths of conditional diffusion processes, enabling efficient simulation of trajectories of diffusion processes that respect rare events or boundary constraints. In this work, we present the adjoint sampler for infinite-dimensional function spaces, a stochastic optimal control-based diffusion sampler that operates in function space and targets Gibbs-type distributions on infinite-dimensional Hilbert spaces. Our Functional Adjoint Sampler (FAS) generalizes Adjoint Sampling (Havens et al., 2025) to Hilbert spaces based on a SOC theory called stochastic maximum principle, yielding a simple and scalable matching-type objective for a functional representation. We show that FAS achieves superior transition path sampling performance across synthetic potential and real molecular systems, including Alanine Dipeptide and Chignolin.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Setting $\varepsilon$ is not the Issue in Differential Privacy</title>
<link>https://arxiv.org/abs/2511.06305</link>
<guid>https://arxiv.org/abs/2511.06305</guid>
<content:encoded><![CDATA[

arXiv:2511.06305v1 Announce Type: cross 
Abstract: This position paper argues that setting the privacy budget in differential privacy should not be viewed as an important limitation of differential privacy compared to alternative methods for privacy-preserving machine learning. The so-called problem of interpreting the privacy budget is often presented as a major hindrance to the wider adoption of differential privacy in real-world deployments and is sometimes used to promote alternative mitigation techniques for data protection. We believe this misleads decision-makers into choosing unsafe methods. We argue that the difficulty in interpreting privacy budgets does not stem from the definition of differential privacy itself, but from the intrinsic difficulty of estimating privacy risks in context, a challenge that any rigorous method for privacy risk assessment face. Moreover, we claim that any sound method for estimating privacy risks should, given the current state of research, be expressible within the differential privacy framework or justify why it cannot.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Precision-Scalable Microscaling Datapaths with Optimized Reduction Tree for Efficient NPU Integration</title>
<link>https://arxiv.org/abs/2511.06313</link>
<guid>https://arxiv.org/abs/2511.06313</guid>
<content:encoded><![CDATA[

arXiv:2511.06313v1 Announce Type: cross 
Abstract: Emerging continual learning applications necessitate next-generation neural processing unit (NPU) platforms to support both training and inference operations. The promising Microscaling (MX) standard enables narrow bit-widths for inference and large dynamic ranges for training. However, existing MX multiply-accumulate (MAC) designs face a critical trade-off: integer accumulation requires expensive conversions from narrow floating-point products, while FP32 accumulation suffers from quantization losses and costly normalization. To address these limitations, we propose a hybrid precision-scalable reduction tree for MX MACs that combines the benefits of both approaches, enabling efficient mixed-precision accumulation with controlled accuracy relaxation. Moreover, we integrate an 8x8 array of these MACs into the state-of-the-art (SotA) NPU integration platform, SNAX, to provide efficient control and data transfer to our optimized precision-scalable MX datapath. We evaluate our design both on MAC and system level and compare it to the SotA. Our integrated system achieves an energy efficiency of 657, 1438-1675, and 4065 GOPS/W, respectively, for MXINT8, MXFP8/6, and MXFP4, with a throughput of 64, 256, and 512 GOPS.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models</title>
<link>https://arxiv.org/abs/2511.06380</link>
<guid>https://arxiv.org/abs/2511.06380</guid>
<content:encoded><![CDATA[

arXiv:2511.06380v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of reasoning tasks. Recent methods have further improved LLM performance in complex mathematical reasoning. However, when extending these methods beyond the domain of mathematical reasoning to tasks involving complex domain-specific knowledge, we observe a consistent failure of LLMs to generate novel insights during the reflection stage. Instead of conducting genuine cognitive refinement, the model tends to mechanically reiterate earlier reasoning steps without introducing new information or perspectives, a phenomenon referred to as "Echo Reflection". We attribute this behavior to two key defects: (1) Uncontrollable information flow during response generation, which allows premature intermediate thoughts to propagate unchecked and distort final decisions; (2) Insufficient exploration of internal knowledge during reflection, leading to repeating earlier findings rather than generating new cognitive insights. Building on these findings, we proposed a novel reinforcement learning method termed Adaptive Entropy Policy Optimization (AEPO). Specifically, the AEPO framework consists of two major components: (1) Reflection-aware Information Filtration, which quantifies the cognitive information flow and prevents the final answer from being affected by earlier bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically balances exploration and exploitation across different reasoning stages, promoting both reflective diversity and answer correctness. Extensive experiments demonstrate that AEPO consistently achieves state-of-the-art performance over mainstream reinforcement learning baselines across diverse benchmarks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning the Inverse Ryu--Takayanagi Formula with Transformers</title>
<link>https://arxiv.org/abs/2511.06387</link>
<guid>https://arxiv.org/abs/2511.06387</guid>
<content:encoded><![CDATA[

arXiv:2511.06387v1 Announce Type: cross 
Abstract: We study the inverse problem of holographic entanglement entropy in AdS$_3$ using a data-driven generative model. Training data consist of randomly generated geometries and their holographic entanglement entropies using the Ryu--Takayanagi formula. After training, the Transformer reconstructs the blackening function within our metric ansatz from previously unseen inputs. The Transformer achieves accurate reconstructions on smooth black hole geometries and extrapolates to horizonless backgrounds. We describe the architecture and data generation process, and we quantify accuracy on both $f(z)$ and the reconstructed $S(\ell)$. Code and evaluation scripts are available at the provided repository.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Riemannian-manifold Hamiltonian Monte Carlo for hierarchical Gaussian-process models</title>
<link>https://arxiv.org/abs/2511.06407</link>
<guid>https://arxiv.org/abs/2511.06407</guid>
<content:encoded><![CDATA[

arXiv:2511.06407v1 Announce Type: cross 
Abstract: Hierarchical Bayesian models based on Gaussian processes are considered useful for describing complex nonlinear statistical dependencies among variables in real-world data. However, effective Monte Carlo algorithms for inference with these models have not yet been established, except for several simple cases. In this study, we show that, compared with the slow inference achieved with existing program libraries, the performance of Riemannian-manifold Hamiltonian Monte Carlo (RMHMC) can be drastically improved by optimising the computation order according to the model structure and dynamically programming the eigendecomposition. This improvement cannot be achieved when using an existing library based on a naive automatic differentiator. We numerically demonstrate that RMHMC effectively samples from the posterior, allowing the calculation of model evidence, in a Bayesian logistic regression on simulated data and in the estimation of propensity functions for the American national medical expenditure data using several Bayesian multiple-kernel models. These results lay a foundation for implementing effective Monte Carlo algorithms for analysing real-world data with Gaussian processes, and highlight the need to develop a customisable library set that allows users to incorporate dynamically programmed objects and finely optimises the mode of automatic differentiation depending on the model structure.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization</title>
<link>https://arxiv.org/abs/2511.06411</link>
<guid>https://arxiv.org/abs/2511.06411</guid>
<content:encoded><![CDATA[

arXiv:2511.06411v1 Announce Type: cross 
Abstract: The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Negative Stiefel Approximating Flow: Orthogonalish Matrix Optimization for Interpretable Embeddings</title>
<link>https://arxiv.org/abs/2511.06425</link>
<guid>https://arxiv.org/abs/2511.06425</guid>
<content:encoded><![CDATA[

arXiv:2511.06425v1 Announce Type: cross 
Abstract: Interpretable representation learning is a central challenge in modern machine learning, particularly in high-dimensional settings such as neuroimaging, genomics, and text analysis. Current methods often struggle to balance the competing demands of interpretability and model flexibility, limiting their effectiveness in extracting meaningful insights from complex data. We introduce Non-negative Stiefel Approximating Flow (NSA-Flow), a general-purpose matrix estimation framework that unifies ideas from sparse matrix factorization, orthogonalization, and constrained manifold learning. NSA-Flow enforces structured sparsity through a continuous balance between reconstruction fidelity and column-wise decorrelation, parameterized by a single tunable weight. The method operates as a smooth flow near the Stiefel manifold with proximal updates for non-negativity and adaptive gradient control, yielding representations that are simultaneously sparse, stable, and interpretable. Unlike classical regularization schemes, NSA-Flow provides an intuitive geometric mechanism for manipulating sparsity at the level of global structure while simplifying latent features. We demonstrate that the NSA-Flow objective can be optimized smoothly and integrates seamlessly with existing pipelines for dimensionality reduction while improving interpretability and generalization in both simulated and real biomedical data. Empirical validation on the Golub leukemia dataset and in Alzheimer's disease demonstrate that the NSA-Flow constraints can maintain or improve performance over related methods with little additional methodological effort. NSA-Flow offers a scalable, general-purpose tool for interpretable ML, applicable across data science domains.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Chain-of-Thought Confidence via Topological and Dirichlet Risk Analysis</title>
<link>https://arxiv.org/abs/2511.06437</link>
<guid>https://arxiv.org/abs/2511.06437</guid>
<content:encoded><![CDATA[

arXiv:2511.06437v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) prompting enables Large Language Models to solve complex problems, but deploying these models safely requires reliable confidence estimates, a capability where existing methods suffer from poor calibration and severe overconfidence on incorrect predictions. We propose Enhanced Dirichlet and Topology Risk (EDTR), a novel decoding strategy that combines topological analysis with Dirichlet-based uncertainty quantification to measure LLM confidence across multiple reasoning paths. EDTR treats each CoT as a vector in high-dimensional space and extracts eight topological risk features capturing the geometric structure of reasoning distributions: tighter, more coherent clusters indicate higher confidence while dispersed, inconsistent paths signal uncertainty. We evaluate EDTR against three state-of-the-art calibration methods across four diverse reasoning benchmarks spanning olympiad-level mathematics (AIME), grade school math (GSM8K), commonsense reasoning, and stock price prediction \cite{zhang2025aime, cobbe2021training, talmor-etal-2019-commonsenseqa, yahoo_finance}. EDTR achieves 41\% better calibration than competing methods with an average ECE of 0.287 and the best overall composite score of 0.672, while notably achieving perfect accuracy on AIME and exceptional calibration on GSM8K with an ECE of 0.107, domains where baselines exhibit severe overconfidence. Our work provides a geometric framework for understanding and quantifying uncertainty in multi-step LLM reasoning, enabling more reliable deployment where calibrated confidence estimates are essential.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models</title>
<link>https://arxiv.org/abs/2511.06441</link>
<guid>https://arxiv.org/abs/2511.06441</guid>
<content:encoded><![CDATA[

arXiv:2511.06441v1 Announce Type: cross 
Abstract: As AI moves beyond text, large language models (LLMs) increasingly power vision, audio, and document understanding; however, their high inference costs hinder real-time, scalable deployment. Conversely, smaller open-source models offer cost advantages but struggle with complex or multimodal queries. We introduce a unified, modular framework that intelligently routes each query - textual, multimodal, or complex - to the most fitting expert model, using a learned routing network that balances cost and quality. For vision tasks, we employ a two-stage open-source pipeline optimized for efficiency and reviving efficient classical vision components where they remain SOTA for sub-tasks. On benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual Question Answering (VQA), we match or exceed the performance of always-premium LLM (monolithic systems with one model serving all query types) performance, yet reduce the reliance on costly models by over 67%. With its extensible, multi-agent orchestration, we deliver high-quality, resource-efficient AI at scale.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Countering Multi-modal Representation Collapse through Rank-targeted Fusion</title>
<link>https://arxiv.org/abs/2511.06450</link>
<guid>https://arxiv.org/abs/2511.06450</guid>
<content:encoded><![CDATA[

arXiv:2511.06450v1 Announce Type: cross 
Abstract: Multi-modal fusion methods often suffer from two types of representation collapse: feature collapse where individual dimensions lose their discriminative power (as measured by eigenspectra), and modality collapse where one dominant modality overwhelms the other. Applications like human action anticipation that require fusing multifarious sensor data are hindered by both feature and modality collapse. However, existing methods attempt to counter feature collapse and modality collapse separately. This is because there is no unifying framework that efficiently addresses feature and modality collapse in conjunction. In this paper, we posit the utility of effective rank as an informative measure that can be utilized to quantify and counter both the representation collapses. We propose \textit{Rank-enhancing Token Fuser}, a theoretically grounded fusion framework that selectively blends less informative features from one modality with complementary features from another modality. We show that our method increases the effective rank of the fused representation. To address modality collapse, we evaluate modality combinations that mutually increase each others' effective rank. We show that depth maintains representational balance when fused with RGB, avoiding modality collapse. We validate our method on action anticipation, where we present \texttt{R3D}, a depth-informed fusion framework. Extensive experiments on NTURGBD, UTKinect, and DARai demonstrate that our approach significantly outperforms prior state-of-the-art methods by up to 3.74\%. Our code is available at: \href{https://github.com/olivesgatech/R3D}{https://github.com/olivesgatech/R3D}.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EchoMark: Perceptual Acoustic Environment Transfer with Watermark-Embedded Room Impulse Response</title>
<link>https://arxiv.org/abs/2511.06458</link>
<guid>https://arxiv.org/abs/2511.06458</guid>
<content:encoded><![CDATA[

arXiv:2511.06458v1 Announce Type: cross 
Abstract: Acoustic Environment Matching (AEM) is the task of transferring clean audio into a target acoustic environment, enabling engaging applications such as audio dubbing and auditory immersive virtual reality (VR). Recovering similar room impulse response (RIR) directly from reverberant speech offers more accessible and flexible AEM solution. However, this capability also introduces vulnerabilities of arbitrary ``relocation" if misused by malicious user, such as facilitating advanced voice spoofing attacks or undermining the authenticity of recorded evidence. To address this issue, we propose EchoMark, the first deep learning-based AEM framework that generates perceptually similar RIRs with embedded watermark. Our design tackle the challenges posed by variable RIR characteristics, such as different durations and energy decays, by operating in the latent domain. By jointly optimizing the model with a perceptual loss for RIR reconstruction and a loss for watermark detection, EchoMark achieves both high-quality environment transfer and reliable watermark recovery. Experiments on diverse datasets validate that EchoMark achieves room acoustic parameter matching performance comparable to FiNS, the state-of-the-art RIR estimator. Furthermore, a high Mean Opinion Score (MOS) of 4.22 out of 5, watermark detection accuracy exceeding 99\%, and bit error rates (BER) below 0.3\% collectively demonstrate the effectiveness of EchoMark in preserving perceptual quality while ensuring reliable watermark embedding.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-Inspired Planning for Better Generalization in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.06470</link>
<guid>https://arxiv.org/abs/2511.06470</guid>
<content:encoded><![CDATA[

arXiv:2511.06470v1 Announce Type: cross 
Abstract: Existing Reinforcement Learning (RL) systems encounter significant challenges when applied to real-world scenarios, primarily due to poor generalization across environments that differ from their training conditions. This thesis explores the direction of enhancing agents' zero-shot systematic generalization abilities by granting RL agents reasoning behaviors that are found to help systematic generalization in the human brain. Inspired by human conscious planning behaviors, we first introduced a top-down attention mechanism, which allows a decision-time planning agent to dynamically focus its reasoning on the most relevant aspects of the environmental state given its instantaneous intentions, a process we call "spatial abstraction". This approach significantly improves systematic generalization outside the training tasks. Subsequently, building on spatial abstraction, we developed the Skipper framework to automatically decompose complex tasks into simpler, more manageable sub-tasks. Skipper provides robustness against distributional shifts and efficacy in long-term, compositional planning by focusing on pertinent spatial and temporal elements of the environment. Finally, we identified a common failure mode and safety risk in planning agents that rely on generative models to generate state targets during planning. It is revealed that most agents blindly trust the targets they hallucinate, resulting in delusional planning behaviors. Inspired by how the human brain rejects delusional intentions, we propose learning a feasibility evaluator to enable rejecting hallucinated infeasible targets, which led to significant performance improvements in various kinds of planning agents. Finally, we suggest directions for future research, aimed at achieving general task abstraction and fully enabling abstract planning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Theory and Practice: A Stochastic Learning-Optimization Model for Resilient Automotive Supply Chains</title>
<link>https://arxiv.org/abs/2511.06479</link>
<guid>https://arxiv.org/abs/2511.06479</guid>
<content:encoded><![CDATA[

arXiv:2511.06479v1 Announce Type: cross 
Abstract: Supply chain disruptions and volatile demand pose significant challenges to the UK automotive industry, which relies heavily on Just-In-Time (JIT) manufacturing. While qualitative studies highlight the potential of integrating Artificial Intelligence (AI) with traditional optimization, a formal, quantitative demonstration of this synergy is lacking. This paper introduces a novel stochastic learning-optimization framework that integrates Bayesian inference with inventory optimization for supply chain management (SCM). We model a two-echelon inventory system subject to stochastic demand and supply disruptions, comparing a traditional static optimization policy against an adaptive policy where Bayesian learning continuously updates parameter estimates to inform stochastic optimization. Our simulations over 365 periods across three operational scenarios demonstrate that the integrated approach achieves 7.4\% cost reduction in stable environments and 5.7\% improvement during supply disruptions, while revealing important limitations during sudden demand shocks due to the inherent conservatism of Bayesian updating. This work provides mathematical validation for practitioner observations and establishes a formal framework for understanding AI-driven supply chain resilience, while identifying critical boundary conditions for successful implementation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EASE: Practical and Efficient Safety Alignment for Small Language Models</title>
<link>https://arxiv.org/abs/2511.06512</link>
<guid>https://arxiv.org/abs/2511.06512</guid>
<content:encoded><![CDATA[

arXiv:2511.06512v1 Announce Type: cross 
Abstract: Small language models (SLMs) are increasingly deployed on edge devices, making their safety alignment crucial yet challenging. Current shallow alignment methods that rely on direct refusal of malicious queries fail to provide robust protection, particularly against adversarial jailbreaks. While deliberative safety reasoning alignment offers deeper alignment for defending against sophisticated attacks, effectively implanting such reasoning capability in SLMs with limited capabilities remains an open challenge. Moreover, safety reasoning incurs significant computational overhead as models apply reasoning to nearly all queries, making it impractical for resource-constrained edge deployment scenarios that demand rapid responses. We propose EASE, a novel framework that enables practical and Efficient safety Alignment for Small languagE models. Our approach first identifies the optimal safety reasoning teacher that can effectively distill safety reasoning capabilities to SLMs. We then align models to selectively activate safety reasoning for dangerous adversarial jailbreak queries while providing direct responses to straightforward malicious queries and general helpful tasks. This selective mechanism enables small models to maintain robust safety guarantees against sophisticated attacks while preserving computational efficiency for benign interactions. Experimental results demonstrate that EASE reduces jailbreak attack success rates by up to 17% compared to shallow alignment methods while reducing inference overhead by up to 90% compared to deliberative safety reasoning alignment, making it practical for SLMs real-world edge deployments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program Synthesis</title>
<link>https://arxiv.org/abs/2511.06522</link>
<guid>https://arxiv.org/abs/2511.06522</guid>
<content:encoded><![CDATA[

arXiv:2511.06522v1 Announce Type: cross 
Abstract: Mathematical reasoning requires abstracting symbolic rules from visual patterns -- inferring the infinite from the finite. We investigate whether multimodal AI systems possess this capability through FractalBench, a benchmark evaluating fractal program synthesis from images. Fractals provide ideal test cases: Iterated Function Systems with only a few contraction maps generate complex self-similar patterns through simple recursive rules, requiring models to bridge visual perception with mathematical abstraction. We evaluate four leading MLLMs -- GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, and Qwen 2.5-VL -- on 12 canonical fractals. Models must generate executable Python code reproducing the fractal, enabling objective evaluation. Results reveal a striking disconnect: 76% generate syntactically valid code but only 4% capture mathematical structure. Success varies systematically -- models handle geometric transformations (Koch curves: 17-21%) but fail at branching recursion (trees: <2%), revealing fundamental gaps in mathematical abstraction. FractalBench provides a contamination-resistant diagnostic for visual-mathematical reasoning and is available at https://github.com/NaiveNeuron/FractalBench
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep2Text: Decoding Full Text from a Single LLM Token Representation</title>
<link>https://arxiv.org/abs/2511.06571</link>
<guid>https://arxiv.org/abs/2511.06571</guid>
<content:encoded><![CDATA[

arXiv:2511.06571v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved remarkable progress across diverse tasks, yet their internal mechanisms remain largely opaque. In this work, we address a fundamental question: to what extent can the original input text be recovered from a single last-token representation within an LLM? We propose Rep2Text, a novel framework for decoding full text from last-token representations. Rep2Text employs a trainable adapter that projects a target model's internal representations into the embedding space of a decoding language model, which then autoregressively reconstructs the input text. Experiments on various model combinations (Llama-3.1-8B, Gemma-7B, Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the information in 16-token sequences can be recovered from this compressed representation while maintaining strong semantic integrity and coherence. Furthermore, our analysis reveals an information bottleneck effect: longer sequences exhibit decreased token-level recovery while preserving strong semantic integrity. Besides, our framework also demonstrates robust generalization to out-of-distribution medical data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning</title>
<link>https://arxiv.org/abs/2511.06575</link>
<guid>https://arxiv.org/abs/2511.06575</guid>
<content:encoded><![CDATA[

arXiv:2511.06575v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently emerged as planners for language-instructed agents, generating sequences of actions to accomplish natural language tasks. However, their reliability remains a challenge, especially in long-horizon tasks, since they often produce overconfident yet wrong outputs. Conformal Prediction (CP) has been leveraged to address this issue by wrapping LLM outputs into prediction sets that contain the correct action with a user-defined confidence. When the prediction set is a singleton, the planner executes that action; otherwise, it requests help from a user. This has led to LLM-based planners that can ensure plan correctness with a user-defined probability. However, as LLMs are trained in an uncertainty-agnostic manner, without awareness of prediction sets, they tend to produce unnecessarily large sets, particularly at higher confidence levels, resulting in frequent human interventions limiting autonomous deployment. To address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first CP-aware finetuning framework for LLM-based planners that explicitly reduces prediction-set size and, in turn, the need for user interventions. We evaluate our approach on multiple language-instructed robot planning problems and show consistent improvements over uncertainty-aware and uncertainty-agnostic finetuning baselines in terms of prediction-set size, and help rates. Finally, we demonstrate robustness of our method to out-of-distribution scenarios in hardware experiments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabRAG: Tabular Document Retrieval via Structured Language Representations</title>
<link>https://arxiv.org/abs/2511.06582</link>
<guid>https://arxiv.org/abs/2511.06582</guid>
<content:encoded><![CDATA[

arXiv:2511.06582v1 Announce Type: cross 
Abstract: Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Biomolecular Motion: The Physics-Informed Machine Learning Paradigm</title>
<link>https://arxiv.org/abs/2511.06585</link>
<guid>https://arxiv.org/abs/2511.06585</guid>
<content:encoded><![CDATA[

arXiv:2511.06585v1 Announce Type: cross 
Abstract: The convergence of statistical learning and molecular physics is transforming our approach to modeling biomolecular systems. Physics-informed machine learning (PIML) offers a systematic framework that integrates data-driven inference with physical constraints, resulting in models that are accurate, mechanistic, generalizable, and able to extrapolate beyond observed domains. This review surveys recent advances in physics-informed neural networks and operator learning, differentiable molecular simulation, and hybrid physics-ML potentials, with emphasis on long-timescale kinetics, rare events, and free-energy estimation. We frame these approaches as solutions to the "biomolecular closure problem", recovering unresolved interactions beyond classical force fields while preserving thermodynamic consistency and mechanistic interpretability. We examine theoretical foundations, tools and frameworks, computational trade-offs, and unresolved issues, including model expressiveness and stability. We outline prospective research avenues at the intersection of machine learning, statistical physics, and computational chemistry, contending that future advancements will depend on mechanistic inductive biases, and integrated differentiable physical learning frameworks for biomolecular simulation and discovery.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2511.06618</link>
<guid>https://arxiv.org/abs/2511.06618</guid>
<content:encoded><![CDATA[

arXiv:2511.06618v1 Announce Type: cross 
Abstract: Contracts are complex documents featuring detailed formal structures, explicit and implicit dependencies and rich semantic content. Given these document properties, contract drafting and manual examination of contracts have proven to be both arduous and susceptible to errors. This work aims to simplify and automate the task of contract review and analysis using a novel framework for transforming legal contracts into structured semantic graphs, enabling computational analysis and data-driven insights. We introduce a detailed ontology mapping core legal contract elements to their graph-theoretic equivalents of nodes and edges. We then present a reinforcement learning based Large Language Model (LLM) framework for segmentation and extraction of entities and relationships from contracts. Our method, GRAPH-GRPO-LEX, incorporates both LLMs and reinforcement learning with group relative policy optimization (GRPO). By applying a carefully drafted reward function of graph metrics, we demonstrate the ability to automatically identify direct relationships between clauses, and even uncover hidden dependencies. Our introduction of the gated GRPO approach shows a strong learning signal and can move contract analysis from a linear, manual reading process to an easily visualized graph. This allows for a more dynamic analysis, including building the groundwork for contract linting similar to what is now practiced in software engineering.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Cross-Disease Reasoning for Cardiovascular Risk Assessment from LDCT</title>
<link>https://arxiv.org/abs/2511.06625</link>
<guid>https://arxiv.org/abs/2511.06625</guid>
<content:encoded><![CDATA[

arXiv:2511.06625v1 Announce Type: cross 
Abstract: Low-dose chest computed tomography (LDCT) inherently captures both pulmonary and cardiac structures, offering a unique opportunity for joint assessment of lung and cardiovascular health. However, most existing approaches treat these domains as independent tasks, overlooking their physiological interplay and shared imaging biomarkers. We propose an Explainable Cross-Disease Reasoning Framework that enables interpretable cardiopulmonary risk assessment from a single LDCT scan. The framework introduces an agentic reasoning process that emulates clinical diagnostic thinking-first perceiving pulmonary findings, then reasoning through established medical knowledge, and finally deriving a cardiovascular judgment with explanatory rationale. It integrates three synergistic components: a pulmonary perception module that summarizes lung abnormalities, a knowledge-guided reasoning module that infers their cardiovascular implications, and a cardiac representation module that encodes structural biomarkers. Their outputs are fused to produce a holistic cardiovascular risk prediction that is both accurate and physiologically grounded. Experiments on the NLST cohort demonstrate that the proposed framework achieves state-of-the-art performance for CVD screening and mortality prediction, outperforming single-disease and purely image-based baselines. Beyond quantitative gains, the framework provides human-verifiable reasoning that aligns with cardiological understanding, revealing coherent links between pulmonary abnormalities and cardiac stress mechanisms. Overall, this work establishes a unified and explainable paradigm for cardiovascular analysis from LDCT, bridging the gap between image-based prediction and mechanism-based medical interpretation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Testing for Segmenting Watermarked Texts From Language Models</title>
<link>https://arxiv.org/abs/2511.06645</link>
<guid>https://arxiv.org/abs/2511.06645</guid>
<content:encoded><![CDATA[

arXiv:2511.06645v1 Announce Type: cross 
Abstract: The rapid adoption of large language models (LLMs), such as GPT-4 and Claude 3.5, underscores the need to distinguish LLM-generated text from human-written content to mitigate the spread of misinformation and misuse in education. One promising approach to address this issue is the watermark technique, which embeds subtle statistical signals into LLM-generated text to enable reliable identification. In this paper, we first generalize the likelihood-based LLM detection method of a previous study by introducing a flexible weighted formulation, and further adapt this approach to the inverse transform sampling method. Moving beyond watermark detection, we extend this adaptive detection strategy to tackle the more challenging problem of segmenting a given text into watermarked and non-watermarked substrings. In contrast to the approach in a previous study, which relies on accurate estimation of next-token probabilities that are highly sensitive to prompt estimation, our proposed framework removes the need for precise prompt estimation. Extensive numerical experiments demonstrate that the proposed methodology is both effective and robust in accurately segmenting texts containing a mixture of watermarked and non-watermarked content.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GNN-Enabled Robust Hybrid Beamforming with Score-Based CSI Generation and Denoising</title>
<link>https://arxiv.org/abs/2511.06663</link>
<guid>https://arxiv.org/abs/2511.06663</guid>
<content:encoded><![CDATA[

arXiv:2511.06663v1 Announce Type: cross 
Abstract: Accurate Channel State Information (CSI) is critical for Hybrid Beamforming (HBF) tasks. However, obtaining high-resolution CSI remains challenging in practical wireless communication systems. To address this issue, we propose to utilize Graph Neural Networks (GNNs) and score-based generative models to enable robust HBF under imperfect CSI conditions. Firstly, we develop the Hybrid Message Graph Attention Network (HMGAT) which updates both node and edge features through node-level and edge-level message passing. Secondly, we design a Bidirectional Encoder Representations from Transformers (BERT)-based Noise Conditional Score Network (NCSN) to learn the distribution of high-resolution CSI, facilitating CSI generation and data augmentation to further improve HMGAT's performance. Finally, we present a Denoising Score Network (DSN) framework and its instantiation, termed DeBERT, which can denoise imperfect CSI under arbitrary channel error levels, thereby facilitating robust HBF. Experiments on DeepMIMO urban datasets demonstrate the proposed models' superior generalization, scalability, and robustness across various HBF tasks with perfect and imperfect CSI.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare</title>
<link>https://arxiv.org/abs/2511.06668</link>
<guid>https://arxiv.org/abs/2511.06668</guid>
<content:encoded><![CDATA[

arXiv:2511.06668v1 Announce Type: cross 
Abstract: In high-stakes information domains such as healthcare, where large language models (LLMs) can produce hallucinations or misinformation, retrieval-augmented generation (RAG) has been proposed as a mitigation strategy, grounding model outputs in external, domain-specific documents. Yet, this approach can introduce errors when source documents contain outdated or contradictory information. This work investigates the performance of five LLMs in generating RAG-based responses to medicine-related queries. Our contributions are three-fold: i) the creation of a benchmark dataset using consumer medicine information documents from the Australian Therapeutic Goods Administration (TGA), where headings are repurposed as natural language questions, ii) the retrieval of PubMed abstracts using TGA headings, stratified across multiple publication years, to enable controlled temporal evaluation of outdated evidence, and iii) a comparative analysis of the frequency and impact of outdated or contradictory content on model-generated responses, assessing how LLMs integrate and reconcile temporally inconsistent information. Our findings show that contradictions between highly similar abstracts do, in fact, degrade performance, leading to inconsistencies and reduced factual accuracy in model answers. These results highlight that retrieval similarity alone is insufficient for reliable medical RAG and underscore the need for contradiction-aware filtering strategies to ensure trustworthy responses in high-stakes domains.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adam symmetry theorem: characterization of the convergence of the stochastic Adam optimizer</title>
<link>https://arxiv.org/abs/2511.06675</link>
<guid>https://arxiv.org/abs/2511.06675</guid>
<content:encoded><![CDATA[

arXiv:2511.06675v1 Announce Type: cross 
Abstract: Beside the standard stochastic gradient descent (SGD) method, the Adam optimizer due to Kingma & Ba (2014) is currently probably the best-known optimization method for the training of deep neural networks in artificial intelligence (AI) systems. Despite the popularity and the success of Adam it remains an \emph{open research problem} to provide a rigorous convergence analysis for Adam even for the class of strongly convex SOPs. In one of the main results of this work we establish convergence rates for Adam in terms of the number of gradient steps (convergence rate \nicefrac{1}{2} w.r.t. the size of the learning rate), the size of the mini-batches (convergence rate 1 w.r.t. the size of the mini-batches), and the size of the second moment parameter of Adam (convergence rate 1 w.r.t. the distance of the second moment parameter to 1) for the class of strongly convex SOPs. In a further main result of this work, which we refer to as \emph{Adam symmetry theorem}, we illustrate the optimality of the established convergence rates by proving for a special class of simple quadratic strongly convex SOPs that Adam converges as the number of gradient steps increases to infinity to the solution of the SOP (the unique minimizer of the strongly convex objective function) if and \emph{only} if the random variables in the SOP (the data in the SOP) are \emph{symmetrically distributed}. In particular, in the standard case where the random variables in the SOP are not symmetrically distributed we \emph{disprove} that Adam converges to the minimizer of the SOP as the number of Adam steps increases to infinity. We also complement the conclusions of our convergence analysis and the Adam symmetry theorem by several numerical simulations that indicate the sharpness of the established convergence rates and that illustrate the practical appearance of the phenomena revealed in the \emph{Adam symmetry theorem}.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flexible Concept Bottleneck Model</title>
<link>https://arxiv.org/abs/2511.06678</link>
<guid>https://arxiv.org/abs/2511.06678</guid>
<content:encoded><![CDATA[

arXiv:2511.06678v1 Announce Type: cross 
Abstract: Concept bottleneck models (CBMs) improve neural network interpretability by introducing an intermediate layer that maps human-understandable concepts to predictions. Recent work has explored the use of vision-language models (VLMs) to automate concept selection and annotation. However, existing VLM-based CBMs typically require full model retraining when new concepts are involved, which limits their adaptability and flexibility in real-world scenarios, especially considering the rapid evolution of vision-language foundation models. To address these issues, we propose Flexible Concept Bottleneck Model (FCBM), which supports dynamic concept adaptation, including complete replacement of the original concept set. Specifically, we design a hypernetwork that generates prediction weights based on concept embeddings, allowing seamless integration of new concepts without retraining the entire model. In addition, we introduce a modified sparsemax module with a learnable temperature parameter that dynamically selects the most relevant concepts, enabling the model to focus on the most informative features. Extensive experiments on five public benchmarks demonstrate that our method achieves accuracy comparable to state-of-the-art baselines with a similar number of effective concepts. Moreover, the model generalizes well to unseen concepts with just a single epoch of fine-tuning, demonstrating its strong adaptability and flexibility.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lassoed Forests: Random Forests with Adaptive Lasso Post-selection</title>
<link>https://arxiv.org/abs/2511.06698</link>
<guid>https://arxiv.org/abs/2511.06698</guid>
<content:encoded><![CDATA[

arXiv:2511.06698v1 Announce Type: cross 
Abstract: Random forests are a statistical learning technique that use bootstrap aggregation to average high-variance and low-bias trees. Improvements to random forests, such as applying Lasso regression to the tree predictions, have been proposed in order to reduce model bias. However, these changes can sometimes degrade performance (e.g., an increase in mean squared error). In this paper, we show in theory that the relative performance of these two methods, standard and Lasso-weighted random forests, depends on the signal-to-noise ratio. We further propose a unified framework to combine random forests and Lasso selection by applying adaptive weighting and show mathematically that it can strictly outperform the other two methods. We compare the three methods through simulation, including bias-variance decomposition, error estimates evaluation, and variable importance analysis. We also show the versatility of our method by applications to a variety of real-world datasets.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Wisdom of the Crowd: High-Fidelity Classification of Cyber-Attacks and Faults in Power Systems Using Ensemble and Machine Learning</title>
<link>https://arxiv.org/abs/2511.06714</link>
<guid>https://arxiv.org/abs/2511.06714</guid>
<content:encoded><![CDATA[

arXiv:2511.06714v1 Announce Type: cross 
Abstract: This paper presents a high-fidelity evaluation framework for machine learning (ML)-based classification of cyber-attacks and physical faults using electromagnetic transient simulations with digital substation emulation at 4.8 kHz. Twelve ML models, including ensemble algorithms and a multi-layer perceptron (MLP), were trained on labeled time-domain measurements and evaluated in a real-time streaming environment designed for sub-cycle responsiveness. The architecture incorporates a cycle-length smoothing filter and confidence threshold to stabilize decisions. Results show that while several models achieved near-perfect offline accuracies (up to 99.9%), only the MLP sustained robust coverage (98-99%) under streaming, whereas ensembles preserved perfect anomaly precision but abstained frequently (10-49% coverage). These findings demonstrate that offline accuracy alone is an unreliable indicator of field readiness and underscore the need for realistic testing and inference pipelines to ensure dependable classification in inverter-based resources (IBR)-rich networks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SRNN: Spatiotemporal Relational Neural Network for Intuitive Physics Understanding</title>
<link>https://arxiv.org/abs/2511.06761</link>
<guid>https://arxiv.org/abs/2511.06761</guid>
<content:encoded><![CDATA[

arXiv:2511.06761v1 Announce Type: cross 
Abstract: Human prowess in intuitive physics remains unmatched by machines. To bridge this gap, we argue for a fundamental shift towards brain-inspired computational principles. This paper introduces the Spatiotemporal Relational Neural Network (SRNN), a model that establishes a unified neural representation for object attributes, relations, and timeline, with computations governed by a Hebbian ``Fire Together, Wire Together'' mechanism across dedicated \textit{What} and \textit{How} pathways. This unified representation is directly used to generate structured linguistic descriptions of the visual scene, bridging perception and language within a shared neural substrate. Moreover, unlike the prevalent ``pretrain-then-finetune'' paradigm, SRNN adopts a ``predefine-then-finetune'' approach. On the CLEVRER benchmark, SRNN achieves competitive performance. Our analysis further reveals a benchmark bias, outlines a path for a more holistic evaluation, and demonstrates SRNN's white-box utility for precise error diagnosis. Our work confirms the viability of translating biological intelligence into engineered systems for intuitive physics understanding.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bilevel Learning via Inexact Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2511.06774</link>
<guid>https://arxiv.org/abs/2511.06774</guid>
<content:encoded><![CDATA[

arXiv:2511.06774v1 Announce Type: cross 
Abstract: Bilevel optimization is a central tool in machine learning for high-dimensional hyperparameter tuning. Its applications are vast; for instance, in imaging it can be used for learning data-adaptive regularizers and optimizing forward operators in variational regularization. These problems are large in many ways: a lot of data is usually available to train a large number of parameters, calling for stochastic gradient-based algorithms. However, exact gradients with respect to parameters (so-called hypergradients) are not available, and their precision is usually linearly related to computational cost. Hence, algorithms must solve the problem efficiently without unnecessary precision. The design of such methods is still not fully understood, especially regarding how accuracy requirements and step size schedules affect theoretical guarantees and practical performance. Existing approaches introduce stochasticity at both the upper level (e.g., in sampling or mini-batch estimates) and the lower level (e.g., in solving the inner problem) to improve generalization, but they typically fix the number of lower-level iterations, which conflicts with asymptotic convergence assumptions. In this work, we advance the theory of inexact stochastic bilevel optimization. We prove convergence and establish rates under decaying accuracy and step size schedules, showing that with optimal configurations convergence occurs at an $\mathcal{O}(k^{-1/4})$ rate in expectation. Experiments on image denoising and inpainting with convex ridge regularizers and input-convex networks confirm our analysis: decreasing step sizes improve stability, accuracy scheduling is more critical than step size strategy, and adaptive preconditioning (e.g., Adam) further boosts performance. These results bridge theory and practice, providing convergence guarantees and practical guidance for large-scale imaging problems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OntoTune: Ontology-Driven Learning for Query Optimization with Convolutional Models</title>
<link>https://arxiv.org/abs/2511.06780</link>
<guid>https://arxiv.org/abs/2511.06780</guid>
<content:encoded><![CDATA[

arXiv:2511.06780v1 Announce Type: cross 
Abstract: Query optimization has been studied using machine learning, reinforcement learning, and, more recently, graph-based convolutional networks. Ontology, as a structured, information-rich knowledge representation, can provide context, particularly in learning problems. This paper presents OntoTune, an ontology-based platform for enhancing learning for query optimization. By connecting SQL queries, database metadata, and statistics, the ontology developed in this research is promising in capturing relationships and important determinants of query performance. This research also develops a method to embed ontologies while preserving as much of the relationships and key information as possible, before feeding it into learning algorithms such as tree-based and graph-based convolutional networks. A case study shows how OntoTune's ontology-driven learning delivers performance gains compared with database system default query execution.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEDN: A Hard-Easy Dual Network with Task Difficulty Assessment for EEG Emotion Recognition</title>
<link>https://arxiv.org/abs/2511.06782</link>
<guid>https://arxiv.org/abs/2511.06782</guid>
<content:encoded><![CDATA[

arXiv:2511.06782v1 Announce Type: cross 
Abstract: Multi-source domain adaptation represents an effective approach to addressing individual differences in cross-subject EEG emotion recognition. However, existing methods treat all source domains equally, neglecting the varying transfer difficulties between different source domains and the target domain. This oversight can lead to suboptimal adaptation. To address this challenge, we propose a novel Hard-Easy Dual Network (HEDN), which dynamically identifies "Hard Source" and "Easy Source" through a Task Difficulty Assessment (TDA) mechanism and establishes two specialized knowledge adaptation branches. Specifically, the Hard Network is dedicated to handling "Hard Source" with higher transfer difficulty by aligning marginal distribution differences between source and target domains. Conversely, the Easy Network focuses on "Easy Source" with low transfer difficulty, utilizing a prototype classifier to model intra-class clustering structures while generating reliable pseudo-labels for the target domain through a prototype-guided label propagation algorithm. Extensive experiments on two benchmark datasets, SEED and SEED-IV, demonstrate that HEDN achieves state-of-the-art performance in cross-subject EEG emotion recognition, with average accuracies of 93.58\% on SEED and 79.82\% on SEED-IV, respectively. These results confirm the effectiveness and generalizability of HEDN in cross-subject EEG emotion recognition.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Fast Unrank in Collaborative Filtering Recommendation</title>
<link>https://arxiv.org/abs/2511.06803</link>
<guid>https://arxiv.org/abs/2511.06803</guid>
<content:encoded><![CDATA[

arXiv:2511.06803v1 Announce Type: cross 
Abstract: Modern data-driven recommendation systems risk memorizing sensitive user behavioral patterns, raising privacy concerns. Existing recommendation unlearning methods, while capable of removing target data influence, suffer from inefficient unlearning speed and degraded performance, failing to meet real-time unlearning demands. Considering the ranking-oriented nature of recommendation systems, we present unranking, the process of reducing the ranking positions of target items while ensuring the formal guarantees of recommendation unlearning. To achieve efficient unranking, we propose Learning to Fast Unrank in Collaborative Filtering Recommendation (L2UnRank), which operates through three key stages: (a) identifying the influenced scope via interaction-based p-hop propagation, (b) computing structural and semantic influences for entities within this scope, and (c) performing efficient, ranking-aware parameter updates guided by influence information. Extensive experiments across multiple datasets and backbone models demonstrate L2UnRank's model-agnostic nature, achieving state-of-the-art unranking effectiveness and maintaining recommendation quality comparable to retraining, while also delivering a 50x speedup over existing methods. Codes are available at https://github.com/Juniper42/L2UnRank.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.06805</link>
<guid>https://arxiv.org/abs/2511.06805</guid>
<content:encoded><![CDATA[

arXiv:2511.06805v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \textbf{\method}, a \textbf{Math}ematical \textbf{S}elf-\textbf{E}volving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at \texttt{https://zheny2751\allowbreak-dotcom.github.io/\allowbreak MathSE.github.io/}.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence of Actor-Critic Learning for Mean Field Games and Mean Field Control in Continuous Spaces</title>
<link>https://arxiv.org/abs/2511.06812</link>
<guid>https://arxiv.org/abs/2511.06812</guid>
<content:encoded><![CDATA[

arXiv:2511.06812v1 Announce Type: cross 
Abstract: We establish the convergence of the deep actor-critic reinforcement learning algorithm presented in [Angiuli et al., 2023a] in the setting of continuous state and action spaces with an infinite discrete-time horizon. This algorithm provides solutions to Mean Field Game (MFG) or Mean Field Control (MFC) problems depending on the ratio between two learning rates: one for the value function and the other for the mean field term. In the MFC case, to rigorously identify the limit, we introduce a discretization of the state and action spaces, following the approach used in the finite-space case in [Angiuli et al., 2023b]. The convergence proofs rely on a generalization of the two-timescale framework introduced in [Borkar, 1997]. We further extend our convergence results to Mean Field Control Games, which involve locally cooperative and globally competitive populations. Finally, we present numerical experiments for linear-quadratic problems in one and two dimensions, for which explicit solutions are available.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Focus: Focal Attention for Selective and Scalable Transformers</title>
<link>https://arxiv.org/abs/2511.06818</link>
<guid>https://arxiv.org/abs/2511.06818</guid>
<content:encoded><![CDATA[

arXiv:2511.06818v1 Announce Type: cross 
Abstract: Attention is a core component of transformer architecture, whether encoder-only, decoder-only, or encoder-decoder model. However, the standard softmax attention often produces noisy probability distribution, which can impair effective feature selection at every layer of these models, particularly for long contexts. We propose Focal Attention, a simple yet effective modification that sharpens the attention distribution by controlling the softmax temperature, either as a fixed hyperparameter or as a learnable parameter during training. This sharpening enables the model to concentrate on the most relevant tokens while suppressing irrelevant ones. Empirically, Focal Attention scales more favorably than standard transformer with respect to model size, training data, and context length. Across diverse benchmarks, it achieves the same accuracy with up to 42% fewer parameters or 33% less training data. On long-context tasks, it delivers substantial relative improvements ranging from 17% to 82%, demonstrating its effectiveness in real world applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dimensionality reduction and width of deep neural networks based on topological degree theory</title>
<link>https://arxiv.org/abs/2511.06821</link>
<guid>https://arxiv.org/abs/2511.06821</guid>
<content:encoded><![CDATA[

arXiv:2511.06821v1 Announce Type: cross 
Abstract: In this paper we present a mathematical framework on linking of embeddings of compact topological spaces into Euclidean spaces and separability of linked embeddings under a specific class of dimension reduction maps. As applications of the established theory, we provide some fascinating insights into classification and approximation problems in deep learning theory in the setting of deep neural networks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats</title>
<link>https://arxiv.org/abs/2511.06838</link>
<guid>https://arxiv.org/abs/2511.06838</guid>
<content:encoded><![CDATA[

arXiv:2511.06838v1 Announce Type: cross 
Abstract: The substantial memory bandwidth and computational demand of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator co-design for P3-LLM, featuring lightweight compute units to support our hybrid numerical formats. The enhanced PIM compute units significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Our evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art quantization accuracy in terms of both KV-cache-only quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\times$, $2.0\times$, and $3.4\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiated Directional Intervention A Framework for Evading LLM Safety Alignment</title>
<link>https://arxiv.org/abs/2511.06852</link>
<guid>https://arxiv.org/abs/2511.06852</guid>
<content:encoded><![CDATA[

arXiv:2511.06852v1 Announce Type: cross 
Abstract: Safety alignment instills in Large Language Models (LLMs) a critical capacity to refuse malicious requests. Prior works have modeled this refusal mechanism as a single linear direction in the activation space. We posit that this is an oversimplification that conflates two functionally distinct neural processes: the detection of harm and the execution of a refusal. In this work, we deconstruct this single representation into a Harm Detection Direction and a Refusal Execution Direction. Leveraging this fine-grained model, we introduce Differentiated Bi-Directional Intervention (DBDI), a new white-box framework that precisely neutralizes the safety alignment at critical layer. DBDI applies adaptive projection nullification to the refusal execution direction while suppressing the harm detection direction via direct steering. Extensive experiments demonstrate that DBDI outperforms prominent jailbreaking methods, achieving up to a 97.88\% attack success rate on models such as Llama-2. By providing a more granular and mechanistic framework, our work offers a new direction for the in-depth understanding of LLM safety alignment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inclusion of Role into Named Entity Recognition and Ranking</title>
<link>https://arxiv.org/abs/2511.06886</link>
<guid>https://arxiv.org/abs/2511.06886</guid>
<content:encoded><![CDATA[

arXiv:2511.06886v1 Announce Type: cross 
Abstract: Most of the Natural Language Processing sys- tems are involved in entity-based processing for several tasks like Information Extraction, Question-Answering, Text-Summarization and so on. A new challenge comes when entities play roles according to their act or attributes in certain context. Entity Role Detection is the task of assigning such roles to the entities. Usu- ally real-world entities are of types: person, lo- cation and organization etc. Roles could be con- sidered as domain-dependent subtypes of these types. In the cases, where retrieving a subset of entities based on their roles is needed, poses the problem of defining the role and entities having those roles. This paper presents the study of study of solving Entity Role Detection prob- lem by modeling it as Named Entity Recogni- tion (NER) and Entity Retrieval/Ranking task. In NER, these roles could be considered as mutually exclusive classes and standard NER methods like sequence tagging could be used. For Entity Retrieval, Roles could be formulated as Query and entities as Collection on which the query needs to be executed. The aspect of Entity Retrieval task, which is different than document retrieval task is that the entities and roles against which they need to be retrieved are indirectly described. We have formulated au- tomated ways of learning representative words and phrases and building representations of roles and entities using them. We have also explored different contexts like sentence and document. Since the roles depend upon con- text, so it is not always possible to have large domain-specific dataset or knowledge bases for learning purposes, so we have tried to exploit the information from small dataset in domain- agnostic way.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement Learning with Reward Function Optimization</title>
<link>https://arxiv.org/abs/2511.06937</link>
<guid>https://arxiv.org/abs/2511.06937</guid>
<content:encoded><![CDATA[

arXiv:2511.06937v1 Announce Type: cross 
Abstract: Diffusion models recently emerged as a powerful paradigm for recommender systems, offering state-of-the-art performance by modeling the generative process of user-item interactions. However, training such models from scratch is both computationally expensive and yields diminishing returns once convergence is reached. To remedy these challenges, we propose ReFiT, a new framework that integrates Reinforcement learning (RL)-based Fine-Tuning into diffusion-based recommender systems. In contrast to prior RL approaches for diffusion models depending on external reward models, ReFiT adopts a task-aligned design: it formulates the denoising trajectory as a Markov decision process (MDP) and incorporates a collaborative signal-aware reward function that directly reflects recommendation quality. By tightly coupling the MDP structure with this reward signal, ReFiT empowers the RL agent to exploit high-order connectivity for fine-grained optimization, while avoiding the noisy or uninformative feedback common in naive reward designs. Leveraging policy gradient optimization, ReFiT maximizes exact log-likelihood of observed interactions, thereby enabling effective post hoc fine-tuning of diffusion recommenders. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed ReFiT framework (a) exhibits substantial performance gains over strong competitors (up to 36.3% on sequential recommendation), (b) demonstrates strong efficiency with linear complexity in the number of users or items, and (c) generalizes well across multiple diffusion-based recommendation scenarios. The source code and datasets are publicly available at https://anonymous.4open.science/r/ReFiT-4C60.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrueCity: Real and Simulated Urban Data for Cross-Domain 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2511.07007</link>
<guid>https://arxiv.org/abs/2511.07007</guid>
<content:encoded><![CDATA[

arXiv:2511.07007v1 Announce Type: cross 
Abstract: 3D semantic scene understanding remains a long-standing challenge in the 3D computer vision community. One of the key issues pertains to limited real-world annotated data to facilitate generalizable models. The common practice to tackle this issue is to simulate new data. Although synthetic datasets offer scalability and perfect labels, their designer-crafted scenes fail to capture real-world complexity and sensor noise, resulting in a synthetic-to-real domain gap. Moreover, no benchmark provides synchronized real and simulated point clouds for segmentation-oriented domain shift analysis. We introduce TrueCity, the first urban semantic segmentation benchmark with cm-accurate annotated real-world point clouds, semantic 3D city models, and annotated simulated point clouds representing the same city. TrueCity proposes segmentation classes aligned with international 3D city modeling standards, enabling consistent evaluation of synthetic-to-real gap. Our extensive experiments on common baselines quantify domain shift and highlight strategies for exploiting synthetic data to enhance real-world 3D scene understanding. We are convinced that the TrueCity dataset will foster further development of sim-to-real gap quantification and enable generalizable data-driven models. The data, code, and 3D models are available online: https://tum-gis.github.io/TrueCity/
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity</title>
<link>https://arxiv.org/abs/2511.07011</link>
<guid>https://arxiv.org/abs/2511.07011</guid>
<content:encoded><![CDATA[

arXiv:2511.07011v1 Announce Type: cross 
Abstract: Background: Captured between clinical appointments using mobile devices, spoken language has potential for objective, more regular assessment of symptom severity and earlier detection of relapse in major depressive disorder. However, research to date has largely been in non-clinical cross-sectional samples of written language using complex machine learning (ML) approaches with limited interpretability.
  Methods: We describe an initial exploratory analysis of longitudinal speech data and PHQ-8 assessments from 5,836 recordings of 586 participants in the UK, Netherlands, and Spain, collected in the RADAR-MDD study. We sought to identify interpretable lexical features associated with MDD symptom severity with linear mixed-effects modelling. Interpretable features and high-dimensional vector embeddings were also used to test the prediction performance of four regressor ML models.
  Results: In English data, MDD symptom severity was associated with 7 features including lexical diversity measures and absolutist language. In Dutch, associations were observed with words per sentence and positive word frequency; no associations were observed in recordings collected in Spain. The predictive power of lexical features and vector embeddings was near chance level across all languages.
  Limitations: Smaller samples in non-English speech and methodological choices, such as the elicitation prompt, may have also limited the effect sizes observable. A lack of NLP tools in languages other than English restricted our feature choice.
  Conclusion: To understand the value of lexical markers in clinical research and practice, further research is needed in larger samples across several languages using improved protocols, and ML models that account for within- and between-individual variations in language.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy-Aware Lymphoma Lesion Detection in Whole-Body PET/CT</title>
<link>https://arxiv.org/abs/2511.07047</link>
<guid>https://arxiv.org/abs/2511.07047</guid>
<content:encoded><![CDATA[

arXiv:2511.07047v1 Announce Type: cross 
Abstract: Early cancer detection is crucial for improving patient outcomes, and 18F FDG PET/CT imaging plays a vital role by combining metabolic and anatomical information. Accurate lesion detection remains challenging due to the need to identify multiple lesions of varying sizes. In this study, we investigate the effect of adding anatomy prior information to deep learning-based lesion detection models. In particular, we add organ segmentation masks from the TotalSegmentator tool as auxiliary inputs to provide anatomical context to nnDetection, which is the state-of-the-art for lesion detection, and Swin Transformer. The latter is trained in two stages that combine self-supervised pre-training and supervised fine-tuning. The method is tested in the AutoPET and Karolinska lymphoma datasets. The results indicate that the inclusion of anatomical priors substantially improves the detection performance within the nnDetection framework, while it has almost no impact on the performance of the vision transformer. Moreover, we observe that Swin Transformer does not offer clear advantages over conventional convolutional neural network (CNN) encoders used in nnDetection. These findings highlight the critical role of the anatomical context in cancer lesion detection, especially in CNN-based models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Sufficient is not Enough: Utilizing the Rashomon Effect for Complete Evidence Extraction</title>
<link>https://arxiv.org/abs/2511.07055</link>
<guid>https://arxiv.org/abs/2511.07055</guid>
<content:encoded><![CDATA[

arXiv:2511.07055v1 Announce Type: cross 
Abstract: Feature attribution methods typically provide minimal sufficient evidence justifying a model decision. However, in many applications this is inadequate. For compliance and cataloging, the full set of contributing features must be identified - complete evidence. We perform a case study on a medical dataset which contains human-annotated complete evidence. We show that individual models typically recover only subsets of complete evidence and that aggregating evidence from several models improves evidence recall from $\sim$0.60 (single best model) to $\sim$0.86 (ensemble). We analyze the recall-precision trade-off, the role of training with evidence, dynamic ensembles with certainty thresholds, and discuss implications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection</title>
<link>https://arxiv.org/abs/2511.07065</link>
<guid>https://arxiv.org/abs/2511.07065</guid>
<content:encoded><![CDATA[

arXiv:2511.07065v1 Announce Type: cross 
Abstract: The opaque nature of deep learning models presents significant challenges for the ethical deployment of hate speech detection systems. To address this limitation, we introduce Supervised Rational Attention (SRA), a framework that explicitly aligns model attention with human rationales, improving both interpretability and fairness in hate speech classification. SRA integrates a supervised attention mechanism into transformer-based classifiers, optimizing a joint objective that combines standard classification loss with an alignment loss term that minimizes the discrepancy between attention weights and human-annotated rationales. We evaluated SRA on hate speech benchmarks in English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations. Empirically, SRA achieves 2.4x better explainability compared to current baselines, and produces token-level explanations that are more faithful and human-aligned. In terms of fairness, SRA achieves competitive fairness across all measures, with second-best performance in detecting toxic posts targeting identity groups, while maintaining comparable results on other metrics. These findings demonstrate that incorporating human rationales into attention mechanisms can enhance interpretability and faithfulness without compromising fairness.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClusterMine: Robust Label-Free Visual Out-Of-Distribution Detection via Concept Mining from Text Corpora</title>
<link>https://arxiv.org/abs/2511.07068</link>
<guid>https://arxiv.org/abs/2511.07068</guid>
<content:encoded><![CDATA[

arXiv:2511.07068v1 Announce Type: cross 
Abstract: Large-scale visual out-of-distribution (OOD) detection has witnessed remarkable progress by leveraging vision-language models such as CLIP. However, a significant limitation of current methods is their reliance on a pre-defined set of in-distribution (ID) ground-truth label names (positives). These fixed label names can be unavailable, unreliable at scale, or become less relevant due to in-distribution shifts after deployment. Towards truly unsupervised OOD detection, we utilize widely available text corpora for positive label mining, bypassing the need for positives. In this paper, we utilize widely available text corpora for positive label mining under a general concept mining paradigm. Within this framework, we propose ClusterMine, a novel positive label mining method. ClusterMine is the first method to achieve state-of-the-art OOD detection performance without access to positive labels. It extracts positive concepts from a large text corpus by combining visual-only sample consistency (via clustering) and zero-shot image-text consistency. Our experimental study reveals that ClusterMine is scalable across a plethora of CLIP models and achieves state-of-the-art robustness to covariate in-distribution shifts. The code is available at https://github.com/HHU-MMBS/clustermine_wacv_official.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services</title>
<link>https://arxiv.org/abs/2511.07070</link>
<guid>https://arxiv.org/abs/2511.07070</guid>
<content:encoded><![CDATA[

arXiv:2511.07070v1 Announce Type: cross 
Abstract: As a key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers a ``seesaw'' between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes a competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-efficient quantum error mitigation via classical learning surrogates</title>
<link>https://arxiv.org/abs/2511.07092</link>
<guid>https://arxiv.org/abs/2511.07092</guid>
<content:encoded><![CDATA[

arXiv:2511.07092v1 Announce Type: cross 
Abstract: The pursuit of practical quantum utility on near-term quantum processors is critically challenged by their inherent noise. Quantum error mitigation (QEM) techniques are leading solutions to improve computation fidelity with relatively low qubit-overhead, while full-scale quantum error correction remains a distant goal. However, QEM techniques incur substantial measurement overheads, especially when applied to families of quantum circuits parameterized by classical inputs. Focusing on zero-noise extrapolation (ZNE), a widely adopted QEM technique, here we devise the surrogate-enabled ZNE (S-ZNE), which leverages classical learning surrogates to perform ZNE entirely on the classical side. Unlike conventional ZNE, whose measurement cost scales linearly with the number of circuits, S-ZNE requires only constant measurement overhead for an entire family of quantum circuits, offering superior scalability. Theoretical analysis indicates that S-ZNE achieves accuracy comparable to conventional ZNE in many practical scenarios, and numerical experiments on up to 100-qubit ground-state energy and quantum metrology tasks confirm its effectiveness. Our approach provides a template that can be effectively extended to other quantum error mitigation protocols, opening a promising path toward scalable error mitigation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End Speech Synthesis</title>
<link>https://arxiv.org/abs/2511.07099</link>
<guid>https://arxiv.org/abs/2511.07099</guid>
<content:encoded><![CDATA[

arXiv:2511.07099v1 Announce Type: cross 
Abstract: Recent advancements in speech synthesis technology have enriched our daily lives, with high-quality and human-like audio widely adopted across real-world applications. However, malicious exploitation like voice-cloning fraud poses severe security risks. Existing defense techniques struggle to address the production large language model (LLM)-based speech synthesis. While previous studies have considered the protection for fine-tuning synthesizers, they assume manually annotated transcripts. Given the labor intensity of manual annotation, end-to-end (E2E) systems leveraging automatic speech recognition (ASR) to generate transcripts are becoming increasingly prevalent, e.g., voice cloning via commercial APIs. Therefore, this E2E speech synthesis also requires new security mechanisms. To tackle these challenges, we propose E2E-VGuard, a proactive defense framework for two emerging threats: (1) production LLM-based speech synthesis, and (2) the novel attack arising from ASR-driven E2E scenarios. Specifically, we employ the encoder ensemble with a feature extractor to protect timbre, while ASR-targeted adversarial examples disrupt pronunciation. Moreover, we incorporate the psychoacoustic model to ensure perturbative imperceptibility. For a comprehensive evaluation, we test 16 open-source synthesizers and 3 commercial APIs across Chinese and English datasets, confirming E2E-VGuard's effectiveness in timbre and pronunciation protection. Real-world deployment validation is also conducted. Our code and demo page are available at https://wxzyd123.github.io/e2e-vguard/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Provably-Correct and Robust Convex Model for Smooth Separable NMF</title>
<link>https://arxiv.org/abs/2511.07109</link>
<guid>https://arxiv.org/abs/2511.07109</guid>
<content:encoded><![CDATA[

arXiv:2511.07109v1 Announce Type: cross 
Abstract: Nonnegative matrix factorization (NMF) is a linear dimensionality reduction technique for nonnegative data, with applications such as hyperspectral unmixing and topic modeling. NMF is a difficult problem in general (NP-hard), and its solutions are typically not unique. To address these two issues, additional constraints or assumptions are often used. In particular, separability assumes that the basis vectors in the NMF are equal to some columns of the input matrix. In that case, the problem is referred to as separable NMF (SNMF) and can be solved in polynomial-time with robustness guarantees, while identifying a unique solution. However, in real-world scenarios, due to noise or variability, multiple data points may lie near the basis vectors, which SNMF does not leverage. In this work, we rely on the smooth separability assumption, which assumes that each basis vector is close to multiple data points. We explore the properties of the corresponding problem, referred to as smooth SNMF (SSNMF), and examine how it relates to SNMF and orthogonal NMF. We then propose a convex model for SSNMF and show that it provably recovers the sought-after factors, even in the presence of noise. We finally adapt an existing fast gradient method to solve this convex model for SSNMF, and show that it compares favorably with state-of-the-art methods on both synthetic and hyperspectral datasets.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Consistently, Reason Efficiently: Energy-Based Calibration for Implicit Chain-of-Thought</title>
<link>https://arxiv.org/abs/2511.07124</link>
<guid>https://arxiv.org/abs/2511.07124</guid>
<content:encoded><![CDATA[

arXiv:2511.07124v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities through \emph{Chain-of-Thought} (CoT) prompting, which enables step-by-step intermediate reasoning. However, explicit CoT methods rely on discrete token-level reasoning processes that are prone to error propagation and limited by vocabulary expressiveness, often resulting in rigid and inconsistent reasoning trajectories. Recent research has explored implicit or continuous reasoning in latent spaces, allowing models to perform internal reasoning before generating explicit output. Although such approaches alleviate some limitations of discrete CoT, they generally lack explicit mechanisms to enforce consistency among reasoning steps, leading to divergent reasoning paths and unstable outcomes. To address this issue, we propose EBM-CoT, an Energy-Based Chain-of-Thought Calibration framework that refines latent thought representations through an energy-based model (EBM). Our method dynamically adjusts latent reasoning trajectories toward lower-energy, high-consistency regions in the embedding space, improving both reasoning accuracy and consistency without modifying the base language model. Extensive experiments across mathematical, commonsense, and symbolic reasoning benchmarks demonstrate that the proposed framework significantly enhances the consistency and efficiency of multi-step reasoning in LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging</title>
<link>https://arxiv.org/abs/2511.07129</link>
<guid>https://arxiv.org/abs/2511.07129</guid>
<content:encoded><![CDATA[

arXiv:2511.07129v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language models.However, conventional LoRA adapters are typically trained for a single task, limiting their applicability in real-world settings where inputs may span diverse and unpredictable domains. At inference time, existing approaches combine multiple LoRAs for improving performance on diverse tasks, while usually requiring labeled data or additional task-specific training, which is expensive at scale. In this work, we introduce LoRA on the Go (LoGo), a training-free framework that dynamically selects and merges adapters at the instance level without any additional requirements. LoGo leverages signals extracted from a single forward pass through LoRA adapters, to identify the most relevant adapters and determine their contributions on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo outperforms training-based baselines on some tasks upto a margin of 3.6% while remaining competitive on other tasks and maintaining inference throughput, highlighting its effectiveness and practicality.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trading Vector Data in Vector Databases</title>
<link>https://arxiv.org/abs/2511.07139</link>
<guid>https://arxiv.org/abs/2511.07139</guid>
<content:encoded><![CDATA[

arXiv:2511.07139v1 Announce Type: cross 
Abstract: Vector data trading is essential for cross-domain learning with vector databases, yet it remains largely unexplored. We study this problem under online learning, where sellers face uncertain retrieval costs and buyers provide stochastic feedback to posted prices. Three main challenges arise: (1) heterogeneous and partial feedback in configuration learning, (2) variable and complex feedback in pricing learning, and (3) inherent coupling between configuration and pricing decisions.
  We propose a hierarchical bandit framework that jointly optimizes retrieval configurations and pricing. Stage I employs contextual clustering with confidence-based exploration to learn effective configurations with logarithmic regret. Stage II adopts interval-based price selection with local Taylor approximation to estimate buyer responses and achieve sublinear regret. We establish theoretical guarantees with polynomial time complexity and validate the framework on four real-world datasets, demonstrating consistent improvements in cumulative reward and regret reduction compared with existing methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.07155</link>
<guid>https://arxiv.org/abs/2511.07155</guid>
<content:encoded><![CDATA[

arXiv:2511.07155v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning for Video Violence Detection: Complementary Roles of Lightweight CNNs and Vision-Language Models for Energy-Efficient Use</title>
<link>https://arxiv.org/abs/2511.07171</link>
<guid>https://arxiv.org/abs/2511.07171</guid>
<content:encoded><![CDATA[

arXiv:2511.07171v1 Announce Type: cross 
Abstract: Deep learning-based video surveillance increasingly demands privacy-preserving architectures with low computational and environmental overhead. Federated learning preserves privacy but deploying large vision-language models (VLMs) introduces major energy and sustainability challenges. We compare three strategies for federated violence detection under realistic non-IID splits on the RWF-2000 and RLVS datasets: zero-shot inference with pretrained VLMs, LoRA-based fine-tuning of LLaVA-NeXT-Video-7B, and personalized federated learning of a 65.8M-parameter 3D CNN. All methods exceed 90% accuracy in binary violence detection. The 3D CNN achieves superior calibration (ROC AUC 92.59%) at roughly half the energy cost (240 Wh vs. 570 Wh) of federated LoRA, while VLMs provide richer multimodal reasoning. Hierarchical category grouping (based on semantic similarity and class exclusion) boosts VLM multiclass accuracy from 65.31% to 81% on the UCF-Crime dataset. To our knowledge, this is the first comparative simulation study of LoRA-tuned VLMs and personalized CNNs for federated violence detection, with explicit energy and CO2e quantification. Our results inform hybrid deployment strategies that default to efficient CNNs for routine inference and selectively engage VLMs for complex contextual reasoning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulation-based Methods for Optimal Sampling Design in Systems Biology</title>
<link>https://arxiv.org/abs/2511.07197</link>
<guid>https://arxiv.org/abs/2511.07197</guid>
<content:encoded><![CDATA[

arXiv:2511.07197v1 Announce Type: cross 
Abstract: In many areas of systems biology, including virology, pharmacokinetics, and population biology, dynamical systems are commonly used to describe biological processes. These systems can be characterized by estimating their parameters from sampled data. The key problem is how to optimally select sampling points to achieve accurate parameter estimation. Classical approaches often rely on Fisher information matrix-based criteria such as A-, D-, and E-optimality, which require an initial parameter estimate and may yield suboptimal results when the estimate is inaccurate. This study proposes two simulation-based methods for optimal sampling design that do not depend on initial parameter estimates. The first method, E-optimal-ranking (EOR), employs the E-optimal criterion, while the second utilizes a Long Short-Term Memory (LSTM) neural network. Simulation studies based on the Lotka-Volterra and three-compartment models demonstrate that the proposed methods outperform both random selection and classical E-optimal design.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Stealth-Potency Trade-off in Clean-Image Backdoors with Generative Trigger Optimization</title>
<link>https://arxiv.org/abs/2511.07210</link>
<guid>https://arxiv.org/abs/2511.07210</guid>
<content:encoded><![CDATA[

arXiv:2511.07210v1 Announce Type: cross 
Abstract: Clean-image backdoor attacks, which use only label manipulation in training datasets to compromise deep neural networks, pose a significant threat to security-critical applications. A critical flaw in existing methods is that the poison rate required for a successful attack induces a proportional, and thus noticeable, drop in Clean Accuracy (CA), undermining their stealthiness. This paper presents a new paradigm for clean-image attacks that minimizes this accuracy degradation by optimizing the trigger itself. We introduce Generative Clean-Image Backdoors (GCB), a framework that uses a conditional InfoGAN to identify naturally occurring image features that can serve as potent and stealthy triggers. By ensuring these triggers are easily separable from benign task-related features, GCB enables a victim model to learn the backdoor from an extremely small set of poisoned examples, resulting in a CA drop of less than 1%. Our experiments demonstrate GCB's remarkable versatility, successfully adapting to six datasets, five architectures, and four tasks, including the first demonstration of clean-image backdoors in regression and segmentation. GCB also exhibits resilience against most of the existing backdoor defenses.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise &amp; pattern: identity-anchored Tikhonov regularization for robust structural anomaly detection</title>
<link>https://arxiv.org/abs/2511.07233</link>
<guid>https://arxiv.org/abs/2511.07233</guid>
<content:encoded><![CDATA[

arXiv:2511.07233v1 Announce Type: cross 
Abstract: Anomaly detection plays a pivotal role in automated industrial inspection, aiming to identify subtle or rare defects in otherwise uniform visual patterns. As collecting representative examples of all possible anomalies is infeasible, we tackle structural anomaly detection using a self-supervised autoencoder that learns to repair corrupted inputs. To this end, we introduce a corruption model that injects artificial disruptions into training images to mimic structural defects. While reminiscent of denoising autoencoders, our approach differs in two key aspects. First, instead of unstructured i.i.d.\ noise, we apply structured, spatially coherent perturbations that make the task a hybrid of segmentation and inpainting. Second, and counterintuitively, we add and preserve Gaussian noise on top of the occlusions, which acts as a Tikhonov regularizer anchoring the Jacobian of the reconstruction function toward identity. This identity-anchored regularization stabilizes reconstruction and further improves both detection and segmentation accuracy. On the MVTec AD benchmark, our method achieves state-of-the-art results (I/P-AUROC: 99.9/99.4), supporting our theoretical framework and demonstrating its practical relevance for automatic inspection.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fully Polynomial-Time Algorithm for Robustly Learning Halfspaces over the Hypercube</title>
<link>https://arxiv.org/abs/2511.07244</link>
<guid>https://arxiv.org/abs/2511.07244</guid>
<content:encoded><![CDATA[

arXiv:2511.07244v1 Announce Type: cross 
Abstract: We give the first fully polynomial-time algorithm for learning halfspaces with respect to the uniform distribution on the hypercube in the presence of contamination, where an adversary may corrupt some fraction of examples and labels arbitrarily. We achieve an error guarantee of $\eta^{O(1)}+\epsilon$ where $\eta$ is the noise rate. Such a result was not known even in the agnostic setting, where only labels can be adversarially corrupted. All prior work over the last two decades has a superpolynomial dependence in $1/\epsilon$ or succeeds only with respect to continuous marginals (such as log-concave densities).
  Previous analyses rely heavily on various structural properties of continuous distributions such as anti-concentration. Our approach avoids these requirements and makes use of a new algorithm for learning Generalized Linear Models (GLMs) with only a polylogarithmic dependence on the activation function's Lipschitz constant. More generally, our framework shows that supervised learning with respect to discrete distributions is not as difficult as previously thought.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PADiff: Predictive and Adaptive Diffusion Policies for Ad Hoc Teamwork</title>
<link>https://arxiv.org/abs/2511.07260</link>
<guid>https://arxiv.org/abs/2511.07260</guid>
<content:encoded><![CDATA[

arXiv:2511.07260v1 Announce Type: cross 
Abstract: Ad hoc teamwork (AHT) requires agents to collaborate with previously unseen teammates, which is crucial for many real-world applications. The core challenge of AHT is to develop an ego agent that can predict and adapt to unknown teammates on the fly. Conventional RL-based approaches optimize a single expected return, which often causes policies to collapse into a single dominant behavior, thus failing to capture the multimodal cooperation patterns inherent in AHT. In this work, we introduce PADiff, a diffusion-based approach that captures agent's multimodal behaviors, unlocking its diverse cooperation modes with teammates. However, standard diffusion models lack the ability to predict and adapt in highly non-stationary AHT scenarios. To address this limitation, we propose a novel diffusion-based policy that integrates critical predictive information about teammates into the denoising process. Extensive experiments across three cooperation environments demonstrate that PADiff outperforms existing AHT methods significantly.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning</title>
<link>https://arxiv.org/abs/2511.07262</link>
<guid>https://arxiv.org/abs/2511.07262</guid>
<content:encoded><![CDATA[

arXiv:2511.07262v1 Announce Type: cross 
Abstract: Scientific Machine Learning (SciML) integrates data-driven inference with physical modeling to solve complex problems in science and engineering. However, the design of SciML architectures, loss formulations, and training strategies remains an expert-driven research process, requiring extensive experimentation and problem-specific insights. Here we introduce AgenticSciML, a collaborative multi-agent system in which over 10 specialized AI agents collaborate to propose, critique, and refine SciML solutions through structured reasoning and iterative evolution. The framework integrates structured debate, retrieval-augmented method memory, and ensemble-guided evolutionary search, enabling the agents to generate and assess new hypotheses about architectures and optimization procedures. Across physics-informed learning and operator learning tasks, the framework discovers solution methods that outperform single-agent and human-designed baselines by up to four orders of magnitude in error reduction. The agents produce novel strategies -- including adaptive mixture-of-expert architectures, decomposition-based PINNs, and physics-informed operator learning models -- that do not appear explicitly in the curated knowledge base. These results show that collaborative reasoning among AI agents can yield emergent methodological innovation, suggesting a path toward scalable, transparent, and autonomous discovery in scientific computing.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Dimensional Asymptotics of Differentially Private PCA</title>
<link>https://arxiv.org/abs/2511.07270</link>
<guid>https://arxiv.org/abs/2511.07270</guid>
<content:encoded><![CDATA[

arXiv:2511.07270v1 Announce Type: cross 
Abstract: In differential privacy, statistics of a sensitive dataset are privatized by introducing random noise. Most privacy analyses provide privacy bounds specifying a noise level sufficient to achieve a target privacy guarantee. Sometimes, these bounds are pessimistic and suggest adding excessive noise, which overwhelms the meaningful signal. It remains unclear if such high noise levels are truly necessary or a limitation of the proof techniques. This paper explores whether we can obtain sharp privacy characterizations that identify the smallest noise level required to achieve a target privacy level for a given mechanism. We study this problem in the context of differentially private principal component analysis, where the goal is to privatize the leading principal components (PCs) of a dataset with n samples and p features. We analyze the exponential mechanism for this problem in a model-free setting and provide sharp utility and privacy characterizations in the high-dimensional limit ($p\rightarrow\infty$). Our privacy result shows that, in high dimensions, detecting the presence of a target individual in the dataset using the privatized PCs is exactly as hard as distinguishing two Gaussians with slightly different means, where the mean difference depends on certain spectral properties of the dataset. Our privacy analysis combines the hypothesis-testing formulation of privacy guarantees proposed by Dong, Roth, and Su (2022) with classical contiguity arguments due to Le Cam to obtain sharp high-dimensional privacy characterizations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Value of Personalized Recommendations: Evidence from Netflix</title>
<link>https://arxiv.org/abs/2511.07280</link>
<guid>https://arxiv.org/abs/2511.07280</guid>
<content:encoded><![CDATA[

arXiv:2511.07280v1 Announce Type: cross 
Abstract: Personalized recommendation systems shape much of user choice online, yet their targeted nature makes separating out the value of recommendation and the underlying goods challenging. We build a discrete choice model that embeds recommendation-induced utility, low-rank heterogeneity, and flexible state dependence and apply the model to viewership data at Netflix. We exploit idiosyncratic variation introduced by the recommendation algorithm to identify and separately value these components as well as to recover model-free diversion ratios that we can use to validate our structural model. We use the model to evaluate counterfactuals that quantify the incremental engagement generated by personalized recommendations. First, we show that replacing the current recommender system with a matrix factorization or popularity-based algorithm would lead to 4% and 12% reduction in engagement, respectively, and decreased consumption diversity. Second, most of the consumption increase from recommendations comes from effective targeting, not mechanical exposure, with the largest gains for mid-popularity goods (as opposed to broadly appealing or very niche goods).
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>De-Individualizing fMRI Signals via Mahalanobis Whitening and Bures Geometry</title>
<link>https://arxiv.org/abs/2511.07313</link>
<guid>https://arxiv.org/abs/2511.07313</guid>
<content:encoded><![CDATA[

arXiv:2511.07313v1 Announce Type: cross 
Abstract: Functional connectivity has been widely investigated to understand brain disease in clinical studies and imaging-based neuroscience, and analyzing changes in functional connectivity has proven to be valuable for understanding and computationally evaluating the effects on brain function caused by diseases or experimental stimuli. By using Mahalanobis data whitening prior to the use of dimensionality reduction algorithms, we are able to distill meaningful information from fMRI signals about subjects and the experimental stimuli used to prompt them. Furthermore, we offer an interpretation of Mahalanobis whitening as a two-stage de-individualization of data which is motivated by similarity as captured by the Bures distance, which is connected to quantum mechanics. These methods have potential to aid discoveries about the mechanisms that link brain function with cognition and behavior and may improve the accuracy and consistency of Alzheimer's diagnosis, especially in the preclinical stage of disease progression.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments</title>
<link>https://arxiv.org/abs/2511.07317</link>
<guid>https://arxiv.org/abs/2511.07317</guid>
<content:encoded><![CDATA[

arXiv:2511.07317v1 Announce Type: cross 
Abstract: We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2511.07318</link>
<guid>https://arxiv.org/abs/2511.07318</guid>
<content:encoded><![CDATA[

arXiv:2511.07318v1 Announce Type: cross 
Abstract: Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Garbage Vulnerable Point Monitoring using IoT and Computer Vision</title>
<link>https://arxiv.org/abs/2511.07325</link>
<guid>https://arxiv.org/abs/2511.07325</guid>
<content:encoded><![CDATA[

arXiv:2511.07325v1 Announce Type: cross 
Abstract: This paper proposes a smart way to manage municipal solid waste by using the Internet of Things (IoT) and computer vision (CV) to monitor illegal waste dumping at garbage vulnerable points (GVPs) in urban areas. The system can quickly detect and monitor dumped waste using a street-level camera and object detection algorithm. Data was collected from the Sangareddy district in Telangana, India. A series of comprehensive experiments was carried out using the proposed dataset to assess the accuracy and overall performance of various object detection models. Specifically, we performed an in-depth evaluation of YOLOv8, YOLOv10, YOLO11m, and RT-DETR on our dataset. Among these models, YOLO11m achieved the highest accuracy of 92.39\% in waste detection, demonstrating its effectiveness in detecting waste. Additionally, it attains an mAP@50 of 0.91, highlighting its high precision. These findings confirm that the object detection model is well-suited for monitoring and tracking waste dumping events at GVP locations. Furthermore, the system effectively captures waste disposal patterns, including hourly, daily, and weekly dumping trends, ensuring comprehensive daily and nightly monitoring.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas</title>
<link>https://arxiv.org/abs/2511.07338</link>
<guid>https://arxiv.org/abs/2511.07338</guid>
<content:encoded><![CDATA[

arXiv:2511.07338v1 Announce Type: cross 
Abstract: Simulating human profiles by instilling personas into large language models (LLMs) is rapidly transforming research in agentic behavioral simulation, LLM personalization, and human-AI alignment. However, most existing synthetic personas remain shallow and simplistic, capturing minimal attributes and failing to reflect the rich complexity and diversity of real human identities. We introduce DEEPPERSONA, a scalable generative engine for synthesizing narrative-complete synthetic personas through a two-stage, taxonomy-guided method. First, we algorithmically construct the largest-ever human-attribute taxonomy, comprising over hundreds of hierarchically organized attributes, by mining thousands of real user-ChatGPT conversations. Second, we progressively sample attributes from this taxonomy, conditionally generating coherent and realistic personas that average hundreds of structured attributes and roughly 1 MB of narrative text, two orders of magnitude deeper than prior works. Intrinsic evaluations confirm significant improvements in attribute diversity (32 percent higher coverage) and profile uniqueness (44 percent greater) compared to state-of-the-art baselines. Extrinsically, our personas enhance GPT-4.1-mini's personalized question answering accuracy by 11.6 percent on average across ten metrics and substantially narrow (by 31.7 percent) the gap between simulated LLM citizens and authentic human responses in social surveys. Our generated national citizens reduced the performance gap on the Big Five personality test by 17 percent relative to LLM-simulated citizens. DEEPPERSONA thus provides a rigorous, scalable, and privacy-free platform for high-fidelity human simulation and personalized AI research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients</title>
<link>https://arxiv.org/abs/2511.07347</link>
<guid>https://arxiv.org/abs/2511.07347</guid>
<content:encoded><![CDATA[

arXiv:2511.07347v1 Announce Type: cross 
Abstract: Neural operators have emerged as powerful tools for learning solution operators of partial differential equations (PDEs). However, standard spectral methods based on Fourier transforms struggle with problems involving discontinuous coefficients due to the Gibbs phenomenon and poor representation of sharp interfaces. We introduce the Walsh-Hadamard Neural Operator (WHNO), which leverages Walsh-Hadamard transforms-a spectral basis of rectangular wave functions naturally suited for piecewise constant fields-combined with learnable spectral weights that transform low-sequency Walsh coefficients to capture global dependencies efficiently. We validate WHNO on three problems: steady-state Darcy flow (preliminary validation), heat conduction with discontinuous thermal conductivity, and the 2D Burgers equation with discontinuous initial conditions. In controlled comparisons with Fourier Neural Operators (FNO) under identical conditions, WHNO demonstrates superior accuracy with better preservation of sharp solution features at material interfaces. Critically, we discover that weighted ensemble combinations of WHNO and FNO achieve substantial improvements over either model alone: for both heat conduction and Burgers equation, optimal ensembles reduce mean squared error by 35-40 percent and maximum error by up to 25 percent compared to individual models. This demonstrates that Walsh-Hadamard and Fourier representations capture complementary aspects of discontinuous PDE solutions, with WHNO excelling at sharp interfaces while FNO captures smooth features effectively.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference-Time Scaling of Diffusion Models for Infrared Data Generation</title>
<link>https://arxiv.org/abs/2511.07362</link>
<guid>https://arxiv.org/abs/2511.07362</guid>
<content:encoded><![CDATA[

arXiv:2511.07362v1 Announce Type: cross 
Abstract: Infrared imagery enables temperature-based scene understanding using passive sensors, particularly under conditions of low visibility where traditional RGB imaging fails. Yet, developing downstream vision models for infrared applications is hindered by the scarcity of high-quality annotated data, due to the specialized expertise required for infrared annotation. While synthetic infrared image generation has the potential to accelerate model development by providing large-scale, diverse training data, training foundation-level generative diffusion models in the infrared domain has remained elusive due to limited datasets. In light of such data constraints, we explore an inference-time scaling approach using a domain-adapted CLIP-based verifier for enhanced infrared image generation quality. We adapt FLUX.1-dev, a state-of-the-art text-to-image diffusion model, to the infrared domain by finetuning it on a small sample of infrared images using parameter-efficient techniques. The trained verifier is then employed during inference to guide the diffusion sampling process toward higher quality infrared generations that better align with input text prompts. Empirically, we find that our approach leads to consistent improvements in generation quality, reducing FID scores on the KAIST Multispectral Pedestrian Detection Benchmark dataset by 10% compared to unguided baseline samples. Our results suggest that inference-time guidance offers a promising direction for bridging the domain gap in low-data infrared settings.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UAV-Assisted Resilience in 6G and Beyond Network Energy Saving: A Multi-Agent DRL Approach</title>
<link>https://arxiv.org/abs/2511.07366</link>
<guid>https://arxiv.org/abs/2511.07366</guid>
<content:encoded><![CDATA[

arXiv:2511.07366v1 Announce Type: cross 
Abstract: This paper investigates the unmanned aerial vehicle (UAV)-assisted resilience perspective in the 6G network energy saving (NES) scenario. More specifically, we consider multiple ground base stations (GBSs) and each GBS has three different sectors/cells in the terrestrial networks, and multiple cells are turned off due to NES or incidents, e.g., disasters, hardware failures, or outages. To address this, we propose a Multi-Agent Deep Deterministic Policy Gradient (MADDPG) framework to enable UAV-assisted communication by jointly optimizing UAV trajectories, transmission power, and user-UAV association under a sleeping ground base station (GBS) strategy. This framework aims to ensure the resilience of active users in the network and the long-term operability of UAVs. Specifically, it maximizes service coverage for users during power outages or NES zones, while minimizing the energy consumption of UAVs. Simulation results demonstrate that the proposed MADDPG policy consistently achieves high coverage ratio across different testing episodes, outperforming other baselines. Moreover, the MADDPG framework attains the lowest total energy consumption, with a reduction of approximately 24\% compared to the conventional all GBS ON configuration, while maintaining a comparable user service rate. These results confirm the effectiveness of the proposed approach in achieving a superior trade-off between energy efficiency and service performance, supporting the development of sustainable and resilient UAV-assisted cellular networks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence</title>
<link>https://arxiv.org/abs/2511.07384</link>
<guid>https://arxiv.org/abs/2511.07384</guid>
<content:encoded><![CDATA[

arXiv:2511.07384v1 Announce Type: cross 
Abstract: Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving bilevel optimization via sequential minimax optimization</title>
<link>https://arxiv.org/abs/2511.07398</link>
<guid>https://arxiv.org/abs/2511.07398</guid>
<content:encoded><![CDATA[

arXiv:2511.07398v1 Announce Type: cross 
Abstract: In this paper we propose a sequential minimax optimization (SMO) method for solving a class of constrained bilevel optimization problems in which the lower-level part is a possibly nonsmooth convex optimization problem, while the upper-level part is a possibly nonconvex optimization problem. Specifically, SMO applies a first-order method to solve a sequence of minimax subproblems, which are obtained by employing a hybrid of modified augmented Lagrangian and penalty schemes on the bilevel optimization problems. Under suitable assumptions, we establish an operation complexity of $O(\varepsilon^{-7}\log\varepsilon^{-1})$ and $O(\varepsilon^{-6}\log\varepsilon^{-1})$, measured in terms of fundamental operations, for SMO in finding an $\varepsilon$-KKT solution of the bilevel optimization problems with merely convex and strongly convex lower-level objective functions, respectively. The latter result improves the previous best-known operation complexity by a factor of $\varepsilon^{-1}$. Preliminary numerical results demonstrate significantly superior computational performance compared to the recently developed first-order penalty method.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation</title>
<link>https://arxiv.org/abs/2511.07399</link>
<guid>https://arxiv.org/abs/2511.07399</guid>
<content:encoded><![CDATA[

arXiv:2511.07399v1 Announce Type: cross 
Abstract: Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards</title>
<link>https://arxiv.org/abs/2511.07403</link>
<guid>https://arxiv.org/abs/2511.07403</guid>
<content:encoded><![CDATA[

arXiv:2511.07403v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DigiData: Training and Evaluating General-Purpose Mobile Control Agents</title>
<link>https://arxiv.org/abs/2511.07413</link>
<guid>https://arxiv.org/abs/2511.07413</guid>
<content:encoded><![CDATA[

arXiv:2511.07413v1 Announce Type: cross 
Abstract: AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Generation with Infinite Contamination</title>
<link>https://arxiv.org/abs/2511.07417</link>
<guid>https://arxiv.org/abs/2511.07417</guid>
<content:encoded><![CDATA[

arXiv:2511.07417v1 Announce Type: cross 
Abstract: We study language generation in the limit, where an algorithm observes an adversarial enumeration of strings from an unknown target language $K$ and must eventually generate new, unseen strings from $K$. Kleinberg and Mullainathan [KM24] proved that generation is achievable in surprisingly general settings. But their generator suffers from ``mode collapse,'' producing from an ever-smaller subset of the target. To address this, Kleinberg and Wei [KW25] require the generator's output to be ``dense'' in the target language. They showed that generation with density, surprisingly, remains achievable at the same generality.
  Both results assume perfect data: no noisy insertions and no omissions. This raises a central question: how much contamination can generation tolerate? Recent works made partial progress on this question by studying (non-dense) generation with either finite amounts of noise (but no omissions) or omissions (but no noise).
  We characterize robustness under contaminated enumerations: 1. Generation under Contamination: Language generation in the limit is achievable for all countable collections iff the fraction of contaminated examples converges to zero. When this fails, we characterize which collections are generable. 2. Dense Generation under Contamination: Dense generation is strictly less robust to contamination than generation. As a byproduct, we resolve an open question of Raman and Raman [ICML25] by showing that generation is possible with only membership oracle access under finitely many contaminated examples.
  Finally, we introduce a beyond-worst-case model inspired by curriculum learning and prove that dense generation is achievable even with infinite contamination provided the fraction of contaminated examples converges to zero. This suggests curriculum learning may be crucial for learning from noisy web data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A CNN-LSTM Quantifier for Single Access Point CSI Indoor Localization</title>
<link>https://arxiv.org/abs/2005.06394</link>
<guid>https://arxiv.org/abs/2005.06394</guid>
<content:encoded><![CDATA[

arXiv:2005.06394v2 Announce Type: replace 
Abstract: This paper proposes a combined network structure between convolutional neural network (CNN) and long-short term memory (LSTM) quantifier for WiFi fingerprinting indoor localization. In contrast to conventional methods that utilize only spatial data with classification models, our CNN-LSTM network extracts both space and time features of the received channel state information (CSI) from a single router. Furthermore, the proposed network builds a quantification model rather than a limited classification model as in most of the literature work, which enables the estimation of testing points that are not identical to the reference points. We analyze the instability of CSI and demonstrate a mitigation solution using a comprehensive filter and normalization scheme. The localization accuracy is investigated through extensive on-site experiments with several mobile devices including mobile phone (Nexus 5) and laptop (Intel 5300 NIC) on hundreds of testing locations. Using only a single WiFi router, our structure achieves an average localization error of 2.5~m with $\mathrm{80\%}$ of the errors under 4~m, which outperforms the other reported algorithms by approximately $\mathrm{50\%}$ under the same test environment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revenue Maximization and Learning in Products Ranking</title>
<link>https://arxiv.org/abs/2012.03800</link>
<guid>https://arxiv.org/abs/2012.03800</guid>
<content:encoded><![CDATA[

arXiv:2012.03800v3 Announce Type: replace 
Abstract: We consider the revenue maximization problem for an online retailer who plans to display in order a set of products differing in their prices and qualities. Consumers have attention spans, i.e., the maximum number of products they are willing to view, and inspect the products sequentially before purchasing a product or leaving the platform empty-handed when the attention span gets exhausted. Our framework extends the well-known cascade model in two directions: the consumers have random attention spans instead of fixed ones, and the firm maximizes revenues instead of clicking probabilities. We show a nested structure of the optimal product ranking as a function of the attention span when the attention span is fixed. \sg{Using this fact, we develop an approximation algorithm when only the distribution of the attention spans is given. Under mild conditions, it achieves $1/e$ of the revenue of the clairvoyant case when the realized attention span is known. We also show that no algorithms can achieve more than 0.5 of the revenue of the same benchmark. The model and the algorithm can be generalized to the ranking problem when consumers make multiple purchases.} When the conditional purchase probabilities are not known and may depend on consumer and product features, we devise an online learning algorithm that achieves $\tilde{\mathcal{O}}(\sqrt{T})$ regret relative to the approximation algorithm, despite the censoring of information: the attention span of a customer who purchases an item is not observable. Numerical experiments demonstrate the outstanding performance of the approximation and online learning algorithms.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2108.10346</link>
<guid>https://arxiv.org/abs/2108.10346</guid>
<content:encoded><![CDATA[

arXiv:2108.10346v2 Announce Type: replace 
Abstract: To advance the transparency of learning machines such as Deep Neural Networks (DNNs), the field of Explainable AI (XAI) was established to provide interpretations of DNNs' predictions. While different explanation techniques exist, a popular approach is given in the form of attribution maps, which illustrate, given a particular data point, the relevant patterns the model has used for making its prediction. Although Bayesian models such as Bayesian Neural Networks (BNNs) have a limited form of transparency built-in through their prior weight distribution, they lack explanations of their predictions for given instances. In this work, we take a step toward combining these two perspectives by examining how local attributions can be extended to BNNs. Within the Bayesian framework, network weights follow a probability distribution; hence, the standard point explanation extends naturally to an explanation distribution. Viewing explanations probabilistically, we aggregate and analyze multiple local attributions drawn from an approximate posterior to explore variability in explanation patterns. The diversity of explanations offers a way to further explore how predictive rationales may vary across posterior samples. Quantitative and qualitative experiments on toy and benchmark data, as well as on a real-world pathology dataset, illustrate that our framework enriches standard explanations with uncertainty information and may support the visualization of explanation stability.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impacts of Individual Fairness on Group Fairness from the Perspective of Generalized Entropy</title>
<link>https://arxiv.org/abs/2202.11966</link>
<guid>https://arxiv.org/abs/2202.11966</guid>
<content:encoded><![CDATA[

arXiv:2202.11966v4 Announce Type: replace 
Abstract: This paper investigates how the degree of group fairness changes when the degree of individual fairness is actively controlled. As a metric quantifying individual fairness, we consider generalized entropy (GE) recently introduced into machine learning community. To control the degree of individual fairness, we design a classification algorithm satisfying a given degree of individual fairness through an empirical risk minimization (ERM) with a fairness constraint specified in terms of GE. We show the PAC learnability of the fair ERM problem by proving that the true fairness degree does not deviate much from an empirical one with high probability for finite VC dimension if the sample size is big enough. Our experiments show that strengthening individual fairness degree does not always lead to enhancement of group fairness.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Corruptions of Supervised Learning Problems: Typology and Mitigations</title>
<link>https://arxiv.org/abs/2307.08643</link>
<guid>https://arxiv.org/abs/2307.08643</guid>
<content:encoded><![CDATA[

arXiv:2307.08643v3 Announce Type: replace 
Abstract: Corruption is notoriously widespread in data collection. Despite extensive research, the existing literature predominantly focuses on specific settings and learning scenarios, lacking a unified view of corruption modelization and mitigation. In this work, we develop a general theory of corruption, which incorporates all modifications to a supervised learning problem, including changes in model class and loss. Focusing on changes to the underlying probability distributions via Markov kernels, our approach leads to three novel opportunities. First, it enables the construction of a novel, provably exhaustive corruption framework, distinguishing among different corruption types. This serves to unify existing models and establish a consistent nomenclature. Second, it facilitates a systematic analysis of corruption's consequences on learning tasks, by comparing Bayes risks in the clean and corrupted scenarios. Notably, while label corruptions affect only the loss function, attribute corruptions additionally influence the hypothesis class. Third, building upon these results, we investigate mitigations for various corruption types. We expand existing loss-correction methods for label corruption to handle dependent corruption types. Our findings highlight the necessity to generalize the classical corruption-corrected learning framework to a new paradigm with weaker requirements to encompass more corruption types. We provide such a paradigm as well as loss correction formulas in the attribute and joint corruption cases.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weight-Entanglement Meets Gradient-Based Neural Architecture Search</title>
<link>https://arxiv.org/abs/2312.10440</link>
<guid>https://arxiv.org/abs/2312.10440</guid>
<content:encoded><![CDATA[

arXiv:2312.10440v2 Announce Type: replace 
Abstract: Weight sharing is a fundamental concept in neural architecture search (NAS), enabling gradient-based methods to explore cell-based architectural spaces significantly faster than traditional black-box approaches. In parallel, weight-entanglement has emerged as a technique for more intricate parameter sharing amongst macro-architectural spaces. Since weight-entanglement is not directly compatible with gradient-based NAS methods, these two paradigms have largely developed independently in parallel sub-communities. This paper aims to bridge the gap between these sub-communities by proposing a novel scheme to adapt gradient-based methods for weight-entangled spaces. This enables us to conduct an in-depth comparative assessment and analysis of the performance of gradient-based NAS in weight-entangled search spaces. Our findings reveal that this integration of weight-entanglement and gradient-based NAS brings forth the various benefits of gradient-based methods, while preserving the memory efficiency of weight-entangled spaces. The code for our work is openly accessible https://github.com/automl/TangleNAS.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss</title>
<link>https://arxiv.org/abs/2402.00152</link>
<guid>https://arxiv.org/abs/2402.00152</guid>
<content:encoded><![CDATA[

arXiv:2402.00152v3 Announce Type: replace 
Abstract: Constructing the architecture of a neural network is a challenging pursuit for the machine learning community, and the dilemma of whether to go deeper or wider remains a persistent question. This paper explores a comparison between deeper neural networks (DeNNs) with a flexible number of layers and wider neural networks (WeNNs) with limited hidden layers, focusing on their optimal generalization error in Sobolev losses. Analytical investigations reveal that the architecture of a neural network can be significantly influenced by various factors, including the number of sample points, parameters within the neural networks, and the regularity of the loss function. Specifically, a higher number of parameters tends to favor WeNNs, while an increased number of sample points and greater regularity in the loss function lean towards the adoption of DeNNs. We ultimately apply this theory to address partial differential equations using deep Ritz and physics-informed neural network (PINN) methods, guiding the design of neural networks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Posterior Sampling is Computationally Intractable</title>
<link>https://arxiv.org/abs/2402.12727</link>
<guid>https://arxiv.org/abs/2402.12727</guid>
<content:encoded><![CDATA[

arXiv:2402.12727v2 Announce Type: replace 
Abstract: Diffusion models are a remarkably effective way of learning and sampling from a distribution $p(x)$. In posterior sampling, one is also given a measurement model $p(y \mid x)$ and a measurement $y$, and would like to sample from $p(x \mid y)$. Posterior sampling is useful for tasks such as inpainting, super-resolution, and MRI reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time.
  In this paper we show that posterior sampling is computationally intractable: under the most basic assumption in cryptography -- that one-way functions exist -- there are instances for which every algorithm takes superpolynomial time, even though unconditional sampling is provably fast. We also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption that there are one-way functions that take exponential time to invert.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimization without Retraction on the Random Generalized Stiefel Manifold</title>
<link>https://arxiv.org/abs/2405.01702</link>
<guid>https://arxiv.org/abs/2405.01702</guid>
<content:encoded><![CDATA[

arXiv:2405.01702v4 Announce Type: replace 
Abstract: Optimization over the set of matrices $X$ that satisfy $X^\top B X = I_p$, referred to as the generalized Stiefel manifold, appears in many applications involving sampled covariance matrices such as the canonical correlation analysis (CCA), independent component analysis (ICA), and the generalized eigenvalue problem (GEVP). Solving these problems is typically done by iterative methods that require a fully formed $B$. We propose a cheap stochastic iterative method that solves the optimization problem while having access only to random estimates of $B$. Our method does not enforce the constraint in every iteration; instead, it produces iterations that converge to critical points on the generalized Stiefel manifold defined in expectation. The method has lower per-iteration cost, requires only matrix multiplications, and has the same convergence rates as its Riemannian optimization counterparts that require the full matrix $B$. Experiments demonstrate its effectiveness in various machine learning applications involving generalized orthogonality constraints, including CCA, ICA, and the GEVP.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\mu$LO: Compute-Efficient Meta-Generalization of Learned Optimizers</title>
<link>https://arxiv.org/abs/2406.00153</link>
<guid>https://arxiv.org/abs/2406.00153</guid>
<content:encoded><![CDATA[

arXiv:2406.00153v4 Announce Type: replace 
Abstract: Learned optimizers (LOs) have the potential to significantly reduce the wall-clock training time of neural networks. However, they can struggle to optimize unseen tasks (\emph{meta-generalize}), especially when training networks wider than those seen during meta-training. To address this, we derive the Maximal Update Parametrization ($\mu$P) for two state-of-the-art learned optimizer architectures and propose a simple meta-training recipe for $\mu$-parameterized LOs ($\mu$LOs). Our empirical evaluation demonstrates that LOs meta-trained with our recipe substantially improve meta-generalization to wider unseen tasks when compared to LOs trained under standard parametrization (SP) using the same compute budget. We also empirically observe that $\mu$LOs exhibit unexpectedly improved meta-generalization to deeper networks ($5\times$ meta-training) and surprising generalization to much longer training horizons ($25\times$ meta-training) when compared to SP LOs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GC4NC: A Benchmark Framework for Graph Condensation on Node Classification with New Insights</title>
<link>https://arxiv.org/abs/2406.16715</link>
<guid>https://arxiv.org/abs/2406.16715</guid>
<content:encoded><![CDATA[

arXiv:2406.16715v3 Announce Type: replace 
Abstract: Graph condensation (GC) is an emerging technique designed to learn a significantly smaller graph that retains the essential information of the original graph. This condensed graph has shown promise in accelerating graph neural networks while preserving performance comparable to those achieved with the original, larger graphs. Additionally, this technique facilitates downstream applications like neural architecture search and deepens our understanding of redundancies in large graphs. Despite the rapid development of GC methods, particularly for node classification, a unified evaluation framework is still lacking to systematically compare different GC methods or clarify key design choices for improving their effectiveness. To bridge these gaps, we introduce \textbf{GC4NC}, a comprehensive framework for evaluating diverse GC methods on node classification across multiple dimensions including performance, efficiency, privacy preservation, denoising ability, NAS effectiveness, and transferability. Our systematic evaluation offers novel insights into how condensed graphs behave and the critical design choices that drive their success. These findings pave the way for future advancements in GC methods, enhancing both performance and expanding their real-world applications. Our code is available at https://github.com/Emory-Melody/GraphSlim/tree/main/benchmark.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addressing Polarization and Unfairness in Performative Prediction</title>
<link>https://arxiv.org/abs/2406.16756</link>
<guid>https://arxiv.org/abs/2406.16756</guid>
<content:encoded><![CDATA[

arXiv:2406.16756v2 Announce Type: replace 
Abstract: In many real-world applications of machine learning such as recommendations, hiring, and lending, deployed models influence the data they are trained on, leading to feedback loops between predictions and data distribution. The performative prediction (PP) framework captures this phenomenon by modeling the data distribution as a function of the deployed model. While prior work has focused on finding performative stable (PS) solutions for robustness, their societal impacts, particularly regarding fairness, remain underexplored. We show that PS solutions can lead to severe polarization and prediction performance disparities, and that conventional fairness interventions in previous works often fail under model-dependent distribution shifts due to failing the PS criteria. To address these challenges in PP, we introduce novel fairness mechanisms that provably ensure both stability and fairness, validated by theoretical analysis and empirical results.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference-Guided Reinforcement Learning for Efficient Exploration</title>
<link>https://arxiv.org/abs/2407.06503</link>
<guid>https://arxiv.org/abs/2407.06503</guid>
<content:encoded><![CDATA[

arXiv:2407.06503v2 Announce Type: replace 
Abstract: In this paper, we investigate preference-based reinforcement learning (PbRL), which enables reinforcement learning (RL) agents to learn from human feedback. This is particularly valuable when defining a fine-grain reward function is not feasible. However, this approach is inefficient and impractical for promoting deep exploration in hard-exploration tasks with long horizons and sparse rewards. To tackle this issue, we introduce LOPE: \textbf{L}earning \textbf{O}nline with trajectory \textbf{P}reference guidanc\textbf{E}, an end-to-end preference-guided RL framework that enhances exploration efficiency in hard-exploration tasks. Our intuition is that LOPE directly adjusts the focus of online exploration by considering human feedback as guidance, thereby avoiding the need to learn a separate reward model from preferences. Specifically, LOPE includes a two-step sequential policy optimization technique consisting of trust-region-based policy improvement and preference guidance steps. We reformulate preference guidance as a trajectory-wise state marginal matching problem that minimizes the maximum mean discrepancy distance between the preferred trajectories and the learned policy. Furthermore, we provide a theoretical analysis to characterize the performance improvement bound and evaluate the effectiveness of the LOPE. When assessed in various challenging hard-exploration environments, LOPE outperforms several state-of-the-art methods in terms of convergence rate and overall performance.The code used in this study is available at https://github.com/buaawgj/LOPE.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Training for Enhanced Multi-task Generalization in Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.13567</link>
<guid>https://arxiv.org/abs/2408.13567</guid>
<content:encoded><![CDATA[

arXiv:2408.13567v2 Announce Type: replace 
Abstract: In multi-agent reinforcement learning (MARL), achieving multi-task generalization to diverse agents and objectives presents significant challenges. Existing online MARL algorithms primarily focus on single-task performance, but their lack of multi-task generalization capabilities typically results in substantial computational waste and limited real-life applicability. Meanwhile, existing offline multi-task MARL approaches are heavily dependent on data quality, often resulting in poor performance on unseen tasks. In this paper, we introduce HyGen, a novel hybrid MARL framework, Hybrid Training for Enhanced Multi-Task Generalization, which integrates online and offline learning to ensure both multi-task generalization and training efficiency. Specifically, our framework extracts potential general skills from offline multi-task datasets. We then train policies to select the optimal skills under the centralized training and decentralized execution paradigm (CTDE). During this stage, we utilize a replay buffer that integrates both offline data and online interactions. We empirically demonstrate that our framework effectively extracts and refines general skills, yielding impressive generalization to unseen tasks. Comparative analyses on the StarCraft multi-agent challenge show that HyGen outperforms a wide range of existing solely online and offline methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Condensed Data Expansion Using Model Inversion for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2408.13850</link>
<guid>https://arxiv.org/abs/2408.13850</guid>
<content:encoded><![CDATA[

arXiv:2408.13850v2 Announce Type: replace 
Abstract: Condensed datasets offer a compact representation of larger datasets, but training models directly on them or using them to enhance model performance through knowledge distillation (KD) can result in suboptimal outcomes due to limited information. To address this, we propose a method that expands condensed datasets using model inversion, a technique for generating synthetic data based on the impressions of a pre-trained model on its training data. This approach is particularly well-suited for KD scenarios, as the teacher model is already pre-trained and retains knowledge of the original training data. By creating synthetic data that complements the condensed samples, we enrich the training set and better approximate the underlying data distribution, leading to improvements in student model accuracy during knowledge distillation. Our method demonstrates significant gains in KD accuracy compared to using condensed datasets alone and outperforms standard model inversion-based KD methods by up to 11.4% across various datasets and model architectures. Importantly, it remains effective even when using as few as one condensed sample per class, and can also enhance performance in few-shot scenarios where only limited real data samples are available.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MENSA: A Multi-Event Network for Survival Analysis with Trajectory-based Likelihood Estimation</title>
<link>https://arxiv.org/abs/2409.06525</link>
<guid>https://arxiv.org/abs/2409.06525</guid>
<content:encoded><![CDATA[

arXiv:2409.06525v5 Announce Type: replace 
Abstract: Most existing time-to-event methods focus on either single-event or competing-risks settings, leaving multi-event scenarios relatively underexplored. In many healthcare applications, for example, a patient may experience multiple clinical events, that can be non-exclusive and semi-competing. A common workaround is to train independent single-event models for such multi-event problems, but this approach fails to exploit dependencies and shared structures across events. To overcome these limitations, we propose MENSA (Multi-Event Network for Survival Analysis), a deep learning model that jointly learns flexible time-to-event distributions for multiple events, whether competing or co-occurring. In addition, we introduce a novel trajectory-based likelihood term that captures the temporal ordering between events. Across four multi-event datasets, MENSA improves predictive performance over many state-of-the-art baselines. Source code is available at https://github.com/thecml/mensa.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards</title>
<link>https://arxiv.org/abs/2409.15922</link>
<guid>https://arxiv.org/abs/2409.15922</guid>
<content:encoded><![CDATA[

arXiv:2409.15922v5 Announce Type: replace 
Abstract: While Vision-Language Models (VLMs) are increasingly used to generate reward signals for training embodied agents to follow instructions, our research reveals that agents guided by VLM rewards often underperform compared to those employing only intrinsic (exploration-driven) rewards, contradicting expectations set by recent work. We hypothesize that false positive rewards -- instances where unintended trajectories are incorrectly rewarded -- are more detrimental than false negatives. Our analysis confirms this hypothesis, revealing that the widely used cosine similarity metric is prone to false positive reward estimates. To address this, we introduce BiMI ({Bi}nary {M}utual {I}nformation), a novel reward function designed to mitigate noise. BiMI significantly enhances learning efficiency across diverse and challenging embodied navigation environments. Our findings offer a nuanced understanding of how different types of reward noise impact agent learning and highlight the importance of addressing multimodal reward signal noise when training embodied agents
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedMAC: Tackling Partial-Modality Missing in Federated Learning with Cross-Modal Aggregation and Contrastive Regularization</title>
<link>https://arxiv.org/abs/2410.03070</link>
<guid>https://arxiv.org/abs/2410.03070</guid>
<content:encoded><![CDATA[

arXiv:2410.03070v2 Announce Type: replace 
Abstract: Federated Learning (FL) is a method for training machine learning models using distributed data sources. It ensures privacy by allowing clients to collaboratively learn a shared global model while storing their data locally. However, a significant challenge arises when dealing with missing modalities in clients' datasets, where certain features or modalities are unavailable or incomplete, leading to heterogeneous data distribution. While previous studies have addressed the issue of complete-modality missing, they fail to tackle partial-modality missing on account of severe heterogeneity among clients at an instance level, where the pattern of missing data can vary significantly from one sample to another. To tackle this challenge, this study proposes a novel framework named FedMAC, designed to address multi-modality missing under conditions of partial-modality missing in FL. Additionally, to avoid trivial aggregation of multi-modal features, we introduce contrastive-based regularization to impose additional constraints on the latent representation space. The experimental results demonstrate the effectiveness of FedMAC across various client configurations with statistical heterogeneity, outperforming baseline methods by up to 26% in severe missing scenarios, highlighting its potential as a solution for the challenge of partially missing modalities in federated systems. Our source code is provided at https://github.com/nmduonggg/PEPSY
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning - A Convex Optimization Perspective</title>
<link>https://arxiv.org/abs/2410.15483</link>
<guid>https://arxiv.org/abs/2410.15483</guid>
<content:encoded><![CDATA[

arXiv:2410.15483v4 Announce Type: replace 
Abstract: The post-training of LLMs, which typically consists of the supervised fine-tuning (SFT) stage and the preference learning stage (RLHF or DPO), is crucial to effective and safe LLM applications. The widely adopted approach in post-training popular open-source LLMs is to sequentially perform SFT and RLHF/DPO. However, this is suboptimal in terms of SFT and RLHF/DPO trade-off: the LLM gradually forgets about the first stage's training when undergoing the second stage's training. This sequential paradigm persists largely due to its simplicity and modularity, which make it easier to implement and manage at scale despite its limitations. We theoretically prove the sub-optimality of sequential post-training and propose a practical joint post-training framework which has theoretical convergence guarantees and empirically outperforms sequential post-training framework, with up to 23% overall performance improvement across multiple LLM evaluation benchmarks, while having minimal computational overhead. Our code is available at https://github.com/heshandevaka/XRIGHT.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Large Language Models with Integer Sequence Generation Tasks</title>
<link>https://arxiv.org/abs/2411.04372</link>
<guid>https://arxiv.org/abs/2411.04372</guid>
<content:encoded><![CDATA[

arXiv:2411.04372v3 Announce Type: replace 
Abstract: We present a novel benchmark designed to rigorously evaluate the capabilities of large language models (LLMs) in mathematical reasoning and algorithmic code synthesis tasks. The benchmark comprises integer sequence generation tasks sourced from the Online Encyclopedia of Integer Sequences (OEIS), testing LLMs' abilities to accurately and efficiently generate Python code to compute these sequences without using lookup tables. Our comprehensive evaluation includes leading models from OpenAI (including the specialized reasoning-focused o-series), Anthropic, Meta, and Google across a carefully selected set of 1000 OEIS sequences categorized as ``easy'' or ``hard.'' Half of these sequences are classical sequences from the early days of OEIS and half were recently added to avoid contamination with the models' training data. To prevent models from exploiting memorized sequence values, we introduce an automated cheating detection mechanism that flags usage of lookup tables, validated by comparison with human expert evaluations. Experimental results demonstrate that reasoning-specialized models (o3, o3-mini, o4-mini from OpenAI, and Gemini 2.5-pro from Google) achieve substantial improvements in accuracy over non-reasoning models, especially on more complex tasks. However, overall model performance on the hard sequences is poor, highlighting persistent challenges in algorithmic reasoning. Our benchmark provides important insights into the strengths and limitations of state-of-the-art LLMs, particularly emphasizing the necessity for further advancements to reliably solve complex mathematical reasoning tasks algorithmically.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training</title>
<link>https://arxiv.org/abs/2411.07066</link>
<guid>https://arxiv.org/abs/2411.07066</guid>
<content:encoded><![CDATA[

arXiv:2411.07066v4 Announce Type: replace 
Abstract: Network pruning focuses on algorithms that aim to reduce a given model's computational cost by removing a subset of its parameters while having minimal impact on performance. Throughout the last decade, the most widely used pruning paradigm has been pruning and re-training, which nowadays is inconvenient due to the vast amount of pre-trained models, which are, in any case, too expensive to re-train. In this paper, we exploit functional information from dense pre-trained models, i.e., their input activations, to obtain sparse models that maximize the activations' alignment with respect to their corresponding dense models. Hence, we propose \textbf{NeuroAl}, a \emph{top-up} algorithm that can be used on top of any given pruning algorithm for LLMs, which modifies the block-wise and row-wise sparsity, exploiting information from both the dense model and its sparse version to maximize the \emph{neuron alignment} among activations. Different from existing methods, our approach adaptively selects the best hyperparameters for the block-wise and row-wise sparsity ratios w.r.t. the model and the desired sparsity, and requires \emph{no re-training}. We test our method over $\sim$300 test cases with four LLM families, three sparsity ratios, and ten language tasks (three language modeling and seven zero-shot datasets), showing how it consistently outperforms the latest state-of-the-art methods in terms of performance-runtime trade-off. The code is available at \href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Convergence of Continual Federated Learning Using Incrementally Aggregated Gradients</title>
<link>https://arxiv.org/abs/2411.07959</link>
<guid>https://arxiv.org/abs/2411.07959</guid>
<content:encoded><![CDATA[

arXiv:2411.07959v3 Announce Type: replace 
Abstract: The holy grail of machine learning is to enable Continual Federated Learning (CFL) to enhance the efficiency, privacy, and scalability of AI systems while learning from streaming data. The primary challenge of a CFL system is to overcome global catastrophic forgetting, wherein the accuracy of the global model trained on new tasks declines on the old tasks. In this work, we propose Continual Federated Learning with Aggregated Gradients (C-FLAG), a novel replay-memory based federated strategy consisting of edge-based gradient updates on memory and aggregated gradients on the current data. We provide convergence analysis of the C-FLAG approach which addresses forgetting and bias while converging at a rate of $O(1/\sqrt{T})$ over $T$ communication rounds. We formulate an optimization sub-problem that minimizes catastrophic forgetting, translating CFL into an iterative algorithm with adaptive learning rates that ensure seamless learning across tasks. We empirically show that C-FLAG outperforms several state-of-the-art baselines on both task and class-incremental settings with respect to metrics such as accuracy and forgetting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Group Robust Ensemble Knowledge Distillation</title>
<link>https://arxiv.org/abs/2411.14984</link>
<guid>https://arxiv.org/abs/2411.14984</guid>
<content:encoded><![CDATA[

arXiv:2411.14984v2 Announce Type: replace 
Abstract: Neural networks can learn spurious correlations in the data, often leading to performance degradation for underrepresented subgroups. Studies have demonstrated that the disparity is amplified when knowledge is distilled from a complex teacher model to a relatively ``simple'' student model. Prior work has shown that ensemble deep learning methods can improve the performance of the worst-case subgroups; however, it is unclear if this advantage carries over when distilling knowledge from an ensemble of teachers, especially when the teacher models are debiased. This study demonstrates that traditional ensemble knowledge distillation can significantly drop the performance of the worst-case subgroups in the distilled student model even when the teacher models are debiased. To overcome this, we propose Adaptive Group Robust Ensemble Knowledge Distillation (AGRE-KD), a simple ensembling strategy to ensure that the student model receives knowledge beneficial for unknown underrepresented subgroups. Leveraging an additional biased model, our method selectively chooses teachers whose knowledge would better improve the worst-performing subgroups by upweighting the teachers with gradient directions deviating from the biased model. Our experiments on several datasets demonstrate the superiority of the proposed ensemble distillation technique and show that it can even outperform classic model ensembles based on majority voting. Our source code is available at https://github.com/patrikken/AGRE-KD
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Machine Learning to Discover Parsimonious and Physically-Interpretable Representations of Catchment-Scale Rainfall-Runoff Dynamics</title>
<link>https://arxiv.org/abs/2412.04845</link>
<guid>https://arxiv.org/abs/2412.04845</guid>
<content:encoded><![CDATA[

arXiv:2412.04845v5 Announce Type: replace 
Abstract: Due largely to challenges associated with physical interpretability of machine learning (ML) methods, and because model interpretability is key to credibility in management applications, many scientists and practitioners are hesitant to discard traditional physical-conceptual (PC) modeling approaches despite their poorer predictive performance. Here, we examine how to develop parsimonious minimally-optimal representations that can facilitate better insight regarding system functioning. The term minimally-optimal indicates that the desired outcome can be achieved with the smallest possible effort and resources, while parsimony is widely held to support understanding. Accordingly, we suggest that ML-based modeling should use computational units that are inherently physically-interpretable, and explore how generic network architectures comprised of Mass-Conserving-Perceptron can be used to model dynamical systems in a physically-interpretable manner.
  In the context of spatially-lumped catchment-scale modeling, we find that both physical interpretability and good predictive performance can be achieved using a distributed-state network with context-dependent gating and information sharing across nodes. The distributed-state mechanism ensures a sufficient number of temporally-evolving properties of system storage while information-sharing ensures proper synchronization of such properties. The results indicate that MCP-based ML models with only a few layers (up to two) and relativity few physical flow pathways (up to three) can play a significant role in ML-based streamflow modelling.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Mitigating Memorization in Diffusion Models for Tabular Data</title>
<link>https://arxiv.org/abs/2412.11044</link>
<guid>https://arxiv.org/abs/2412.11044</guid>
<content:encoded><![CDATA[

arXiv:2412.11044v2 Announce Type: replace 
Abstract: Tabular data generation has attracted significant research interest in recent years, with the tabular diffusion models greatly improving the quality of synthetic data. However, while memorization, where models inadvertently replicate exact or near-identical training data, has been thoroughly investigated in image and text generation, its effects on tabular data remain largely unexplored. In this paper, we conduct the first comprehensive investigation of memorization phenomena in diffusion models for tabular data. Our empirical analysis reveals that memorization appears in tabular diffusion models and increases with larger training epochs. We further examine the influence of factors such as dataset sizes, feature dimensions, and different diffusion models on memorization. Additionally, we provide a theoretical explanation for why memorization occurs in tabular diffusion models. To address this issue, we propose TabCutMix, a simple yet effective data augmentation technique that exchanges randomly selected feature segments between random same-class training sample pairs. Building upon this, we introduce TabCutMixPlus, an enhanced method that clusters features based on feature correlations and ensures that features within the same cluster are exchanged together during augmentation. This clustering mechanism mitigates out-of-distribution (OOD) generation issues by maintaining feature coherence. Experimental results across various datasets and diffusion models demonstrate that TabCutMix effectively mitigates memorization while maintaining high-quality data generation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A solvable model of learning generative diffusion: theory and insights</title>
<link>https://arxiv.org/abs/2501.03937</link>
<guid>https://arxiv.org/abs/2501.03937</guid>
<content:encoded><![CDATA[

arXiv:2501.03937v2 Announce Type: replace 
Abstract: In this manuscript, we consider the problem of learning a flow or diffusion-based generative model parametrized by a two-layer auto-encoder, trained with online stochastic gradient descent, on a high-dimensional target density with an underlying low-dimensional manifold structure. We derive a tight asymptotic characterization of low-dimensional projections of the distribution of samples generated by the learned model, ascertaining in particular its dependence on the number of training samples. Building on this analysis, we discuss how mode collapse can arise, and lead to model collapse when the generative model is re-trained on generated synthetic data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>pMixFed: Efficient Personalized Federated Learning through Adaptive Layer-Wise Mixup</title>
<link>https://arxiv.org/abs/2501.11002</link>
<guid>https://arxiv.org/abs/2501.11002</guid>
<content:encoded><![CDATA[

arXiv:2501.11002v2 Announce Type: replace 
Abstract: Traditional Federated Learning (FL) methods encounter significant challenges when dealing with heterogeneous data and providing personalized solutions for non-IID scenarios. Personalized Federated Learning (PFL) approaches aim to address these issues by balancing generalization and personalization, often through parameter decoupling or partial models that freeze some neural network layers for personalization while aggregating other layers globally. However, existing methods still face challenges of global-local model discrepancy, client drift, and catastrophic forgetting, which degrade model accuracy. To overcome these limitations, we propose $\textit{pMixFed}$, a dynamic, layer-wise PFL approach that integrates $\textit{mixup}$ between shared global and personalized local models. Our method introduces an adaptive strategy for partitioning between personalized and shared layers, a gradual transition of personalization degree to enhance local client adaptation, improved generalization across clients, and a novel aggregation mechanism to mitigate catastrophic forgetting. Extensive experiments demonstrate that pMixFed outperforms state-of-the-art PFL methods, showing faster model training, increased robustness, and improved handling of data heterogeneity under different heterogeneous settings.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HumorReject: Decoupling LLM Safety from Refusal Prefix via A Little Humor</title>
<link>https://arxiv.org/abs/2501.13677</link>
<guid>https://arxiv.org/abs/2501.13677</guid>
<content:encoded><![CDATA[

arXiv:2501.13677v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) commonly rely on explicit refusal prefixes for safety, making them vulnerable to prefix injection attacks. We introduce HumorReject, a novel data-driven approach that reimagines LLM safety by decoupling it from refusal prefixes through humor as an indirect refusal strategy. Rather than explicitly rejecting harmful instructions, HumorReject responds with contextually appropriate humor that naturally defuses potentially dangerous requests. Our approach effectively addresses common "over-defense" issues while demonstrating superior robustness against various attack vectors. Our findings suggest that improvements in training data design can be as important as the alignment algorithm itself in achieving effective LLM safety. The code and dataset are available at https://github.com/wooozihui/HumorReject.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributional Surgery for Language Model Activations</title>
<link>https://arxiv.org/abs/2501.15758</link>
<guid>https://arxiv.org/abs/2501.15758</guid>
<content:encoded><![CDATA[

arXiv:2501.15758v2 Announce Type: replace 
Abstract: Language models, while capable of generating remarkably coherent and seemingly accurate text, can occasionally produce undesirable content, including harmful or toxic outputs. In this paper, we present a new two-stage approach to detect and mitigate undesirable content generations by rectifying activations. First, we train an ensemble of layerwise classifiers to detect undesirable content using activations by minimizing a smooth surrogate of the risk-aware score. Then, for detected undesirable contents, we propose layerwise distributional steering policies that transform the attention heads. These policies are computed through principled semidefinite programming, which aims to minimally perturb the attention distribution while probabilistically guaranteeing the effectiveness of the editions. Empirical evaluations across multiple language models and datasets show that our method outperforms baselines in reducing the generation of undesirable output.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperSHAP: Shapley Values and Interactions for Explaining Hyperparameter Optimization</title>
<link>https://arxiv.org/abs/2502.01276</link>
<guid>https://arxiv.org/abs/2502.01276</guid>
<content:encoded><![CDATA[

arXiv:2502.01276v2 Announce Type: replace 
Abstract: Hyperparameter optimization (HPO) is a crucial step in achieving strong predictive performance. Yet, the impact of individual hyperparameters on model generalization is highly context-dependent, prohibiting a one-size-fits-all solution and requiring opaque HPO methods to find optimal configurations. However, the black-box nature of most HPO methods undermines user trust and discourages adoption. To address this, we propose a game-theoretic explainability framework for HPO based on Shapley values and interactions. Our approach provides an additive decomposition of a performance measure across hyperparameters, enabling local and global explanations of hyperparameters' contributions and their interactions. The framework, named HyperSHAP, offers insights into ablation studies, the tunability of learning algorithms, and optimizer behavior across different hyperparameter spaces. We demonstrate HyperSHAP's capabilities on various HPO benchmarks to analyze the interaction structure of the corresponding HPO problems, demonstrating its broad applicability and actionable insights for improving HPO.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Choose Your Model Size: Any Compression of Large Language Models Without Re-Computation</title>
<link>https://arxiv.org/abs/2502.01717</link>
<guid>https://arxiv.org/abs/2502.01717</guid>
<content:encoded><![CDATA[

arXiv:2502.01717v2 Announce Type: replace 
Abstract: The adoption of Foundation Models in resource-constrained environments remains challenging due to their large size and inference costs. A promising way to overcome these limitations is post-training compression, which aims to balance reduced model size against performance degradation. This work presents Any Compression via Iterative Pruning (ACIP), a novel algorithmic approach to determine a compression-performance trade-off from a single stochastic gradient descent run. To achieve parameter efficiency, we use an SVD-reparametrization of linear layers and iteratively prune their singular values with a sparsity-inducing penalty. Importantly, the pruning order of the parameters is used to derive a global score map that allows compressing a model to any target size without re-computation. We evaluate ACIP on a large selection of open-weight LLMs and downstream tasks, demonstrating state-of-the-art results compared to existing factorization-based compression methods. We also show that ACIP seamlessly complements common quantization-based compression techniques.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tight Bounds for Jensen's Gap with Applications to Variational Inference</title>
<link>https://arxiv.org/abs/2502.03988</link>
<guid>https://arxiv.org/abs/2502.03988</guid>
<content:encoded><![CDATA[

arXiv:2502.03988v2 Announce Type: replace 
Abstract: Since its original formulation, Jensen's inequality has played a fundamental role across mathematics, statistics, and machine learning, with its probabilistic version highlighting the nonnegativity of the so-called Jensen's gap, i.e., the difference between the expectation of a convex function and the function at the expectation. Of particular importance is the case when the function is logarithmic, as this setting underpins many applications in variational inference, where the term variational gap is often used interchangeably. Recent research has focused on estimating the size of Jensen's gap and establishing tight lower and upper bounds under various assumptions on the underlying function and distribution, driven by practical challenges such as the intractability of log-likelihood in graphical models like variational autoencoders (VAEs). In this paper, we propose new, general bounds for Jensen's gap that accommodate a broad range of assumptions on both the function and the random variable, with special attention to exponential and logarithmic cases. We provide both analytical and empirical evidence for the performance of our method. Furthermore, we relate our bounds to the PAC-Bayes framework, providing new insights into generalization performance in probabilistic models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Representation Dynamics of Diffusion Models via Low-Dimensional Modeling</title>
<link>https://arxiv.org/abs/2502.05743</link>
<guid>https://arxiv.org/abs/2502.05743</guid>
<content:encoded><![CDATA[

arXiv:2502.05743v3 Announce Type: replace 
Abstract: Diffusion models, though originally designed for generative tasks, have demonstrated impressive self-supervised representation learning capabilities. A particularly intriguing phenomenon in these models is the emergence of unimodal representation dynamics, where the quality of learned features peaks at an intermediate noise level. In this work, we conduct a comprehensive theoretical and empirical investigation of this phenomenon. Leveraging the inherent low-dimensionality structure of image data, we theoretically demonstrate that the unimodal dynamic emerges when the diffusion model successfully captures the underlying data distribution. The unimodality arises from an interplay between denoising strength and class confidence across noise scales. Empirically, we further show that, in classification tasks, the presence of unimodal dynamics reliably reflects the generalization of the diffusion model: it emerges when the model generates novel images and gradually transitions to a monotonically decreasing curve as the model begins to memorize the training data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPIRIT: Short-term Prediction of solar IRradIance for zero-shot Transfer learning using Foundation Models</title>
<link>https://arxiv.org/abs/2502.10307</link>
<guid>https://arxiv.org/abs/2502.10307</guid>
<content:encoded><![CDATA[

arXiv:2502.10307v2 Announce Type: replace 
Abstract: Traditional solar forecasting models are based on several years of site-specific historical irradiance data, often spanning five or more years, which are unavailable for newer photovoltaic farms. As renewable energy is highly intermittent, building accurate solar irradiance forecasting systems is essential for efficient grid management and enabling the ongoing proliferation of solar energy, which is crucial to achieve the United Nations' net zero goals. In this work, we propose SPIRIT, a novel approach leveraging foundation models for solar irradiance forecasting, making it applicable to newer solar installations. Our approach outperforms state-of-the-art models in zero-shot transfer learning by about 70%, enabling effective performance at new locations without relying on any historical data. Further improvements in performance are achieved through fine-tuning, as more location-specific data becomes available. These findings are supported by statistical significance, further validating our approach. SPIRIT represents a pivotal step towards rapid, scalable, and adaptable solar forecasting solutions, advancing the integration of renewable energy into global power systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrustChain: A Blockchain Framework for Auditing and Verifying Aggregators in Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2502.16406</link>
<guid>https://arxiv.org/abs/2502.16406</guid>
<content:encoded><![CDATA[

arXiv:2502.16406v2 Announce Type: replace 
Abstract: The server-less nature of Decentralized Federated Learning (DFL) requires allocating the aggregation role to specific participants in each federated round. Current DFL architectures ensure the trustworthiness of the aggregator node upon selection. However, most of these studies overlook the possibility that the aggregating node may turn rogue and act maliciously after being nominated. To address this problem, this paper proposes a DFL structure, called TrustChain, that scores the aggregators before selection based on their past behavior and additionally audits them after the aggregation. To do this, the statistical independence between the client updates and the aggregated model is continuously monitored using the Hilbert-Schmidt Independence Criterion (HSIC). The proposed method relies on several principles, including blockchain, anomaly detection, and concept drift analysis. The designed structure is evaluated on several federated datasets and attack scenarios with different numbers of Byzantine nodes.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Loss Function for Deep Learning Based Daily Stock Trading System</title>
<link>https://arxiv.org/abs/2502.17493</link>
<guid>https://arxiv.org/abs/2502.17493</guid>
<content:encoded><![CDATA[

arXiv:2502.17493v2 Announce Type: replace 
Abstract: Making consistently profitable financial decisions in a continuously evolving and volatile stock market has always been a difficult task. Professionals from different disciplines have developed foundational theories to anticipate price movement and evaluate securities such as the famed Capital Asset Pricing Model (CAPM). In recent years, the role of artificial intelligence (AI) in asset pricing has been growing. Although the black-box nature of deep learning models lacks interpretability, they have continued to solidify their position in the financial industry. We aim to further enhance AI's potential and utility by introducing a return-weighted loss function that will drive top growth while providing the ML models a limited amount of information. Using only publicly accessible stock data (open/close/high/low, trading volume, sector information) and several technical indicators constructed from them, we propose an efficient daily trading system that detects top growth opportunities. Our best models achieve 61.73\% annual return on daily rebalancing with an annualized Sharpe Ratio of 1.18 over 1340 testing days from 2019 to 2024, and 37.61\% annual return with an annualized Sharpe Ratio of 0.97 over 1360 testing days from 2005 to 2010. The main drivers for success, especially independent of any domain knowledge, are the novel return-weighted loss function, the integration of categorical and continuous data, and the ML model architecture. We also demonstrate the superiority of our novel loss function over traditional loss functions via several performance metrics and statistical evidence.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weak-to-Strong Generalization Even in Random Feature Networks, Provably</title>
<link>https://arxiv.org/abs/2503.02877</link>
<guid>https://arxiv.org/abs/2503.02877</guid>
<content:encoded><![CDATA[

arXiv:2503.02877v3 Announce Type: replace 
Abstract: Weak-to-Strong Generalization (Burns et al., 2024) is the phenomenon whereby a strong student, say GPT-4, learns a task from a weak teacher, say GPT-2, and ends up significantly outperforming the teacher. We show that this phenomenon does not require a strong learner like GPT-4. We consider student and teacher that are random feature models, described by two-layer networks with a random and fixed bottom layer and a trained top layer. A "weak" teacher, with a small number of units (i.e. random features), is trained on the population, and a "strong" student, with a much larger number of units (i.e. random features), is trained only on labels generated by the weak teacher. We demonstrate, prove, and understand how the student can outperform the teacher, even though trained only on data labeled by the teacher. We also explain how such weak-to-strong generalization is enabled by early stopping. Importantly, we also show the quantitative limits of weak-to-strong generalization in this model.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Pre-training of MoEs: How robust is your router?</title>
<link>https://arxiv.org/abs/2503.05029</link>
<guid>https://arxiv.org/abs/2503.05029</guid>
<content:encoded><![CDATA[

arXiv:2503.05029v2 Announce Type: replace 
Abstract: Sparsely-activated Mixture of Experts (MoE) transformers are promising architectures for foundation models. Compared to dense transformers that require the same amount of floating-point operations (FLOPs) per forward pass, MoEs benefit from improved sample efficiency at training time and achieve much stronger performance. Many closed-source and open-source frontier language models have thus adopted an MoE architecture. Naturally, practitioners will want to extend the capabilities of these models with large amounts of newly collected data without completely re-training them. Prior work has shown that a simple combination of replay, learning rate re-warming, and re-decaying can enable the continual pre-training (CPT) of dense decoder-only transformers with minimal performance degradation compared to full re-training. In the case of decoder-only MoE transformers, however, it is unclear how the routing algorithm will impact continual pre-training performance: 1) do the MoE transformer's routers exacerbate forgetting relative to a dense model?; 2) do the routers maintain a balanced load on previous distributions after CPT?; 3) are the same strategies applied to dense models sufficient to continually pre-train MoE LLMs? In what follows, we conduct a large-scale study training a 500M parameter dense transformer and four 500M-active/2B-total parameter MoE transformers. Each model is trained for 600B tokens. Our results establish a surprising robustness to distribution shifts for MoEs using both Sinkhorn-Balanced and Z-and-Aux-loss-balanced routing algorithms, even in MoEs continually pre-trained without replay. Moreover, we show that MoE LLMs maintain their sample efficiency (relative to a FLOP-matched dense model) during CPT and that they can match the performance of a fully re-trained MoE at a fraction of the cost.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Network Structural Consensus via Greedy Min-Cut Analysis</title>
<link>https://arxiv.org/abs/2504.00467</link>
<guid>https://arxiv.org/abs/2504.00467</guid>
<content:encoded><![CDATA[

arXiv:2504.00467v2 Announce Type: replace 
Abstract: This paper presents the Min-Cut Bayesian Network Consensus (MCBNC) algorithm, a greedy method for structural consensus of Bayesian Networks (BNs), with applications in federated learning and model aggregation. MCBNC prunes weak edges from an initial unrestricted fusion using a structural score based on min-cut analysis, integrated into a modified Backward Equivalence Search (BES) phase of the Greedy Equivalence Search (GES) algorithm. The score quantifies edge support across input networks and is computed using max-flow. Unlike methods with fixed treewidth bounds, MCBNC introduces a pruning threshold $\theta$ that can be selected post hoc using only structural information. Experiments on real-world BNs show that MCBNC yields sparser, more accurate consensus structures than both canonical fusion and the input networks. The method is scalable, data-agnostic, and well-suited for distributed or federated scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching</title>
<link>https://arxiv.org/abs/2504.06319</link>
<guid>https://arxiv.org/abs/2504.06319</guid>
<content:encoded><![CDATA[

arXiv:2504.06319v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Hallucination Detection in LLMs via Adaptive Token Selection</title>
<link>https://arxiv.org/abs/2504.07863</link>
<guid>https://arxiv.org/abs/2504.07863</guid>
<content:encoded><![CDATA[

arXiv:2504.07863v3 Announce Type: replace 
Abstract: Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Doubly Stochastic Transformers</title>
<link>https://arxiv.org/abs/2504.16275</link>
<guid>https://arxiv.org/abs/2504.16275</guid>
<content:encoded><![CDATA[

arXiv:2504.16275v2 Announce Type: replace 
Abstract: At the core of the Transformer, the softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often de-stabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn's algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn's algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard ViT and other doubly stochastic Transformers. Beyond the Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. Our QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Bias Helps Learning: Bridging Initial Prejudice and Trainability</title>
<link>https://arxiv.org/abs/2505.12096</link>
<guid>https://arxiv.org/abs/2505.12096</guid>
<content:encoded><![CDATA[

arXiv:2505.12096v3 Announce Type: replace 
Abstract: Understanding the statistical properties of deep neural networks (DNNs) at initialization is crucial for elucidating both their trainability and the intrinsic architectural biases they encode prior to data exposure. Mean-field (MF) analyses have demonstrated that the parameter distribution in randomly initialized networks dictates whether gradients vanish or explode. Recent work has shown that untrained DNNs exhibit an initial-guessing bias (IGB), in which large regions of the input space are assigned to a single class. In this work, we provide a theoretical proof linking IGB to MF analyses, establishing that a network predisposition toward specific classes is intrinsically tied to the conditions for efficient learning. This connection leads to a counterintuitive conclusion: the initialization that optimizes trainability is systematically biased rather than neutral. We validate our theory through experiments across multiple architectures and datasets.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Thermodynamics: Entropic Forces in Deep and Universal Representation Learning</title>
<link>https://arxiv.org/abs/2505.12387</link>
<guid>https://arxiv.org/abs/2505.12387</guid>
<content:encoded><![CDATA[

arXiv:2505.12387v3 Announce Type: replace 
Abstract: With the rapid discovery of emergent phenomena in deep learning and large language models, understanding their cause has become an urgent need. Here, we propose a rigorous entropic-force theory for understanding the learning dynamics of neural networks trained with stochastic gradient descent (SGD) and its variants. Building on the theory of parameter symmetries and an entropic loss landscape, we show that representation learning is crucially governed by emergent entropic forces arising from stochasticity and discrete-time updates. These forces systematically break continuous parameter symmetries and preserve discrete ones, leading to a series of gradient balance phenomena that resemble the equipartition property of thermal systems. These phenomena, in turn, (a) explain the universal alignment of neural representations between AI models and lead to a proof of the Platonic Representation Hypothesis, and (b) reconcile the seemingly contradictory observations of sharpness- and flatness-seeking behavior of deep learning optimization. Our theory and experiments demonstrate that a combination of entropic forces and symmetry breaking is key to understanding emergent phenomena in deep learning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Private Statistical Estimation via Truncation</title>
<link>https://arxiv.org/abs/2505.12541</link>
<guid>https://arxiv.org/abs/2505.12541</guid>
<content:encoded><![CDATA[

arXiv:2505.12541v2 Announce Type: replace 
Abstract: We introduce a novel framework for differentially private (DP) statistical estimation via data truncation, addressing a key challenge in DP estimation when the data support is unbounded. Traditional approaches rely on problem-specific sensitivity analysis, limiting their applicability. By leveraging techniques from truncated statistics, we develop computationally efficient DP estimators for exponential family distributions, including Gaussian mean and covariance estimation, achieving near-optimal sample complexity. Previous works on exponential families only consider bounded or one-dimensional families. Our approach mitigates sensitivity through truncation while carefully correcting for the introduced bias using maximum likelihood estimation and DP stochastic gradient descent. Along the way, we establish improved uniform convergence guarantees for the log-likelihood function of exponential families, which may be of independent interest. Our results provide a general blueprint for DP algorithm design via truncated statistics.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Velocity-Growth Flow Matching for Single-Cell Dynamics Modeling</title>
<link>https://arxiv.org/abs/2505.13413</link>
<guid>https://arxiv.org/abs/2505.13413</guid>
<content:encoded><![CDATA[

arXiv:2505.13413v2 Announce Type: replace 
Abstract: Learning the underlying dynamics of single cells from snapshot data has gained increasing attention in scientific and machine learning research. The destructive measurement technique and cell proliferation/death result in unpaired and unbalanced data between snapshots, making the learning of the underlying dynamics challenging. In this paper, we propose joint Velocity-Growth Flow Matching (VGFM), a novel paradigm that jointly learns state transition and mass growth of single-cell populations via flow matching. VGFM builds an ideal single-cell dynamics containing velocity of state and growth of mass, driven by a presented two-period dynamic understanding of the static semi-relaxed optimal transport, a mathematical tool that seeks the coupling between unpaired and unbalanced data. To enable practical usage, we approximate the ideal dynamics using neural networks, forming our joint velocity and growth matching framework. A distribution fitting loss is also employed in VGFM to further improve the fitting performance for snapshot data. Extensive experimental results on both synthetic and real datasets demonstrate that VGFM can capture the underlying biological dynamics accounting for mass and state variations over time, outperforming existing approaches for single-cell dynamics modeling.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute</title>
<link>https://arxiv.org/abs/2505.14733</link>
<guid>https://arxiv.org/abs/2505.14733</guid>
<content:encoded><![CDATA[

arXiv:2505.14733v2 Announce Type: replace 
Abstract: Scaling large language models (LLMs) has driven significant advancements, yet it faces diminishing returns and escalating energy demands. This work explores how test-time compute (TTC) can serve as an energy-efficient complement to conventional scaling strategies by allocating additional computational resources at inference time rather than during training. Specifically, we investigate whether employing TTC can achieve superior accuracy-energy trade-offs compared to simply increasing model size. Our empirical analysis reveals that TTC surpasses traditional model scaling in accuracy/energy efficiency, with notable gains in tasks demanding complex reasoning rather than mere factual recall. Further, we identify a critical interaction between TTC performance and output sequence length, demonstrating that strategically adjusting compute resources at inference time according to query complexity can substantially enhance efficiency. Our findings advocate for TTC as a promising direction, enabling more sustainable, accurate, and adaptable deployment of future language models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Loss-Guided Auxiliary Agents for Overcoming Mode Collapse in GFlowNets</title>
<link>https://arxiv.org/abs/2505.15251</link>
<guid>https://arxiv.org/abs/2505.15251</guid>
<content:encoded><![CDATA[

arXiv:2505.15251v2 Announce Type: replace 
Abstract: Although Generative Flow Networks (GFlowNets) are designed to capture multiple modes of a reward function, they often suffer from mode collapse in practice, getting trapped in early-discovered modes and requiring prolonged training to find diverse solutions. Existing exploration techniques often rely on heuristic novelty signals. We propose Loss-Guided GFlowNets (LGGFN), a novel approach where an auxiliary GFlowNet's exploration is \textbf{directly driven by the main GFlowNet's training loss}. By prioritizing trajectories where the main model exhibits \textbf{high loss}, LGGFN focuses sampling on poorly understood regions of the state space. This targeted exploration significantly accelerates the discovery of diverse, high-reward samples. Empirically, across \textbf{diverse benchmarks} including grid environments, structured sequence generation, Bayesian structure learning, and biological sequence design, LGGFN consistently \textbf{outperforms} baselines in exploration efficiency and sample diversity. For instance, on a challenging sequence generation task, it discovered over 40 times more unique valid modes while simultaneously reducing the exploration error metric by approximately 99\%.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-Conditional Flow Matching for Relational Data Generation</title>
<link>https://arxiv.org/abs/2505.15668</link>
<guid>https://arxiv.org/abs/2505.15668</guid>
<content:encoded><![CDATA[

arXiv:2505.15668v2 Announce Type: replace 
Abstract: Data synthesis is gaining momentum as a privacy-enhancing technology. While single-table tabular data generation has seen considerable progress, current methods for multi-table data often lack the flexibility and expressiveness needed to capture complex relational structures. In particular, they struggle with long-range dependencies and complex foreign-key relationships, such as tables with multiple parent tables or multiple types of links between the same pair of tables. We propose a generative model for relational data that generates the content of a relational dataset given the graph formed by the foreign-key relationships. We do this by learning a deep generative model of the content of the whole relational database by flow matching, where the neural network trained to denoise records leverages a graph neural network to obtain information from connected records. Our method is flexible, as it can support relational datasets with complex structures, and expressive, as the generation of each record can be influenced by any other record within the same connected component. We evaluate our method on several benchmark datasets and show that it achieves state-of-the-art performance in terms of synthetic data fidelity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Forward-Forward Learning through Representational Dimensionality Compression</title>
<link>https://arxiv.org/abs/2505.16649</link>
<guid>https://arxiv.org/abs/2505.16649</guid>
<content:encoded><![CDATA[

arXiv:2505.16649v3 Announce Type: replace 
Abstract: The Forward-Forward (FF) learning algorithm provides a bottom-up alternative to backpropagation (BP) for training neural networks, relying on a layer-wise "goodness" function with well-designed negative samples for contrastive learning. Existing goodness functions are typically defined as the sum of squared postsynaptic activations, neglecting correlated variability between neurons. In this work, we propose a novel goodness function termed dimensionality compression that uses the effective dimensionality (ED) of fluctuating neural responses to incorporate second-order statistical structure. Our objective minimizes ED for noisy copies of individual inputs while maximizing it across the sample distribution, promoting structured representations without the need to prepare negative samples.We demonstrate that this formulation achieves competitive performance compared to other non-BP methods. Moreover, we show that noise plays a constructive role that can enhance generalization and improve inference when predictions are derived from the mean of squared output, which is equivalent to making predictions based on an energy term. Our findings contribute to the development of more biologically plausible learning algorithms and suggest a natural fit for neuromorphic computing, where stochasticity is a computational resource rather than a nuisance. The code is available at https://github.com/ZhichaoZhu/StochasticForwardForward
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided Diffusion Sampling on Function Spaces with Applications to PDEs</title>
<link>https://arxiv.org/abs/2505.17004</link>
<guid>https://arxiv.org/abs/2505.17004</guid>
<content:encoded><![CDATA[

arXiv:2505.17004v2 Announce Type: replace 
Abstract: We propose a general framework for conditional sampling in PDE-based inverse problems, targeting the recovery of whole solutions from extremely sparse or noisy measurements. This is accomplished by a function-space diffusion model and plug-and-play guidance for conditioning. Our method first trains an unconditional discretization-agnostic denoising model using neural operator architectures. At inference, we refine the samples to satisfy sparse observation data via a gradient-based guidance mechanism. Through rigorous mathematical analysis, we extend Tweedie's formula to infinite-dimensional Hilbert spaces, providing the theoretical foundation for our posterior sampling approach. Our method (FunDPS) accurately captures posterior distributions in function spaces under minimal supervision and severe data scarcity. Across five PDE tasks with only 3% observation, our method achieves an average 32% accuracy improvement over state-of-the-art fixed-resolution diffusion baselines while reducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning ensures strong cross-resolution generalizability. To the best of our knowledge, this is the first diffusion-based framework to operate independently of discretization, offering a practical and flexible solution for forward and inverse problems in the context of PDEs. Code is available at https://github.com/neuraloperator/FunDPS
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Relation between Rectified Flows and Optimal Transport</title>
<link>https://arxiv.org/abs/2505.19712</link>
<guid>https://arxiv.org/abs/2505.19712</guid>
<content:encoded><![CDATA[

arXiv:2505.19712v2 Announce Type: replace 
Abstract: This paper investigates the connections between rectified flows, flow matching, and optimal transport. Flow matching is a recent approach to learning generative models by estimating velocity fields that guide transformations from a source to a target distribution. Rectified flow matching aims to straighten the learned transport paths, yielding more direct flows between distributions. Our first contribution is a set of invariance properties of rectified flows and explicit velocity fields. In addition, we also provide explicit constructions and analysis in the Gaussian (not necessarily independent) and Gaussian mixture settings and study the relation to optimal transport. Our second contribution addresses recent claims suggesting that rectified flows, when constrained such that the learned velocity field is a gradient, can yield (asymptotically) solutions to optimal transport problems. We study the existence of solutions for this problem and demonstrate that they only relate to optimal transport under assumptions that are significantly stronger than those previously acknowledged. In particular, we present several counterexamples that invalidate earlier equivalence results in the literature, and we argue that enforcing a gradient constraint on rectified flows is, in general, not a reliable method for computing optimal transport maps.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rotary Masked Autoencoders are Versatile Learners</title>
<link>https://arxiv.org/abs/2505.20535</link>
<guid>https://arxiv.org/abs/2505.20535</guid>
<content:encoded><![CDATA[

arXiv:2505.20535v2 Announce Type: replace 
Abstract: Applying Transformers to irregular time-series typically requires specializations to their baseline architecture, which can result in additional computational overhead and increased method complexity. We present the Rotary Masked Autoencoder (RoMAE), which utilizes the popular Rotary Positional Embedding (RoPE) method for continuous positions. RoMAE is an extension to the Masked Autoencoder (MAE) that enables interpolation and representation learning with multidimensional continuous positional information while avoiding any time-series-specific architectural specializations. We showcase RoMAE's performance on a variety of modalities including irregular and multivariate time-series, images, and audio, demonstrating that RoMAE surpasses specialized time-series architectures on difficult datasets such as the DESC ELAsTiCC Challenge while maintaining MAE's usual performance across other modalities. In addition, we investigate RoMAE's ability to reconstruct the embedded continuous positions, demonstrating that including learned embeddings in the input sequence breaks RoPE's relative position property.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields</title>
<link>https://arxiv.org/abs/2505.24434</link>
<guid>https://arxiv.org/abs/2505.24434</guid>
<content:encoded><![CDATA[

arXiv:2505.24434v3 Announce Type: replace 
Abstract: Flow matching casts sample generation as learning a continuous-time velocity field that transports noise to data. Existing flow matching networks typically predict each point's velocity independently, considering only its location and time along its flow trajectory, and ignoring neighboring points. However, this pointwise approach may overlook correlations between points along the generation trajectory that could enhance velocity predictions, thereby improving downstream generation quality. To address this, we propose Graph Flow Matching (GFM), a lightweight enhancement that decomposes the learned velocity into a reaction term -- any standard flow matching network -- and a diffusion term that aggregates neighbor information via a graph neural module. This reaction-diffusion formulation retains the scalability of deep flow models while enriching velocity predictions with local context, all at minimal additional computational cost. Operating in the latent space of a pretrained variational autoencoder, GFM consistently improves Fr\'echet Inception Distance (FID) and recall across five image generation benchmarks (LSUN Church, LSUN Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its effectiveness as a modular enhancement to existing flow matching architectures.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching</title>
<link>https://arxiv.org/abs/2505.24843</link>
<guid>https://arxiv.org/abs/2505.24843</guid>
<content:encoded><![CDATA[

arXiv:2505.24843v2 Announce Type: replace 
Abstract: Models that learn spurious correlations from training data often fail when deployed in new environments. While many methods aim to learn invariant representations to address this, they often underperform standard empirical risk minimization (ERM). We propose a data-centric alternative that shifts the focus from learning invariant representations to leveraging invariant data pairs -- pairs of samples that should have the same prediction. We prove that certain counterfactuals naturally satisfy this invariance property. Based on this, we introduce Noisy Counterfactual Matching (NCM), a simple constraint-based method that improves robustness by leveraging even a small number of \emph{noisy} counterfactual pairs -- improving upon prior works that do not explicitly consider noise. For linear causal models, we prove that NCM's test-domain error is bounded by its in-domain error plus a term dependent on the counterfactuals' quality and diversity. Experiments on synthetic data validate our theory, and we demonstrate NCM's effectiveness on real-world datasets.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differential privacy for medical deep learning: methods, tradeoffs, and deployment implications</title>
<link>https://arxiv.org/abs/2506.00660</link>
<guid>https://arxiv.org/abs/2506.00660</guid>
<content:encoded><![CDATA[

arXiv:2506.00660v2 Announce Type: replace 
Abstract: Differential privacy (DP) is a key technique for protecting sensitive patient data in medical deep learning (DL). As clinical models grow more data-dependent, balancing privacy with utility and fairness has become a critical challenge. This scoping review synthesizes recent developments in applying DP to medical DL, with a particular focus on DP-SGD and alternative mechanisms across centralized and federated settings. Using a structured search strategy, we identified 74 studies published up to March 2025. Our analysis spans diverse data modalities, training setups, and downstream tasks, and highlights the tradeoffs between privacy guarantees, model accuracy, and subgroup fairness. We find that while DP-especially at strong privacy budgets-can preserve performance in well-structured imaging tasks, severe degradation often occurs under strict privacy, particularly in underrepresented or complex modalities. Furthermore, privacy-induced performance gaps disproportionately affect demographic subgroups, with fairness impacts varying by data type and task. A small subset of studies explicitly addresses these tradeoffs through subgroup analysis or fairness metrics, but most omit them entirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms, generative models, and hybrid federated designs, though reporting remains inconsistent. We conclude by outlining key gaps in fairness auditing, standardization, and evaluation protocols, offering guidance for future work toward equitable and clinically robust privacy-preserving DL systems in medicine.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies</title>
<link>https://arxiv.org/abs/2506.02703</link>
<guid>https://arxiv.org/abs/2506.02703</guid>
<content:encoded><![CDATA[

arXiv:2506.02703v3 Announce Type: replace 
Abstract: This study critically examines the methodological rigor in credit card fraud detection research, revealing how fundamental evaluation flaws can overshadow algorithmic sophistication. Through deliberate experimentation with improper evaluation protocols, we demonstrate that even simple models can achieve deceptively impressive results when basic methodological principles are violated. Our analysis identifies four critical issues plaguing current approaches: (1) pervasive data leakage from improper preprocessing sequences, (2) intentional vagueness in methodological reporting, (3) inadequate temporal validation for transaction data, and (4) metric manipulation through recall optimization at precision's expense. We present a case study showing how a minimal neural network architecture with data leakage outperforms many sophisticated methods reported in literature, achieving 99.9\% recall despite fundamental evaluation flaws. These findings underscore that proper evaluation methodology matters more than model complexity in fraud detection research. The study serves as a cautionary example of how methodological rigor must precede architectural sophistication, with implications for improving research practices across machine learning applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Discovery in Dynamic Fading Wireless Networks</title>
<link>https://arxiv.org/abs/2506.03163</link>
<guid>https://arxiv.org/abs/2506.03163</guid>
<content:encoded><![CDATA[

arXiv:2506.03163v2 Announce Type: replace 
Abstract: Dynamic causal discovery in wireless networks is essential due to evolving interference, fading, and mobility, which complicate traditional static causal models. This paper addresses causal inference challenges in dynamic fading wireless environments by proposing a sequential regression-based algorithm with a novel application of the NOTEARS acyclicity constraint, enabling efficient online updates. We derive theoretical lower and upper bounds on the detection delay required to identify structural changes, explicitly quantifying their dependence on network size, noise variance, and fading severity. Monte Carlo simulations validate these theoretical results, demonstrating linear increases in detection delay with network size, quadratic growth with noise variance, and inverse-square dependence on the magnitude of structural changes. Our findings provide rigorous theoretical insights and practical guidelines for designing robust online causal inference mechanisms to maintain network reliability under nonstationary wireless conditions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dissecting Long-Chain-of-Thought Reasoning Models: An Empirical Study</title>
<link>https://arxiv.org/abs/2506.04913</link>
<guid>https://arxiv.org/abs/2506.04913</guid>
<content:encoded><![CDATA[

arXiv:2506.04913v2 Announce Type: replace 
Abstract: Despite recent progress in training long-chain-of-thought reasoning models via scaling reinforcement learning (RL), its underlying training dynamics remain poorly understood, and several counterintuitive behaviors persist. This work focuses on three key aspects: (1) We systematically analyze the roles of positive and negative samples in scaling RL, revealing that positive samples mainly facilitate precise fitting to the training data, whereas negative samples significantly enhance generalization and robustness. Interestingly, while positive samples are essential for convergence in the zero-RL setting, training on negative samples alone suffices to attain strong reasoning performance and even better generalization in cold-start scenarios. (2) We identify substantial data inefficiency in group relative policy optimization, where over half of the samples yield zero advantage. To address this, we explore two strategies, including relative length rewards and offline sample injection, to leverage these data better and enhance reasoning efficiency and capability. (3) We investigate unstable performance across various reasoning models and benchmarks, attributing instability to uncertain problems with ambiguous outcomes, and demonstrate that greedy decoding can distort evaluation by flipping the correctness of responses. Our code is available at: https://github.com/takagi97/Dissect-Long-Reason-Models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PyLO: Towards Accessible Learned Optimizers in PyTorch</title>
<link>https://arxiv.org/abs/2506.10315</link>
<guid>https://arxiv.org/abs/2506.10315</guid>
<content:encoded><![CDATA[

arXiv:2506.10315v2 Announce Type: replace 
Abstract: Learned optimizers have been an active research topic over the past decade, with increasing progress toward practical, general-purpose optimizers that can serve as drop-in replacements for widely used methods like Adam. However, recent advances -- such as VeLO, which was meta-trained for 4000 TPU-months -- remain largely inaccessible to the broader community, in part due to their reliance on JAX and the absence of user-friendly packages for applying the optimizers after meta-training. To address this gap, we introduce PyLO, a PyTorch-based library that brings learned optimizers to the broader machine learning community through familiar, widely adopted workflows. Unlike prior work focused on synthetic or convex tasks, our emphasis is on applying learned optimization to real-world large-scale pre-training tasks. Our release includes a CUDA-accelerated version of the small_fc_lopt learned optimizer architecture from (Metz et al., 2022a), delivering substantial speedups -- from 39.36 to 205.59 samples/sec throughput for training ViT B/16 with batch size 32. PyLO also allows us to easily combine learned optimizers with existing optimization tools such as learning rate schedules and weight decay. When doing so, we find that learned optimizers can substantially benefit. Our code is available at https://github.com/Belilovsky-Lab/pylo
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2506.17631</link>
<guid>https://arxiv.org/abs/2506.17631</guid>
<content:encoded><![CDATA[

arXiv:2506.17631v3 Announce Type: replace 
Abstract: Time series forecasting aims to model temporal dependencies among variables for future state inference, holding significant importance and widespread applications in real-world scenarios. Although deep learning-based methods have achieved remarkable progress, they still exhibit suboptimal performance in long-term forecasting. Recent research demonstrates that large language models (LLMs) achieve promising performance in time series forecasting, but this progress is still met with skepticism about whether LLMs are truly useful for this task. To address this, we propose Time-Prompt, a framework for activating LLMs for time series forecasting. Specifically, we first construct a unified prompt paradigm with learnable soft prompts to guide the LLM's behavior and textualized hard prompts to enhance the time series representations. Second, to enhance LLM' comprehensive understanding of the forecasting task, we design a semantic space embedding and cross-modal alignment module to achieve fusion of temporal and textual data. Finally, we efficiently fine-tune the LLM's parameters using time series data. Furthermore, we focus on carbon emissions, aiming to provide a modest contribution to global carbon neutrality. Comprehensive evaluations on 6 public datasets and 3 carbon emission datasets demonstrate that Time-Prompt is a powerful framework for time series forecasting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiple Streams of Knowledge Retrieval: Enriching and Recalling in Transformers</title>
<link>https://arxiv.org/abs/2506.20746</link>
<guid>https://arxiv.org/abs/2506.20746</guid>
<content:encoded><![CDATA[

arXiv:2506.20746v2 Announce Type: replace 
Abstract: When an LLM learns a new fact during finetuning (e.g., new movie releases, newly elected pope, etc.), where does this information go? Are entities enriched with relation information, or do models recall information just-in-time before a prediction? Or, are ``all of the above'' true with LLMs implementing multiple redundant heuristics? Existing localization approaches (e.g., activation patching) are ill-suited for this analysis because they usually \textit{replace} parts of the residual stream, thus overriding previous information. To fill this gap, we propose \emph{dynamic weight grafting}, a technique that selectively grafts weights from a finetuned model onto a pretrained model. Using this technique, we show two separate pathways for retrieving finetuned relation information: 1) ``enriching" the residual stream with relation information while processing the tokens that correspond to an entity (e.g., ``Zendaya'' in ``Zendaya co-starred with John David Washington'') and 2) ``recalling" this information at the final token position before generating a target fact. In some cases, models need information from both of these pathways to correctly generate finetuned facts while, in other cases, either the ``enrichment" or ``recall" pathway alone is sufficient. We localize the ``recall'' pathway to model components -- finding that ``recall" occurs via both task-specific attention mechanisms and an entity-specific extraction step in the feedforward networks of the final layers before the target prediction. By targeting model components and parameters, as opposed to just activations, we are able to understand the \textit{mechanisms} by which finetuned knowledge is retrieved during generation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Stochastic Multiscale Models</title>
<link>https://arxiv.org/abs/2506.22655</link>
<guid>https://arxiv.org/abs/2506.22655</guid>
<content:encoded><![CDATA[

arXiv:2506.22655v2 Announce Type: replace 
Abstract: The physical sciences are replete with dynamical systems that require the resolution of a wide range of length and time scales. This presents significant computational challenges since direct numerical simulation requires discretization at the finest relevant scales, leading to a high-dimensional state space. In this work, we propose an approach to learn stochastic multiscale models in the form of stochastic differential equations directly from observational data. Drawing inspiration from physics-based multiscale modeling approaches, we resolve the macroscale state on a coarse mesh while introducing a microscale latent state to explicitly model unresolved dynamics. We learn the parameters of the multiscale model using a simulator-free amortized variational inference method with a Product of Experts likelihood that enforces scale separation. We present detailed numerical studies to demonstrate that our learned multiscale models achieve superior predictive accuracy compared to under-resolved direct numerical simulation and closure-type models at equivalent resolution, as well as reduced-order modeling approaches.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes</title>
<link>https://arxiv.org/abs/2506.23165</link>
<guid>https://arxiv.org/abs/2506.23165</guid>
<content:encoded><![CDATA[

arXiv:2506.23165v3 Announce Type: replace 
Abstract: Safety is an essential requirement for reinforcement learning systems. The newly emerging framework of robust constrained Markov decision processes allows learning policies that satisfy long-term constraints while providing guarantees under epistemic uncertainty. This paper presents mirror descent policy optimisation for robust constrained Markov decision processes, making use of policy gradient techniques to optimise both the policy (as a maximiser) and the transition kernel (as an adversarial minimiser) on the Lagrangian representing a constrained Markov decision process. Our proposed algorithm obtains an $\tilde{\mathcal{O}}\left(1/T^{1/3}\right)$ convergence rate in the sample-based robust constrained Markov decision process setting. The paper also contributes an algorithm for approximate gradient descent in the space of transition kernels, which is of independent interest for designing adversarial environments in general Markov decision processes. Experiments confirm the benefits of mirror descent policy optimisation in constrained and unconstrained optimisation, and significant improvements are observed in robustness tests when compared to baseline policy optimisation algorithms.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPT, But Backwards: Exactly Inverting Language Model Outputs</title>
<link>https://arxiv.org/abs/2507.01693</link>
<guid>https://arxiv.org/abs/2507.01693</guid>
<content:encoded><![CDATA[

arXiv:2507.01693v2 Announce Type: replace 
Abstract: The task of reconstructing unknown textual inputs to language models is a fundamental auditing primitive that allows us to assess the model's vulnerability to a range of security issues, including stealing hidden system prompts, detecting backdoors, and leaking private data. Existing inversion works assume access to differing levels of information (e.g. requiring input-output examples, the model parameters, intermediate activations or output logits) but oftentimes fail to fully reconstruct the desired input. In this paper, we present the Sparse One-hot Discrete Adam (SODA) algorithm, a search-based inversion method that can accurately reconstruct the input text, given white-box access to the language model and its output. Our experiments demonstrate for the first time that exact language model inversion is possible on both natural language and random inputs. Indeed, SODA achieves respectively 98% and 79% reconstruction rates on inputs with lengths up to 10 tokens. Furthermore, we show that input length and vocabulary size have a far greater impact on the probability of a successful reconstruction than the size of the language model itself, thus allowing us to scale to models from 33M to 3B parameters.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Parallelism: Synergistic Computational Graph Effects in Multi-Head Attention</title>
<link>https://arxiv.org/abs/2507.02944</link>
<guid>https://arxiv.org/abs/2507.02944</guid>
<content:encoded><![CDATA[

arXiv:2507.02944v2 Announce Type: replace 
Abstract: Multi-head attention powers Transformer networks, the primary deep learning architecture behind the success of large language models (LLMs). Yet, the theoretical advantages of multi-head versus single-head attention, beyond mere parallel processing, remain underexplored. In this paper, we reframe multi-head attention as a system of potentially synergistic computational graphs, where each head functions as a feedforward directed acyclic graph (DAG) with a common sink state. We provide intuition and preliminary theoretical analysis of mixing time and minimax fidelity in this framework. Our results show that multi-head attention can synergistically enhance information propagation, yielding faster mixing times and minimax fidelity amplification under specific head-diversity conditions. Finally, we train single-head and multi-head Transformers, each with the same total number of parameters, on sequence manipulation tasks and empirically verify the predicted effects. The code is available at https://github.com/haitzsaezdeocariz/beyondparallelism.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Plausibility-Validity Gap by Fine-Tuning a Reasoning-Enhanced LLM for Chemical Synthesis and Discovery</title>
<link>https://arxiv.org/abs/2507.07328</link>
<guid>https://arxiv.org/abs/2507.07328</guid>
<content:encoded><![CDATA[

arXiv:2507.07328v2 Announce Type: replace 
Abstract: Large Language Models frequently generate outputs that appear scientifically reasonable yet violate fundamental principles--a phenomenon we characterize as the "plausibility-validity gap." This challenge proves especially acute in chemistry, where superficial correctness masks deeper errors in molecular structure, reaction mechanisms, and synthetic pathways. We present a systematic approach combining a reasoning-centric model architecture (Magistral Small) with Low-Rank Adaptation fine-tuning on a dual-domain dataset covering molecular properties and chemical transformations. Evaluation reveals substantial improvements: the fine-tuned system achieves 96.3% format adherence, 97.4% chemical validity, and 74.4% synthesis feasibility. Comparative analysis shows our approach outperforms specialized translation models like MolT5 (97.4% vs 77.2% validity) while achieving performance comparable to complex tool-augmented systems like ChemCrow (9.0/10 vs 9.24/10 expert rating) through a more transparent, efficient methodology. Results demonstrate a learning hierarchy where syntactic correctness develops before chemical understanding, which precedes synthetic planning capability. This work establishes a reproducible framework for transforming generalist language models into dependable scientific tools while identifying critical areas including stereochemical precision, knowledge currency, and computational accessibility as key challenges for future advancement.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Monitoring Risks in Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2507.08721</link>
<guid>https://arxiv.org/abs/2507.08721</guid>
<content:encoded><![CDATA[

arXiv:2507.08721v2 Announce Type: replace 
Abstract: Encountering shifted data at test time is a ubiquitous challenge when deploying predictive models. Test-time adaptation (TTA) methods address this issue by continuously adapting a deployed model using only unlabeled test data. While TTA can extend the model's lifespan, it is only a temporary solution. Eventually the model might degrade to the point that it must be taken offline and retrained. To detect such points of ultimate failure, we propose pairing TTA with risk monitoring frameworks that track predictive performance and raise alerts when predefined performance criteria are violated. Specifically, we extend existing monitoring tools based on sequential testing with confidence sequences to accommodate scenarios in which the model is updated at test time and no test labels are available to estimate the performance metrics of interest. Our extensions unlock the application of rigorous statistical risk monitoring to TTA, and we demonstrate the effectiveness of our proposed TTA monitoring framework across a representative set of datasets, distribution shift types, and TTA methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoTM: Towards a Foundation Model for Time Series Imputation based on Continuous Modeling</title>
<link>https://arxiv.org/abs/2507.13207</link>
<guid>https://arxiv.org/abs/2507.13207</guid>
<content:encoded><![CDATA[

arXiv:2507.13207v3 Announce Type: replace 
Abstract: Recent years have witnessed a growing interest for time series foundation models, with a strong emphasis on the forecasting task. Yet, the crucial task of out-of-domain imputation of missing values remains largely underexplored. We propose a first step to fill this gap by leveraging implicit neural representations (INRs). INRs model time series as continuous functions and naturally handle various missing data scenarios and sampling rates. While they have shown strong performance within specific distributions, they struggle under distribution shifts. To address this, we introduce MoTM (Mixture of Timeflow Models), a step toward a foundation model for time series imputation. Building on the idea that a new time series is a mixture of previously seen patterns, MoTM combines a basis of INRs, each trained independently on a distinct family of time series, with a ridge regressor that adapts to the observed context at inference. We demonstrate robust in-domain and out-of-domain generalization across diverse imputation scenarios (e.g., block and pointwise missingness, variable sampling rates), paving the way for adaptable foundation imputation models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.16795</link>
<guid>https://arxiv.org/abs/2507.16795</guid>
<content:encoded><![CDATA[

arXiv:2507.16795v2 Announce Type: replace 
Abstract: Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Learning with Synthetic Boundary Experience Blending</title>
<link>https://arxiv.org/abs/2507.23534</link>
<guid>https://arxiv.org/abs/2507.23534</guid>
<content:encoded><![CDATA[

arXiv:2507.23534v2 Announce Type: replace 
Abstract: Continual learning (CL) seeks to mitigate catastrophic forgetting when models are trained with sequential tasks. A common approach, experience replay (ER), stores past exemplars but only sparsely approximates the data distribution, yielding fragile and oversimplified decision boundaries. We address this limitation by introducing synthetic boundary data (SBD), generated via differential privacy: inspired noise into latent features to create boundary-adjacent representations that implicitly regularize decision boundaries. Building on this idea, we propose Experience Blending (EB), a framework that jointly trains on exemplars and SBD through a dual-model aggregation strategy. EB has two components: (1) latent-space noise injection to synthesize boundary data, and (2) end-to-end training that jointly leverages exemplars and SBD. Unlike standard experience replay, SBD enriches the feature space near decision boundaries, leading to more stable and robust continual learning. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet demonstrate consistent accuracy improvements of 10%, 6%, and 13%, respectively, over strong baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization</title>
<link>https://arxiv.org/abs/2508.00357</link>
<guid>https://arxiv.org/abs/2508.00357</guid>
<content:encoded><![CDATA[

arXiv:2508.00357v2 Announce Type: replace 
Abstract: Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct node features, particularly on heterophilic graphs where adjacent nodes often have dissimilar labels. Although sheaf neural networks partially mitigate this problem, they typically rely on static or heavily parameterized sheaf structures that hinder generalization and scalability. Existing sheaf-based models either predefine restriction maps or introduce excessive complexity, yet fail to provide rigorous stability guarantees. In this paper, we introduce a novel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified architecture that combines cellular-sheaf message passing with several mechanisms, including optimal transport-based lifting, variance-reduced diffusion, and PAC-Bayes spectral regularization for robust semi-supervised node classification. We establish performance bounds theoretically and demonstrate that end-to-end training in linear computational complexity can achieve the resulting bound-aware objective. Experiments on nine homophilic and heterophilic benchmarks show that SGPC outperforms state-of-the-art spectral and sheaf-based GNNs while providing certified confidence intervals on unseen nodes. The code and proofs are in https://github.com/ChoiYoonHyuk/SGPC.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Neurons in GPT-2: Emergence, Persistence, and Functional Impact</title>
<link>https://arxiv.org/abs/2508.00903</link>
<guid>https://arxiv.org/abs/2508.00903</guid>
<content:encoded><![CDATA[

arXiv:2508.00903v2 Announce Type: replace 
Abstract: We investigate the phenomenon of neuron universality in independently trained GPT-2 Small models, examining these universal neurons-neurons with consistently correlated activations across models-emerge and evolve throughout training. By analyzing five GPT-2 models at five checkpoints, we identify universal neurons through pairwise correlation analysis of activations over a dataset of 5 million tokens. Ablation experiments reveal significant functional impacts of universal neurons on model predictions, measured via cross entropy loss. Additionally, we quantify neuron persistence, demonstrating high stability of universal neurons across training checkpoints, particularly in early and deeper layers. These findings suggest stable and universal representational structures emerge during language model training.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PSEO: Optimizing Post-hoc Stacking Ensemble Through Hyperparameter Tuning</title>
<link>https://arxiv.org/abs/2508.05144</link>
<guid>https://arxiv.org/abs/2508.05144</guid>
<content:encoded><![CDATA[

arXiv:2508.05144v2 Announce Type: replace 
Abstract: The Combined Algorithm Selection and Hyperparameter Optimization (CASH) problem is fundamental in Automated Machine Learning (AutoML). Inspired by the success of ensemble learning, recent AutoML systems construct post-hoc ensembles for final predictions rather than relying on the best single model. However, while most CASH methods conduct extensive searches for the optimal single model, they typically employ fixed strategies during the ensemble phase that fail to adapt to specific task characteristics. To tackle this issue, we propose PSEO, a framework for post-hoc stacking ensemble optimization. First, we conduct base model selection through binary quadratic programming, with a trade-off between diversity and performance. Furthermore, we introduce two mechanisms to fully realize the potential of multi-layer stacking. Finally, PSEO builds a hyperparameter space and searches for the optimal post-hoc ensemble strategy within it. Empirical results on 80 public datasets show that \sys achieves the best average test rank (2.96) among 16 methods, including post-hoc designs in recent AutoML systems and state-of-the-art ensemble learning methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniMove: A Unified Model for Multi-city Human Mobility Prediction</title>
<link>https://arxiv.org/abs/2508.06986</link>
<guid>https://arxiv.org/abs/2508.06986</guid>
<content:encoded><![CDATA[

arXiv:2508.06986v2 Announce Type: replace 
Abstract: Human mobility prediction is vital for urban planning, transportation optimization, and personalized services. However, the inherent randomness, non-uniform time intervals, and complex patterns of human mobility, compounded by the heterogeneity introduced by varying city structures, infrastructure, and population densities, present significant challenges in modeling. Existing solutions often require training separate models for each city due to distinct spatial representations and geographic coverage. In this paper, we propose UniMove, a unified model for multi-city human mobility prediction, addressing two challenges: (1) constructing universal spatial representations for effective token sharing across cities, and (2) modeling heterogeneous mobility patterns from varying city characteristics. We propose a trajectory-location dual-tower architecture, with a location tower for universal spatial encoding and a trajectory tower for sequential mobility modeling. We also design MoE Transformer blocks to adaptively select experts to handle diverse movement patterns. Extensive experiments across multiple datasets from diverse cities demonstrate that UniMove truly embodies the essence of a unified model. By enabling joint training on multi-city data with mutual data enhancement, it significantly improves mobility prediction accuracy by over 10.2\%. UniMove represents a key advancement toward realizing a true foundational model with a unified architecture for human mobility. We release the implementation at https://github.com/tsinghua-fib-lab/UniMove/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tight Bounds for Schr\"odinger Potential Estimation in Unpaired Data Translation</title>
<link>https://arxiv.org/abs/2508.07392</link>
<guid>https://arxiv.org/abs/2508.07392</guid>
<content:encoded><![CDATA[

arXiv:2508.07392v2 Announce Type: replace 
Abstract: Modern methods of generative modelling and unpaired data translation based on Schr\"odinger bridges and stochastic optimal control theory aim to transform an initial density to a target one in an optimal way. In the present paper, we assume that we only have access to i.i.d. samples from initial and final distributions. This makes our setup suitable for both generative modelling and unpaired data translation. Relying on the stochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as the reference one and estimate the corresponding Schr\"odinger potential. Introducing a risk function as the Kullback-Leibler divergence between couplings, we derive tight bounds on generalization ability of an empirical risk minimizer in a class of Schr\"odinger potentials including Gaussian mixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we almost achieve fast rates of convergence up to some logarithmic factors in favourable scenarios. We also illustrate performance of the suggested approach with numerical experiments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering Spatial Correlations of Earth Observations for weather forecasting by using Graph Structure Learning</title>
<link>https://arxiv.org/abs/2508.07659</link>
<guid>https://arxiv.org/abs/2508.07659</guid>
<content:encoded><![CDATA[

arXiv:2508.07659v2 Announce Type: replace 
Abstract: This study aims to improve the accuracy of weather predictions by discovering spatial correlations between Earth observations and atmospheric states. Existing numerical weather prediction (NWP) systems predict future atmospheric states at fixed locations, which are called NWP grid points, by analyzing previous atmospheric states and newly acquired Earth observations. However, the shifting locations of observations and the surrounding meteorological context induce complex, dynamic spatial correlations that are difficult for traditional NWP systems to capture, since they rely on strict statistical and physical formulations. To handle complicated spatial correlations, which change dynamically, we employ a spatiotemporal graph neural networks (STGNNs) with structure learning. However, structure learning has an inherent limitation that this can cause structural information loss and over-smoothing problem by generating excessive edges. To solve this problem, we regulate edge sampling by adaptively determining node degrees and considering the spatial distances between NWP grid points and observations. We validated the effectiveness of the proposed method (CloudNine-v2) using real-world atmospheric state and observation data from East Asia, achieving up to 15\% reductions in RMSE over existing STGNN models. Even in areas with high atmospheric variability, CloudNine-v2 consistently outperformed baselines with and without structure learning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Oblivionis: A Lightweight Learning and Unlearning Framework for Federated Large Language Models</title>
<link>https://arxiv.org/abs/2508.08875</link>
<guid>https://arxiv.org/abs/2508.08875</guid>
<content:encoded><![CDATA[

arXiv:2508.08875v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) increasingly leverage Federated Learning (FL) to utilize private, task-specific datasets for fine-tuning while preserving data privacy. However, while federated LLM frameworks effectively enable collaborative training without raw data sharing, they critically lack built-in mechanisms for regulatory compliance like GDPR's right to be forgotten. Integrating private data heightens concerns over data quality and long-term governance, yet existing distributed training frameworks offer no principled way to selectively remove specific client contributions post-training. Due to distributed data silos, stringent privacy constraints, and the intricacies of interdependent model aggregation, federated LLM unlearning is significantly more complex than centralized LLM unlearning. To address this gap, we introduce Oblivionis, a lightweight learning and unlearning framework that enables clients to selectively remove specific private data during federated LLM training, enhancing trustworthiness and regulatory compliance. By unifying FL and unlearning as a dual optimization objective, we incorporate 6 FL and 5 unlearning algorithms for comprehensive evaluation and comparative analysis, establishing a robust pipeline for federated LLM unlearning. Extensive experiments demonstrate that Oblivionis outperforms local training, achieving a robust balance between forgetting efficacy and model utility, with cross-algorithm comparisons providing clear directions for future LLM development.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control</title>
<link>https://arxiv.org/abs/2508.10684</link>
<guid>https://arxiv.org/abs/2508.10684</guid>
<content:encoded><![CDATA[

arXiv:2508.10684v2 Announce Type: replace 
Abstract: We study the problem of learning a neural sampler to generate samples from discrete state spaces where the target probability mass function $\pi\propto\mathrm{e}^{-U}$ is known up to a normalizing constant, which is an important task in fields such as statistical physics, machine learning, combinatorial optimization, etc. To better address this challenging task when the state space has a large cardinality and the distribution is multi-modal, we propose $\textbf{M}$asked $\textbf{D}$iffusion $\textbf{N}$eural $\textbf{S}$ampler ($\textbf{MDNS}$), a novel framework for training discrete neural samplers by aligning two path measures through a family of learning objectives, theoretically grounded in the stochastic optimal control of the continuous-time Markov chains. We validate the efficiency and scalability of MDNS through extensive experiments on various distributions with distinct statistical properties, where MDNS learns to accurately sample from the target distributions despite the extremely high problem dimensions and outperforms other learning-based baselines by a large margin. A comprehensive study of ablations and extensions is also provided to demonstrate the efficacy and potential of the proposed framework. Our code is available at https://github.com/yuchen-zhu-zyc/MDNS.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Medical Event Models Improve with Scale</title>
<link>https://arxiv.org/abs/2508.12104</link>
<guid>https://arxiv.org/abs/2508.12104</guid>
<content:encoded><![CDATA[

arXiv:2508.12104v3 Announce Type: replace 
Abstract: Realizing personalized medicine at scale calls for methods that distill insights from longitudinal patient journeys, which can be viewed as a sequence of medical events. Foundation models pretrained on large-scale medical event data represent a promising direction for scaling real-world evidence generation and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with medical events from de-identified longitudinal health records for 16.3 billion encounters over 300 million unique patient records from 310 health systems, we introduce the Curiosity models, a family of decoder-only transformer models pretrained on 118 million patients representing 115 billion discrete medical events (151 billion tokens). We present the largest scaling-law study of medical event data, establishing a methodology for pretraining and revealing power-law scaling relationships for compute, tokens, and model size. Consequently, we pretrained a series of compute-optimal models with up to 1 billion parameters. Conditioned on a patient's real-world history, Curiosity autoregressively predicts the next medical event to simulate patient health timelines. We studied 78 real-world tasks, including diagnosis prediction, disease prognosis, and healthcare operations. Remarkably for a foundation model with generic pretraining and simulation-based inference, Curiosity generally outperformed or matched task-specific supervised models on these tasks, without requiring task-specific fine-tuning or few-shot examples. Curiosity's predictive power consistently improves as the model and pretraining scale. Our results show that Curiosity, a generative medical event foundation model, can effectively capture complex clinical dynamics, providing an extensible and generalizable framework to support clinical decision-making, streamline healthcare operations, and improve patient outcomes.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-Aware Multi-view Evidential Learning with Adaptive Prior</title>
<link>https://arxiv.org/abs/2508.12997</link>
<guid>https://arxiv.org/abs/2508.12997</guid>
<content:encoded><![CDATA[

arXiv:2508.12997v2 Announce Type: replace 
Abstract: Multi-view evidential learning aims to integrate information from multiple views to improve prediction performance and provide trustworthy uncertainty esitimation. Most previous methods assume that view-specific evidence learning is naturally reliable. However, in practice, the evidence learning process tends to be biased. Through empirical analysis on real-world data, we reveal that samples tend to be assigned more evidence to support data-rich classes, thereby leading to unreliable uncertainty estimation in predictions. This motivates us to delve into a new Biased Evidential Multi-view Learning (BEML) problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning (FAML). FAML first introduces an adaptive prior based on training trajectory, which acts as a regularization strategy to flexibly calibrate the biased evidence learning process. Furthermore, we explicitly incorporate a fairness constraint based on class-wise evidence variance to promote balanced evidence allocation. In the multi-view fusion stage, we propose an opinion alignment mechanism to mitigate view-specific bias across views, thereby encouraging the integration of consistent and mutually supportive evidence.Theoretical analysis shows that FAML enhances fairness in the evidence learning process. Extensive experiments on five real-world multi-view datasets demonstrate that FAML achieves more balanced evidence allocation and improves both prediction performance and the reliability of uncertainty estimation compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learned Structure in Cartridges: Keys as Shareable Routers in Self-Studied Representations</title>
<link>https://arxiv.org/abs/2508.17032</link>
<guid>https://arxiv.org/abs/2508.17032</guid>
<content:encoded><![CDATA[

arXiv:2508.17032v2 Announce Type: replace 
Abstract: A bottleneck for long-context LLM inference is the linearly growing KV cache. Recent work has proposed Cartridges, an approach which leverages offline compute to train a much smaller KV cache than is typically required for a full document (up to 40x less memory usage at inference time). In this paper, we present the first mechanistic exploration of the learned Cartridge key-value cache structure. In particular, we propose that (1) Cartridge keys act as stable, shareable retrieval routers for the compressed corpora and (2) most of the learned compression occurs within the Cartridge value vectors. We present empirical evidence of our routing theory across tasks, model families, and model sizes; for example, we can ablate the learned Cartridge key vectors between tasks with little performance loss. Finally, we propose a slight improvement in initialization called Sampled Chunk Initialization (SCI). We suggest that SCI can lead to faster Cartridge convergence than previously demonstrated in the literature. Our findings lay the groundwork for broader empirical study of Cartridge training optimization which may be crucial for further scaling.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ANO : Faster is Better in Noisy Landscape</title>
<link>https://arxiv.org/abs/2508.18258</link>
<guid>https://arxiv.org/abs/2508.18258</guid>
<content:encoded><![CDATA[

arXiv:2508.18258v2 Announce Type: replace 
Abstract: Stochastic optimizers are central to deep learning, yet widely used methods such as Adam and Adan can degrade in non-stationary or noisy environments, partly due to their reliance on momentum-based magnitude estimates. We introduce Ano, a novel optimizer that decouples direction and magnitude: momentum is used for directional smoothing, while instantaneous gradient magnitudes determine step size. This design improves robustness to gradient noise while retaining the simplicity and efficiency of first-order methods. We further propose Anolog, which removes sensitivity to the momentum coefficient by expanding its window over time via a logarithmic schedule. We establish non-convex convergence guarantees with a convergence rate similar to other sign-based methods, and empirically show that Ano provides substantial gains in noisy and non-stationary regimes such as reinforcement learning, while remaining competitive on low-noise tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach</title>
<link>https://arxiv.org/abs/2508.20013</link>
<guid>https://arxiv.org/abs/2508.20013</guid>
<content:encoded><![CDATA[

arXiv:2508.20013v2 Announce Type: replace 
Abstract: This study addresses critical industrial challenges in e-commerce product categorization, namely platform heterogeneity and the structural limitations of existing taxonomies, by developing and deploying a multimodal hierarchical classification framework. Using a dataset of 271,700 products from 40 international fashion e-commerce platforms, we integrate textual features (RoBERTa), visual features (ViT), and joint vision-language representations (CLIP). We investigate fusion strategies, including early, late, and attention-based fusion within a hierarchical architecture enhanced by dynamic masking to ensure taxonomic consistency. Results show that CLIP embeddings combined via an MLP-based late-fusion strategy achieve the highest hierarchical F1 (98.59%), outperforming unimodal baselines. To address shallow or inconsistent categories, we further introduce a self-supervised "product recategorization" pipeline using SimCLR, UMAP, and cascade clustering, which discovered new, fine-grained categories (for example, subtypes of "Shoes") with cluster purities above 86%. Cross-platform experiments reveal a deployment-relevant trade-off: complex late-fusion methods maximize accuracy with diverse training data, while simpler early-fusion methods generalize more effectively to unseen platforms. Finally, we demonstrate the framework's industrial scalability through deployment in EURWEB's commercial transaction intelligence platform via a two-stage inference pipeline, combining a lightweight RoBERTa stage with a GPU-accelerated multimodal stage to balance cost and accuracy.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Graph Laplacian Eigenvector-based Pre-training Method for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.02803</link>
<guid>https://arxiv.org/abs/2509.02803</guid>
<content:encoded><![CDATA[

arXiv:2509.02803v3 Announce Type: replace 
Abstract: The development of self-supervised graph pre-training methods is a crucial ingredient in recent efforts to design robust graph foundation models (GFMs). Structure-based pre-training methods are under-explored yet crucial for downstream applications which rely on underlying graph structure. In addition, pre-training traditional message passing GNNs to capture global and regional structure is often challenging due to the risk of oversmoothing as network depth increases. We address these gaps by proposing the Laplacian Eigenvector Learning Module (LELM), a novel pre-training module for graph neural networks (GNNs) based on predicting the low-frequency eigenvectors of the graph Laplacian. Moreover, LELM introduces a novel architecture that overcomes oversmoothing, allowing the GNN model to learn long-range interdependencies. Empirically, we show that models pre-trained via our framework outperform baseline models on downstream molecular property prediction tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Branch Convolutional Framework for Spatial and Frequency-Based Image Forgery Detection</title>
<link>https://arxiv.org/abs/2509.05281</link>
<guid>https://arxiv.org/abs/2509.05281</guid>
<content:encoded><![CDATA[

arXiv:2509.05281v2 Announce Type: replace 
Abstract: With a very rapid increase in deepfakes and digital image forgeries, ensuring the authenticity of images is becoming increasingly challenging. This report introduces a forgery detection framework that combines spatial and frequency-based features for detecting forgeries. We propose a dual branch convolution neural network that operates on features extracted from spatial and frequency domains. Features from both branches are fused and compared within a Siamese network, yielding 64 dimensional embeddings for classification. When benchmarked on CASIA 2.0 dataset, our method achieves an accuracy of 77.9%, outperforming traditional statistical methods. Despite its relatively weaker performance compared to larger, more complex forgery detection pipelines, our approach balances computational complexity and detection reliability, making it ready for practical deployment. It provides a strong methodology for forensic scrutiny of digital images. In a broader sense, it advances the state of the art in visual forensics, addressing an urgent requirement in media verification, law enforcement and digital content reliability.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An upper bound of the silhouette validation metric for clustering</title>
<link>https://arxiv.org/abs/2509.08625</link>
<guid>https://arxiv.org/abs/2509.08625</guid>
<content:encoded><![CDATA[

arXiv:2509.08625v2 Announce Type: replace 
Abstract: The silhouette coefficient quantifies, for each observation, the balance between within-cluster cohesion and between-cluster separation, taking values in [-1, 1]. The average silhouette width (ASW) is a widely used internal measure of clustering quality, with higher values indicating more cohesive and well-separated clusters. However, the dataset-specific maximum of ASW is typically unknown, and the standard upper limit of 1 is rarely attainable. In this work, we derive for each data point a sharp upper bound on its silhouette width and aggregate these to obtain a canonical upper bound on the ASW. This bound-often substantially below 1-enhances the interpretability of empirical ASW values by indicating how close a given clustering result is to the best possible outcome on that dataset. It can be used to confirm global optimality, guide the evaluation of clustering solutions, and be refined to incorporate minimum cluster-size constraints for greater practical relevance. Finally, we extend the framework to establish a corresponding bound for the macro-averaged silhouette.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System</title>
<link>https://arxiv.org/abs/2509.08736</link>
<guid>https://arxiv.org/abs/2509.08736</guid>
<content:encoded><![CDATA[

arXiv:2509.08736v2 Announce Type: replace 
Abstract: Bayesian optimization (BO) is a powerful tool for scientific discovery in chemistry, yet its efficiency is often hampered by the sparse experimental data and vast search space. Here, we introduce ChemBOMAS: a large language model (LLM)-enhanced multi-agent system that accelerates BO through synergistic data- and knowledge-driven strategies. Firstly, the data-driven strategy involves an 8B-scale LLM regressor fine-tuned on a mere 1% labeled samples for pseudo-data generation, robustly initializing the optimization process. Secondly, the knowledge-driven strategy employs a hybrid Retrieval-Augmented Generation approach to guide LLM in dividing the search space while mitigating LLM hallucinations. An Upper Confidence Bound algorithm then identifies high-potential subspaces within this established partition. Across the LLM-refined subspaces and supported by LLM-generated data, BO achieves the improvement of effectiveness and efficiency. Comprehensive evaluations across multiple scientific benchmarks demonstrate that ChemBOMAS set a new state-of-the-art, accelerating optimization efficiency by up to 5-fold compared to baseline methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Personalization in Education: A Federated Recommender System for Student Performance Prediction</title>
<link>https://arxiv.org/abs/2509.10516</link>
<guid>https://arxiv.org/abs/2509.10516</guid>
<content:encoded><![CDATA[

arXiv:2509.10516v3 Announce Type: replace 
Abstract: The increasing digitalization of education presents unprecedented opportunities for data-driven personalization, but it also introduces significant challenges to student data privacy. Conventional recommender systems rely on centralized data, a paradigm often incompatible with modern data protection regulations. A novel privacy-preserving recommender system is proposed and evaluated to address this critical issue using Federated Learning (FL). The approach utilizes a Deep Neural Network (DNN) with rich, engineered features from the large-scale ASSISTments educational dataset. A rigorous comparative analysis of federated aggregation strategies was conducted, identifying FedProx as a significantly more stable and effective method for handling heterogeneous student data than the standard FedAvg baseline. The optimized federated model achieves a high-performance F1-Score of 76.28%, corresponding to 92% of the performance of a powerful, centralized XGBoost model. These findings validate that a federated approach can provide highly effective content recommendations without centralizing sensitive student data. Consequently, our work presents a viable and robust solution to the personalization-privacy dilemma in modern educational platforms.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator Learning</title>
<link>https://arxiv.org/abs/2509.12344</link>
<guid>https://arxiv.org/abs/2509.12344</guid>
<content:encoded><![CDATA[

arXiv:2509.12344v3 Announce Type: replace 
Abstract: Deep Operator Networks (DeepONets) have recently emerged as powerful data-driven frameworks for learning nonlinear operators, particularly suited for approximating solutions to partial differential equations. Despite their promising capabilities, the standard implementation of DeepONets, which typically employs fully connected linear layers in the trunk network, can encounter limitations in capturing complex spatial structures inherent to various PDEs. To address this limitation, we introduce Fourier-Embedded trunk networks within the DeepONet architecture, leveraging random fourier feature mappings to enrich spatial representation capabilities. Our proposed Fourier-Embedded DeepONet, FEDONet demonstrates superior performance compared to the traditional DeepONet across a comprehensive suite of PDE-driven datasets, including the two-dimensional Poisson, Burgers', Lorenz-63, Eikonal, Allen-Cahn, and the Kuramoto-Sivashinsky equation. FEDONet delivers consistently superior reconstruction accuracy across all benchmark PDEs, with particularly large relative $L^2$ error reductions observed in chaotic and stiff systems. This study highlights the effectiveness of Fourier embeddings in enhancing neural operator learning, offering a robust and broadly applicable methodology for PDE surrogate modeling.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Convergence of Muon and Beyond</title>
<link>https://arxiv.org/abs/2509.15816</link>
<guid>https://arxiv.org/abs/2509.15816</guid>
<content:encoded><![CDATA[

arXiv:2509.15816v3 Announce Type: replace 
Abstract: The Muon optimizer has demonstrated remarkable empirical success in handling matrix-structured parameters for training neural networks. However, a significant gap remains between its practical performance and theoretical understanding. Existing analyses show that the Muon variants achieve only a suboptimal iteration complexity of $\mathcal{O}(T^{-1/4})$ in stochastic non-convex settings, where $T$ denotes the number of iterations. To explore the theoretical limits of the Muon framework, we analyze two Momentum-based Variance-Reduced variants: a one-batch version (Muon-MVR1) and a two-batch version (Muon-MVR2). We provide the first rigorous proof that incorporating variance reduction enables Muon-MVR2 to attain the optimal iteration complexity of $\tilde{\mathcal{O}}(T^{-1/3})$, thereby matching the theoretical lower bound for this class of problems. Furthermore, our analysis establishes last-iterate convergence guarantees for Muon variants under the Polyak-{\L}ojasiewicz (P{\L}) condition. Extensive experiments on vision (CIFAR-10) and language (C4) benchmarks corroborate our theoretical findings on per-iteration convergence. Overall, this work offers the first proof of optimality for a Muon-style optimizer and clarifies the path toward developing more practically efficient, accelerated variants.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeMosaic: Temporal Heterogeneity Guided Time Series Forecasting via Adaptive Granularity Patch and Segment-wise Decoding</title>
<link>https://arxiv.org/abs/2509.19406</link>
<guid>https://arxiv.org/abs/2509.19406</guid>
<content:encoded><![CDATA[

arXiv:2509.19406v3 Announce Type: replace 
Abstract: Multivariate time series forecasting is essential in domains such as finance, transportation, climate, and energy. However, existing patch-based methods typically adopt fixed-length segmentation, overlooking the heterogeneity of local temporal dynamics and the decoding heterogeneity of forecasting. Such designs lose details in information-dense regions, introduce redundancy in stable segments, and fail to capture the distinct complexities of short-term and long-term horizons. We propose TimeMosaic, a forecasting framework that aims to address temporal heterogeneity. TimeMosaic employs adaptive patch embedding to dynamically adjust granularity according to local information density, balancing motif reuse with structural clarity while preserving temporal continuity. In addition, it introduces segment-wise decoding that treats each prediction horizon as a related subtask and adapts to horizon-specific difficulty and information requirements, rather than applying a single uniform decoder. Extensive evaluations on benchmark datasets demonstrate that TimeMosaic delivers consistent improvements over existing methods, and our model trained on the large-scale corpus with 321 billion observations achieves performance competitive with state-of-the-art TSFMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demystifying Network Foundation Models</title>
<link>https://arxiv.org/abs/2509.23089</link>
<guid>https://arxiv.org/abs/2509.23089</guid>
<content:encoded><![CDATA[

arXiv:2509.23089v3 Announce Type: replace 
Abstract: This work presents a systematic investigation into the latent knowledge encoded within Network Foundation Models (NFMs) that focuses on hidden representations analysis rather than pure downstream task performance. Different from existing efforts, we analyze the models through a three-part evaluation: Embedding Geometry Analysis to assess representation space utilization, Metric Alignment Assessment to measure correspondence with domain-expert features, and Causal Sensitivity Testing to evaluate robustness to protocol perturbations. Using five diverse network datasets spanning controlled and real-world environments, we evaluate four state-of-the-art NFMs, revealing that they all exhibit significant anisotropy, inconsistent feature sensitivity patterns, an inability to separate the high-level context, payload dependency, and other properties. Our work identifies numerous limitations across all models and demonstrates that addressing them can significantly improve model performance (by up to +0.35 $F_1$ score without architectural changes).
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Private Online Learning against an Adaptive Adversary: Realizable and Agnostic Settings</title>
<link>https://arxiv.org/abs/2510.00574</link>
<guid>https://arxiv.org/abs/2510.00574</guid>
<content:encoded><![CDATA[

arXiv:2510.00574v2 Announce Type: replace 
Abstract: We revisit the problem of private online learning, in which a learner receives a sequence of $T$ data points and has to respond at each time-step a hypothesis. It is required that the entire stream of output hypotheses should satisfy differential privacy. Prior work of Golowich and Livni [2021] established that every concept class $\mathcal{H}$ with finite Littlestone dimension $d$ is privately online learnable in the realizable setting. In particular, they proposed an algorithm that achieves an $O_{d}(\log T)$ mistake bound against an oblivious adversary. However, their approach yields a suboptimal $\tilde{O}_{d}(\sqrt{T})$ bound against an adaptive adversary. In this work, we present a new algorithm with a mistake bound of $O_{d}(\log T)$ against an adaptive adversary, closing this gap. We further investigate the problem in the agnostic setting, which is more general than the realizable setting as it does not impose any assumptions on the data. We give an algorithm that obtains a sublinear regret of $\tilde{O}_d(\sqrt{T})$ for generic Littlestone classes, demonstrating that they are also privately online learnable in the agnostic setting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectifying Regression in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.00885</link>
<guid>https://arxiv.org/abs/2510.00885</guid>
<content:encoded><![CDATA[

arXiv:2510.00885v2 Announce Type: replace 
Abstract: This paper investigates the impact of the loss function in value-based methods for reinforcement learning through an analysis of underlying prediction objectives. We theoretically show that mean absolute error is a better prediction objective than the traditional mean squared error for controlling the learned policy's suboptimality gap. Furthermore, we present results that different loss functions are better aligned with these different regression objectives: binary and categorical cross-entropy losses with the mean absolute error and squared loss with the mean squared error. We then provide empirical evidence that algorithms minimizing these cross-entropy losses can outperform those based on the squared loss in linear reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints</title>
<link>https://arxiv.org/abs/2510.04058</link>
<guid>https://arxiv.org/abs/2510.04058</guid>
<content:encoded><![CDATA[

arXiv:2510.04058v2 Announce Type: replace 
Abstract: For a responsible and safe deployment of diffusion models in various domains, regulating the generated outputs from these models is desirable because such models could generate undesired, violent, and obscene outputs. To tackle this problem, recent works use machine unlearning methodology to forget training data points containing these undesired features from pre-trained generative models. However, these methods proved to be ineffective in data-constrained settings where the whole training dataset is inaccessible. Thus, the principal objective of this work is to propose a machine unlearning methodology that can prevent the generation of outputs containing undesired features from a pre-trained diffusion model in such a data-constrained setting. Our proposed method, termed as Variational Diffusion Unlearning (VDU), is a computationally efficient method that only requires access to a subset of training data containing undesired features. Our approach is inspired by the variational inference framework with the objective of minimizing a loss function consisting of two terms: plasticity inducer and stability regularizer. Plasticity inducer reduces the log-likelihood of the undesired training data points, while the stability regularizer, essential for preventing loss of image generation quality, regularizes the model in parameter space. We validate the effectiveness of our method through comprehensive experiments for both class unlearning and feature unlearning. For class unlearning, we unlearn some user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a pre-trained unconditional denoising diffusion probabilistic model (DDPM). Similarly, for feature unlearning, we unlearn the generation of certain high-level features from a pre-trained Stable Diffusion model
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics</title>
<link>https://arxiv.org/abs/2510.06367</link>
<guid>https://arxiv.org/abs/2510.06367</guid>
<content:encoded><![CDATA[

arXiv:2510.06367v2 Announce Type: replace 
Abstract: Neural ODEs are a widely used, powerful machine learning technique in particular for physics. However, not every solution is physical in that it is an Euler-Lagrange equation. We present Helmholtz metrics to quantify this resemblance for a given ODE and demonstrate their capabilities on several fundamental systems with noise. We combine them with a second order neural ODE to form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations in a direct fashion and with zero additional inference cost. We demonstrate that, using only positional data, they can distinguish Lagrangian and non-Lagrangian systems and improve the neural ODE solutions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Markovian Thinker</title>
<link>https://arxiv.org/abs/2510.06557</link>
<guid>https://arxiv.org/abs/2510.06557</guid>
<content:encoded><![CDATA[

arXiv:2510.06557v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL "thinking environment", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops</title>
<link>https://arxiv.org/abs/2510.08662</link>
<guid>https://arxiv.org/abs/2510.08662</guid>
<content:encoded><![CDATA[

arXiv:2510.08662v2 Announce Type: replace 
Abstract: Genomic Selection (GS) uses whole-genome information to predict crop phenotypes and accelerate breeding. Traditional GS methods, however, struggle with prediction accuracy for complex traits and large datasets. We propose DPCformer, a deep learning model integrating convolutional neural networks with a self-attention mechanism to model complex genotype-phenotype relationships. We applied DPCformer to 13 traits across five crops (maize, cotton, tomato, rice, chickpea). Our approach uses an 8-dimensional one-hot encoding for SNP data, ordered by chromosome, and employs the PMF algorithm for feature selection. Evaluations show DPCformer outperforms existing methods. In maize datasets, accuracy for traits like days to tasseling and plant height improved by up to 2.92%. For cotton, accuracy gains for fiber traits reached 8.37%. On small-sample tomato data, the Pearson Correlation Coefficient for a key trait increased by up to 57.35%. In chickpea, the yield correlation was boosted by 16.62%. DPCformer demonstrates superior accuracy, robustness in small-sample scenarios, and enhanced interpretability, providing a powerful tool for precision breeding and addressing global food security challenges.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Driving Control for Autonomous Vehicles: An Intelligent General-sum Constrained Adversarial Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2510.09041</link>
<guid>https://arxiv.org/abs/2510.09041</guid>
<content:encoded><![CDATA[

arXiv:2510.09041v2 Announce Type: replace 
Abstract: Deep reinforcement learning (DRL) has demonstrated remarkable success in developing autonomous driving policies. However, its vulnerability to adversarial attacks remains a critical barrier to real-world deployment. Although existing robust methods have achieved success, they still suffer from three key issues: (i) these methods are trained against myopic adversarial attacks, limiting their abilities to respond to more strategic threats, (ii) they have trouble causing truly safety-critical events (e.g., collisions), but instead often result in minor consequences, and (iii) these methods can introduce learning instability and policy drift during training due to the lack of robust constraints. To address these issues, we propose Intelligent General-sum Constrained Adversarial Reinforcement Learning (IGCARL), a novel robust autonomous driving approach that consists of a strategic targeted adversary and a robust driving agent. The strategic targeted adversary is designed to leverage the temporal decision-making capabilities of DRL to execute strategically coordinated multi-step attacks. In addition, it explicitly focuses on inducing safety-critical events by adopting a general-sum objective. The robust driving agent learns by interacting with the adversary to develop a robust autonomous driving policy against adversarial attacks. To ensure stable learning in adversarial environments and to mitigate policy drift caused by attacks, the agent is optimized under a constrained formulation. Extensive experiments show that IGCARL improves the success rate by at least 27.9% over state-of-the-art methods, demonstrating superior robustness to adversarial attacks and enhancing the safety and reliability of DRL-based autonomous driving.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Bayesian Flow Networks for Molecular Graph Generation</title>
<link>https://arxiv.org/abs/2510.10211</link>
<guid>https://arxiv.org/abs/2510.10211</guid>
<content:encoded><![CDATA[

arXiv:2510.10211v2 Announce Type: replace 
Abstract: Molecular graph generation is essentially a classification generation problem, aimed at predicting categories of atoms and bonds. Currently, prevailing paradigms such as continuous diffusion models are trained to predict continuous numerical values, treating the training process as a regression task. However, the final generation necessitates a rounding step to convert these predictions back into discrete classification categories, which is intrinsically a classification operation. Given that the rounding operation is not incorporated during training, there exists a significant discrepancy between the model's training objective and its inference procedure. As a consequence, an excessive emphasis on point-wise precision can lead to overfitting and inefficient learning. This occurs because considerable efforts are devoted to capturing intra-bin variations that are ultimately irrelevant to the discrete nature of the task at hand. Such a flaw results in diminished molecular diversity and constrains the model's generalization capabilities. To address this fundamental limitation, we propose GraphBFN, a novel hierarchical coarse-to-fine framework based on Bayesian Flow Networks that operates on the parameters of distributions. By innovatively introducing Cumulative Distribution Function, GraphBFN is capable of calculating the probability of selecting the correct category, thereby unifying the training objective with the sampling rounding operation. We demonstrate that our method achieves superior performance and faster generation, setting new state-of-the-art results on the QM9 and ZINC250k molecular graph generation benchmarks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Evolving Nature of Latent Spaces: From GANs to Diffusion</title>
<link>https://arxiv.org/abs/2510.17383</link>
<guid>https://arxiv.org/abs/2510.17383</guid>
<content:encoded><![CDATA[

arXiv:2510.17383v2 Announce Type: replace 
Abstract: This paper examines the evolving nature of internal representations in generative visual models, focusing on the conceptual and technical shift from GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's account of synthesis as the amalgamation of distributed representations, we propose a distinction between "synthesis in a strict sense", where a compact latent space wholly determines the generative process, and "synthesis in a broad sense," which characterizes models whose representational labor is distributed across layers. Through close readings of model architectures and a targeted experimental setup that intervenes in layerwise representations, we show how diffusion models fragment the burden of representation and thereby challenge assumptions of unified internal space. By situating these findings within media theoretical frameworks and critically engaging with metaphors such as the latent space and the Platonic Representation Hypothesis, we argue for a reorientation of how generative AI is understood: not as a direct synthesis of content, but as an emergent configuration of specialized processes.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local properties of neural networks through the lens of layer-wise Hessians</title>
<link>https://arxiv.org/abs/2510.17486</link>
<guid>https://arxiv.org/abs/2510.17486</guid>
<content:encoded><![CDATA[

arXiv:2510.17486v2 Announce Type: replace 
Abstract: We introduce a methodology for analyzing neural networks through the lens of layer-wise Hessian matrices. The local Hessian of each functional block (layer) is defined as the matrix of second derivatives of a scalar function with respect to the parameters of that layer. This concept provides a formal tool for characterizing the local geometry of the parameter space. We show that the spectral properties of local Hessians, such as the distribution of eigenvalues, reveal quantitative patterns associated with overfitting, underparameterization, and expressivity in neural network architectures. We conduct an extensive empirical study involving 111 experiments across 37 datasets. The results demonstrate consistent structural regularities in the evolution of local Hessians during training and highlight correlations between their spectra and generalization performance. These findings establish a foundation for using local geometric analysis to guide the diagnosis and design of deep neural networks. The proposed framework connects optimization geometry with functional behavior and offers practical insight for improving network architectures and training stability.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees</title>
<link>https://arxiv.org/abs/2510.18406</link>
<guid>https://arxiv.org/abs/2510.18406</guid>
<content:encoded><![CDATA[

arXiv:2510.18406v2 Announce Type: replace 
Abstract: Weakly supervised learning often operates with coarse aggregate signals rather than instance labels. We study a setting where each training example is an $n$-tuple containing exactly m positives, while only the count m per tuple is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g., image classification with region proposals and multi-instance measurements. We show that tuple counts admit a trainable unbiased risk estimator (URE) by linking the tuple-generation process to latent instance marginals. Starting from fixed (n,m), we derive a closed-form URE and extend it to variable tuple sizes, variable counts, and their combination. Identification holds whenever the effective mixing rate is separated from the class prior. We establish generalization bounds via Rademacher complexity and prove statistical consistency with standard rates under mild regularity assumptions. To improve finite-sample stability, we introduce simple ReLU corrections to the URE that preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the approach consistently outperforms representative weak-supervision baselines and yields favorable precision-recall and F1 trade-offs. It remains robust under class-prior imbalance and across diverse tuple configurations, demonstrating that count-only supervision can be exploited effectively through a theoretically grounded and practically stable objective.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training</title>
<link>https://arxiv.org/abs/2510.18784</link>
<guid>https://arxiv.org/abs/2510.18784</guid>
<content:encoded><![CDATA[

arXiv:2510.18784v2 Announce Type: replace 
Abstract: Despite significant work on low-bit quantization-aware training (QAT), there is still an accuracy gap between such techniques and native training. To address this, we introduce CAGE (Curvature-Aware Gradient Estimation), a new QAT method that augments the straight-through estimator (STE) gradient with a curvature-aware correction designed to counteract the loss increase induced by quantization. CAGE is derived from a multi-objective view of QAT that balances loss minimization with the quantization constraints, yielding a principled correction term that depends on local curvature information. On the theoretical side, we introduce the notion of Pareto-optimal solutions for quantized optimization, and establish that CAGE yields strong convergence guarantees in the smooth non-convex setting. In terms of implementation, our approach is optimizer-agnostic, but we provide a highly-efficient implementation that leverages Adam statistics. CAGE significantly improves upon the prior state-of-the-art methods in terms of accuracy, for similar computational cost: for QAT fine-tuning, it halves the compression accuracy loss relative to the prior best method, while for QAT pre-training of Llama models, its accuracy for 3-bit weights-and-activations (W3A3) matches the accuracy achieved at 4-bits (W4A4) with the prior best method. The official implementation can be found over https://github.com/IST-DASLab/CAGE .
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Policy Learning with Abstention</title>
<link>https://arxiv.org/abs/2510.19672</link>
<guid>https://arxiv.org/abs/2510.19672</guid>
<content:encoded><![CDATA[

arXiv:2510.19672v2 Announce Type: replace 
Abstract: Policy learning algorithms are widely used in areas such as personalized medicine and advertising to develop individualized treatment regimes. However, most methods force a decision even when predictions are uncertain, which is risky in high-stakes settings. We study policy learning with abstention, where a policy may defer to a safe default or an expert. When a policy abstains, it receives a small additive reward on top of the value of a random guess. We propose a two-stage learner that first identifies a set of near-optimal policies and then constructs an abstention rule from their disagreements. We establish fast O(1/n)-type regret guarantees when propensities are known, and extend these guarantees to the unknown-propensity case via a doubly robust (DR) objective. We further show that abstention is a versatile tool with direct applications to other core problems in policy learning: it yields improved guarantees under margin conditions without the common realizability assumption, connects to distributionally robust policy learning by hedging against small data shifts, and supports safe policy improvement by ensuring improvement over a baseline policy with high probability.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shift is Good: Mismatched Data Mixing Improves Test Performance</title>
<link>https://arxiv.org/abs/2510.25108</link>
<guid>https://arxiv.org/abs/2510.25108</guid>
<content:encoded><![CDATA[

arXiv:2510.25108v2 Announce Type: replace 
Abstract: We consider training and testing on mixture distributions with different training and test proportions. We show that in many settings, and in some sense generically, distribution shift can be beneficial, and test performance can improve due to mismatched training proportions, even if the components are unrelated and with no transfer between components. In a variety of scenarios, we identify the optimal training proportions and the extent to which such distribution shift can be beneficial. We show how the same analysis applies also to a compositional setting with differing distribution of component "skills'' at training and test.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedAdamW: A Communication-Efficient Optimizer with Convergence and Generalization Guarantees for Federated Large Models</title>
<link>https://arxiv.org/abs/2510.27486</link>
<guid>https://arxiv.org/abs/2510.27486</guid>
<content:encoded><![CDATA[

arXiv:2510.27486v2 Announce Type: replace 
Abstract: AdamW has become one of the most effective optimizers for training large-scale models. We have also observed its effectiveness in the context of federated learning (FL). However, directly applying AdamW in federated learning settings poses significant challenges: (1) due to data heterogeneity, AdamW often yields high variance in the second-moment estimate $\boldsymbol{v}$; (2) the local overfitting of AdamW may cause client drift; and (3) Reinitializing moment estimates ($\boldsymbol{v}$, $\boldsymbol{m}$) at each round slows down convergence. To address these challenges, we propose the first \underline{Fed}erated \underline{AdamW} algorithm, called \texttt{FedAdamW}, for training and fine-tuning various large models. \texttt{FedAdamW} aligns local updates with the global update using both a \textbf{local correction mechanism} and decoupled weight decay to mitigate local overfitting. \texttt{FedAdamW} efficiently aggregates the \texttt{mean} of the second-moment estimates to reduce their variance and reinitialize them. Theoretically, we prove that \texttt{FedAdamW} achieves a linear speedup convergence rate of $\mathcal{O}(\sqrt{(L \Delta \sigma_l^2)/(S K R \epsilon^2)}+(L \Delta)/R)$ without \textbf{heterogeneity assumption}, where $S$ is the number of participating clients per round, $K$ is the number of local iterations, and $R$ is the total number of communication rounds. We also employ PAC-Bayesian generalization analysis to explain the effectiveness of decoupled weight decay in local training. Empirically, we validate the effectiveness of \texttt{FedAdamW} on language and vision Transformer models. Compared to several baselines, \texttt{FedAdamW} significantly reduces communication rounds and improves test accuracy. The code is available in https://github.com/junkangLiu0/FedAdamW.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Planning for Language Models</title>
<link>https://arxiv.org/abs/2511.00521</link>
<guid>https://arxiv.org/abs/2511.00521</guid>
<content:encoded><![CDATA[

arXiv:2511.00521v2 Announce Type: replace 
Abstract: Selecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy. We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fixed generation distributions and candidate sizes. Building on these insights, we introduce EPIC, an Ensemble Planning with Contrastive learning framework to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. EPIC incorporates our probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead. Our code can be found at https://github.com/nguyenngocbaocmt02/EPIC.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sensitivity Analysis for Climate Science with Generative Flow Models</title>
<link>https://arxiv.org/abs/2511.00663</link>
<guid>https://arxiv.org/abs/2511.00663</guid>
<content:encoded><![CDATA[

arXiv:2511.00663v2 Announce Type: replace 
Abstract: Sensitivity analysis is a cornerstone of climate science, essential for understanding phenomena ranging from storm intensity to long-term climate feedbacks. However, computing these sensitivities using traditional physical models is often prohibitively expensive in terms of both computation and development time. While modern AI-based generative models are orders of magnitude faster to evaluate, computing sensitivities with them remains a significant bottleneck. This work addresses this challenge by applying the adjoint state method for calculating gradients in generative flow models. We apply this method to the cBottle generative model, trained on ERA5 and ICON data, to perform sensitivity analysis of any atmospheric variable with respect to sea surface temperatures. We quantitatively validate the computed sensitivities against the model's own outputs. Our results provide initial evidence that this approach can produce reliable gradients, reducing the computational cost of sensitivity analysis from weeks on a supercomputer with a physical model to hours on a GPU, thereby simplifying a critical workflow in climate science. The code can be found at https://github.com/Kwartzl8/cbottle_adjoint_sensitivity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment</title>
<link>https://arxiv.org/abs/2511.00804</link>
<guid>https://arxiv.org/abs/2511.00804</guid>
<content:encoded><![CDATA[

arXiv:2511.00804v3 Announce Type: replace 
Abstract: Erasing harmful or proprietary concepts from powerful text to image generators is an emerging safety requirement, yet current "concept erasure" techniques either collapse image quality, rely on brittle adversarial losses, or demand prohibitive retraining cycles. We trace these limitations to a myopic view of the denoising trajectories that govern diffusion based generation. We introduce EraseFlow, the first framework that casts concept unlearning as exploration in the space of denoising paths and optimizes it with GFlowNets equipped with the trajectory balance objective. By sampling entire trajectories rather than single end states, EraseFlow learns a stochastic policy that steers generation away from target concepts while preserving the model's prior. EraseFlow eliminates the need for carefully crafted reward models and by doing this, it generalizes effectively to unseen concepts and avoids hackable rewards while improving the performance. Extensive empirical results demonstrate that EraseFlow outperforms existing baselines and achieves an optimal trade off between performance and prior preservation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Priors in Time: Missing Inductive Biases for Language Model Interpretability</title>
<link>https://arxiv.org/abs/2511.01836</link>
<guid>https://arxiv.org/abs/2511.01836</guid>
<content:encoded><![CDATA[

arXiv:2511.01836v2 Announce Type: replace 
Abstract: Recovering meaningful concepts from language model activations is a central aim of interpretability. While existing feature extraction methods aim to identify concepts that are independent directions, it is unclear if this assumption can capture the rich temporal structure of language. Specifically, via a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose priors that assume independence of concepts across time, implying stationarity. Meanwhile, language model representations exhibit rich temporal dynamics, including systematic growth in conceptual dimensionality, context-dependent correlations, and pronounced non-stationarity, in direct conflict with the priors of SAEs. Taking inspiration from computational neuroscience, we introduce a new interpretability objective -- Temporal Feature Analysis -- which possesses a temporal inductive bias to decompose representations at a given time into two parts: a predictable component, which can be inferred from the context, and a residual component, which captures novel information unexplained by the context. Temporal Feature Analyzers correctly parse garden path sentences, identify event boundaries, and more broadly delineate abstract, slow-moving information from novel, fast-moving information, while existing SAEs show significant pitfalls in all the above tasks. Overall, our results underscore the need for inductive biases that match the data in designing robust interpretability tools.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixed Semi-Supervised Generalized-Linear-Regression with Applications to Deep-Learning and Interpolators</title>
<link>https://arxiv.org/abs/2302.09526</link>
<guid>https://arxiv.org/abs/2302.09526</guid>
<content:encoded><![CDATA[

arXiv:2302.09526v5 Announce Type: replace-cross 
Abstract: We present a methodology for using unlabeled data to design semi-supervised learning (SSL) methods that improve the predictive performance of supervised learning for regression tasks. The main idea is to design different mechanisms for integrating the unlabeled data, and include in each of them a mixing parameter $\alpha$, controlling the weight given to the unlabeled data. Focusing on Generalized Linear Models (GLM) and linear interpolators classes of models, we analyze the characteristics of different mixing mechanisms, and prove that it is consistently beneficial to integrate the unlabeled data with some nonzero mixing ratio $\alpha>0$, in terms of predictive performance. Moreover, we provide a rigorous framework to estimate the best mixing ratio where mixed-SSL delivers the best predictive performance, while using the labeled and unlabeled data on hand. The effectiveness of our methodology in delivering substantial improvement compared to the standard supervised models, in a variety of settings, is demonstrated empirically through extensive simulation, providing empirical support for our theoretical analysis. We also demonstrate the applicability of our methodology (with some heuristic modifications) to improve more complex models, such as deep neural networks, in real-world regression tasks
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Last-Iterate Convergence of Adaptive Riemannian Gradient Descent for Equilibrium Computation</title>
<link>https://arxiv.org/abs/2306.16617</link>
<guid>https://arxiv.org/abs/2306.16617</guid>
<content:encoded><![CDATA[

arXiv:2306.16617v2 Announce Type: replace-cross 
Abstract: Equilibrium computation on Riemannian manifolds provides a unifying framework for numerous problems in machine learning and data analytics. One of the simplest yet most fundamental methods is Riemannian gradient descent (RGD). While its Euclidean counterpart has been extensively studied, it remains unclear how the manifold curvature affects RGD in game-theoretic settings. This paper addresses this gap by establishing new convergence results for \textit{geodesic strongly monotone} games. Our key result shows that RGD attains last-iterate linear convergence in a \textit{geometry-agnostic} fashion, a key property for applications in machine learning. We extend this guarantee to stochastic and adaptive variants -- SRGD and FARGD -- and establish that: (i) the sample complexity of SRGD is geometry-agnostic and optimal with respect to noise; (ii) FARGD matches the convergence rate of its non-adaptive counterpart up to constant factors, while avoiding reliance on the condition number. Overall, this paper presents the first geometry-agnostic last-iterate convergence analysis for games beyond the Euclidean settings, underscoring the surprising power of RGD -- despite its simplicity -- in solving a wide spectrum of machine learning problems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Dynamic Variational Autoencoder for Counterfactual Regression in Longitudinal Data</title>
<link>https://arxiv.org/abs/2310.10559</link>
<guid>https://arxiv.org/abs/2310.10559</guid>
<content:encoded><![CDATA[

arXiv:2310.10559v3 Announce Type: replace-cross 
Abstract: Accurately estimating treatment effects over time is crucial in fields such as precision medicine, epidemiology, economics, and marketing. Many current methods for estimating treatment effects over time assume that all confounders are observed or attempt to infer unobserved ones. In contrast, our approach focuses on unobserved adjustment variables, which specifically have a causal effect on the outcome sequence. Under the assumption of unconfoundedness, we address estimating Conditional Average Treatment Effects (CATEs) while accounting for unobserved heterogeneity in response to treatment due to these unobserved adjustment variables. Our proposed Causal Dynamic Variational Autoencoder (CDVAE) is grounded in theoretical guarantees concerning the validity of latent adjustment variables and generalization bounds on CATE estimation error. Extensive evaluations on synthetic and real-world datasets show that CDVAE outperforms existing baselines. Moreover, we demonstrate that state-of-the-art models significantly improve their CATE estimates when augmented with the latent substitutes learned by CDVAE, approaching oracle-level performance without direct access to the true adjustment variables.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPU Cluster Scheduling for Network-Sensitive Deep Learning</title>
<link>https://arxiv.org/abs/2401.16492</link>
<guid>https://arxiv.org/abs/2401.16492</guid>
<content:encoded><![CDATA[

arXiv:2401.16492v2 Announce Type: replace-cross 
Abstract: We propose a novel GPU-cluster scheduler for distributed DL (DDL) workloads that enables proximity based consolidation of GPU resources based on the DDL jobs' sensitivities to the anticipated communication-network delays. Our scheduler consists of three major components: (i) a classical delay scheduling algorithm to facilitate job placement and consolidation; (ii) a network-sensitive job preemption strategy; and (iii) an "auto-tuner" mechanism to optimize delay timers for effective delay scheduling. Additionally, to enable a cost-effective methodology for large-scale experiments, we develop a data-driven DDL cluster simulation platform. Employing the simulation platform we compare against several state-of-the-art alternatives on real-world workload traces to demonstrate the benefits of our design. Our scheduler can provide improvement of up to 69% in end-to-end Makespan for training all jobs compared to the prevailing consolidation-based scheduling methods, while reducing the average job completion time by up to 83% and minimizing the communication overheads by up to 98% under congested networking conditions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-Efficient "Clustering and Conquer" Procedures for Parallel Large-Scale Ranking and Selection</title>
<link>https://arxiv.org/abs/2402.02196</link>
<guid>https://arxiv.org/abs/2402.02196</guid>
<content:encoded><![CDATA[

arXiv:2402.02196v5 Announce Type: replace-cross 
Abstract: This work aims to improve the sample efficiency of parallel large-scale ranking and selection (R&amp;S) problems by leveraging correlation information. We modify the commonly used "divide and conquer" framework in parallel computing by adding a correlation-based clustering step, transforming it into "clustering and conquer". Analytical results under a symmetric benchmark scenario show that this seemingly simple modification yields an $\mathcal{O}(p)$ reduction in sample complexity for a widely used class of sample-optimal R&amp;S procedures. Our approach enjoys two key advantages: 1) it does not require highly accurate correlation estimation or precise clustering, and 2) it allows for seamless integration with various existing R&amp;S procedures, while achieving optimal sample complexity. Theoretically, we develop a novel gradient analysis framework to analyze sample efficiency and guide the design of large-scale R&amp;S procedures. We also introduce a new parallel clustering algorithm tailored for large-scale scenarios. Finally, in large-scale AI applications such as neural architecture search, our methods demonstrate superior performance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Model Empowered Next-Generation MIMO Networks: Fundamentals, Challenges, and Visions</title>
<link>https://arxiv.org/abs/2404.08878</link>
<guid>https://arxiv.org/abs/2404.08878</guid>
<content:encoded><![CDATA[

arXiv:2404.08878v2 Announce Type: replace-cross 
Abstract: Next-generation Multiple-Input Multiple-Output (MIMO) is expected to be intelligent and scalable. In this paper, we study Large Language Model (LLM)-enabled next-generation MIMO networks. Firstly, we provide an overview of the development, fundamentals, and challenges of the next-generation MIMO. Then, we propose the concept of the generative AI agent, which is capable of generating tailored and specialized contents with the aid of LLM and Retrieval Augmented Generation (RAG). Next, we comprehensively discuss the features and advantages of the generative AI agent framework. More importantly, to tackle existing challenges of next-generation MIMO, we discuss generative AI agent-enabled next-generation MIMO networks from the perspective of performance analysis, signal processing, and resource allocation. Furthermore, we present two compelling case studies that demonstrate the effectiveness of leveraging the generative AI agent for performance analysis in complex configuration scenarios. These examples highlight how the integration of generative AI agents can significantly enhance the analysis and design of next-generation MIMO systems. Finally, we discuss important potential research future directions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Linear Optimization with Partial Feedback</title>
<link>https://arxiv.org/abs/2405.16564</link>
<guid>https://arxiv.org/abs/2405.16564</guid>
<content:encoded><![CDATA[

arXiv:2405.16564v3 Announce Type: replace-cross 
Abstract: Contextual linear optimization (CLO) uses predictive contextual features to reduce uncertainty in random cost coefficients in the objective and thereby improve decision-making performance. A canonical example is the stochastic shortest path problem with random edge costs (e.g., travel time) and contextual features (e.g., lagged traffic, weather). While existing work on CLO assumes fully observed cost coefficient vectors, in many applications the decision maker observes only partial feedback corresponding to each chosen decision in the history. In this paper, we study both a bandit-feedback setting (e.g., only the overall travel time of each historical path is observed) and a semi-bandit-feedback setting (e.g., travel times of the individual segments on each chosen path are additionally observed). We propose a unified class of offline learning algorithms for CLO with different types of feedback, following a powerful induced empirical risk minimization (IERM) framework that integrates estimation and optimization. We provide a novel fast-rate regret bound for IERM that allows for misspecified model classes and flexible choices of estimation methods. To solve the partial-feedback IERM, we also tailor computationally tractable surrogate losses. A byproduct of our theory of independent interest is the fast-rate regret bound for IERM with full feedback and a misspecified policy class. We compare the performance of different methods numerically using stochastic shortest path examples on simulated and real data and provide practical insights from the empirical results.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry through Curiosity-Driven Queries</title>
<link>https://arxiv.org/abs/2405.20318</link>
<guid>https://arxiv.org/abs/2405.20318</guid>
<content:encoded><![CDATA[

arXiv:2405.20318v4 Announce Type: replace-cross 
Abstract: Recent progress in Large Language Model (LLM) technology has changed our role in interacting with these models. Instead of primarily testing these models with questions we already know answers to, we are now using them for queries where the answers are unknown to us, driven by human curiosity. This shift highlights the growing need to understand curiosity-driven human questions - those that are more complex, open-ended, and reflective of real-world needs. To this end, we present Quriosity, a collection of 13.5K naturally occurring questions from three diverse sources: human-to-search-engine queries, human-to-human interactions, and human-to-LLM conversations. Our comprehensive collection enables a rich understanding of human curiosity across various domains and contexts. Our analysis reveals a significant presence of causal questions (up to 42%) in the dataset, for which we develop an iterative prompt improvement framework to identify all causal queries and examine their unique linguistic properties, cognitive complexity and source distribution. Our paper paves the way for future work on causal question identification and open-ended chatbot interactions. Our code and data are at https://github.com/roberto-ceraolo/quriosity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uniform Convergence of Adversarially Robust Classifiers</title>
<link>https://arxiv.org/abs/2406.14682</link>
<guid>https://arxiv.org/abs/2406.14682</guid>
<content:encoded><![CDATA[

arXiv:2406.14682v2 Announce Type: replace-cross 
Abstract: In recent years there has been significant interest in the effect of different types of adversarial perturbations in data classification problems. Many of these models incorporate the adversarial power, which is an important parameter with an associated trade-off between accuracy and robustness. This work considers a general framework for adversarially-perturbed classification problems, in a large data or population-level limit. In such a regime, we demonstrate that as adversarial strength goes to zero that optimal classifiers converge to the Bayes classifier in the Hausdorff distance. This significantly strengthens previous results, which generally focus on $L^1$-type convergence. The main argument relies upon direct geometric comparisons and is inspired by techniques from geometric measure theory.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models</title>
<link>https://arxiv.org/abs/2407.01599</link>
<guid>https://arxiv.org/abs/2407.01599</guid>
<content:encoded><![CDATA[

arXiv:2407.01599v3 Announce Type: replace-cross 
Abstract: The rapid evolution of artificial intelligence (AI) through developments in Large Language Models (LLMs) and Vision-Language Models (VLMs) has brought significant advancements across various technological domains. While these models enhance capabilities in natural language processing and visual interactive tasks, their growing adoption raises critical concerns regarding security and ethical alignment. This survey provides an extensive review of the emerging field of jailbreaking--deliberately circumventing the ethical and operational boundaries of LLMs and VLMs--and the consequent development of defense mechanisms. Our study categorizes jailbreaks into seven distinct types and elaborates on defense strategies that address these vulnerabilities. Through this comprehensive examination, we identify research gaps and propose directions for future studies to enhance the security frameworks of LLMs and VLMs. Our findings underscore the necessity for a unified perspective that integrates both jailbreak strategies and defensive solutions to foster a robust, secure, and reliable environment for the next generation of language models. More details can be found on our website: https://chonghan-chen.com/llm-jailbreak-zoo-survey/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Employing Sentence Space Embedding for Classification of Data Stream from Fake News Domain</title>
<link>https://arxiv.org/abs/2407.10807</link>
<guid>https://arxiv.org/abs/2407.10807</guid>
<content:encoded><![CDATA[

arXiv:2407.10807v2 Announce Type: replace-cross 
Abstract: Tabular data is considered the last unconquered castle of deep learning, yet the task of data stream classification is stated to be an equally important and demanding research area. Due to the temporal constraints, it is assumed that deep learning methods are not the optimal solution for application in this field. However, excluding the entire -- and prevalent -- group of methods seems rather rash given the progress that has been made in recent years in its development. For this reason, the following paper is the first to present an approach to natural language data stream classification using the sentence space method, which allows for encoding text into the form of a discrete digital signal. This allows the use of convolutional deep networks dedicated to image classification to solve the task of recognizing fake news based on text data. Based on the real-life Fakeddit dataset, the proposed approach was compared with state-of-the-art algorithms for data stream classification based on generalization ability and time complexity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment</title>
<link>https://arxiv.org/abs/2407.17716</link>
<guid>https://arxiv.org/abs/2407.17716</guid>
<content:encoded><![CDATA[

arXiv:2407.17716v2 Announce Type: replace-cross 
Abstract: Speech emotion recognition (SER) systems often struggle in real-world environments, where ambient noise severely degrades their performance. This paper explores a novel approach that exploits prior knowledge of testing environments to maximize SER performance under noisy conditions. To address this task, we propose a text-guided, environment-aware training where an SER model is trained with contaminated speech samples and their paired noise description. We use a pre-trained text encoder to extract the text-based environment embedding and then fuse it to a transformer-based SER model during training and inference. We demonstrate the effectiveness of our approach through our experiment with the MSP-Podcast corpus and real-world additive noise samples collected from the Freesound and DEMAND repositories. Our experiment indicates that the text-based environment descriptions processed by a large language model (LLM) produce representations that improve the noise-robustness of the SER system. With a contrastive learning (CL)-based representation, our proposed method can be improved by jointly fine-tuning the text encoder with the emotion recognition model. Under the -5dB signal-to-noise ratio (SNR) level, fine-tuning the text encoder improves our CL-based representation method by 76.4% (arousal), 100.0% (dominance), and 27.7% (valence).
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disturbance-based Discretization, Differentiable IDS Channel, and an IDS-Correcting Code for DNA-based Storage</title>
<link>https://arxiv.org/abs/2407.18929</link>
<guid>https://arxiv.org/abs/2407.18929</guid>
<content:encoded><![CDATA[

arXiv:2407.18929v5 Announce Type: replace-cross 
Abstract: With recent advancements in next-generation data storage, especially in biological molecule-based storage, insertion, deletion, and substitution (IDS) error-correcting codes have garnered increased attention. However, a universal method for designing tailored IDS-correcting codes across varying channel settings remains underexplored. We present an autoencoder-based approach, THEA-code, aimed at efficiently generating IDS-correcting codes for complex IDS channels. In the work, a disturbance-based discretization is proposed to discretize the features of the autoencoder, and a simulated differentiable IDS channel is developed as a differentiable alternative for IDS operations. These innovations facilitate the successful convergence of the autoencoder, producing channel-customized IDS-correcting codes that demonstrate commendable performance across complex IDS channels, particularly in realistic DNA-based storage channels.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finite sample learning of moving targets</title>
<link>https://arxiv.org/abs/2408.04406</link>
<guid>https://arxiv.org/abs/2408.04406</guid>
<content:encoded><![CDATA[

arXiv:2408.04406v3 Announce Type: replace-cross 
Abstract: We consider a moving target that we seek to learn from samples. Our results extend randomized techniques developed in control and optimization for a constant target to the case where the target is changing. We derive a novel bound on the number of samples that are required to construct a probably approximately correct (PAC) estimate of the target. Furthermore, when the moving target is a convex polytope, we provide a constructive method of generating the PAC estimate using a mixed integer linear program (MILP). The proposed method is demonstrated on an application to autonomous emergency braking.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient descent with adaptive stepsize converges (nearly) linearly under fourth-order growth</title>
<link>https://arxiv.org/abs/2409.19791</link>
<guid>https://arxiv.org/abs/2409.19791</guid>
<content:encoded><![CDATA[

arXiv:2409.19791v2 Announce Type: replace-cross 
Abstract: A prevalent belief among optimization specialists is that linear convergence of gradient descent is contingent on the function growing quadratically away from its minimizers. In this work, we argue that this belief is inaccurate. We show that gradient descent with an adaptive stepsize converges at a local (nearly) linear rate on any smooth function that merely exhibits fourth-order growth away from its minimizer. The adaptive stepsize we propose arises from an intriguing decomposition theorem: any such function admits a smooth manifold around the optimal solution -- which we call the ravine -- so that the function grows at least quadratically away from the ravine and has constant order growth along it. The ravine allows one to interlace many short gradient steps with a single long Polyak gradient step, which together ensure rapid convergence to the minimizer. We illustrate the theory and algorithm on the problems of matrix sensing and factorization and learning a single neuron in the overparameterized regime.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conceptual Belief-Informed Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.01739</link>
<guid>https://arxiv.org/abs/2410.01739</guid>
<content:encoded><![CDATA[

arXiv:2410.01739v4 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has achieved significant success but is hindered by inefficiency and instability, relying on large amounts of trial-and-error data and failing to efficiently use past experiences to guide decisions. However, humans achieve remarkably efficient learning from experience, attributed to abstracting concepts and updating associated probabilistic beliefs by integrating both uncertainty and prior knowledge, as observed by cognitive science. Inspired by this, we introduce Conceptual Belief-Informed Reinforcement Learning to emulate human intelligence (HI-RL), an efficient experience utilization paradigm that can be directly integrated into existing RL frameworks. HI-RL forms concepts by extracting high-level categories of critical environmental information and then constructs adaptive concept-associated probabilistic beliefs as experience priors to guide value or policy updates. We evaluate HI-RL by integrating it into various existing value- and policy-based algorithms (DQN, PPO, SAC, and TD3) and demonstrate consistent improvements in sample efficiency and performance across both discrete and continuous control benchmarks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning</title>
<link>https://arxiv.org/abs/2410.05664</link>
<guid>https://arxiv.org/abs/2410.05664</guid>
<content:encoded><![CDATA[

arXiv:2410.05664v3 Announce Type: replace-cross 
Abstract: As text-to-image diffusion models gain widespread commercial applications, there are increasing concerns about unethical or harmful use, including the unauthorized generation of copyrighted or sensitive content. Concept unlearning has emerged as a promising solution to these challenges by removing undesired and harmful information from the pre-trained model. However, the previous evaluations primarily focus on whether target concepts are removed while preserving image quality, neglecting the broader impacts such as unintended side effects. In this work, we propose Holistic Unlearning Benchmark (HUB), a comprehensive framework for evaluating unlearning methods across six key dimensions: faithfulness, alignment, pinpoint-ness, multilingual robustness, attack robustness, and efficiency. Our benchmark covers 33 target concepts, including 16,000 prompts per concept, spanning four categories: Celebrity, Style, Intellectual Property, and NSFW. Our investigation reveals that no single method excels across all evaluation criteria. By releasing our evaluation code and dataset, we hope to inspire further research in this area, leading to more reliable and effective unlearning methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grouped Discrete Representation for Object-Centric Learning</title>
<link>https://arxiv.org/abs/2411.02299</link>
<guid>https://arxiv.org/abs/2411.02299</guid>
<content:encoded><![CDATA[

arXiv:2411.02299v3 Announce Type: replace-cross 
Abstract: Object-Centric Learning (OCL) aims to discover objects in images or videos by reconstructing the input. Representative methods achieve this by reconstructing the input as its Variational Autoencoder (VAE) discrete representations, which suppress (super-)pixel noise and enhance object separability. However, these methods treat features as indivisible units, overlooking their compositional attributes, and discretize features via scalar code indexes, losing attribute-level similarities and differences. We propose Grouped Discrete Representation (GDR) for OCL. For better generalization, features are decomposed into combinatorial attributes by organized channel grouping. For better convergence, features are quantized into discrete representations via tuple code indexes. Experiments demonstrate that GDR consistently improves both mainstream and state-of-the-art OCL methods across various datasets. Visualizations further highlight GDR's superior object separability and interpretability. The source code is available on https://github.com/Genera1Z/GroupedDiscreteRepresentation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</title>
<link>https://arxiv.org/abs/2411.05007</link>
<guid>https://arxiv.org/abs/2411.05007</guid>
<content:encoded><![CDATA[

arXiv:2411.05007v4 Announce Type: replace-cross 
Abstract: Diffusion models can effectively generate high-quality images. However, as they scale, rising memory demands and higher latency pose substantial deployment challenges. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where existing post-training quantization methods like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing, which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights. Then, we use a high-precision, low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD), while a low-bit quantized branch handles the residuals. This process eases the quantization on both sides. However, naively running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without re-quantization. Extensive experiments on SDXL, PixArt-$\Sigma$, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5$\times$, achieving 3.0$\times$ speedup over the 4-bit weight-only quantization (W4A16) baseline on the 16GB laptop 4090 GPU with INT4 precision. On the latest RTX 5090 desktop with Blackwell architecture, we achieve a 3.1$\times$ speedup compared to the W4A16 model using NVFP4 precision.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How do Machine Learning Models Change?</title>
<link>https://arxiv.org/abs/2411.09645</link>
<guid>https://arxiv.org/abs/2411.09645</guid>
<content:encoded><![CDATA[

arXiv:2411.09645v2 Announce Type: replace-cross 
Abstract: The proliferation of Machine Learning (ML) models and their open-source implementations has transformed Artificial Intelligence research and applications. Platforms like Hugging Face (HF) enable this evolving ecosystem, yet a large-scale longitudinal study of how these models change is lacking. This study addresses this gap by analyzing over 680,000 commits from 100,000 models and 2,251 releases from 202 of these models on HF using repository mining and longitudinal methods. We apply an extended ML change taxonomy to classify commits and use Bayesian networks to model temporal patterns in commit and release activities. Our findings show that commit activities align with established data science methodologies, such as the Cross-Industry Standard Process for Data Mining (CRISP-DM), emphasizing iterative refinement. Release patterns tend to consolidate significant updates, particularly in model outputs, sharing, and documentation, distinguishing them from granular commits. Furthermore, projects with higher popularity exhibit distinct evolutionary paths, often starting from a more mature baseline with fewer foundational commits in their public history. In contrast, those with intensive collaboration show unique documentation and technical evolution patterns. These insights enhance the understanding of model changes on community platforms and provide valuable guidance for best practices in model maintenance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparsifying Suprema of Gaussian Processes</title>
<link>https://arxiv.org/abs/2411.14664</link>
<guid>https://arxiv.org/abs/2411.14664</guid>
<content:encoded><![CDATA[

arXiv:2411.14664v2 Announce Type: replace-cross 
Abstract: We give a dimension-independent sparsification result for suprema of centered Gaussian processes: Let $T$ be any (possibly infinite) bounded set of vectors in $\mathbb{R}^n$, and let $\{\boldsymbol{X}_t := t \cdot \boldsymbol{g} \}_{t\in T}$ be the canonical Gaussian process on $T$, where $\boldsymbol{g}\sim N(0, I_n)$. We show that there is an $O_\varepsilon(1)$-size subset $S \subseteq T$ and a set of real values $\{c_s\}_{s \in S}$ such that the random variable $\sup_{s \in S} \{{\boldsymbol{X}}_s + c_s\}$ is an $\varepsilon$-approximator\,(in $L^1$) of the random variable $\sup_{t \in T} {\boldsymbol{X}}_t$. Notably, the size of the sparsifier $S$ is completely independent of both $|T|$ and the ambient dimension $n$.
  We give two applications of this sparsification theorem:
  - A "Junta Theorem" for Norms: We show that given any norm $\nu(x)$ on $\mathbb{R}^n$, there is another norm $\psi(x)$ depending only on the projection of $x$ onto $O_\varepsilon(1)$ directions, for which $\psi({\boldsymbol{g}})$ is a multiplicative $(1 \pm \varepsilon)$-approximation of $\nu({\boldsymbol{g}})$ with probability $1-\varepsilon$ for ${\boldsymbol{g}} \sim N(0,I_n)$.
  - Sparsification of Convex Sets: We show that any intersection of (possibly infinitely many) halfspaces in $\mathbb{R}^n$ that are at distance $r$ from the origin is $\varepsilon$-close (under $N(0,I_n)$) to an intersection of only $O_{r,\varepsilon}(1)$ halfspaces. This yields new polynomial-time \emph{agnostic learning} and \emph{tolerant property testing} algorithms for intersections of halfspaces.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic interior-point methods for smooth conic optimization with applications</title>
<link>https://arxiv.org/abs/2412.12987</link>
<guid>https://arxiv.org/abs/2412.12987</guid>
<content:encoded><![CDATA[

arXiv:2412.12987v3 Announce Type: replace-cross 
Abstract: Conic optimization plays a crucial role in many machine learning (ML) problems. However, practical algorithms for conic constrained ML problems with large datasets are often limited to specific use cases, as stochastic algorithms for general conic optimization remain underdeveloped. To fill this gap, we introduce a stochastic interior-point method (SIPM) framework for general conic optimization, along with four novel SIPM variants leveraging distinct stochastic gradient estimators. Under mild assumptions, we establish the iteration complexity of our proposed SIPMs, which, up to a polylogarithmic factor, match the best-known {results} in stochastic unconstrained optimization. Finally, our numerical experiments on robust linear regression, multi-task relationship learning, and clustering data streams demonstrate the effectiveness and efficiency of our approach.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Sexual Content Generation via Embedding Distortion in Text-conditioned Diffusion Models</title>
<link>https://arxiv.org/abs/2501.18877</link>
<guid>https://arxiv.org/abs/2501.18877</guid>
<content:encoded><![CDATA[

arXiv:2501.18877v2 Announce Type: replace-cross 
Abstract: Diffusion models show remarkable image generation performance following text prompts, but risk generating sexual contents. Existing approaches, such as prompt filtering, concept removal, and even sexual contents mitigation methods, struggle to defend against adversarial attacks while maintaining benign image quality. In this paper, we propose a novel approach called Distorting Embedding Space (DES), a text encoder-based defense mechanism that effectively tackles these issues through innovative embedding space control. DES transforms unsafe embeddings, extracted from a text encoder using unsafe prompts, toward carefully calculated safe embedding regions to prevent unsafe contents generation, while reproducing the original safe embeddings. DES also neutralizes the ``nudity'' embedding, by aligning it with neutral embedding to enhance robustness against adversarial attacks. As a result, extensive experiments on explicit content mitigation and adaptive attack defense show that DES achieves state-of-the-art (SOTA) defense, with attack success rate (ASR) of 9.47% on FLUX.1, a recent popular model, and 0.52% on the widely adopted Stable Diffusion v1.5. These correspond to ASR reductions of 76.5% and 63.9% compared to previous SOTA methods, EraseAnything and AdvUnlearn, respectively. Furthermore, DES maintains benign image quality, achieving Frechet Inception Distance and CLIP score comparable to those of the original FLUX.1 and Stable Diffusion v1.5.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On (Approximate) Pareto Optimality for the Multinomial Logistic Bandit</title>
<link>https://arxiv.org/abs/2501.19277</link>
<guid>https://arxiv.org/abs/2501.19277</guid>
<content:encoded><![CDATA[

arXiv:2501.19277v3 Announce Type: replace-cross 
Abstract: We provide a new online learning algorithm for tackling the Multinomial Logit Bandit (MNL-Bandit) problem. Despite the challenges posed by the combinatorial nature of the MNL model, we develop a novel Upper Confidence Bound (UCB)-based method that achieves Approximate Pareto Optimality by balancing regret minimization and estimation error of the assortment revenues and the MNL parameters. We develop theoretical guarantees characterizing the tradeoff between regret and estimation error for the MNL-Bandit problem through information-theoretic bounds, and propose a modified UCB algorithm that incorporates forced exploration to improve parameter estimation accuracy while maintaining low regret. Our analysis sheds critical insights into how to optimally balance the collected revenues and the treatment estimation in dynamic assortment optimization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification with the Empirical Neural Tangent Kernel</title>
<link>https://arxiv.org/abs/2502.02870</link>
<guid>https://arxiv.org/abs/2502.02870</guid>
<content:encoded><![CDATA[

arXiv:2502.02870v3 Announce Type: replace-cross 
Abstract: While neural networks have demonstrated impressive performance across various tasks, accurately quantifying uncertainty in their predictions is essential to ensure their trustworthiness and enable widespread adoption in critical systems. Several Bayesian uncertainty quantification (UQ) methods exist that are either cheap or reliable, but not both. We propose a post-hoc, sampling-based UQ method for over-parameterized networks at the end of training. Our approach constructs efficient and meaningful deep ensembles by employing a (stochastic) gradient-descent sampling process on appropriately linearized networks. We demonstrate that our method effectively approximates the posterior of a Gaussian process using the empirical Neural Tangent Kernel. Through a series of numerical experiments, we show that our method not only outperforms competing approaches in computational efficiency-often reducing costs by multiple factors-but also maintains state-of-the-art performance across a variety of UQ metrics for both regression and classification tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Task Representations from In-Context Learning</title>
<link>https://arxiv.org/abs/2502.05390</link>
<guid>https://arxiv.org/abs/2502.05390</guid>
<content:encoded><![CDATA[

arXiv:2502.05390v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable proficiency in in-context learning (ICL), where models adapt to new tasks through example-based prompts without requiring parameter updates. However, understanding how tasks are internally encoded and generalized remains a challenge. To address some of the empirical and technical gaps in the literature, we introduce an automated formulation for encoding task information in ICL prompts as a function of attention heads within the transformer architecture. This approach computes a single task vector as a weighted sum of attention heads, with the weights optimized causally via gradient descent. Our findings show that existing methods fail to generalize effectively to modalities beyond text. In response, we also design a benchmark to evaluate whether a task vector can preserve task fidelity in functional regression tasks. The proposed method successfully extracts task-specific information from in-context demonstrations and excels in both text and regression tasks, demonstrating its generalizability across modalities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCS: Perceived Confidence Scoring of Black Box LLMs with Metamorphic Relations</title>
<link>https://arxiv.org/abs/2502.07186</link>
<guid>https://arxiv.org/abs/2502.07186</guid>
<content:encoded><![CDATA[

arXiv:2502.07186v2 Announce Type: replace-cross 
Abstract: Zero-shot LLMs are now also used for textual classification tasks, e.g., sentiment and bias detection in a sentence or article. However, their performance can be suboptimal in such data annotation tasks. We introduce a novel technique that evaluates an LLM's confidence for classifying a textual input by leveraging Metamorphic Relations (MRs). The MRs generate semantically equivalent yet textually divergent versions of the input. Following the principles of Metamorphic Testing (MT), the mutated versions are expected to have annotation labels similar to the input. By analyzing the consistency of an LLM's responses across these variations, we compute a perceived confidence score (PCS) based on the frequency of the predicted labels. PCS can be used for both single and multiple LLM settings (e.g., when multiple LLMs are vetted in a majority-voting setup). Empirical evaluation shows that our PCS-based approach improves the performance of zero-shot LLMs by 9.3% in textual classification tasks. When multiple LLMs are used in a majority-voting setup, we obtain a performance boost of 5.8% with PCS.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImitDiff: Transferring Foundation-Model Priors for Distraction Robust Visuomotor Policy</title>
<link>https://arxiv.org/abs/2502.09649</link>
<guid>https://arxiv.org/abs/2502.09649</guid>
<content:encoded><![CDATA[

arXiv:2502.09649v2 Announce Type: replace-cross 
Abstract: Visuomotor imitation learning policies enable robots to efficiently acquire manipulation skills from visual demonstrations. However, as scene complexity and visual distractions increase, policies that perform well in simple settings often experience substantial performance degradation. To address this challenge, we propose ImitDiff, a diffusion-based imitation learning policy guided by fine-grained semantics within a dual-resolution workflow. Leveraging pretrained priors of vision-language foundation models, our method transforms high-level instructions into pixel-level visual semantic masks. These masks guide a dual-resolution perception pipeline that captures both global context (e.g., overall layout) from low-resolution observation and fine-grained local features (e.g., geometric details) from high-resolution observation, enabling the policy to focus on task-relevant regions. Additionally, we introduce a consistency-driven diffusion transformer action head that bridges visual semantic conditions and real-time action generation. Extensive experiments demonstrate that ImitDiff outperforms state-of-the-art vision-language manipulation frameworks, as well as visuomotor imitation learning policies, particularly under increased scene complexity and visual distractions. Notably, ImitDiff exhibits strong generalization in zero-shot settings involving novel objects and visual distractions. Furthermore, our consistency-driven action head achieves an order-of-magnitude improvement in inference speed while maintaining competitive success rates.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation</title>
<link>https://arxiv.org/abs/2502.15857</link>
<guid>https://arxiv.org/abs/2502.15857</guid>
<content:encoded><![CDATA[

arXiv:2502.15857v2 Announce Type: replace-cross 
Abstract: Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we propose PPC-GPT, a novel unified framework that systematically addresses both privacy preservation and model compression in federated settings. PPC-GPT works on a server-client federated architecture, where the client sends differentially private (DP) perturbed task-specific data to the server's LLM. The LLM then generates synthetic data along with their corresponding rationales. This synthetic data is subsequently used for both LLM pruning and retraining processes. Our framework's key innovation lies in its holistic integration of privacy-preserving mechanisms, synthetic data generation, and task-specific compression techniques, creating unique benefits through component interaction. Our experiments across diverse text generation tasks demonstrate that PPC-GPT successfully achieves dual objectives: maintaining competitive performance comparable to full-sized LLMs while ensuring robust privacy protection through its federated architecture. Our code has been contributed to the FATE open-source project and is now publicly accessible at \textit{https://github.com/FederatedAI/FATE-LLM/tree/main/python/fate_llm/algo/ppc-gpt}
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting intermittent time series with Gaussian Processes and Tweedie likelihood</title>
<link>https://arxiv.org/abs/2502.19086</link>
<guid>https://arxiv.org/abs/2502.19086</guid>
<content:encoded><![CDATA[

arXiv:2502.19086v5 Announce Type: replace-cross 
Abstract: We adopt Gaussian Processes (GPs) as latent functions for probabilistic forecasting of intermittent time series. The model is trained in a Bayesian framework that accounts for the uncertainty about the latent function. We couple the latent GP variable with two types of forecast distributions: the negative binomial (NegBinGP) and the Tweedie distribution (TweedieGP). While the negative binomial has already been used in forecasting intermittent time series, this is the first time in which a fully parameterized Tweedie density is used for intermittent time series. We properly evaluate the Tweedie density, which has both a point mass at zero and heavy tails, avoiding simplifying assumptions made in existing models. We test our models on thousands of intermittent count time series. Results show that our models provide consistently better probabilistic forecasts than the competitors. In particular, TweedieGP obtains the best estimates of the highest quantiles, thus showing that it is more flexible than NegBinGP.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topology-Aware Conformal Prediction for Stream Networks</title>
<link>https://arxiv.org/abs/2503.04981</link>
<guid>https://arxiv.org/abs/2503.04981</guid>
<content:encoded><![CDATA[

arXiv:2503.04981v2 Announce Type: replace-cross 
Abstract: Stream networks, a unique class of spatiotemporal graphs, exhibit complex directional flow constraints and evolving dependencies, making uncertainty quantification a critical yet challenging task. Traditional conformal prediction methods struggle in this setting due to the need for joint predictions across multiple interdependent locations and the intricate spatio-temporal dependencies inherent in stream networks. Existing approaches either neglect dependencies, leading to overly conservative predictions, or rely solely on data-driven estimations, failing to capture the rich topological structure of the network. To address these challenges, we propose Spatio-Temporal Adaptive Conformal Inference (\texttt{STACI}), a novel framework that integrates network topology and temporal dynamics into the conformal prediction framework. \texttt{STACI} introduces a topology-aware nonconformity score that respects directional flow constraints and dynamically adjusts prediction sets to account for temporal distributional shifts. We provide theoretical guarantees on the validity of our approach and demonstrate its superior performance on both synthetic and real-world datasets. Our results show that \texttt{STACI} effectively balances prediction efficiency and coverage, outperforming existing conformal prediction methods for stream networks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seismic inversion using hybrid quantum neural networks</title>
<link>https://arxiv.org/abs/2503.05009</link>
<guid>https://arxiv.org/abs/2503.05009</guid>
<content:encoded><![CDATA[

arXiv:2503.05009v2 Announce Type: replace-cross 
Abstract: Seismic inversion-including post-stack, pre-stack, and full waveform inversion is compute and memory-intensive. Recently, several approaches, including physics-informed machine learning, have been developed to address some of these limitations. Motivated by the potential of quantum computing, we report on our attempt to map one such classical physics-informed algorithm to a quantum framework. The primary goal is to investigate the technical challenges of this mapping, given that quantum algorithms rely on computing principles fundamentally different from those in classical computing. Quantum computers operate using qubits, which exploit superposition and entanglement, offering the potential to solve classically intractable problems. While current quantum hardware is limited, hybrid quantum-classical algorithms-particularly in quantum machine learning (QML)-demonstrate potential for near-term applications and can be readily simulated. We apply QML to subsurface imaging through the development of a hybrid quantum physics-informed neural network (HQ-PINN) for post-stack and pre-stack seismic inversion. The HQ-PINN architecture adopts an encoder-decoder structure: a hybrid quantum neural network encoder estimates P- and S-impedances from seismic data, while the decoder reconstructs seismic responses using geophysical relationships. Training is guided by minimizing the misfit between the input and reconstructed seismic traces. We systematically assess the impact of quantum layer design, differentiation strategies, and simulator backends on inversion performance. We demonstrate the efficacy of our approach through the inversion of both synthetic and the Sleipner field datasets. The HQ-PINN framework consistently yields accurate results, showcasing quantum computing's promise for geosciences and paving the way for future quantum-enhanced geophysical workflows.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Technical and Legal Aspects of Federated Learning in Bioinformatics: Applications, Challenges and Opportunities</title>
<link>https://arxiv.org/abs/2503.09649</link>
<guid>https://arxiv.org/abs/2503.09649</guid>
<content:encoded><![CDATA[

arXiv:2503.09649v4 Announce Type: replace-cross 
Abstract: Federated learning leverages data across institutions to improve clinical discovery while complying with data-sharing restrictions and protecting patient privacy. This paper provides a gentle introduction to this approach in bioinformatics, and is the first to review key applications in proteomics, genome-wide association studies (GWAS), single-cell and multi-omics studies in their legal as well as methodological and infrastructural challenges. As the evolution of biobanks in genetics and systems biology has proved, accessing more extensive and varied data pools leads to a faster and more robust exploration and translation of results. More widespread use of federated learning may have a similar impact in bioinformatics, allowing academic and clinical institutions to access many combinations of genotypic, phenotypic and environmental information that are undercovered or not included in existing biobanks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantitative Evaluation of Quantum/Classical Neural Network Using a Game Solver Metric</title>
<link>https://arxiv.org/abs/2503.21514</link>
<guid>https://arxiv.org/abs/2503.21514</guid>
<content:encoded><![CDATA[

arXiv:2503.21514v2 Announce Type: replace-cross 
Abstract: To evaluate the performance of quantum computing systems relative to classical counterparts and explore the potential, we propose a game-solving benchmark based on Elo ratings in the game of tic-tac-toe. We compare classical convolutional neural networks (CCNNs), quantum or quantum convolutional neural networks (QNNs, QCNNs), and hybrid classical-quantum neural networks (Hybrid NNs) by assessing their performance based on round-robin matches. Our results show that the Hybrid NNs engines achieve Elo ratings comparable to those of CCNNs engines, while the quantum engines underperform under current hardware constraints. Additionally, we implement a QNN integrated with quantum communication and evaluate its performance to quantify the overhead introduced by noisy quantum channels, and the communication overhead was found to be modest. These results demonstrate the viability of using game-based benchmarks for evaluating quantum computing systems and suggest that quantum communication can be incorporated with limited impact on performance, providing a foundation for future hybrid quantum applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence</title>
<link>https://arxiv.org/abs/2504.02904</link>
<guid>https://arxiv.org/abs/2504.02904</guid>
<content:encoded><![CDATA[

arXiv:2504.02904v3 Announce Type: replace-cross 
Abstract: Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training algorithms and evaluated post-training models by their outputs, it remains understudied how post-training reshapes LLMs internally. In this paper, we compare base and post-trained LLMs mechanistically from four perspectives to better understand post-training effects. Our findings across model families and datasets reveal that: (1) Post-training does not change the factual knowledge storage locations, and it adapts knowledge representations from the base model while developing new knowledge representations; (2) Both truthfulness and refusal can be represented by vectors in the hidden representation space. The truthfulness direction is highly similar between the base and post-trained model, and it is effectively transferable for interventions; (3) The refusal direction is different between the base and post-trained models, and it shows limited forward transferability; (4) Differences in confidence between the base and post-trained models cannot be attributed to entropy neurons. Our study provides insights into the fundamental mechanisms preserved and altered during post-training, facilitates downstream tasks like model steering, and could potentially benefit future research in interpretability and LLM post-training. Our code is publicly available at https://github.com/HZD01/post-training-mechanistic-analysis.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation</title>
<link>https://arxiv.org/abs/2504.03546</link>
<guid>https://arxiv.org/abs/2504.03546</guid>
<content:encoded><![CDATA[

arXiv:2504.03546v3 Announce Type: replace-cross 
Abstract: Multilingual speech translation (ST) and machine translation (MT) in the medical domain enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we present the first systematic study on medical ST, to our best knowledge, by releasing MultiMed-ST, a large-scale ST dataset for the medical domain, spanning all translation directions in five languages: Vietnamese, English, German, French, and Simplified/Traditional Chinese, together with the models. With 290,000 samples, this is the largest medical MT dataset and the largest many-to-many multilingual ST among all domains. Secondly, we present the most comprehensive ST analysis in the field's history, to our best knowledge, including: empirical baselines, bilingual-multilingual comparative study, end-to-end vs. cascaded comparative study, task-specific vs. multi-task sequence-to-sequence comparative study, code-switch analysis, and quantitative-qualitative error analysis. All code, data, and models are available online: https://github.com/leduckhai/MultiMed-ST
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stacking Variational Bayesian Monte Carlo</title>
<link>https://arxiv.org/abs/2504.05004</link>
<guid>https://arxiv.org/abs/2504.05004</guid>
<content:encoded><![CDATA[

arXiv:2504.05004v3 Announce Type: replace-cross 
Abstract: Approximate Bayesian inference for models with computationally expensive, black-box likelihoods poses a significant challenge, especially when the posterior distribution is complex. Many inference methods struggle to explore the parameter space efficiently under a limited budget of likelihood evaluations. Variational Bayesian Monte Carlo (VBMC) is a sample-efficient method that addresses this by building a local surrogate model of the log-posterior. However, its conservative exploration strategy, while promoting stability, can cause it to miss important regions of the posterior, such as distinct modes or long tails. In this work, we introduce Stacking Variational Bayesian Monte Carlo (S-VBMC), a method that overcomes this limitation by constructing a robust, global posterior approximation from multiple independent VBMC runs. Our approach merges these local approximations through a principled and inexpensive post-processing step that leverages VBMC's mixture posterior representation and per-component evidence estimates. Crucially, S-VBMC requires no additional likelihood evaluations and is naturally parallelisable, fitting seamlessly into existing inference workflows. We demonstrate its effectiveness on two synthetic problems designed to challenge VBMC's exploration and two real-world applications from computational neuroscience, showing substantial improvements in posterior approximation quality across all cases. Our code is available as a Python package at https://github.com/acerbilab/svbmc.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness</title>
<link>https://arxiv.org/abs/2504.10514</link>
<guid>https://arxiv.org/abs/2504.10514</guid>
<content:encoded><![CDATA[

arXiv:2504.10514v3 Announce Type: replace-cross 
Abstract: Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Environment-Aware Indoor LoRaWAN Ranging Using Path Loss Model Inversion and Adaptive RSSI Filtering</title>
<link>https://arxiv.org/abs/2505.01185</link>
<guid>https://arxiv.org/abs/2505.01185</guid>
<content:encoded><![CDATA[

arXiv:2505.01185v2 Announce Type: replace-cross 
Abstract: Achieving sub-10 m indoor ranging with LoRaWAN is difficult because multipath, human blockage, and micro-climate dynamics induce non-stationary attenuation in received signal strength indicator (RSSI) measurements. We present a lightweight, interpretable pipeline that couples an environment-aware multi-wall path loss model with a forward-only, innovation-driven Kalman prefilter for RSSI. The model augments distance and wall terms with frequency, signal-to-noise ratio (SNR), and co-located environmental covariates (temperature, relative humidity, carbon dioxide, particulate matter, and barometric pressure), and is inverted deterministically for distance estimation. On a one-year single-gateway office dataset comprising over 2 million uplinks, the approach attains a mean absolute error (MAE) of 4.74 m and a root mean square error (RMSE) of 6.76 m in distance estimation, improving over a COST-231 multi-wall baseline (12.07 m MAE) and its environment-augmented variant (7.76 m MAE. Filtering reduces RSSI volatility from 10.33 to 5.43 dB and halves path loss error to 5.35 dB while raising R-squared from 0.82 to 0.89. The result is a single-anchor LoRaWAN ranging method with constant per-packet cost that is accurate, robust, and interpretable, providing a strong building block for multi-gateway localization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real</title>
<link>https://arxiv.org/abs/2505.07096</link>
<guid>https://arxiv.org/abs/2505.07096</guid>
<content:encoded><![CDATA[

arXiv:2505.07096v5 Announce Type: replace-cross 
Abstract: Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Wind Power Modelling via Heteroscedastic Non-Stationary Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.09026</link>
<guid>https://arxiv.org/abs/2505.09026</guid>
<content:encoded><![CDATA[

arXiv:2505.09026v2 Announce Type: replace-cross 
Abstract: Accurate probabilistic prediction of wind power is crucial for maintaining grid stability and facilitating the efficient integration of renewable energy sources. Gaussian process (GP) models offer a principled framework for quantifying uncertainty; however, conventional approaches typically rely on stationary kernels and homoscedastic noise assumptions, which are inadequate for modelling the inherently non-stationary and heteroscedastic nature of wind speed and power output. We propose a heteroscedastic non-stationary GP framework based on the generalised spectral mixture kernel, enabling the model to capture input-dependent correlations as well as input-dependent variability in wind speed-power data. We evaluate the proposed model on 10-minute supervisory control and data acquisition (SCADA) measurements and compare it against GP variants with stationary and non-stationary kernels, as well as commonly used non-GP probabilistic baselines. The results highlight the necessity of modelling both non-stationarity and heteroscedasticity in wind power prediction and demonstrate the practical value of flexible non-stationary GP models in operational SCADA settings.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Stochastic Approximation and Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2505.11343</link>
<guid>https://arxiv.org/abs/2505.11343</guid>
<content:encoded><![CDATA[

arXiv:2505.11343v3 Announce Type: replace-cross 
Abstract: In this paper, we introduce a new approach to proving the convergence of the Stochastic Approximation (SA) and the Stochastic Gradient Descent (SGD) algorithms. The new approach is based on a concept called GSLLN (Generalized Strong Law of Large Numbers), which extends the traditional SLLN. Using this concept, we provide sufficient conditions for convergence, which effectively decouple the properties of the function whose zero we are trying to find, from the properties of the measurement errors (noise sequence). The new approach provides an alternative to the two widely used approaches, namely the ODE approach and the martingale approach, and also permits a wider class of noise signals than either of the two known approaches. In particular, the ``noise'' or measurement error \textit{need not} have a finite second moment, and under suitable conditions, not even a finite mean. By adapting this method of proof, we also derive sufficient conditions for the convergence of zero-order SGD, wherein the stochastic gradient is computed using $2d$ function evaluations, but no gradient computations. The sufficient conditions derived here are the weakest to date, thus leading to a considerable expansion of the applicability of SA and SGD theory.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains</title>
<link>https://arxiv.org/abs/2506.00708</link>
<guid>https://arxiv.org/abs/2506.00708</guid>
<content:encoded><![CDATA[

arXiv:2506.00708v3 Announce Type: replace-cross 
Abstract: Knowledge graph completion (KGC) aims to predict missing triples in knowledge graphs (KGs) by leveraging existing triples and textual information. Recently, generative large language models (LLMs) have been increasingly employed for graph tasks. However, current approaches typically encode graph context in textual form, which fails to fully exploit the potential of LLMs for perceiving and reasoning about graph structures. To address this limitation, we propose DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion). DrKGC employs a flexible lightweight model training strategy to learn structural embeddings and logical rules within the KG. It then leverages a novel bottom-up graph retrieval method to extract a subgraph for each query guided by the learned rules. Finally, a graph convolutional network (GCN) adapter uses the retrieved subgraph to enhance the structural embeddings, which are then integrated into the prompt for effective LLM fine-tuning. Experimental results on two general domain benchmark datasets and two biomedical datasets demonstrate the superior performance of DrKGC. Furthermore, a realistic case study in the biomedical domain highlights its interpretability and practical utility.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-assimilated model-informed reinforcement learning</title>
<link>https://arxiv.org/abs/2506.01755</link>
<guid>https://arxiv.org/abs/2506.01755</guid>
<content:encoded><![CDATA[

arXiv:2506.01755v3 Announce Type: replace-cross 
Abstract: The control of spatio-temporally chaos is challenging because of high dimensionality and unpredictability. Model-free reinforcement learning (RL) discovers optimal control policies by interacting with the system, typically requiring observations of the full physical state. In practice, sensors often provide only partial and noisy measurements (observations) of the system. The objective of this paper is to develop a framework that enables the control of chaotic systems with partial and noisy observability. The proposed method, data-assimilated model-informed reinforcement learning (DA-MIRL), integrates (i) low-order models to approximate high-dimensional dynamics; (ii) sequential data assimilation to correct the model prediction when observations become available; and (iii) an off-policy actor-critic RL algorithm to adaptively learn an optimal control strategy based on the corrected state estimates. We test DA-MIRL on the spatiotemporally chaotic solutions of the Kuramoto-Sivashinsky equation. We estimate the full state of the environment with (i) a physics-based model, here, a coarse-grained model; and (ii) a data-driven model, here, the control-aware echo state network, which is proposed in this paper. We show that DA-MIRL successfully estimates and suppresses the chaotic dynamics of the environment in real time from partial observations and approximate models. This work opens opportunities for the control of partially observable chaotic systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Average-Iterate to Last-Iterate Convergence in Games: A Reduction and Its Applications</title>
<link>https://arxiv.org/abs/2506.03464</link>
<guid>https://arxiv.org/abs/2506.03464</guid>
<content:encoded><![CDATA[

arXiv:2506.03464v2 Announce Type: replace-cross 
Abstract: The convergence of online learning algorithms in games under self-play is a fundamental question in game theory and machine learning. Among various notions of convergence, last-iterate convergence is particularly desirable, as it reflects the actual decisions made by the learners and captures the day-to-day behavior of the learning dynamics. While many algorithms are known to converge in the average-iterate, achieving last-iterate convergence typically requires considerably more effort in both the design and the analysis of the algorithm. Somewhat surprisingly, we show in this paper that for a large family of games, there exists a simple black-box reduction that transforms the average iterates of an uncoupled learning dynamics into the last iterates of a new uncoupled learning dynamics, thus also providing a reduction from last-iterate convergence to average-iterate convergence. Our reduction applies to games where each player's utility is linear in both their own strategy and the joint strategy of all opponents. This family includes two-player bimatrix games and generalizations such as multi-player polymatrix games. By applying our reduction to the Optimistic Multiplicative Weights Update algorithm, we obtain new state-of-the-art last-iterate convergence rates for uncoupled learning dynamics in multi-player zero-sum polymatrix games: (1) an $O(\frac{\log d}{T})$ last-iterate convergence rate under gradient feedback, representing an exponential improvement in the dependence on the dimension $d$ (i.e., the maximum number of actions available to either player); and (2) an $\widetilde{O}(d^{\frac{1}{5}} T^{-\frac{1}{5}})$ last-iterate convergence rate under bandit feedback, improving upon the previous best rates of $\widetilde{O}(\sqrt{d} T^{-\frac{1}{8}})$ and $\widetilde{O}(\sqrt{d} T^{-\frac{1}{6}})$.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentially Private Distribution Release of Gaussian Mixture Models via KL-Divergence Minimization</title>
<link>https://arxiv.org/abs/2506.03467</link>
<guid>https://arxiv.org/abs/2506.03467</guid>
<content:encoded><![CDATA[

arXiv:2506.03467v2 Announce Type: replace-cross 
Abstract: Gaussian Mixture Models (GMMs) are widely used statistical models for representing multi-modal data distributions, with numerous applications in data mining, pattern recognition, data simulation, and machine learning. However, recent research has shown that releasing GMM parameters poses significant privacy risks, potentially exposing sensitive information about the underlying data. In this paper, we address the challenge of releasing GMM parameters while ensuring differential privacy (DP) guarantees. Specifically, we focus on the privacy protection of mixture weights, component means, and covariance matrices. We propose to use Kullback-Leibler (KL) divergence as a utility metric to assess the accuracy of the released GMM, as it captures the joint impact of noise perturbation on all the model parameters. To achieve privacy, we introduce a DP mechanism that adds carefully calibrated random perturbations to the GMM parameters. Through theoretical analysis, we quantify the effects of privacy budget allocation and perturbation statistics on the DP guarantee, and derive a tractable expression for evaluating KL divergence. We formulate and solve an optimization problem to minimize the KL divergence between the released and original models, subject to a given $(\epsilon, \delta)$-DP constraint. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach achieves strong privacy guarantees while maintaining high utility.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashMoE: Fast Distributed MoE in a Single Kernel</title>
<link>https://arxiv.org/abs/2506.04667</link>
<guid>https://arxiv.org/abs/2506.04667</guid>
<content:encoded><![CDATA[

arXiv:2506.04667v3 Announce Type: replace-cross 
Abstract: The computational sparsity of Mixture-of-Experts (MoE) models enables sub-linear growth in compute cost as model size increases, thus offering a scalable path to training massive neural networks. However, existing implementations suffer from low GPU utilization, significant latency overhead, and a fundamental inability to leverage task locality, primarily due to CPU-managed scheduling, host-initiated communication, and frequent kernel launches. To overcome these limitations, we develop FlashMoE, a fully GPU-resident MoE operator that fuses expert computation and inter-GPU communication into a single persistent GPU kernel. FlashMoE enables fine-grained pipelining of dispatch, compute, and combine phases, eliminating launch overheads and reducing idle gaps. Unlike existing work, FlashMoE eliminates bulk-synchronous collectives for one-sided, device-initiated, inter-GPU (R)DMA transfers, thereby unlocking payload efficiency by eliminating bloated or redundant network payloads in sparsely activated layers. When evaluated on an 8-H100 GPU node with MoE models comprising up to 128 experts and 16K token sequences, FlashMoE achieves up to 9x higher GPU utilization, 6x lower latency, 5.7x higher throughput, and 4x better overlap efficiency compared to state-of-the-art baselines, despite using FP32, whereas the baselines use FP16. FlashMoE shows that principled GPU kernel-hardware co-design is key to unlocking the performance ceiling of large-scale distributed ML. We provide code at https://github.com/osayamenja/FlashMoE.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fairness-Aware Strategy for B5G Physical-layer Security Leveraging Reconfigurable Intelligent Surfaces</title>
<link>https://arxiv.org/abs/2506.06344</link>
<guid>https://arxiv.org/abs/2506.06344</guid>
<content:encoded><![CDATA[

arXiv:2506.06344v4 Announce Type: replace-cross 
Abstract: Reconfigurable Intelligent Surfaces are composed of physical elements that can dynamically alter electromagnetic wave properties to enhance beamforming and lead to improvements in areas with low coverage properties. When combined with Reinforcement Learning techniques, they have the potential to enhance both system behavior and physical-layer security hardening. In addition to security improvements, it is crucial to consider the concept of fair communication. Reconfigurable Intelligent Surfaces must ensure that User Equipment units receive their signals with adequate strength, without other units being deprived of ser- vice due to insufficient power. In this paper, we address such a problem. We explore the fairness properties of previous work and propose a novel method that aims at obtaining both an efficient and fair duplex Reconfigurable Intelligent Surface-Reinforcement Learning system for multiple legitimate User Equipment units without reducing the level of achieved physical-layer security hardening. In terms of contributions, we uncover a fairness imbalance of a previous physical-layer security hardening solution, validate our findings and report experimental work via simulation results. We also provide an alternative reward strategy to solve the uncovered problems and release both code and datasets to foster further research in the topics of this paper.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion</title>
<link>https://arxiv.org/abs/2506.08009</link>
<guid>https://arxiv.org/abs/2506.08009</guid>
<content:encoded><![CDATA[

arXiv:2506.08009v2 Announce Type: replace-cross 
Abstract: We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention on flow control: transformer-based reinforcement learning for lift regulation in highly disturbed flows</title>
<link>https://arxiv.org/abs/2506.10153</link>
<guid>https://arxiv.org/abs/2506.10153</guid>
<content:encoded><![CDATA[

arXiv:2506.10153v3 Announce Type: replace-cross 
Abstract: A linear flow control strategy designed for weak disturbances may not remain effective in sequences of strong disturbances due to nonlinear interactions, but it is sensible to leverage it for developing a better strategy. In the present study, we propose a transformer-based reinforcement learning (RL) framework to learn an effective control strategy for regulating aerodynamic lift in arbitrarily long gust sequences via pitch control. The random gusts produce intermittent, high-variance flows observed only through limited surface pressure sensors, making this control problem inherently challenging compared to stationary flows. The transformer addresses the challenge of partial observability from the limited surface pressures. We demonstrate that the training can be accelerated with two techniques -- pretraining with an expert policy (here, linear control) and task-level transfer learning (here, extending a policy trained on isolated gusts to multiple gusts). We show that the learned strategy outperforms the best proportional control, with the performance gap widening as the number of gusts increases. The control strategy learned in an environment with a small number of successive gusts is shown to effectively generalize to an environment with an arbitrarily long sequence of gusts. We investigate the pivot configuration and show that quarter-chord pitching control can achieve superior lift regulation with substantially less control effort compared to mid-chord pitching control. Through a decomposition of the lift, we attribute this advantage to the dominant added-mass contribution accessible via quarter-chord pitching.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CT Radiomics-Based Explainable Machine Learning Model for Accurate Differentiation of Malignant and Benign Endometrial Tumors: A Two-Center Study</title>
<link>https://arxiv.org/abs/2506.18106</link>
<guid>https://arxiv.org/abs/2506.18106</guid>
<content:encoded><![CDATA[

arXiv:2506.18106v2 Announce Type: replace-cross 
Abstract: Aimed to develop and validate a CT radiomics-based explainable machine learning model for precise diagnosing malignancy and benignity specifically in endometrial cancer (EC) patients. A total of 83 EC patients from two centers, including 46 with malignant and 37 with benign conditions, were included, with data split into a training set (n=59) and a testing set (n=24). The regions of interest (ROIs) were manually segmented from pre-surgical CT scans, and 1132 radiomic features were extracted from the pre-surgical CT scans using Pyradiomics. Six explainable machine learning (ML) modeling algorithms were implemented respectively, for determining the optimal radiomics pipeline. The diagnostic performance of the radiomic model was evaluated by using sensitivity, specificity, accuracy, precision, F1 score, AUROC, and AUPRC. To enhance clinical understanding and usability, we separately implemented SHAP analysis and feature mapping visualization, and evaluated the calibration curve and decision curve. By comparing six modeling strategies, the Random Forest model emerged as the optimal choice for diagnosing EC, with a training AUROC of 1.00 and a testing AUROC of 0.96. SHAP identified the most important radiomic features, revealing that all selected features were significantly associated with EC (P < 0.05). Radiomics feature maps also provide a feasible assessment tool for clinical applications. Decision Curve Analysis (DCA) indicated a higher net benefit for our model compared to the "All" and "None" strategies, suggesting its clinical utility in identifying high-risk cases and reducing unnecessary interventions. In conclusion, the CT radiomics-based explainable ML model achieved high diagnostic performance, which could be used as an intelligent auxiliary tool for the diagnosis of endometrial cancer.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCode: Updating Code API Knowledge with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.20495</link>
<guid>https://arxiv.org/abs/2506.20495</guid>
<content:encoded><![CDATA[

arXiv:2506.20495v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs</title>
<link>https://arxiv.org/abs/2506.22146</link>
<guid>https://arxiv.org/abs/2506.22146</guid>
<content:encoded><![CDATA[

arXiv:2506.22146v4 Announce Type: replace-cross 
Abstract: Despite progress in Large Vision-Language Models (LVLMs), their capacity for visual reasoning is often limited by the binding problem: the failure to reliably associate perceptual features with their correct visual referents. This limitation underlies persistent errors in tasks such as counting, visual search, scene description, and spatial relationship understanding. A key factor is that current LVLMs process visual features largely in parallel, lacking mechanisms for spatially grounded, serial attention. This paper introduces Visual Input Structure for Enhanced Reasoning (VISER), a simple, effective method that augments visual inputs with low-level spatial structures and pairs them with a textual prompt that encourages sequential, spatially-aware parsing. We empirically demonstrate substantial performance improvements across core visual reasoning tasks, using only a single-query inference. Specifically, VISER improves GPT-4o performance on visual search, counting, and spatial relationship tasks by 25.0%, 26.8%, and 9.5%, respectively, and reduces edit distance error in scene description by 0.32 on 2D datasets. Furthermore, we find that the visual modification is essential for these gains; purely textual strategies, including Chain-of-Thought prompting, are insufficient and can even degrade performance. VISER underscores the importance of visual input design over purely linguistically based reasoning strategies and suggests that visual structuring is a powerful and general approach for enhancing compositional and spatial reasoning in LVLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing forced responses and causality in data-driven climate emulators: conceptual limitations and the role of reduced-order models</title>
<link>https://arxiv.org/abs/2506.22552</link>
<guid>https://arxiv.org/abs/2506.22552</guid>
<content:encoded><![CDATA[

arXiv:2506.22552v4 Announce Type: replace-cross 
Abstract: A central challenge in climate science and applied mathematics is developing data-driven models of multiscale systems that capture both stationary statistics and responses to external perturbations. Current neural climate emulators aim to resolve the atmosphere-ocean system in all its complexity but often fail to reproduce forced responses, limiting their use in causal studies such as Green's function experiments. To investigate the origin of these limitations, we first focus on a simplified dynamical system that retains key features of climate variability. We interpret the results through linear response theory, providing a rigorous framework to evaluate neural models beyond stationary statistics and probe causal mechanisms. We argue that the ability of multiscale systems' emulators to reproduce perturbed statistics depends critically on (i) identifying an appropriate coarse-grained representation and (ii) careful parameterizations of unresolved processes. For low-frequency climate dynamics, these insights highlight reduced-order models, tailored to specific processes and scales, as valuable alternatives to general-purpose emulators. We next consider a real-world application, developing a neural model to investigate the joint variability of the surface temperature field and radiative fluxes. The model infers a multiplicative noise process directly from data, largely reproduces the system's probability distribution, and enables causal studies through forced responses. We discuss its limitations and outline directions for future work. These results expose fundamental challenges in data-driven modeling of multiscale physical systems and underscore the value of coarse-grained, stochastic approaches, with response theory as a principled framework to guide model design.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-Fusion: Token-Level Differentially Private Inference for Large Language Models</title>
<link>https://arxiv.org/abs/2507.04531</link>
<guid>https://arxiv.org/abs/2507.04531</guid>
<content:encoded><![CDATA[

arXiv:2507.04531v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) do not preserve privacy at inference-time. The LLM's outputs can inadvertently reveal information about the model's context, which presents a privacy challenge when the LLM is augmented via tools or databases containing sensitive information. Existing privacy-preserving methods at inference-time have significant limitations since they (i) lack provable guarantees or (ii) have a poor utility/privacy trade-off. We propose DP-Fusion, a Differentially Private Inference (DPI) mechanism for LLMs that provably bounds the influence a set of tokens in the context can have on the LLM's output. DP-Fusion works as follows: (1) label a subset of sensitive tokens, (2) infer the LLM without any sensitive tokens to obtain a baseline, (3) infer the LLM with the sensitive tokens, and (4) blend distributions so that the final output remains within a bounded distance of the baseline distribution. While this per-token influence bound also mitigates jailbreak-style prompt injection, we focus on \emph{document privatization}, where the goal is to paraphrase a document containing sensitive tokens, e.g., personally identifiable information, so that no attacker can reliably infer them from the paraphrased document while preserving high text quality. The privacy/utility trade-off is controlled by $\epsilon$, where $\epsilon=0$ hides sensitive tokens entirely, while higher values trade off privacy for improved text quality. We show that our method creates token-level provably privatized documents with substantially improved theoretical and empirical privacy, achieving $6\times$ lower perplexity than related DPI methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework</title>
<link>https://arxiv.org/abs/2507.13659</link>
<guid>https://arxiv.org/abs/2507.13659</guid>
<content:encoded><![CDATA[

arXiv:2507.13659v2 Announce Type: replace-cross 
Abstract: Recent researchers have proposed using event cameras for person re-identification (ReID) due to their promising performance and better balance in terms of privacy protection, event camera-based person ReID has attracted significant attention. Currently, mainstream event-based person ReID algorithms primarily focus on fusing visible light and event stream, as well as preserving privacy. Although significant progress has been made, these methods are typically trained and evaluated on small-scale or simulated event camera datasets, making it difficult to assess their real identification performance and generalization ability. To address the issue of data scarcity, this paper introduces a large-scale RGB-event based person ReID dataset, called EvReID. The dataset contains 118,988 image pairs and covers 1200 pedestrian identities, with data collected across multiple seasons, scenes, and lighting conditions. We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid foundation for future research in terms of both data and benchmarking. Based on our newly constructed dataset, this paper further proposes a pedestrian attribute-guided contrastive learning framework to enhance feature learning for person re-identification, termed TriPro-ReID. This framework not only effectively explores the visual features from both RGB frames and event streams, but also fully utilizes pedestrian attributes as mid-level semantic features. Extensive experiments on the EvReID dataset and MARS datasets fully validated the effectiveness of our proposed RGB-Event person ReID framework. The benchmark dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning</title>
<link>https://arxiv.org/abs/2508.00024</link>
<guid>https://arxiv.org/abs/2508.00024</guid>
<content:encoded><![CDATA[

arXiv:2508.00024v2 Announce Type: replace-cross 
Abstract: Quantum Support Vector Machines face scalability challenges due to high-dimensional quantum states and hardware limitations. We propose an embedding-aware quantum-classical pipeline combining class-balanced k-means distillation with pretrained Vision Transformer embeddings. Our key finding: ViT embeddings uniquely enable quantum advantage, achieving up to 8.02% accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST, while CNN features show performance degradation. Using 16-qubit tensor network simulation via cuTensorNet, we provide the first systematic evidence that quantum kernel advantage depends critically on embedding choice, revealing fundamental synergy between transformer attention and quantum feature spaces. This provides a practical pathway for scalable quantum machine learning that leverages modern neural architectures.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications</title>
<link>https://arxiv.org/abs/2508.01710</link>
<guid>https://arxiv.org/abs/2508.01710</guid>
<content:encoded><![CDATA[

arXiv:2508.01710v4 Announce Type: replace-cross 
Abstract: The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets. We present CultureGuard, a novel solution for curating culturally aligned, high-quality safety datasets across multiple languages. Our approach introduces a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering. This pipeline enables the conversion and expansion of the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese. The resulting dataset, Nemotron-Safety-Guard-Dataset-v3, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-8B-v3 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. Furthermore, we show our moderately multilingual fine-tuning enables robust cross-lingual transfer and strong zero-shot generalization to unseen languages. We also benchmark the latest open LLMs on multilingual safety and observe that these LLMs are more prone to give unsafe responses when prompted in non-English languages. This work advances multilingual LLM safety by enabling the development of culturally aware safety guard models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Design of Expressive and Trainable Pulse-based Quantum Machine Learning Models</title>
<link>https://arxiv.org/abs/2508.05559</link>
<guid>https://arxiv.org/abs/2508.05559</guid>
<content:encoded><![CDATA[

arXiv:2508.05559v2 Announce Type: replace-cross 
Abstract: Pulse-based Quantum Machine Learning (QML) has emerged as a novel paradigm in quantum artificial intelligence due to its exceptional hardware efficiency. For practical applications, pulse-based models must be both expressive and trainable. Previous studies suggest that pulse-based models under dynamic symmetry can be effectively trained, thanks to a favorable loss landscape that avoids barren plateaus. However, the resulting uncontrollability may compromise expressivity when the model is inadequately designed. This paper investigates the requirements for pulse-based QML models to be expressive while preserving trainability. We establish a necessary condition pertaining to the system's initial state, the measurement observable, and the underlying dynamical symmetry Lie algebra, supported by numerical simulations. Our findings provide a framework for designing practical pulse-based QML models that balance expressivity and trainability.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Does a Deep Neural Network Look at Lexical Stress?</title>
<link>https://arxiv.org/abs/2508.07229</link>
<guid>https://arxiv.org/abs/2508.07229</guid>
<content:encoded><![CDATA[

arXiv:2508.07229v2 Announce Type: replace-cross 
Abstract: Despite their success in speech processing, neural networks often operate as black boxes, prompting the question: what informs their decisions, and how can we interpret them? This work examines this issue in the context of lexical stress. A dataset of English disyllabic words was automatically constructed from read and spontaneous speech. Several Convolutional Neural Network (CNN) architectures were trained to predict stress position from a spectrographic representation of disyllabic words lacking minimal stress pairs (e.g., initial stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out test data. Layerwise Relevance Propagation (LRP), a technique for CNN interpretability analysis, revealed that predictions for held-out minimal pairs (PROtest vs. proTEST ) were most strongly influenced by information in stressed versus unstressed syllables, particularly the spectral properties of stressed vowels. However, the classifiers also attended to information throughout the word. A feature-specific relevance analysis is proposed, and its results suggest that our best-performing classifier is strongly influenced by the stressed vowel's first and second formants, with some evidence that its pitch and third formant also contribute. These results reveal deep learning's ability to acquire distributed cues to stress from naturally occurring data, extending traditional phonetic work based around highly controlled stimuli.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning</title>
<link>https://arxiv.org/abs/2508.10419</link>
<guid>https://arxiv.org/abs/2508.10419</guid>
<content:encoded><![CDATA[

arXiv:2508.10419v2 Announce Type: replace-cross 
Abstract: Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and its high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods could fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition on reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global context comprehension, offering a principled, cognitively motivated paradigm towards retrieval-based stateful reasoning. Our framework is made publicly available at https://github.com/EternityJune25/ComoRAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Swarm: A Methodological Framework for Interpreting Swarm Intelligence</title>
<link>https://arxiv.org/abs/2509.06272</link>
<guid>https://arxiv.org/abs/2509.06272</guid>
<content:encoded><![CDATA[

arXiv:2509.06272v3 Announce Type: replace-cross 
Abstract: Swarm based optimization algorithms have demonstrated remarkable success in solving complex optimization problems. However, their widespread adoption remains sceptical due to limited transparency in how different algorithmic components influence the overall performance of the algorithm. This work presents a multi-faceted interpretability related investigations of one of the popular swarm algorithms, Particle Swarm Optimization. Through this work, we provide a framework that makes the role of different topologies and parameters in PSO interpretable and explainable using novel machine learning approach. We first developed a comprehensive landscape characterization framework using Exploratory Landscape Analysis to quantify problem difficulty and identify critical features in the problem that affects the optimization performance of PSO. Secondly, we rigorously compare three topologies - Ring, Star, and Von Neumann analyzing their distinct impacts on exploration-exploitation balance, convergence behavior, and solution quality and eventually develop an explainable benchmarking framework for PSO. The work successfully decodes how swarm topologies affect information flow, diversity, and convergence. Through systematic experimentation across 24 benchmark functions in multiple dimensions, we establish practical guidelines for topology selection and parameter configuration. These findings uncover the black-box nature of PSO, providing more transparency and interpretability to swarm intelligence systems. The source code is available at \textcolor{blue}{https://github.com/GitNitin02/ioh_pso}.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2509.13624</link>
<guid>https://arxiv.org/abs/2509.13624</guid>
<content:encoded><![CDATA[

arXiv:2509.13624v2 Announce Type: replace-cross 
Abstract: Large language models are increasingly deployed across diverse applications. This often includes tasks LLMs have not encountered during training. This implies that enumerating and obtaining the high-quality training data for all tasks is infeasible. Thus, we often need to rely on transfer learning using datasets with different characteristics, and anticipate out-of-distribution requests. Motivated by this practical need, we propose an analysis framework, building a transfer learning matrix and dimensionality reduction, to dissect these cross-task interactions. We train and analyze 10 models to identify latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic) and discover the side effects of the transfer learning. Our findings reveal that performance improvements often defy explanations based on surface-level dataset similarity or source data quality. Instead, hidden statistical factors of the source dataset, such as class distribution and generation length proclivities, alongside specific linguistic features, are actually more influential. This work offers insights into the complex dynamics of transfer learning, paving the way for more predictable and effective LLM adaptation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Scenario Highway Lane-Change Intention Prediction: A Physics-Informed AI Framework for Three-Class Classification</title>
<link>https://arxiv.org/abs/2509.17354</link>
<guid>https://arxiv.org/abs/2509.17354</guid>
<content:encoded><![CDATA[

arXiv:2509.17354v3 Announce Type: replace-cross 
Abstract: Lane-change maneuvers are a leading cause of highway accidents, underscoring the need for accurate intention prediction to improve the safety and decision-making of autonomous driving systems. While prior studies using machine learning and deep learning methods (e.g., SVM, CNN, LSTM, Transformers) have shown promise, most approaches remain limited by binary classification, lack of scenario diversity, and degraded performance under longer prediction horizons. In this study, we propose a physics-informed AI framework that explicitly integrates vehicle kinematics, interaction feasibility, and traffic-safety metrics (e.g., distance headway, time headway, time-to-collision, closing gap time) into the learning process. lane-change prediction is formulated as a three-class problem that distinguishes left change, right change, and no change, and is evaluated across both straight highway segments (highD) and complex ramp scenarios (exiD). By integrating vehicle kinematics with interaction features, our machine learning models, particularly LightGBM, achieve state-of-the-art accuracy and strong generalization. Results show up to 99.8% accuracy and 93.6% macro F1 on highD, and 96.1% accuracy and 88.7% macro F1 on exiD at a 1-second horizon, outperforming a two-layer stacked LSTM baseline. These findings demonstrate the practical advantages of a physics-informed and feature-rich machine learning framework for real-time lane-change intention prediction in autonomous driving systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services</title>
<link>https://arxiv.org/abs/2509.18101</link>
<guid>https://arxiv.org/abs/2509.18101</guid>
<content:encoded><![CDATA[

arXiv:2509.18101v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are becoming increasingly widespread. Organizations that want to use AI for productivity now face an important decision. They can subscribe to commercial LLM services or deploy models on their own infrastructure. Cloud services from providers such as OpenAI, Anthropic, and Google are attractive because they provide easy access to state-of-the-art models and are easy to scale. However, concerns about data privacy, the difficulty of switching service providers, and long-term operating costs have driven interest in local deployment of open-source models. This paper presents a cost-benefit analysis framework to help organizations determine when on-premise LLM deployment becomes economically viable compared to commercial subscription services. We consider the hardware requirements, operational expenses, and performance benchmarks of the latest open-source models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost of deploying these models locally with the major cloud providers subscription fee. Our findings provide an estimated breakeven point based on usage levels and performance needs. These results give organizations a practical framework for planning their LLM strategies.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Web API Integration Code Generation</title>
<link>https://arxiv.org/abs/2509.20172</link>
<guid>https://arxiv.org/abs/2509.20172</guid>
<content:encoded><![CDATA[

arXiv:2509.20172v4 Announce Type: replace-cross 
Abstract: API integration is a cornerstone of our digital infrastructure, enabling software systems to connect and interact. However, as shown by many studies, writing or generating correct code to invoke APIs, particularly web APIs, is challenging. Although large language models (LLMs) have become popular in software development, their effectiveness in automating the generation of web API integration code remains unexplored. In order to address this, we present WAPIIBench, a dataset and evaluation pipeline designed to assess the ability of LLMs to generate web API invocation code. Our experiments with several open-source LLMs reveal that generating API invocations poses a significant challenge, resulting in hallucinated endpoints, incorrect argument usage, and other errors. None of the evaluated open-source models was able to solve more than 40% of the tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Early Universe with Deep Learning</title>
<link>https://arxiv.org/abs/2509.22018</link>
<guid>https://arxiv.org/abs/2509.22018</guid>
<content:encoded><![CDATA[

arXiv:2509.22018v3 Announce Type: replace-cross 
Abstract: Hydrogen is the most abundant element in our Universe. The first generation of stars and galaxies produced photons that ionized hydrogen gas, driving a cosmological event known as the Epoch of Reionization (EoR). The upcoming Square Kilometre Array Observatory (SKAO) will map the distribution of neutral hydrogen during this era, aiding in the study of the properties of these first-generation objects. Extracting astrophysical information will be challenging, as SKAO will produce a tremendous amount of data where the hydrogen signal will be contaminated with undesired foreground contamination and instrumental systematics. To address this, we develop the latest deep learning techniques to extract information from the 2D power spectra of the hydrogen signal expected from SKAO. We apply a series of neural network models to these measurements and quantify their ability to predict the history of cosmic hydrogen reionization, which is connected to the increasing number and efficiency of early photon sources. We show that the study of the early Universe benefits from modern deep learning technology. In particular, we demonstrate that dedicated machine learning algorithms can achieve more than a $0.95$ $R^2$ score on average in recovering the reionization history. This enables accurate and precise cosmological and astrophysical inference of structure formation in the early Universe.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference</title>
<link>https://arxiv.org/abs/2509.24257</link>
<guid>https://arxiv.org/abs/2509.24257</guid>
<content:encoded><![CDATA[

arXiv:2509.24257v3 Announce Type: replace-cross 
Abstract: Decentralized inference provides a scalable and resilient paradigm for serving large language models (LLMs), enabling distributed resource utilization and reducing reliance on centralized providers. However, in a permissionless environment without trusted nodes, ensuring the correctness of model outputs remains a core challenge. We introduce VeriLLM, a publicly verifiable protocol for decentralized LLM inference that achieves security under a one-honest-verifier assumption while maintaining practical efficiency. VeriLLM combines lightweight empirical rerunning with cryptographic commitments, allowing verifiers to validate results at approximately 1% of the underlying inference cost. To prevent verification bottlenecks, we design an isomorphic inference-verification architecture that multiplexes both inference and verification roles across the same GPU workers. This design (i) improves GPU utilization and overall throughput, (ii) enlarges the effective validator set, enhancing robustness and liveness, and (iii) enforces task indistinguishability to prevent node-specific optimizations or selective behavior. Through theoretical analysis and system-level evaluation, we show that VeriLLM achieves reliable public verifiability with minimal overhead, offering a practical foundation for trustworthy and scalable decentralized LLM inference.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference-Based Dynamic Ranking Structure Recognition</title>
<link>https://arxiv.org/abs/2509.24493</link>
<guid>https://arxiv.org/abs/2509.24493</guid>
<content:encoded><![CDATA[

arXiv:2509.24493v2 Announce Type: replace-cross 
Abstract: Preference-based data often appear complex and noisy but may conceal underlying homogeneous structures. This paper introduces a novel framework of ranking structure recognition for preference-based data. We first develop an approach to identify dynamic ranking groups by incorporating temporal penalties into a spectral estimation for the celebrated Bradley-Terry model. To detect structural changes, we introduce an innovative objective function and present a practicable algorithm based on dynamic programming. Theoretically, we establish the consistency of ranking group recognition by exploiting properties of a random `design matrix' induced by a reversible Markov chain. We also tailor a group inverse technique to quantify the uncertainty in item ability estimates. Additionally, we prove the consistency of structure change recognition, ensuring the robustness of the proposed framework. Experiments on both synthetic and real-world datasets demonstrate the practical utility and interpretability of our approach.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The causal structure of galactic astrophysics</title>
<link>https://arxiv.org/abs/2510.01112</link>
<guid>https://arxiv.org/abs/2510.01112</guid>
<content:encoded><![CDATA[

arXiv:2510.01112v2 Announce Type: replace-cross 
Abstract: Data-driven astrophysics currently relies on the detection and characterisation of correlations between objects' properties, which are then used to test physical theories that make predictions for them. This process fails to utilise information in the data that forms a crucial part of the theories' predictions, namely which variables are directly correlated (as opposed to accidentally correlated through others), the directions of these determinations, and the presence or absence of confounders that correlate variables in the dataset but are themselves absent from it. We propose to recover this information through causal discovery, a well-developed methodology for inferring the causal structure of datasets that is however almost entirely unknown to astrophysics. We develop a causal discovery algorithm suitable for large astrophysical datasets and illustrate it on $\sim$5$\times10^5$ low-redshift galaxies from the Nasa Sloan Atlas, demonstrating its ability to distinguish physical mechanisms that are degenerate on the basis of correlations alone.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCE: Towards a General Framework for Handling Missing Modalities under Imbalanced Missing Rates</title>
<link>https://arxiv.org/abs/2510.10534</link>
<guid>https://arxiv.org/abs/2510.10534</guid>
<content:encoded><![CDATA[

arXiv:2510.10534v2 Announce Type: replace-cross 
Abstract: Multi-modal learning has made significant advances across diverse pattern recognition applications. However, handling missing modalities, especially under imbalanced missing rates, remains a major challenge. This imbalance triggers a vicious cycle: modalities with higher missing rates receive fewer updates, leading to inconsistent learning progress and representational degradation that further diminishes their contribution. Existing methods typically focus on global dataset-level balancing, often overlooking critical sample-level variations in modality utility and the underlying issue of degraded feature quality. We propose Modality Capability Enhancement (MCE) to tackle these limitations. MCE includes two synergistic components: i) Learning Capability Enhancement (LCE), which introduces multi-level factors to dynamically balance modality-specific learning progress, and ii) Representation Capability Enhancement (RCE), which improves feature semantics and robustness through subset prediction and cross-modal completion tasks. Comprehensive evaluations on four multi-modal benchmarks show that MCE consistently outperforms state-of-the-art methods under various missing configurations. The final published version is now available at https://doi.org/10.1016/j.patcog.2025.112591. Our code is available at https://github.com/byzhaoAI/MCE.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models</title>
<link>https://arxiv.org/abs/2510.13847</link>
<guid>https://arxiv.org/abs/2510.13847</guid>
<content:encoded><![CDATA[

arXiv:2510.13847v2 Announce Type: replace-cross 
Abstract: Speculative decoding has become a standard way to accelerate LLM inference: a small drafter proposes multiple tokens and a large target model verifies them once per speculation length. Recently, scaling of the LLM vocabulary has pushed the number of tokens to grow substantially. While verification over the full vocabulary leaves the target model largely unaffected, the O(|V|d) parameters in the drafter's output head become a latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g., FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed top frequent subset of the target model's vocabulary. Although this reduces draft-time compute, it is brittle, since: (i) frequency lists are corpus-dependent and require retuning to generalize, and (ii) static shortlists suppress rare or domain-specific tokens, lowering the expected number of tokens per verification step. We propose DynaSpec, a context-dependent dynamic shortlisting mechanism that is robust, speeds up drafting, and generalizes across diverse tasks. Concretely, we introduce lightweight, coarse-grained meta-classifiers that route contexts to a small number of token clusters; the union of the top-k selected clusters forms the drafter's shortlist, while verification retains the full vocabulary and exactness. The meta-classifier finishes its computation earlier than the drafter's hidden state generation by exploiting parallel execution of draft encoding and meta shortlisting on separate streams. Across standard speculative decoding benchmarks, DynaSpec delivers consistent improvements in mean accepted length, for Llama-3-8B, reaching upto 98.2% of full-vocabulary performance, while fixed-shortlist baselines attain only 84.4%. By leveraging context-dependent selection, DynaSpec achieves up to a 2.18 times increase in generated tokens compared to 1.91 times for fixed-vocabulary approaches.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization</title>
<link>https://arxiv.org/abs/2510.14150</link>
<guid>https://arxiv.org/abs/2510.14150</guid>
<content:encoded><![CDATA[

arXiv:2510.14150v2 Announce Type: replace-cross 
Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding agent that unites Large Language Models (LLMs) with genetic algorithms to solve complex computational problems. Our framework adapts powerful evolutionary concepts to the LLM domain, building upon recent methods for generalized scientific discovery. CodeEvolve employs an island-based genetic algorithm to maintain population diversity and increase throughput, introduces a novel inspiration-based crossover mechanism that leverages the LLMs context window to combine features from successful solutions, and implements meta-prompting strategies for dynamic exploration of the solution space. We conduct a rigorous evaluation of CodeEvolve on a subset of the mathematical benchmarks used to evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that our method surpasses AlphaEvolve's performance on several challenging problems. To foster collaboration and accelerate progress, we release our complete framework as an open-source repository.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Spectral Tokenization via Self-Supervised Panchromatic Representation Learning</title>
<link>https://arxiv.org/abs/2510.17959</link>
<guid>https://arxiv.org/abs/2510.17959</guid>
<content:encoded><![CDATA[

arXiv:2510.17959v2 Announce Type: replace-cross 
Abstract: Sequential scientific data span many resolutions and domains, and unifying them into a common representation is a key step toward developing foundation models for the sciences. Astronomical spectra exemplify this challenge: massive surveys have collected millions of spectra across a wide range of wavelengths and resolutions, yet analyses remain fragmented across spectral domains (e.g., optical vs. infrared) and object types (e.g., stars vs. galaxies), limiting the ability to pool information across datasets. We present a deep learning model that jointly learns from heterogeneous spectra in a self-supervised manner. Our universal spectral tokenizer processes spectra from a variety of object types and resolutions directly on their native wavelength grids, producing intrinsically aligned, homogeneous, and physically meaningful representations that can be efficiently adapted to achieve competitive performance across a range of downstream tasks. For the first time, we demonstrate that a single model can unify spectral data across resolutions and domains, suggesting that our model can serve as a powerful building block for foundation models in astronomy -- and potentially extend to other scientific domains with heterogeneous sequential data, such as climate and healthcare.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Encoding Matrices using Quantum Circuits</title>
<link>https://arxiv.org/abs/2510.20030</link>
<guid>https://arxiv.org/abs/2510.20030</guid>
<content:encoded><![CDATA[

arXiv:2510.20030v2 Announce Type: replace-cross 
Abstract: Over a decade ago, it was demonstrated that quantum computing has the potential to revolutionize numerical linear algebra by enabling algorithms with complexity superior to what is classically achievable, e.g., the seminal HHL algorithm for solving linear systems. Efficient execution of such algorithms critically depends on representing inputs (matrices and vectors) as quantum circuits that encode or implement these inputs. For that task, two common circuit representations emerged in the literature: block encodings and state preparation circuits. In this paper, we systematically study encodings matrices in the form of block encodings and state preparation circuits. We examine methods for constructing these representations from matrices given in classical form, as well as quantum two-way conversions between circuit representations. Two key results we establish (among others) are: (a) a general method for efficiently constructing a block encoding of an arbitrary matrix given in classical form (entries stored in classical random access memory); and (b) low-overhead, bidirectional conversion algorithms between block encodings and state preparation circuits, showing that these models are essentially equivalent. From a technical perspective, two central components of our constructions are: (i) a special constant-depth multiplexer that simultaneously multiplexes all higher-order Pauli matrices of a given size, and (ii) an algorithm for performing a quantum conversion between a matrix's expansion in the standard basis and its expansion in the basis of higher-order Pauli matrices.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secure Retrieval-Augmented Generation against Poisoning Attacks</title>
<link>https://arxiv.org/abs/2510.25025</link>
<guid>https://arxiv.org/abs/2510.25025</guid>
<content:encoded><![CDATA[

arXiv:2510.25025v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have transformed natural language processing (NLP), enabling applications from content generation to decision support. Retrieval-Augmented Generation (RAG) improves LLMs by incorporating external knowledge but also introduces security risks, particularly from data poisoning, where the attacker injects poisoned texts into the knowledge database to manipulate system outputs. While various defenses have been proposed, they often struggle against advanced attacks. To address this, we introduce RAGuard, a detection framework designed to identify poisoned texts. RAGuard first expands the retrieval scope to increase the proportion of clean texts, reducing the likelihood of retrieving poisoned content. It then applies chunk-wise perplexity filtering to detect abnormal variations and text similarity filtering to flag highly similar texts. This non-parametric approach enhances RAG security, and experiments on large-scale datasets demonstrate its effectiveness in detecting and mitigating poisoning attacks, including strong adaptive attacks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Inversion Attacks Meet Cryptographic Fuzzy Extractors</title>
<link>https://arxiv.org/abs/2510.25687</link>
<guid>https://arxiv.org/abs/2510.25687</guid>
<content:encoded><![CDATA[

arXiv:2510.25687v2 Announce Type: replace-cross 
Abstract: Model inversion attacks pose an open challenge to privacy-sensitive applications that use machine learning (ML) models. For example, face authentication systems use modern ML models to compute embedding vectors from face images of the enrolled users and store them. If leaked, inversion attacks can accurately reconstruct user faces from the leaked vectors. There is no systematic characterization of properties needed in an ideal defense against model inversion, even for the canonical example application of a face authentication system susceptible to data breaches, despite a decade of best-effort solutions.
  In this paper, we formalize the desired properties of a provably strong defense against model inversion and connect it, for the first time, to the cryptographic concept of fuzzy extractors. We further show that existing fuzzy extractors are insecure for use in ML-based face authentication. We do so through a new model inversion attack called PIPE, which achieves a success rate of over 89% in most cases against prior schemes. We then propose L2FE-Hash, the first candidate fuzzy extractor which supports standard Euclidean distance comparators as needed in many ML-based applications, including face authentication. We formally characterize its computational security guarantees, even in the extreme threat model of full breach of stored secrets, and empirically show its usable accuracy in face authentication for practical face distributions. It offers attack-agnostic security without requiring any re-training of the ML model it protects. Empirically, it nullifies both prior state-of-the-art inversion attacks as well as our new PIPE attack.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs</title>
<link>https://arxiv.org/abs/2510.26512</link>
<guid>https://arxiv.org/abs/2510.26512</guid>
<content:encoded><![CDATA[

arXiv:2510.26512v2 Announce Type: replace-cross 
Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer critical insights but are often unstructured, lexically dense, and filled with ambiguous or shifting references, which pose significant challenges for automated knowledge graph (KG) construction. While recent LLM-based approaches improve over static templates, they still generate noisy, fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution. The recently proposed CORE-KG framework addresses these limitations by integrating a type-aware coreference module and domain-guided structured prompts, significantly reducing node duplication and legal noise. In this work, we present a systematic ablation study of CORE-KG to quantify the individual contributions of its two key components. Our results show that removing coreference resolution results in a 28.25% increase in node duplication and a 4.32% increase in noisy nodes, while removing structured prompts leads to a 4.29% increase in node duplication and a 73.33% increase in noisy nodes. These findings offer empirical insights for designing robust LLM-based pipelines for extracting structured representations from complex legal texts.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Mixture Models and Surrogate Models for Precision Additive Manufacturing</title>
<link>https://arxiv.org/abs/2510.26586</link>
<guid>https://arxiv.org/abs/2510.26586</guid>
<content:encoded><![CDATA[

arXiv:2510.26586v2 Announce Type: replace-cross 
Abstract: In this study, we leverage a mixture model learning approach to identify defects in laser-based Additive Manufacturing (AM) processes. By incorporating physics based principles, we also ensure that the model is sensitive to meaningful physical parameter variations. The empirical evaluation was conducted by analyzing real-world data from two AM processes: Directed Energy Deposition and Laser Powder Bed Fusion. In addition, we also studied the performance of the developed framework over public datasets with different alloy type and experimental parameter information. The results show the potential of physics-guided mixture models to examine the underlying physical behavior of an AM system.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimation of aboveground biomass in a tropical dry forest: An intercomparison of airborne, unmanned, and space laser scanning</title>
<link>https://arxiv.org/abs/2510.27408</link>
<guid>https://arxiv.org/abs/2510.27408</guid>
<content:encoded><![CDATA[

arXiv:2510.27408v4 Announce Type: replace-cross 
Abstract: According to the Paris Climate Change Agreement, all nations are required to submit reports on their greenhouse gas emissions and absorption every two years by 2024. Consequently, forests play a crucial role in reducing carbon emissions, which is essential for meeting these obligations. Recognizing the significance of forest conservation in the global battle against climate change, Article 5 of the Paris Agreement emphasizes the need for high-quality forest data. This study focuses on enhancing methods for mapping aboveground biomass in tropical dry forests. Tropical dry forests are considered one of the least understood tropical forest environments; therefore, there is a need for accurate approaches to estimate carbon pools. We employ a comparative analysis of AGB estimates, utilizing different discrete and full-waveform laser scanning datasets in conjunction with Ordinary Least Squares and Bayesian approaches SVM. Airborne Laser Scanning, Unmanned Laser Scanning, and Space Laser Scanning were used as independent variables for extracting forest metrics. Variable selection, SVM regression tuning, and cross-validation via a machine-learning approach were applied to account for overfitting and underfitting. The results indicate that six key variables primarily related to tree height: Elev\.minimum, Elev\.L3, lev\.MAD\.mode, Elev\.mode, Elev\.MAD\.median, and Elev\.skewness, are important for AGB estimation using ALSD and ULSD, while Leaf Area Index, canopy coverage and height, terrain elevation, and full-waveform signal energy emerged as the most vital variables. AGB values estimated from ten permanent tropical dry forest plots in Costa Rica Guanacaste province ranged from 26.02 Mg/ha to 175.43 Mg/ha. The SVM regressions demonstrated a 17.89 error across all laser scanning systems, with SLSF W exhibiting the lowest error 17.07 in estimating total biomass per plot.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disciplined Biconvex Programming</title>
<link>https://arxiv.org/abs/2511.01813</link>
<guid>https://arxiv.org/abs/2511.01813</guid>
<content:encoded><![CDATA[

arXiv:2511.01813v2 Announce Type: replace-cross 
Abstract: We introduce disciplined biconvex programming (DBCP), a modeling framework for specifying and solving biconvex optimization problems. Biconvex optimization problems arise in various applications, including machine learning, signal processing, computational science, and control. Solving a biconvex optimization problem in practice usually resolves to heuristic methods based on alternate convex search (ACS), which iteratively optimizes over one block of variables while keeping the other fixed, so that the resulting subproblems are convex and can be efficiently solved. However, designing and implementing an ACS solver for a specific biconvex optimization problem usually requires significant effort from the user, which can be tedious and error-prone. DBCP extends the principles of disciplined convex programming to biconvex problems, allowing users to specify biconvex optimization problems in a natural way based on a small number of syntax rules. The resulting problem can then be automatically split and transformed into convex subproblems, for which a customized ACS solver is then generated and applied. DBCP allows users to quickly experiment with different biconvex problem formulations, without expertise in convex optimization. We implement DBCP into the open source Python package dbcp, as an extension to the famous domain specific language CVXPY for convex optimization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity</title>
<link>https://arxiv.org/abs/2511.04686</link>
<guid>https://arxiv.org/abs/2511.04686</guid>
<content:encoded><![CDATA[
<div> cache management, language models, key-value cache, positional encodings, eviction strategies
<br />
Summary: 
This paper explores the impact of key-value (KV) cache management on the efficiency of large language models (LLMs), particularly in stateful multi-turn scenarios. The study shows that as the KV cache grows close to the model's context window limit, LLM generation quality deteriorates. Eviction strategies, even those with high retention rates, can worsen performance if they disrupt positional coherence. Preserving contiguous context blocks through simple strategies is found to yield more coherent outputs. The study emphasizes the importance of eviction techniques that respect architectural limits, preserve positional structure, and consider "cache health" holistically beyond size. It highlights the need to maintain consistency in positional signals like RoPE to achieve optimal model performance. Overall, the paper underscores the significance of managing the KV cache effectively in enhancing the performance of LLMs in various scenarios. 
<br /><br /> <div>
arXiv:2511.04686v1 Announce Type: new 
Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in large language models (LLMs), yet its unbounded growth in stateful multi-turn scenarios presents major challenges. This paper examines the interplay between KV cache management strategies, the architectural context limits of models like meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of positional encodings. Through empirical analysis using a stateful benchmarking framework, we show that LLM generation quality degrades sharply when the accumulated KV cache approaches or exceeds the model's trained context window (e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via AttentionTop), can worsen performance if they disrupt positional coherence. Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a cache by removing non-contiguous tokens can scramble these signals and lead to degenerative outputs. We further show that simple strategies preserving contiguous context blocks (e.g., keeping an initial "gist") can yield more coherent generations than complex or positionally disruptive ones. We advocate for eviction techniques that respect architectural limits, preserve positional structure, and view "cache health" holistically beyond mere size.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification</title>
<link>https://arxiv.org/abs/2511.04718</link>
<guid>https://arxiv.org/abs/2511.04718</guid>
<content:encoded><![CDATA[
<div> Keywords: resting-state fMRI, brain disorders, frequency bands, functional connectivity networks, diagnostic prediction

Summary: 
Resting-state functional MRI (fMRI) is crucial for classifying brain disorders and constructing functional connectivity networks. Current models overlook the multi-frequency nature of neuronal oscillations, leading to limitations in diagnostic sensitivities. A novel framework is proposed utilizing Adaptive Cascade Decomposition to learn task-relevant frequency sub-bands for each brain region and Frequency-Coupled Connectivity Learning to capture intra- and cross-band interactions in a unified functional network. This unified network informs a message-passing mechanism within the Unified-GCN, generating refined node representations for diagnostic prediction. Experimental results on ADNI and ABIDE datasets show superior performance over existing methods. The code is available on GitHub for further exploration. 

Summary: <div>
arXiv:2511.04718v1 Announce Type: new 
Abstract: Resting-state fMRI has become a valuable tool for classifying brain disorders and constructing brain functional connectivity networks
  by tracking BOLD signals across brain regions. However, existing mod els largely neglect the multi-frequency nature of neuronal oscillations,
  treating BOLD signals as monolithic time series. This overlooks the cru cial fact that neurological disorders often manifest as disruptions within
  specific frequency bands, limiting diagnostic sensitivity and specificity.
  While some methods have attempted to incorporate frequency informa tion, they often rely on predefined frequency bands, which may not be
  optimal for capturing individual variability or disease-specific alterations.
  To address this, we propose a novel framework featuring Adaptive Cas cade Decomposition to learn task-relevant frequency sub-bands for each
  brain region and Frequency-Coupled Connectivity Learning to capture
  both intra- and nuanced cross-band interactions in a unified functional
  network. This unified network informs a novel message-passing mecha nism within our Unified-GCN, generating refined node representations
  for diagnostic prediction. Experimental results on the ADNI and ABIDE
  datasets demonstrate superior performance over existing methods. The
  code is available at https://github.com/XXYY20221234/Ada-FCN.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AWEMixer: Adaptive Wavelet-Enhanced Mixer Network for Long-Term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.04722</link>
<guid>https://arxiv.org/abs/2511.04722</guid>
<content:encoded><![CDATA[
<div> Frequency Router, Fast Fourier Transform, Wavelet subband, Coherent Gated Fusion Block, Multi-scale temporal representation

Summary:<br /><br />Forecasting long-term time series in IoT environments is challenging due to non-stationary and multi-scale sensor signals. AWEMixer, an Adaptive Wavelet-Enhanced Mixer Network, addresses this issue with a Frequency Router that utilizes global periodicity patterns from FFT to adaptively weight wavelet subbands. The Coherent Gated Fusion Block integrates frequency features with multi-scale temporal representation, achieving accurate time-frequency localization and noise robustness. Validated on seven benchmarks, AWEMixer outperforms state-of-the-art models in long-sequence time series forecasting, surpassing transformer-based and MLP-based models consistently. The code is available on GitHub for further exploration. <div>
arXiv:2511.04722v1 Announce Type: new 
Abstract: Forecasting long-term time series in IoT environments remains a significant challenge due to the non-stationary and multi-scale characteristics of sensor signals. Furthermore, error accumulation causes a decrease in forecast quality when predicting further into the future. Traditional methods are restricted to operate in time-domain, while the global frequency information achieved by Fourier transform would be regarded as stationary signals leading to blur the temporal patterns of transient events. We propose AWEMixer, an Adaptive Wavelet-Enhanced Mixer Network including two innovative components: 1) a Frequency Router designs to utilize the global periodicity pattern achieved by Fast Fourier Transform to adaptively weight localized wavelet subband, and 2) a Coherent Gated Fusion Block to achieve selective integration of prominent frequency features with multi-scale temporal representation through cross-attention and gating mechanism, which realizes accurate time-frequency localization while remaining robust to noise. Seven public benchmarks validate that our model is more effective than recent state-of-the-art models. Specifically, our model consistently achieves performance improvement compared with transformer-based and MLP-based state-of-the-art models in long-sequence time series forecasting. Code is available at https://github.com/hit636/AWEMixer
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction</title>
<link>https://arxiv.org/abs/2511.04723</link>
<guid>https://arxiv.org/abs/2511.04723</guid>
<content:encoded><![CDATA[
<div> TCNs, TFT, Bi-LSTM, RUL prediction, Time-series transformers
<br />
Summary:
This study introduces a novel framework for Health prediction in industrial systems, focusing on Remaining Useful Life (RUL) prediction. The proposed model integrates Temporal Convolutional Networks (TCNs) for localized temporal feature extraction with a modified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder. The framework effectively captures fine-grained temporal dependencies and prioritizes critical features for robust prognostics. By incorporating a multi-time-window methodology, the model improves adaptability across diverse operating conditions. Evaluation on benchmark datasets shows a significant reduction in average RMSE, indicating improved predictive accuracy compared to existing methods. The proposed framework bridges short- and long-term dependencies, emphasizes salient temporal patterns, and advances the effectiveness of industrial prognostic systems. It highlights the potential of advanced time-series transformers for RUL prediction. 
<br /><br /> <div>
arXiv:2511.04723v1 Announce Type: new 
Abstract: Health prediction is crucial for ensuring reliability, minimizing downtime, and optimizing maintenance in industrial systems. Remaining Useful Life (RUL) prediction is a key component of this process; however, many existing models struggle to capture fine-grained temporal dependencies while dynamically prioritizing critical features across time for robust prognostics. To address these challenges, we propose a novel framework that integrates Temporal Convolutional Networks (TCNs) for localized temporal feature extraction with a modified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder. This architecture effectively bridges short- and long-term dependencies while emphasizing salient temporal patterns. Furthermore, the incorporation of a multi-time-window methodology improves adaptability across diverse operating conditions. Extensive evaluations on benchmark datasets demonstrate that the proposed model reduces the average RMSE by up to 5.5%, underscoring its improved predictive accuracy compared to state-of-the-art methods. By closing critical gaps in current approaches, this framework advances the effectiveness of industrial prognostic systems and highlights the potential of advanced time-series transformers for RUL prediction.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regularized GLISp for sensor-guided human-in-the-loop optimization</title>
<link>https://arxiv.org/abs/2511.04751</link>
<guid>https://arxiv.org/abs/2511.04751</guid>
<content:encoded><![CDATA[
<div> Preference-based optimization, pairwise comparisons, sensor-guided, regularized extension, GLISp <br />
Summary: 
This article introduces a sensor-guided regularized extension of Global optimization based on active preference learning with radial basis functions (GLISp) to incorporate measurable descriptors into the preference-learning process. By integrating sensor measurements through a physics-informed hypothesis function and a least-squares regularization term, this approach adds grey-box structure to the preference-based search, combining subjective feedback with quantitative sensor information. Numerical evaluations on analytical benchmarks and a human-in-the-loop vehicle suspension tuning task demonstrate faster convergence and superior final solutions compared to baseline GLISp. This method enhances the efficiency and effectiveness of human-in-the-loop calibration by leveraging both subjective preferences and objective sensor data. <br /><br />Summary: <div>
arXiv:2511.04751v1 Announce Type: new 
Abstract: Human-in-the-loop calibration is often addressed via preference-based optimization, where algorithms learn from pairwise comparisons rather than explicit cost evaluations. While effective, methods such as Preferential Bayesian Optimization or Global optimization based on active preference learning with radial basis functions (GLISp) treat the system as a black box and ignore informative sensor measurements. In this work, we introduce a sensor-guided regularized extension of GLISp that integrates measurable descriptors into the preference-learning loop through a physics-informed hypothesis function and a least-squares regularization term. This injects grey-box structure, combining subjective feedback with quantitative sensor information while preserving the flexibility of preference-based search. Numerical evaluations on an analytical benchmark and on a human-in-the-loop vehicle suspension tuning task show faster convergence and superior final solutions compared to baseline GLISp.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Data Falls Short: Grokking Below the Critical Threshold</title>
<link>https://arxiv.org/abs/2511.04760</link>
<guid>https://arxiv.org/abs/2511.04760</guid>
<content:encoded><![CDATA[
<div> grokking, delayed generalization, knowledge distillation, data-scarce regimes, distribution shift

Summary:
- The study explores grokking, a phenomenon where models exhibit delayed generalization after overfitting, particularly in data-scarce scenarios.
- Knowledge Distillation (KD) from a grokked model on one distribution can induce and accelerate grokking on another distribution, even with limited data.
- Training on a joint distribution can fail when data is insufficient, but distillation from grokked models on individual distributions enables generalization.
- Continual pretraining with KD allows for accelerated generalization and mitigates catastrophic forgetting in transitioning between distributions.
- The results emphasize the importance of KD in facilitating generalization in low-data and evolving distribution environments. 

<br /><br />Summary: <div>
arXiv:2511.04760v1 Announce Type: new 
Abstract: In this paper, we investigate the phenomenon of grokking, where models exhibit delayed generalization following overfitting on training data. We focus on data-scarce regimes where the number of training samples falls below the critical threshold, making grokking unobservable, and on practical scenarios involving distribution shift. We first show that Knowledge Distillation (KD) from a model that has already grokked on a distribution (p1) can induce and accelerate grokking on a different distribution (p2), even when the available data lies below the critical threshold. This highlights the value of KD for deployed models that must adapt to new distributions under limited data. We then study training on the joint distribution (p1, p2) and demonstrate that while standard supervised training fails when either distribution has insufficient data, distilling from models grokked on the individual distributions enables generalization. Finally, we examine a continual pretraining setup, where a grokked model transitions from p1 to p2, and find that KD both accelerates generalization and mitigates catastrophic forgetting, achieving strong performance even with only 10% of the data. Together, our results provide new insights into the mechanics of grokking under knowledge transfer and underscore the central role of KD in enabling generalization in low-data and evolving distribution settings.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow</title>
<link>https://arxiv.org/abs/2511.04768</link>
<guid>https://arxiv.org/abs/2511.04768</guid>
<content:encoded><![CDATA[
<div> PyTorch, sparse computation, specialized dataflow hardware, compiler, FuseFlow

Summary:<br />
- FuseFlow is a compiler that converts sparse machine learning models written in PyTorch to fused sparse dataflow graphs for reconfigurable dataflow architectures (RDAs).
- It supports general cross-expression fusion of sparse operations, optimizations like parallelization, dataflow ordering, and sparsity blocking, and targets a cycle-accurate dataflow simulator for microarchitectural analysis.
- Design-space exploration across real-world machine learning applications with sparsity using FuseFlow shows that full fusion may not always be optimal for sparse models as fusion granularity depends on the model.
- FuseFlow provides a heuristic to identify and prune suboptimal configurations, leading to performance improvements, including a ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse attention.<br /><br />Summary: <div>
arXiv:2511.04768v1 Announce Type: new 
Abstract: As deep learning models scale, sparse computation and specialized dataflow hardware have emerged as powerful solutions to address efficiency. We propose FuseFlow, a compiler that converts sparse machine learning models written in PyTorch to fused sparse dataflow graphs for reconfigurable dataflow architectures (RDAs). FuseFlow is the first compiler to support general cross-expression fusion of sparse operations. In addition to fusion across kernels (expressions), FuseFlow also supports optimizations like parallelization, dataflow ordering, and sparsity blocking. It targets a cycle-accurate dataflow simulator for microarchitectural analysis of fusion strategies. We use FuseFlow for design-space exploration across four real-world machine learning applications with sparsity, showing that full fusion (entire cross-expression fusion across all computation in an end-to-end model) is not always optimal for sparse models-fusion granularity depends on the model itself. FuseFlow also provides a heuristic to identify and prune suboptimal configurations. Using Fuseflow, we achieve performance improvements, including a ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse attention.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLOFetch: Compressed-Hierarchical Instruction Prefetching for Cloud Microservices</title>
<link>https://arxiv.org/abs/2511.04774</link>
<guid>https://arxiv.org/abs/2511.04774</guid>
<content:encoded><![CDATA[
<div> Keywords: networked services, instruction prefetching, microservice orchestration, tail latency, energy

Summary:
Large-scale networked services depend on complex software stacks and microservice orchestration, leading to increased instruction footprints and frontend stalls that impact tail latency and energy consumption. In this study, the authors propose a novel design for instruction prefetching in cloud workloads that aligns with SLO-driven and self-optimizing systems. By enhancing the Entangling Instruction Prefetcher (EIP) with a Compressed Entry and Hierarchical Metadata Storage scheme, they aim to capture multiple destinations efficiently and virtualize bulk metadata. Additionally, an Online ML Controller evaluates prefetch profitability using context features and adjusts thresholds using bandit algorithms. The proposed approach delivers EIP-like speedups while reducing on-chip state and improving efficiency for networked services in the machine learning era. <br /><br />Summary: Large-scale networked services with deep software stacks and microservice orchestration face challenges with instruction footprints and frontend stalls. The study introduces a design for instruction prefetching that optimizes performance and energy efficiency by leveraging spatial clustering, hierarchical metadata storage, and online machine learning-based control mechanisms. <div>
arXiv:2511.04774v1 Announce Type: new 
Abstract: Large-scale networked services rely on deep soft-ware stacks and microservice orchestration, which increase instruction footprints and create frontend stalls that inflate tail latency and energy. We revisit instruction prefetching for these cloud workloads and present a design that aligns with SLO driven and self optimizing systems. Building on the Entangling Instruction Prefetcher (EIP), we introduce a Compressed Entry that captures up to eight destinations around a base using 36 bits by exploiting spatial clustering, and a Hierarchical Metadata Storage scheme that keeps only L1 resident and frequently queried entries on chip while virtualizing bulk metadata into lower levels. We further add a lightweight Online ML Controller that scores prefetch profitability using context features and a bandit adjusted threshold. On data center applications, our approach preserves EIP like speedups with smaller on chip state and improves efficiency for networked services in the ML era.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting</title>
<link>https://arxiv.org/abs/2511.04789</link>
<guid>https://arxiv.org/abs/2511.04789</guid>
<content:encoded><![CDATA[
<div> Keywords: Parkinson's disease, brain-morphometry, longitudinal trajectories, neural ODE model, individualized forecasting

Summary: 
CNODE is a novel framework developed to forecast individualized Parkinson's disease progression by modeling brain changes as continuous temporal processes using a neural ODE model. By jointly learning patient-specific initial time and progress speed, CNODE aligns individual trajectories into a shared progression trajectory, capturing the heterogeneous nature of PD. The study validates CNODE using the PPMI dataset and demonstrates superior performance compared to existing methods in forecasting longitudinal PD progression. <div>
arXiv:2511.04789v1 Announce Type: new 
Abstract: Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry patterns. Modeling these longitudinal trajectories enables mechanistic insight, treatment development, and individualized 'digital-twin' forecasting. However, existing methods usually adopt recurrent neural networks and transformer architectures, which rely on discrete, regularly sampled data while struggling to handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts. Moreover, these methods have difficulty capturing individual heterogeneity including variations in disease onset, progression rate, and symptom severity, which is a hallmark of PD. To address these challenges, we propose CNODE (Conditional Neural ODE), a novel framework for continuous, individualized PD progression forecasting. The core of CNODE is to model morphological brain changes as continuous temporal processes using a neural ODE model. In addition, we jointly learn patient-specific initial time and progress speed to align individual trajectories into a shared progression trajectory. We validate CNODE on the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental results show that our method outperforms state-of-the-art baselines in forecasting longitudinal PD progression.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Structure and Representation Learning with Biomedical Applications</title>
<link>https://arxiv.org/abs/2511.04790</link>
<guid>https://arxiv.org/abs/2511.04790</guid>
<content:encoded><![CDATA[
<div> Keywords: data collection, representation learning, causal inference, multi-modal data, biomedical

Summary: 
- Massive data collection can lead to better understanding and decision-making.
- Representation learning is vital for deep learning applications but can fail in causal tasks.
- There is a need to combine representation learning with causal inference for more accurate predictions of perturbations.
- Multi-modal data, such as observational and perturbational data at various levels, offer new opportunities for causal structure and representation learning.
- A statistical and computational framework is proposed to address fundamental biomedical questions, including effective use of diverse data types for causal discovery, learning causal variables from multi-modal views, and optimizing perturbations.<br /><br />Summary: <div>
arXiv:2511.04790v1 Announce Type: new 
Abstract: Massive data collection holds the promise of a better understanding of complex phenomena and, ultimately, better decisions. Representation learning has become a key driver of deep learning applications, as it allows learning latent spaces that capture important properties of the data without requiring any supervised annotations. Although representation learning has been hugely successful in predictive tasks, it can fail miserably in causal tasks including predicting the effect of a perturbation/intervention. This calls for a marriage between representation learning and causal inference. An exciting opportunity in this regard stems from the growing availability of multi-modal data (observational and perturbational, imaging-based and sequencing-based, at the single-cell level, tissue-level, and organism-level). We outline a statistical and computational framework for causal structure and representation learning motivated by fundamental biomedical questions: how to effectively use observational and perturbational data to perform causal discovery on observed causal variables; how to use multi-modal views of the system to learn causal variables; and how to design optimal perturbations.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing</title>
<link>https://arxiv.org/abs/2511.04791</link>
<guid>https://arxiv.org/abs/2511.04791</guid>
<content:encoded><![CDATA[
<div> framework, LLM serving, GPU, latency, throughput
Summary:
DuetServe is a new framework for serving Large Language Models (LLMs) that aims to achieve high throughput while meeting strict latency Service Level Objectives (SLOs) across prefill and decode phases. It operates on a single GPU and dynamically activates GPU spatial multiplexing to maintain isolation between the phases when necessary. The framework uses an attention-aware roofline model to predict latency, a partitioning optimizer to select optimal SM split for maximizing throughput under time-between-tokens (TBT) constraints, and an interruption-free execution engine to reduce CPU-GPU synchronization overhead. Evaluations demonstrate that DuetServe improves total throughput by up to 1.3x while ensuring low generation latency compared to existing frameworks. <br /><br />Summary: <div>
arXiv:2511.04791v1 Announce Type: new 
Abstract: Modern LLM serving systems must sustain high throughput while meeting strict latency SLOs across two distinct inference phases: compute-intensive prefill and memory-bound decode phases. Existing approaches either (1) aggregate both phases on shared GPUs, leading to interference between prefill and decode phases, which degrades time-between-tokens (TBT); or (2) disaggregate the two phases across GPUs, improving latency but wasting resources through duplicated models and KV cache transfers. We present DuetServe, a unified LLM serving framework that achieves disaggregation-level isolation within a single GPU. DuetServe operates in aggregated mode by default and dynamically activates SM-level GPU spatial multiplexing when TBT degradation is predicted. Its key idea is to decouple prefill and decode execution only when needed through fine-grained, adaptive SM partitioning that provides phase isolation only when contention threatens latency service level objectives (SLOs). DuetServe integrates (1) an attention-aware roofline model to forecast iteration latency, (2) a partitioning optimizer that selects the optimal SM split to maximize throughput under TBT constraints, and (3) an interruption-free execution engine that eliminates CPU-GPU synchronization overhead. Evaluations show that DuetServe improves total throughput by up to 1.3x while maintaining low generation latency compared to state-of-the-art frameworks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator</title>
<link>https://arxiv.org/abs/2511.04804</link>
<guid>https://arxiv.org/abs/2511.04804</guid>
<content:encoded><![CDATA[
<div> Keywords: Simplex-FEM Networks, piecewise-polynomial predictor, finite-element field, warp, mesh

Summary: 
Simplex-FEM Networks (SiFEN) is a novel approach for representing functions using a globally continuous finite-element field on a learned simplicial mesh. By using barycentric coordinates, SiFEN achieves explicit locality and controllable smoothness, leading to improved efficiency and performance. By pairing Bernstein-Bezier polynomials with a warp and training with shape regularization, SiFEN can achieve the classic FEM approximation rate under certain assumptions. Empirical results show that SiFEN outperforms MLPs and KANs on various tasks while reducing inference latency and improving calibration. Overall, SiFEN offers a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks. <div>
arXiv:2511.04804v1 Announce Type: new 
Abstract: We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents f: R^d -> R^k as a globally C^r finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most d+1 basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PuzzleMoE: Efficient Compression of Large Mixture-of-Experts Models via Sparse Expert Merging and Bit-packed inference</title>
<link>https://arxiv.org/abs/2511.04805</link>
<guid>https://arxiv.org/abs/2511.04805</guid>
<content:encoded><![CDATA[
<div> compression, Mixture-of-Experts, sparse merging, bit-packed encoding, inference speedup
Summary:
PuzzleMoE is a new compression method for Mixture-of-Experts (MoE) models that achieves high accuracy and efficient inference without the need for training. It incorporates sparse expert merging by identifying weight redundancy and specialization, using a dual-mask to capture shared and expert-specific parameters. Additionally, PuzzleMoE introduces a bit-packed encoding scheme that reuses exponent bits to enable efficient MoE inference on GPUs. The method can compress MoE models by up to 50% while maintaining accuracy, outperforming prior methods by up to 16.7% on MMLU at a 50% compression ratio. It also achieves up to 1.28 times inference speedup, making it a promising approach for scaling language models efficiently. <br /><br />Summary: <div>
arXiv:2511.04805v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) models have shown strong potential in scaling language models efficiently by activating only a small subset of experts per input. However, their widespread deployment remains limited due to the high memory overhead associated with storing all expert parameters, particularly as the number of experts increases. To address this challenge, prior works have explored expert dropping and merging strategies, yet they often suffer from performance drop at high compression ratios. In this paper, we introduce PuzzleMoE, a training-free MoE compression method that achieves both high accuracy and efficient inference through two key innovations: First, PuzzleMoE performs sparse expert merging by identifying element-wise weight redundancy and specialization. It uses a dual-mask to capture both shared and expert-specific parameters. Second, to avoid the overhead of storing binary masks and signs, PuzzleMoE introduces a bit-packed encoding scheme that reuses underutilized exponent bits, enabling efficient MoE inference on GPUs. Extensive experiments demonstrate that PuzzleMoE can compress MoE models by up to 50% while maintaining accuracy across various tasks. Specifically, it outperforms prior MoE compression methods by up to 16.7% on MMLU at 50% compression ratio, and achieves up to 1.28\times inference speedup.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoding Dynamics: Topological Limitations and Capabilities</title>
<link>https://arxiv.org/abs/2511.04807</link>
<guid>https://arxiv.org/abs/2511.04807</guid>
<content:encoded><![CDATA[
<div> Keywords: data manifold, latent space, autoencoder, topological limitations, dynamical systems

Summary:
Autoencoders consist of an encoder and decoder that aim to capture the essence of a data manifold in a latent space. The goal is to map data points to a lower-dimensional space and back with minimal loss of information. This study delves into the topological challenges and advantages in finding an optimal autoencoder. It explores the intricacies of representing data manifolds and the constraints faced in the process. Additionally, the potential of autoencoders in capturing complex dynamical systems with invariant manifolds is discussed. The research sheds light on the fundamental principles underlying the search for efficient autoencoders and their applications in various fields.<br /><br />Summary: <div>
arXiv:2511.04807v1 Announce Type: new 
Abstract: Given a "data manifold" $M\subset \mathbb{R}^n$ and "latent space" $\mathbb{R}^\ell$, an autoencoder is a pair of continuous maps consisting of an "encoder" $E\colon \mathbb{R}^n\to \mathbb{R}^\ell$ and "decoder" $D\colon \mathbb{R}^\ell\to \mathbb{R}^n$ such that the "round trip" map $D\circ E$ is as close as possible to the identity map $\mbox{id}_M$ on $M$. We present various topological limitations and capabilites inherent to the search for an autoencoder, and describe capabilities for autoencoding dynamical systems having $M$ as an invariant manifold.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharp Minima Can Generalize: A Loss Landscape Perspective On Data</title>
<link>https://arxiv.org/abs/2511.04808</link>
<guid>https://arxiv.org/abs/2511.04808</guid>
<content:encoded><![CDATA[
<div> Volume hypothesis, deep learning, flat minima, large datasets, generalization <br />
<br />
Summary: 
The volume hypothesis proposes that deep learning's effectiveness is due to finding flat minima with large volumes, which generalize well. However, this theory does not explain the impact of large datasets on generalization. Research on measuring minima volumes under different amounts of training data reveals the existence of sharp minima that generalize effectively, despite their small volumes. These minima are often overlooked due to their size. Increasing the dataset size alters the loss landscape, causing previously small minima that generalize well to become relatively larger and more likely to be discovered. <div>
arXiv:2511.04808v1 Announce Type: new 
Abstract: The volume hypothesis suggests deep learning is effective because it is likely to find flat minima due to their large volumes, and flat minima generalize well. This picture does not explain the role of large datasets in generalization. Measuring minima volumes under varying amounts of training data reveals sharp minima which generalize well exist, but are unlikely to be found due to their small volumes. Increasing data changes the loss landscape, such that previously small generalizing minima become (relatively) large.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Standardized Benchmark for Multilabel Antimicrobial Peptide Classification</title>
<link>https://arxiv.org/abs/2511.04814</link>
<guid>https://arxiv.org/abs/2511.04814</guid>
<content:encoded><![CDATA[
<div> Keywords: Antimicrobial peptides, ESCAPE, dataset, multilabel classification, transformer-based model

Summary:
The article introduces ESCAPE, an Expanded Standardized Collection for Antimicrobial Peptide Evaluation, which addresses challenges in antimicrobial peptide research by integrating a vast dataset of peptides from validated repositories. ESCAPE separates antimicrobial peptides from negative sequences and organizes functional annotations into a coherent multilabel hierarchy covering various antimicrobial activities. A transformer-based model is proposed to predict multiple functional activities of peptides, achieving a new state-of-the-art in multilabel peptide classification with a significant improvement in mean Average Precision. This framework provides a comprehensive and reproducible evaluation platform to advance AI-driven antimicrobial peptide discovery research. <div>
arXiv:2511.04814v1 Announce Type: new 
Abstract: Antimicrobial peptides have emerged as promising molecules to combat antimicrobial resistance. However, fragmented datasets, inconsistent annotations, and the lack of standardized benchmarks hinder computational approaches and slow down the discovery of new candidates. To address these challenges, we present the Expanded Standardized Collection for Antimicrobial Peptide Evaluation (ESCAPE), an experimental framework integrating over 80.000 peptides from 27 validated repositories. Our dataset separates antimicrobial peptides from negative sequences and incorporates their functional annotations into a biologically coherent multilabel hierarchy, capturing activities across antibacterial, antifungal, antiviral, and antiparasitic classes. Building on ESCAPE, we propose a transformer-based model that leverages sequence and structural information to predict multiple functional activities of peptides. Our method achieves up to a 2.56% relative average improvement in mean Average Precision over the second-best method adapted for this task, establishing a new state-of-the-art multilabel peptide classification. ESCAPE provides a comprehensive and reproducible evaluation framework to advance AI-driven antimicrobial peptide research.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent reachability homology in machine learning applications</title>
<link>https://arxiv.org/abs/2511.04825</link>
<guid>https://arxiv.org/abs/2511.04825</guid>
<content:encoded><![CDATA[
<div> Persistent reachability homology, digraph data, network classification, epilepsy detection, topological features  
Summary:  
- The study explores the use of persistent reachability homology (PRH) on directed graph data for network classification in epilepsy detection, comparing it to directed flag complex (DPH).  
- PRH is advantageous as it considers condensations of digraphs in the persistent filtration, leading to computation from smaller digraphs.  
- Betti curves and their integrals are utilized as topological features in the classification task, implemented on support vector machine.  
- Results demonstrate that PRH outperforms DPH in classification accuracy, highlighting the effectiveness of PRH in this key neuroscience problem.  
- The comparison between PRH and DPH showcases the potential of topological methods in analyzing and distinguishing complex network structures in medical data.  
<br /><br /> <div>
arXiv:2511.04825v1 Announce Type: new 
Abstract: We explore the recently introduced persistent reachability homology (PRH) of digraph data, i.e. data in the form of directed graphs. In particular, we study the effectiveness of PRH in network classification task in a key neuroscience problem: epilepsy detection. PRH is a variation of the persistent homology of digraphs, more traditionally based on the directed flag complex (DPH). A main advantage of PRH is that it considers the condensations of the digraphs appearing in the persistent filtration and thus is computed from smaller digraphs. We compare the effectiveness of PRH to that of DPH and we show that PRH outperforms DPH in the classification task. We use the Betti curves and their integrals as topological features and implement our pipeline on support vector machine.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2511.04834</link>
<guid>https://arxiv.org/abs/2511.04834</guid>
<content:encoded><![CDATA[
<div> concept inversion, text-to-image generative models, harmful content, defense mechanisms, implicit negative embeddings

Summary: 
This article discusses the challenges faced in addressing the potential of text-to-image generative models to produce harmful content. Two main approaches have been proposed to tackle this issue: fine-tuning models to unlearn harmful concepts and training-free guidance methods using negative prompts. However, combining these approaches has shown limited effectiveness, leading to a critical incompatibility between them. The authors propose a novel method that replaces negative prompts with implicit negative embeddings obtained through concept inversion. This method requires no modifications to existing approaches and can be easily integrated into pipelines. Experimental validation on nudity and violence benchmarks shows improved defense success rates while preserving the core semantics of input prompts. <div>
arXiv:2511.04834v1 Announce Type: new 
Abstract: Recent advances in text-to-image generative models have raised concerns about their potential to produce harmful content when provided with malicious input text prompts. To address this issue, two main approaches have emerged: (1) fine-tuning the model to unlearn harmful concepts and (2) training-free guidance methods that leverage negative prompts. However, we observe that combining these two orthogonal approaches often leads to marginal or even degraded defense performance. This observation indicates a critical incompatibility between two paradigms, which hinders their combined effectiveness. In this work, we address this issue by proposing a conceptually simple yet experimentally robust method: replacing the negative prompts used in training-free methods with implicit negative embeddings obtained through concept inversion. Our method requires no modification to either approach and can be easily integrated into existing pipelines. We experimentally validate its effectiveness on nudity and violence benchmarks, demonstrating consistent improvements in defense success rate while preserving the core semantics of input prompts.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression</title>
<link>https://arxiv.org/abs/2511.04838</link>
<guid>https://arxiv.org/abs/2511.04838</guid>
<content:encoded><![CDATA[
<div> Sparse regions, molecular property prediction, Graph Neural Networks, SPECTRA, spectral domain<br />
Summary:<br />
The study introduces SPECTRA, a framework for molecular property prediction that addresses the challenge of sparse regions in the target space. By reconstructing multi-attribute molecular graphs and aligning molecule pairs through Gromov-Wasserstein couplings, SPECTRA generates realistic molecular graphs in the spectral domain. The framework uses a rarity-aware budgeting scheme to focus augmentation where data are scarce. Utilizing edge-aware Chebyshev convolutions in a spectral GNN, SPECTRA effectively densifies underrepresented regions without compromising global accuracy. Results on benchmarks show that SPECTRA improves error in relevant target ranges while maintaining competitive overall mean absolute error. Additionally, the synthesized molecules produced by SPECTRA exhibit interpretable structures that reflect the underlying spectral geometry. This demonstrates the effectiveness and efficiency of spectral, geometry-aware augmentation in addressing imbalanced molecular property regression. <br /><br />Summary: <div>
arXiv:2511.04838v1 Announce Type: new 
Abstract: In molecular property prediction, the most valuable compounds (e.g., high potency) often occupy sparse regions of the target space. Standard Graph Neural Networks (GNNs) commonly optimize for the average error, underperforming on these uncommon but critical cases, with existing oversampling methods often distorting molecular topology. In this paper, we introduce SPECTRA, a Spectral Target-Aware graph augmentation framework that generates realistic molecular graphs in the spectral domain. SPECTRA (i) reconstructs multi-attribute molecular graphs from SMILES; (ii) aligns molecule pairs via (Fused) Gromov-Wasserstein couplings to obtain node correspondences; (iii) interpolates Laplacian eigenvalues, eigenvectors and node features in a stable share-basis; and (iv) reconstructs edges to synthesize physically plausible intermediates with interpolated targets. A rarity-aware budgeting scheme, derived from a kernel density estimation of labels, concentrates augmentation where data are scarce. Coupled with a spectral GNN using edge-aware Chebyshev convolutions, SPECTRA densifies underrepresented regions without degrading global accuracy. On benchmarks, SPECTRA consistently improves error in relevant target ranges while maintaining competitive overall MAE, and yields interpretable synthetic molecules whose structure reflects the underlying spectral geometry. Our results demonstrate that spectral, geometry-aware augmentation is an effective and efficient strategy for imbalanced molecular property regression.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sublinear iterations can suffice even for DDPMs</title>
<link>https://arxiv.org/abs/2511.04844</link>
<guid>https://arxiv.org/abs/2511.04844</guid>
<content:encoded><![CDATA[
<div> Keywords: SDE-based methods, denoising diffusion probabilistic models, DDRaM, sublinear complexity bound, image synthesis models

Summary:<br />
The article introduces a new integrator called the denoising diffusion randomized midpoint method (DDRaM) for denoising diffusion probabilistic models (DDPMs). Unlike prior analyses that focused on exponential Euler discretization, DDRaM leverages a randomized midpoint for better SDE approximation. Using the "shifted composition rule" framework, the algorithm demonstrates favorable discretization properties with sublinear complexity of $\widetilde{O}(\sqrt{d})$ score evaluations for convergence. This is a significant improvement as it is the first sublinear complexity bound for pure DDPM sampling, contrasting prior works using ODE-based sampling. Experimental validations confirm DDRaM's practical advantages, particularly in conjunction with pre-trained image synthesis models. Overall, the study highlights the effectiveness of DDRaM in improving sample generation tasks within the realm of denoising diffusion probabilistic models. 

<br /><br />Summary: <div>
arXiv:2511.04844v1 Announce Type: new 
Abstract: SDE-based methods such as denoising diffusion probabilistic models (DDPMs) have shown remarkable success in real-world sample generation tasks. Prior analyses of DDPMs have been focused on the exponential Euler discretization, showing guarantees that generally depend at least linearly on the dimension or initial Fisher information. Inspired by works in log-concave sampling (Shen and Lee, 2019), we analyze an integrator -- the denoising diffusion randomized midpoint method (DDRaM) -- that leverages an additional randomized midpoint to better approximate the SDE. Using a recently-developed analytic framework called the "shifted composition rule", we show that this algorithm enjoys favorable discretization properties under appropriate smoothness assumptions, with sublinear $\widetilde{O}(\sqrt{d})$ score evaluations needed to ensure convergence. This is the first sublinear complexity bound for pure DDPM sampling -- prior works which obtained such bounds worked instead with ODE-based sampling and had to make modifications to the sampler which deviate from how they are used in practice. We also provide experimental validation of the advantages of our method, showing that it performs well in practice with pre-trained image synthesis models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating U.S. Consumer Demand for Food Products with Innovative Transportation Certificates Based on Stated Preferences and Machine Learning Approaches</title>
<link>https://arxiv.org/abs/2511.04845</link>
<guid>https://arxiv.org/abs/2511.04845</guid>
<content:encoded><![CDATA[
<div> Machine learning model, consumer behavior, food products, transportation certificates, U.S.

Summary:
The paper utilizes a machine learning model to estimate consumer behavior for food products with innovative transportation certificates in the U.S. It identifies transportation factors as significant in consumer food purchasing choices and proposes five innovative certificates related to transportation. The study shows a preference for safety and energy certificates in the U.S. food supply chain. Additionally, the influence of price, product type, certificates, and decision-maker factors on purchasing choices is examined. The findings offer data-driven recommendations for improving food supply chain systems.<br /><br />Summary: <div>
arXiv:2511.04845v1 Announce Type: new 
Abstract: This paper utilizes a machine learning model to estimate the consumer's behavior for food products with innovative transportation certificates in the U.S. Building on previous research that examined demand for food products with supply chain traceability using stated preference analysis, transportation factors were identified as significant in consumer food purchasing choices. Consequently, a second experiment was conducted to pinpoint the specific transportation attributes valued by consumers. A machine learning model was applied, and five innovative certificates related to transportation were proposed: Transportation Mode, Internet of Things (IoT), Safety measures, Energy Source, and Must Arrive By Dates (MABDs). The preference experiment also incorporated product-specific and decision-maker factors for control purposes. The findings reveal a notable inclination toward safety and energy certificates within the transportation domain of the U.S. food supply chain. Additionally, the study examined the influence of price, product type, certificates, and decision-maker factors on purchasing choices. Ultimately, the study offers data-driven recommendations for improving food supply chain systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounded Test-Time Adaptation for LLM Agents</title>
<link>https://arxiv.org/abs/2511.04847</link>
<guid>https://arxiv.org/abs/2511.04847</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, adaptation, environment-specific information, deployment-time dynamics grounding, generalization

Summary:
Two strategies are proposed to adapt Large Language Model (LLM) agents for better generalization to novel and complex environments. The first strategy involves online distributional adaptation to align the model's output distribution with environment-specific formats. The second strategy, deployment-time dynamics grounding, helps the agent learn the environment's causal dynamics before task execution. Both strategies are shown to be effective across various benchmarks, including function calling and web navigation, with minimal computational cost. The dynamics grounding method proves particularly effective in complex environments with unpredictable dynamics, significantly improving the agent's success rate. For instance, on the WebArena multi-site split, the method boosts the success rate from 2% to 23%. These strategies offer a promising approach to enhancing the generalizability and capability of LLM-based agents in diverse environments. 

<br /><br />Summary: <div>
arXiv:2511.04847v1 Announce Type: new 
Abstract: Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion</title>
<link>https://arxiv.org/abs/2511.04854</link>
<guid>https://arxiv.org/abs/2511.04854</guid>
<content:encoded><![CDATA[
<div> fragmentation, ligand, protein, molecular docking, SigmaDock

Summary:<br />
- The article introduces SigmaDock, a novel deep learning approach for molecular docking in drug discovery.
- SigmaDock utilizes a fragmentation scheme to decompose ligands into rigid-body fragments and reassembles them within a protein's binding pocket using an SE(3) Riemannian diffusion model.
- By operating at the fragment level in SE(3), SigmaDock leverages geometric priors, achieving state-of-the-art performance with high success rates on the PoseBusters set.
- The model outperforms recent deep learning approaches, reaching Top-1 success rates above 79.9% for RMSD<2 & PB-valid, showcasing consistent generalization to unseen proteins.
- SigmaDock marks a significant advancement in the reliability and feasibility of deep learning for molecular modeling, surpassing classical physics-based docking under the PB train-test split.

Summary: <div>
arXiv:2511.04854v1 Announce Type: new 
Abstract: Determining the binding pose of a ligand to a protein, known as molecular docking, is a fundamental task in drug discovery. Generative approaches promise faster, improved, and more diverse pose sampling than physics-based methods, but are often hindered by chemically implausible outputs, poor generalisability, and high computational cost. To address these challenges, we introduce a novel fragmentation scheme, leveraging inductive biases from structural chemistry, to decompose ligands into rigid-body fragments. Building on this decomposition, we present SigmaDock, an SE(3) Riemannian diffusion model that generates poses by learning to reassemble these rigid bodies within the binding pocket. By operating at the level of fragments in SE(3), SigmaDock exploits well-established geometric priors while avoiding overly complex diffusion processes and unstable training dynamics. Experimentally, we show SigmaDock achieves state-of-the-art performance, reaching Top-1 success rates (RMSD<2 & PB-valid) above 79.9% on the PoseBusters set, compared to 12.7-30.8% reported by recent deep learning approaches, whilst demonstrating consistent generalisation to unseen proteins. SigmaDock is the first deep learning approach to surpass classical physics-based docking under the PB train-test split, marking a significant leap forward in the reliability and feasibility of deep learning for molecular modelling.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.04856</link>
<guid>https://arxiv.org/abs/2511.04856</guid>
<content:encoded><![CDATA[
<div> Keywords: Continuous Semi-Quantum Boltzmann Machines, continuous-action reinforcement learning, quantum-classical model, Actor-Critic algorithms, continuous Q-learning framework <br />
Summary: <br />
This article introduces Continuous Semi-Quantum Boltzmann Machines (CSQBMs) for continuous-action reinforcement learning. CSQBMs combine exponential-family priors with quantum Boltzmann distributions to reduce qubit requirements while maintaining strong expressiveness. Gradients with respect to continuous variables can be computed analytically, facilitating integration into Actor-Critic algorithms. A continuous Q-learning framework is proposed, replacing global maximization with efficient sampling from the CSQBM distribution to address stability issues in continuous control. This hybrid quantum-classical model offers a promising approach for continuous-action reinforcement learning tasks, providing a theoretical foundation and practical implementation strategies for improved performance and efficiency.<br /> <div>
arXiv:2511.04856v1 Announce Type: new 
Abstract: We introduce theoretically grounded Continuous Semi-Quantum Boltzmann Machines (CSQBMs) that supports continuous-action reinforcement learning. By combining exponential-family priors over visible units with quantum Boltzmann distributions over hidden units, CSQBMs yield a hybrid quantum-classical model that reduces qubit requirements while retaining strong expressiveness. Crucially, gradients with respect to continuous variables can be computed analytically, enabling direct integration into Actor-Critic algorithms. Building on this, we propose a continuous Q-learning framework that replaces global maximization by efficient sampling from the CSQBM distribution, thereby overcoming instability issues in continuous control.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting</title>
<link>https://arxiv.org/abs/2511.04865</link>
<guid>https://arxiv.org/abs/2511.04865</guid>
<content:encoded><![CDATA[
<div> Keywords: food banks, forecasting, reinforcement learning, metalearning, humanitarian supply chains

Summary:<br /><br />
The study introduces FoodRL, a novel reinforcement learning framework designed to improve the accuracy of forecasting in-kind donations for food banks. Traditional forecasting models struggle to adapt to the highly volatile nature of donations, influenced by seasonal variations and natural disasters such as hurricanes and wildfires. FoodRL addresses these challenges by clustering and weighting diverse forecasting models based on recent performance and contextual information. Tested on data from two U.S. food banks - one impacted by wildfires on the West Coast and the other by hurricanes on the East Coast - FoodRL consistently outperformed baseline methods, especially during disruptive periods. By providing more reliable and adaptive forecasts, FoodRL could potentially increase the redistribution of food by the equivalent of 1.7 million meals annually. This approach showcases the potential for adaptive ensemble learning in humanitarian supply chains, highlighting its significant social impact. <div>
arXiv:2511.04865v1 Announce Type: new 
Abstract: Food banks are crucial for alleviating food insecurity, but their effectiveness hinges on accurately forecasting highly volatile in-kind donations to ensure equitable and efficient resource distribution. Traditional forecasting models often fail to maintain consistent accuracy due to unpredictable fluctuations and concept drift driven by seasonal variations and natural disasters such as hurricanes in the Southeastern U.S. and wildfires in the West Coast. To address these challenges, we propose FoodRL, a novel reinforcement learning (RL) based metalearning framework that clusters and dynamically weights diverse forecasting models based on recent performance and contextual information. Evaluated on multi-year data from two structurally distinct U.S. food banks-one large regional West Coast food bank affected by wildfires and another state-level East Coast food bank consistently impacted by hurricanes, FoodRL consistently outperforms baseline methods, particularly during periods of disruption or decline. By delivering more reliable and adaptive forecasts, FoodRL can facilitate the redistribution of food equivalent to 1.7 million additional meals annually, demonstrating its significant potential for social impact as well as adaptive ensemble learning for humanitarian supply chains.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Interest and Systemic Benefits: Emergence of Collective Rationality in Mixed Autonomy Traffic Through Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.04883</link>
<guid>https://arxiv.org/abs/2511.04883</guid>
<content:encoded><![CDATA[
<div> Autonomous Vehicles, Mixed Autonomy Traffic, Collective Rationality, Deep Reinforcement Learning, System-Level Objectives <br /> 
Summary: <br /> 
Autonomous vehicles (AVs) are set to join human-driven vehicles (HVs) on the roads, raising questions about how self-interested agents can benefit the overall traffic system. This study explores the concept of collective rationality (CR) in mixed autonomy traffic, where driving agents may cooperate for mutual gain despite pursuing individual interests. Using deep reinforcement learning (DRL) and a simple reward design, the research demonstrates the emergence of CR among self-interested traffic agents in various scenarios. Results indicate the robustness of this phenomenon, with a proposed mechanism explaining its occurrence in dynamic environments. The findings suggest the potential for leveraging advanced learning methods, such as federated learning, to foster collective cooperation among self-interested driving agents in mixed-autonomy systems. <div>
arXiv:2511.04883v1 Announce Type: new 
Abstract: Autonomous vehicles (AVs) are expected to be commercially available in the near future, leading to mixed autonomy traffic consisting of both AVs and human-driven vehicles (HVs). Although numerous studies have shown that AVs can be deployed to benefit the overall traffic system performance by incorporating system-level goals into their decision making, it is not clear whether the benefits still exist when agents act out of self-interest -- a trait common to all driving agents, both human and autonomous. This study aims to understand whether self-interested AVs can bring benefits to all driving agents in mixed autonomy traffic systems. The research is centered on the concept of collective rationality (CR). This concept, originating from game theory and behavioral economics, means that driving agents may cooperate collectively even when pursuing individual interests. Our recent research has proven the existence of CR in an analytical game-theoretical model and empirically in mixed human-driven traffic. In this paper, we demonstrate that CR can be attained among driving agents trained using deep reinforcement learning (DRL) with a simple reward design. We examine the extent to which self-interested traffic agents can achieve CR without directly incorporating system-level objectives. Results show that CR consistently emerges in various scenarios, which indicates the robustness of this property. We also postulate a mechanism to explain the emergence of CR in the microscopic and dynamic environment and verify it based on simulation evidence. This research suggests the possibility of leveraging advanced learning methods (such as federated learning) to achieve collective cooperation among self-interested driving agents in mixed-autonomy systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models</title>
<link>https://arxiv.org/abs/2511.04902</link>
<guid>https://arxiv.org/abs/2511.04902</guid>
<content:encoded><![CDATA[
<div> unsupervised reinforcement learning, large language models, model sizes, reasoning capabilities, data curation <br />
<br />
Summary: <br />
Recent advancements in large language models have highlighted the potential of unsupervised reinforcement learning for enhancing reasoning abilities without external supervision. However, the applicability of label-free RL methods to smaller base models with limited reasoning capabilities is not well understood. This study systematically investigates the performance of label-free RL across a range of model sizes and reasoning strengths, revealing significant limitations. It is found that smaller models struggle to generate sufficient chain-of-thought reasoning for effective self-reflection, with training data difficulty also playing a critical role in success. To address these challenges, a novel approach incorporating curriculum learning and data curation is proposed, demonstrating consistent enhancements across all model sizes and reasoning capabilities. This method offers a promising direction for enhancing unsupervised RL and improving reasoning abilities in resource-constrained models. <div>
arXiv:2511.04902v1 Announce Type: new 
Abstract: Recent advances in large language models have demonstrated the promise of unsupervised reinforcement learning (RL) methods for enhancing reasoning capabilities without external supervision. However, the generalizability of these label-free RL approaches to smaller base models with limited reasoning capabilities remains unexplored. In this work, we systematically investigate the performance of label-free RL methods across different model sizes and reasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals critical limitations: label-free RL is highly dependent on the base model's pre-existing reasoning capability, with performance often degrading below baseline levels for weaker models. We find that smaller models fail to generate sufficiently long or diverse chain-of-thought reasoning to enable effective self-reflection, and that training data difficulty plays a crucial role in determining success. To address these challenges, we propose a simple yet effective method for label-free RL that utilizes curriculum learning to progressively introduce harder problems during training and mask no-majority rollouts during training. Additionally, we introduce a data curation pipeline to generate samples with predefined difficulty. Our approach demonstrates consistent improvements across all model sizes and reasoning capabilities, providing a path toward more robust unsupervised RL that can bootstrap reasoning abilities in resource-constrained models. We make our code available at https://github.com/BorealisAI/CuMa
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Craftax: Benchmarking Open-Ended Multi-Agent Reinforcement Learning at the Hyperscale</title>
<link>https://arxiv.org/abs/2511.04904</link>
<guid>https://arxiv.org/abs/2511.04904</guid>
<content:encoded><![CDATA[
<div> Craftax-MA, Craftax, JAX, multi-agent reinforcement learning, benchmarks<br />
<br />
Summary:<br />
The article introduces Craftax-MA, an extension of Craftax for multi-agent reinforcement learning (MARL) benchmarks. Craftax-MA is written in JAX, allowing for efficient training runs. Additionally, Craftax-Coop is introduced, adding complexity through heterogeneous agents and trading mechanics that require advanced cooperation. Existing algorithms struggle with challenges in this benchmark such as long-horizon credit assignment, exploration, and cooperation. The article argues that Craftax-MA and Craftax-Coop provide a compelling challenge for MARL and have the potential to drive further research in the field. <div>
arXiv:2511.04904v1 Announce Type: new 
Abstract: Progress in multi-agent reinforcement learning (MARL) requires challenging benchmarks that assess the limits of current methods. However, existing benchmarks often target narrow short-horizon challenges that do not adequately stress the long-term dependencies and generalization capabilities inherent in many multi-agent systems. To address this, we first present \textit{Craftax-MA}: an extension of the popular open-ended RL environment, Craftax, that supports multiple agents and evaluates a wide range of general abilities within a single environment. Written in JAX, \textit{Craftax-MA} is exceptionally fast with a training run using 250 million environment interactions completing in under an hour. To provide a more compelling challenge for MARL, we also present \textit{Craftax-Coop}, an extension introducing heterogeneous agents, trading and more mechanics that require complex cooperation among agents for success. We provide analysis demonstrating that existing algorithms struggle with key challenges in this benchmark, including long-horizon credit assignment, exploration and cooperation, and argue for its potential to drive long-term research in MARL.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Swap Multicalibration of Elicitable Properties</title>
<link>https://arxiv.org/abs/2511.04907</link>
<guid>https://arxiv.org/abs/2511.04907</guid>
<content:encoded><![CDATA[
<div> multicalibration, elicitable property, swap multicalibration, online agnostic learner, algorithm

Summary: 
This paper explores the concept of multicalibration for elicitable properties in arbitrary bounded hypothesis classes, introducing the notion of swap multicalibration. A new oracle-efficient algorithm is proposed, leveraging an online agnostic learner to achieve improved $\ell_r$-swap multicalibration error rates with high probability. Specifically, for $r\geq2$, the algorithm achieves $T^{1/(r+1)}$ $\ell_r$-swap multicalibration error, significantly outperforming previous bounds. For the special case of $r=2$, the algorithm attains $T^{1/3}$ $\ell_2$-swap multicalibration error, addressing an open question on achieving $\sqrt{T}$ $\ell_2$-mean multicalibration error in a strongly affirmative manner. This work extends the understanding of multicalibration and elicitable properties in the online setting, offering efficient algorithms for achieving accurate predictions while ensuring algorithmic fairness. <div>
arXiv:2511.04907v1 Announce Type: new 
Abstract: Multicalibration [HJKRR18] is an algorithmic fairness perspective that demands that the predictions of a predictor are correct conditional on themselves and membership in a collection of potentially overlapping subgroups of a population. The work of [NR23] established a surprising connection between multicalibration for an arbitrary property $\Gamma$ (e.g., mean or median) and property elicitation: a property $\Gamma$ can be multicalibrated if and only if it is elicitable, where elicitability is the notion that the true property value of a distribution can be obtained by solving a regression problem over the distribution. In the online setting, [NR23] proposed an inefficient algorithm that achieves $\sqrt T$ $\ell_2$-multicalibration error for a hypothesis class of group membership functions and an elicitable property $\Gamma$, after $T$ rounds of interaction between a forecaster and adversary.
  In this paper, we generalize multicalibration for an elicitable property $\Gamma$ from group membership functions to arbitrary bounded hypothesis classes and introduce a stronger notion -- swap multicalibration, following [GKR23]. Subsequently, we propose an oracle-efficient algorithm which, when given access to an online agnostic learner, achieves $T^{1/(r+1)}$ $\ell_r$-swap multicalibration error with high probability (for $r\ge2$) for a hypothesis class with bounded sequential Rademacher complexity and an elicitable property $\Gamma$. For the special case of $r=2$, this implies an oracle-efficient algorithm that achieves $T^{1/3}$ $\ell_2$-swap multicalibration error, which significantly improves on the previously established bounds for the problem [NR23, GMS25, LSS25a], and completely resolves an open question raised in [GJRR24] on the possibility of an oracle-efficient algorithm that achieves $\sqrt{T}$ $\ell_2$-mean multicalibration error by answering it in a strongly affirmative sense.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dual Perspective on Decision-Focused Learning: Scalable Training via Dual-Guided Surrogates</title>
<link>https://arxiv.org/abs/2511.04909</link>
<guid>https://arxiv.org/abs/2511.04909</guid>
<content:encoded><![CDATA[
<div> prediction, optimization, decision-focused learning, combinatorial selection problems, dual variables

Summary:<br />
- The paper introduces Dual-Guided Loss (DGL) for decision-focused learning in optimization problems with predicted quantities.
- DGL aims to reduce solver dependence and improve performance by leveraging dual variables and introducing a scalable objective.
- It decouples optimization from gradient updates and periodically solves the downstream problem, training on dual-adjusted targets in between refreshes.
- DGL is designed specifically for combinatorial selection problems with natural constraints, such as matching, knapsack, and shortest path.
- The approach demonstrates asymptotically diminishing decision regret, has low runtime complexity, and outperforms state-of-the-art methods with fewer solver calls and less training time.<br /><br /> <div>
arXiv:2511.04909v1 Announce Type: new 
Abstract: Many real-world decisions are made under uncertainty by solving optimization problems using predicted quantities. This predict-then-optimize paradigm has motivated decision-focused learning, which trains models with awareness of how the optimizer uses predictions, improving the performance of downstream decisions. Despite its promise, scaling is challenging: state-of-the-art methods either differentiate through a solver or rely on task-specific surrogates, both of which require frequent and expensive calls to an optimizer, often a combinatorial one. In this paper, we leverage dual variables from the downstream problem to shape learning and introduce Dual-Guided Loss (DGL), a simple, scalable objective that preserves decision alignment while reducing solver dependence. We construct DGL specifically for combinatorial selection problems with natural one-of-many constraints, such as matching, knapsack, and shortest path. Our approach (a) decouples optimization from gradient updates by solving the downstream problem only periodically; (b) between refreshes, trains on dual-adjusted targets using simple differentiable surrogate losses; and (c) as refreshes become less frequent, drives training cost toward standard supervised learning while retaining strong decision alignment. We prove that DGL has asymptotically diminishing decision regret, analyze runtime complexity, and show on two problem classes that DGL matches or exceeds state-of-the-art DFL methods while using far fewer solver calls and substantially less training time. Code is available at https://github.com/paularodr/Dual-Guided-Learning.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Algorithms in Statistical Modelling Bridging Theory and Application</title>
<link>https://arxiv.org/abs/2511.04918</link>
<guid>https://arxiv.org/abs/2511.04918</guid>
<content:encoded><![CDATA[
<div> novel, ML algorithms, statistical modelling, predictive analytics, hybrid models  
Summary:  
This paper introduces innovative ways of integrating machine learning (ML) algorithms with traditional statistical models, revolutionizing data analysis, predictive analytics, and decision-making processes. By exploring the connections between ML and statistical models, the study reveals how modern ML algorithms enhance conventional models, improving performance, scalability, flexibility, and robustness. The integration of new algorithms results in hybrid models that offer significant enhancements in predictive accuracy, robustness, and interpretability. This approach highlights the potential of combining ML techniques with traditional statistical methods to achieve superior outcomes in data analysis and decision-making processes.<br /><br />Summary: <div>
arXiv:2511.04918v1 Announce Type: new 
Abstract: It involves the completely novel ways of integrating ML algorithms with traditional statistical modelling that has changed the way we analyze data, do predictive analytics or make decisions in the fields of the data. In this paper, we study some ML and statistical model connections to understand ways in which some modern ML algorithms help 'enrich' conventional models; we demonstrate how new algorithms improve performance, scale, flexibility and robustness of the traditional models. It shows that the hybrid models are of great improvement in predictive accuracy, robustness, and interpretability
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding</title>
<link>https://arxiv.org/abs/2511.04934</link>
<guid>https://arxiv.org/abs/2511.04934</guid>
<content:encoded><![CDATA[
<div> unlearning, large language models, forgetting, knowledge removal, probabilistic decoding <br />
Summary: <br />
Unlearning in large language models is essential for regulatory compliance and ethical AI development. Current unlearning methods, despite appearing successful under deterministic decoding, often fail to completely forget sensitive information when models are sampled with probabilistic decoding. A new metric called leak@$k quantifies the likelihood of forgotten knowledge resurfacing during generation. Evaluating existing unlearning techniques using leak@$k on benchmarks like TOFU, MUSE, and WMDP reveals persistent knowledge leakage across tasks. This study highlights the inadequacy of current methods in achieving full forgetting in language models and emphasizes the need for more robust unlearning approaches. <br /> <div>
arXiv:2511.04934v1 Announce Type: new 
Abstract: Unlearning in large language models (LLMs) is critical for regulatory compliance and for building ethical generative AI systems that avoid producing private, toxic, illegal, or copyrighted content. Despite rapid progress, in this work we show that \textit{almost all} existing unlearning methods fail to achieve true forgetting in practice. Specifically, while evaluations of these `unlearned' models under deterministic (greedy) decoding often suggest successful knowledge removal using standard benchmarks (as has been done in the literature), we show that sensitive information reliably resurfaces when models are sampled with standard probabilistic decoding. To rigorously capture this vulnerability, we introduce \texttt{leak@$k$}, a new meta-evaluation metric that quantifies the likelihood of forgotten knowledge reappearing when generating $k$ samples from the model under realistic decoding strategies. Using three widely adopted benchmarks, TOFU, MUSE, and WMDP, we conduct the first large-scale, systematic study of unlearning reliability using our newly defined \texttt{leak@$k$} metric. Our findings demonstrate that knowledge leakage persists across methods and tasks, underscoring that current state-of-the-art unlearning techniques provide only limited forgetting and highlighting the urgent need for more robust approaches to LLM unlearning.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural Properties, Cycloid Trajectories and Non-Asymptotic Guarantees of EM Algorithm for Mixed Linear Regression</title>
<link>https://arxiv.org/abs/2511.04937</link>
<guid>https://arxiv.org/abs/2511.04937</guid>
<content:encoded><![CDATA[
<div> Keywords: EM algorithm, mixed linear regression, convergence guarantees, cycloid trajectories, unknown parameters<br />
<br />
Summary: In this work, the Expectation-Maximization (EM) algorithm for two-component Mixed Linear Regression (2MLR) with unknown mixing weights and regression parameters is investigated. The study focuses on the structural properties, cycloid trajectories, and non-asymptotic convergence guarantees of the EM algorithm in the fully unknown setting. Explicit EM update expressions are derived for 2MLR with unknown parameters across all signal-to-noise ratio (SNR) regimes. The trajectory analysis reveals that in the noiseless case, the regression parameters in EM iterations follow a cycloid trajectory. The order of convergence is found to be linear or quadratic depending on the angle between the estimate and ground truth. Non-asymptotic guarantees are established by bounding statistical errors between finite-sample and population EM updates, linking EM's accuracy to the sub-optimality angle, and proving convergence with arbitrary initialization at the finite-sample level. This work presents a novel trajectory-based framework for analyzing the EM algorithm in Mixed Linear Regression. <br /><br />Summary: <div>
arXiv:2511.04937v1 Announce Type: new 
Abstract: This work investigates the structural properties, cycloid trajectories, and non-asymptotic convergence guarantees of the Expectation-Maximization (EM) algorithm for two-component Mixed Linear Regression (2MLR) with unknown mixing weights and regression parameters. Recent studies have established global convergence for 2MLR with known balanced weights and super-linear convergence in noiseless and high signal-to-noise ratio (SNR) regimes. However, the theoretical behavior of EM in the fully unknown setting remains unclear, with its trajectory and convergence order not yet fully characterized. We derive explicit EM update expressions for 2MLR with unknown mixing weights and regression parameters across all SNR regimes and analyze their structural properties and cycloid trajectories. In the noiseless case, we prove that the trajectory of the regression parameters in EM iterations traces a cycloid by establishing a recurrence relation for the sub-optimality angle, while in high SNR regimes we quantify its discrepancy from the cycloid trajectory. The trajectory-based analysis reveals the order of convergence: linear when the EM estimate is nearly orthogonal to the ground truth, and quadratic when the angle between the estimate and ground truth is small at the population level. Our analysis establishes non-asymptotic guarantees by sharpening bounds on statistical errors between finite-sample and population EM updates, relating EM's statistical accuracy to the sub-optimality angle, and proving convergence with arbitrary initialization at the finite-sample level. This work provides a novel trajectory-based framework for analyzing EM in Mixed Linear Regression.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Risk Prediction of Cardiovascular Disease for Diabetic Patients with Machine Learning and Deep Learning Techniques</title>
<link>https://arxiv.org/abs/2511.04971</link>
<guid>https://arxiv.org/abs/2511.04971</guid>
<content:encoded><![CDATA[
<div> diabetes, cardiovascular disease, machine learning, deep learning, risk prediction <br />
Summary: <br />
This study focuses on developing a CVD risk prediction model for diabetic patients using machine learning and deep learning techniques. The BRFSS dataset was preprocessed, and various ML models like Decision Trees and XGBoost, as well as DL models such as LSTM and hybrid models, were implemented. XGBoost and LSTM achieved the highest accuracies of 0.9050, showcasing the effectiveness of these models in predicting CVD risk in diabetic patients. The research highlights the potential of ML and DL models in automating clinical decision-making and enhancing personalized risk management strategies. The high accuracy and F1 scores of the models indicate their capability to aid in improving preventive strategies for cardiovascular diseases. <div>
arXiv:2511.04971v1 Announce Type: new 
Abstract: Accurate prediction of cardiovascular disease (CVD) risk is crucial for healthcare institutions. This study addresses the growing prevalence of diabetes and its strong link to heart disease by proposing an efficient CVD risk prediction model for diabetic patients using machine learning (ML) and hybrid deep learning (DL) approaches. The BRFSS dataset was preprocessed by removing duplicates, handling missing values, identifying categorical and numerical features, and applying Principal Component Analysis (PCA) for feature extraction. Several ML models, including Decision Trees (DT), Random Forest (RF), k-Nearest Neighbors (KNN), Support Vector Machine (SVM), AdaBoost, and XGBoost, were implemented, with XGBoost achieving the highest accuracy of 0.9050. Various DL models, such as Artificial Neural Networks (ANN), Deep Neural Networks (DNN), Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), and Gated Recurrent Unit (GRU), as well as hybrid models combining CNN with LSTM, BiLSTM, and GRU, were also explored. Some of these models achieved perfect recall (1.00), with the LSTM model achieving the highest accuracy of 0.9050. Our research highlights the effectiveness of ML and DL models in predicting CVD risk among diabetic patients, automating and enhancing clinical decision-making. High accuracy and F1 scores demonstrate these models' potential to improve personalized risk management and preventive strategies.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More: Generating Time Series with LLaMA-Style Autoregression in Simple Factorized Latent Spaces</title>
<link>https://arxiv.org/abs/2511.04973</link>
<guid>https://arxiv.org/abs/2511.04973</guid>
<content:encoded><![CDATA[
<div> Generative models, multivariate time series, FAR-TS, disentangled factorization, autoregressive Transformer<br />
Summary:<br />
The article introduces FAR-TS, a novel framework for generating multivariate time series data efficiently. FAR-TS combines disentangled factorization with an autoregressive Transformer model over a discrete latent space to capture static cross-channel correlations and temporal coefficients. This approach allows for fast generation of time series of varying lengths, outperforming existing diffusion-based methods like Diffusion-TS. FAR-TS' streamlined design preserves data quality and interpretability of the latent space, enabling high-quality and flexible time series synthesis. The model's basis decomposition and vector quantization of temporal coefficients ensure efficient and controlled generation of time series data while maintaining cross-channel correlations. <div>
arXiv:2511.04973v1 Announce Type: new 
Abstract: Generative models for multivariate time series are essential for data augmentation, simulation, and privacy preservation, yet current state-of-the-art diffusion-based approaches are slow and limited to fixed-length windows. We propose FAR-TS, a simple yet effective framework that combines disentangled factorization with an autoregressive Transformer over a discrete, quantized latent space to generate time series. Each time series is decomposed into a data-adaptive basis that captures static cross-channel correlations and temporal coefficients that are vector-quantized into discrete tokens. A LLaMA-style autoregressive Transformer then models these token sequences, enabling fast and controllable generation of sequences with arbitrary length. Owing to its streamlined design, FAR-TS achieves orders-of-magnitude faster generation than Diffusion-TS while preserving cross-channel correlations and an interpretable latent space, enabling high-quality and flexible time series synthesis.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Up ROC-Optimizing Support Vector Machines</title>
<link>https://arxiv.org/abs/2511.04979</link>
<guid>https://arxiv.org/abs/2511.04979</guid>
<content:encoded><![CDATA[
<div> scalable variant, ROC-SVM, incomplete U-statistics, low-rank kernel approximation, computational complexity <br />
Summary:<br />
The article introduces a scalable variant of the ROC-SVM classifier that reduces computational complexity by leveraging incomplete U-statistics, making it more practical for real-world applications. The method extends to nonlinear classification by employing a low-rank kernel approximation for efficient training in reproducing kernel Hilbert spaces. Theoretical analysis supports the proposed approximation with an error bound. Empirical results demonstrate that the new method achieves comparable area under the ROC curve (AUC) performance to the original ROC-SVM but with significantly reduced training time. This innovation addresses the limitation of the original ROC-SVM and makes it more feasible for use in scenarios with class imbalance. <div>
arXiv:2511.04979v1 Announce Type: new 
Abstract: The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the area under the ROC curve (AUC) and has become an attractive alternative of the conventional binary classification under the presence of class imbalance. However, its practical use is limited by high computational cost, as training involves evaluating all $O(n^2)$. To overcome this limitation, we develop a scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby substantially reducing computational complexity. We further extend the framework to nonlinear classification through a low-rank kernel approximation, enabling efficient training in reproducing kernel Hilbert spaces. Theoretical analysis establishes an error bound that justifies the proposed approximation, and empirical results on both synthetic and real datasets demonstrate that the proposed method achieves comparable AUC performance to the original ROC-SVM with drastically reduced training time.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking the Black Box: A Five-Dimensional Framework for Evaluating Explainable AI in Credit Risk</title>
<link>https://arxiv.org/abs/2511.04980</link>
<guid>https://arxiv.org/abs/2511.04980</guid>
<content:encoded><![CDATA[
<div> Interpretability, Machine Learning, Financial Industry, Explainability Frameworks, Model Evaluation<br />
Summary:<br />
The paper addresses the challenge faced by the financial industry in balancing the predictability of advanced machine learning models with the explainability required by regulatory entities. It discusses the application of explainability frameworks such as LIME and SHAP to achieve both predictability and transparency in complex models. The authors introduce a novel five-dimensional framework for evaluating model explainability, considering factors like Global and Local Explanations, Consistency, and Complexity. The research demonstrates that sophisticated ML models can be effectively used in regulated financial environments by leveraging modern explainability methods. The structured approach proposed in the paper helps assess the trade-offs between model performance and interpretability, paving the way for more transparent and reliable risk portfolios in the financial sector.<br /> <div>
arXiv:2511.04980v1 Announce Type: new 
Abstract: The financial industry faces a significant challenge modeling and risk portfolios: balancing the predictability of advanced machine learning models, neural network models, and explainability required by regulatory entities (such as Office of the Comptroller of the Currency, Consumer Financial Protection Bureau). This paper intends to fill the gap in the application between these "black box" models and explainability frameworks, such as LIME and SHAP. Authors elaborate on the application of these frameworks on different models and demonstrates the more complex models with better prediction powers could be applied and reach the same level of the explainability, using SHAP and LIME. Beyond the comparison and discussion of performances, this paper proposes a novel five dimensional framework evaluating Inherent Interpretability, Global Explanations, Local Explanations, Consistency, and Complexity to offer a nuanced method for assessing and comparing model explainability beyond simple accuracy metrics. This research demonstrates the feasibility of employing sophisticated, high performing ML models in regulated financial environments by utilizing modern explainability techniques and provides a structured approach to evaluate the crucial trade offs between model performance and interpretability.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Progressive Training: scaling up depth capacity of zero/one-layer models</title>
<link>https://arxiv.org/abs/2511.04981</link>
<guid>https://arxiv.org/abs/2511.04981</guid>
<content:encoded><![CDATA[
<div> progressive training, deep learning, model depth, optimization theory, feature learning
Summary:
Progressive training in deep learning involves scaling up model capacity during training to reduce computation with minimal performance degradation. This study delves into the depth expansion of large models from an optimization theory and feature learning perspective. Insights are offered on aspects such as initializing new layers, transferring hyperparameters, defining learning rate schedules, and determining the timing of model expansion. The proposed zero/one-layer progressive training strategy enables an optimal tradeoff between computation and loss. For example, applying this method to GPT2 can lead to savings of approximately 80% in compute or a 5x acceleration while maintaining nearly the same loss compared to a fully trained 60-layer model with 7B parameters. <div>
arXiv:2511.04981v1 Announce Type: new 
Abstract: Model depth is a double-edged sword in deep learning: deeper models achieve higher accuracy but require higher computational cost. To efficiently train models at scale, an effective strategy is the progressive training, which scales up model capacity during training, hence significantly reducing computation with little to none performance degradation. In this work, we study the depth expansion of large models through the lens of optimization theory and feature learning, offering insights on the initialization of new layers, hyperparameter transfer, learning rate schedule, and timing of model expansion. Specifically, we propose zero/one-layer progressive training for the optimal tradeoff between computation and loss. For example, zero/one-layer progressive training on GPT2 can save $\approx 80\%$ compute, or equivalently accelerate $\approx 5\times$ while achieving almost the same loss, compared to to a fully trained 60-layer model with 7B parameters.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding</title>
<link>https://arxiv.org/abs/2511.04984</link>
<guid>https://arxiv.org/abs/2511.04984</guid>
<content:encoded><![CDATA[
<div> AI, drug design, peptides, protein interactions, generative model
Summary:<br /><br />Peptide2Mol is a novel E(3)-equivariant graph neural network diffusion model that utilizes artificial intelligence to generate small molecules by considering both peptide binders and their protein pocket environments. This approach addresses the limitations of traditional AI-driven methods that overlook endogenous protein interactions with peptides. Peptide2Mol demonstrates state-of-the-art performance in generating bioactive molecules while maintaining similarity to the original peptide binder. The model is trained on large datasets and incorporates sophisticated modeling techniques to optimize molecule design and facilitate peptidomimetic development through a partial diffusion process. These results showcase Peptide2Mol as an effective deep generative model for creating and refining bioactive small molecules from protein binding pockets. <div>
arXiv:2511.04984v1 Announce Type: new 
Abstract: Structure-based drug design has seen significant advancements with the integration of artificial intelligence (AI), particularly in the generation of hit and lead compounds. However, most AI-driven approaches neglect the importance of endogenous protein interactions with peptides, which may result in suboptimal molecule designs. In this work, we present Peptide2Mol, an E(3)-equivariant graph neural network diffusion model that generates small molecules by referencing both the original peptide binders and their surrounding protein pocket environments. Trained on large datasets and leveraging sophisticated modeling techniques, Peptide2Mol not only achieves state-of-the-art performance in non-autoregressive generative tasks, but also produces molecules with similarity to the original peptide binder. Additionally, the model allows for molecule optimization and peptidomimetic design through a partial diffusion process. Our results highlight Peptide2Mol as an effective deep generative model for generating and optimizing bioactive small molecules from protein binding pockets.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Carbon Price Forecasting with Structural Breaks: A Comparative Study of Deep Learning Models</title>
<link>https://arxiv.org/abs/2511.04988</link>
<guid>https://arxiv.org/abs/2511.04988</guid>
<content:encoded><![CDATA[
<div> Breakpoints, deep learning, carbon price forecasting, structural break detection, wavelet signal denoising<br />
Summary:<br />
This paper introduces a hybrid framework for accurate carbon price forecasting by integrating structural break detection algorithms, wavelet signal denoising, and deep learning models (LSTM, GRU, TCN). The framework, tested on European Union Allowance spot prices with exogenous features, outperforms existing models by reducing forecasting errors. The proposed PELT-WT-TCN model achieves the highest prediction accuracy, highlighting the value of integrating structural awareness and multiscale decomposition into deep learning architectures for enhancing accuracy and interpretability in carbon price forecasting and other nonstationary financial time series. <div>
arXiv:2511.04988v1 Announce Type: new 
Abstract: Accurately forecasting carbon prices is essential for informed energy market decision-making, guiding sustainable energy planning, and supporting effective decarbonization strategies. However, it remains challenging due to structural breaks and high-frequency noise caused by frequent policy interventions and market shocks. Existing studies, including the most recent baseline approaches, have attempted to incorporate breakpoints but often treat denoising and modeling as separate processes and lack systematic evaluation across advanced deep learning architectures, limiting the robustness and the generalization capability. To address these gaps, this paper proposes a comprehensive hybrid framework that integrates structural break detection (Bai-Perron, ICSS, and PELT algorithms), wavelet signal denoising, and three state-of-the-art deep learning models (LSTM, GRU, and TCN). Using European Union Allowance (EUA) spot prices from 2007 to 2024 and exogenous features such as energy prices and policy indicators, the framework constructs univariate and multivariate datasets for comparative evaluation. Experimental results demonstrate that our proposed PELT-WT-TCN achieves the highest prediction accuracy, reducing forecasting errors by 22.35% in RMSE and 18.63% in MAE compared to the state-of-the-art baseline model (Breakpoints with Wavelet and LSTM), and by 70.55% in RMSE and 74.42% in MAE compared to the original LSTM without decomposition from the same baseline study. These findings underscore the value of integrating structural awareness and multiscale decomposition into deep learning architectures to enhance accuracy and interpretability in carbon price forecasting and other nonstationary financial time series.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records</title>
<link>https://arxiv.org/abs/2511.04998</link>
<guid>https://arxiv.org/abs/2511.04998</guid>
<content:encoded><![CDATA[
<div> Transformer-based deep learning models, BiPETE, mental health cohorts, alcohol and substance use disorders, AUPRC<br />
<br />
BiPETE, a Bi-Positional Embedding Transformer Encoder, is proposed for disease risk prediction using electronic health records. It integrates rotary positional embeddings and sinusoidal embeddings to model temporal dependencies. Trained on EHR data from depressive disorder and PTSD cohorts for predicting alcohol and substance use disorders (ASUD) risk, BiPETE outperforms baseline models with a 34% and 50% improvement in AUPRC. An ablation study confirms the effectiveness of the dual positional encoding strategy. Integrated Gradients method is applied for model interpretation, revealing key clinical features associated with ASUD risk and protection. These include abnormal inflammatory, hematologic, and metabolic markers, medications, and comorbidities. The study offers a practical and interpretable framework for disease risk prediction using EHR data, providing insights into the risk assessment process and potential risk mitigation strategies.<br /><br />Summary: <div>
arXiv:2511.04998v1 Announce Type: new 
Abstract: Transformer-based deep learning models have shown promise for disease risk prediction using electronic health records(EHRs), but modeling temporal dependencies remains a key challenge due to irregular visit intervals and lack of uniform structure. We propose a Bi-Positional Embedding Transformer Encoder or BiPETE for single-disease prediction, which integrates rotary positional embeddings to encode relative visit timing and sinusoidal embeddings to preserve visit order. Without relying on large-scale pretraining, BiPETE is trained on EHR data from two mental health cohorts-depressive disorder and post-traumatic stress disorder (PTSD)-to predict the risk of alcohol and substance use disorders (ASUD). BiPETE outperforms baseline models, improving the area under the precision-recall curve (AUPRC) by 34% and 50% in the depression and PTSD cohorts, respectively. An ablation study further confirms the effectiveness of the dual positional encoding strategy. We apply the Integrated Gradients method to interpret model predictions, identifying key clinical features associated with ASUD risk and protection, such as abnormal inflammatory, hematologic, and metabolic markers, as well as specific medications and comorbidities. Overall, these key clinical features identified by the attribution methods contribute to a deeper understanding of the risk assessment process and offer valuable clues for mitigating potential risks. In summary, our study presents a practical and interpretable framework for disease risk prediction using EHR data, which can achieve strong performance.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent Coordination via Flow Matching</title>
<link>https://arxiv.org/abs/2511.05005</link>
<guid>https://arxiv.org/abs/2511.05005</guid>
<content:encoded><![CDATA[
<div> learning, coordination, multi-agent, representation, decentralized

Summary:
1. The MAC-Flow framework addresses the trade-off between complexity and computational speed in multi-agent coordination.
2. Effective coordination requires a rich representation of joint behaviors from offline data and the ability to act efficiently in real time.
3. Prior approaches often sacrifice one aspect for the other, leading to either complex coordination but slow computation or fast execution but inability to handle multi-agent interactions.
4. MAC-Flow overcomes this trade-off by learning a flow-based representation of joint behaviors and distilling it into decentralized one-step policies.
5. Across four benchmarks, MAC-Flow achieves significantly faster inference times compared to diffusion-based MARL methods while maintaining good performance, similar to prior Gaussian policy-based offline MARL methods. 

<br /><br />Summary: <div>
arXiv:2511.05005v1 Announce Type: new 
Abstract: This work presents MAC-Flow, a simple yet expressive framework for multi-agent coordination. We argue that requirements of effective coordination are twofold: (i) a rich representation of the diverse joint behaviors present in offline data and (ii) the ability to act efficiently in real time. However, prior approaches often sacrifice one for the other, i.e., denoising diffusion-based solutions capture complex coordination but are computationally slow, while Gaussian policy-based solutions are fast but brittle in handling multi-agent interaction. MAC-Flow addresses this trade-off by first learning a flow-based representation of joint behaviors, and then distilling it into decentralized one-step policies that preserve coordination while enabling fast execution. Across four different benchmarks, including $12$ environments and $34$ datasets, MAC-Flow alleviates the trade-off between performance and computational cost, specifically achieving about $\boldsymbol{\times14.5}$ faster inference compared to diffusion-based MARL methods, while maintaining good performance. At the same time, its inference speed is similar to that of prior Gaussian policy-based offline multi-agent reinforcement learning (MARL) methods.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data</title>
<link>https://arxiv.org/abs/2511.05028</link>
<guid>https://arxiv.org/abs/2511.05028</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated fine-tuning, decentralized data, local drift, OvA-LP, robust FFT

Summary:<br /><br />
The article introduces OvA-LP, a framework designed to address drift in federated fine-tuning (FFT) models caused by heterogeneous client distributions. OvA-LP utilizes linear probing on a frozen encoder combined with a one-vs-all head and a two-stage procedure to suppress drift at its source. It retains 95.9% of its IID accuracy on CIFAR-100 with 100 clients, outperforming state-of-the-art FFT baselines like PFPT and FFT-MoE under the same conditions. OvA-LP also showcases resilience to symmetric and asymmetric label noise. Moreover, the precomputed encoder features ensure that the per-round cost is independent of the encoder size. Overall, OvA-LP provides a principled and efficient approach for achieving robust FFT performance in the presence of client heterogeneity. <div>
arXiv:2511.05028v1 Announce Type: new 
Abstract: Federated fine-tuning (FFT) adapts foundation models to decentralized data but remains fragile under heterogeneous client distributions due to local drift, i.e., client-level update divergences that induce systematic bias and amplified variance in the global model. Existing aggregation and personalization methods largely correct drift post hoc, which proves brittle under extreme non-IID conditions. We introduce OvA-LP, a minimalist framework that is, to our knowledge, the first explicitly designed to suppress drift at its source within the PEFT-based FFT paradigm. OvA-LP combines linear probing on a frozen encoder with a one-vs-all head and a simple two-stage procedure, preserving pretrained feature geometry and decoupling logits to prevent the mechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over shard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of its IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1% (PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains resilience under both symmetric and asymmetric label noise. In addition, precomputing encoder features makes per-round cost nearly independent of encoder size. Together, these results demonstrate that OvA-LP provides a principled and efficient basis for robust FFT under heterogeneity.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Usando LLMs para Programar Jogos de Tabuleiro e Varia\c{c}\~oes</title>
<link>https://arxiv.org/abs/2511.05114</link>
<guid>https://arxiv.org/abs/2511.05114</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, board games, code generation, Claude, DeepSeek, ChatGPT

Summary:
Large Language Models (LLMs) such as Claude, DeepSeek, and ChatGPT are evaluated for their ability to generate code for board games and new game variants. The study aims to expedite the process of creating programs to represent board games by utilizing the efficiency of LLMs in generating code based on contextual information. The three LLMs are tested for their capacity to create code for board games and variants of existing games. By employing LLMs, the time-consuming task of coding board games can be expedited, offering a potential solution for game developers seeking to streamline their programming efforts. This research signifies the emergence of LLMs as promising tools in automating the creation of code for board games, showcasing their potential to enhance the efficiency and speed of game development processes. <div>
arXiv:2511.05114v1 Announce Type: new 
Abstract: Creating programs to represent board games can be a time-consuming task. Large Language Models (LLMs) arise as appealing tools to expedite this process, given their capacity to efficiently generate code from simple contextual information. In this work, we propose a method to test how capable three LLMs (Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as new variants of existing games.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuAnTS: Question Answering on Time Series</title>
<link>https://arxiv.org/abs/2511.05124</link>
<guid>https://arxiv.org/abs/2511.05124</guid>
<content:encoded><![CDATA[
<div> Keywords: Time series, Question answering, Human motion, Dataset, Text interactions
Summary:
The article introduces a new dataset called QuAnTS for Question Answering on Time Series data, specifically focusing on human motion in tracked skeleton trajectories. The dataset aims to fill the gap in research on question answering for time series data. Extensive experiments validate the quality and comprehensiveness of QuAnTS. The study evaluates existing and proposed baselines to pave the way for further exploration in Time Series QA using QuAnTS. Human performance is also provided as a benchmark for assessing the practical usability of models. The authors hope to stimulate future research in enhancing interactions with time series models through text, ultimately improving decision-making and transparency in systems.<br /><br />Summary: <div>
arXiv:2511.05124v1 Announce Type: new 
Abstract: Text offers intuitive access to information. This can, in particular, complement the density of numerical time series, thereby allowing improved interactions with time series models to enhance accessibility and decision-making. While the creation of question-answering datasets and models has recently seen remarkable growth, most research focuses on question answering (QA) on vision and text, with time series receiving minute attention. To bridge this gap, we propose a challenging novel time series QA (TSQA) dataset, QuAnTS, for Question Answering on Time Series data. Specifically, we pose a wide variety of questions and answers about human motion in the form of tracked skeleton trajectories. We verify that the large-scale QuAnTS dataset is well-formed and comprehensive through extensive experiments. Thoroughly evaluating existing and newly proposed baselines then lays the groundwork for a deeper exploration of TSQA using QuAnTS. Additionally, we provide human performances as a key reference for gauging the practical usability of such models. We hope to encourage future research on interacting with time series models through text, enabling better decision-making and more transparent systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DL101 Neural Network Outputs and Loss Functions</title>
<link>https://arxiv.org/abs/2511.05131</link>
<guid>https://arxiv.org/abs/2511.05131</guid>
<content:encoded><![CDATA[
<div> activation functions, neural network output layer, loss functions, statistical principles, deep learning model

Summary:<br />
This technical report discusses the relationship between the loss function used to train a neural network and its output layer from a statistical perspective. It examines common activation functions for the output layer, such as linear, sigmoid, ReLU, and softmax, and their respective mathematical properties and suitable use cases. The selection of an appropriate loss function for training a deep learning model is statistically justified, connecting common loss functions like Mean Squared Error (MSE) and Mean Absolute Error (MAE) to Maximum Likelihood Estimation (MLE). The report highlights the link between loss functions and Generalized Linear Models (GLMs) underlying network output layers and considers practical scenarios such as alternative output encodings, constrained outputs, and distributions with heavy tails. <div>
arXiv:2511.05131v1 Announce Type: new 
Abstract: The loss function used to train a neural network is strongly connected to its output layer from a statistical point of view. This technical report analyzes common activation functions for a neural network output layer, like linear, sigmoid, ReLU, and softmax, detailing their mathematical properties and their appropriate use cases. A strong statistical justification exists for the selection of the suitable loss function for training a deep learning model. This report connects common loss functions such as Mean Squared Error (MSE), Mean Absolute Error (MAE), and various Cross-Entropy losses to the statistical principle of Maximum Likelihood Estimation (MLE). Choosing a specific loss function is equivalent to assuming a specific probability distribution for the model output, highlighting the link between these functions and the Generalized Linear Models (GLMs) that underlie network output layers. Additional scenarios of practical interest are also considered, such as alternative output encodings, constrained outputs, and distributions with heavy tails.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consecutive Preferential Bayesian Optimization</title>
<link>https://arxiv.org/abs/2511.05163</link>
<guid>https://arxiv.org/abs/2511.05163</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, preferential optimization, costs, Just-Noticeable Difference threshold, information-theoretic acquisition strategy <br />
Summary: 
Consecutive Preferential Bayesian Optimization is introduced to optimize objectives efficiently with minimal comparative evaluations and account for production and evaluation costs. The method constrains comparisons to involve previously generated candidates, reducing production cost. Incorporating a Just-Noticeable Difference threshold into the probabilistic preference model addresses perceptual ambiguity in feedback, capturing indifference to small utility differences. An information-theoretic acquisition strategy selects new configurations that are most informative under a preference model considering perceptual ambiguity. Empirical results show increased accuracy in setups with high production costs or indifference feedback. <div>
arXiv:2511.05163v1 Announce Type: new 
Abstract: Preferential Bayesian optimization allows optimization of objectives that are either expensive or difficult to measure directly, by relying on a minimal number of comparative evaluations done by a human expert. Generating candidate solutions for evaluation is also often expensive, but this cost is ignored by existing methods. We generalize preference-based optimization to explicitly account for production and evaluation costs with Consecutive Preferential Bayesian Optimization, reducing production cost by constraining comparisons to involve previously generated candidates. We also account for the perceptual ambiguity of the oracle providing the feedback by incorporating a Just-Noticeable Difference threshold into a probabilistic preference model to capture indifference to small utility differences. We adapt an information-theoretic acquisition strategy to this setting, selecting new configurations that are most informative about the unknown optimum under a preference model accounting for the perceptual ambiguity. We empirically demonstrate a notable increase in accuracy in setups with high production costs or with indifference feedback.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide Therapy</title>
<link>https://arxiv.org/abs/2511.05169</link>
<guid>https://arxiv.org/abs/2511.05169</guid>
<content:encoded><![CDATA[
<div> Keywords: Peptide receptor radionuclide therapy, neuroendocrine tumors, predictive modeling, deep learning, multimodal fusion.

Summary:
Peptide receptor radionuclide therapy (PRRT) is commonly used for metastatic neuroendocrine tumors, but predicting treatment outcomes remains challenging. This study aimed to develop predictive models for progression-free survival (PFS) in PRRT-treated patients. Data from 116 patients were analyzed, with laboratory values, imaging results, and deep learning models used to predict PFS. Short PFS (< 1 year) was observed in 36% of patients, associated with higher chromogranin A levels, elevated gamma-GT, and fewer PRRT cycles. A multimodal fusion model incorporating laboratory values, somatostatin receptor PET/CT, and CT imaging demonstrated the best predictive performance, outperforming unimodal approaches. This study highlights the potential of multimodal deep learning models to improve PFS prediction in PRRT-treated patients, paving the way for personalized treatment planning and follow-up strategies based on individual risk profiles.

<br /><br />Summary: <div>
arXiv:2511.05169v1 Announce Type: new 
Abstract: Peptide receptor radionuclide therapy (PRRT) is an established treatment for metastatic neuroendocrine tumors (NETs), yet long-term disease control occurs only in a subset of patients. Predicting progression-free survival (PFS) could support individualized treatment planning. This study evaluates laboratory, imaging, and multimodal deep learning models for PFS prediction in PRRT-treated patients. In this retrospective, single-center study 116 patients with metastatic NETs undergoing 177Lu-DOTATOC were included. Clinical characteristics, laboratory values, and pretherapeutic somatostatin receptor positron emission tomography/computed tomographies (SR-PET/CT) were collected. Seven models were trained to classify low- vs. high-PFS groups, including unimodal (laboratory, SR-PET, or CT) and multimodal fusion approaches. Explainability was evaluated by feature importance analysis and gradient maps. Forty-two patients (36%) had short PFS (< 1 year), 74 patients long PFS (>1 year). Groups were similar in most characteristics, except for higher baseline chromogranin A (p = 0.003), elevated gamma-GT (p = 0.002), and fewer PRRT cycles (p < 0.001) in short-PFS patients. The Random Forest model trained only on laboratory biomarkers reached an AUROC of 0.59 +- 0.02. Unimodal three-dimensional convolutional neural networks using SR-PET or CT performed worse (AUROC 0.42 +- 0.03 and 0.54 +- 0.01, respectively). A multimodal fusion model laboratory values, SR-PET, and CT -augmented with a pretrained CT branch - achieved the best results (AUROC 0.72 +- 0.01, AUPRC 0.80 +- 0.01). Multimodal deep learning combining SR-PET, CT, and laboratory biomarkers outperformed unimodal approaches for PFS prediction after PRRT. Upon external validation, such models may support risk-adapted follow-up strategies.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models</title>
<link>https://arxiv.org/abs/2511.05171</link>
<guid>https://arxiv.org/abs/2511.05171</guid>
<content:encoded><![CDATA[
<div> Keywords: Foundation models, bioacoustics, NatureLM, instruction-following, zero-shot generalization

Summary: 
Foundation models in bioacoustics, such as NatureLM, show promise in generalizing across species and tasks but may lack flexibility in instruction-following. Although NatureLM performs well when prompted for either common or scientific names individually, its accuracy decreases when both are requested in a single prompt. A model merging strategy is proposed to combine NatureLM with its base language model, enhancing instruction-following capabilities while preserving domain expertise. The merged model demonstrates significantly improved zero-shot generalization, surpassing previous benchmarks with a 200% relative improvement and achieving a new state-of-the-art in closed-set zero-shot classification of unseen species. This approach addresses the trade-offs between domain expertise and instruction-following flexibility, showcasing the potential for enhanced performance in bioacoustic applications. 

Summary: <br /><br /> <div>
arXiv:2511.05171v1 Announce Type: new 
Abstract: Foundation models capable of generalizing across species and tasks represent a promising new frontier in bioacoustics, with NatureLM being one of the most prominent examples. While its domain-specific fine-tuning yields strong performance on bioacoustic benchmarks, we observe that it also introduces trade-offs in instruction-following flexibility. For instance, NatureLM achieves high accuracy when prompted for either the common or scientific name individually, but its accuracy drops significantly when both are requested in a single prompt. We address this by applying a simple model merging strategy that interpolates NatureLM with its base language model, recovering instruction-following capabilities with minimal loss of domain expertise. Finally, we show that the merged model exhibits markedly stronger zero-shot generalization, achieving over a 200% relative improvement and setting a new state-of-the-art in closed-set zero-shot classification of unseen species.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Associative Poisoning to Generative Machine Learning</title>
<link>https://arxiv.org/abs/2511.05177</link>
<guid>https://arxiv.org/abs/2511.05177</guid>
<content:encoded><![CDATA[
<div> generative models, data poisoning, associative poisoning, statistical integrity, countermeasure strategy

Summary:
Associative poisoning is a new data poisoning technique targeting generative models like Stable Diffusion and ChatGPT. Unlike existing attacks, it manipulates specific feature associations in generated data without needing control over the training process. This attack subtly alters training data to affect statistical relationships between features in outputs, maintaining high quality while evading detection. Evaluations on state-of-the-art models show its effectiveness in inducing or suppressing feature associations. The attack poses a threat to generative systems used in various applications like image synthesis and natural language processing by compromising their statistical integrity. Existing defenses have limitations in addressing this risk. A novel countermeasure strategy is proposed to mitigate the impact of associative poisoning attacks. <div>
arXiv:2511.05177v1 Announce Type: new 
Abstract: The widespread adoption of generative models such as Stable Diffusion and ChatGPT has made them increasingly attractive targets for malicious exploitation, particularly through data poisoning. Existing poisoning attacks compromising synthesised data typically either cause broad degradation of generated data or require control over the training process, limiting their applicability in real-world scenarios. In this paper, we introduce a novel data poisoning technique called associative poisoning, which compromises fine-grained features of the generated data without requiring control of the training process. This attack perturbs only the training data to manipulate statistical associations between specific feature pairs in the generated outputs. We provide a formal mathematical formulation of the attack and prove its theoretical feasibility and stealthiness. Empirical evaluations using two state-of-the-art generative models demonstrate that associative poisoning effectively induces or suppresses feature associations while preserving the marginal distributions of the targeted features and maintaining high-quality outputs, thereby evading visual detection. These results suggest that generative systems used in image synthesis, synthetic dataset generation, and natural language processing are susceptible to subtle, stealthy manipulations that compromise their statistical integrity. To address this risk, we examine the limitations of existing defensive strategies and propose a novel countermeasure strategy.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs with Graph Neural Networks and Foundation Models</title>
<link>https://arxiv.org/abs/2511.05179</link>
<guid>https://arxiv.org/abs/2511.05179</guid>
<content:encoded><![CDATA[
arXiv:2511.05179v1 Announce Type: new 
Abstract: Modern IoT deployments for environmental sensing produce high volume spatiotemporal data to support downstream tasks such as forecasting, typically powered by machine learning models. While existing filtering and strategic deployment techniques optimize collected data volume at the edge, they overlook how variations in sampling frequencies and spatial coverage affect downstream model performance. In many forecasting models, incorporating data from additional sensors denoise predictions by providing broader spatial contexts. This interplay between sampling frequency, spatial coverage and different forecasting model architectures remain underexplored. This work presents a systematic study of forecasting models - classical models (VAR), neural networks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs), and time series foundation models (TSFMs: Chronos Moirai, TimesFM) under varying spatial sensor nodes density and sampling intervals using real-world temperature data in a wireless sensor network. Our results show that STGNNs are effective when sensor deployments are sparse and sampling rate is moderate, leveraging spatial correlations via encoded graph structure to compensate for limited coverage. In contrast, TSFMs perform competitively at high frequencies but degrade when spatial coverage from neighboring sensors is reduced. Crucially, the multivariate TSFM Moirai outperforms all models by natively learning cross-sensor dependencies. These findings offer actionable insights for building efficient forecasting pipelines in spatio-temporal systems. All code for model configurations, training, dataset, and logs are open-sourced for reproducibility: https://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear Gradient Prediction with Control Variates</title>
<link>https://arxiv.org/abs/2511.05187</link>
<guid>https://arxiv.org/abs/2511.05187</guid>
<content:encoded><![CDATA[
arXiv:2511.05187v1 Announce Type: new 
Abstract: We propose a new way of training neural networks, with the goal of reducing training cost. Our method uses approximate predicted gradients instead of the full gradients that require an expensive backward pass. We derive a control-variate-based technique that ensures our updates are unbiased estimates of the true gradient. Moreover, we propose a novel way to derive a predictor for the gradient inspired by the theory of the Neural Tangent Kernel. We empirically show the efficacy of the technique on a vision transformer classification task.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep Behavior Disorder Screening through Standardized Actigraphy</title>
<link>https://arxiv.org/abs/2511.05221</link>
<guid>https://arxiv.org/abs/2511.05221</guid>
<content:encoded><![CDATA[
arXiv:2511.05221v1 Announce Type: new 
Abstract: Isolated rapid eye movement sleep behavior disorder (iRBD) is a major prodromal marker of $\alpha$-synucleinopathies, often preceding the clinical onset of Parkinson's disease, dementia with Lewy bodies, or multiple system atrophy. While wrist-worn actimeters hold significant potential for detecting RBD in large-scale screening efforts by capturing abnormal nocturnal movements, they become inoperable without a reliable and efficient analysis pipeline. This study presents ActiTect, a fully automated, open-source machine learning tool to identify RBD from actigraphy recordings. To ensure generalizability across heterogeneous acquisition settings, our pipeline includes robust preprocessing and automated sleep-wake detection to harmonize multi-device data and extract physiologically interpretable motion features characterizing activity patterns. Model development was conducted on a cohort of 78 individuals, yielding strong discrimination under nested cross-validation (AUROC = 0.95). Generalization was confirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two independent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To assess real-world robustness, leave-one-dataset-out cross-validation across the internal and external cohorts demonstrated consistent performance (AUROC range = 0.84-0.89). A complementary stability analysis showed that key predictive features remained reproducible across datasets, supporting the final pooled multi-center model as a robust pre-trained resource for broader deployment. By being open-source and easy to use, our tool promotes widespread adoption and facilitates independent validation and collaborative improvements, thereby advancing the field toward a unified and generalizable RBD detection model using wearable devices.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Causal Round Trip: Generating Authentic Counterfactuals by Eliminating Information Loss</title>
<link>https://arxiv.org/abs/2511.05236</link>
<guid>https://arxiv.org/abs/2511.05236</guid>
<content:encoded><![CDATA[
arXiv:2511.05236v1 Announce Type: new 
Abstract: Judea Pearl's vision of Structural Causal Models (SCMs) as engines for counterfactual reasoning hinges on faithful abduction: the precise inference of latent exogenous noise. For decades, operationalizing this step for complex, non-linear mechanisms has remained a significant computational challenge. The advent of diffusion models, powerful universal function approximators, offers a promising solution. However, we argue that their standard design, optimized for perceptual generation over logical inference, introduces a fundamental flaw for this classical problem: an inherent information loss we term the Structural Reconstruction Error (SRE). To address this challenge, we formalize the principle of Causal Information Conservation (CIC) as the necessary condition for faithful abduction. We then introduce BELM-MDCM, the first diffusion-based framework engineered to be causally sound by eliminating SRE by construction through an analytically invertible mechanism. To operationalize this framework, a Targeted Modeling strategy provides structural regularization, while a Hybrid Training Objective instills a strong causal inductive bias. Rigorous experiments demonstrate that our Zero-SRE framework not only achieves state-of-the-art accuracy but, more importantly, enables the high-fidelity, individual-level counterfactuals required for deep causal inquiries. Our work provides a foundational blueprint that reconciles the power of modern generative models with the rigor of classical causal theory, establishing a new and more rigorous standard for this emerging field.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones</title>
<link>https://arxiv.org/abs/2511.05265</link>
<guid>https://arxiv.org/abs/2511.05265</guid>
<content:encoded><![CDATA[
arXiv:2511.05265v1 Announce Type: new 
Abstract: The emergence of truck-drone collaborative systems in last-mile logistics has positioned the Traveling Salesman Problem with Drones (TSP-D) as a pivotal extension of classical routing optimization, where synchronized vehicle coordination promises substantial operational efficiency and reduced environmental impact, yet introduces NP-hard combinatorial complexity beyond the reach of conventional optimization paradigms. Deep reinforcement learning offers a theoretically grounded framework to address TSP-D's inherent challenges through self-supervised policy learning and adaptive decision-making. This study proposes a hierarchical Actor-Critic deep reinforcement learning framework for solving the TSP-D problem. The architecture consists of two primary components: a Transformer-inspired encoder and an efficient Minimal Gated Unit decoder. The encoder incorporates a novel, optimized k-nearest neighbors sparse attention mechanism specifically for focusing on relevant spatial relationships, further enhanced by the integration of global node features. The Minimal Gated Unit decoder processes these encoded representations to efficiently generate solution sequences. The entire framework operates within an asynchronous advantage actor-critic paradigm. Experimental results show that, on benchmark TSP-D instances of various scales (N=10 to 100), the proposed model can obtain competitive or even superior solutions in shorter average computation times compared to high-performance heuristic algorithms and existing reinforcement learning methods. Moreover, compared to advanced reinforcement learning algorithm benchmarks, the proposed framework significantly reduces the total training time required while achieving superior final performance, highlighting its notable advantage in training efficiency.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Score-Based Diffusion Models with Machine Learning-Enhanced Localization for Advanced Data Assimilation in Geological Carbon Storage</title>
<link>https://arxiv.org/abs/2511.05266</link>
<guid>https://arxiv.org/abs/2511.05266</guid>
<content:encoded><![CDATA[
arXiv:2511.05266v1 Announce Type: new 
Abstract: Accurate characterization of subsurface heterogeneity is important for the safe and effective implementation of geological carbon storage (GCS) projects. This paper explores how machine learning methods can enhance data assimilation for GCS with a framework that integrates score-based diffusion models with machine learning-enhanced localization in channelized reservoirs during CO$_2$ injection. We employ a machine learning-enhanced localization framework that uses large ensembles ($N_s = 5000$) with permeabilities generated by the diffusion model and states computed by simple ML algorithms to improve covariance estimation for the Ensemble Smoother with Multiple Data Assimilation (ESMDA). We apply ML algorithms to a prior ensemble of channelized permeability fields, generated with the geostatistical model FLUVSIM. Our approach is applied on a CO$_2$ injection scenario simulated using the Delft Advanced Research Terra Simulator (DARTS). Our ML-based localization maintains significantly more ensemble variance than when localization is not applied, while achieving comparable data-matching quality. This framework has practical implications for GCS projects, helping improve the reliability of uncertainty quantification for risk assessment.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding-Space Data Augmentation to Prevent Membership Inference Attacks in Clinical Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.05289</link>
<guid>https://arxiv.org/abs/2511.05289</guid>
<content:encoded><![CDATA[
arXiv:2511.05289v1 Announce Type: new 
Abstract: Balancing strong privacy guarantees with high predictive performance is critical for time series forecasting (TSF) tasks involving Electronic Health Records (EHR). In this study, we explore how data augmentation can mitigate Membership Inference Attacks (MIA) on TSF models. We show that retraining with synthetic data can substantially reduce the effectiveness of loss-based MIAs by reducing the attacker's true-positive to false-positive ratio. The key challenge is generating synthetic samples that closely resemble the original training data to confuse the attacker, while also introducing enough novelty to enhance the model's ability to generalize to unseen data. We examine multiple augmentation strategies - Zeroth-Order Optimization (ZOO), a variant of ZOO constrained by Principal Component Analysis (ZOO-PCA), and MixUp - to strengthen model resilience without sacrificing accuracy. Our experimental results show that ZOO-PCA yields the best reductions in TPR/FPR ratio for MIA attacks without sacrificing performance on test data.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention and Compression is all you need for Controllably Efficient Language Models</title>
<link>https://arxiv.org/abs/2511.05313</link>
<guid>https://arxiv.org/abs/2511.05313</guid>
<content:encoded><![CDATA[
arXiv:2511.05313v1 Announce Type: new 
Abstract: The quadratic cost of attention in transformers motivated the development of efficient approaches: namely sparse and sliding window attention, convolutions and linear attention. Although these approaches result in impressive reductions in compute and memory, they often trade-off with quality, specifically in-context recall performance. Moreover, apriori fixing this quality-compute tradeoff means being suboptimal from the get-go: some downstream applications require more memory for in-context recall, while others require lower latency and memory. Further, these approaches rely on heuristic choices that artificially restrict attention, or require handcrafted and complex recurrent state update rules, or they must be carefully composed with attention at specific layers to form a hybrid architecture that complicates the design process, especially at scale. To address above issues, we propose Compress & Attend Transformer (CAT), a conceptually simple architecture employing two simple ingredients only: dense attention and compression. CAT decodes chunks of tokens by attending to compressed chunks of the sequence so far. Compression results in decoding from a reduced sequence length that yields compute and memory savings, while choosing a particular chunk size trades-off quality for efficiency. Moreover, CAT can be trained with multiple chunk sizes at once, unlocking control of quality-compute trade-offs directly at test-time without any retraining, all in a single adaptive architecture. In exhaustive evaluations on common language modeling tasks, in-context recall, and long-context understanding, a single adaptive CAT model outperforms existing efficient baselines, including hybrid architectures, across different compute-memory budgets. Further, a single CAT matches dense transformer in language modeling across model scales while being 1.4-3x faster and requiring 2-9x lower total memory usage.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval</title>
<link>https://arxiv.org/abs/2511.05325</link>
<guid>https://arxiv.org/abs/2511.05325</guid>
<content:encoded><![CDATA[
arXiv:2511.05325v1 Announce Type: new 
Abstract: Multimodal product retrieval systems in e-commerce platforms rely on effectively combining visual and textual signals to improve search relevance and user experience. However, vision-language models such as CLIP are vulnerable to typographic attacks, where misleading or irrelevant text embedded in images skews model predictions. In this work, we propose a novel method that reverses the logic of typographic attacks by rendering relevant textual content (e.g., titles, descriptions) directly onto product images to perform vision-text compression, thereby strengthening image-text alignment and boosting multimodal product retrieval performance. We evaluate our method on three vertical-specific e-commerce datasets (sneakers, handbags, and trading cards) using six state-of-the-art vision foundation models. Our experiments demonstrate consistent improvements in unimodal and multimodal retrieval accuracy across categories and model families. Our findings suggest that visually rendering product metadata is a simple yet effective enhancement for zero-shot multimodal retrieval in e-commerce applications.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Dynamics from Input-Output Data with Hamiltonian Gaussian Processes</title>
<link>https://arxiv.org/abs/2511.05330</link>
<guid>https://arxiv.org/abs/2511.05330</guid>
<content:encoded><![CDATA[
arXiv:2511.05330v1 Announce Type: new 
Abstract: Embedding non-restrictive prior knowledge, such as energy conservation laws, in learning-based approaches is a key motive to construct physically consistent models from limited data, relevant for, e.g., model-based control. Recent work incorporates Hamiltonian dynamics into Gaussian Process (GP) regression to obtain uncertainty-quantifying models that adhere to the underlying physical principles. However, these works rely on velocity or momentum data, which is rarely available in practice. In this paper, we consider dynamics learning with non-conservative Hamiltonian GPs, and address the more realistic problem setting of learning from input-output data. We provide a fully Bayesian scheme for estimating probability densities of unknown hidden states, of GP hyperparameters, as well as of structural hyperparameters, such as damping coefficients. Considering the computational complexity of GPs, we take advantage of a reduced-rank GP approximation and leverage its properties for computationally efficient prediction and training. The proposed method is evaluated in a nonlinear simulation case study and compared to a state-of-the-art approach that relies on momentum measurements.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning</title>
<link>https://arxiv.org/abs/2511.05355</link>
<guid>https://arxiv.org/abs/2511.05355</guid>
<content:encoded><![CDATA[
arXiv:2511.05355v1 Announce Type: new 
Abstract: Flow matching (FM) has shown promising results in data-driven planning. However, it inherently lacks formal guarantees for ensuring state and action constraints, whose satisfaction is a fundamental and crucial requirement for the safety and admissibility of planned trajectories on various systems. Moreover, existing FM planners do not ensure the dynamical consistency, which potentially renders trajectories inexecutable. We address these shortcomings by proposing SAD-Flower, a novel framework for generating Safe, Admissible, and Dynamically consistent trajectories. Our approach relies on an augmentation of the flow with a virtual control input. Thereby, principled guidance can be derived using techniques from nonlinear control theory, providing formal guarantees for state constraints, action constraints, and dynamic consistency. Crucially, SAD-Flower operates without retraining, enabling test-time satisfaction of unseen constraints. Through extensive experiments across several tasks, we demonstrate that SAD-Flower outperforms various generative-model-based baselines in ensuring constraint satisfaction.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-Based Electromagnetic Inverse Design of Scattering Structured Media</title>
<link>https://arxiv.org/abs/2511.05357</link>
<guid>https://arxiv.org/abs/2511.05357</guid>
<content:encoded><![CDATA[
arXiv:2511.05357v1 Announce Type: new 
Abstract: We present a conditional diffusion model for electromagnetic inverse design that generates structured media geometries directly from target differential scattering cross-section profiles, bypassing expensive iterative optimization. Our 1D U-Net architecture with Feature-wise Linear Modulation learns to map desired angular scattering patterns to 2x2 dielectric sphere structure, naturally handling the non-uniqueness of inverse problems by sampling diverse valid designs. Trained on 11,000 simulated metasurfaces, the model achieves median MPE below 19% on unseen targets (best: 1.39%), outperforming CMA-ES evolutionary optimization while reducing design time from hours to seconds. These results demonstrate that employing diffusion models is promising for advancing electromagnetic inverse design research, potentially enabling rapid exploration of complex metasurface architectures and accelerating the development of next-generation photonic and wireless communication systems. The code is publicly available at https://github.com/mikzuker/inverse_design_metasurface_generation.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction</title>
<link>https://arxiv.org/abs/2511.05396</link>
<guid>https://arxiv.org/abs/2511.05396</guid>
<content:encoded><![CDATA[
arXiv:2511.05396v1 Announce Type: new 
Abstract: Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProDER: A Continual Learning Approach for Fault Prediction in Evolving Smart Grids</title>
<link>https://arxiv.org/abs/2511.05420</link>
<guid>https://arxiv.org/abs/2511.05420</guid>
<content:encoded><![CDATA[
arXiv:2511.05420v1 Announce Type: new 
Abstract: As smart grids evolve to meet growing energy demands and modern operational challenges, the ability to accurately predict faults becomes increasingly critical. However, existing AI-based fault prediction models struggle to ensure reliability in evolving environments where they are required to adapt to new fault types and operational zones. In this paper, we propose a continual learning (CL) framework in the smart grid context to evolve the model together with the environment. We design four realistic evaluation scenarios grounded in class-incremental and domain-incremental learning to emulate evolving grid conditions. We further introduce Prototype-based Dark Experience Replay (ProDER), a unified replay-based approach that integrates prototype-based feature regularization, logit distillation, and a prototype-guided replay memory. ProDER achieves the best performance among tested CL techniques, with only a 0.045 accuracy drop for fault type prediction and 0.015 for fault zone prediction. These results demonstrate the practicality of CL for scalable, real-world fault prediction in smart grids.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>APP: Accelerated Path Patching with Task-Specific Pruning</title>
<link>https://arxiv.org/abs/2511.05442</link>
<guid>https://arxiv.org/abs/2511.05442</guid>
<content:encoded><![CDATA[
arXiv:2511.05442v1 Announce Type: new 
Abstract: Circuit discovery is a key step in many mechanistic interpretability pipelines. Current methods, such as Path Patching, are computationally expensive and have limited in-depth circuit analysis for smaller models. In this study, we propose Accelerated Path Patching (APP), a hybrid approach leveraging our novel contrastive attention head pruning method to drastically reduce the search space of circuit discovery methods. Our Contrastive-FLAP pruning algorithm uses techniques from causal mediation analysis to assign higher pruning scores to task-specific attention heads, leading to higher performing sparse models compared to traditional pruning techniques. Although Contrastive-FLAP is successful at preserving task-specific heads that existing pruning algorithms remove at low sparsity ratios, the circuits found by Contrastive-FLAP alone are too large to satisfy the minimality constraint required in circuit analysis. APP first applies Contrastive-FLAP to reduce the search space on required for circuit discovery algorithms by, on average, 56\%. Next, APP, applies traditional Path Patching on the remaining attention heads, leading to a speed up of 59.63\%-93.27\% compared to Path Patching applied to the dense model. Despite the substantial computational saving that APP provides, circuits obtained from APP exhibit substantial overlap and similar performance to previously established Path Patching circuits
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarially Robust Multitask Adaptive Control</title>
<link>https://arxiv.org/abs/2511.05444</link>
<guid>https://arxiv.org/abs/2511.05444</guid>
<content:encoded><![CDATA[
arXiv:2511.05444v1 Announce Type: new 
Abstract: We study adversarially robust multitask adaptive linear quadratic control; a setting where multiple systems collaboratively learn control policies under model uncertainty and adversarial corruption. We propose a clustered multitask approach that integrates clustering and system identification with resilient aggregation to mitigate corrupted model updates. Our analysis characterizes how clustering accuracy, intra-cluster heterogeneity, and adversarial behavior affect the expected regret of certainty-equivalent (CE) control across LQR tasks. We establish non-asymptotic bounds demonstrating that the regret decreases inversely with the number of honest systems per cluster and that this reduction is preserved under a bounded fraction of adversarial systems within each cluster.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter-Efficient Conditioning for Material Generalization in Graph-Based Simulators</title>
<link>https://arxiv.org/abs/2511.05456</link>
<guid>https://arxiv.org/abs/2511.05456</guid>
<content:encoded><![CDATA[
arXiv:2511.05456v1 Announce Type: new 
Abstract: Graph network-based simulators (GNS) have demonstrated strong potential for learning particle-based physics (such as fluids, deformable solids, and granular flows) while generalizing to unseen geometries due to their inherent inductive biases. However, existing models are typically trained for a single material type and fail to generalize across distinct constitutive behaviors, limiting their applicability in real-world engineering settings. Using granular flows as a running example, we propose a parameter-efficient conditioning mechanism that makes the GNS model adaptive to material parameters. We identify that sensitivity to material properties is concentrated in the early message-passing (MP) layers, a finding we link to the local nature of constitutive models (e.g., Mohr-Coulomb) and their effects on information propagation. We empirically validate this by showing that fine-tuning only the first few (1-5) of 10 MP layers of a pretrained model achieves comparable test performance as compared to fine-tuning the entire network. Building on this insight, we propose a parameter-efficient Feature-wise Linear Modulation (FiLM) conditioning mechanism designed to specifically target these early layers. This approach produces accurate long-term rollouts on unseen, interpolated, or moderately extrapolated values (e.g., up to 2.5 degrees for friction angle and 0.25 kPa for cohesion) when trained exclusively on as few as 12 short simulation trajectories from new materials, representing a 5-fold data reduction compared to a baseline multi-task learning method. Finally, we validate the model's utility by applying it to an inverse problem, successfully identifying unknown cohesion parameters from trajectory data. This approach enables the use of GNS in inverse design and closed-loop control tasks where material properties are treated as design variables.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models</title>
<link>https://arxiv.org/abs/2511.05460</link>
<guid>https://arxiv.org/abs/2511.05460</guid>
<content:encoded><![CDATA[
arXiv:2511.05460v1 Announce Type: new 
Abstract: Pre-trained Time Series Foundational Models (TSFMs) represent a significant advance, capable of forecasting diverse time series with complex characteristics, including varied seasonalities, trends, and long-range dependencies. Despite their primary goal of universal time series forecasting, their efficacy is far from uniform; divergent training protocols and data sources cause individual TSFMs to exhibit highly variable performance across different forecasting tasks, domains, and horizons. Leveraging this complementary expertise by arbitrating existing TSFM outputs presents a compelling strategy, yet this remains a largely unexplored area of research. In this paper, we conduct a thorough examination of how different TSFMs exhibit specialized performance profiles across various forecasting settings, and how we can effectively leverage this behavior in arbitration between different time series models. We specifically analyze how factors such as model selection and forecast horizon distribution can influence the efficacy of arbitration strategies. Based on this analysis, we propose Synapse, a novel arbitration framework for TSFMs. Synapse is designed to dynamically leverage a pool of TSFMs, assign and adjust predictive weights based on their relative, context-dependent performance, and construct a robust forecast distribution by adaptively sampling from the output quantiles of constituent models. Experimental results demonstrate that Synapse consistently outperforms other popular ensembling techniques as well as individual TSFMs, demonstrating Synapse's efficacy in time series forecasting.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning</title>
<link>https://arxiv.org/abs/2511.05462</link>
<guid>https://arxiv.org/abs/2511.05462</guid>
<content:encoded><![CDATA[
arXiv:2511.05462v1 Announce Type: new 
Abstract: Recent studies have demonstrated the effectiveness of clustering-based approaches for self-supervised and unsupervised learning. However, the application of clustering is often heuristic, and the optimal methodology remains unclear. In this work, we establish connections between these unsupervised clustering methods and classical mixture models from statistics. Through this framework, we demonstrate significant enhancements to these clustering methods, leading to the development of a novel model named SiamMM. Our method attains state-of-the-art performance across various self-supervised learning benchmarks. Inspection of the learned clusters reveals a strong resemblance to unseen ground truth labels, uncovering potential instances of mislabeling.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Precipitation nowcasting of satellite data using physically conditioned neural networks</title>
<link>https://arxiv.org/abs/2511.05471</link>
<guid>https://arxiv.org/abs/2511.05471</guid>
<content:encoded><![CDATA[
arXiv:2511.05471v1 Announce Type: new 
Abstract: Accurate short-term precipitation forecasts predominantly rely on dense weather-radar networks, limiting operational value in places most exposed to climate extremes. We present TUPANN (Transferable and Universal Physics-Aligned Nowcasting Network), a satellite-only model trained on GOES-16 RRQPE. Unlike most deep learning models for nowcasting, TUPANN decomposes the forecast into physically meaningful components: a variational encoder-decoder infers motion and intensity fields from recent imagery under optical-flow supervision, a lead-time-conditioned MaxViT evolves the latent state, and a differentiable advection operator reconstructs future frames. We evaluate TUPANN on both GOES-16 and IMERG data, in up to four distinct climates (Rio de Janeiro, Manaus, Miami, La Paz) at 10-180min lead times using the CSI and HSS metrics over 4-64 mm/h thresholds. Comparisons against optical-flow, deep learning and hybrid baselines show that TUPANN achieves the best or second-best skill in most settings, with pronounced gains at higher thresholds. Training on multiple cities further improves performance, while cross-city experiments show modest degradation and occasional gains for rare heavy-rain regimes. The model produces smooth, interpretable motion fields aligned with numerical optical flow and runs in near real time due to the low latency of GOES-16. These results indicate that physically aligned learning can provide nowcasts that are skillful, transferable and global.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Flow Matching KL Divergence</title>
<link>https://arxiv.org/abs/2511.05480</link>
<guid>https://arxiv.org/abs/2511.05480</guid>
<content:encoded><![CDATA[
arXiv:2511.05480v1 Announce Type: new 
Abstract: We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler (KL) divergence of the flow-matching distribution approximation. In particular, if the $L_2$ flow-matching loss is bounded by $\epsilon^2 > 0$, then the KL divergence between the true data distribution and the estimated distribution is bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$ depend only on the regularities of the data and velocity fields. Consequently, this bound implies statistical convergence rates of Flow Matching Transformers under the Total Variation (TV) distance. We show that, flow matching achieves nearly minimax-optimal efficiency in estimating smooth distributions. Our results make the statistical efficiency of flow matching comparable to that of diffusion models under the TV distance. Numerical studies on synthetic and learned velocities corroborate our theory.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoilX: Calibration-Free Comprehensive Soil Sensing Through Contrastive Cross-Component Learning</title>
<link>https://arxiv.org/abs/2511.05482</link>
<guid>https://arxiv.org/abs/2511.05482</guid>
<content:encoded><![CDATA[
arXiv:2511.05482v1 Announce Type: new 
Abstract: Precision agriculture demands continuous and accurate monitoring of soil moisture (M) and key macronutrients, including nitrogen (N), phosphorus (P), and potassium (K), to optimize yields and conserve resources. Wireless soil sensing has been explored to measure these four components; however, current solutions require recalibration (i.e., retraining the data processing model) to handle variations in soil texture, characterized by aluminosilicates (Al) and organic carbon (C), limiting their practicality. To address this, we introduce SoilX, a calibration-free soil sensing system that jointly measures six key components: {M, N, P, K, C, Al}. By explicitly modeling C and Al, SoilX eliminates texture- and carbon-dependent recalibration. SoilX incorporates Contrastive Cross-Component Learning (3CL), with two customized terms: the Orthogonality Regularizer and the Separation Loss, to effectively disentangle cross-component interference. Additionally, we design a novel tetrahedral antenna array with an antenna-switching mechanism, which can robustly measure soil dielectric permittivity independent of device placement. Extensive experiments demonstrate that SoilX reduces estimation errors by 23.8% to 31.5% over baselines and generalizes well to unseen fields.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction</title>
<link>https://arxiv.org/abs/2511.05483</link>
<guid>https://arxiv.org/abs/2511.05483</guid>
<content:encoded><![CDATA[
arXiv:2511.05483v1 Announce Type: new 
Abstract: Predicting the effect of amino acid mutations on enzyme thermodynamic stability (DDG) is fundamental to protein engineering and drug design. While recent deep learning approaches have shown promise, they often process sequence and structure information independently, failing to capture the intricate coupling between local structural geometry and global sequential patterns. We present DGTN (Diffused Graph-Transformer Network), a novel architecture that co-learns graph neural network (GNN) weights for structural priors and transformer attention through a diffusion mechanism. Our key innovation is a bidirectional diffusion process where: (1) GNN-derived structural embeddings guide transformer attention via learnable diffusion kernels, and (2) transformer representations refine GNN message passing through attention-modulated graph updates. We provide rigorous mathematical analysis showing this co-learning scheme achieves provably better approximation bounds than independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves state-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with 6.2% improvement over best baselines. Ablation studies confirm the diffusion mechanism contributes 4.8 points to correlation. Our theoretical analysis proves the diffused attention converges to optimal structure-sequence coupling, with convergence rate O(1/sqrt(T) ) where T is diffusion steps. This work establishes a principled framework for integrating heterogeneous protein representations through learnable diffusion.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization in Representation Models via Random Matrix Theory: Application to Recurrent Networks</title>
<link>https://arxiv.org/abs/2511.02401</link>
<guid>https://arxiv.org/abs/2511.02401</guid>
<content:encoded><![CDATA[
arXiv:2511.02401v1 Announce Type: cross 
Abstract: We first study the generalization error of models that use a fixed feature representation (frozen intermediate layers) followed by a trainable readout layer. This setting encompasses a range of architectures, from deep random-feature models to echo-state networks (ESNs) with recurrent dynamics. Working in the high-dimensional regime, we apply Random Matrix Theory to derive a closed-form expression for the asymptotic generalization error. We then apply this analysis to recurrent representations and obtain concise formula that characterize their performance. Surprisingly, we show that a linear ESN is equivalent to ridge regression with an exponentially time-weighted (''memory'') input covariance, revealing a clear inductive bias toward recent inputs. Experiments match predictions: ESNs win in low-sample, short-memory regimes, while ridge prevails with more data or long-range dependencies. Our methodology provides a general framework for analyzing overparameterized models and offers insights into the behavior of deep learning networks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLMs' Reasoning Over Ordered Procedural Steps</title>
<link>https://arxiv.org/abs/2511.04688</link>
<guid>https://arxiv.org/abs/2511.04688</guid>
<content:encoded><![CDATA[
arXiv:2511.04688v1 Announce Type: cross 
Abstract: Reasoning over procedural sequences, where the order of steps directly impacts outcomes, is a critical capability for large language models (LLMs). In this work, we study the task of reconstructing globally ordered sequences from shuffled procedural steps, using a curated dataset of food recipes, a domain where correct sequencing is essential for task success. We evaluate several LLMs under zero-shot and few-shot settings and present a comprehensive evaluation framework that adapts established metrics from ranking and sequence alignment. These include Kendall's Tau, Normalized Longest Common Subsequence (NLCS), and Normalized Edit Distance (NED), which capture complementary aspects of ordering quality. Our analysis shows that model performance declines with increasing sequence length, reflecting the added complexity of longer procedures. We also find that greater step displacement in the input, corresponding to more severe shuffling, leads to further degradation. These findings highlight the limitations of current LLMs in procedural reasoning, especially with longer and more disordered inputs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Communication-Constrained Private Decentralized Online Personalized Mean Estimation</title>
<link>https://arxiv.org/abs/2511.04702</link>
<guid>https://arxiv.org/abs/2511.04702</guid>
<content:encoded><![CDATA[
arXiv:2511.04702v1 Announce Type: cross 
Abstract: We consider the problem of communication-constrained collaborative personalized mean estimation under a privacy constraint in an environment of several agents continuously receiving data according to arbitrary unknown agent-specific distributions. A consensus-based algorithm is studied under the framework of differential privacy in order to protect each agent's data. We give a theoretical convergence analysis of the proposed consensus-based algorithm for any bounded unknown distributions on the agents' data, showing that collaboration provides faster convergence than a fully local approach where agents do not share data, under an oracle decision rule and under some restrictions on the privacy level and the agents' connectivity, which illustrates the benefit of private collaboration in an online setting under a communication restriction on the agents. The theoretical faster-than-local convergence guarantee is backed up by several numerical results.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jailbreaking in the Haystack</title>
<link>https://arxiv.org/abs/2511.04707</link>
<guid>https://arxiv.org/abs/2511.04707</guid>
<content:encoded><![CDATA[
arXiv:2511.04707v1 Announce Type: cross 
Abstract: Recent advances in long-context language models (LMs) have enabled million-token inputs, expanding their capabilities across complex tasks like computer-use agents. Yet, the safety implications of these extended contexts remain unclear. To bridge this gap, we introduce NINJA (short for Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by appending benign, model-generated content to harmful user goals. Critical to our method is the observation that the position of harmful goals play an important role in safety. Experiments on standard safety benchmark, HarmBench, show that NINJA significantly increases attack success rates across state-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral, and Gemini. Unlike prior jailbreaking methods, our approach is low-resource, transferable, and less detectable. Moreover, we show that NINJA is compute-optimal -- under a fixed compute budget, increasing context length can outperform increasing the number of trials in best-of-N jailbreak. These findings reveal that even benign long contexts -- when crafted with careful goal positioning -- introduce fundamental vulnerabilities in modern LMs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking</title>
<link>https://arxiv.org/abs/2511.04711</link>
<guid>https://arxiv.org/abs/2511.04711</guid>
<content:encoded><![CDATA[
arXiv:2511.04711v1 Announce Type: cross 
Abstract: Large-scale vision-language models, especially CLIP, have demonstrated remarkable performance across diverse downstream tasks. Soft prompts, as carefully crafted modules that efficiently adapt vision-language models to specific tasks, necessitate effective copyright protection. In this paper, we investigate model copyright protection by auditing whether suspicious third-party models incorporate protected soft prompts. While this can be viewed as a special case of model ownership auditing, our analysis shows that existing techniques are ineffective due to prompt learning's unique characteristics. Non-intrusive auditing is inherently prone to false positives when independent models share similar data distributions with victim models. Intrusive approaches also fail: backdoor methods designed for CLIP cannot embed functional triggers, while extending traditional DNN backdoor techniques to prompt learning suffers from harmfulness and ambiguity challenges. We find that these failures in intrusive auditing stem from the same fundamental reason: watermarking operates within the same decision space as the primary task yet pursues opposing objectives. Motivated by these findings, we propose sequential watermarking for soft prompts (SWAP), which implants watermarks into a different and more complex space. SWAP encodes watermarks through a specific order of defender-specified out-of-distribution classes, inspired by the zero-shot prediction capability of CLIP. This watermark, which is embedded in a more complex space, keeps the original prediction label unchanged, making it less opposed to the primary task. We further design a hypothesis-test-guided verification protocol for SWAP and provide theoretical analyses of success conditions. Extensive experiments on 11 datasets demonstrate SWAP's effectiveness, harmlessness, and robustness against potential adaptive attacks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation</title>
<link>https://arxiv.org/abs/2511.04715</link>
<guid>https://arxiv.org/abs/2511.04715</guid>
<content:encoded><![CDATA[
arXiv:2511.04715v1 Announce Type: cross 
Abstract: Identifying how training samples influence/impact Large Language Model (LLM) decision-making is essential for effectively interpreting model decisions and auditing large-scale datasets. Current training sample influence estimation methods (also known as influence functions) undertake this goal by utilizing information flow through the model via its first-order and higher-order gradient terms. However, owing to the large model sizes of today consisting of billions of parameters, these influence computations are often restricted to some subset of model layers to ensure computational feasibility. Prior seminal work by Yeh et al. (2022) in assessing which layers are best suited for computing language data influence concluded that the first (embedding) layers are the most informative for this purpose, using a hypothesis based on influence scores canceling out (i.e., the cancellation effect). In this work, we propose theoretical and empirical evidence demonstrating how the cancellation effect is unreliable, and that middle attention layers are better estimators for influence. Furthermore, we address the broader challenge of aggregating influence scores across layers, and showcase how alternatives to standard averaging (such as ranking and vote-based methods) can lead to significantly improved performance. Finally, we propose better methods for evaluating influence score efficacy in LLMs without undertaking model retraining, and propose a new metric known as the Noise Detection Rate (NDR) that exhibits strong predictive capability compared to the cancellation effect. Through extensive experiments across LLMs of varying types and scales, we concretely determine that the first (layers) are not necessarily better than the last (layers) for LLM influence estimation, contrasting with prior knowledge in the field.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs</title>
<link>https://arxiv.org/abs/2511.04727</link>
<guid>https://arxiv.org/abs/2511.04727</guid>
<content:encoded><![CDATA[
arXiv:2511.04727v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have demonstrated impressive generalization across multimodal tasks, yet most evaluation benchmarks remain Western-centric, leaving open questions about their performance in culturally diverse and multilingual settings. To address this gap, we introduce IndicVisionBench, the first large-scale benchmark centered on the Indian subcontinent. Covering English and 10 Indian languages, our benchmark spans 3 multimodal tasks, including Optical Character Recognition (OCR), Multimodal Machine Translation (MMT), and Visual Question Answering (VQA), covering 6 kinds of question types. Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across 13 culturally grounded topics. In addition, we release a paired parallel corpus of annotations across 10 Indic languages, creating a unique resource for analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum of 8 models, from proprietary closed-source systems to open-weights medium and large-scale models. Our experiments reveal substantial performance gaps, underscoring the limitations of current VLMs in culturally diverse contexts. By centering cultural diversity and multilinguality, IndicVisionBench establishes a reproducible evaluation framework that paves the way for more inclusive multimodal research.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CPO: Condition Preference Optimization for Controllable Image Generation</title>
<link>https://arxiv.org/abs/2511.04753</link>
<guid>https://arxiv.org/abs/2511.04753</guid>
<content:encoded><![CDATA[
arXiv:2511.04753v1 Announce Type: cross 
Abstract: To enhance controllability in text-to-image generation, ControlNet introduces image-based control signals, while ControlNet++ improves pixel-level cycle consistency between generated images and the input control signal. To avoid the prohibitive cost of back-propagating through the sampling process, ControlNet++ optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step approximation, which not only ignores the contribution of high-noise timesteps but also introduces additional approximation errors. A straightforward alternative for optimizing controllability across all timesteps is Direct Preference Optimization (DPO), a fine-tuning method that increases model preference for more controllable images ($I^{w}$) over less controllable ones ($I^{l}$). However, due to uncertainty in generative models, it is difficult to ensure that win--lose image pairs differ only in controllability while keeping other factors, such as image quality, fixed. To address this, we propose performing preference learning over control conditions rather than generated images. Specifically, we construct winning and losing control signals, $\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer $\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference Optimization} (CPO), eliminates confounding factors and yields a low-variance training objective. Our approach theoretically exhibits lower contrastive loss variance than DPO and empirically achieves superior results. Moreover, CPO requires less computation and storage for dataset curation. Extensive experiments show that CPO significantly improves controllability over the state-of-the-art ControlNet++ across multiple control types: over $10\%$ error rate reduction in segmentation, $70$--$80\%$ in human pose, and consistent $2$--$5\%$ reductions in edge and depth maps.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning-Driven Analysis of kSZ Maps to Predict CMB Optical Depth $\tau$</title>
<link>https://arxiv.org/abs/2511.04770</link>
<guid>https://arxiv.org/abs/2511.04770</guid>
<content:encoded><![CDATA[
arXiv:2511.04770v1 Announce Type: cross 
Abstract: Upcoming measurements of the kinetic Sunyaev-Zel'dovich (kSZ) effect, which results from Cosmic Microwave Background (CMB) photons scattering off moving electrons, offer a powerful probe of the Epoch of Reionization (EoR). The kSZ signal contains key information about the timing, duration, and spatial structure of the EoR. A precise measurement of the CMB optical depth $\tau$, a key parameter that characterizes the universe's integrated electron density, would significantly constrain models of early structure formation. However, the weak kSZ signal is difficult to extract from CMB observations due to significant contamination from astrophysical foregrounds. We present a machine learning approach to extract $\tau$ from simulated kSZ maps. We train advanced machine learning models, including swin transformers, on high-resolution seminumeric simulations of the kSZ signal. To robustly quantify prediction uncertainties of $\tau$, we employ the Laplace Approximation (LA). This approach provides an efficient and principled Gaussian approximation to the posterior distribution over the model's weights, allowing for reliable error estimation. We investigate and compare two distinct application modes: a post-hoc LA applied to a pre-trained model, and an online LA where model weights and hyperparameters are optimized jointly by maximizing the marginal likelihood. This approach provides a framework for robustly constraining $\tau$ and its associated uncertainty, which can enhance the analysis of upcoming CMB surveys like the Simons Observatory and CMB-S4.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blind Strong Gravitational Lensing Inversion: Joint Inference of Source and Lens Mass with Score-Based Models</title>
<link>https://arxiv.org/abs/2511.04792</link>
<guid>https://arxiv.org/abs/2511.04792</guid>
<content:encoded><![CDATA[
arXiv:2511.04792v1 Announce Type: cross 
Abstract: Score-based models can serve as expressive, data-driven priors for scientific inverse problems. In strong gravitational lensing, they enable posterior inference of a background galaxy from its distorted, multiply-imaged observation. Previous work, however, assumes that the lens mass distribution (and thus the forward operator) is known. We relax this assumption by jointly inferring the source and a parametric lens-mass profile, using a sampler based on GibbsDDRM but operating in continuous time. The resulting reconstructions yield residuals consistent with the observational noise, and the marginal posteriors of the lens parameters recover true values without systematic bias. To our knowledge, this is the first successful demonstration of joint source-and-lens inference with a score-based prior.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MDM: Manhattan Distance Mapping of DNN Weights for Parasitic-Resistance-Resilient Memristive Crossbars</title>
<link>https://arxiv.org/abs/2511.04798</link>
<guid>https://arxiv.org/abs/2511.04798</guid>
<content:encoded><![CDATA[
arXiv:2511.04798v1 Announce Type: cross 
Abstract: Manhattan Distance Mapping (MDM) is a post-training deep neural network (DNN) weight mapping technique for memristive bit-sliced compute-in-memory (CIM) crossbars that reduces parasitic resistance (PR) nonidealities.
  PR limits crossbar efficiency by mapping DNN matrices into small crossbar tiles, reducing CIM-based speedup. Each crossbar executes one tile, requiring digital synchronization before the next layer. At this granularity, designers either deploy many small crossbars in parallel or reuse a few sequentially-both increasing analog-to-digital conversions, latency, I/O pressure, and chip area.
  MDM alleviates PR effects by optimizing active-memristor placement. Exploiting bit-level structured sparsity, it feeds activations from the denser low-order side and reorders rows according to the Manhattan distance, relocating active cells toward regions less affected by PR and thus lowering the nonideality factor (NF).
  Applied to DNN models on ImageNet-1k, MDM reduces NF by up to 46% and improves accuracy under analog distortion by an average of 3.6% in ResNets. Overall, it provides a lightweight, spatially informed method for scaling CIM DNN accelerators.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose</title>
<link>https://arxiv.org/abs/2511.04803</link>
<guid>https://arxiv.org/abs/2511.04803</guid>
<content:encoded><![CDATA[
arXiv:2511.04803v1 Announce Type: cross 
Abstract: Generalist biomedical image segmentation models such as Cellpose are increasingly applied across diverse imaging modalities and cell types. However, two critical challenges remain underexplored: (1) the extent of training data redundancy and (2) the impact of cross domain transfer on model retention. In this study, we conduct a systematic empirical analysis of these challenges using Cellpose as a case study. First, to assess data redundancy, we propose a simple dataset quantization (DQ) strategy for constructing compact yet diverse training subsets. Experiments on the Cyto dataset show that image segmentation performance saturates with only 10% of the data, revealing substantial redundancy and potential for training with minimal annotations. Latent space analysis using MAE embeddings and t-SNE confirms that DQ selected patches capture greater feature diversity than random sampling. Second, to examine catastrophic forgetting, we perform cross domain finetuning experiments and observe significant degradation in source domain performance, particularly when adapting from generalist to specialist domains. We demonstrate that selective DQ based replay reintroducing just 5-10% of the source data effectively restores source performance, while full replay can hinder target adaptation. Additionally, we find that training domain sequencing improves generalization and reduces forgetting in multi stage transfer. Our findings highlight the importance of data centric design in biomedical image segmentation and suggest that efficient training requires not only compact subsets but also retention aware learning strategies and informed domain ordering. The code is available at https://github.com/MMV-Lab/biomedseg-efficiency.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention</title>
<link>https://arxiv.org/abs/2511.04811</link>
<guid>https://arxiv.org/abs/2511.04811</guid>
<content:encoded><![CDATA[
arXiv:2511.04811v1 Announce Type: cross 
Abstract: Biomedical image segmentation is critical for precise structure delineation and downstream analysis. Traditional methods often struggle with noisy data, while deep learning models such as U-Net have set new benchmarks in segmentation performance. nnU-Net further automates model configuration, making it adaptable across datasets without extensive tuning. However, it requires a substantial amount of annotated data for cross-validation, posing a challenge when only raw images but no labels are available. Large foundation models offer zero-shot generalizability, but may underperform on specific datasets with unique characteristics, limiting their direct use for analysis. This work addresses these bottlenecks by proposing a data-centric AI workflow that leverages active learning and pseudo-labeling to combine the strengths of traditional neural networks and large foundation models while minimizing human intervention. The pipeline starts by generating pseudo-labels from a foundation model, which are then used for nnU-Net's self-configuration. Subsequently, a representative core-set is selected for minimal manual annotation, enabling effective fine-tuning of the nnU-Net model. This approach significantly reduces the need for manual annotations while maintaining competitive performance, providing an accessible solution for biomedical researchers to apply state-of-the-art AI techniques in their segmentation tasks. The code is available at https://github.com/MMV-Lab/AL_BioMed_img_seg.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Multimodal Diffusion Forcing for Forceful Manipulation</title>
<link>https://arxiv.org/abs/2511.04812</link>
<guid>https://arxiv.org/abs/2511.04812</guid>
<content:encoded><![CDATA[
arXiv:2511.04812v1 Announce Type: cross 
Abstract: Given a dataset of expert trajectories, standard imitation learning approaches typically learn a direct mapping from observations (e.g., RGB images) to actions. However, such methods often overlook the rich interplay between different modalities, i.e., sensory inputs, actions, and rewards, which is crucial for modeling robot behavior and understanding task outcomes. In this work, we propose Multimodal Diffusion Forcing, a unified framework for learning from multimodal robot trajectories that extends beyond action generation. Rather than modeling a fixed distribution, MDF applies random partial masking and trains a diffusion model to reconstruct the trajectory. This training objective encourages the model to learn temporal and cross-modal dependencies, such as predicting the effects of actions on force signals or inferring states from partial observations. We evaluate MDF on contact-rich, forceful manipulation tasks in simulated and real-world environments. Our results show that MDF not only delivers versatile functionalities, but also achieves strong performance, and robustness under noisy observations. More visualizations can be found on our website https://unified-df.github.io
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs</title>
<link>https://arxiv.org/abs/2511.04869</link>
<guid>https://arxiv.org/abs/2511.04869</guid>
<content:encoded><![CDATA[
arXiv:2511.04869v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often lack meaningful confidence estimates for their outputs. While base LLMs are known to exhibit next-token calibration, it remains unclear whether they can assess confidence in the actual meaning of their responses beyond the token level. We find that, when using a certain sampling-based notion of semantic calibration, base LLMs are remarkably well-calibrated: they can meaningfully assess confidence in open-domain question-answering tasks, despite not being explicitly trained to do so. Our main theoretical contribution establishes a mechanism for why semantic calibration emerges as a byproduct of next-token prediction, leveraging a recent connection between calibration and local loss optimality. The theory relies on a general definition of "B-calibration," which is a notion of calibration parameterized by a choice of equivalence classes (semantic or otherwise). This theoretical mechanism leads to a testable prediction: base LLMs will be semantically calibrated when they can easily predict their own distribution over semantic answer classes before generating a response. We state three implications of this prediction, which we validate through experiments: (1) Base LLMs are semantically calibrated across question-answering tasks, (2) RL instruction-tuning systematically breaks this calibration, and (3) chain-of-thought reasoning breaks calibration. To our knowledge, our work provides the first principled explanation of when and why semantic calibration emerges in LLMs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prototype Selection Using Topological Data Analysis</title>
<link>https://arxiv.org/abs/2511.04873</link>
<guid>https://arxiv.org/abs/2511.04873</guid>
<content:encoded><![CDATA[
arXiv:2511.04873v1 Announce Type: cross 
Abstract: Recently, there has been an explosion in statistical learning literature to represent data using topological principles to capture structure and relationships. We propose a topological data analysis (TDA)-based framework, named Topological Prototype Selector (TPS), for selecting representative subsets (prototypes) from large datasets. We demonstrate the effectiveness of TPS on simulated data under different data intrinsic characteristics, and compare TPS against other currently used prototype selection methods in real data settings. In all simulated and real data settings, TPS significantly preserves or improves classification performance while substantially reducing data size. These contributions advance both algorithmic and geometric aspects of prototype learning and offer practical tools for parallelized, interpretable, and efficient classification.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs</title>
<link>https://arxiv.org/abs/2511.04875</link>
<guid>https://arxiv.org/abs/2511.04875</guid>
<content:encoded><![CDATA[
arXiv:2511.04875v1 Announce Type: cross 
Abstract: Recent studies have revealed that LLMs can exhibit behavioral self-awareness: the ability to accurately describe or predict their own learned behaviors without explicit supervision. This capability raises safety concerns as it may, for example, allow models to better conceal their true abilities during evaluation. We attempt to characterize the minimal conditions under which such self-awareness emerges, and the mechanistic processes through which it manifests. Through controlled finetuning experiments on instruction-tuned LLMs with low-rank adapters (LoRA), we find: (1) that self-awareness can be reliably induced using a single rank-1 LoRA adapter; (2) that the learned self-aware behavior can be largely captured by a single steering vector in activation space, recovering nearly all of the fine-tune's behavioral effect; and (3) that self-awareness is non-universal and domain-localized, with independent representations across tasks. Together, these findings suggest that behavioral self-awareness emerges as a domain-specific, linear feature that can be easily induced and modulated.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Fourier shapes to probe the geometric world of deep neural networks</title>
<link>https://arxiv.org/abs/2511.04970</link>
<guid>https://arxiv.org/abs/2511.04970</guid>
<content:encoded><![CDATA[
arXiv:2511.04970v1 Announce Type: cross 
Abstract: While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that optimized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model's salient regions; and third, that they constitute a new, generalizable adversarial paradigm capable of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance optimization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Cognitive Assessment Scores in Older Adults with Cognitive Impairment Using Wearable Sensors</title>
<link>https://arxiv.org/abs/2511.04983</link>
<guid>https://arxiv.org/abs/2511.04983</guid>
<content:encoded><![CDATA[
arXiv:2511.04983v1 Announce Type: cross 
Abstract: Background and Objectives: This paper focuses on using AI to assess the cognitive function of older adults with mild cognitive impairment or mild dementia using physiological data provided by a wearable device. Cognitive screening tools are disruptive, time-consuming, and only capture brief snapshots of activity. Wearable sensors offer an attractive alternative by continuously monitoring physiological signals. This study investigated whether physiological data can accurately predict scores on established cognitive tests. Research Design and Methods: We recorded physiological signals from 23 older adults completing three NIH Toolbox Cognitive Battery tests, which assess working memory, processing speed, and attention. The Empatica EmbracePlus, a wearable device, measured blood volume pulse, skin conductance, temperature, and movement. Statistical features were extracted using wavelet-based and segmentation methods. We then applied supervised learning and validated predictions via cross-validation, hold-out testing, and bootstrapping. Results: Our models showed strong performance with Spearman's \rho of 0.73-0.82 and mean absolute errors of 0.14-0.16, significantly outperforming a naive mean predictor. Sensor roles varied: heart-related signals combined with movement and temperature best predicted working memory, movement paired with skin conductance was most informative for processing speed, and heart in tandem with skin conductance worked best for attention. Discussion and Implications: These findings suggest that wearable sensors paired with AI tools such as supervised learning and feature engineering can noninvasively track specific cognitive functions in older adults, enabling continuous monitoring. Our study demonstrates how AI can be leveraged when the data sample is small. This approach may support remote assessments and facilitate clinical interventions.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies</title>
<link>https://arxiv.org/abs/2511.05018</link>
<guid>https://arxiv.org/abs/2511.05018</guid>
<content:encoded><![CDATA[
arXiv:2511.05018v1 Announce Type: cross 
Abstract: Large language models (LLMs) are typically aligned to a universal set of safety and usage principles intended for broad public acceptability. Yet, real-world applications of LLMs often take place within organizational ecosystems shaped by distinctive corporate policies, regulatory requirements, use cases, brand guidelines, and ethical commitments. This reality highlights the need for rigorous and comprehensive evaluation of LLMs with pluralistic alignment goals, an alignment paradigm that emphasizes adaptability to diverse user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE (PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs' capacity to adhere to pluralistic alignment specifications in multi-turn, interactive conversations. PBSUITE consists of (1) a diverse dataset of 300 realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic evaluation framework for stress-testing model compliance with custom behavioral specifications under adversarial conditions. Using PBSUITE, We find that leading open- and closed-source LLMs maintain robust adherence to behavioral policies in single-turn settings (less than 4% failure rates), but their compliance weakens substantially in multi-turn adversarial interactions (up to 84% failure rates). These findings highlight that existing model alignment and safety moderation methods fall short in coherently enforcing pluralistic behavioral policies in real-world LLM interactions. Our work contributes both the dataset and analytical framework to support future research toward robust and context-aware pluralistic alignment techniques.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Bidirectional Causal Effects with Large Scale Online Kernel Learning</title>
<link>https://arxiv.org/abs/2511.05050</link>
<guid>https://arxiv.org/abs/2511.05050</guid>
<content:encoded><![CDATA[
arXiv:2511.05050v1 Announce Type: cross 
Abstract: In this study, a scalable online kernel learning framework is proposed for estimating bidirectional causal effects in systems characterized by mutual dependence and heteroskedasticity. Traditional causal inference often focuses on unidirectional effects, overlooking the common bidirectional relationships in real-world phenomena. Building on heteroskedasticity-based identification, the proposed method integrates a quasi-maximum likelihood estimator for simultaneous equation models with large scale online kernel learning. It employs random Fourier feature approximations to flexibly model nonlinear conditional means and variances, while an adaptive online gradient descent algorithm ensures computational efficiency for streaming and high-dimensional data. Results from extensive simulations demonstrate that the proposed method achieves superior accuracy and stability than single equation and polynomial approximation baselines, exhibiting lower bias and root mean squared error across various data-generating processes. These results confirm that the proposed approach effectively captures complex bidirectional causal effects with near-linear computational scaling. By combining econometric identification with modern machine learning techniques, the proposed framework offers a practical, scalable, and theoretically grounded solution for large scale causal inference in natural/social science, policy making, business, and industrial applications.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Layer-wise Distillation for Efficient Compression of Large Language Models</title>
<link>https://arxiv.org/abs/2511.05085</link>
<guid>https://arxiv.org/abs/2511.05085</guid>
<content:encoded><![CDATA[
arXiv:2511.05085v1 Announce Type: cross 
Abstract: This work investigates distillation methods for large language models (LLMs) with the goal of developing compact models that preserve high performance. Several existing approaches are reviewed, with a discussion of their respective strengths and limitations. An improved method based on the ShortGPT approach has been developed, building upon the idea of incorporating iterative evaluation of layer importance. At each step, importance is assessed by measuring performance degradation when individual layers are removed, using a set of representative datasets. This process is combined with further training using a joint loss function based on KL divergence and mean squared error. Experiments on the Qwen2.5-3B model show that the number of layers can be reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a 9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that the middle transformer layers contribute less to inference, underscoring the potential of the proposed method for creating efficient models. The results demonstrate the effectiveness of iterative distillation and fine-tuning, making the approach suitable for deployment in resource-limited settings.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Early Alzheimer's Disease Detection from Retinal OCT Images: A UK Biobank Study</title>
<link>https://arxiv.org/abs/2511.05106</link>
<guid>https://arxiv.org/abs/2511.05106</guid>
<content:encoded><![CDATA[
arXiv:2511.05106v1 Announce Type: cross 
Abstract: Alterations in retinal layer thickness, measurable using Optical Coherence Tomography (OCT), have been associated with neurodegenerative diseases such as Alzheimer's disease (AD). While previous studies have mainly focused on segmented layer thickness measurements, this study explored the direct classification of OCT B-scan images for the early detection of AD. To our knowledge, this is the first application of deep learning to raw OCT B-scans for AD prediction in the literature. Unlike conventional medical image classification tasks, early detection is more challenging than diagnosis because imaging precedes clinical diagnosis by several years. We fine-tuned and evaluated multiple pretrained models, including ImageNet-based networks and the OCT-specific RETFound transformer, using subject-level cross-validation datasets matched for age, sex, and imaging instances from the UK Biobank cohort. To reduce overfitting in this small, high-dimensional dataset, both standard and OCT-specific augmentation techniques were applied, along with a year-weighted loss function that prioritized cases diagnosed within four years of imaging. ResNet-34 produced the most stable results, achieving an AUC of 0.62 in the 4-year cohort. Although below the threshold for clinical application, our explainability analyses confirmed localized structural differences in the central macular subfield between the AD and control groups. These findings provide a baseline for OCT-based AD prediction, highlight the challenges of detecting subtle retinal biomarkers years before AD diagnosis, and point to the need for larger datasets and multimodal approaches.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks</title>
<link>https://arxiv.org/abs/2511.05156</link>
<guid>https://arxiv.org/abs/2511.05156</guid>
<content:encoded><![CDATA[
arXiv:2511.05156v1 Announce Type: cross 
Abstract: With more and more existing networks being transformed to Software-Defined Networking (SDN), they need to be more secure and demand smarter ways of traffic control. This work, SmartSecChain-SDN, is a platform that combines machine learning based intrusion detection, blockchain-based storage of logs, and application-awareness-based priority in SDN networks. To detect network intrusions in a real-time, precision and low-false positives setup, the framework utilizes the application of advanced machine learning algorithms, namely Random Forest, XGBoost, CatBoost, and CNN-BiLSTM. SmartSecChain-SDN is based on the Hyperledger Fabric, which is a permissioned blockchain technology, to provide secure, scalable, and privacy-preserving storage and, thus, guarantee that the Intrusion Detection System (IDS) records cannot be altered and can be analyzed comprehensively. The system also has Quality of Service (QoS) rules and traffic shaping based on applications, which enables prioritization of critical services, such as VoIP, video conferencing, and business applications, as well as de-prioritization of non-essential traffic, such as downloads and updates. Mininet can simulate real-time SDN scenarios because it is used to prototype whole architectures. It is also compatible with controllers OpenDaylight and Ryu. It has tested the framework using the InSDN dataset and proved that it can identify different kinds of cyberattacks and handle bandwidth allocation efficiently under circumstances of resource constraints. SmartSecChain-SDN comprehensively addresses SDN system protection, securing and enhancing. The proposed study offers an innovative, extensible way to improve cybersecurity, regulatory compliance, and the administration of next-generation programmable networks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Follow-Me in Micro-Mobility with End-to-End Imitation Learning</title>
<link>https://arxiv.org/abs/2511.05158</link>
<guid>https://arxiv.org/abs/2511.05158</guid>
<content:encoded><![CDATA[
arXiv:2511.05158v1 Announce Type: cross 
Abstract: Autonomous micro-mobility platforms face challenges from the perspective of the typical deployment environment: large indoor spaces or urban areas that are potentially crowded and highly dynamic. While social navigation algorithms have progressed significantly, optimizing user comfort and overall user experience over other typical metrics in robotics (e.g., time or distance traveled) is understudied. Specifically, these metrics are critical in commercial applications. In this paper, we show how imitation learning delivers smoother and overall better controllers, versus previously used manually-tuned controllers. We demonstrate how DAAV's autonomous wheelchair achieves state-of-the-art comfort in follow-me mode, in which it follows a human operator assisting persons with reduced mobility (PRM). This paper analyzes different neural network architectures for end-to-end control and demonstrates their usability in real-world production-level deployments.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A New Framework for Convex Clustering in Kernel Spaces: Finite Sample Bounds, Consistency and Performance Insights</title>
<link>https://arxiv.org/abs/2511.05159</link>
<guid>https://arxiv.org/abs/2511.05159</guid>
<content:encoded><![CDATA[
arXiv:2511.05159v1 Announce Type: cross 
Abstract: Convex clustering is a well-regarded clustering method, resembling the similar centroid-based approach of Lloyd's $k$-means, without requiring a predefined cluster count. It starts with each data point as its centroid and iteratively merges them. Despite its advantages, this method can fail when dealing with data exhibiting linearly non-separable or non-convex structures. To mitigate the limitations, we propose a kernelized extension of the convex clustering method. This approach projects the data points into a Reproducing Kernel Hilbert Space (RKHS) using a feature map, enabling convex clustering in this transformed space. This kernelization not only allows for better handling of complex data distributions but also produces an embedding in a finite-dimensional vector space. We provide a comprehensive theoretical underpinnings for our kernelized approach, proving algorithmic convergence and establishing finite sample bounds for our estimates. The effectiveness of our method is demonstrated through extensive experiments on both synthetic and real-world datasets, showing superior performance compared to state-of-the-art clustering techniques. This work marks a significant advancement in the field, offering an effective solution for clustering in non-linear and non-convex data scenarios.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Another BRIXEL in the Wall: Towards Cheaper Dense Features</title>
<link>https://arxiv.org/abs/2511.05168</link>
<guid>https://arxiv.org/abs/2511.05168</guid>
<content:encoded><![CDATA[
arXiv:2511.05168v1 Announce Type: cross 
Abstract: Vision foundation models achieve strong performance on both global and locally dense downstream tasks. Pretrained on large images, the recent DINOv3 model family is able to produce very fine-grained dense feature maps, enabling state-of-the-art performance. However, computing these feature maps requires the input image to be available at very high resolution, as well as large amounts of compute due to the squared complexity of the transformer architecture. To address these issues, we propose BRIXEL, a simple knowledge distillation approach that has the student learn to reproduce its own feature maps at higher resolution. Despite its simplicity, BRIXEL outperforms the baseline DINOv3 models by large margins on downstream tasks when the resolution is kept fixed. Moreover, it is able to produce feature maps that are very similar to those of the teacher at a fraction of the computational cost. Code and model weights are available at https://github.com/alexanderlappe/BRIXEL.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A differentiable model of supply-chain shocks</title>
<link>https://arxiv.org/abs/2511.05231</link>
<guid>https://arxiv.org/abs/2511.05231</guid>
<content:encoded><![CDATA[
arXiv:2511.05231v1 Announce Type: cross 
Abstract: Modelling how shocks propagate in supply chains is an increasingly important challenge in economics. Its relevance has been highlighted in recent years by events such as Covid-19 and the Russian invasion of Ukraine. Agent-based models (ABMs) are a promising approach for this problem. However, calibrating them is hard. We show empirically that it is possible to achieve speed ups of over 3 orders of magnitude when calibrating ABMs of supply networks by running them on GPUs and using automatic differentiation, compared to non-differentiable baselines. This opens the door to scaling ABMs to model the whole global supply network.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-aware Learned Mesh-based Simulation via Trajectory-Level Meta-Learning</title>
<link>https://arxiv.org/abs/2511.05234</link>
<guid>https://arxiv.org/abs/2511.05234</guid>
<content:encoded><![CDATA[
arXiv:2511.05234v1 Announce Type: cross 
Abstract: Simulating object deformations is a critical challenge across many scientific domains, including robotics, manufacturing, and structural mechanics. Learned Graph Network Simulators (GNSs) offer a promising alternative to traditional mesh-based physics simulators. Their speed and inherent differentiability make them particularly well suited for applications that require fast and accurate simulations, such as robotic manipulation or manufacturing optimization. However, existing learned simulators typically rely on single-step observations, which limits their ability to exploit temporal context. Without this information, these models fail to infer, e.g., material properties. Further, they rely on auto-regressive rollouts, which quickly accumulate error for long trajectories. We instead frame mesh-based simulation as a trajectory-level meta-learning problem. Using Conditional Neural Processes, our method enables rapid adaptation to new simulation scenarios from limited initial data while capturing their latent simulation properties. We utilize movement primitives to directly predict fast, stable and accurate simulations from a single model call. The resulting approach, Movement-primitive Meta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of the runtime cost compared to state-of-the-art GNSs across several tasks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.05275</link>
<guid>https://arxiv.org/abs/2511.05275</guid>
<content:encoded><![CDATA[
arXiv:2511.05275v1 Announce Type: cross 
Abstract: Vision-language-action models (VLAs) trained on large-scale robotic datasets have demonstrated strong performance on manipulation tasks, including bimanual tasks. However, because most public datasets focus on single-arm demonstrations, adapting VLAs for bimanual tasks typically requires substantial additional bimanual data and fine-tuning. To address this challenge, we introduce TwinVLA, a modular framework that composes two copies of a pretrained single-arm VLA into a coordinated bimanual VLA. Unlike monolithic cross-embodiment models trained on mixtures of single-arm and bimanual data, TwinVLA improves both data efficiency and performance by composing pretrained single-arm policies. Across diverse bimanual tasks in real-world and simulation settings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model without requiring any bimanual pretraining. Furthermore, it narrows the gap to state-of-the-art model, $\pi_0$ which rely on extensive proprietary bimanual data and compute cost. These results establish our modular composition approach as a data-efficient and scalable path toward high-performance bimanual manipulation, leveraging public single-arm data.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs</title>
<link>https://arxiv.org/abs/2511.05292</link>
<guid>https://arxiv.org/abs/2511.05292</guid>
<content:encoded><![CDATA[
arXiv:2511.05292v1 Announce Type: cross 
Abstract: Accurate food intake detection is vital for dietary monitoring and chronic disease prevention. Traditional self-report methods are prone to recall bias, while camera-based approaches raise concerns about privacy. Furthermore, existing wearable-based methods primarily focus on a limited number of food types, such as hamburgers and pizza, failing to address the vast diversity of Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that classifies Chinese food types by integrating hand motion cues from a smartwatch with head dynamics from smart glasses. To filter out irrelevant daily activities, we design a two-stage detection pipeline. The first stage identifies eating states by distinguishing characteristic temporal patterns from non-eating behaviors. The second stage then conducts fine-grained food type recognition based on the motions captured during food intake. To evaluate CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings across 11 food categories and 10 participants. Experiments demonstrate that CuisineSense achieves high accuracy in both eating state detection and food classification, offering a practical solution for unobtrusive, wearable-based dietary monitoring.The system code is publicly available at https://github.com/joeeeeyin/CuisineSense.git.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Generation and Identification From Partial Enumeration: Tight Density Bounds and Topological Characterizations</title>
<link>https://arxiv.org/abs/2511.05295</link>
<guid>https://arxiv.org/abs/2511.05295</guid>
<content:encoded><![CDATA[
arXiv:2511.05295v1 Announce Type: cross 
Abstract: The success of large language models (LLMs) has motivated formal theories of language generation and learning. We study the framework of \emph{language generation in the limit}, where an adversary enumerates strings from an unknown language $K$ drawn from a countable class, and an algorithm must generate unseen strings from $K$. Prior work showed that generation is always possible, and that some algorithms achieve positive lower density, revealing a \emph{validity--breadth} trade-off between correctness and coverage. We resolve a main open question in this line, proving a tight bound of $1/2$ on the best achievable lower density. We then strengthen the model to allow \emph{partial enumeration}, where the adversary reveals only an infinite subset $C \subseteq K$. We show that generation in the limit remains achievable, and if $C$ has lower density $\alpha$ in $K$, the algorithm's output achieves density at least $\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the partial-information setting, where the generator must recover within a factor $1/2$ of the revealed subset's density. We further revisit the classical Gold--Angluin model of \emph{language identification} under partial enumeration. We characterize when identification in the limit is possible -- when hypotheses $M_t$ eventually satisfy $C \subseteq M \subseteq K$ -- and in the process give a new topological formulation of Angluin's characterization, showing that her condition is precisely equivalent to an appropriate topological space having the $T_D$ separation property.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building Specialized Software-Assistant ChatBot with Graph-Based Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2511.05297</link>
<guid>https://arxiv.org/abs/2511.05297</guid>
<content:encoded><![CDATA[
arXiv:2511.05297v1 Announce Type: cross 
Abstract: Digital Adoption Platforms (DAPs) have become essential tools for helping employees navigate complex enterprise software such as CRM, ERP, or HRMS systems. Companies like LemonLearning have shown how digital guidance can reduce training costs and accelerate onboarding. However, building and maintaining these interactive guides still requires extensive manual effort. Leveraging Large Language Models as virtual assistants is an appealing alternative, yet without a structured understanding of the target software, LLMs often hallucinate and produce unreliable answers. Moreover, most production-grade LLMs are black-box APIs, making fine-tuning impractical due to the lack of access to model weights. In this work, we introduce a Graph-based Retrieval-Augmented Generation framework that automatically converts enterprise web applications into state-action knowledge graphs, enabling LLMs to generate grounded and context-aware assistance. The framework was co-developed with the AI enterprise RAKAM, in collaboration with Lemon Learning. We detail the engineering pipeline that extracts and structures software interfaces, the design of the graph-based retrieval process, and the integration of our approach into production DAP workflows. Finally, we discuss scalability, robustness, and deployment lessons learned from industrial use cases.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QUESTER: Query Specification for Generative Retrieval</title>
<link>https://arxiv.org/abs/2511.05301</link>
<guid>https://arxiv.org/abs/2511.05301</guid>
<content:encoded><![CDATA[
arXiv:2511.05301v1 Announce Type: cross 
Abstract: Generative Retrieval (GR) differs from the traditional index-then-retrieve pipeline by storing relevance in model parameters and directly generating document identifiers. However, GR often struggles to generalize and is costly to scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval), which reframes GR as query specification generation - in this work, a simple keyword query handled by BM25 - using a (small) LLM. The policy is trained using reinforcement learning techniques (GRPO). Across in- and out-of-domain evaluations, we show that our model is more effective than BM25, and competitive with neural IR models, while maintaining a good efficiency
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</title>
<link>https://arxiv.org/abs/2511.05308</link>
<guid>https://arxiv.org/abs/2511.05308</guid>
<content:encoded><![CDATA[
arXiv:2511.05308v1 Announce Type: cross 
Abstract: As 3D point clouds become a cornerstone of modern technology, the need for sophisticated generative models and reliable evaluation metrics has grown exponentially. In this work, we first expose that some commonly used metrics for evaluating generated point clouds, particularly those based on Chamfer Distance (CD), lack robustness against defects and fail to capture geometric fidelity and local shape consistency when used as quality indicators. We further show that introducing samples alignment prior to distance calculation and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet essential steps to ensure the consistency and robustness of point cloud generative model evaluation metrics. While existing metrics primarily focus on directly comparing 3D Euclidean coordinates, we present a novel metric, named Surface Normal Concordance (SNC), which approximates surface similarity by comparing estimated point normals. This new metric, when combined with traditional ones, provides a more comprehensive evaluation of the quality of generated samples. Finally, leveraging recent advancements in transformer-based models for point cloud analysis, such as serialized patch attention , we propose a new architecture for generating high-fidelity 3D structures, the Diffusion Point Transformer. We perform extensive experiments and comparisons on the ShapeNet dataset, showing that our model outperforms previous solutions, particularly in terms of quality of generated point clouds, achieving new state-of-the-art. Code available at https://github.com/matteo-bastico/DiffusionPointTransformer.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance</title>
<link>https://arxiv.org/abs/2511.05311</link>
<guid>https://arxiv.org/abs/2511.05311</guid>
<content:encoded><![CDATA[
arXiv:2511.05311v1 Announce Type: cross 
Abstract: Economic constraints, limited availability of datasets for reproducibility and shortages of specialized expertise have long been recognized as key challenges to the adoption and advancement of predictive maintenance (PdM) in the automotive sector. Recent progress in large language models (LLMs) presents an opportunity to overcome these barriers and speed up the transition of PdM from research to industrial practice. Under these conditions, we explore the potential of LLM-based agents to support PdM cleaning pipelines. Specifically, we focus on maintenance logs, a critical data source for training well-performing machine learning (ML) models, but one often affected by errors such as typos, missing fields, near-duplicate entries, and incorrect dates. We evaluate LLM agents on cleaning tasks involving six distinct types of noise. Our findings show that LLMs are effective at handling generic cleaning tasks and offer a promising foundation for future industrial applications. While domain-specific errors remain challenging, these results highlight the potential for further improvements through specialized training and enhanced agentic capabilities.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering Language Models with Weight Arithmetic</title>
<link>https://arxiv.org/abs/2511.05408</link>
<guid>https://arxiv.org/abs/2511.05408</guid>
<content:encoded><![CDATA[
arXiv:2511.05408v1 Announce Type: cross 
Abstract: Providing high-quality feedback to Large Language Models (LLMs) on a diverse training distribution can be difficult and expensive, and providing feedback only on a narrow distribution can result in unintended generalizations. To better leverage narrow training data, we propose contrastive weight steering, a simple post-training method that edits the model parameters using weight arithmetic. We isolate a behavior direction in weight-space by subtracting the weight deltas from two small fine-tunes -- one that induces the desired behavior and another that induces its opposite -- and then add or remove this direction to modify the model's weights. We apply this technique to mitigate sycophancy and induce misalignment, and find that weight steering often generalizes further than activation steering, achieving stronger out-of-distribution behavioral control before degrading general capabilities. We also show that, in the context of task-specific fine-tuning, weight steering can partially mitigate undesired behavioral drift: it can reduce sycophancy and under-refusals introduced during fine-tuning while preserving task performance gains. Finally, we provide preliminary evidence that emergent misalignment can be detected by measuring the similarity between fine-tuning updates and an "evil" weight direction, suggesting that it may be possible to monitor the evolution of weights during training and detect rare misaligned behaviors that never manifest during training or evaluations.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?</title>
<link>https://arxiv.org/abs/2511.05449</link>
<guid>https://arxiv.org/abs/2511.05449</guid>
<content:encoded><![CDATA[
arXiv:2511.05449v1 Announce Type: cross 
Abstract: Recent advances in 3D point cloud transformers have led to state-of-the-art results in tasks such as semantic segmentation and reconstruction. However, these models typically rely on dense token representations, incurring high computational and memory costs during training and inference. In this work, we present the finding that tokens are remarkably redundant, leading to substantial inefficiency. We introduce gitmerge3D, a globally informed graph token merging method that can reduce the token count by up to 90-95% while maintaining competitive performance. This finding challenges the prevailing assumption that more tokens inherently yield better performance and highlights that many current models are over-tokenized and under-optimized for scalability. We validate our method across multiple 3D vision tasks and show consistent improvements in computational efficiency. This work is the first to assess redundancy in large-scale 3D transformer models, providing insights into the development of more efficient 3D foundation architectures. Our code and checkpoints are publicly available at https://gitmerge3d.github.io
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-adaptive weighting and sampling for physics-informed neural networks</title>
<link>https://arxiv.org/abs/2511.05452</link>
<guid>https://arxiv.org/abs/2511.05452</guid>
<content:encoded><![CDATA[
arXiv:2511.05452v1 Announce Type: cross 
Abstract: Physics-informed deep learning has emerged as a promising framework for solving partial differential equations (PDEs). Nevertheless, training these models on complex problems remains challenging, often leading to limited accuracy and efficiency. In this work, we introduce a hybrid adaptive sampling and weighting method to enhance the performance of physics-informed neural networks (PINNs). The adaptive sampling component identifies training points in regions where the solution exhibits rapid variation, while the adaptive weighting component balances the convergence rate across training points. Numerical experiments show that applying only adaptive sampling or only adaptive weighting is insufficient to consistently achieve accurate predictions, particularly when training points are scarce. Since each method emphasizes different aspects of the solution, their effectiveness is problem dependent. By combining both strategies, the proposed framework consistently improves prediction accuracy and training efficiency, offering a more robust approach for solving PDEs with PINNs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?</title>
<link>https://arxiv.org/abs/2511.05476</link>
<guid>https://arxiv.org/abs/2511.05476</guid>
<content:encoded><![CDATA[
arXiv:2511.05476v1 Announce Type: cross 
Abstract: Transformer-based language models of code have achieved state-of-the-art performance across a wide range of software analytics tasks, but their practical deployment remains limited due to high computational costs, slow inference speeds, and significant environmental impact. To address these challenges, recent research has increasingly explored knowledge distillation as a method for compressing a large language model of code (the teacher) into a smaller model (the student) while maintaining performance. However, the degree to which a student model deeply mimics the predictive behavior and internal representations of its teacher remains largely unexplored, as current accuracy-based evaluation provides only a surface-level view of model quality and often fails to capture more profound discrepancies in behavioral fidelity between the teacher and student models. To address this gap, we empirically show that the student model often fails to deeply mimic the teacher model, resulting in up to 285% greater performance drop under adversarial attacks, which is not captured by traditional accuracy-based evaluation. Therefore, we propose MetaCompress, a metamorphic testing framework that systematically evaluates behavioral fidelity by comparing the outputs of teacher and student models under a set of behavior-preserving metamorphic relations. We evaluate MetaCompress on two widely studied tasks, using compressed versions of popular language models of code, obtained via three different knowledge distillation techniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress identifies up to 62% behavioral discrepancies in student models, underscoring the need for behavioral fidelity evaluation within the knowledge distillation pipeline and establishing MetaCompress as a practical framework for testing compressed language models of code derived through knowledge distillation.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RNN(p) for Power Consumption Forecasting</title>
<link>https://arxiv.org/abs/2209.01378</link>
<guid>https://arxiv.org/abs/2209.01378</guid>
<content:encoded><![CDATA[
arXiv:2209.01378v2 Announce Type: replace 
Abstract: An elementary Recurrent Neural Network that operates on p time lags, called an RNN(p), is the natural generalisation of a linear autoregressive model ARX(p). It is a powerful forecasting tool for variables displaying inherent seasonal patterns across multiple time scales, as is often observed in energy, economic, and financial time series. The architecture of RNN(p) models, characterised by structured feedbacks across time lags, enables the design of efficient training strategies. We conduct a comparative study of learning algorithms for these models, providing a rigorous analysis of their computational complexity and training performance. We present two applications of RNN(p) models in power consumption forecasting, a key domain within the energy sector where accurate forecasts inform both operational and financial decisions. Experimental results show that RNN(p) models achieve excellent forecasting accuracy while maintaining a high degree of interpretability. These features make them well-suited for decision-making in energy markets and other fintech applications where reliable predictions play a significant economic role.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Stage Automated Online Network Data Stream Analytics Framework for IIoT Systems</title>
<link>https://arxiv.org/abs/2210.01985</link>
<guid>https://arxiv.org/abs/2210.01985</guid>
<content:encoded><![CDATA[
arXiv:2210.01985v2 Announce Type: replace 
Abstract: Industry 5.0 aims at maximizing the collaboration between humans and machines. Machines are capable of automating repetitive jobs, while humans handle creative tasks. As a critical component of Industrial Internet of Things (IIoT) systems for service delivery, network data stream analytics often encounter concept drift issues due to dynamic IIoT environments, causing performance degradation and automation difficulties. In this paper, we propose a novel Multi-Stage Automated Network Analytics (MSANA) framework for concept drift adaptation in IIoT systems, consisting of dynamic data pre-processing, the proposed Drift-based Dynamic Feature Selection (DD-FS) method, dynamic model learning & selection, and the proposed Window-based Performance Weighted Probability Averaging Ensemble (W-PWPAE) model. It is a complete automated data stream analytics framework that enables automatic, effective, and efficient data analytics for IIoT systems in Industry 5.0. Experimental results on two public IoT datasets demonstrate that the proposed framework outperforms state-of-the-art methods for IIoT data stream analytics.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-stationary Delayed Online Convex Optimization: From Full-information to Bandit Setting</title>
<link>https://arxiv.org/abs/2305.12131</link>
<guid>https://arxiv.org/abs/2305.12131</guid>
<content:encoded><![CDATA[
arXiv:2305.12131v4 Announce Type: replace 
Abstract: Although online convex optimization (OCO) under arbitrary delays has received increasing attention recently, previous studies focus on stationary environments with the goal of minimizing static regret. In this paper, we investigate the delayed OCO in non-stationary environments, and choose dynamic regret with respect to any sequence of comparators as the performance metric. To this end, we first propose an algorithm called Mild-OGD for the full-information case, where delayed gradients are available. The basic idea is to maintain multiple experts in parallel, each performing a gradient descent step with different learning rates for every delayed gradient according to their arrival order, and utilize a meta-algorithm to track the best one based on their delayed performance. Despite the simplicity of this idea, our novel analysis shows that the dynamic regret of Mild-OGD can be automatically bounded by $O(\sqrt{\bar{d}T(P_T+1)})$ under the in-order assumption and $O(\sqrt{dT(P_T+1)})$ in the worst case, where $\bar{d}$ and $d$ denote the average and maximum delay respectively, $T$ is the time horizon, and $P_T$ is the path-length of comparators. Moreover, we demonstrate that the result in the worst case is optimal by deriving a matching lower bound. Finally, we develop a bandit variant of Mild-OGD for a more challenging case with only delayed loss values. Interestingly, we prove that under a relatively large amount of delay, our bandit algorithm even enjoys the best dynamic regret bound of existing non-delayed bandit algorithms.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning for Interval Prediction of Electricity Demand: A Cluster-based Bootstrapping Approach</title>
<link>https://arxiv.org/abs/2309.01336</link>
<guid>https://arxiv.org/abs/2309.01336</guid>
<content:encoded><![CDATA[
arXiv:2309.01336v2 Announce Type: replace 
Abstract: Accurate predictions of electricity demands are necessary for managing operations in a small aggregation load setting like a Microgrid. Due to low aggregation, the electricity demands can be highly stochastic and point estimates would lead to inflated errors. Interval estimation in this scenario, would provide a range of values within which the future values might lie and helps quantify the errors around the point estimates. This paper introduces a residual bootstrap algorithm to generate interval estimates of day-ahead electricity demand. A machine learning algorithm is used to obtain the point estimates of electricity demand and respective residuals on the training set. The obtained residuals are stored in memory and the memory is further partitioned. Days with similar demand patterns are grouped in clusters using an unsupervised learning algorithm and these clusters are used to partition the memory. The point estimates for test day are used to find the closest cluster of similar days and the residuals are bootstrapped from the chosen cluster. This algorithm is evaluated on the real electricity demand data from EULR(End Use Load Research) and is compared to other bootstrapping methods for varying confidence intervals.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing the Training Dynamics of Private Fine-tuning with Langevin diffusion</title>
<link>https://arxiv.org/abs/2402.18905</link>
<guid>https://arxiv.org/abs/2402.18905</guid>
<content:encoded><![CDATA[
arXiv:2402.18905v2 Announce Type: replace 
Abstract: We show that differentially private full fine-tuning (DP-FFT) can distort pre-trained backbone features based on both theoretical and empirical results. We identify the cause of the distortion as the misalignment between the pre-trained backbone and the randomly initialized linear head. We prove that a sequential fine-tuning strategy can mitigate the feature distortion: first-linear-probing-then-fine-tuning (DP-LP-FFT). A new approximation scheme allows us to derive approximate upper and lower bounds on the training loss of DP-LP and DP-FFT, in a simple but canonical setting of 2-layer neural networks with ReLU activation. Experiments on real-world datasets and architectures are consistent with our theoretical insights. We also derive new upper bounds for 2-layer linear networks without the approximation. Moreover, our theory suggests a trade-off of privacy budget allocation in multi-phase fine-tuning methods like DP-LP-FFT.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward</title>
<link>https://arxiv.org/abs/2403.06524</link>
<guid>https://arxiv.org/abs/2403.06524</guid>
<content:encoded><![CDATA[
arXiv:2403.06524v2 Announce Type: replace 
Abstract: We develop a deep reinforcement learning framework for tactical decision making in an autonomous truck, specifically for Adaptive Cruise Control (ACC) and lane change maneuvers in a highway scenario. Our results demonstrate that it is beneficial to separate high-level decision-making processes and low-level control actions between the reinforcement learning agent and the low-level controllers based on physical models. In the following, we study optimizing the performance with a realistic and multi-objective reward function based on Total Cost of Operation (TCOP) of the truck using different approaches; by adding weights to reward components, by normalizing the reward components and by using curriculum learning techniques.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Closer Look at Deep Learning Methods on Tabular Datasets</title>
<link>https://arxiv.org/abs/2407.00956</link>
<guid>https://arxiv.org/abs/2407.00956</guid>
<content:encoded><![CDATA[
arXiv:2407.00956v4 Announce Type: replace 
Abstract: Tabular data is prevalent across diverse domains in machine learning. With the rapid progress of deep tabular prediction methods, especially pretrained (foundation) models, there is a growing need to evaluate these methods systematically and to understand their behavior. We present an extensive study on TALENT, a collection of 300+ datasets spanning broad ranges of size, feature composition (numerical/categorical mixes), domains, and output types (binary, multi--class, regression). Our evaluation shows that ensembling benefits both tree-based and neural approaches. Traditional gradient-boosted trees remain very strong baselines, yet recent pretrained tabular models now match or surpass them on many tasks, narrowing--but not eliminating--the historical advantage of tree ensembles. Despite architectural diversity, top performance concentrates within a small subset of models, providing practical guidance for method selection. To explain these outcomes, we quantify dataset heterogeneity by learning from meta-features and early training dynamics to predict later validation behavior. This dynamics-aware analysis indicates that heterogeneity--such as the interplay of categorical and numerical attributes--largely determines which family of methods is favored. Finally, we introduce a two-level design beyond the 300 common-size datasets: a compact TALENT-tiny core (45 datasets) for rapid, reproducible evaluation, and a TALENT-extension suite targeting high-dimensional, many-class, and very large-scale settings for stress testing. In summary, these results offer actionable insights into the strengths, limitations, and future directions for improving deep tabular learning.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExGra-Med: Extended Context Graph Alignment for Medical Vision-Language Models</title>
<link>https://arxiv.org/abs/2410.02615</link>
<guid>https://arxiv.org/abs/2410.02615</guid>
<content:encoded><![CDATA[
arXiv:2410.02615v4 Announce Type: replace 
Abstract: State-of-the-art medical multi-modal LLMs (med-MLLMs), such as LLaVA-Med and BioMedGPT, primarily depend on scaling model size and data volume, with training driven largely by autoregressive objectives. However, we reveal that this approach can lead to weak vision-language alignment, making these models overly dependent on costly instruction-following data. To address this, we introduce ExGra-Med, a novel multi-graph alignment framework that jointly aligns images, instruction responses, and extended captions in the latent space, advancing semantic grounding and cross-modal coherence. To scale to large LLMs (e.g., LLaMA-7B), we develop an efficient end-to-end training scheme using black-box gradient estimation, enabling fast and scalable optimization. Empirically, ExGra-Med matches LLaVA-Med's performance using just 10% of the pre-training data, achieving a 20.13% gain on VQA-RAD and approaching full-data performance. It also outperforms strong baselines like BioMedGPT and RadFM on visual chatbot and zero-shot classification tasks, demonstrating its promise for efficient, high-quality vision-language integration in medical AI.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>P1-KAN: an effective Kolmogorov-Arnold network with application to hydraulic valley optimization</title>
<link>https://arxiv.org/abs/2410.03801</link>
<guid>https://arxiv.org/abs/2410.03801</guid>
<content:encoded><![CDATA[
arXiv:2410.03801v4 Announce Type: replace 
Abstract: A new Kolmogorov-Arnold network (KAN) is proposed to approximate potentially irregular functions in high dimensions. We provide error bounds for this approximation, assuming that the Kolmogorov-Arnold expansion functions are sufficiently smooth. When the function is only continuous, we also provide universal approximation theorems. We show that it outperforms multilayer perceptrons in terms of accuracy and convergence speed. We also compare it with several proposed KAN networks: it outperforms all networks for irregular functions and achieves similar accuracy to the original spline-based KAN network for smooth functions. Finally, we compare some of the KAN networks in optimizing a French hydraulic valley.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Learn with Contrastive Meta-Objective</title>
<link>https://arxiv.org/abs/2410.05975</link>
<guid>https://arxiv.org/abs/2410.05975</guid>
<content:encoded><![CDATA[
arXiv:2410.05975v5 Announce Type: replace 
Abstract: Meta-learning enables learning systems to adapt quickly to new tasks, similar to humans. Different meta-learning approaches all work under/with the mini-batch episodic training framework. Such framework naturally gives the information about task identity, which can serve as additional supervision for meta-training to improve generalizability. We propose to exploit task identity as additional supervision in meta-training, inspired by the alignment and discrimination ability which is is intrinsic in human's fast learning. This is achieved by contrasting what meta-learners learn, i.e., model representations. The proposed ConML is evaluating and optimizing the contrastive meta-objective under a problem- and learner-agnostic meta-training framework. We demonstrate that ConML integrates seamlessly with existing meta-learners, as well as in-context learning models, and brings significant boost in performance with small implementation cost.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoKO: Low-Rank Kalman Optimizer for Online Fine-Tuning of Large Models</title>
<link>https://arxiv.org/abs/2410.11551</link>
<guid>https://arxiv.org/abs/2410.11551</guid>
<content:encoded><![CDATA[
arXiv:2410.11551v2 Announce Type: replace 
Abstract: Training large models with millions or even billions of parameters from scratch incurs substantial computational costs. Parameter Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), address this challenge by adapting only a reduced number of parameters to specific tasks with gradient-based optimizers. In this paper, we cast PEFT as an optimal filtering/state estimation problem and present Low-Rank Kalman Optimizer (LoKO) to estimate the optimal trainable parameters in an online manner. We leverage the low-rank decomposition in LoRA to significantly reduce matrix sizes in Kalman iterations and further capitalize on a diagonal approximation of the covariance matrix to effectively decrease computational complexity from quadratic to linear in the number of trainable parameters. Moreover, we discovered that the initialization of the covariance matrix within the Kalman algorithm and the accurate estimation of the observation noise covariance are the keys in this formulation, and we propose robust approaches that work well across a vast range of well-established computer vision and language models. Our results show that LoKO converges with fewer iterations and yields better performance models compared to commonly used optimizers with LoRA in both image classifications and language tasks. Our study opens up the possibility of leveraging the Kalman filter as an effective optimizer for the online fine-tuning of large models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProFL: Performative Robust Optimal Federated Learning</title>
<link>https://arxiv.org/abs/2410.18075</link>
<guid>https://arxiv.org/abs/2410.18075</guid>
<content:encoded><![CDATA[
arXiv:2410.18075v2 Announce Type: replace 
Abstract: Performative prediction is a framework that captures distribution shifts that occur during the training of machine learning models due to their deployment. As the trained model is used, data generation causes the model to evolve, leading to deviations from the original data distribution. The impact of such model-induced distribution shifts in federated learning is increasingly likely to transpire in real-life use cases. A recently proposed approach extends performative prediction to federated learning with the resulting model converging to a performative stable point, which may be far from the performative optimal point. Earlier research in centralized settings has shown that the performative optimal point can be achieved under model-induced distribution shifts, but these approaches require the performative risk to be convex and the training data to be noiseless, assumptions often violated in realistic federated learning systems. This paper overcomes all of these shortcomings and proposes Performative Robust Optimal Federated Learning, an algorithm that finds performative optimal points in federated learning from noisy and contaminated data. We present the convergence analysis under the Polyak-Lojasiewicz condition, which applies to non-convex objectives. Extensive experiments on multiple datasets demonstrate the advantage of Robust Optimal Federated Learning over the state-of-the-art.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Approximation with Unbounded Markovian Noise: A General-Purpose Theorem</title>
<link>https://arxiv.org/abs/2410.21704</link>
<guid>https://arxiv.org/abs/2410.21704</guid>
<content:encoded><![CDATA[
arXiv:2410.21704v2 Announce Type: replace 
Abstract: Motivated by engineering applications such as resource allocation in networks and inventory systems, we consider average-reward Reinforcement Learning with unbounded state space and reward function. Recent works studied this problem in the actor-critic framework and established finite sample bounds assuming access to a critic with certain error guarantees. We complement their work by studying Temporal Difference (TD) learning with linear function approximation and establishing finite-time bounds with the optimal $\mathcal{O}\left(1/\epsilon^2\right)$ sample complexity. These results are obtained using the following general-purpose theorem for non-linear Stochastic Approximation (SA).
  Suppose that one constructs a Lyapunov function for a non-linear SA with certain drift condition. Then, our theorem establishes finite-time bounds when this SA is driven by unbounded Markovian noise under suitable conditions. It serves as a black box tool to generalize sample guarantees on SA from i.i.d. or martingale difference case to potentially unbounded Markovian noise. The generality and the mild assumption of the setup enables broad applicability of our theorem. We illustrate its power by studying two more systems: (i) We improve upon the finite-time bounds of $Q$-learning by tightening the error bounds and also allowing for a larger class of behavior policies. (ii) We establish the first ever finite-time bounds for distributed stochastic optimization of high-dimensional smooth strongly convex function using cyclic block coordinate descent.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CTPD: Cross-Modal Temporal Pattern Discovery for Enhanced Multimodal Electronic Health Records Analysis</title>
<link>https://arxiv.org/abs/2411.00696</link>
<guid>https://arxiv.org/abs/2411.00696</guid>
<content:encoded><![CDATA[
arXiv:2411.00696v3 Announce Type: replace 
Abstract: Integrating multimodal Electronic Health Records (EHR) data, such as numerical time series and free-text clinical reports, has great potential in predicting clinical outcomes. However, prior work has primarily focused on capturing temporal interactions within individual samples and fusing multimodal information, overlooking critical temporal patterns across patients. These patterns, such as trends in vital signs like abnormal heart rate or blood pressure, can indicate deteriorating health or an impending critical event. Similarly, clinical notes often contain textual descriptions that reflect these patterns. Identifying corresponding temporal patterns across different modalities is crucial for improving the accuracy of clinical outcome predictions, yet it remains a challenging task. To address this gap, we introduce a Cross-Modal Temporal Pattern Discovery (CTPD) framework, designed to efficiently extract meaningful cross-modal temporal patterns from multimodal EHR data. Our approach introduces shared initial temporal pattern representations which are refined using slot attention to generate temporal semantic embeddings. To ensure rich cross-modal temporal semantics in the learned patterns, we introduce a contrastive-based TPNCE loss for cross-modal alignment, along with two reconstruction losses to retain core information of each modality. Evaluations on two clinically critical tasks, 48-hour in-hospital mortality and 24-hour phenotype classification, using the MIMIC-III database demonstrate the superiority of our method over existing approaches.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TOBUGraph: Knowledge Graph-Based Retrieval for Enhanced LLM Performance Beyond RAG</title>
<link>https://arxiv.org/abs/2412.05447</link>
<guid>https://arxiv.org/abs/2412.05447</guid>
<content:encoded><![CDATA[
arXiv:2412.05447v3 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) is one of the leading and most widely used techniques for enhancing LLM retrieval capabilities, but it still faces significant limitations in commercial use cases. RAG primarily relies on the query-chunk text-to-text similarity in the embedding space for retrieval and can fail to capture deeper semantic relationships across chunks, is highly sensitive to chunking strategies, and is prone to hallucinations. To address these challenges, we propose TOBUGraph, a graph-based retrieval framework that first constructs the knowledge graph from unstructured data dynamically and automatically. Using LLMs, TOBUGraph extracts structured knowledge and diverse relationships among data, going beyond RAG's text-to-text similarity. Retrieval is achieved through graph traversal, leveraging the extracted relationships and structures to enhance retrieval accuracy, eliminating the need for chunking configurations while reducing hallucination. We demonstrate TOBUGraph's effectiveness in TOBU, a real-world application in production for personal memory organization and retrieval. Our evaluation using real user data demonstrates that TOBUGraph outperforms multiple RAG implementations in both precision and recall, significantly improving user experience through improved retrieval accuracy.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large Models and AI Agents for Pervasive Deployment</title>
<link>https://arxiv.org/abs/2501.03265</link>
<guid>https://arxiv.org/abs/2501.03265</guid>
<content:encoded><![CDATA[
arXiv:2501.03265v2 Announce Type: replace 
Abstract: This article surveys Cognitive Edge Computing as a practical and methodical pathway for deploying reasoning-capable Large Language Models (LLMs) and autonomous AI agents on resource-constrained devices at the network edge. We present a unified, cognition-preserving framework spanning: (1) model optimization (quantization, sparsity, low-rank adaptation, distillation) aimed at retaining multi-step reasoning under tight memory/compute budgets; (2) system architecture (on-device inference, elastic offloading, cloud-edge collaboration) that trades off latency, energy, privacy, and capacity; and (3) adaptive intelligence (context compression, dynamic routing, federated personalization) that tailors computation to task difficulty and device constraints. We synthesize advances in efficient Transformer design, multimodal integration, hardware-aware compilation, privacy-preserving learning, and agentic tool use, and map them to edge-specific operating envelopes. We further outline a standardized evaluation protocol covering latency, throughput, energy per token, accuracy, robustness, privacy, and sustainability, with explicit measurement assumptions to enhance comparability. Remaining challenges include modality-aware reasoning benchmarks, transparent and reproducible energy reporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds. We conclude with practitioner guidelines for cross-layer co-design of algorithms, runtime, and hardware to deliver reliable, efficient, and privacy-preserving cognitive capabilities on edge devices.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiplayer Federated Learning: Reaching Equilibrium with Less Communication</title>
<link>https://arxiv.org/abs/2501.08263</link>
<guid>https://arxiv.org/abs/2501.08263</guid>
<content:encoded><![CDATA[
arXiv:2501.08263v2 Announce Type: replace 
Abstract: Traditional Federated Learning (FL) approaches assume collaborative clients with aligned objectives working towards a shared global model. However, in many real-world scenarios, clients act as rational players with individual objectives and strategic behaviors, a concept that existing FL frameworks are not equipped to adequately address. To bridge this gap, we introduce Multiplayer Federated Learning (MpFL), a novel framework that models the clients in the FL environment as players in a game-theoretic context, aiming to reach an equilibrium. In this scenario, each player tries to optimize their own utility function, which may not align with the collective goal. Within MpFL, we propose Per-Player Local Stochastic Gradient Descent (PEARL-SGD), an algorithm in which each player/client performs local updates independently and periodically communicates with other players. We theoretically analyze PEARL-SGD and prove that it reaches a neighborhood of equilibrium with less communication in the stochastic setup compared to its non-local counterpart. Finally, we verify our theoretical findings through numerical experiments.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Computational Cognitive Models using Large Language Models</title>
<link>https://arxiv.org/abs/2502.00879</link>
<guid>https://arxiv.org/abs/2502.00879</guid>
<content:encoded><![CDATA[
arXiv:2502.00879v3 Announce Type: replace 
Abstract: Computational cognitive models, which formalize theories of cognition, enable researchers to quantify cognitive processes and arbitrate between competing theories by fitting models to behavioral data. Traditionally, these models are handcrafted, which requires significant domain knowledge, coding expertise, and time investment. However, recent advances in machine learning offer solutions to these challenges. In particular, Large Language Models (LLMs) have demonstrated remarkable capabilities for in-context pattern recognition, leveraging knowledge from diverse domains to solve complex problems, and generating executable code that can be used to facilitate the generation of cognitive models. Building on this potential, we introduce a pipeline for Guided generation of Computational Cognitive Models (GeCCo). Given task instructions, participant data, and a template function, GeCCo prompts an LLM to propose candidate models, fits proposals to held-out data, and iteratively refines them based on feedback constructed from their predictive performance. We benchmark this approach across four different cognitive domains -- decision making, learning, planning, and memory -- using three open-source LLMs, spanning different model sizes, capacities, and families. On four human behavioral data sets, the LLM generated models that consistently matched or outperformed the best domain-specific models from the cognitive science literature. Taken together, our results suggest that LLMs can generate cognitive models with conceptually plausible theories that rival -- or even surpass -- the best models from the literature across diverse task domains.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Approximate Gaussian Inference in Classification</title>
<link>https://arxiv.org/abs/2502.03366</link>
<guid>https://arxiv.org/abs/2502.03366</guid>
<content:encoded><![CDATA[
arXiv:2502.03366v3 Announce Type: replace 
Abstract: In classification tasks, softmax functions are ubiquitously used as output activations to produce predictive probabilities. Such outputs only capture aleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian inference methods have been proposed. We develop a common formalism to describe such methods, which we view as outputting Gaussian distributions over the logit space. Predictives are then obtained as the expectations of the Gaussian distributions pushed forward through the softmax. However, such softmax Gaussian integrals cannot be solved analytically, and Monte Carlo (MC) approximations can be costly and noisy. We propose to replace the softmax activation by element-wise normCDF or sigmoid, which allows for the accurate sampling-free approximation of predictives. This also enables the approximation of the Gaussian pushforwards by Dirichlet distributions with moment matching. This approach entirely eliminates the runtime and memory overhead associated with MC sampling. We evaluate it combined with several approximate Gaussian inference methods (Laplace, HET, SNGP) on large- and small-scale datasets (ImageNet, CIFAR-100, CIFAR-10), demonstrating improved uncertainty quantification capabilities compared to softmax MC sampling. Our code is available at https://github.com/bmucsanyi/probit.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two-stage hybrid models for enhancing forecasting accuracy on heterogeneous time series</title>
<link>https://arxiv.org/abs/2502.08600</link>
<guid>https://arxiv.org/abs/2502.08600</guid>
<content:encoded><![CDATA[
arXiv:2502.08600v2 Announce Type: replace 
Abstract: A time series forecasting model--which is typically built on a single time series--is known as a local time series model (tsLM). In contrast, a forecasting model trained on multiple time series is referred to as a global time series model (tsGM). tsGMs can enhance forecasting accuracy and improve generalisation by learning cross-series information. As such, developing tsGMs has become a prominent research focus within the time series forecasting community. However, the benefits of tsGMs may not always be realised if the given set of time series is heterogeneous. While increasing model complexity can help tsGMs adapt to such a set of data, it can also increase the risk of overfitting and forecasting error. Additionally, the definition of homogeneity remains ambiguous in the literature. To address these challenges, this paper explores how to define data heterogeneity and proposes a two-stage modelling framework: At stage one, a tsGM is learnt to identify homogeneous patterns; and at stage two, tsLMs (e.g., ARIMA) or sub-tsGMs tailored to different groups are learnt to capture the heterogeneity. Numerical experiments on four open datasets demonstrate that the proposed approach significantly outperforms six state-of-the-art models. These results highlight its effectiveness in unlocking the full potential of global forecasting models for heterogeneous datasets.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science</title>
<link>https://arxiv.org/abs/2502.16395</link>
<guid>https://arxiv.org/abs/2502.16395</guid>
<content:encoded><![CDATA[
arXiv:2502.16395v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used to automate data analysis through executable code generation. Yet, data science tasks often admit multiple statistically valid solutions, e.g. different modeling strategies, making it critical to understand the reasoning behind analyses, not just their outcomes. While manual review of LLM-generated code can help ensure statistical soundness, it is labor-intensive and requires expertise. A more scalable approach is to evaluate the underlying workflows-the logical plans guiding code generation. However, it remains unclear how to assess whether an LLM-generated workflow supports reproducible implementations.
  To address this, we present AIRepr, an Analyst-Inspector framework for automatically evaluating and improving the reproducibility of LLM-generated data analysis workflows. Our framework is grounded in statistical principles and supports scalable, automated assessment. We introduce two novel reproducibility-enhancing prompting strategies and benchmark them against standard prompting across 15 analyst-inspector LLM pairs and 1,032 tasks from three public benchmarks. Our findings show that workflows with higher reproducibility also yield more accurate analyses, and that reproducibility-enhancing prompts substantially improve both metrics. This work provides a foundation for transparent, reliable, and efficient human-AI collaboration in data science. Our code is publicly available.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models</title>
<link>https://arxiv.org/abs/2502.17206</link>
<guid>https://arxiv.org/abs/2502.17206</guid>
<content:encoded><![CDATA[
arXiv:2502.17206v2 Announce Type: replace 
Abstract: Transformer models typically calculate attention matrices using dot products, which have limitations when capturing nonlinear relationships between embedding vectors. We propose Neural Attention, a technique that replaces dot products with feed-forward networks, enabling a more expressive representation of relationships between tokens. This approach modifies only the attention matrix calculation while preserving the matrix dimensions, making it easily adaptable to existing transformer-based architectures. We provide a detailed mathematical justification for why Neural Attention increases representational capacity and conduct controlled experiments to validate this claim. When comparing Neural Attention and Dot-Product Attention, NLP experiments on WikiText-103 show a reduction in perplexity of over 2 percent. Similarly, experiments on CIFAR-10 and CIFAR-100 show improvements in accuracy of more than 4 percentage points for image classification tasks. While Neural Attention introduces higher computational demands, we develop techniques to mitigate these challenges, ensuring practical usability without sacrificing the increased expressivity it provides. This work establishes Neural Attention as an effective means of enhancing the predictive capabilities of transformer models across a variety of applications. The code for all experiments is available at https://github.com/awayfromzel/neural-attention-research.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Greedy Algorithm for Structured Bandits: A Sharp Characterization of Asymptotic Success / Failure</title>
<link>https://arxiv.org/abs/2503.04010</link>
<guid>https://arxiv.org/abs/2503.04010</guid>
<content:encoded><![CDATA[
arXiv:2503.04010v3 Announce Type: replace 
Abstract: We study the greedy (exploitation-only) algorithm in bandit problems with a known reward structure. We allow arbitrary finite reward structures, while prior work focused on a few specific ones. We fully characterize when the greedy algorithm asymptotically succeeds or fails, in the sense of sublinear vs. linear regret as a function of time. Our characterization identifies a partial identifiability property of the problem instance as the necessary and sufficient condition for the asymptotic success. Notably, once this property holds, the problem becomes easy -- any algorithm will succeed (in the same sense as above), provided it satisfies a mild non-degeneracy condition. Our characterization extends to contextual bandits and interactive decision-making with arbitrary feedback. Examples demonstrating broad applicability and extensions to infinite reward structures are provided.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large language models as uncertainty-calibrated optimizers for experimental discovery</title>
<link>https://arxiv.org/abs/2504.06265</link>
<guid>https://arxiv.org/abs/2504.06265</guid>
<content:encoded><![CDATA[
arXiv:2504.06265v3 Announce Type: replace 
Abstract: Scientific discovery increasingly depends on efficient experimental optimization to navigate vast design spaces under time and resource constraints. Traditional approaches often require extensive domain expertise and feature engineering. While large language models, with their vast scientific knowledge, circumvent the feature engineering limitations, they lack the calibrated uncertainty estimates required for high-stakes decision making. Hence, current optimization methods force a choice between domain knowledge and reliability, with no principled approach that affords both. In this work, we show that training language models through the uncertainty-aware objectives of traditional optimization methods enables their use as reliable optimizers guided by natural language. By teaching LLMs from experimental outcomes under uncertainty, we transform their overconfidence from a fundamental limitation into a precise calibration mechanism. Applied to Buchwald-Hartwig reactions, a cornerstone of pharmaceutical synthesis, our method nearly doubles the discovery rate of high-yielding reaction conditions, from 24% to 43% in 50 experimental iterations starting from 10 unsuccessful conditions. Across 19 diverse optimization problems spanning organic synthesis, materials science and catalysis, process chemistry, and molecular design, our approach ranks first on average, establishing a new paradigm for reliable, uncertainty-guided optimization with LLMs. Our approach can accelerate discovery by lowering the barrier to using powerful optimization methods, replacing the need for domain-specific feature engineering with more accessible natural language interfaces. These findings highlight that ensuring reliability through principled uncertainty quantification is critical for realizing the full potential of AI-guided experimentation.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Anytime Reasoning via Budget Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2505.13438</link>
<guid>https://arxiv.org/abs/2505.13438</guid>
<content:encoded><![CDATA[
arXiv:2505.13438v3 Announce Type: replace 
Abstract: Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better Neural Network Expressivity: Subdividing the Simplex</title>
<link>https://arxiv.org/abs/2505.14338</link>
<guid>https://arxiv.org/abs/2505.14338</guid>
<content:encoded><![CDATA[
arXiv:2505.14338v2 Announce Type: replace 
Abstract: This work studies the expressivity of ReLU neural networks with a focus on their depth. A sequence of previous works showed that $\lceil \log_2(n+1) \rceil$ hidden layers are sufficient to compute all continuous piecewise linear (CPWL) functions on $\mathbb{R}^n$. Hertrich, Basu, Di Summa, and Skutella (NeurIPS'21 / SIDMA'23) conjectured that this result is optimal in the sense that there are CPWL functions on $\mathbb{R}^n$, like the maximum function, that require this depth. We disprove the conjecture and show that $\lceil\log_3(n-1)\rceil+1$ hidden layers are sufficient to compute all CPWL functions on $\mathbb{R}^n$.
  A key step in the proof is that ReLU neural networks with two hidden layers can exactly represent the maximum function of five inputs. More generally, we show that $\lceil\log_3(n-2)\rceil+1$ hidden layers are sufficient to compute the maximum of $n\geq 4$ numbers. Our constructions almost match the $\lceil\log_3(n)\rceil$ lower bound of Averkov, Hojny, and Merkert (ICLR'25) in the special case of ReLU networks with weights that are decimal fractions. The constructions have a geometric interpretation via polyhedral subdivisions of the simplex into ``easier'' polytopes.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Are Concepts Erased From Diffusion Models?</title>
<link>https://arxiv.org/abs/2505.17013</link>
<guid>https://arxiv.org/abs/2505.17013</guid>
<content:encoded><![CDATA[
arXiv:2505.17013v5 Announce Type: replace 
Abstract: In concept erasure, a model is modified to selectively prevent it from generating a target concept. Despite the rapid development of new methods, it remains unclear how thoroughly these approaches remove the target concept from the model. We begin by proposing two conceptual models for the erasure mechanism in diffusion models: (i) interfering with the model's internal guidance processes, and (ii) reducing the unconditional likelihood of generating the target concept, potentially removing it entirely. To assess whether a concept has been truly erased from the model, we introduce a comprehensive suite of independent probing techniques: supplying visual context, modifying the diffusion trajectory, applying classifier guidance, and analyzing the model's alternative generations that emerge in place of the erased concept. Our results shed light on the value of exploring concept erasure robustness outside of adversarial text inputs, and emphasize the importance of comprehensive evaluations for erasure in diffusion models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models</title>
<link>https://arxiv.org/abs/2505.21005</link>
<guid>https://arxiv.org/abs/2505.21005</guid>
<content:encoded><![CDATA[
arXiv:2505.21005v2 Announce Type: replace 
Abstract: Score-based diffusion models (SBDMs) are powerful amortized samplers for Boltzmann distributions; however, imperfect score estimates bias downstream Monte Carlo estimates. Classical importance sampling (IS) can correct this bias, but computing exact likelihoods requires solving the probability-flow ordinary differential equation (PF-ODE), a procedure that is prohibitively costly and scales poorly with dimensionality. We introduce Variance-Tuned Diffusion Importance Sampling (VT-DIS), a lightweight post-training method that adapts the per-step noise covariance of a pretrained SBDM by minimizing the $\alpha$-divergence ($\alpha=2$) between its forward diffusion and reverse denoising trajectories. VT-DIS assigns a single trajectory-wise importance weight to the joint forward-reverse process, yielding unbiased expectation estimates at test time with negligible overhead compared to standard sampling. On the DW-4, LJ-13, and alanine-dipeptide benchmarks, VT-DIS achieves effective sample sizes of approximately 80 %, 35 %, and 3.5 %, respectively, while using only a fraction of the computational budget required by vanilla diffusion + IS or PF-ODE-based IS.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget</title>
<link>https://arxiv.org/abs/2505.24149</link>
<guid>https://arxiv.org/abs/2505.24149</guid>
<content:encoded><![CDATA[
arXiv:2505.24149v2 Announce Type: replace 
Abstract: Machine learning (ML) algorithms deployed in real-world environments are often faced with the challenge of adapting models to concept drift, where the task data distributions are shifting over time. The problem becomes even more difficult when model performance must be maintained under adherence to strict resource constraints. Existing solutions often depend on drift-detection methods that produce high computational overhead for resource-constrained environments, and fail to provide strict guarantees on resource usage or theoretical performance assurances. To address these shortcomings, we propose RCCDA: a dynamic model update policy that optimizes ML training dynamics while ensuring compliance to predefined resource constraints, utilizing only past loss information and a tunable drift threshold. In developing our policy, we analytically characterize the evolution of model loss under concept drift with arbitrary training update decisions. Integrating these results into a Lyapunov drift-plus-penalty framework produces a lightweight greedy-optimal policy that provably limits update frequency and cost. Experimental results on four domain generalization datasets demonstrate that our policy outperforms baseline methods in inference accuracy while adhering to strict resource constraints under several schedules of concept drift, making our solution uniquely suited for real-time ML deployments.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning of Population Dynamics: Inverse Optimization Meets JKO Scheme</title>
<link>https://arxiv.org/abs/2506.01502</link>
<guid>https://arxiv.org/abs/2506.01502</guid>
<content:encoded><![CDATA[
arXiv:2506.01502v2 Announce Type: replace 
Abstract: Learning population dynamics involves recovering the underlying process that governs particle evolution, given evolutionary snapshots of samples at discrete time points. Recent methods frame this as an energy minimization problem in probability space and leverage the celebrated JKO scheme for efficient time discretization. In this work, we introduce $\texttt{iJKOnet}$, an approach that combines the JKO framework with inverse optimization techniques to learn population dynamics. Our method relies on a conventional $\textit{end-to-end}$ adversarial training procedure and does not require restrictive architectural choices, e.g., input-convex neural networks. We establish theoretical guarantees for our methodology and demonstrate improved performance over prior JKO-based methods.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning</title>
<link>https://arxiv.org/abs/2506.03777</link>
<guid>https://arxiv.org/abs/2506.03777</guid>
<content:encoded><![CDATA[
arXiv:2506.03777v2 Announce Type: replace 
Abstract: With the emerging application of Federated Learning (FL) in decision-making scenarios, it is imperative to regulate model fairness to prevent disparities across sensitive groups (e.g., female, male). Current research predominantly focuses on two concepts of group fairness within FL: Global Fairness (overall model disparity across all clients) and Local Fairness (the disparity within each client). However, the non-decomposable, non-differentiable nature of fairness criteria poses two fundamental, unresolved challenges for fair FL: (i) Harmonizing global and local fairness, especially in multi-class setting; (ii) Enabling a controllable, optimal accuracy-fairness trade-off. To tackle these challenges, we propose a novel controllable federated group-fairness calibration framework, named FedFACT. FedFACT identifies the Bayes-optimal classifiers under both global and local fairness constraints, yielding models with minimal performance decline while guaranteeing fairness. Building on the characterization of the optimal fair classifiers, we reformulate fair federated learning as a personalized cost-sensitive learning problem for in-processing and a bi-level optimization for post-processing. Theoretically, we provide convergence and generalization guarantees for FedFACT to approach the near-optimal accuracy under given fairness levels. Extensive experiments on multiple datasets across various data heterogeneity demonstrate that FedFACT consistently outperforms baselines in balancing accuracy and global-local fairness.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference-Time Hyper-Scaling with KV Cache Compression</title>
<link>https://arxiv.org/abs/2506.05345</link>
<guid>https://arxiv.org/abs/2506.05345</guid>
<content:encoded><![CDATA[
arXiv:2506.05345v2 Announce Type: replace 
Abstract: Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8$\times$ compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference latency and memory load. For instance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and 9.7 on LiveCodeBench on average for an equivalent number of memory reads.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Prediction Adaptive to Unknown Subpopulation Shifts</title>
<link>https://arxiv.org/abs/2506.05583</link>
<guid>https://arxiv.org/abs/2506.05583</guid>
<content:encoded><![CDATA[
arXiv:2506.05583v2 Announce Type: replace 
Abstract: Conformal prediction is widely used to equip black-box machine learning models with uncertainty quantification, offering formal coverage guarantees under exchangeable data. However, these guarantees fail when faced with subpopulation shifts, where the test environment contains a different mix of subpopulations than the calibration data. In this work, we focus on unknown subpopulation shifts where we are not given group-information i.e. the subpopulation labels of datapoints have to be inferred. We propose new methods that provably adapt conformal prediction to such shifts, ensuring valid coverage without explicit knowledge of subpopulation structure. While existing methods in similar setups assume perfect subpopulation labels, our framework explicitly relaxes this requirement and characterizes conditions where formal coverage guarantees remain feasible. Further, our algorithms scale to high-dimensional settings and remain practical in realistic machine learning tasks. Extensive experiments on vision (with vision transformers) and language (with large language models) benchmarks demonstrate that our methods reliably maintain coverage and effectively control risks in scenarios where standard conformal prediction fails.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-Compressing Networks</title>
<link>https://arxiv.org/abs/2506.09714</link>
<guid>https://arxiv.org/abs/2506.09714</guid>
<content:encoded><![CDATA[
arXiv:2506.09714v3 Announce Type: replace 
Abstract: Deep neural networks with short residual connections have demonstrated remarkable success across domains, but increasing depth often introduces computational redundancy without corresponding improvements in representation quality. We introduce Auto-Compressing Networks (ACNs), an architectural variant where additive long feedforward connections from each layer to the output replace traditional short residual connections. By analyzing the distinct dynamics induced by this modification, we reveal a unique property we coin as auto-compression, the ability of a network to organically compress information during training with gradient descent, through architectural design alone. Through auto-compression, information is dynamically "pushed" into early layers during training, enhancing their representational quality and revealing potential redundancy in deeper ones. We theoretically show that this property emerges from layer-wise training patterns present in ACNs, where layers are dynamically utilized during training based on task requirements. We also find that ACNs exhibit enhanced noise robustness compared to residual networks, superior performance in low-data settings, improved transfer learning capabilities, and mitigate catastrophic forgetting suggesting that they learn representations that generalize better despite using fewer parameters. Our results demonstrate up to 18% reduction in catastrophic forgetting and 30-80% architectural compression while maintaining accuracy across vision transformers, MLP-mixers, and BERT architectures. These findings establish ACNs as a practical approach to developing efficient neural architectures that automatically adapt their computational footprint to task complexity, while learning robust representations suitable for noisy real-world tasks and continual learning scenarios.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering</title>
<link>https://arxiv.org/abs/2506.10751</link>
<guid>https://arxiv.org/abs/2506.10751</guid>
<content:encoded><![CDATA[
arXiv:2506.10751v2 Announce Type: replace 
Abstract: Automated question answering (QA) over electronic health records (EHRs) can bridge critical information gaps for clinicians and patients, yet it demands both precise evidence retrieval and faithful answer generation under limited supervision. In this work, we present Neural, the runner-up in the BioNLP 2025 ArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method decouples the task into (1) sentence-level evidence identification and (2) answer synthesis with explicit citations. For each stage, we automatically explore the prompt space with DSPy's MIPROv2 optimizer, jointly tuning instructions and few-shot demonstrations on the development set. A self-consistency voting scheme further improves evidence recall without sacrificing precision. On the hidden test set, our method attains an overall score of 51.5, placing second stage while outperforming standard zero-shot and few-shot prompting by over 20 and 10 points, respectively. These results indicate that data-driven prompt optimization is a cost-effective alternative to model fine-tuning for high-stakes clinical QA, advancing the reliability of AI assistants in healthcare.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentially Private Bilevel Optimization: Efficient Algorithms with Near-Optimal Rates</title>
<link>https://arxiv.org/abs/2506.12994</link>
<guid>https://arxiv.org/abs/2506.12994</guid>
<content:encoded><![CDATA[
arXiv:2506.12994v2 Announce Type: replace 
Abstract: Bilevel optimization, in which one optimization problem is nested inside another, underlies many machine learning applications with a hierarchical structure -- such as meta-learning and hyperparameter optimization. Such applications often involve sensitive training data, raising pressing concerns about individual privacy. Motivated by this, we study differentially private bilevel optimization. We first focus on settings where the outer-level objective is convex, and provide novel upper and lower bounds on the excess empirical risk for both pure and approximate differential privacy. These bounds are nearly tight and essentially match the optimal rates for standard single-level differentially private ERM, up to additional terms that capture the intrinsic complexity of the nested bilevel structure. We also provide population loss bounds for bilevel stochastic optimization. The bounds are achieved in polynomial time via efficient implementations of the exponential and regularized exponential mechanisms. A key technical contribution is a new method and analysis of log-concave sampling under inexact function evaluations, which may be of independent interest. In the non-convex setting, we develop novel algorithms with state-of-the-art rates for privately finding approximate stationary points. Notably, our bounds do not depend on the dimension of the inner problem.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities</title>
<link>https://arxiv.org/abs/2506.16471</link>
<guid>https://arxiv.org/abs/2506.16471</guid>
<content:encoded><![CDATA[
arXiv:2506.16471v2 Announce Type: replace 
Abstract: Sampling efficiently from a target unnormalized probability density remains a core challenge, with relevance across countless high-impact scientific applications. A promising approach towards this challenge is the design of amortized samplers that borrow key ideas, such as probability path design, from state-of-the-art generative diffusion models. However, all existing diffusion-based samplers remain unable to draw samples from distributions at the scale of even simple molecular systems. In this paper, we propose Progressive Inference-Time Annealing (PITA), a novel framework to learn diffusion-based samplers that combines two complementary interpolation techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion smoothing. PITA trains a sequence of diffusion models from high to low temperatures by sequentially training each model at progressively higher temperatures, leveraging engineered easy access to samples of the temperature-annealed target density. In the subsequent step, PITA enables simulating the trained diffusion model to procure training samples at a lower temperature for the next diffusion model through inference-time annealing using a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA enables, for the first time, equilibrium sampling of N-body particle systems, Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically lower energy function evaluations. Code available at: https://github.com/taraak/pita
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimism Without Regularization: Constant Regret in Zero-Sum Games</title>
<link>https://arxiv.org/abs/2506.16736</link>
<guid>https://arxiv.org/abs/2506.16736</guid>
<content:encoded><![CDATA[
arXiv:2506.16736v2 Announce Type: replace 
Abstract: This paper studies the optimistic variant of Fictitious Play for learning in two-player zero-sum games. While it is known that Optimistic FTRL -- a regularized algorithm with a bounded stepsize parameter -- obtains constant regret in this setting, we show for the first time that similar, optimal rates are also achievable without regularization: we prove for two-strategy games that Optimistic Fictitious Play (using any tiebreaking rule) obtains only constant regret, providing surprising new evidence on the ability of non-no-regret algorithms for fast learning in games. Our proof technique leverages a geometric view of Optimistic Fictitious Play in the dual space of payoff vectors, where we show a certain energy function of the iterates remains bounded over time. Additionally, we also prove a regret lower bound of $\Omega(\sqrt{T})$ for Alternating Fictitious Play. In the unregularized regime, this separates the ability of optimism and alternation in achieving $o(\sqrt{T})$ regret.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design</title>
<link>https://arxiv.org/abs/2506.21158</link>
<guid>https://arxiv.org/abs/2506.21158</guid>
<content:encoded><![CDATA[
arXiv:2506.21158v2 Announce Type: replace 
Abstract: In many real-world applications, evaluating the quality of instances is costly and time-consuming, e.g., human feedback and physics simulations, in contrast to proposing new instances. In particular, this is even more critical in reinforcement learning, since it relies on interactions with the environment (i.e., new instances) that must be evaluated to provide a reward signal for learning. At the same time, performing sufficient exploration is crucial in reinforcement learning to find high-rewarding solutions, meaning that the agent should observe and learn from a diverse set of experiences to find different solutions. Thus, we argue that learning from a diverse mini-batch of experiences can have a large impact on the exploration and help mitigate mode collapse.In this paper, we introduce mini-batch diversification for reinforcement learning and study this framework in the context of a real-world problem, namely, drug discovery. We extensively evaluate how our proposed framework can enhance the effectiveness of chemical exploration in de novo drug design, where finding diverse and high-quality solutions is crucial. Our experiments demonstrate that our proposed diverse mini-batch selection framework can substantially enhance the diversity of solutions while maintaining high-quality solutions. In drug discovery, such an outcome can potentially lead to fulfilling unmet medical needs faster.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Greedy Equivalence Search</title>
<link>https://arxiv.org/abs/2506.22331</link>
<guid>https://arxiv.org/abs/2506.22331</guid>
<content:encoded><![CDATA[
arXiv:2506.22331v2 Announce Type: replace 
Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal discovery from observational data. In the sample limit, it recovers the Markov equivalence class of graphs that describe the data. Still, it faces two challenges in practice: computational cost and finite-sample accuracy. In this paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that retains its theoretical guarantees while partially addressing these limitations. LGES modifies the greedy step; rather than always applying the highest-scoring insertion, it avoids edge insertions between variables for which the score implies some conditional independence. This more targeted search yields up to a \(10\)-fold speed-up and a substantial reduction in structural error relative to GES. Moreover, LGES can guide the search using prior knowledge, and can correct this knowledge when contradicted by data. Finally, LGES can use interventional data to refine the learned observational equivalence class. We prove that LGES recovers the true equivalence class in the sample limit, even with misspecified knowledge. Experiments demonstrate that LGES outperforms GES and other baselines in speed, accuracy, and robustness to misspecified knowledge. Our code is available at https://github.com/CausalAILab/lges.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Information Pursuit for Interactively Guiding Large Language Models</title>
<link>https://arxiv.org/abs/2507.03279</link>
<guid>https://arxiv.org/abs/2507.03279</guid>
<content:encoded><![CDATA[
arXiv:2507.03279v2 Announce Type: replace 
Abstract: A significant use case of instruction-finetuned Large Language Models (LLMs) is to solve question-answering tasks interactively. In this setting, an LLM agent is tasked with making a prediction by sequentially querying relevant information from the user, as opposed to a single-turn conversation. This paper explores sequential querying strategies that aim to minimize the expected number of queries. One such strategy is Information Pursuit (IP), a greedy algorithm that at each iteration selects the query that maximizes information gain or equivalently minimizes uncertainty. However, obtaining accurate estimates of mutual information or conditional entropy for LLMs is very difficult in practice due to over- or under-confident LLM proba- bilities, which leads to suboptimal query selection and predictive performance. To better estimate the uncertainty at each iteration, we propose Conformal Information Pursuit (C-IP), an alternative approach to sequential information gain based on conformal prediction sets. More specifically, C-IP leverages a relationship between prediction sets and conditional entropy at each iteration to estimate uncertainty based on the average size of conformal prediction sets. In contrast to conditional entropy, we find that conformal prediction sets are a distribution-free and robust method of measuring uncertainty. Experiments with 20 Questions show that C-IP obtains better predictive performance and shorter query-answer chains compared to previous approaches to IP and uncertainty-based chain-of-thought methods. Furthermore, extending to an interactive medical setting between a doctor and a patient on the MediQ dataset, C-IP achieves competitive performance with direct single-turn prediction while offering greater interpretability.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Learning</title>
<link>https://arxiv.org/abs/2507.05636</link>
<guid>https://arxiv.org/abs/2507.05636</guid>
<content:encoded><![CDATA[
arXiv:2507.05636v2 Announce Type: replace 
Abstract: Graph learning has rapidly evolved into a critical subfield of machine learning and artificial intelligence (AI). Its development began with early graph-theoretic methods, gaining significant momentum with the advent of graph neural networks (GNNs). Over the past decade, progress in scalable architectures, dynamic graph modeling, multimodal learning, generative AI, explainable AI (XAI), and responsible AI has broadened the applicability of graph learning to various challenging environments. Graph learning is significant due to its ability to model complex, non-Euclidean relationships that traditional machine learning struggles to capture, thus better supporting real-world applications ranging from drug discovery and fraud detection to recommender systems and scientific reasoning. However, challenges like scalability, generalization, heterogeneity, interpretability, and trustworthiness must be addressed to unlock its full potential. This survey provides a comprehensive introduction to graph learning, focusing on key dimensions including scalable, temporal, multimodal, generative, explainable, and responsible graph learning. We review state-of-the-art techniques for efficiently handling large-scale graphs, capturing dynamic temporal dependencies, integrating heterogeneous data modalities, generating novel graph samples, and enhancing interpretability to foster trust and transparency. We also explore ethical considerations, such as privacy and fairness, to ensure responsible deployment of graph learning models. Additionally, we identify and discuss emerging topics, highlighting recent integration of graph learning and other AI paradigms and offering insights into future directions. This survey serves as a valuable resource for researchers and practitioners seeking to navigate the rapidly evolving landscape of graph learning.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Latent Graph Geometry via Fixed-Point Schr\"odinger-Type Activation: A Theoretical Study</title>
<link>https://arxiv.org/abs/2507.20088</link>
<guid>https://arxiv.org/abs/2507.20088</guid>
<content:encoded><![CDATA[
arXiv:2507.20088v2 Announce Type: replace 
Abstract: We develop a unified theoretical framework for neural architectures whose internal representations evolve as stationary states of dissipative Schr\"odinger-type dynamics on learned latent graphs. Each layer is defined by a fixed-point Schr\"odinger-type equation depending on a weighted Laplacian encoding latent geometry and a convex local potential. We prove existence, uniqueness, and smooth dependence of equilibria, and show that the dynamics are equivalent under the Bloch map to norm-preserving Landau--Lifshitz flows. Training over graph weights and topology is formulated as stochastic optimization on a stratified moduli space of graphs equipped with a natural K\"{a}hler--Hessian metric, ensuring convergence and differentiability across strata. We derive generalization bounds -- PAC-Bayes, stability, and Rademacher complexity -- in terms of geometric quantities such as edge count, maximal degree, and Gromov--Hausdorff distortion, establishing that sparsity and geometric regularity control capacity. Feed-forward composition of stationary layers is proven equivalent to a single global stationary diffusion on a supra-graph; backpropagation is its adjoint stationary system. Finally, directed and vector-valued extensions are represented as sheaf Laplacians with unitary connections, unifying scalar graph, directed, and sheaf-based architectures. The resulting model class provides a compact, geometrically interpretable, and analytically tractable foundation for learning latent graph geometry via fixed-point Schr\"odinger-type activations.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advanced Hybrid Transformer LSTM Technique with Attention and TS Mixer for Drilling Rate of Penetration Prediction</title>
<link>https://arxiv.org/abs/2508.05210</link>
<guid>https://arxiv.org/abs/2508.05210</guid>
<content:encoded><![CDATA[
arXiv:2508.05210v3 Announce Type: replace 
Abstract: Rate of Penetration (ROP) prediction is critical for drilling optimization yet remains challenging due to the nonlinear, dynamic, and heterogeneous characteristics of drilling data. Conventional empirical, physics-based, and standard machine learning models rely on oversimplified assumptions or intensive feature engineering, constraining their capacity to model long-term dependencies and intricate feature interactions. To address these issues, this study presents a new deep learning Hybrid LSTM-Trans-Mixer-Att framework that first processes input data through a customized Long Short-Term Memory (LSTM) network to capture multi-scale temporal dependencies aligned with drilling cycles. Subsequently, an Enhanced Transformer encoder with drilling-specific positional encodings and real-time optimization refines the features. Concurrently, a parallel Time-Series Mixer (TS-Mixer) block introduced facilitates efficient cross-feature interaction modeling of static and categorical parameters, including lithological indices and mud properties. The feature representations extracted from the Enhanced Transformer and TS-Mixer modules are integrated through a dedicated fusion layer. Finally, an adaptive attention mechanism then dynamically assigns contextual weights to salient features, enhancing discriminative representation learning and enabling high-fidelity ROP prediction. The proposed framework combines sequential memory, static feature interactions, global context learning, and dynamic feature weighting, providing a comprehensive solution for the heterogeneous and event-driven nature of drilling dynamics. Experimental validation on real-world drilling datasets demonstrates superior performance, achieving an Rsquare of 0.9991 and a MAPE of 1.447%, significantly outperforming existing baseline and hybrid models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Perfectly Truthful Calibration Measure</title>
<link>https://arxiv.org/abs/2508.13100</link>
<guid>https://arxiv.org/abs/2508.13100</guid>
<content:encoded><![CDATA[
arXiv:2508.13100v2 Announce Type: replace 
Abstract: Calibration requires that predictions are conditionally unbiased and, therefore, reliably interpretable as probabilities. A calibration measure quantifies how far a predictor is from perfect calibration. As introduced by Haghtalab et al. (2024), a calibration measure is truthful if it is minimized in expectation when a predictor outputs the ground-truth probabilities. Predicting the true probabilities guarantees perfect calibration, but in reality, when calibration is evaluated on a random sample, all known calibration measures incentivize predictors to lie in order to appear more calibrated. Such lack of truthfulness motivated Haghtalab et al. (2024) and Qiao and Zhao (2025) to construct approximately truthful calibration measures in the sequential prediction setting, but no perfectly truthful calibration measure was known to exist even in the more basic batch setting.
  We design a simple, perfectly and strictly truthful, sound and complete calibration measure in the batch setting: averaged two-bin calibration error (ATB). ATB is quadratically related to two existing calibration measures: the smooth calibration error smCal and the lower distance to calibration distCal. The simplicity in our definition of ATB makes it efficient and straightforward to compute, allowing us to give the first linear-time calibration testing algorithm, improving a result of Hu et al. (2024). We also introduce a general recipe for constructing truthful measures based on the variance additivity of independent random variables, which proves the truthfulness of ATB as a special case and allows us to construct other truthful calibration measures such as quantile-binned l_2-ECE.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ethics-Aware Safe Reinforcement Learning for Rare-Event Risk Control in Interactive Urban Driving</title>
<link>https://arxiv.org/abs/2508.14926</link>
<guid>https://arxiv.org/abs/2508.14926</guid>
<content:encoded><![CDATA[
arXiv:2508.14926v3 Announce Type: replace 
Abstract: Autonomous vehicles hold great promise for reducing traffic fatalities and improving transportation efficiency, yet their widespread adoption hinges on embedding credible and transparent ethical reasoning into routine and emergency maneuvers, particularly to protect vulnerable road users (VRUs) such as pedestrians and cyclists. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that augments standard driving objectives with ethics-aware cost signals. At the decision level, a Safe RL agent is trained using a composite ethical risk cost, combining collision probability and harm severity, to generate high-level motion targets. A dynamic, risk-sensitive Prioritized Experience Replay mechanism amplifies learning from rare but critical, high-risk events. At the execution level, polynomial path planning coupled with Proportional-Integral-Derivative (PID) and Stanley controllers translates these targets into smooth, feasible trajectories, ensuring both accuracy and comfort. We train and validate our approach on closed-loop simulation environments derived from large-scale, real-world traffic datasets encompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that it outperforms baseline methods in reducing risk to others while maintaining ego performance and comfort. This work provides a reproducible benchmark for Safe RL with explicitly ethics-aware objectives in human-mixed traffic scenarios. Our results highlight the potential of combining formal control theory and data-driven learning to advance ethically accountable autonomy that explicitly protects those most at risk in urban traffic environments. Across two interactive benchmarks and five random seeds, our policy decreases conflict frequency by 25-45% compared to matched task successes while maintaining comfort metrics within 5%.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boardwalk: Towards a Framework for Creating Board Games with LLMs</title>
<link>https://arxiv.org/abs/2508.16447</link>
<guid>https://arxiv.org/abs/2508.16447</guid>
<content:encoded><![CDATA[
arXiv:2508.16447v2 Announce Type: replace 
Abstract: Implementing board games in code can be a time-consuming task. However, Large Language Models (LLMs) have been proven effective at generating code for domain-specific tasks with simple contextual information. We aim to investigate whether LLMs can implement digital versions of board games from rules described in natural language. This would be a step towards an LLM-assisted framework for quick board game code generation. We expect to determine the main challenges for LLMs to implement the board games, and how different approaches and models compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek and ChatGPT) with coding a selection of 12 popular and obscure games in free-form and within Boardwalk, our proposed General Game Playing API. We anonymize the games and components to avoid evoking pre-trained LLM knowledge. The implementations are tested for playability and rule compliance. We evaluate success rate and common errors across LLMs and game popularity. Our approach proves viable, with the best performing model, Claude 3.7 Sonnet, yielding 55.6\% of games without any errors. While compliance with the API increases error frequency, the severity of errors is more significantly dependent on the LLM. We outline future steps for creating a framework to integrate this process, making the elaboration of board games more accessible.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Matters in Data for DPO?</title>
<link>https://arxiv.org/abs/2508.18312</link>
<guid>https://arxiv.org/abs/2508.18312</guid>
<content:encoded><![CDATA[
arXiv:2508.18312v3 Announce Type: replace 
Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective approach for aligning large language models (LLMs) with human preferences, bypassing the need for a learned reward model. Despite its growing adoption, a fundamental question remains open: what characteristics of preference data are most critical for DPO performance? In this work, we provide a systematic study of how preference data distribution influences DPO, from both theoretical and empirical perspectives. We show that the quality of chosen responses plays a dominant role in optimizing the DPO objective, while the quality of rejected responses may have relatively limited impact. Our theoretical analysis characterizes the optimal response distribution under DPO and reveals how contrastiveness between responses helps primarily by improving the chosen samples. We further study an online DPO setting and show it effectively reduces to supervised fine-tuning on the chosen responses. Extensive experiments across diverse tasks confirm our findings: improving the quality of chosen responses consistently boosts performance regardless of the quality of the rejected responses. We also investigate the benefit of mixing the on-policy data. Our results interpret the mechanism behind some widely adopted strategies and offer practical insights for constructing high-impact preference datasets for LLM alignment.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeCopilot</title>
<link>https://arxiv.org/abs/2509.00616</link>
<guid>https://arxiv.org/abs/2509.00616</guid>
<content:encoded><![CDATA[
arXiv:2509.00616v3 Announce Type: replace 
Abstract: We introduce TimeCopilot, the first open-source agentic framework for forecasting that combines multiple Time Series Foundation Models (TSFMs) with Large Language Models (LLMs) through a single unified API. TimeCopilot automates the forecasting pipeline: feature analysis, model selection, cross-validation, and forecast generation, while providing natural language explanations and supporting direct queries about the future. The framework is LLM-agnostic, compatible with both commercial and open-source models, and supports ensembles across diverse forecasting families. Results on the large-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art probabilistic forecasting performance at low cost. Our framework provides a practical foundation for reproducible, explainable, and accessible agentic forecasting systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence</title>
<link>https://arxiv.org/abs/2509.03505</link>
<guid>https://arxiv.org/abs/2509.03505</guid>
<content:encoded><![CDATA[
arXiv:2509.03505v2 Announce Type: replace 
Abstract: We argue that progress toward general intelligence requires complementary foundation models grounded in language, the physical world, and structured data. This report presents LimiX-16M and LimiX-2M, two instantiations of our large structured-data models (LDMs). Both models treat structured data as a joint distribution over variables and missingness, thus capable of addressing a wide range of tabular tasks through query-based conditional prediction via a single model. They are pretrained using masked joint-distribution modeling with an episodic, context-conditional objective, supporting rapid, training-free adaptation at inference. We evaluate LimiX models across 11 large structured-data benchmarks with broad regimes of sample size, feature dimensionality, class number, categorical-to-numerical feature ratio, missingness, and sample-to-feature ratios. LimiX-16M consistently surpasses strong baselines, as shown in Figure 1 and Figure 2. The superiority holds across a wide range of tasks, such as classification, regression, missing value imputation, and data generation, often by substantial margins, while avoiding task-specific architectures or bespoke training per task. Notably, LimiX-2M delivers strong results under tight compute and memory budgets. We also present the first scaling law study for LDMs, revealing how data and model scaling jointly influence downstream performance and offering quantitative guidance for tabular foundation modeling. All LimiX models are publicly accessible under Apache 2.0.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures</title>
<link>https://arxiv.org/abs/2509.10227</link>
<guid>https://arxiv.org/abs/2509.10227</guid>
<content:encoded><![CDATA[
arXiv:2509.10227v2 Announce Type: replace 
Abstract: Fatigue life prediction is essential in both the design and operational phases of any aircraft, and in this sense safety in the aerospace industry requires early detection of fatigue cracks to prevent in-flight failures. Robust and precise fatigue life predictors are thus essential to ensure safety. Traditional engineering methods, while reliable, are time consuming and involve complex workflows, including steps such as conducting several Finite Element Method (FEM) simulations, deriving the expected loading spectrum, and applying cycle counting techniques like peak-valley or rainflow counting. These steps often require collaboration between multiple teams and tools, added to the computational time and effort required to achieve fatigue life predictions. Machine learning (ML) offers a promising complement to traditional fatigue life estimation methods, enabling faster iterations and generalization, providing quick estimates that guide decisions alongside conventional simulations.
  In this paper, we present a ML-based pipeline that aims to estimate the fatigue life of different aircraft wing locations given the flight parameters of the different missions that the aircraft will be operating throughout its operational life. We validate the pipeline in a realistic use case of fatigue life estimation, yielding accurate predictions alongside a thorough statistical validation and uncertainty quantification. Our pipeline constitutes a complement to traditional methodologies by reducing the amount of costly simulations and, thereby, lowering the required computational and human resources.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization</title>
<link>https://arxiv.org/abs/2509.18116</link>
<guid>https://arxiv.org/abs/2509.18116</guid>
<content:encoded><![CDATA[
arXiv:2509.18116v2 Announce Type: replace 
Abstract: Test-time optimization remains impractical at scale due to prohibitive inference costs--techniques like iterative refinement and multi-step verification can require $10-100\times$ more compute per query than standard decoding. Latent space test-time optimization methods like LatentSeek offer a more direct approach by steering hidden representations, but still demand expensive per-query optimization loops with multiple backward passes. We propose Amortized Latent Steering (ALS), which collapses this iterative optimization into a single offline-computed vector applied at constant cost during inference. ALS computes the mean difference between hidden states from successful versus unsuccessful generations, then uses this direction to calibrate the model's hidden representations: when decoding drifts away from the success manifold, ALS nudges activations back toward it. Across GSM8K and MATH-500 benchmarks, ALS achieves $2-5\times$ speedup over iterative methods while matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency baselines, yielding up to 101% improvement in efficiency--accuracy trade-off. These results show that much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment. Code is available at https://github.com/negbuna/ALS.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation</title>
<link>https://arxiv.org/abs/2509.19215</link>
<guid>https://arxiv.org/abs/2509.19215</guid>
<content:encoded><![CDATA[
arXiv:2509.19215v2 Announce Type: replace 
Abstract: Photoplethysmography (PPG) is widely used in wearable health monitoring, yet large PPG foundation models remain difficult to deploy on resource-limited devices. We present PPG-Distill, a knowledge distillation framework that transfers both global and local knowledge through prediction-, feature-, and patch-level distillation. PPG-Distill incorporates morphology distillation to preserve local waveform patterns and rhythm distillation to capture inter-patch temporal structures. On heart rate estimation and atrial fibrillation detection, PPG-Distill improves student performance by up to 21.8% while achieving 7X faster inference and reducing memory usage by 19X, enabling efficient PPG analysis on wearables.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from Delayed Feedback in Games via Extra Prediction</title>
<link>https://arxiv.org/abs/2509.22426</link>
<guid>https://arxiv.org/abs/2509.22426</guid>
<content:encoded><![CDATA[
arXiv:2509.22426v2 Announce Type: replace 
Abstract: This study raises and addresses the problem of time-delayed feedback in learning in games. Because learning in games assumes that multiple agents independently learn their strategies, a discrepancy in optimization often emerges among the agents. To overcome this discrepancy, the prediction of the future reward is incorporated into algorithms, typically known as Optimistic Follow-the-Regularized-Leader (OFTRL). However, the time delay in observing the past rewards hinders the prediction. Indeed, this study firstly proves that even a single-step delay worsens the performance of OFTRL from the aspects of social regret and convergence. This study proposes the weighted OFTRL (WOFTRL), where the prediction vector of the next reward in OFTRL is weighted $n$ times. We further capture an intuition that the optimistic weight cancels out this time delay. We prove that when the optimistic weight exceeds the time delay, our WOFTRL recovers the good performances that social regret is constant in general-sum normal-form games, and the strategies last-iterate converge to the Nash equilibrium in poly-matrix zero-sum games. The theoretical results are supported and strengthened by our experiments.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Masked Diffusion for Provable Self-Correction</title>
<link>https://arxiv.org/abs/2510.01384</link>
<guid>https://arxiv.org/abs/2510.01384</guid>
<content:encoded><![CDATA[
arXiv:2510.01384v2 Announce Type: replace 
Abstract: A natural desideratum for generative models is self-correction--detecting and revising low-quality tokens at inference. While Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces, their capacity for self-correction remains poorly understood. Prior attempts to incorporate self-correction into MDMs either require overhauling MDM architectures/training or rely on imprecise proxies for token quality, limiting their applicability. Motivated by this, we introduce PRISM--Plug-in Remasking for Inference-time Self-correction of Masked Diffusions--a lightweight, model-agnostic approach that applies to any pretrained MDM. Theoretically, PRISM defines a self-correction loss that provably learns per-token quality scores, without RL or a verifier. These quality scores are computed in the same forward pass with MDM and used to detect low-quality tokens. Empirically, PRISM advances MDM inference across domains and scales: Sudoku; unconditional text (170M); and code with LLaDA (8B).
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward</title>
<link>https://arxiv.org/abs/2510.03222</link>
<guid>https://arxiv.org/abs/2510.03222</guid>
<content:encoded><![CDATA[
arXiv:2510.03222v2 Announce Type: replace 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \textbf{\textit{reasoning sparks}}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of \textit{reasoning sparks} is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy RL, sustaining continuous scaling across $3,000$ training steps and $81,204$ GPU-hours, where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a $60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$ over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatio-Temporal Graph Convolutional Networks for EV Charging Demand Forecasting Using Real-World Multi-Modal Data Integration</title>
<link>https://arxiv.org/abs/2510.09048</link>
<guid>https://arxiv.org/abs/2510.09048</guid>
<content:encoded><![CDATA[
arXiv:2510.09048v3 Announce Type: replace 
Abstract: Transportation remains a major contributor to greenhouse gas emissions, highlighting the urgency of transitioning toward sustainable alternatives such as electric vehicles (EVs). Yet, uneven spatial distribution and irregular utilization of charging infrastructure create challenges for both power grid stability and investment planning. This study introduces TW-GCN, a spatio-temporal forecasting framework that combines Graph Convolutional Networks with temporal architectures to predict EV charging demand in Tennessee, United States (U.S.). We utilize real-world traffic flows, weather conditions, and proprietary data provided by one of the largest EV infrastructure company in the U.S. to capture both spatial dependencies and temporal dynamics. Extensive experiments across varying lag horizons, clustering strategies, and sequence lengths reveal that mid-horizon (3-hour) forecasts achieve the best balance between responsiveness and stability, with 1DCNN consistently outperforming other temporal models. Regional analysis shows disparities in predictive accuracy across East, Middle, and West Tennessee, reflecting how station density, population, and local demand variability shape model performance. The proposed TW-GCN framework advances the integration of data-driven intelligence into EV infrastructure planning, supporting both sustainable mobility transitions and resilient grid management.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bidirectional Time-Frequency Pyramid Network for Enhanced Robust EEG Classification</title>
<link>https://arxiv.org/abs/2510.10004</link>
<guid>https://arxiv.org/abs/2510.10004</guid>
<content:encoded><![CDATA[
arXiv:2510.10004v2 Announce Type: replace 
Abstract: Existing EEG recognition models suffer from poor cross-paradigm generalization due to dataset-specific constraints and individual variability. To overcome these limitations, we propose BITE (Bidirectional Time-Freq Pyramid Network), an end-to-end unified architecture featuring robust multistream synergy, pyramid time-frequency attention (PTFA), and bidirectional adaptive convolutions. The framework uniquely integrates: 1) Aligned time-frequency streams maintaining temporal synchronization with STFT for bidirectional modeling, 2) PTFA-based multi-scale feature enhancement amplifying critical neural patterns, 3) BiTCN with learnable fusion capturing forward/backward neural dynamics. Demonstrating enhanced robustness, BITE achieves state-of-the-art performance across four divergent paradigms (BCICIV-2A/2B, HGD, SD-SSVEP), excelling in both within-subject accuracy and cross-subject generalization. As a unified architecture, it combines robust performance across both MI and SSVEP tasks with exceptional computational efficiency. Our work validates that paradigm-aligned spectral-temporal processing is essential for reliable BCI systems. Just as its name suggests, BITE "takes a bite out of EEG." The source code is available at https://github.com/cindy-hong/BiteEEG.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits</title>
<link>https://arxiv.org/abs/2510.26690</link>
<guid>https://arxiv.org/abs/2510.26690</guid>
<content:encoded><![CDATA[
arXiv:2510.26690v2 Announce Type: replace 
Abstract: Low-Rank Adaptation (LoRA) has become a popular technique for parameter-efficient fine-tuning of large language models (LLMs). In many real-world scenarios, multiple adapters are loaded simultaneously to enable LLM customization for personalized user experiences or to support a diverse range of tasks. Although each adapter is lightweight in isolation, their aggregate cost becomes substantial at scale. To address this, we propose LoRAQuant, a mixed-precision post-training quantization method tailored to LoRA. Specifically, LoRAQuant reparameterizes each adapter by singular value decomposition (SVD) to concentrate the most important information into specific rows and columns. This makes it possible to quantize the important components to higher precision, while quantizing the rest to ultra-low bitwidth. We conduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B models on mathematical reasoning, coding, and summarization tasks. Results show that our LoRAQuant uses significantly lower bits than other quantization methods, but achieves comparable or even higher performance.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asymptotically Unbiased Synthetic Control Methods by Moment Matching</title>
<link>https://arxiv.org/abs/2307.11127</link>
<guid>https://arxiv.org/abs/2307.11127</guid>
<content:encoded><![CDATA[
arXiv:2307.11127v5 Announce Type: replace-cross 
Abstract: Synthetic Control Methods (SCMs) have become a fundamental tool for comparative case studies. The core idea behind SCMs is to estimate treatment effects by predicting counterfactual outcomes for a treated unit using a weighted combination of observed outcomes from untreated units. The accuracy of these predictions is crucial for evaluating the treatment effect of a policy intervention. Subsequent research has therefore focused on estimating SC weights. In this study, we highlight a key endogeneity issue in existing SCMs-namely, the correlation between the outcomes of untreated units and the error term of the synthetic control, which leads to bias in both counterfactual outcome prediction and treatment effect estimation. To address this issue, we propose a novel SCM based on moment matching, assuming that the outcome distribution of the treated unit can be approximated by a weighted mixture of the distributions of untreated units. Under this assumption, we estimate SC weights by matching the moments of the treated outcomes with the weighted sum of the moments of the untreated outcomes. Our method offers three advantages: first, under the mixture model assumption, our estimator is asymptotically unbiased; second, this asymptotic unbiasedness reduces the mean squared error in counterfactual predictions; and third, our method provides full distributions of the treatment effect rather than just expected values, thereby broadening the applicability of SCMs. Finally, we present experimental results that demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-and-Out: Algorithmic Diffusion for Sampling Convex Bodies</title>
<link>https://arxiv.org/abs/2405.01425</link>
<guid>https://arxiv.org/abs/2405.01425</guid>
<content:encoded><![CDATA[
arXiv:2405.01425v3 Announce Type: replace-cross 
Abstract: We present a new random walk for uniformly sampling high-dimensional convex bodies. It achieves state-of-the-art runtime complexity with stronger guarantees on the output than previously known, namely in R\'enyi divergence (which implies TV, $\mathcal{W}_2$, KL, $\chi^2$). The proof departs from known approaches for polytime algorithms for the problem -- we utilize a stochastic diffusion perspective to show contraction to the target distribution with the rate of convergence determined by functional isoperimetric constants of the target distribution.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Low Rank Neural Representation of Entropy Solutions</title>
<link>https://arxiv.org/abs/2406.05694</link>
<guid>https://arxiv.org/abs/2406.05694</guid>
<content:encoded><![CDATA[
arXiv:2406.05694v3 Announce Type: replace-cross 
Abstract: We construct a new representation of entropy solutions to nonlinear scalar conservation laws with a smooth convex flux function in a single spatial dimension. The representation is a generalization of the method of characteristics and posseses a compositional form. While it is a nonlinear representation, the embedded dynamics of the solution in the time variable is linear. This representation is then discretized as a manifold of implicit neural representations where the feedforward neural network architecture has a low rank structure. Finally, we show that the low rank neural representation with a fixed number of layers and a small number of coefficients can approximate any entropy solution regardless of the complexity of the shock topology, while retaining the linearity of the embedded dynamics.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Study on Noise-Augmented Training and its Effect on Adversarial Robustness in ASR Systems</title>
<link>https://arxiv.org/abs/2409.01813</link>
<guid>https://arxiv.org/abs/2409.01813</guid>
<content:encoded><![CDATA[
arXiv:2409.01813v4 Announce Type: replace-cross 
Abstract: In this study, we investigate whether noise-augmented training can concurrently improve adversarial robustness in automatic speech recognition (ASR) systems. We conduct a comparative analysis of the adversarial robustness of four different ASR architectures, each trained under three different augmentation conditions: (1) background noise, speed variations, and reverberations; (2) speed variations only; (3) no data augmentation. We then evaluate the robustness of all resulting models against attacks with white-box or black-box adversarial examples. Our results demonstrate that noise augmentation not only enhances model performance on noisy speech but also improves the model's robustness to adversarial attacks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying Drift, Diffusion, and Causal Structure from Temporal Snapshots</title>
<link>https://arxiv.org/abs/2410.22729</link>
<guid>https://arxiv.org/abs/2410.22729</guid>
<content:encoded><![CDATA[
arXiv:2410.22729v3 Announce Type: replace-cross 
Abstract: Stochastic differential equations (SDEs) are a fundamental tool for modelling dynamic processes, including gene regulatory networks (GRNs), contaminant transport, financial markets, and image generation. However, learning the underlying SDE from data is a challenging task, especially if individual trajectories are not observable. Motivated by burgeoning research in single-cell datasets, we present the first comprehensive approach for jointly identifying the drift and diffusion of an SDE from its temporal marginals. Assuming linear drift and additive diffusion, we prove that these parameters are identifiable from marginals if and only if the initial distribution lacks any generalized rotational symmetries. We further prove that the causal graph of any SDE with additive diffusion can be recovered from the SDE parameters. To complement this theory, we adapt entropy-regularized optimal transport to handle anisotropic diffusion, and introduce APPEX (Alternating Projection Parameter Estimation from $X_0$), an iterative algorithm designed to estimate the drift, diffusion, and causal graph of an additive noise SDE, solely from temporal marginals. We show that APPEX iteratively decreases Kullback-Leibler divergence to the true solution, and demonstrate its effectiveness on simulated data from linear additive noise SDEs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use</title>
<link>https://arxiv.org/abs/2501.09766</link>
<guid>https://arxiv.org/abs/2501.09766</guid>
<content:encoded><![CDATA[
arXiv:2501.09766v5 Announce Type: replace-cross 
Abstract: Augmenting large language models (LLMs) with external tools is a promising approach to enhance their capabilities, especially for complex tasks. Synthesizing tool-use data through real-world simulations is an effective way to achieve this. However, our investigation reveals that training gains significantly decay as synthetic data increases. The model struggles to benefit from additional synthetic data, which fails to endow it with advanced tool-use capabilities in complex scenarios Moreover, we discovered that the above limitation usually manifests as a fragment deficiency (i.e., parameter errors) in response. To this end, we propose an iterative reinforced fine-tuning strategy designed to alleviate this limitation. This strategy involves: (1) enhancing the diversity of response for synthetic data through path exploration of Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency by constructing fine-grained preference pairs, and then improving it by preference optimization algorithms for targeted improvement. The experiments show that our method achieves 13.11% better performance than the same-size base model. It achieves an improvement of 6.5% in complex scenarios compared to the baseline, and it also outperforms larger open-source and closed-source models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing limits for in-context learning</title>
<link>https://arxiv.org/abs/2502.03503</link>
<guid>https://arxiv.org/abs/2502.03503</guid>
<content:encoded><![CDATA[
arXiv:2502.03503v3 Announce Type: replace-cross 
Abstract: Our paper challenges claims from prior research that transformer-based models, when learning in context, implicitly implement standard learning algorithms. We present empirical evidence inconsistent with this view and provide a mathematical analysis demonstrating that transformers cannot achieve general predictive accuracy due to inherent architectural limitations.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction-Powered Adaptive Shrinkage Estimation</title>
<link>https://arxiv.org/abs/2502.14166</link>
<guid>https://arxiv.org/abs/2502.14166</guid>
<content:encoded><![CDATA[
arXiv:2502.14166v3 Announce Type: replace-cross 
Abstract: Prediction-Powered Inference (PPI) is a powerful framework for enhancing statistical estimates by combining limited gold-standard data with machine learning (ML) predictions. While prior work has demonstrated PPI's benefits for individual statistical problems, modern applications require answering numerous parallel statistical questions. We introduce Prediction-Powered Adaptive Shrinkage (PAS), a method that bridges PPI with empirical Bayes shrinkage to improve the estimation of multiple means. PAS debiases noisy ML predictions within each task and then borrows strength across tasks by using those same predictions as a reference point for shrinkage. The amount of shrinkage is determined by minimizing an unbiased estimate of risk, and we prove that this tuning strategy is asymptotically optimal. Experiments on both synthetic and real-world datasets show that PAS adapts to the reliability of the ML predictions and outperforms traditional and modern baselines in large-scale applications.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XBreaking: Understanding how LLMs security alignment can be broken</title>
<link>https://arxiv.org/abs/2504.21700</link>
<guid>https://arxiv.org/abs/2504.21700</guid>
<content:encoded><![CDATA[
arXiv:2504.21700v3 Announce Type: replace-cross 
Abstract: Large Language Models are fundamental actors in the modern IT landscape dominated by AI solutions. However, security threats associated with them might prevent their reliable adoption in critical application scenarios such as government organizations and medical institutions. For this reason, commercial LLMs typically undergo a sophisticated censoring mechanism to eliminate any harmful output they could possibly produce. These mechanisms maintain the integrity of LLM alignment by guaranteeing that the models respond safely and ethically. In response to this, attacks on LLMs are a significant threat to such protections, and many previous approaches have already demonstrated their effectiveness across diverse domains. Existing LLM attacks mostly adopt a generate-and-test strategy to craft malicious input. To improve the comprehension of censoring mechanisms and design a targeted attack, we propose an Explainable-AI solution that comparatively analyzes the behavior of censored and uncensored models to derive unique exploitable alignment patterns. Then, we propose XBreaking, a novel approach that exploits these unique patterns to break the security and alignment constraints of LLMs by targeted noise injection. Our thorough experimental campaign returns important insights about the censoring mechanisms and demonstrates the effectiveness and performance of our approach.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols</title>
<link>https://arxiv.org/abs/2505.16821</link>
<guid>https://arxiv.org/abs/2505.16821</guid>
<content:encoded><![CDATA[
arXiv:2505.16821v3 Announce Type: replace-cross 
Abstract: Integrating Large AI Models (LAMs) into 6G mobile networks is a key enabler of the AI-Native Air Interface (AI-AI), where protocol intelligence must scale beyond handcrafted logic. This paper presents, to our knowledge, the first standards-compliant emulation of the Radio Resource Control (RRC) layer using a decoder-only LAM (LLAMA-class) fine-tuned with Low-Rank Adaptation (LoRA) on a multi-vendor corpus of real-world traces spanning both 5G and 4G systems. We treat RRC as a domain-specific language and construct a segmentation-safe, question--answer (Question-and-Answer (QA)) dataset that preserves Abstract Syntax Notation (ASN.1) structure through linearization prior to Byte Pair Encoding (BPE) tokenization. The proposed approach combines parameter-efficient adaptation with schema-bounded prompting to ensure syntactic and procedural fidelity. Evaluation introduces a standards-aware triad -- ASN.1 conformance, field-level coverage analysis, and uplink-to-downlink state-machine checks -- alongside semantic similarity and latency profiling across 120 configurations. On 30k 5G request--response pairs plus an additional 4.8k QA turns from 4G sessions, our 8B model achieves a median cosine similarity of 0.97, a 61% relative gain over a zero-shot baseline, while sustaining high conformance rates. These results demonstrate that LAMs, when augmented with protocol-aware reasoning, can directly orchestrate control-plane procedures, laying the foundation for the future Artificial Intelligence (AI)-native Radio Access Network (RAN).
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProRefine: Inference-Time Prompt Refinement with Textual Feedback</title>
<link>https://arxiv.org/abs/2506.05305</link>
<guid>https://arxiv.org/abs/2506.05305</guid>
<content:encoded><![CDATA[
arXiv:2506.05305v3 Announce Type: replace-cross 
Abstract: Agentic workflows, where multiple AI agents collaborate to accomplish complex tasks like reasoning or planning, play a substantial role in many cutting-edge commercial applications, and continue to fascinate researchers across fields for their potential to accomplish expensive, complex tasks that, until recently, only humans have been trusted to do. These workflows depend critically on the prompts used to provide the roles models play in such workflows. Poorly designed prompts that fail even slightly to guide individual agents can lead to sub-optimal performance that may snowball within a system of agents, limiting their reliability and scalability. To address this important problem of inference-time prompt optimization, we introduce ProRefine, an innovative inference-time optimization method that uses an agentic loop of LLMs to generate and apply textual feedback. ProRefine dynamically refines prompts for multi-step reasoning tasks without additional training or ground truth labels. Evaluated on five benchmark mathematical reasoning datasets, ProRefine significantly surpasses zero-shot Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only boosts accuracy but also allows smaller models to approach the performance of their larger counterparts. This highlights its potential for building more cost-effective and powerful hybrid AI systems, thereby democratizing access to high-performing AI.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizable, real-time neural decoding with hybrid state-space models</title>
<link>https://arxiv.org/abs/2506.05320</link>
<guid>https://arxiv.org/abs/2506.05320</guid>
<content:encoded><![CDATA[
arXiv:2506.05320v2 Announce Type: replace-cross 
Abstract: Real-time decoding of neural activity is central to neuroscience and neurotechnology applications, from closed-loop experiments to brain-computer interfaces, where models are subject to strict latency constraints. Traditional methods, including simple recurrent neural networks, are fast and lightweight but often struggle to generalize to unseen data. In contrast, recent Transformer-based approaches leverage large-scale pretraining for strong generalization performance, but typically have much larger computational requirements and are not always suitable for low-resource or real-time settings. To address these shortcomings, we present POSSM, a novel hybrid architecture that combines individual spike tokenization via a cross-attention module with a recurrent state-space model (SSM) backbone to enable (1) fast and causal online prediction on neural activity and (2) efficient generalization to new sessions, individuals, and tasks through multi-dataset pretraining. We evaluate POSSM's decoding performance and inference speed on intracortical decoding of monkey motor tasks, and show that it extends to clinical applications, namely handwriting and speech decoding in human subjects. Notably, we demonstrate that pretraining on monkey motor-cortical recordings improves decoding performance on the human handwriting task, highlighting the exciting potential for cross-species transfer. In all of these tasks, we find that POSSM achieves decoding accuracy comparable to state-of-the-art Transformers, at a fraction of the inference cost (up to 9x faster on GPU). These results suggest that hybrid SSMs are a promising approach to bridging the gap between accuracy, inference speed, and generalization when training neural decoders for real-time, closed-loop applications.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Know What You Don't Know: Uncertainty Calibration of Process Reward Models</title>
<link>https://arxiv.org/abs/2506.09338</link>
<guid>https://arxiv.org/abs/2506.09338</guid>
<content:encoded><![CDATA[
arXiv:2506.09338v2 Announce Type: replace-cross 
Abstract: Process reward models (PRMs) play a central role in guiding inference-time scaling algorithms for large language models (LLMs). However, we observe that even state-of-the-art PRMs can be poorly calibrated. Specifically, they tend to overestimate the success probability that a partial reasoning step will lead to a correct final answer, particularly when smaller LLMs are used to complete the reasoning trajectory. To address this, we present a calibration approach -- performed via quantile regression -- that adjusts PRM outputs to better align with true success probabilities. Leveraging these calibrated success estimates and their associated confidence bounds, we introduce an \emph{instance-adaptive scaling} (IAS) framework that dynamically adjusts the compute budget based on the estimated likelihood that a partial reasoning trajectory will yield a correct final answer. Unlike conventional methods that allocate a fixed number of reasoning trajectories per query, this approach adapts to each instance and reasoning step when using our calibrated PRMs. Experiments on mathematical reasoning benchmarks show that (i) our PRM calibration method achieves small calibration error, outperforming the baseline methods, (ii) calibration is crucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces inference costs while maintaining final answer accuracy, utilizing less compute on more confident problems as desired.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performative Validity of Recourse Explanations</title>
<link>https://arxiv.org/abs/2506.15366</link>
<guid>https://arxiv.org/abs/2506.15366</guid>
<content:encoded><![CDATA[
arXiv:2506.15366v2 Announce Type: replace-cross 
Abstract: When applicants get rejected by an algorithmic decision system, recourse explanations provide actionable suggestions for how to change their input features to get a positive evaluation. A crucial yet overlooked phenomenon is that recourse explanations are performative: When many applicants act according to their recommendations, their collective behavior may change statistical regularities in the data and, once the model is refitted, also the decision boundary. Consequently, the recourse algorithm may render its own recommendations invalid, such that applicants who make the effort of implementing their recommendations may be rejected again when they reapply. In this work, we formally characterize the conditions under which recourse explanations remain valid under performativity. A key finding is that recourse actions may become invalid if they are influenced by or if they intervene on non-causal variables. Based on our analysis, we caution against the use of standard counterfactual explanations and causal recourse methods, and instead advocate for recourse methods that recommend actions exclusively on causal variables.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles</title>
<link>https://arxiv.org/abs/2507.01542</link>
<guid>https://arxiv.org/abs/2507.01542</guid>
<content:encoded><![CDATA[
arXiv:2507.01542v2 Announce Type: replace-cross 
Abstract: Gaussian mixture models (GMMs) are ubiquitous in statistical learning, particularly for unsupervised problems. While full GMMs suffer from the overparameterization of their covariance matrices in high-dimensional spaces, spherical GMMs (with isotropic covariance matrices) certainly lack flexibility to fit certain anisotropic distributions. Connecting these two extremes, we introduce a new family of parsimonious GMMs with piecewise-constant covariance eigenvalue profiles. These extend several low-rank models like the celebrated mixtures of probabilistic principal component analyzers (MPPCA), by enabling any possible sequence of eigenvalue multiplicities. If the latter are prespecified, then we can naturally derive an expectation-maximization (EM) algorithm to learn the mixture parameters. Otherwise, to address the notoriously-challenging issue of jointly learning the mixture parameters and hyperparameters, we propose a componentwise penalized EM algorithm, whose monotonicity is proven. We show the superior likelihood-parsimony tradeoffs achieved by our models on a variety of unsupervised experiments: density fitting, clustering and single-image denoising.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation</title>
<link>https://arxiv.org/abs/2507.02275</link>
<guid>https://arxiv.org/abs/2507.02275</guid>
<content:encoded><![CDATA[
arXiv:2507.02275v3 Announce Type: replace-cross 
Abstract: Structure-agnostic causal inference studies how well one can estimate a treatment effect given black-box machine learning estimates of nuisance functions (like the impact of confounders on treatment and outcomes). Here, we find that the answer depends in a surprising way on the distribution of the treatment noise. Focusing on the partially linear model of \citet{robinson1988root}, we first show that the widely adopted double machine learning (DML) estimator is minimax rate-optimal for Gaussian treatment noise, resolving an open problem of \citet{mackey2018orthogonal}. Meanwhile, for independent non-Gaussian treatment noise, we show that DML is always suboptimal by constructing new practical procedures with higher-order robustness to nuisance errors. These \emph{ACE} procedures use structure-agnostic cumulant estimators to achieve $r$-th order insensitivity to nuisance errors whenever the $(r+1)$-st treatment cumulant is non-zero. We complement these core results with novel minimax guarantees for binary treatments in the partially linear model. Finally, using synthetic demand estimation experiments, we demonstrate the practical benefits of our higher-order robust estimators.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Trust or Not to Trust: On Calibration in ML-based Resource Allocation for Wireless Networks</title>
<link>https://arxiv.org/abs/2507.17494</link>
<guid>https://arxiv.org/abs/2507.17494</guid>
<content:encoded><![CDATA[
arXiv:2507.17494v2 Announce Type: replace-cross 
Abstract: In next-generation communications and networks, machine learning (ML) models are expected to deliver not only accurate predictions but also well-calibrated confidence scores that reflect the true likelihood of correct decisions. This paper studies the calibration performance of an ML-based outage predictor within a single-user, multi-resource allocation framework. We first establish key theoretical properties of this system's outage probability (OP) under perfect calibration. Importantly, we show that as the number of resources grows, the OP of a perfectly calibrated predictor approaches the expected output conditioned on it being below the classification threshold. In contrast, when only one resource is available, the system's OP equals the model's overall expected output. We then derive the OP conditions for a perfectly calibrated predictor. These findings guide the choice of the classification threshold to achieve a desired OP, helping system designers meet specific reliability requirements. We also demonstrate that post-processing calibration cannot improve the system's minimum achievable OP, as it does not introduce new information about future channel states. Additionally, we show that well-calibrated models are part of a broader class of predictors that necessarily improve OP. In particular, we establish a monotonicity condition that the accuracy-confidence function must satisfy for such improvement to occur. To demonstrate these theoretical properties, we conduct a rigorous simulation-based analysis using post-processing calibration techniques: Platt scaling and isotonic regression. As part of this framework, the predictor is trained using an outage loss function specifically designed for this system. Furthermore, this analysis is performed on Rayleigh fading channels with temporal correlation captured by Clarke's 2D model, which accounts for receiver mobility.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closed-Form Beta Distribution Estimation from Sparse Statistics with Random Forest Implicit Regularization</title>
<link>https://arxiv.org/abs/2507.23767</link>
<guid>https://arxiv.org/abs/2507.23767</guid>
<content:encoded><![CDATA[
arXiv:2507.23767v2 Announce Type: replace-cross 
Abstract: This work advances distribution recovery from sparse data and ensemble classification through three main contributions. First, we introduce a closed-form estimator that reconstructs scaled beta distributions from limited statistics (minimum, maximum, mean, and median) via composite quantile and moment matching. The recovered parameters $(\alpha,\beta)$, when used as features in Random Forest classifiers, improve pairwise classification on time-series snapshots, validating the fidelity of the recovered distributions. Second, we establish a link between classification accuracy and distributional closeness by deriving error bounds that constrain total variation distance and Jensen-Shannon divergence, the latter exhibiting quadratic convergence. Third, we show that zero-variance features act as an implicit regularizer, increasing selection probability for mid-ranked predictors and producing deeper, more varied trees. A SeatGeek pricing dataset serves as the primary application, illustrating distributional recovery and event-level classification while situating these methods within the structure and dynamics of the secondary ticket marketplace. The UCI handwritten digits dataset confirms the broader regularization effect. Overall, the study outlines a practical route from sparse distributional snapshots to closed-form estimation and improved ensemble accuracy, with reliability enhanced through implicit regularization.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holistic Evaluation of Multimodal LLMs on Spatial Intelligence</title>
<link>https://arxiv.org/abs/2508.13142</link>
<guid>https://arxiv.org/abs/2508.13142</guid>
<content:encoded><![CDATA[
arXiv:2508.13142v3 Announce Type: replace-cross 
Abstract: Multimodal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, the very capability that anchors artificial general intelligence in the physical world. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path toward spatial intelligence. We thus propose EASI for holistic Evaluation of multimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and a standardized protocol for the fair evaluation of state-of-the-art proprietary and open-source models. In this report, we conduct the study across eight key benchmarks, at a cost exceeding ten billion total tokens. Our empirical study then reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence (SI), yet (2) still falls short of human performance significantly across a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose greater model capability deficiency than non-SI tasks, to the extent that (4) proprietary models do not exhibit a decisive advantage when facing the most difficult ones. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans, yet fail even the most advanced multimodal models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phase Transition for Stochastic Block Model with more than $\sqrt{n}$ Communities</title>
<link>https://arxiv.org/abs/2509.15822</link>
<guid>https://arxiv.org/abs/2509.15822</guid>
<content:encoded><![CDATA[
arXiv:2509.15822v2 Announce Type: replace-cross 
Abstract: Predictions from statistical physics postulate that recovery of the communities in Stochastic Block Model (SBM) is possible in polynomial time above, and only above, the Kesten-Stigum (KS) threshold. This conjecture has given rise to a rich literature, proving that non-trivial community recovery is indeed possible in SBM above the KS threshold, as long as the number $K$ of communities remains smaller than $\sqrt{n}$, where $n$ is the number of nodes in the observed graph. Failure of low-degree polynomials below the KS threshold was also proven when $K=o(\sqrt{n})$.
  When $K\geq \sqrt{n}$, Chin et al.(2025) recently prove that, in a sparse regime, community recovery in polynomial time is possible below the KS threshold by counting non-backtracking paths. This breakthrough result lead them to postulate a new threshold for the many communities regime $K\geq \sqrt{n}$. In this work, we provide evidences that confirm their conjecture for $K\geq \sqrt{n}$:
  1- We prove that, for any density of the graph, low-degree polynomials fail to recover communities below the threshold postulated by Chin et al.(2025);
  2- We prove that community recovery is possible in polynomial time above the postulated threshold, not only in the sparse regime of~Chin et al., but also in some (but not all) moderately sparse regimes by essentially by counting occurrences of cliques or self-avoiding paths of suitable size in the observed graph.
  In addition, we propose a detailed conjecture regarding the structure of motifs that are optimal in sparsity regimes not covered by cliques or self-avoiding paths counting. In particular, counting self-avoiding paths of length $\log(n)$--which is closely related to spectral algorithms based on the Non-Backtracking operator--is optimal only in the sparse regime. Other motif counts--unrelated to spectral properties--should be considered in denser regimes.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bilinear relational structure fixes reversal curse and enables consistent model editing</title>
<link>https://arxiv.org/abs/2509.21993</link>
<guid>https://arxiv.org/abs/2509.21993</guid>
<content:encoded><![CDATA[
arXiv:2509.21993v2 Announce Type: replace-cross 
Abstract: The reversal curse -- a language model's (LM) inability to infer an unseen fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a fundamental limitation. We show that this is not an inherent failure but an artifact of how models encode knowledge. By training LMs from scratch on a synthetic dataset of relational knowledge graphs, we demonstrate that bilinear relational structure emerges in their hidden representations. This structure substantially alleviates the reversal curse, enabling LMs to infer unseen reverse facts. Crucially, we also find that this bilinear structure plays a key role in consistent model editing. When a fact is updated in a LM with this structure, the edit correctly propagates to its reverse and other logically dependent facts. In contrast, models lacking this representation not only suffer from the reversal curse but also fail to generalize edits, further introducing logical inconsistencies. Our results establish that training on a relational knowledge dataset induces the emergence of bilinear internal representations, which in turn enable LMs to behave in a logically consistent manner after editing. This implies that the success of model editing depends critically not just on editing algorithms but on the underlying representational geometry of the knowledge being modified.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Orbital Parameters of Direct Imaging Exoplanet Using Neural Network</title>
<link>https://arxiv.org/abs/2510.17459</link>
<guid>https://arxiv.org/abs/2510.17459</guid>
<content:encoded><![CDATA[
arXiv:2510.17459v2 Announce Type: replace-cross 
Abstract: In this work, we propose a new flow-matching Markov chain Monte Carlo (FM-MCMC) algorithm for estimating the orbital parameters of exoplanetary systems, especially for those only one exoplanet is involved. Compared to traditional methods that rely on random sampling within the Bayesian framework, our approach first leverages flow matching posterior estimation (FMPE) to efficiently constrain the prior range of physical parameters, and then employs MCMC to accurately infer the posterior distribution. For example, in the orbital parameter inference of beta Pictoris b, our model achieved a substantial speed-up while maintaining comparable accuracy-running 77.8 times faster than Parallel Tempered MCMC (PTMCMC) and 365.4 times faster than nested sampling. Moreover, our FM-MCMC method also attained the highest average log-likelihood among all approaches, demonstrating its superior sampling efficiency and accuracy. This highlights the scalability and efficiency of our approach, making it well-suited for processing the massive datasets expected from future exoplanet surveys. Beyond astrophysics, our methodology establishes a versatile paradigm for synergizing deep generative models with traditional sampling, which can be adopted to tackle complex inference problems in other fields, such as cosmology, biomedical imaging, and particle physics.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base</title>
<link>https://arxiv.org/abs/2510.26854</link>
<guid>https://arxiv.org/abs/2510.26854</guid>
<content:encoded><![CDATA[
arXiv:2510.26854v2 Announce Type: replace-cross 
Abstract: Most scientific materials compress reasoning, presenting conclusions while omitting the derivational chains that justify them. This compression hinders verification by lacking explicit, step-wise justifications and inhibits cross-domain links by collapsing the very pathways that establish the logical and causal connections between concepts. We introduce a scalable framework that decompresses scientific reasoning, constructing a verifiable Long Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven, reductionist strategy: a Socratic agent, guided by a curriculum of around 200 courses, generates approximately 3 million first-principles questions. To ensure high fidelity, multiple independent solver models generate LCoTs, which are then rigorously filtered by prompt sanitization and cross-model answer consensus, retaining only those with verifiable endpoints. This verified corpus powers the Brainstorm Search Engine, which performs inverse knowledge search -- retrieving diverse, first-principles derivations that culminate in a target concept. This engine, in turn, feeds the Plato synthesizer, which narrates these verified chains into coherent articles. The initial SciencePedia comprises approximately 200,000 fine-grained entries spanning mathematics, physics, chemistry, biology, engineering, and computation. In evaluations across six disciplines, Plato-synthesized articles (conditioned on retrieved LCoTs) exhibit substantially higher knowledge-point density and significantly lower factual error rates than an equally-prompted baseline without retrieval (as judged by an external LLM). Built on this verifiable LCoT knowledge base, this reasoning-centric approach enables trustworthy, cross-domain scientific synthesis at scale and establishes the foundation for an ever-expanding encyclopedia.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AURA: A Reinforcement Learning Framework for AI-Driven Adaptive Conversational Surveys</title>
<link>https://arxiv.org/abs/2510.27126</link>
<guid>https://arxiv.org/abs/2510.27126</guid>
<content:encoded><![CDATA[
arXiv:2510.27126v2 Announce Type: replace-cross 
Abstract: Conventional online surveys provide limited personalization, often resulting in low engagement and superficial responses. Although AI survey chatbots improve convenience, most are still reactive: they rely on fixed dialogue trees or static prompt templates and therefore cannot adapt within a session to fit individual users, which leads to generic follow-ups and weak response quality. We address these limitations with AURA (Adaptive Understanding through Reinforcement Learning for Assessment), a reinforcement learning framework for AI-driven adaptive conversational surveys. AURA quantifies response quality using a four-dimensional LSDE metric (Length, Self-disclosure, Emotion, and Specificity) and selects follow-up question types via an epsilon-greedy policy that updates the expected quality gain within each session. Initialized with priors extracted from 96 prior campus-climate conversations (467 total chatbot-user exchanges), the system balances exploration and exploitation across 10-15 dialogue exchanges, dynamically adapting to individual participants in real time. In controlled evaluations, AURA achieved a +0.076 mean gain in response quality and a statistically significant improvement over non-adaptive baselines (p=0.044, d=0.66), driven by a 63% reduction in specification prompts and a 10x increase in validation behavior. These results demonstrate that reinforcement learning can give survey chatbots improved adaptivity, transforming static questionnaires into interactive, self-improving assessment systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>L2T-Tune:LLM-Guided Hybrid Database Tuning with LHS and TD3</title>
<link>https://arxiv.org/abs/2511.01602</link>
<guid>https://arxiv.org/abs/2511.01602</guid>
<content:encoded><![CDATA[
arXiv:2511.01602v3 Announce Type: replace-cross 
Abstract: Configuration tuning is critical for database performance. Although recent advancements in database tuning have shown promising results in throughput and latency improvement, challenges remain. First, the vast knob space makes direct optimization unstable and slow to converge. Second, reinforcement learning pipelines often lack effective warm-start guidance and require long offline training. Third, transferability is limited: when hardware or workloads change, existing models typically require substantial retraining to recover performance.
  To address these limitations, we propose L2T-Tune, a new LLM-guided hybrid database tuning framework that features a three-stage pipeline: Stage one performs a warm start that simultaneously generates uniform samples across the knob space and logs them into a shared pool; Stage two leverages a large language model to mine and prioritize tuning hints from manuals and community documents for rapid convergence. Stage three uses the warm-start sample pool to reduce the dimensionality of knobs and state features, then fine-tunes the configuration with the Twin Delayed Deep Deterministic Policy Gradient algorithm.
  We conduct experiments on L2T-Tune and the state-of-the-art models. Compared with the best-performing alternative, our approach improves performance by an average of 37.1% across all workloads, and by up to 73% on TPC-C. Compared with models trained with reinforcement learning, it achieves rapid convergence in the offline tuning stage on a single server. Moreover, during the online tuning stage, it only takes 30 steps to achieve best results.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADPO: Anchored Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2510.18913</link>
<guid>https://arxiv.org/abs/2510.18913</guid>
<content:encoded><![CDATA[
<div> Anchored Direct Preference Optimization, Soft listwise supervision, Reference anchoring, KL divergence, Dynamic-anchor updates<br />
Summary:<br />
Anchored Direct Preference Optimization (ADPO) introduces a framework that extends preference learning to soft listwise supervision through reference anchoring, minimizing KL divergence between the target distribution and the anchored distribution. ADPO encompasses various strategies such as supervised fine-tuning, knowledge distillation, and maximum-entropy reinforcement learning. It creates an implicit trust region utilizing the softmax Fisher metric and supports stable dynamic-anchor updates. Empirical results show that dynamic anchors enhance online exploration in the presence of noise, while fixed anchors are more effective for offline distillation tasks, achieving significant reductions in student-teacher KL divergence on benchmark datasets. <div>
arXiv:2510.18913v5 Announce Type: replace 
Abstract: Direct Preference Optimization (DPO) is effective but brittle under annotator noise and distribution shift because it operates on hard, pairwise labels and only regularizes log-probability differences. We introduce Anchored Direct Preference Optimization (ADPO), a framework that extends preference learning to soft listwise supervision via reference anchoring. ADPO minimizes KL(q || softmax((s - s_ref) / tau_anc)), which (i) recovers supervised fine-tuning, knowledge distillation, maximum-entropy reinforcement learning, and DPO as special cases through suitable choices of target q, anchor policy, and temperature; (ii) induces an implicit trust region governed by the softmax Fisher metric, independent of the anchor; and (iii) supports stable dynamic-anchor updates. Empirically, we observe a task-dependent tradeoff: dynamic anchors improve online exploration under noise, while fixed anchors excel at offline distillation, achieving up to 170 to 5000 times reduction in student-teacher KL on our benchmarks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autocomp: A Powerful and Portable Code Optimizer for Tensor Accelerators</title>
<link>https://arxiv.org/abs/2505.18574</link>
<guid>https://arxiv.org/abs/2505.18574</guid>
<content:encoded><![CDATA[
<div> accelerator programming, code optimization, large language models, hardware feedback, Autocomp<br />
Summary:<br />
The article discusses the challenges of programming hardware accelerators, particularly tensor processing units, and introduces Autocomp, an approach that utilizes large language models to optimize code generation for specialized tensor accelerator code. Autocomp formulates optimization passes as structured prompts, incorporating domain knowledge and hardware feedback to improve code performance. Results show that Autocomp-optimized code outperforms vendor-provided libraries, expert hand-tuned code, and machine learning-based cost models, achieving significant speedups across different hardware platforms. The approach also demonstrates the reusability of optimization schedules, leading to further performance improvements under fixed sample budgets. Autocomp's integration of domain knowledge and hardware feedback enables accelerator programmers to leverage expertise in optimizing code for tensor accelerators effectively. <div>
arXiv:2505.18574v5 Announce Type: replace-cross 
Abstract: Hardware accelerators, especially those designed for tensor processing, have become ubiquitous in today's computing landscape. However, even with significant efforts in building compilers, programming these tensor accelerators remains challenging, leaving much of their potential underutilized. Recently, large language models (LLMs), trained on large amounts of code, have shown significant promise in code generation and optimization tasks, but generating low-resource languages, such as specialized tensor accelerator code still poses a significant challenge. We tackle this challenge with Autocomp, an approach that empowers accelerator programmers to leverage domain knowledge and hardware feedback to optimize code via an automated LLM-driven search. We accomplish this by: 1) formulating each optimization pass as a structured two-phase prompt, divided into planning and code generation phases, 2) inserting domain knowledge during planning via a concise and adaptable optimization menu, and 3) integrating correctness and performance metrics from hardware as feedback at each search iteration. Across three distinct hardware platforms, we demonstrate that Autocomp-optimized code runs 5.6x faster than the vendor-provided library (Gemmini), outperforms expert-level hand-tuned code by 1.9x (AWS Trainium), and achieves 3.8x higher performance than a machine learning-based cost model for GPUs (NVIDIA L40S). Additionally, we demonstrate that optimization schedules generated from Autocomp can be reused across similar tensor operations, improving speedups by up to 24% under a fixed sample budget.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights</title>
<link>https://arxiv.org/abs/2511.01019</link>
<guid>https://arxiv.org/abs/2511.01019</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Oceanographic Data, NOAA, Conversational Platform, Transparency

Summary:
OceanAI is a conversational platform that combines the fluency of large language models with real-time access to authoritative oceanographic data from NOAA. It enables users to ask questions and receive natural-language responses and data visualizations based on real-time API calls to relevant datasets. In a comparison with other AI chat interfaces, OceanAI was the only platform to provide NOAA-sourced values with original data references, promoting transparency and trust. The platform is designed for scalability and extensibility, connecting to multiple NOAA data products for applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding responses in verifiable observations, OceanAI enhances scientific rigor and reproducibility in decision support systems for the oceans. A public demonstration of OceanAI is available at https://oceanai.ai4ocean.xyz. 

<br /><br />Summary: <div>
arXiv:2511.01019v2 Announce Type: replace-cross 
Abstract: Artificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified "hallucinations" undermining scientific rigor. We present OceanAI, a conversational platform that integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by the National Oceanic and Atmospheric Administration (NOAA). Each query such as "What was Boston Harbor's highest water level in 2024?" triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations. In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. Designed for extensibility, OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding outputs and verifiable observations, OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans. A public demonstration is available at https://oceanai.ai4ocean.xyz.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland</title>
<link>https://arxiv.org/abs/2511.03749</link>
<guid>https://arxiv.org/abs/2511.03749</guid>
<content:encoded><![CDATA[
<div> Keywords: Grasslands, carbon cycle, dairy sector, deep learning models, forecasting

Summary:<br />
Grasslands, the world's second-largest terrestrial carbon sink, are vital for biodiversity and carbon cycle regulation. The Irish dairy sector faces challenges of profitability and sustainability, requiring advanced forecasting methods. This study introduces deep learning models tailored for univariate datasets, offering cost-effective alternatives to existing mechanistic models. A temporal convolutional network designed for forecasting Perennial Ryegrass growth in Cork demonstrates high performance, with RMSE of 2.74 and MAE of 3.46. Validation over a 34-year dataset informs optimal model configurations. By enhancing our understanding of model behavior, this research enhances the reliability of grass growth forecasting and supports sustainable dairy farming practices. <div>
arXiv:2511.03749v1 Announce Type: new 
Abstract: Grasslands, constituting the world's second-largest terrestrial carbon sink, play a crucial role in biodiversity and the regulation of the carbon cycle. Currently, the Irish dairy sector, a significant economic contributor, grapples with challenges related to profitability and sustainability. Presently, grass growth forecasting relies on impractical mechanistic models. In response, we propose deep learning models tailored for univariate datasets, presenting cost-effective alternatives. Notably, a temporal convolutional network designed for forecasting Perennial Ryegrass growth in Cork exhibits high performance, leveraging historical grass height data with RMSE of 2.74 and MAE of 3.46. Validation across a comprehensive dataset spanning 1,757 weeks over 34 years provides insights into optimal model configurations. This study enhances our understanding of model behavior, thereby improving reliability in grass growth forecasting and contributing to the advancement of sustainable dairy farming practices.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices</title>
<link>https://arxiv.org/abs/2511.03753</link>
<guid>https://arxiv.org/abs/2511.03753</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, privacy-preserving, electrocardiogram, IoT healthcare, GAF images<br />
<br />
Summary: <br />
This study introduces a federated learning framework for protecting privacy in ECG classification within IoT healthcare. It uses GAF images to extract features efficiently via CNNs, keeping sensitive medical data local. Experimental validation on diverse IoT devices shows high classification accuracy of 95.18% in a multi-client setup, surpassing a single-client baseline in accuracy and training time. The framework runs on a server, laptop, and Raspberry Pi 4, demonstrating edge-cloud integration. Despite the added complexity of GAF transformations, resource utilization and communication overhead are managed effectively. These results demonstrate the potential of lightweight, privacy-preserving AI for scalable and secure edge deployments in smart health systems. <div>
arXiv:2511.03753v1 Announce Type: new 
Abstract: This study presents a federated learning (FL) framework for privacy-preserving electrocardiogram (ECG) classification in Internet of Things (IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian Angular Field (GAF) images, the proposed approach enables efficient feature extraction through Convolutional Neural Networks (CNNs) while ensuring that sensitive medical data remain local to each device. This work is among the first to experimentally validate GAF-based federated ECG classification across heterogeneous IoT devices, quantifying both performance and communication efficiency. To evaluate feasibility in realistic IoT settings, we deployed the framework across a server, a laptop, and a resource-constrained Raspberry Pi 4, reflecting edge-cloud integration in IoT ecosystems. Experimental results demonstrate that the FL-GAF model achieves a high classification accuracy of 95.18% in a multi-client setup, significantly outperforming a single-client baseline in both accuracy and training time. Despite the added computational complexity of GAF transformations, the framework maintains efficient resource utilization and communication overhead. These findings highlight the potential of lightweight, privacy-preserving AI for IoT-based healthcare monitoring, supporting scalable and secure edge deployments in smart health systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Laugh, Relate, Engage: Stylized Comment Generation for Short Videos</title>
<link>https://arxiv.org/abs/2511.03757</link>
<guid>https://arxiv.org/abs/2511.03757</guid>
<content:encoded><![CDATA[
<div> Keywords: short-video platforms, comment generation, multi-agent system, style control, user engagement <br />
Summary: <br />
Short-video platforms are increasingly popular on the internet, creating a need for efficient and diverse comment generation. LOLGORITHM is a multi-agent system developed for generating controllable short-video comments with various styles such as puns, rhymes, memes, sarcasm, humor, and content extraction. Using a large language model, LOLGORITHM processes video inputs and allows fine-grained style control through prompt markers and examples. A bilingual dataset from Douyin and YouTube is used to evaluate comment generation across popular video genres. Results show LOLGORITHM outperforms baseline models in terms of originality, relevance, and style conformity, with preference rates exceeding 90% on Douyin and 87.55% on YouTube. This work presents a scalable and culturally adaptive framework for enhancing user engagement and creative interaction on short-video platforms. <br /> 
Summary: <div>
arXiv:2511.03757v1 Announce Type: new 
Abstract: Short-video platforms have become a central medium in the modern Internet landscape, where efficient information delivery and strong interactivity are reshaping user engagement and cultural dissemination. Among the various forms of user interaction, comments play a vital role in fostering community participation and enabling content re-creation. However, generating comments that are both compliant with platform guidelines and capable of exhibiting stylistic diversity and contextual awareness remains a significant challenge. We introduce LOLGORITHM, a modular multi-agent system (MAS) designed for controllable short-video comment generation. The system integrates video segmentation, contextual and affective analysis, and style-aware prompt construction. It supports six distinct comment styles: puns (homophones), rhyming, meme application, sarcasm (irony), plain humor, and content extraction. Powered by a multimodal large language model (MLLM), LOLGORITHM directly processes video inputs and achieves fine-grained style control through explicit prompt markers and few-shot examples. To support development and evaluation, we construct a bilingual dataset using official APIs from Douyin (Chinese) and YouTube (English), covering five popular video genres: comedy skits, daily life jokes, funny animal clips, humorous commentary, and talk shows. Evaluation combines automated metrics originality, relevance, and style conformity with a large-scale human preference study involving 40 videos and 105 participants. Results show that LOLGORITHM significantly outperforms baseline models, achieving preference rates of over 90% on Douyin and 87.55% on YouTube. This work presents a scalable and culturally adaptive framework for stylized comment generation on short-video platforms, offering a promising path to enhance user engagement and creative interaction.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes</title>
<link>https://arxiv.org/abs/2511.03768</link>
<guid>https://arxiv.org/abs/2511.03768</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal language models, Common-O benchmark, in-the-wild scenes, reasoning, hallucination

Summary: 
Multimodal language models excel at perceiving objects in single images but struggle with reasoning across scenes, leading to instances of hallucination. A new benchmark called Common-O has been introduced, consisting of in-the-wild scenes to test models' abilities to reason across scenes by identifying similarities. Despite strong performance on perception benchmarks, current models, including those trained for chain-of-thought reasoning, struggle on Common-O, achieving only 35% accuracy. The challenge is further evident on Common-O Complex, where the best model achieves just 1% accuracy. Models tend to hallucinate more when similar objects are present in a scene, possibly due to reliance on object co-occurrence seen during training. Scale and multi-image training show some promise in improving model performance. The availability of the Common-O benchmark aims to stimulate research into addressing the issue of hallucination in multimodal language models. 

<br /><br />Summary: <div>
arXiv:2511.03768v1 Announce Type: new 
Abstract: Multimodal language models possess a remarkable ability to handle an open-vocabulary's worth of objects. Yet the best models still suffer from hallucinations when reasoning about scenes in the real world, revealing a gap between their seemingly strong performance on existing perception benchmarks that are saturating and their reasoning in the real world. To address this gap, we build a novel benchmark of in-the-wild scenes that we call Common-O. With more than 10.5k examples using exclusively new images not found in web training data to avoid contamination, Common-O goes beyond just perception, inspired by cognitive tests for humans, to probe reasoning across scenes by asking "what's in common?". We evaluate leading multimodal language models, including models specifically trained to perform chain-of-thought reasoning. We find that perceiving objects in single images is tractable for most models, yet reasoning across scenes is very challenging even for the best models, including reasoning models. Despite saturating many leaderboards focusing on perception, the best performing model only achieves 35% on Common-O -- and on Common-O Complex, consisting of more complex scenes, the best model achieves only 1%. Curiously, we find models are more prone to hallucinate when similar objects are present in the scene, suggesting models may be relying on object co-occurrence seen during training. Among the models we evaluated, we found scale can provide modest improvements while models explicitly trained with multi-image inputs show bigger improvements, suggesting scaled multi-image training may offer promise. We make our benchmark publicly available to spur research into the challenge of hallucination when reasoning across scenes.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contamination Detection for VLMs using Multi-Modal Semantic Perturbation</title>
<link>https://arxiv.org/abs/2511.03774</link>
<guid>https://arxiv.org/abs/2511.03774</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, test-set leakage, detection methods, perturbation, benchmark redesign

Summary: 
This paper addresses concerns about inflated performance in Vision-Language Models (VLMs) due to test-set leakage from internet-scale pretraining corpora. Existing detection methods for contaminated VLMs are found to be inadequate, prompting the development of a novel detection approach based on multi-modal semantic perturbation. By deliberately contaminating open-source VLMs on popular benchmarks and testing the proposed method, it is shown that contaminated models fail to generalize under controlled perturbations. The effectiveness and robustness of the new detection approach are validated across multiple realistic contamination strategies. The code and perturbed dataset used in the study will be made publicly available. <div>
arXiv:2511.03774v1 Announce Type: new 
Abstract: Recent advances in Vision-Language Models (VLMs) have achieved state-of-the-art performance on numerous benchmark tasks. However, the use of internet-scale, often proprietary, pretraining corpora raises a critical concern for both practitioners and users: inflated performance due to test-set leakage. While prior works have proposed mitigation strategies such as decontamination of pretraining data and benchmark redesign for LLMs, the complementary direction of developing detection methods for contaminated VLMs remains underexplored. To address this gap, we deliberately contaminate open-source VLMs on popular benchmarks and show that existing detection approaches either fail outright or exhibit inconsistent behavior. We then propose a novel simple yet effective detection method based on multi-modal semantic perturbation, demonstrating that contaminated models fail to generalize under controlled perturbations. Finally, we validate our approach across multiple realistic contamination strategies, confirming its robustness and effectiveness. The code and perturbed dataset will be released publicly.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features</title>
<link>https://arxiv.org/abs/2511.03806</link>
<guid>https://arxiv.org/abs/2511.03806</guid>
<content:encoded><![CDATA[
<div> privacy-preserving machine learning, feature-level differential privacy, FusionDP, foundation models, imputation <br />
Summary: <br />
- The article discusses the importance of ensuring the privacy of sensitive training data in privacy-preserving machine learning, particularly in scenarios where privacy protection is needed for only specific features.
- Traditional methods like DP-SGD inject excessive noise and degrade model utility when enforcing privacy protection on all features in a sample.
- FusionDP is proposed as a two-step framework that leverages foundation models to impute sensitive features based on non-sensitive features, improving model utility while maintaining feature-level privacy.
- A modified DP-SGD algorithm is introduced to train models on both original and imputed features, preserving the privacy of sensitive features.
- Evaluation on sepsis prediction and clinical note classification tasks shows that FusionDP outperforms privacy-preserving baselines, illustrating the effectiveness of foundation model-driven imputation in balancing privacy and utility trade-offs across different modalities. <br /> <div>
arXiv:2511.03806v1 Announce Type: new 
Abstract: Ensuring the privacy of sensitive training data is crucial in privacy-preserving machine learning. However, in practical scenarios, privacy protection may be required for only a subset of features. For instance, in ICU data, demographic attributes like age and gender pose higher privacy risks due to their re-identification potential, whereas raw lab results are generally less sensitive. Traditional DP-SGD enforces privacy protection on all features in one sample, leading to excessive noise injection and significant utility degradation. We propose FusionDP, a two-step framework that enhances model utility under feature-level differential privacy. First, FusionDP leverages large foundation models to impute sensitive features given non-sensitive features, treating them as external priors that provide high-quality estimates of sensitive attributes without accessing the true values during model training. Second, we introduce a modified DP-SGD algorithm that trains models on both original and imputed features while formally preserving the privacy of the original sensitive features. We evaluate FusionDP on two modalities: a sepsis prediction task on tabular data from PhysioNet and a clinical note classification task from MIMIC-III. By comparing against privacy-preserving baselines, our results show that FusionDP significantly improves model performance while maintaining rigorous feature-level privacy, demonstrating the potential of foundation model-driven imputation to enhance the privacy-utility trade-off for various modalities.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair and Explainable Credit-Scoring under Concept Drift: Adaptive Explanation Frameworks for Evolving Populations</title>
<link>https://arxiv.org/abs/2511.03807</link>
<guid>https://arxiv.org/abs/2511.03807</guid>
<content:encoded><![CDATA[
<div> dynamic data distributions, credit-scoring systems, explainability techniques, adaptive explanation frameworks, fairness

Summary: 
This study explores the challenges posed by evolving borrower behaviors and changing economic conditions on credit-scoring systems. It introduces adaptive explanation frameworks that address these challenges by recalibrating interpretability and fairness in dynamically evolving credit models. Three adaptive SHAP variants are developed: per-slice explanation reweighting, drift-aware SHAP rebaselining, and online surrogate calibration. These methods are compared to static SHAP explanations using metrics of predictive performance, stability, and fairness. Results indicate that adaptive methods, particularly rebaselined and surrogate-based explanations, significantly improve temporal stability and reduce disparate impact across demographic groups without compromising predictive accuracy. Robustness tests confirm the resilience of adaptive explanations under real-world drift conditions. The study underscores the importance of adaptive explainability in maintaining transparency, accountability, and ethical reliability in data-driven credit systems and other domains where decision models evolve with population change.<br /><br /> <div>
arXiv:2511.03807v1 Announce Type: new 
Abstract: Evolving borrower behaviors, shifting economic conditions, and changing regulatory landscapes continuously reshape the data distributions underlying modern credit-scoring systems. Conventional explainability techniques, such as SHAP, assume static data and fixed background distributions, making their explanations unstable and potentially unfair when concept drift occurs. This study addresses that challenge by developing adaptive explanation frameworks that recalibrate interpretability and fairness in dynamically evolving credit models. Using a multi-year credit dataset, we integrate predictive modeling via XGBoost with three adaptive SHAP variants: (A) per-slice explanation reweighting that adjusts for feature distribution shifts, (B) drift-aware SHAP rebaselining with sliding-window background samples, and (C) online surrogate calibration using incremental Ridge regression. Each method is benchmarked against static SHAP explanations using metrics of predictive performance (AUC, F1), directional and rank stability (cosine, Kendall tau), and fairness (demographic parity and recalibration). Results show that adaptive methods, particularly rebaselined and surrogate-based explanations, substantially improve temporal stability and reduce disparate impact across demographic groups without degrading predictive accuracy. Robustness tests, including counterfactual perturbations, background sensitivity analysis, and proxy-variable detection, confirm the resilience of adaptive explanations under real-world drift conditions. These findings establish adaptive explainability as a practical mechanism for sustaining transparency, accountability, and ethical reliability in data-driven credit systems, and more broadly, in any domain where decision models evolve with population change.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Reasoning Efficiency through Prompt Difficulty Prediction</title>
<link>https://arxiv.org/abs/2511.03808</link>
<guid>https://arxiv.org/abs/2511.03808</guid>
<content:encoded><![CDATA[
<div> complex tasks, reasoning models, routing approach, problem difficulty, cost-efficient deployment <br />
<br />
Summary: 
The article proposes a routing approach to efficiently deploy reasoning language models by assigning problems to the most suitable model based on problem difficulty or model correctness predictors. This approach aims to reduce the computational cost of deploying large reasoning models without compromising accuracy. By leveraging intermediate representations from s1.1-32B, lightweight predictors are trained to guide the routing process across a pool of reasoning models. The study conducted on various math benchmarks demonstrates that difficulty-aware routing significantly improves efficiency compared to random assignment. The results indicate that the routing approach matches the performance of larger models like s1.1-32B while utilizing significantly less compute resources. This finding suggests that leveraging problem difficulty awareness in routing can enhance the cost-effectiveness of deploying reasoning models for complex tasks. <br /> <div>
arXiv:2511.03808v1 Announce Type: new 
Abstract: Reasoning language models perform well on complex tasks but are costly to deploy due to their size and long reasoning traces. We propose a routing approach that assigns each problem to the smallest model likely to solve it, reducing compute without sacrificing accuracy. Using intermediate representations from s1.1-32B, we train lightweight predictors of problem difficulty or model correctness to guide routing across a pool of reasoning models. On diverse math benchmarks, routing improves efficiency over random assignment and matches s1.1-32B's performance while using significantly less compute. Our results demonstrate that difficulty-aware routing is effective for cost-efficient deployment of reasoning models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA</title>
<link>https://arxiv.org/abs/2511.03809</link>
<guid>https://arxiv.org/abs/2511.03809</guid>
<content:encoded><![CDATA[
<div> Adaptive batch size methods, architecture-dependent efficacy, DEBA, gradient variance, gradient norm variation, loss variation, ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V3, ViT-B16, CIFAR-10, CIFAR-100.

Summary:  
1. DEBA (Dynamic Efficient Batch Adaptation) is introduced as an adaptive batch scheduler that considers gradient stability metrics to guide batch size adaptations.
2. Different architectures show varying levels of efficacy in batch size adaptation, with lightweight and medium-depth architectures benefiting the most.
3. Shallow residual networks consistently improve in accuracy and speedup, while deep residual networks exhibit more variability.
4. Already-stable architectures like ViT-B16 have minimal speedup benefits from adaptive scheduling.
5. A baseline characterization framework using gradient stability metrics can predict which architectures will benefit from adaptive scheduling.
6. Critical design choices like using sliding window statistics and cooldown periods between adaptations are essential for success in batch size adaptation. 

<br /><br />Summary: <div>
arXiv:2511.03809v1 Announce Type: new 
Abstract: Adaptive batch size methods aim to accelerate neural network training, but existing approaches apply identical adaptation strategies across all architectures, assuming a one-size-fits-all solution. We introduce DEBA (Dynamic Efficient Batch Adaptation), an adaptive batch scheduler that monitors gradient variance, gradient norm variation and loss variation to guide batch size adaptations. Through systematic evaluation across six architectures (ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V3, ViT-B16) on CIFAR-10 and CIFAR-100, with five random seeds per configuration, we demonstrate that the architecture fundamentally determines adaptation efficacy. Our findings reveal that: (1) lightweight and medium-depth architectures (MobileNet-V3, DenseNet-121, EfficientNet-B0) achieve a 45-62% training speedup with simultaneous accuracy improvements of 1-7%; (2) shallow residual networks (ResNet-18) show consistent gains of +2.4 - 4.0% in accuracy, 36 - 43% in speedup, while deep residual networks (ResNet-50) exhibit high variance and occasional degradation; (3) already-stable architectures (ViT-B16) show minimal speedup (6%) despite maintaining accuracy, indicating that adaptation benefits vary with baseline optimization characteristics. We introduce a baseline characterization framework using gradient stability metrics (stability score, gradient norm variation) that predicts which architectures will benefit from adaptive scheduling. Our ablation studies reveal critical design choices often overlooked in prior work: sliding window statistics (vs. full history) and sufficient cooldown periods (5+ epochs) between adaptations are essential for success. This work challenges the prevailing assumption that adaptive methods generalize across architectures and provides the first systematic evidence that batch size adaptation requires an architecture-aware design.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sketch-Augmented Features Improve Learning Long-Range Dependencies in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.03824</link>
<guid>https://arxiv.org/abs/2511.03824</guid>
<content:encoded><![CDATA[
<div> Randomized global embeddings, Sketched Random Features, graph neural networks, long-range dependencies, graph learning tasks. 
<br />
Summary: <br />
In this work, the authors propose injecting Sketched Random Features into Graph Neural Networks to address challenges such as oversquashing long-range information, oversmoothing node representations, and limited expressive power. These unique and distance-sensitive embeddings are topology-agnostic and help GNNs efficiently capture long-range dependencies. Analytical and empirical results show that this strategy improves GNN performance on real-world graph learning tasks. The Sketched Random Features offer a standalone solution and enhance existing techniques like graph positional encodings. The experimental results demonstrate consistent performance improvements over baseline GNNs, showcasing the effectiveness of this approach in enhancing the capabilities of GNNs for graph-structured data learning tasks. <div>
arXiv:2511.03824v1 Announce Type: new 
Abstract: Graph Neural Networks learn on graph-structured data by iteratively aggregating local neighborhood information. While this local message passing paradigm imparts a powerful inductive bias and exploits graph sparsity, it also yields three key challenges: (i) oversquashing of long-range information, (ii) oversmoothing of node representations, and (iii) limited expressive power. In this work we inject randomized global embeddings of node features, which we term \textit{Sketched Random Features}, into standard GNNs, enabling them to efficiently capture long-range dependencies. The embeddings are unique, distance-sensitive, and topology-agnostic -- properties which we analytically and empirically show alleviate the aforementioned limitations when injected into GNNs. Experimental results on real-world graph learning tasks confirm that this strategy consistently improves performance over baseline GNNs, offering both a standalone solution and a complementary enhancement to existing techniques such as graph positional encodings. Our source code is available at \href{https://github.com/ryienh/sketched-random-features}{https://github.com/ryienh/sketched-random-features}.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification</title>
<link>https://arxiv.org/abs/2511.03828</link>
<guid>https://arxiv.org/abs/2511.03828</guid>
<content:encoded><![CDATA[
<div> Diffusion model, reinforcement learning, offline-to-online transition, policy imitation, energy-based functions<br />
<br />
Summary: <br />
The article introduces a new method, StratDiff, to address the challenges of transitioning from offline to online reinforcement learning. By utilizing a diffusion model to learn prior knowledge from the offline dataset and refining it through energy-based functions, the proposed method facilitates smoother transitions in RL. StratDiff computes the KL divergence between generated and sampled actions to stratify training batches into offline-like and online-like subsets, allowing for tailored updates based on sample types. Integrated with existing methods Cal-QL and IQL, StratDiff shows significantly improved adaptability and stability in diverse RL settings, as demonstrated through extensive evaluations on D4RL benchmarks. <div>
arXiv:2511.03828v1 Announce Type: new 
Abstract: Transitioning from offline to online reinforcement learning (RL) poses critical challenges due to distributional shifts between the fixed behavior policy in the offline dataset and the evolving policy during online learning. Although this issue is widely recognized, few methods attempt to explicitly assess or utilize the distributional structure of the offline data itself, leaving a research gap in adapting learning strategies to different types of samples. To address this challenge, we propose an innovative method, Energy-Guided Diffusion Stratification (StratDiff), which facilitates smoother transitions in offline-to-online RL. StratDiff deploys a diffusion model to learn prior knowledge from the offline dataset. It then refines this knowledge through energy-based functions to improve policy imitation and generate offline-like actions during online fine-tuning. The KL divergence between the generated action and the corresponding sampled action is computed for each sample and used to stratify the training batch into offline-like and online-like subsets. Offline-like samples are updated using offline objectives, while online-like samples follow online learning strategies. We demonstrate the effectiveness of StratDiff by integrating it with off-the-shelf methods Cal-QL and IQL. Extensive empirical evaluations on D4RL benchmarks show that StratDiff significantly outperforms existing methods, achieving enhanced adaptability and more stable performance across diverse RL settings.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Higher-Order Causal Structure Learning with Additive Models</title>
<link>https://arxiv.org/abs/2511.03831</link>
<guid>https://arxiv.org/abs/2511.03831</guid>
<content:encoded><![CDATA[
<div> Keywords: causal structure learning, higher-order interactions, causal additive model, directed acyclic hypergraph, identifiability

Summary:
Causal structure learning is crucial for inferring causal insights from data, but the consideration of higher-order interactions has been lacking. This work extends the causal additive model (CAM) to include higher-order interactions, represented by directed acyclic hypergraphs. The novel structure introduces a second level of modularity to the traditional structure learning problem. Identifiability results for hyper DAGs are provided, expanding the typical Markov equivalence classes. Learning the more complex hypergraph structures may lead to better empirical results, with more restrictive assumptions like CAM resulting in easier-to-learn hyper DAGs and improved finite sample complexity. An extension of the greedy CAM algorithm is developed to handle the complex hyper DAG search space, showcasing its empirical usefulness through synthetic experiments. This work highlights the importance of considering higher-order interactions in causal discovery. 

<br /><br />Summary: <div>
arXiv:2511.03831v1 Announce Type: new 
Abstract: Causal structure learning has long been the central task of inferring causal insights from data. Despite the abundance of real-world processes exhibiting higher-order mechanisms, however, an explicit treatment of interactions in causal discovery has received little attention. In this work, we focus on extending the causal additive model (CAM) to additive models with higher-order interactions. This second level of modularity we introduce to the structure learning problem is most easily represented by a directed acyclic hypergraph which extends the DAG. We introduce the necessary definitions and theoretical tools to handle the novel structure we introduce and then provide identifiability results for the hyper DAG, extending the typical Markov equivalence classes. We next provide insights into why learning the more complex hypergraph structure may actually lead to better empirical results. In particular, more restrictive assumptions like CAM correspond to easier-to-learn hyper DAGs and better finite sample complexity. We finally develop an extension of the greedy CAM algorithm which can handle the more complex hyper DAG search space and demonstrate its empirical usefulness in synthetic experiments.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Q-Value Updates in Deep Q-Learning via Successor-State Prediction</title>
<link>https://arxiv.org/abs/2511.03836</link>
<guid>https://arxiv.org/abs/2511.03836</guid>
<content:encoded><![CDATA[
<div> Successor-state Aggregation Deep Q-Network, SADQ, stochastic transition model, stable value updates, policy-aligned, reduced training variance<br />
Summary: <br />
The proposed Successor-state Aggregation Deep Q-Network (SADQ) addresses the limitations of DQN by explicitly modeling environment dynamics using a stochastic transition model. By integrating successor-state distributions into the Q-value estimation process, SADQ enables more stable and policy-aligned value updates, reducing training variance. This approach also leads to a more efficient action selection strategy based on the transition structure. The theoretical guarantees assure unbiased value estimates with SADQ. Empirical results across various RL benchmarks and real-world control tasks show that SADQ consistently outperforms DQN variants in terms of stability and learning efficiency. <div>
arXiv:2511.03836v1 Announce Type: new 
Abstract: Deep Q-Networks (DQNs) estimate future returns by learning from transitions sampled from a replay buffer. However, the target updates in DQN often rely on next states generated by actions from past, potentially suboptimal, policy. As a result, these states may not provide informative learning signals, causing high variance into the update process. This issue is exacerbated when the sampled transitions are poorly aligned with the agent's current policy. To address this limitation, we propose the Successor-state Aggregation Deep Q-Network (SADQ), which explicitly models environment dynamics using a stochastic transition model. SADQ integrates successor-state distributions into the Q-value estimation process, enabling more stable and policy-aligned value updates. Additionally, it explores a more efficient action selection strategy with the modeled transition structure. We provide theoretical guarantees that SADQ maintains unbiased value estimates while reducing training variance. Our extensive empirical results across standard RL benchmarks and real-world vector-based control tasks demonstrate that SADQ consistently outperforms DQN variants in both stability and learning efficiency.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmark Datasets for Lead-Lag Forecasting on Social Platforms</title>
<link>https://arxiv.org/abs/2511.03877</link>
<guid>https://arxiv.org/abs/2511.03877</guid>
<content:encoded><![CDATA[
<div> Keywords: Lead-Lag Forecasting, Time-Series Traces, Benchmark Datasets, Social Platforms, Forecasting Paradigm

Summary:
Lead-Lag Forecasting (LLF) involves predicting outcomes in social and collaborative platforms based on early interactions. The authors introduce two benchmark datasets, arXiv and GitHub, along with other potential domains exhibiting lead-lag dynamics. These datasets provide a comprehensive testbed for LLF research, capturing long-term dynamics and avoiding survivorship bias. Technical details of data curation, statistical tests, and baseline benchmarking are provided. LLF is established as a novel forecasting paradigm for social and usage data, opening avenues for systematic exploration. The datasets, along with documentation, are available at a dedicated data portal. Overall, this study serves as an empirical foundation for advancing LLF research in diverse domains. 

<br /><br />Summary: <div>
arXiv:2511.03877v1 Announce Type: new 
Abstract: Social and collaborative platforms emit multivariate time-series traces in which early interactions-such as views, likes, or downloads-are followed, sometimes months or years later, by higher impact like citations, sales, or reviews. We formalize this setting as Lead-Lag Forecasting (LLF): given an early usage channel (the lead), predict a correlated but temporally shifted outcome channel (the lag). Despite the ubiquity of such patterns, LLF has not been treated as a unified forecasting problem within the time-series community, largely due to the absence of standardized datasets. To anchor research in LLF, here we present two high-volume benchmark datasets-arXiv (accesses -> citations of 2.3M papers) and GitHub (pushes/stars -> forks of 3M repositories)-and outline additional domains with analogous lead-lag dynamics, including Wikipedia (page views -> edits), Spotify (streams -> concert attendance), e-commerce (click-throughs -> purchases), and LinkedIn profile (views -> messages). Our datasets provide ideal testbeds for lead-lag forecasting, by capturing long-horizon dynamics across years, spanning the full spectrum of outcomes, and avoiding survivorship bias in sampling. We documented all technical details of data curation and cleaning, verified the presence of lead-lag dynamics through statistical and classification tests, and benchmarked parametric and non-parametric baselines for regression. Our study establishes LLF as a novel forecasting paradigm and lays an empirical foundation for its systematic exploration in social and usage data. Our data portal with downloads and documentation is available at https://lead-lag-forecasting.github.io/.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets</title>
<link>https://arxiv.org/abs/2511.03911</link>
<guid>https://arxiv.org/abs/2511.03911</guid>
<content:encoded><![CDATA[
<div> Decomposition, deep networks, hyperdimensional computing, compression, class prototypes  
Summary:  
DecoHD introduces a novel approach to shrinking deep networks in hyperdimensional computing by learning directly in a decomposed parameterization. It achieves extreme memory savings while maintaining high accuracy, staying within around 0.1-0.15% of a non-reduced baseline. DecoHD is more robust to random bit-flip noise and reaches its accuracy plateau with significantly fewer parameters, up to ~97% less. In hardware, it delivers substantial energy and speed gains over various processors, highlighting its efficiency. Overall, DecoHD offers a promising solution for compact yet powerful deep networks in hyperdimensional computing.  
Summary: <div>
arXiv:2511.03911v1 Announce Type: new 
Abstract: Decomposition is a proven way to shrink deep networks without changing I/O. We bring this idea to hyperdimensional computing (HDC), where footprint cuts usually shrink the feature axis and erode concentration and robustness. Prior HDC decompositions decode via fixed atomic hypervectors, which are ill-suited for compressing learned class prototypes. We introduce DecoHD, which learns directly in a decomposed HDC parameterization: a small, shared set of per-layer channels with multiplicative binding across layers and bundling at the end, yielding a large representational space from compact factors. DecoHD compresses along the class axis via a lightweight bundling head while preserving native bind-bundle-score; training is end-to-end, and inference remains pure HDC, aligning with in/near-memory accelerators. In evaluation, DecoHD attains extreme memory savings with only minor accuracy degradation under tight deployment budgets. On average it stays within about 0.1-0.15% of a strong non-reduced HDC baseline (worst case 5.7%), is more robust to random bit-flip noise, reaches its accuracy plateau with up to ~97% fewer trainable parameters, and -- in hardware -- delivers roughly 277x/35x energy/speed gains over a CPU (AMD Ryzen 9 9950X), 13.5x/3.7x over a GPU (NVIDIA RTX 4090), and 2.0x/2.4x over a baseline HDC ASIC.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Predicting Sociodemographics from Mobility Signals</title>
<link>https://arxiv.org/abs/2511.03924</link>
<guid>https://arxiv.org/abs/2511.03924</guid>
<content:encoded><![CDATA[
<div> Keywords: sociodemographic attributes, mobility data, predictive accuracy, multitask learning, generalization

Summary:
The article focuses on inferring sociodemographic attributes from mobility data to aid transportation planners in utilizing collected datasets effectively. It addresses challenges by introducing a set of higher-order mobility descriptors based on directed mobility graphs, enhancing predictive accuracy while maintaining interpretability. Metrics and visual diagnostic tools are introduced to quantify uncertainty, ensuring evenness between model confidence and accuracy. A multitask learning framework is developed to jointly predict multiple sociodemographic attributes from a shared representation, outperforming single-task models, particularly when training data are limited or when applying models across different time periods. This approach improves generalization and sample efficiency in predicting age, gender, income, and household structure from mobility patterns.
 
<br /><br />Summary: <div>
arXiv:2511.03924v1 Announce Type: new 
Abstract: Inferring sociodemographic attributes from mobility data could help transportation planners better leverage passively collected datasets, but this task remains difficult due to weak and inconsistent relationships between mobility patterns and sociodemographic traits, as well as limited generalization across contexts. We address these challenges from three angles. First, to improve predictive accuracy while retaining interpretability, we introduce a behaviorally grounded set of higher-order mobility descriptors based on directed mobility graphs. These features capture structured patterns in trip sequences, travel modes, and social co-travel, and significantly improve prediction of age, gender, income, and household structure over baselines features. Second, we introduce metrics and visual diagnostic tools that encourage evenness between model confidence and accuracy, enabling planners to quantify uncertainty. Third, to improve generalization and sample efficiency, we develop a multitask learning framework that jointly predicts multiple sociodemographic attributes from a shared representation. This approach outperforms single-task models, particularly when training data are limited or when applying models across different time periods (i.e., when the test set distribution differs from the training set).
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynQuE: Estimating Synthetic Dataset Quality Without Annotations</title>
<link>https://arxiv.org/abs/2511.03928</link>
<guid>https://arxiv.org/abs/2511.03928</guid>
<content:encoded><![CDATA[
<div> quality estimation, synthetic dataset, real-world task performance, proxy metrics, complex planning tasks

Summary:
The article introduces the Synthetic Dataset Quality Estimation (SynQuE) problem, which aims to rank synthetic datasets based on their expected real-world task performance using limited unannotated real data. It establishes benchmarks for this problem by evaluating proxy metrics that select synthetic data to maximize task performance on real data. The study introduces novel proxy metrics for SynQuE by adapting distribution and diversity-based distance measures via embedding models. Additionally, a new proxy metric called LENS, leveraging large language model reasoning, is proposed to address shortcomings on complex planning tasks. Results show that SynQuE proxies correlate with real task performance across various tasks like sentiment analysis, Text2SQL, web navigation, and image classification, with LENS consistently outperforming others on complex tasks. For example, on text-to-SQL parsing, training on the top-3 synthetic datasets selected via SynQuE proxies can significantly improve accuracy compared to random selection. This work establishes SynQuE as a practical framework for synthetic data selection under data scarcity, highlighting the need for future research on foundation model-based data characterization and fine-grained data selection.<br /><br />Summary: <div>
arXiv:2511.03928v1 Announce Type: new 
Abstract: We introduce and formalize the Synthetic Dataset Quality Estimation (SynQuE) problem: ranking synthetic datasets by their expected real-world task performance using only limited unannotated real data. This addresses a critical and open challenge where data is scarce due to collection costs or privacy constraints. We establish the first comprehensive benchmarks for this problem by introducing and evaluating proxy metrics that choose synthetic data for training to maximize task performance on real data. We introduce the first proxy metrics for SynQuE by adapting distribution and diversity-based distance measures to our context via embedding models. To address the shortcomings of these metrics on complex planning tasks, we propose LENS, a novel proxy that leverages large language model reasoning. Our results show that SynQuE proxies correlate with real task performance across diverse tasks, including sentiment analysis, Text2SQL, web navigation, and image classification, with LENS consistently outperforming others on complex tasks by capturing nuanced characteristics. For instance, on text-to-SQL parsing, training on the top-3 synthetic datasets selected via SynQuE proxies can raise accuracy from 30.4% to 38.4 (+8.1)% on average compared to selecting data indiscriminately. This work establishes SynQuE as a practical framework for synthetic data selection under real-data scarcity and motivates future research on foundation model-based data characterization and fine-grained data selection.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NVIDIA Nemotron Nano V2 VL</title>
<link>https://arxiv.org/abs/2511.03929</link>
<guid>https://arxiv.org/abs/2511.03929</guid>
<content:encoded><![CDATA[
<div> Keywords: Nemotron Nano V2 VL, document understanding, video comprehension, reasoning tasks, model enhancements

Summary: 
Nemotron Nano V2 VL is the latest model in the Nemotron vision-language series, focusing on real-world document understanding, long video comprehension, and reasoning tasks. It surpasses the previous model, Llama-3.1-Nemotron-Nano-VL-8B, with improved performance in both vision and text domains. The model architecture, datasets, and training recipes have all been enhanced to achieve higher inference throughput in scenarios involving long documents and videos. Nemotron Nano V2 VL is built on a hybrid Mamba-Transformer LLM and utilizes innovative token reduction techniques. Model checkpoints are available in BF16, FP8, and FP4 formats, and a significant portion of datasets, recipes, and training code are being made public. This release aims to provide researchers and developers with advanced tools for vision-language tasks. 

<br /><br />Summary: <div>
arXiv:2511.03929v1 Announce Type: new 
Abstract: We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LogHD: Robust Compression of Hyperdimensional Classifiers via Logarithmic Class-Axis Reduction</title>
<link>https://arxiv.org/abs/2511.03938</link>
<guid>https://arxiv.org/abs/2511.03938</guid>
<content:encoded><![CDATA[
<div> logarithmic class-axis reduction, hyperdimensional computing, memory-efficient, robustness, energy-efficient <br />
Summary:<br />
The article introduces LogHD, a novel approach in hyperdimensional computing (HDC) that utilizes a logarithmic class-axis reduction to reduce memory requirements while maintaining robustness. By replacing traditional per-class prototypes with bundle hypervectors and decoding in a lower-dimensional activation space, LogHD significantly cuts memory to O(D log C) without compromising dimensionality D. It leverages a capacity-aware codebook and profile-based decoding, complementing feature-axis sparsification for enhanced efficiency. Experimental results demonstrate that LogHD achieves competitive accuracy with smaller models and improved resilience against bit flips compared to feature-axis compression. An ASIC implementation of LogHD showcases remarkable energy efficiency and speedups over mainstream processors, outperforming both AMD and NVIDIA counterparts while maintaining target accuracy levels. Compared to a feature-axis HDC ASIC baseline, LogHD is more energy-efficient and faster, underscoring its potential for memory-constrained systems.<br /> <div>
arXiv:2511.03938v1 Announce Type: new 
Abstract: Hyperdimensional computing (HDC) suits memory, energy, and reliability-constrained systems, yet the standard "one prototype per class" design requires $O(CD)$ memory (with $C$ classes and dimensionality $D$). Prior compaction reduces $D$ (feature axis), improving storage/compute but weakening robustness. We introduce LogHD, a logarithmic class-axis reduction that replaces the $C$ per-class prototypes with $n\!\approx\!\lceil\log_k C\rceil$ bundle hypervectors (alphabet size $k$) and decodes in an $n$-dimensional activation space, cutting memory to $O(D\log_k C)$ while preserving $D$. LogHD uses a capacity-aware codebook and profile-based decoding, and composes with feature-axis sparsification. Across datasets and injected bit flips, LogHD attains competitive accuracy with smaller models and higher resilience at matched memory. Under equal memory, it sustains target accuracy at roughly $2.5$-$3.0\times$ higher bit-flip rates than feature-axis compression; an ASIC instantiation delivers $498\times$ energy efficiency and $62.6\times$ speedup over an AMD Ryzen 9 9950X and $24.3\times$/$6.58\times$ over an NVIDIA RTX 4090, and is $4.06\times$ more energy-efficient and $2.19\times$ faster than a feature-axis HDC ASIC baseline.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods</title>
<link>https://arxiv.org/abs/2511.03939</link>
<guid>https://arxiv.org/abs/2511.03939</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Human Feedback, Large Language Models, Multi-modal alignment, Cultural fairness

Summary:
Reinforcement Learning from Human Feedback (RLHF) is the standard method for aligning Large Language Models (LLMs), but advancements have expanded beyond text-based approaches. This survey delves into the new frontier of alignment research by addressing gaps in multi-modal alignment, cultural fairness, and low-latency optimization. Foundational algorithms like PPO, DPO, and GRPO are reviewed, followed by an in-depth analysis of the latest innovations. The comparative synthesis of these techniques and identification of open challenges serve as a roadmap for researchers aiming to build more robust, efficient, and equitable AI systems. <div>
arXiv:2511.03939v1 Announce Type: new 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for aligning Large Language Models (LLMs), yet recent progress has moved beyond canonical text-based methods. This survey synthesizes the new frontier of alignment research by addressing critical gaps in multi-modal alignment, cultural fairness, and low-latency optimization. To systematically explore these domains, we first review foundational algo- rithms, including PPO, DPO, and GRPO, before presenting a detailed analysis of the latest innovations. By providing a comparative synthesis of these techniques and outlining open challenges, this work serves as an essential roadmap for researchers building more robust, efficient, and equitable AI systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Score Learning for Quickest Change Detection in Markov Transition Kernels</title>
<link>https://arxiv.org/abs/2511.03953</link>
<guid>https://arxiv.org/abs/2511.03953</guid>
<content:encoded><![CDATA[
<div> quickest change detection, Markov processes, unknown transition kernels, score-based CUSUM procedure, high-dimensional data<br />
<br />
Summary: 
The article addresses the issue of quickest change detection in Markov processes with unknown transition kernels. It proposes learning the conditional score directly from sample data to avoid explicit likelihood evaluation. The developed score-based CUSUM procedure utilizes Hyvarinen score differences for change detection in the kernel, ensuring bounded increments by using a truncated version of the statistic. The article proves exponential lower bounds on the mean time to false alarm using Hoeffding's inequality for uniformly ergodic Markov processes. Additionally, asymptotic upper bounds on detection delay are established, offering theoretical guarantees and practical feasibility for score-based detection in high-dimensional Markov models. <div>
arXiv:2511.03953v1 Announce Type: new 
Abstract: We address the problem of quickest change detection in Markov processes with unknown transition kernels. The key idea is to learn the conditional score $\nabla_{\mathbf{y}} \log p(\mathbf{y}|\mathbf{x})$ directly from sample pairs $( \mathbf{x},\mathbf{y})$, where both $\mathbf{x}$ and $\mathbf{y}$ are high-dimensional data generated by the same transition kernel. In this way, we avoid explicit likelihood evaluation and provide a practical way to learn the transition dynamics. Based on this estimation, we develop a score-based CUSUM procedure that uses conditional Hyvarinen score differences to detect changes in the kernel. To ensure bounded increments, we propose a truncated version of the statistic. With Hoeffding's inequality for uniformly ergodic Markov processes, we prove exponential lower bounds on the mean time to false alarm. We also prove asymptotic upper bounds on detection delay. These results give both theoretical guarantees and practical feasibility for score-based detection in high-dimensional Markov models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivacyCD: Hierarchical Unlearning for Protecting Student Privacy in Cognitive Diagnosis</title>
<link>https://arxiv.org/abs/2511.03966</link>
<guid>https://arxiv.org/abs/2511.03966</guid>
<content:encoded><![CDATA[
<div> Hierarchical importance-guided forgetting, cognitive diagnosis models, data unlearning, privacy preservation, AI systems <br />
<br />
Summary: 
The article addresses the need for data removal from cognitive diagnosis (CD) models to adhere to privacy regulations. Existing CD models lack effective data unlearning mechanisms, leading to the development of a novel algorithm called hierarchical importance-guided forgetting (HIF). This algorithm leverages parameter importance in CD models, utilizing layer-wise characteristics to identify and remove specific student data accurately. Experimental results on real-world datasets demonstrate that HIF outperforms baseline methods in terms of effectiveness, efficiency, and model utility. By introducing HIF, the paper presents the first systematic study on data unlearning for CD models, offering a practical solution for responding to user data removal requests and deploying high-performance, privacy-preserving AI systems. <div>
arXiv:2511.03966v1 Announce Type: new 
Abstract: The need to remove specific student data from cognitive diagnosis (CD) models has become a pressing requirement, driven by users' growing assertion of their "right to be forgotten". However, existing CD models are largely designed without privacy considerations and lack effective data unlearning mechanisms. Directly applying general purpose unlearning algorithms is suboptimal, as they struggle to balance unlearning completeness, model utility, and efficiency when confronted with the unique heterogeneous structure of CD models. To address this, our paper presents the first systematic study of the data unlearning problem for CD models, proposing a novel and efficient algorithm: hierarchical importanceguided forgetting (HIF). Our key insight is that parameter importance in CD models exhibits distinct layer wise characteristics. HIF leverages this via an innovative smoothing mechanism that combines individual and layer, level importance, enabling a more precise distinction of parameters associated with the data to be unlearned. Experiments on three real world datasets show that HIF significantly outperforms baselines on key metrics, offering the first effective solution for CD models to respond to user data removal requests and for deploying high-performance, privacy preserving AI systems
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Asymptotic Optimization and Generalization Bounds for Stochastic Gauss-Newton in Overparameterized Models</title>
<link>https://arxiv.org/abs/2511.03972</link>
<guid>https://arxiv.org/abs/2511.03972</guid>
<content:encoded><![CDATA[
<div> convergence bounds, stochastic Gauss-Newton, generalization bounds, deep learning, parameter space <br />
Summary: 
The article explores the impact of a stochastic Gauss-Newton (SGN) optimization method on generalization in deep learning. The study focuses on training overparameterized deep neural networks with smooth activations in regression scenarios. The researchers establish finite-time convergence bounds using a variable-metric analysis, considering parameters such as batch size, network width, and depth. Additionally, non-asymptotic generalization bounds for SGN are derived through uniform stability analysis in the overparameterized regime, highlighting the effects of curvature, batch size, and overparameterization on generalization performance. The results indicate that a higher minimum eigenvalue of the Gauss-Newton matrix along the optimization path leads to improved stability and tighter generalization bounds for SGN. <div>
arXiv:2511.03972v1 Announce Type: new 
Abstract: An important question in deep learning is how higher-order optimization methods affect generalization. In this work, we analyze a stochastic Gauss-Newton (SGN) method with Levenberg-Marquardt damping and mini-batch sampling for training overparameterized deep neural networks with smooth activations in a regression setting. Our theoretical contributions are twofold. First, we establish finite-time convergence bounds via a variable-metric analysis in parameter space, with explicit dependencies on the batch size, network width and depth. Second, we derive non-asymptotic generalization bounds for SGN using uniform stability in the overparameterized regime, characterizing the impact of curvature, batch size, and overparameterization on generalization performance. Our theoretical results identify a favorable generalization regime for SGN in which a larger minimum eigenvalue of the Gauss-Newton matrix along the optimization path yields tighter stability bounds.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction</title>
<link>https://arxiv.org/abs/2511.03976</link>
<guid>https://arxiv.org/abs/2511.03976</guid>
<content:encoded><![CDATA[
<div> Keywords: SARS-CoV-2, evolutionary trajectory, PETRA, transformer approach, mutation prediction

Summary: 
PETRA is a novel transformer approach designed to predict future mutations of SARS-CoV-2 by utilizing evolutionary trajectories derived from phylogenetic trees. This method effectively reduces sequencing noise and captures the hierarchical structure of viral evolution. By implementing a weighted training framework to address geographical and temporal imbalances in global sequence data, PETRA outperforms baseline models in predicting nucleotide and spike amino-acid mutations. It achieves a high weighted recall@1 for both types of mutations and demonstrates its ability to predict mutations in major clades such as 24F(XEC) and 25A(LP.8.1) in real-time. The open-sourced code for PETRA is available on GitHub, providing a valuable tool for researchers and public health officials working on understanding and combating the ongoing evolution of SARS-CoV-2. 
<br /><br />Summary: <div>
arXiv:2511.03976v1 Announce Type: new 
Abstract: Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable evolutionary trajectory, characterized by the continual emergence of immune-evasive variants. This poses persistent challenges to public health and vaccine development.
  While large-scale generative pre-trained transformers (GPTs) have revolutionized the modeling of sequential data, their direct applications to noisy viral genomic sequences are limited. In this paper, we introduce PETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based on evolutionary trajectories derived from phylogenetic trees rather than raw RNA sequences. This method effectively mitigates sequencing noise and captures the hierarchical structure of viral evolution.
  With a weighted training framework to address substantial geographical and temporal imbalances in global sequence data, PETRA excels in predicting future SARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide mutations and 17.10\% for spike amino-acid mutations, compared to 0.49% and 6.64% respectively for the best baseline. PETRA also demonstrates its ability to aid in the real-time mutation prediction of major clades like 24F(XEC) and 25A(LP.8.1). The code is open sourced on https://github.com/xz-keg/PETra
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural Priors and Modular Adapters in the Composable Fine-Tuning Algorithm of Large-Scale Models</title>
<link>https://arxiv.org/abs/2511.03981</link>
<guid>https://arxiv.org/abs/2511.03981</guid>
<content:encoded><![CDATA[
<div> graph structural priors, modular adapters, fine-tuning, multi-task, computational efficiency

Summary:
The paper introduces a composable fine-tuning method that combines graph structural priors and modular adapters to address challenges faced by large pre-trained models in multi-task adaptation. By using a relation matrix to model task dependencies and incorporating modular adapters with a pluggable mechanism, the method improves parameter efficiency, training stability, and overall computational efficiency. Experimental analyses demonstrate the method's consistency and superior performance, highlighting key factors such as routing temperature, gating thresholds, and relation matrix regularization strength. The proposed framework enhances task prediction accuracy, adapter weight allocation precision, and model lightweight design. The synergistic advantages of graph priors and modular mechanisms in composable fine-tuning contribute to the method's effectiveness in multi-task adaptation. 

<br /><br />Summary: <div>
arXiv:2511.03981v1 Announce Type: new 
Abstract: This paper proposes a composable fine-tuning method that integrates graph structural priors with modular adapters to address the high computational cost and structural instability faced by large-scale pre-trained models in multi-task adaptation. The method introduces a relation matrix to model dependencies among tasks, explicitly encoding correlations between nodes and paths into graph structural priors, which provide unified structural constraints for adapter weight allocation and path selection. Modular adapters are embedded into different layers through low-rank mapping and a pluggable mechanism, enabling efficient cross-task composition and reuse under prior guidance. This mechanism not only improves parameter efficiency and training stability but also alleviates path conflicts and redundant computation in multi-task scenarios. Furthermore, experiments on hyperparameter sensitivity, environmental sensitivity, and data sensitivity are conducted to systematically analyze key factors such as routing temperature, gating thresholds, and relation matrix regularization strength, verifying the consistency and superior performance of the method under structural constraints. The results demonstrate that the proposed framework significantly enhances task prediction accuracy, adapter weight allocation precision, and overall computational efficiency while maintaining model lightweight design, highlighting the synergistic advantages of graph priors and modular mechanisms in composable fine-tuning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TwIST: Rigging the Lottery in Transformers with Independent Subnetwork Training</title>
<link>https://arxiv.org/abs/2511.03983</link>
<guid>https://arxiv.org/abs/2511.03983</guid>
<content:encoded><![CDATA[
<div> Keywords: TwIST, distributed training, large language model, sparsification, structured matrices <br />
Summary:<br />
TwIST is a distributed training framework designed for efficient sparsification of large language models (LLMs). It trains multiple subnetworks simultaneously, aggregates their parameters periodically, and resamples new subnetworks during training to identify high-quality subnetworks without the need for post-training procedures like calibration or Hessian-based recovery. This approach allows for zero-cost pruning at deployment while maintaining competitive perplexity levels compared to state-of-the-art sparsification methods. TwIST excels particularly well under high sparsity levels, outperforming baseline methods significantly. Unlike unstructured pruning, TwIST produces structured, dense matrices, enabling practical inference speedups and memory reductions on standard hardware. The framework offers an efficient path for deploying sparse LLMs without requiring additional fine-tuning or recovery overhead. <br /><br />Summary: <div>
arXiv:2511.03983v1 Announce Type: new 
Abstract: We introduce TwIST, a distributed training framework for efficient large language model (LLM) sparsification. TwIST trains multiple subnetworks in parallel, periodically aggregates their parameters, and resamples new subnetworks during training. This process identifies high-quality subnetworks ("golden tickets") without requiring post-training procedures such as calibration or Hessian-based recovery. As a result, TwIST enables zero-cost pruning at deployment time while achieving perplexity competitive with state-of-the-art post-training sparsification methods. The benefits are most pronounced under aggressive sparsity (e.g., 50%+), where TwIST significantly outperforms baseline methods; for example, reaching 23.14 PPL compared to 31.64 for the closest prior approach. Unlike unstructured pruning, TwIST produces structured, dense matrices that offer practical inference speedups and memory reductions on commodity hardware (e.g., CPUs) that do not support efficient sparse computation. TwIST provides an efficient training-time path to deployable sparse LLMs without additional fine-tuning or recovery overhead.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes</title>
<link>https://arxiv.org/abs/2511.03986</link>
<guid>https://arxiv.org/abs/2511.03986</guid>
<content:encoded><![CDATA[
<div> Keywords: diabetes, prediabetes, continuous glucose monitoring, machine learning, precision medicine

Summary:
Continuous glucose monitoring technologies are revolutionizing the classification and understanding of diabetes and prediabetes by capturing dynamic metabolic data beyond static glucose thresholds. Leveraging high-resolution glucose data and machine learning models, these technologies can accurately predict insulin resistance and beta-cell function. Personalized metabolic phenotyping based on postprandial glycemic responses to specific foods can serve as biomarkers for individual metabolic subtypes. The integration of wearable data on diet, sleep, and physical activity patterns reveals unique associations with metabolic dysfunctions, guiding precision lifestyle interventions. The effectiveness of dietary interventions in attenuating postprandial glucose responses varies based on metabolic phenotypes. By deconstructing early dysglycemia into distinct subphenotypes, continuous glucose monitoring enables targeted nutritional, behavioral, and pharmacological strategies tailored to individual metabolic defects, ushering in a new era of precision diabetes prevention.<br /><br />Summary: <div>
arXiv:2511.03986v1 Announce Type: new 
Abstract: The classification of diabetes and prediabetes by static glucose thresholds obscures the pathophysiological dysglycemia heterogeneity, primarily driven by insulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This review demonstrates that continuous glucose monitoring and wearable technologies enable a paradigm shift towards non-invasive, dynamic metabolic phenotyping. We show evidence that machine learning models can leverage high-resolution glucose data from at-home, CGM-enabled oral glucose tolerance tests to accurately predict gold-standard measures of muscle IR and beta-cell function. This personalized characterization extends to real-world nutrition, where an individual's unique postprandial glycemic response (PPGR) to standardized meals, such as the relative glucose spike to potatoes versus grapes, could serve as a biomarker for their metabolic subtype. Moreover, integrating wearable data reveals that habitual diet, sleep, and physical activity patterns, particularly their timing, are uniquely associated with specific metabolic dysfunctions, informing precision lifestyle interventions. The efficacy of dietary mitigators in attenuating PPGR is also shown to be phenotype-dependent. Collectively, this evidence demonstrates that CGM can deconstruct the complexity of early dysglycemia into distinct, actionable subphenotypes. This approach moves beyond simple glycemic control, paving the way for targeted nutritional, behavioral, and pharmacological strategies tailored to an individual's core metabolic defects, thereby paving the way for a new era of precision diabetes prevention.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiscale Astrocyte Network Calcium Dynamics for Biologically Plausible Intelligence in Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.03993</link>
<guid>https://arxiv.org/abs/2511.03993</guid>
<content:encoded><![CDATA[
<div> Keywords: Network anomaly detection, Ca$^{2+}$-modulated learning framework, deep neural network, astrocytic Ca$^{2+}$ signaling, cybersecurity applications

Summary:
The article introduces a novel approach for network anomaly detection using a Ca$^{2+}$-modulated learning framework inspired by astrocytic Ca$^{2+}$ signaling in the brain. By combining a multicellular astrocyte dynamics simulator with a deep neural network (DNN), the proposed model effectively addresses the challenges of concept drift and emerging threats in cybersecurity. The simulator simulates astrocytic Ca$^{2+}$ dynamics through key mechanisms such as IP$_3$-mediated Ca$^{2+}$ release, SERCA pump uptake, and conductance-aware diffusion. Evaluation on CTU-13 (Neris) network traffic data shows that the Ca$^{2+}$-gated model outperforms a baseline DNN, achieving high accuracy with reduced false positives and negatives. Importantly, this enhanced performance comes with minimal runtime overhead once Ca$^{2+}$ trajectories are precomputed. The Ca$^{2+}$-modulated learning framework provides a versatile solution for streaming detection tasks that require rapid adaptation to evolving data patterns, beyond just cybersecurity applications. 

<br /><br />Summary: <div>
arXiv:2511.03993v1 Announce Type: new 
Abstract: Network anomaly detection systems encounter several challenges with traditional detectors trained offline. They become susceptible to concept drift and new threats such as zero-day or polymorphic attacks. To address this limitation, we propose a Ca$^{2+}$-modulated learning framework that draws inspiration from astrocytic Ca$^{2+}$ signaling in the brain, where rapid, context-sensitive adaptation enables robust information processing. Our approach couples a multicellular astrocyte dynamics simulator with a deep neural network (DNN). The simulator models astrocytic Ca$^{2+}$ dynamics through three key mechanisms: IP$_3$-mediated Ca$^{2+}$ release, SERCA pump uptake, and conductance-aware diffusion through gap junctions between cells. Evaluation of our proposed network on CTU-13 (Neris) network traffic data demonstrates the effectiveness of our biologically plausible approach. The Ca$^{2+}$-gated model outperforms a matched baseline DNN, achieving up to $\sim$98\% accuracy with reduced false positives and negatives across multiple train/test splits. Importantly, this improved performance comes with negligible runtime overhead once Ca$^{2+}$ trajectories are precomputed. While demonstrated here for cybersecurity applications, this Ca$^{2+}$-modulated learning framework offers a generic solution for streaming detection tasks that require rapid, biologically grounded adaptation to evolving data patterns.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations</title>
<link>https://arxiv.org/abs/2511.04000</link>
<guid>https://arxiv.org/abs/2511.04000</guid>
<content:encoded><![CDATA[
<div> Keywords: decision trees, meta-learning, synthetic pre-training data, MetaTree transformer architecture, scalability

Summary:
Decision trees are commonly used in important fields like finance and healthcare due to their interpretability. This study introduces a novel method for efficiently generating synthetic pre-training data to facilitate meta-learning of decision trees. By creating large-scale datasets through sampling near-optimal decision trees synthetically, the proposed approach enables training the MetaTree transformer architecture to achieve performance comparable to pre-training on real-world data or using computationally expensive optimal decision trees. This method significantly reduces computational costs, offers enhanced data generation flexibility, and opens up avenues for scalable and efficient meta-learning of interpretable decision tree models. <div>
arXiv:2511.04000v1 Announce Type: new 
Abstract: Decision trees are widely used in high-stakes fields like finance and healthcare due to their interpretability. This work introduces an efficient, scalable method for generating synthetic pre-training data to enable meta-learning of decision trees. Our approach samples near-optimal decision trees synthetically, creating large-scale, realistic datasets. Using the MetaTree transformer architecture, we demonstrate that this method achieves performance comparable to pre-training on real-world data or with computationally expensive optimal decision trees. This strategy significantly reduces computational costs, enhances data generation flexibility, and paves the way for scalable and efficient meta-learning of interpretable decision tree models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating scientific discovery with the common task framework</title>
<link>https://arxiv.org/abs/2511.04001</link>
<guid>https://arxiv.org/abs/2511.04001</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine learning, artificial intelligence, dynamic systems, modeling paradigms, common task framework

Summary:
Machine learning and artificial intelligence algorithms are increasingly being used to characterize and control dynamic systems in various scientific fields. To evaluate these algorithms effectively, a common task framework (CTF) has been introduced, providing challenge data sets with diverse objectives such as forecasting, state reconstruction, generalization, and control. The CTF aims to compare the performance of different algorithms in scenarios with limited data and noisy measurements. This framework has been instrumental in driving the advancement of ML/AI in traditional applications like speech recognition and computer vision. Objective metrics within the CTF are essential for assessing the effectiveness of algorithms across science and engineering disciplines. <div>
arXiv:2511.04001v1 Announce Type: new 
Abstract: Machine learning (ML) and artificial intelligence (AI) algorithms are transforming and empowering the characterization and control of dynamic systems in the engineering, physical, and biological sciences. These emerging modeling paradigms require comparative metrics to evaluate a diverse set of scientific objectives, including forecasting, state reconstruction, generalization, and control, while also considering limited data scenarios and noisy measurements. We introduce a common task framework (CTF) for science and engineering, which features a growing collection of challenge data sets with a diverse set of practical and common objectives. The CTF is a critically enabling technology that has contributed to the rapid advance of ML/AI algorithms in traditional applications such as speech recognition, language processing, and computer vision. There is a critical need for the objective metrics of a CTF to compare the diverse algorithms being rapidly developed and deployed in practice today across science and engineering.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing</title>
<link>https://arxiv.org/abs/2511.04002</link>
<guid>https://arxiv.org/abs/2511.04002</guid>
<content:encoded><![CDATA[
<div> compression, split computing, autoregressive inference, quantization, edge devices

Summary:
- The article introduces a new framework for deploying large language models (LLMs) on resource-constrained IoT devices.
- The framework addresses the challenges of autoregressive inference and expanding key-value cache requirements.
- It introduces one-point split compression (OPSC) to partition models into front-end and back-end segments with different precision levels.
- A two-stage intermediate compression pipeline, combining threshold splitting and token-wise adaptive bit quantization, reduces communication overhead while preserving critical activations.
- The framework utilizes a unified optimization approach to select optimal split points, quantization settings, and sequence lengths to meet memory and latency constraints. Extensive evaluations demonstrate superior performance compared to existing quantization methods, achieving faster inference speed and reduced communication overhead without sacrificing model accuracy. 

<br /><br />Summary: <div>
arXiv:2511.04002v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved near-human performance across diverse reasoning tasks, yet their deployment on resource-constrained Internet-of-Things (IoT) devices remains impractical due to massive parameter footprints and memory-intensive autoregressive decoding. While split computing offers a promising solution by partitioning model execution between edge devices and cloud servers, existing approaches fail to address the unique challenges of autoregressive inference, particularly the iterative token generation process and expanding key-value (KV) cache requirements. This work introduces the first autoregressive-aware split computing framework designed explicitly for LLM deployment on edge devices. Our approach makes three key contributions. First, we develop one-point split compression (OPSC), a mixed-precision quantization scheme that prevents out-of-memory failures by strategically partitioning models into front-end and back-end segments with different precision levels. Second, we propose a two-stage intermediate compression pipeline that combines threshold splitting (TS) and token-wise adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations while dramatically reducing communication overhead. Third, we formulate a unified optimization framework that jointly selects optimal split points, quantization settings, and sequence lengths to satisfy strict memory and latency constraints. Extensive evaluations across diverse LLMs and hardware platforms demonstrate superior performance compared to state-of-the-art quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework achieves a 1.49 inference speedup and significant communication overhead reduction while maintaining or improving model accuracy.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Multimodal Protein Function Prediction Through Dual-Branch Dynamic Selection with Reconstructive Pre-Training</title>
<link>https://arxiv.org/abs/2511.04040</link>
<guid>https://arxiv.org/abs/2511.04040</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal protein features, protein function prediction, reconstructive pre-training, Bidirectional Interaction Module, Dynamic Selection Module

Summary: 
The article introduces a new method, DSRPGO, for protein function prediction that utilizes dynamic selection and reconstructive pre-training mechanisms. By incorporating reconstructive pre-training, the model can extract more detailed protein information at low semantic levels. The Bidirectional Interaction Module (BInM) enables interactive learning among multimodal features, while the Dynamic Selection Module (DSM) tackles the challenge of hierarchical multi-label classification by selecting the most relevant feature representation for each protein function prediction. The DSRPGO model showcases significant improvements in BPO, MFO, and CCO on human datasets compared to other benchmark models, establishing its superiority in protein function prediction. <div>
arXiv:2511.04040v1 Announce Type: new 
Abstract: Multimodal protein features play a crucial role in protein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to protein attributes and interaction networks, making it challenging to decipher their complex interconnections. In this work, we propose a multimodal protein function prediction method (DSRPGO) by utilizing dynamic selection and reconstructive pre-training mechanisms. To acquire complex protein information, we introduce reconstructive pre-training to mine more fine-grained information with low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM) to facilitate interactive learning among multimodal features. Additionally, to address the difficulty of hierarchical multi-label classification in this task, a Dynamic Selection Module (DSM) is designed to select the feature representation that is most conducive to current protein function prediction. Our proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets, thereby outperforming other benchmark models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization</title>
<link>https://arxiv.org/abs/2511.04063</link>
<guid>https://arxiv.org/abs/2511.04063</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantization, Rotational matrices, DartQuant, QR-Orth optimization, Large language models <br />
<br />
Summary: 
Quantization is essential for speeding up the inference of large-scale models, with rotational matrices being effective in improving performance by smoothing outliers. However, end-to-end fine-tuning of rotational optimization algorithms is computationally expensive and prone to overfitting. To overcome this, the efficient distribution-aware rotational calibration method, DartQuant, is proposed, which reduces optimization complexity by constraining activation distribution post-rotation and minimizing reliance on task-specific losses to prevent overfitting. The introduction of the QR-Orth optimization scheme replaces costly alternating optimization with a more efficient solution. In various model quantization experiments, DartQuant outperforms existing methods, achieving a 47$\times$ acceleration and 10$\times$ memory savings for rotational optimization on a 70B model. Notably, it successfully completes rotational calibration for a 70B model on a single 3090 GPU, making large language model quantization feasible in resource-limited environments. <div>
arXiv:2511.04063v1 Announce Type: new 
Abstract: Quantization plays a crucial role in accelerating the inference of large-scale models, and rotational matrices have been shown to effectively improve quantization performance by smoothing outliers. However, end-to-end fine-tuning of rotational optimization algorithms incurs high computational costs and is prone to overfitting. To address this challenge, we propose an efficient distribution-aware rotational calibration method, DartQuant, which reduces the complexity of rotational optimization by constraining the distribution of the activations after rotation. This approach also effectively reduces reliance on task-specific losses, thereby mitigating the risk of overfitting. Additionally, we introduce the QR-Orth optimization scheme, which replaces expensive alternating optimization with a more efficient solution. In a variety of model quantization experiments, DartQuant demonstrates superior performance. Compared to existing methods, it achieves 47$\times$ acceleration and 10$\times$ memory savings for rotational optimization on a 70B model. Furthermore, it is the first to successfully complete rotational calibration for a 70B model on a single 3090 GPU, making quantization of large language models feasible in resource-constrained environments. Code is available at https://github.com/CAS-CLab/DartQuant.git.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pediatric Appendicitis Detection from Ultrasound Images</title>
<link>https://arxiv.org/abs/2511.04069</link>
<guid>https://arxiv.org/abs/2511.04069</guid>
<content:encoded><![CDATA[
<div> Keywords: Pediatric appendicitis, deep learning, ultrasound images, ResNet, automated detection

Summary: 
The study aimed to develop a deep learning model using a pretrained ResNet architecture for automated detection of pediatric appendicitis from ultrasound images. The research utilized the Regensburg Pediatric Appendicitis Dataset, comprising ultrasound scans of pediatric patients with abdominal pain. The model, after preprocessing and fine-tuning, achieved an impressive overall accuracy of 93.44%, precision of 91.53%, and recall of 89.8%. By effectively learning spatial features, the model demonstrated strong performance in identifying appendicitis, even across varying ultrasound views with differing anatomical structures. This approach was successful in overcoming challenges such as low contrast, speckle noise, and anatomical variability in pediatric imaging. The use of deep learning and ResNet architecture proved to be a promising tool for accurate and automated detection of pediatric appendicitis from ultrasound images. 

<br /><br />Summary: <div>
arXiv:2511.04069v1 Announce Type: new 
Abstract: Pediatric appendicitis remains one of the most common causes of acute abdominal pain in children, and its diagnosis continues to challenge clinicians due to overlapping symptoms and variable imaging quality. This study aims to develop and evaluate a deep learning model based on a pretrained ResNet architecture for automated detection of appendicitis from ultrasound images. We used the Regensburg Pediatric Appendicitis Dataset, which includes ultrasound scans, laboratory data, and clinical scores from pediatric patients admitted with abdominal pain to Children Hospital. Hedwig in Regensburg, Germany. Each subject had 1 to 15 ultrasound views covering the right lower quadrant, appendix, lymph nodes, and related structures. For the image based classification task, ResNet was fine tuned to distinguish appendicitis from non-appendicitis cases. Images were preprocessed by normalization, resizing, and augmentation to enhance generalization. The proposed ResNet model achieved an overall accuracy of 93.44, precision of 91.53, and recall of 89.8, demonstrating strong performance in identifying appendicitis across heterogeneous ultrasound views. The model effectively learned discriminative spatial features, overcoming challenges posed by low contrast, speckle noise, and anatomical variability in pediatric imaging.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Left Atrial Segmentation with nnU-Net Using MRI</title>
<link>https://arxiv.org/abs/2511.04071</link>
<guid>https://arxiv.org/abs/2511.04071</guid>
<content:encoded><![CDATA[
<div> Keywords: Left Atrial Segmentation, MRI, nnU-Net, Deep Learning, Dice Similarity Coefficient

Summary:<br />
Accurate segmentation of the left atrium from cardiac MRI scans is crucial for medical procedures such as atrial fibrillation ablation. Manual segmentation is time-consuming and subjective. This study utilized the nnU-Net deep learning framework to automatically segment the left atrium in MRI scans from the Left Atrial Segmentation Challenge 2013 dataset. The nnU-Net model adjusted its configuration based on the MRI data characteristics, achieving a high Dice similarity coefficient of 93.5, indicating strong agreement with expert annotations. The model surpassed traditional segmentation methods, showing robust performance across variations in left atrial shape, contrast, and image quality. It accurately delineated the atrial body and proximal pulmonary veins. Innovative deep learning approaches like nnU-Net offer a promising solution for improving efficiency and accuracy in cardiac MRI image segmentation tasks. 

Summary: <br /> <div>
arXiv:2511.04071v1 Announce Type: new 
Abstract: Accurate segmentation of the left atrium (LA) from cardiac MRI is critical for guiding atrial fibrillation (AF) ablation and constructing biophysical cardiac models. Manual delineation is time-consuming, observer-dependent, and impractical for large-scale or time-sensitive clinical workflows. Deep learning methods, particularly convolutional architectures, have recently demonstrated superior performance in medical image segmentation tasks. In this study, we applied the nnU-Net framework, an automated, self-configuring deep learning segmentation architecture, to the Left Atrial Segmentation Challenge 2013 dataset. The dataset consists of thirty MRI scans with corresponding expert-annotated masks. The nnU-Net model automatically adapted its preprocessing, network configuration, and training pipeline to the characteristics of the MRI data. Model performance was quantitatively evaluated using the Dice similarity coefficient (DSC), and qualitative results were compared against expert segmentations. The proposed nnUNet model achieved a mean Dice score of 93.5, demonstrating high overlap with expert annotations and outperforming several traditional segmentation approaches reported in previous studies. The network exhibited robust generalization across variations in left atrial shape, contrast, and image quality, accurately delineating both the atrial body and proximal pulmonary veins.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters</title>
<link>https://arxiv.org/abs/2511.04073</link>
<guid>https://arxiv.org/abs/2511.04073</guid>
<content:encoded><![CDATA[
<div> learned weights, filtered ANN search, graph structures, optimization problem, distance function 
Summary:
This study introduces a novel approach to Filtered Approximate Nearest Neighbor (ANN) search that learns the optimal trade-off between vector distance and filter match from data. By formulating this as a constrained linear optimization problem, the weights derived reflect the underlying filter distribution and enhance the effectiveness of filtered ANN search. The learned weights guide both the search process and index construction, resulting in graph structures that better capture filter distribution and semantics. Experimental results show that adapting the distance function to the data leads to a 5-10% improvement in accuracy compared to methods using fixed penalties. This approach offers a more flexible and generalizable framework for addressing the filtered ANN search problem. 
<br /><br />Summary: <div>
arXiv:2511.04073v1 Announce Type: new 
Abstract: Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest vectors for a query vector from a dataset. It enforces that a specified set of discrete labels $S$ for the query must be included in the labels of each retrieved vector. Existing graph-based methods typically incorporate filter awareness by assigning fixed penalties or prioritizing nodes based on filter satisfaction. However, since these methods use fixed, data in- dependent penalties, they often fail to generalize across datasets with diverse label and vector distributions. In this work, we propose a principled alternative that learns the optimal trade-off between vector distance and filter match directly from the data, rather than relying on fixed penalties. We formulate this as a constrained linear optimization problem, deriving weights that better reflect the underlying filter distribution and more effectively address the filtered ANN search problem. These learned weights guide both the search process and index construction, leading to graph structures that more effectively capture the underlying filter distribution and filter semantics. Our experiments demonstrate that adapting the distance function to the data significantly im- proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible and generalizable framework for the filtered ANN search problem.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeNoise: Learning Robust Graph Representations for Unsupervised Graph-Level Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.04086</link>
<guid>https://arxiv.org/abs/2511.04086</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised graph-level anomaly detection, Graph Neural Network, DeNoise, contaminated training data, robust framework

Summary:
DeNoise is a robust framework for unsupervised graph-level anomaly detection designed to handle contaminated training data. It uses an adversarial objective to optimize a graph-level encoder, attribute decoder, and structure decoder, learning noise-resistant embeddings. The encoder anchor-alignment denoising mechanism enhances representation quality by incorporating high-information node embeddings from normal graphs into all graph embeddings, reducing anomaly interference. A contrastive learning component further consolidates normal graph embeddings and pushes anomalous ones apart in the latent space. Experimental results on multiple real-world datasets show that DeNoise consistently produces reliable graph-level representations, outperforming existing state-of-the-art approaches in detecting anomalies even under varying levels of noise.<br /><br />Summary: <div>
arXiv:2511.04086v1 Announce Type: new 
Abstract: With the rapid growth of graph-structured data in critical domains, unsupervised graph-level anomaly detection (UGAD) has become a pivotal task. UGAD seeks to identify entire graphs that deviate from normal behavioral patterns. However, most Graph Neural Network (GNN) approaches implicitly assume that the training set is clean, containing only normal graphs, which is rarely true in practice. Even modest contamination by anomalous graphs can distort learned representations and sharply degrade performance. To address this challenge, we propose DeNoise, a robust UGAD framework explicitly designed for contaminated training data. It jointly optimizes a graph-level encoder, an attribute decoder, and a structure decoder via an adversarial objective to learn noise-resistant embeddings. Further, DeNoise introduces an encoder anchor-alignment denoising mechanism that fuses high-information node embeddings from normal graphs into all graph embeddings, improving representation quality while suppressing anomaly interference. A contrastive learning component then compacts normal graph embeddings and repels anomalous ones in the latent space. Extensive experiments on eight real-world datasets demonstrate that DeNoise consistently learns reliable graph-level representations under varying noise intensities and significantly outperforms state-of-the-art UGAD baselines.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KoTaP: A Panel Dataset for Corporate Tax Avoidance, Performance, and Governance in Korea</title>
<link>https://arxiv.org/abs/2511.04094</link>
<guid>https://arxiv.org/abs/2511.04094</guid>
<content:encoded><![CDATA[
<div> Keywords: Korean Tax Avoidance Panel, firm-level dataset, tax avoidance, corporate governance, financial analysis

Summary: 
The study introduces the Korean Tax Avoidance Panel (KoTaP), a comprehensive panel dataset of non-financial firms listed on KOSPI and KOSDAQ between 2011 and 2024. It includes 12,653 firm-year observations from 1,754 firms after applying various exclusion criteria. KoTaP allows for the analysis of tax avoidance as a predictor variable linked to earnings management, profitability, stability, growth, and governance. It incorporates measures like cash effective tax rate, GAAP effective tax rate, and book-tax difference measures. The dataset's balanced panel structure and standardized variables align with international literature while also reflecting unique features of Korean firms, such as concentrated ownership and high foreign shareholding. KoTaP can be utilized for benchmarking econometric and deep learning models, policy evaluation, audit planning, and investment analysis, serving as a valuable open resource for interdisciplinary research in accounting and finance. 

<br /><br />Summary: <div>
arXiv:2511.04094v1 Announce Type: new 
Abstract: This study introduces the Korean Tax Avoidance Panel (KoTaP), a long-term panel dataset of non-financial firms listed on KOSPI and KOSDAQ between 2011 and 2024. After excluding financial firms, firms with non-December fiscal year ends, capital impairment, and negative pre-tax income, the final dataset consists of 12,653 firm-year observations from 1,754 firms. KoTaP is designed to treat corporate tax avoidance as a predictor variable and link it to multiple domains, including earnings management (accrual- and activity-based), profitability (ROA, ROE, CFO, LOSS), stability (LEV, CUR, SIZE, PPE, AGE, INVREC), growth (GRW, MB, TQ), and governance (BIG4, FORN, OWN). Tax avoidance itself is measured using complementary indicators cash effective tax rate (CETR), GAAP effective tax rate (GETR), and book-tax difference measures (TSTA, TSDA) with adjustments to ensure interpretability. A key strength of KoTaP is its balanced panel structure with standardized variables and its consistency with international literature on the distribution and correlation of core indicators. At the same time, it reflects distinctive institutional features of Korean firms, such as concentrated ownership, high foreign shareholding, and elevated liquidity ratios, providing both international comparability and contextual uniqueness. KoTaP enables applications in benchmarking econometric and deep learning models, external validity checks, and explainable AI analyses. It further supports policy evaluation, audit planning, and investment analysis, making it a critical open resource for accounting, finance, and interdisciplinary research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decomposable Neuro Symbolic Regression</title>
<link>https://arxiv.org/abs/2511.04124</link>
<guid>https://arxiv.org/abs/2511.04124</guid>
<content:encoded><![CDATA[
<div> Keywords: symbolic regression, transformer models, genetic algorithms, genetic programming, explainable AI

Summary: 
The article introduces a novel decomposable symbolic regression (SR) method that aims to generate interpretable multivariate expressions using a combination of transformer models, genetic algorithms (GAs), and genetic programming (GP). The proposed method focuses on creating mathematical expressions that provide explanations for the underlying relationships in observed data. By distilling a trained opaque regression model into mathematical expressions, the method can better capture the governing equations while maintaining interpretability. The approach utilizes a Multi-Set Transformer to generate univariate symbolic skeletons, which are evaluated and merged using GA and GP-based procedures. The final multivariate skeletons undergo coefficient optimization using GA. Evaluation on various problems demonstrates the method's ability to produce accurate expressions that match the original mathematical structure while achieving lower or comparable errors compared to existing GP-based and neural SR methods. This approach represents a significant advancement in developing explainable AI models for complex systems. 

<br /><br />Summary: <div>
arXiv:2511.04124v1 Announce Type: new 
Abstract: Symbolic regression (SR) models complex systems by discovering mathematical expressions that capture underlying relationships in observed data. However, most SR methods prioritize minimizing prediction error over identifying the governing equations, often producing overly complex or inaccurate expressions. To address this, we present a decomposable SR method that generates interpretable multivariate expressions leveraging transformer models, genetic algorithms (GAs), and genetic programming (GP). In particular, our explainable SR method distills a trained ``opaque'' regression model into mathematical expressions that serve as explanations of its computed function. Our method employs a Multi-Set Transformer to generate multiple univariate symbolic skeletons that characterize how each variable influences the opaque model's response. We then evaluate the generated skeletons' performance using a GA-based approach to select a subset of high-quality candidates before incrementally merging them via a GP-based cascade procedure that preserves their original skeleton structure. The final multivariate skeletons undergo coefficient optimization via a GA. We evaluated our method on problems with controlled and varying degrees of noise, demonstrating lower or comparable interpolation and extrapolation errors compared to two GP-based methods, three neural SR methods, and a hybrid approach. Unlike them, our approach consistently learned expressions that matched the original mathematical structure.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Feasibility of End-to-End Large Language Model as a Compiler</title>
<link>https://arxiv.org/abs/2511.04132</link>
<guid>https://arxiv.org/abs/2511.04132</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, compiler, source code, assembly code, optimization

Summary:
Large Language Models (LLMs) have shown promise in various domains, including compiler development. This paper introduces the concept of LLM as a Compiler (LaaC) and evaluates its potential through the CompilerEval dataset. The study reveals that while LLMs demonstrate basic compiler capabilities, their compilation success rates are currently low. By optimizing prompts, scaling up models, and incorporating reasoning methods, the quality of LLM-generated assembly code can be improved significantly. The findings suggest that with targeted training and infrastructure, LaaC has the potential to revolutionize compilation by generating high-quality assembly code. Future research directions focus on enhancing LLM performance through knowledge-rich prompts and specialized architecture.<br /><br />Summary: <div>
arXiv:2511.04132v1 Announce Type: new 
Abstract: In recent years, end-to-end Large Language Model (LLM) technology has shown substantial advantages across various domains. As critical system software and infrastructure, compilers are responsible for transforming source code into target code. While LLMs have been leveraged to assist in compiler development and maintenance, their potential as an end-to-end compiler remains largely unexplored. This paper explores the feasibility of LLM as a Compiler (LaaC) and its future directions. We designed the CompilerEval dataset and framework specifically to evaluate the capabilities of mainstream LLMs in source code comprehension and assembly code generation. In the evaluation, we analyzed various errors, explored multiple methods to improve LLM-generated code, and evaluated cross-platform compilation capabilities. Experimental results demonstrate that LLMs exhibit basic capabilities as compilers but currently achieve low compilation success rates. By optimizing prompts, scaling up the model, and incorporating reasoning methods, the quality of assembly code generated by LLMs can be significantly enhanced. Based on these findings, we maintain an optimistic outlook for LaaC and propose practical architectural designs and future research directions. We believe that with targeted training, knowledge-rich prompts, and specialized infrastructure, LaaC has the potential to generate high-quality assembly code and drive a paradigm shift in the field of compilation.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.04147</link>
<guid>https://arxiv.org/abs/2511.04147</guid>
<content:encoded><![CDATA[
<div> Safe reinforcement learning, semi-infinite constraints, policy optimization, deterministic safety, exchange rule <br />
Summary: <br />
The paper introduces Exchange Policy Optimization (EPO) for semi-infinite safe reinforcement learning problems, addressing safety requirements across continuous parameter spaces. EPO iteratively solves safe RL subproblems with finite constraints, adjusting the active set by adding constraints with violations and removing those with zero Lagrange multipliers. This prevents uncontrolled set growth and aids policy training. Theoretical analysis shows EPO-trained strategies achieve optimal performance with global constraint violations limited to a specified bound. EPO balances policy refinement and constraint management, promoting effective training and ensuring deterministic bounded safety. <div>
arXiv:2511.04147v1 Announce Type: new 
Abstract: Safe reinforcement learning (safe RL) aims to respect safety requirements while optimizing long-term performance. In many practical applications, however, the problem involves an infinite number of constraints, known as semi-infinite safe RL (SI-safe RL). Such constraints typically appear when safety conditions must be enforced across an entire continuous parameter space, such as ensuring adequate resource distribution at every spatial location. In this paper, we propose exchange policy optimization (EPO), an algorithmic framework that achieves optimal policy performance and deterministic bounded safety. EPO works by iteratively solving safe RL subproblems with finite constraint sets and adaptively adjusting the active set through constraint expansion and deletion. At each iteration, constraints with violations exceeding the predefined tolerance are added to refine the policy, while those with zero Lagrange multipliers are removed after the policy update. This exchange rule prevents uncontrolled growth of the working set and supports effective policy training. Our theoretical analysis demonstrates that, under mild assumptions, strategies trained via EPO achieve performance comparable to optimal solutions with global constraint violations strictly remaining within a prescribed bound.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Land Anywhere: Transferable Generative Models for Aircraft Trajectories</title>
<link>https://arxiv.org/abs/2511.04155</link>
<guid>https://arxiv.org/abs/2511.04155</guid>
<content:encoded><![CDATA[
<div> Keyword: trajectory data, Air Traffic Management, transfer learning, generative models, data scarcity

Summary: 
- The study focuses on the use of generative models for trajectory data in Air Traffic Management (ATM) when faced with data scarcity at secondary and regional airports.
- Transfer learning is explored as a method to adapt models trained on data-rich airports to data-scarce airports, specifically between Zurich (source) and Dublin (target) landing trajectory datasets.
- Diffusion-based models show competitive performance with as little as 5% of the Dublin data, outperforming models trained from scratch across metrics, and reaching baseline-level performance around 20%.
- Latent flow matching and diffusion models also benefit from pretraining, while flow matching models exhibit weaker generalization.
- Despite challenges in capturing rare trajectory patterns, transfer learning shows promise in reducing data requirements for trajectory generation in ATM, enabling realistic synthetic data generation even in environments with limited historical records. 

<br /><br />Summary: <div>
arXiv:2511.04155v1 Announce Type: new 
Abstract: Access to trajectory data is a key requirement for developing and validating Air Traffic Management (ATM) solutions, yet many secondary and regional airports face severe data scarcity. This limits the applicability of machine learning methods and the ability to perform large-scale simulations or "what-if" analyses. In this paper, we investigate whether generative models trained on data-rich airports can be efficiently adapted to data-scarce airports using transfer learning. We adapt state-of-the-art diffusion- and flow-matching-based architectures to the aviation domain and evaluate their transferability between Zurich (source) and Dublin (target) landing trajectory datasets. Models are pretrained on Zurich and fine-tuned on Dublin with varying amounts of local data, ranging from 0% to 100%. Results show that diffusion-based models achieve competitive performance with as little as 5% of the Dublin data and reach baseline-level performance around 20%, consistently outperforming models trained from scratch across metrics and visual inspections. Latent flow matching and latent diffusion models also benefit from pretraining, though with more variable gains, while flow matching models show weaker generalization. Despite challenges in capturing rare trajectory patterns, these findings demonstrate the potential of transfer learning to substantially reduce data requirements for trajectory generation in ATM, enabling realistic synthetic data generation even in environments with limited historical records.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning Approach for Clinical Risk Identification Using Transformer Modeling of Heterogeneous EHR Data</title>
<link>https://arxiv.org/abs/2511.04158</link>
<guid>https://arxiv.org/abs/2511.04158</guid>
<content:encoded><![CDATA[
<div> Transformer-based, Longitudinal modeling, Clinical risk classification, Electronic Health Record, Multi-source medical features
<br />
Summary:
This study presents a novel Transformer-based longitudinal modeling approach for clinical risk classification using heterogeneous Electronic Health Record (EHR) data. The method tackles challenges such as irregular temporal patterns, large modality differences, and complex semantic structures. It incorporates multi-source medical features and utilizes feature embedding to create a unified representation of structured and unstructured data. A learnable temporal encoding mechanism captures dynamic evolution under uneven sampling intervals. The model employs a multi-head self-attention structure for global dependency modeling on longitudinal sequences, facilitating aggregation of long-term trends and short-term fluctuations across different temporal scales. Additionally, a semantic-weighted pooling module enhances semantic representation by assigning adaptive importance to key medical events. Experimental results demonstrate superior performance over traditional machine learning and temporal deep learning models, improving accuracy, recall, precision, and F1-Score. The proposed method offers a robust framework for precise risk identification in multi-source heterogeneous EHR environments, aiding clinical intelligent decision-making.
<br /><br /> <div>
arXiv:2511.04158v1 Announce Type: new 
Abstract: This study proposes a Transformer-based longitudinal modeling method to address challenges in clinical risk classification with heterogeneous Electronic Health Record (EHR) data, including irregular temporal patterns, large modality differences, and complex semantic structures. The method takes multi-source medical features as input and employs a feature embedding layer to achieve a unified representation of structured and unstructured data. A learnable temporal encoding mechanism is introduced to capture dynamic evolution under uneven sampling intervals. The core model adopts a multi-head self-attention structure to perform global dependency modeling on longitudinal sequences, enabling the aggregation of long-term trends and short-term fluctuations across different temporal scales. To enhance semantic representation, a semantic-weighted pooling module is designed to assign adaptive importance to key medical events, improving the discriminative ability of risk-related features. Finally, a linear mapping layer generates individual-level risk scores. Experimental results show that the proposed model outperforms traditional machine learning and temporal deep learning models in accuracy, recall, precision, and F1-Score, achieving stable and precise risk identification in multi-source heterogeneous EHR environments and providing an efficient and reliable framework for clinical intelligent decision-making.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Joint Regularization and Calibration in Deep Ensembles</title>
<link>https://arxiv.org/abs/2511.04160</link>
<guid>https://arxiv.org/abs/2511.04160</guid>
<content:encoded><![CDATA[
<div> Deep ensembles, machine learning, weight decay, temperature scaling, early stopping <br />
Summary:<br /> 
This paper explores the impact of jointly tuning weight decay, temperature scaling, and early stopping on deep ensemble models in machine learning. The study shows that jointly tuning these factors can enhance both predictive performance and uncertainty quantification. A partially overlapping holdout strategy is proposed to balance joint evaluation and data utilization efficiently. The results suggest that jointly optimizing the ensemble can match or improve performance, with varying effects based on tasks and metrics. The research highlights the trade-offs between individual and joint optimization in deep ensemble training, with the overlapping holdout strategy offering a practical solution. Overall, this study provides insights and guidance for practitioners seeking to optimize deep ensemble models effectively. The code for implementing the techniques is available on GitHub for reference. <br /> <div>
arXiv:2511.04160v1 Announce Type: new 
Abstract: Deep ensembles are a powerful tool in machine learning, improving both model performance and uncertainty calibration. While ensembles are typically formed by training and tuning models individually, evidence suggests that jointly tuning the ensemble can lead to better performance. This paper investigates the impact of jointly tuning weight decay, temperature scaling, and early stopping on both predictive performance and uncertainty quantification. Additionally, we propose a partially overlapping holdout strategy as a practical compromise between enabling joint evaluation and maximizing the use of data for training. Our results demonstrate that jointly tuning the ensemble generally matches or improves performance, with significant variation in effect size across different tasks and metrics. We highlight the trade-offs between individual and joint optimization in deep ensemble training, with the overlapping holdout strategy offering an attractive practical solution. We believe our findings provide valuable insights and guidance for practitioners looking to optimize deep ensemble models. Code is available at: https://github.com/lauritsf/ensemble-optimality-gap
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScaleDL: Towards Scalable and Efficient Runtime Prediction for Distributed Deep Learning Workloads</title>
<link>https://arxiv.org/abs/2511.04162</link>
<guid>https://arxiv.org/abs/2511.04162</guid>
<content:encoded><![CDATA[
<div> ScaleDL, DNN, runtime prediction, graph neural network, data collection cost
Summary:
ScaleDL is a novel framework for predicting the runtime of deep neural networks (DNNs) accurately. By combining nonlinear layer-wise modeling with a graph neural network-based cross-layer interaction mechanism, ScaleDL achieves hierarchical generalizability across different network architectures. Additionally, the use of the D-optimal method helps reduce data collection costs. Experimental results show that ScaleDL significantly improves runtime prediction accuracy, achieving 6 times lower Mean Relative Error (MRE) and 5 times lower Root Mean Squared Error (RMSE) compared to baseline models. <div>
arXiv:2511.04162v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) form the cornerstone of modern AI services, supporting a wide range of applications, including autonomous driving, chatbots, and recommendation systems. As models increase in size and complexity, DNN workloads like training and inference tasks impose unprecedented demands on distributed computing resources, making the accurate prediction of runtime essential for optimizing development and resource allocation. Traditional methods rely on additive computational unit models, limiting their accuracy and generalizability. In contrast, graph-enhanced modeling improves performance but significantly increases data collection costs. Therefore, there is a critical need for a method that strikes a balance between accuracy, generalizability, and the costs of data collection. To address these challenges, we propose ScaleDL, a novel runtime prediction framework that combines nonlinear layer-wise modeling with graph neural network (GNN)-based cross-layer interaction mechanism, enabling accurate DNN runtime prediction and hierarchical generalizability across different network architectures. Additionally, we employ the D-optimal method to reduce data collection costs. Experiments on the workloads of five popular DNN models prove that ScaleDL enhances runtime prediction accuracy and generalizability, achieving 6$\times$ lower MRE and 5$\times$ lower RMSE compared to baseline models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Block Rotation is All You Need for MXFP4 Quantization</title>
<link>https://arxiv.org/abs/2511.04214</link>
<guid>https://arxiv.org/abs/2511.04214</guid>
<content:encoded><![CDATA[
<div> deep learning, language models, post-training quantization, MXFP4 format, benchmark

Summary: 
This paper explores the challenge of achieving accurate W4A4 quantization for large language models (LLMs) in the context of the MXFP4 format. Existing PTQ methods designed for INT4 formats may not be directly applicable to MXFP4, a new FP4 format with growing hardware support. Through a comprehensive benchmark, the study highlights the strong performance of methods like GPTQ while identifying limitations in rotation-based approaches due to their incompatibility with MXFP4. The root cause of this issue is attributed to a mismatch between MXFP4's block scaling and the global rotation used by existing methods. The paper proposes a block rotation strategy tailored to MXFP4, leading to significant accuracy improvements across various LLMs. These findings not only provide practical guidance for practitioners but also pave the way for further research in the field of PTQ under emerging low-precision formats. 

<br /><br />Summary: <div>
arXiv:2511.04214v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success, but their rapidly growing scale imposes prohibitive costs in memory, computation, and energy. Post-training quantization (PTQ) is a promising solution for efficient deployment, yet achieving accurate W4A4 quantization remains an open challenge. While most existing methods are designed for INT4 formats, the emergence of MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)-- raises questions about the applicability of current techniques. In this work, we establish a comprehensive benchmark of PTQ methods under the MXFP4 format. Through systematic evaluation, we find that methods like GPTQ consistently deliver strong performance, whereas rotation-based approaches, which are almost used by all state-of-the-art approaches, suffer from severe incompatibility with MXFP4. We further provide the first in-depth analysis of this conflict, tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two) block scaling and the redistribution of outlier energy via global rotation. Building on this insight, we propose a simple yet effective block rotation strategy that adapts rotation-based methods to MXFP4, leading to substantial accuracy improvements across diverse LLMs. Our findings not only offer clear guidance for practitioners but also set a foundation for advancing PTQ research under emerging low-precision formats.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms</title>
<link>https://arxiv.org/abs/2511.04217</link>
<guid>https://arxiv.org/abs/2511.04217</guid>
<content:encoded><![CDATA[
<div> lottery ticket hypothesis, transformer architectures, multi-head attention, theoretical analysis, empirical validation 
Summary: 
The article introduces the strong lottery ticket hypothesis (SLTH) in the context of transformer architectures. It aims to understand the existence of strong lottery tickets (SLTs) within multi-head attention (MHA) mechanisms, a key component of transformers. The theoretical analysis proves that randomly initialized MHAs with appropriate hidden dimensions contain SLTs that can approximate arbitrary MHAs with high probability. This analysis extends the SLTH to transformers without normalization layers. Empirical validation confirms that the approximation error between SLTs within source models and approximate target counterparts decreases exponentially with increasing hidden dimensions. The study enhances theoretical understanding of the SLTH in transformer architectures, shedding light on the presence of high-performing subnetworks in randomly initialized neural networks. 
<br /><br />Summary: <div>
arXiv:2511.04217v1 Announce Type: new 
Abstract: The strong lottery ticket hypothesis (SLTH) conjectures that high-performing subnetworks, called strong lottery tickets (SLTs), are hidden in randomly initialized neural networks. Although recent theoretical studies have established the SLTH across various neural architectures, the SLTH for transformer architectures still lacks theoretical understanding. In particular, the current theory of the SLTH does not yet account for the multi-head attention (MHA) mechanism, a core component of transformers. To address this gap, we introduce a theoretical analysis of the existence of SLTs within MHAs. We prove that, if a randomly initialized MHA of $H$ heads and input dimension $d$ has the hidden dimension $O(d\log(Hd^{3/2}))$ for the key and value, it contains an SLT that approximates an arbitrary MHA with the same input dimension with high probability. Furthermore, by leveraging this theory for MHAs, we extend the SLTH to transformers without normalization layers. We empirically validate our theoretical findings, demonstrating that the approximation error between the SLT within a source model (MHA and transformer) and an approximate target counterpart decreases exponentially by increasing the hidden dimension of the source model.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>seqme: a Python library for evaluating biological sequence design</title>
<link>https://arxiv.org/abs/2511.04239</link>
<guid>https://arxiv.org/abs/2511.04239</guid>
<content:encoded><![CDATA[
<div> Keywords: computational methods, biological sequence design, Python library, metrics, evaluation<br />
Summary:<br /> 
A new open-source Python library called seqme has been introduced to evaluate computational methods for designing biological sequences. The library offers a wide range of metrics, including sequence-based, embedding-based, and property-based, to assess the fidelity of designed sequences to target distributions and their desired properties. It is applicable to various biological sequences such as small molecules, DNA, ncRNA, mRNA, peptides, and proteins. seqme includes multiple embedding and property models for sequences, along with diagnostic and visualization functions for result inspection. It can be utilized to evaluate both one-shot and iterative design methods. The library fills a gap in the field by providing a single software library containing comprehensive metrics for assessing the performance of biological sequence design methods. <div>
arXiv:2511.04239v1 Announce Type: new 
Abstract: Recent advances in computational methods for designing biological sequences have sparked the development of metrics to evaluate these methods performance in terms of the fidelity of the designed sequences to a target distribution and their attainment of desired properties. However, a single software library implementing these metrics was lacking. In this work we introduce seqme, a modular and highly extendable open-source Python library, containing model-agnostic metrics for evaluating computational methods for biological sequence design. seqme considers three groups of metrics: sequence-based, embedding-based, and property-based, and is applicable to a wide range of biological sequences: small molecules, DNA, ncRNA, mRNA, peptides and proteins. The library offers a number of embedding and property models for biological sequences, as well as diagnostics and visualization functions to inspect the results. seqme can be used to evaluate both one-shot and iterative computational design methods.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided by Stars: Interpretable Concept Learning Over Time Series via Temporal Logic Semantics</title>
<link>https://arxiv.org/abs/2511.04244</link>
<guid>https://arxiv.org/abs/2511.04244</guid>
<content:encoded><![CDATA[
<div> classification, time series, neuro-symbolic, explanation, interpretability

Summary: 

The article introduces a new approach called STELLE for time series classification that aims to provide both accurate predictions and understandable explanations. STELLE is a neuro-symbolic framework that embeds trajectories into a space of temporal logic concepts, allowing for the direct mapping of raw time series to predefined formulae. This enables the model to provide local explanations in the form of human-readable STL conditions that justify individual predictions, as well as global explanations in the form of class-characterizing formulae. By optimizing for both accuracy and interpretability, STELLE achieves competitive performance on various real-world datasets while offering logically faithful explanations for its predictions. The experiments showcased the effectiveness of STELLE in providing accurate and understandable classifications for time series data. 

<br /><br />Summary: <div>
arXiv:2511.04244v1 Announce Type: new 
Abstract: Time series classification is a task of paramount importance, as this kind of data often arises in safety-critical applications. However, it is typically tackled with black-box deep learning methods, making it hard for humans to understand the rationale behind their output. To take on this challenge, we propose a novel approach, STELLE (Signal Temporal logic Embedding for Logically-grounded Learning and Explanation), a neuro-symbolic framework that unifies classification and explanation through direct embedding of trajectories into a space of temporal logic concepts. By introducing a novel STL-inspired kernel that maps raw time series to their alignment with predefined STL formulae, our model jointly optimises accuracy and interpretability, as each prediction is accompanied by the most relevant logical concepts that characterise it. This yields (i) local explanations as human-readable STL conditions justifying individual predictions, and (ii) global explanations as class-characterising formulae. Experiments demonstrate that STELLE achieves competitive accuracy while providing logically faithful explanations, validated on diverse real-world benchmarks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference</title>
<link>https://arxiv.org/abs/2511.04286</link>
<guid>https://arxiv.org/abs/2511.04286</guid>
<content:encoded><![CDATA[
<div> scalability, preference data, machine learning models, sample efficiency, active querying

Summary:
The article introduces a new hybrid framework that combines Reinforcement Learning from Human Feedback (RLHF) with Probabilistic Bayesian Optimization (PBO) to efficiently learn from human preferences. By integrating an acquisition-driven module into the RLHF pipeline, the framework actively gathers preference data, improving sample efficiency and overall performance. The proposed approach is validated in two domains: high-dimensional preference optimization and Language Model (LLM) fine-tuning. Experimental results show consistent enhancements in sample efficiency and performance for both tasks. <div>
arXiv:2511.04286v1 Announce Type: new 
Abstract: Learning from human preferences is a cornerstone of aligning machine learning models with subjective human judgments. Yet, collecting such preference data is often costly and time-consuming, motivating the need for more efficient learning paradigms. Two established approaches offer complementary advantages: RLHF scales effectively to high-dimensional tasks such as LLM fine-tuning, while PBO achieves greater sample efficiency through active querying. We propose a hybrid framework that unifies RLHF's scalability with PBO's query efficiency by integrating an acquisition-driven module into the RLHF pipeline, thereby enabling active and sample-efficient preference gathering. We validate the proposed approach on two representative domains: (i) high-dimensional preference optimization and (ii) LLM fine-tuning. Experimental results demonstrate consistent improvements in both sample efficiency and overall performance across these tasks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentially Private In-Context Learning with Nearest Neighbor Search</title>
<link>https://arxiv.org/abs/2511.04332</link>
<guid>https://arxiv.org/abs/2511.04332</guid>
<content:encoded><![CDATA[
arXiv:2511.04332v1 Announce Type: new 
Abstract: Differentially private in-context learning (DP-ICL) has recently become an active research topic due to the inherent privacy risks of in-context learning. However, existing approaches overlook a critical component of modern large language model (LLM) pipelines: the similarity search used to retrieve relevant context data. In this work, we introduce a DP framework for in-context learning that integrates nearest neighbor search of relevant examples in a privacy-aware manner. Our method outperforms existing baselines by a substantial margin across all evaluated benchmarks, achieving more favorable privacy-utility trade-offs. To achieve this, we employ nearest neighbor retrieval from a database of context data, combined with a privacy filter that tracks the cumulative privacy cost of selected samples to ensure adherence to a central differential privacy budget. Experimental results on text classification and document question answering show a clear advantage of the proposed method over existing baselines.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in Intensive Care</title>
<link>https://arxiv.org/abs/2511.04333</link>
<guid>https://arxiv.org/abs/2511.04333</guid>
<content:encoded><![CDATA[
arXiv:2511.04333v1 Announce Type: new 
Abstract: Dynamic Bayesian networks (DBNs) are increasingly used in healthcare due to their ability to model complex temporal relationships in patient data while maintaining interpretability, an essential feature for clinical decision-making. However, existing approaches to handling missing data in longitudinal clinical datasets are largely derived from static Bayesian networks literature, failing to properly account for the temporal nature of the data. This gap limits the ability to quantify uncertainty over time, which is particularly critical in settings such as intensive care, where understanding the temporal dynamics is fundamental for model trustworthiness and applicability across diverse patient groups. Despite the potential of DBNs, a full Bayesian framework that integrates missing data handling remains underdeveloped. In this work, we propose a novel Gibbs sampling-based method for learning DBNs from incomplete data. Our method treats each missing value as an unknown parameter following a Gaussian distribution. At each iteration, the unobserved values are sampled from their full conditional distributions, allowing for principled imputation and uncertainty estimation. We evaluate our method on both simulated datasets and real-world intensive care data from critically ill patients. Compared to standard model-agnostic techniques such as MICE, our Bayesian approach demonstrates superior reconstruction accuracy and convergence properties. These results highlight the clinical relevance of incorporating full Bayesian inference in temporal models, providing more reliable imputations and offering deeper insight into model behavior. Our approach supports safer and more informed clinical decision-making, particularly in settings where missing data are frequent and potentially impactful.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness</title>
<link>https://arxiv.org/abs/2511.04401</link>
<guid>https://arxiv.org/abs/2511.04401</guid>
<content:encoded><![CDATA[
arXiv:2511.04401v1 Announce Type: new 
Abstract: Deep learning models achieve strong performance across various domains but often rely on spurious correlations, making them vulnerable to distribution shifts. This issue is particularly severe in subpopulation shift scenarios, where models struggle in underrepresented groups. While existing methods have made progress in mitigating this issue, their performance gains are still constrained. They lack a rigorous theoretical framework connecting the embedding space representations with worst-group error. To address this limitation, we propose Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness (SCER), a novel approach that directly regularizes feature representations to suppress spurious cues. We show theoretically that worst-group error is influenced by how strongly the classifier relies on spurious versus core directions, identified from differences in group-wise mean embeddings across domains and classes. By imposing theoretical constraints at the embedding level, SCER encourages models to focus on core features while reducing sensitivity to spurious patterns. Through systematic evaluation on multiple vision and language, we show that SCER outperforms prior state-of-the-art studies in worst-group accuracy. Our code is available at \href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity</title>
<link>https://arxiv.org/abs/2511.04418</link>
<guid>https://arxiv.org/abs/2511.04418</guid>
<content:encoded><![CDATA[
arXiv:2511.04418v1 Announce Type: new 
Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is critical for trustworthy deployment. While real-world language is inherently ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically benchmarked against tasks with no ambiguity. In this work, we demonstrate that while current uncertainty estimators perform well under the restrictive assumption of no ambiguity, they degrade to close-to-random performance on ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first ambiguous question-answering (QA) datasets equipped with ground-truth answer distributions estimated from factual co-occurrence. We find this performance deterioration to be consistent across different estimation paradigms: using the predictive distribution itself, internal representations throughout the model, and an ensemble of models. We show that this phenomenon can be theoretically explained, revealing that predictive-distribution and ensemble-based estimators are fundamentally limited under ambiguity. Overall, our study reveals a key shortcoming of current UQ methods for LLMs and motivates a rethinking of current modeling paradigms.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Equivalence of Regression and Classification</title>
<link>https://arxiv.org/abs/2511.04422</link>
<guid>https://arxiv.org/abs/2511.04422</guid>
<content:encoded><![CDATA[
arXiv:2511.04422v1 Announce Type: new 
Abstract: A formal link between regression and classification has been tenuous. Even though the margin maximization term $\|w\|$ is used in support vector regression, it has at best been justified as a regularizer. We show that a regression problem with $M$ samples lying on a hyperplane has a one-to-one equivalence with a linearly separable classification task with $2M$ samples. We show that margin maximization on the equivalent classification task leads to a different regression formulation than traditionally used. Using the equivalence, we demonstrate a ``regressability'' measure, that can be used to estimate the difficulty of regressing a dataset, without needing to first learn a model for it. We use the equivalence to train neural networks to learn a linearizing map, that transforms input variables into a space where a linear regressor is adequate.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForecastGAN: A Decomposition-Based Adversarial Framework for Multi-Horizon Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.04445</link>
<guid>https://arxiv.org/abs/2511.04445</guid>
<content:encoded><![CDATA[
arXiv:2511.04445v1 Announce Type: new 
Abstract: Time series forecasting is essential across domains from finance to supply chain management. This paper introduces ForecastGAN, a novel decomposition based adversarial framework addressing limitations in existing approaches for multi-horizon predictions. Although transformer models excel in long-term forecasting, they often underperform in short-term scenarios and typically ignore categorical features. ForecastGAN operates through three integrated modules: a Decomposition Module that extracts seasonality and trend components; a Model Selection Module that identifies optimal neural network configurations based on forecasting horizon; and an Adversarial Training Module that enhances prediction robustness through Conditional Generative Adversarial Network training. Unlike conventional approaches, ForecastGAN effectively integrates both numerical and categorical features. We validate our framework on eleven benchmark multivariate time series datasets that span various forecasting horizons. The results show that ForecastGAN consistently outperforms state-of-the-art transformer models for short-term forecasting while remaining competitive for long-term horizons. This research establishes a more generalizable approach to time series forecasting that adapts to specific contexts while maintaining strong performance across diverse data characteristics without extensive hyperparameter tuning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Stochastic Minimax Optimization under Heavy-Tailed Noises</title>
<link>https://arxiv.org/abs/2511.04456</link>
<guid>https://arxiv.org/abs/2511.04456</guid>
<content:encoded><![CDATA[
arXiv:2511.04456v1 Announce Type: new 
Abstract: Heavy-tailed noise has attracted growing attention in nonconvex stochastic optimization, as numerous empirical studies suggest it offers a more realistic assumption than standard bounded variance assumption. In this work, we investigate nonconvex-PL minimax optimization under heavy-tailed gradient noise in federated learning. We propose two novel algorithms: Fed-NSGDA-M, which integrates normalized gradients, and FedMuon-DA, which leverages the Muon optimizer for local updates. Both algorithms are designed to effectively address heavy-tailed noise in federated minimax optimization, under a milder condition. We theoretically establish that both algorithms achieve a convergence rate of $O({1}/{(TNp)^{\frac{s-1}{2s}}})$. To the best of our knowledge, these are the first federated minimax optimization algorithms with rigorous theoretical guarantees under heavy-tailed noise. Extensive experiments further validate their effectiveness.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Causal Market Simulators</title>
<link>https://arxiv.org/abs/2511.04469</link>
<guid>https://arxiv.org/abs/2511.04469</guid>
<content:encoded><![CDATA[
arXiv:2511.04469v1 Announce Type: new 
Abstract: Market generators using deep generative models have shown promise for synthetic financial data generation, but existing approaches lack causal reasoning capabilities essential for counterfactual analysis and risk assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that combines variational autoencoders with structural causal models to generate counterfactual financial time series while preserving both temporal dependencies and causal relationships. Our approach enforces causal constraints through directed acyclic graphs in the decoder architecture and employs the causal Wasserstein distance for training. We validate our method on synthetic autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating superior performance in counterfactual probability estimation with L1 distances as low as 0.03-0.10 compared to ground truth. The model enables financial stress testing, scenario analysis, and enhanced backtesting by generating plausible counterfactual market trajectories that respect underlying causal mechanisms.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs</title>
<link>https://arxiv.org/abs/2511.04473</link>
<guid>https://arxiv.org/abs/2511.04473</guid>
<content:encoded><![CDATA[
arXiv:2511.04473v1 Announce Type: new 
Abstract: Retrieval of information from graph-structured knowledge bases represents a promising direction for improving the factuality of LLMs. While various solutions have been proposed, a comparison of methods is difficult due to the lack of challenging QA datasets with ground-truth targets for graph retrieval. We present SynthKGQA, a framework for generating high-quality synthetic Knowledge Graph Question Answering datasets from any Knowledge Graph, providing the full set of ground-truth facts in the KG to reason over each question. We show how, in addition to enabling more informative benchmarking of KG retrievers, the data produced with SynthKGQA also allows us to train better models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset designed to test zero-shot generalization abilities of KG retrievers with respect to unseen graph structures and relation types, and benchmark popular solutions for KG-augmented LLMs on it.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training</title>
<link>https://arxiv.org/abs/2511.04485</link>
<guid>https://arxiv.org/abs/2511.04485</guid>
<content:encoded><![CDATA[
arXiv:2511.04485v1 Announce Type: new 
Abstract: Parameter-efficient training, based on low-rank optimization, has become a highly successful tool for fine-tuning large deep-learning models. However, these methods fail at low-rank pre-training tasks where maintaining the low-rank structure and the objective remains a challenging task. We propose the Quadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel low-rank inducing training strategy inspired by the iteratively reweighted least squares (IRLS) framework. Q3R is based on a quadratic regularizer term which majorizes a smoothed log determinant serving as rank surrogate objective. Unlike other low-rank training techniques, Q3R is able to train weight matrices with prescribed, low target ranks of models that achieve comparable predictive performance as dense models, with small computational overhead, while remaining fully compatible with existing architectures. For example, we demonstrated one experiment where we are able to truncate $60\%$ and $80\%$ of the parameters of a ViT-Tiny model with $~1.3\%$ and $~4\%$ accuracy drop in CIFAR-10 performance respectively. The efficacy of Q3R is confirmed on Transformers across both image and language tasks, including for low-rank fine-tuning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2511.04494</link>
<guid>https://arxiv.org/abs/2511.04494</guid>
<content:encoded><![CDATA[
arXiv:2511.04494v1 Announce Type: new 
Abstract: Neural networks are widely used for image-related tasks but typically demand considerable computing power. Once a network has been trained, however, its memory- and compute-footprint can be reduced by compression. In this work, we focus on compression through tensorization and low-rank representations. Whereas classical approaches search for a low-rank approximation by minimizing an isotropic norm such as the Frobenius norm in weight-space, we use data-informed norms that measure the error in function space. Concretely, we minimize the change in the layer's output distribution, which can be expressed as $\lVert (W - \widetilde{W}) \Sigma^{1/2}\rVert_F$ where $\Sigma^{1/2}$ is the square root of the covariance matrix of the layer's input and $W$, $\widetilde{W}$ are the original and compressed weights. We propose new alternating least square algorithms for the two most common tensor decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike conventional compression pipelines, which almost always require post-compression fine-tuning, our data-informed approach often achieves competitive accuracy without any fine-tuning. We further show that the same covariance-based norm can be transferred from one dataset to another with only a minor accuracy drop, enabling compression even when the original training dataset is unavailable. Experiments on several CNN architectures (ResNet-18/50, and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100) confirm the advantages of the proposed method.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternative Fairness and Accuracy Optimization in Criminal Justice</title>
<link>https://arxiv.org/abs/2511.04505</link>
<guid>https://arxiv.org/abs/2511.04505</guid>
<content:encoded><![CDATA[
arXiv:2511.04505v1 Announce Type: new 
Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts remain unsettled, especially in criminal justice. We review group, individual, and process fairness and map the conditions under which they conflict. We then develop a simple modification to standard group fairness. Rather than exact parity across protected groups, we minimize a weighted error loss while keeping differences in false negative rates within a small tolerance. This makes solutions easier to find, can raise predictive accuracy, and surfaces the ethical choice of error costs. We situate this proposal within three classes of critique: biased and incomplete data, latent affirmative action, and the explosion of subgroup constraints. Finally, we offer a practical framework for deployment in public decision systems built on three pillars: need-based decisions, Transparency and accountability, and narrowly tailored definitions and solutions. Together, these elements link technical design to legitimacy and provide actionable guidance for agencies that use risk assessment and related tools.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers</title>
<link>https://arxiv.org/abs/2511.04514</link>
<guid>https://arxiv.org/abs/2511.04514</guid>
<content:encoded><![CDATA[
arXiv:2511.04514v1 Announce Type: new 
Abstract: The phenomenon of linear mode connectivity (LMC) links several aspects of deep learning, including training stability under noisy stochastic gradients, the smoothness and generalization of local minima (basins), the similarity and functional diversity of sampled models, and architectural effects on data processing. In this work, we experimentally study LMC under data shifts and identify conditions that mitigate their impact. We interpret data shifts as an additional source of stochastic gradient noise, which can be reduced through small learning rates and large batch sizes. These parameters influence whether models converge to the same local minimum or to regions of the loss landscape with varying smoothness and generalization. Although models sampled via LMC tend to make similar errors more frequently than those converging to different basins, the benefit of LMC lies in balancing training efficiency against the gains achieved from larger, more diverse ensembles. Code and supplementary materials will be made publicly available at https://github.com/DLR-KI/LMC in due course.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing EPGP Surrogates and Finite Elements Under Degree-of-Freedom Parity</title>
<link>https://arxiv.org/abs/2511.04518</link>
<guid>https://arxiv.org/abs/2511.04518</guid>
<content:encoded><![CDATA[
arXiv:2511.04518v1 Announce Type: new 
Abstract: We present a new benchmarking study comparing a boundary-constrained Ehrenpreis--Palamodov Gaussian Process (B-EPGP) surrogate with a classical finite element method combined with Crank--Nicolson time stepping (CN-FEM) for solving the two-dimensional wave equation with homogeneous Dirichlet boundary conditions. The B-EPGP construction leverages exponential-polynomial bases derived from the characteristic variety to enforce the PDE and boundary conditions exactly and employs penalized least squares to estimate the coefficients. To ensure fairness across paradigms, we introduce a degrees-of-freedom (DoF) matching protocol. Under matched DoF, B-EPGP consistently attains lower space-time $L^2$-error and maximum-in-time $L^{2}$-error in space than CN-FEM, improving accuracy by roughly two orders of magnitude.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Reinforcement Learning of Koopman Models for eNMPC of an Air Separation Unit</title>
<link>https://arxiv.org/abs/2511.04522</link>
<guid>https://arxiv.org/abs/2511.04522</guid>
<content:encoded><![CDATA[
arXiv:2511.04522v1 Announce Type: new 
Abstract: With our recently proposed method based on reinforcement learning (Mayfrank et al. (2024), Comput. Chem. Eng. 190), Koopman surrogate models can be trained for optimal performance in specific (economic) nonlinear model predictive control ((e)NMPC) applications. So far, our method has exclusively been demonstrated on a small-scale case study. Herein, we show that our method scales well to a more challenging demand response case study built on a large-scale model of a single-product (nitrogen) air separation unit. Across all numerical experiments, we assume observability of only a few realistically measurable plant variables. Compared to a purely system identification-based Koopman eNMPC, which generates small economic savings but frequently violates constraints, our method delivers similar economic performance while avoiding constraint violations.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics</title>
<link>https://arxiv.org/abs/2511.04534</link>
<guid>https://arxiv.org/abs/2511.04534</guid>
<content:encoded><![CDATA[
arXiv:2511.04534v1 Announce Type: new 
Abstract: Reduced-order models (ROMs) can efficiently simulate high-dimensional physical systems, but lack robust uncertainty quantification methods. Existing approaches are frequently architecture- or training-specific, which limits flexibility and generalization. We introduce a post hoc, model-agnostic framework for predictive uncertainty quantification in latent space ROMs that requires no modification to the underlying architecture or training procedure. Using conformal prediction, our approach estimates statistical prediction intervals for multiple components of the ROM pipeline: latent dynamics, reconstruction, and end-to-end predictions. We demonstrate the method on a latent space dynamical model for cloud microphysics, where it accurately predicts the evolution of droplet-size distributions and quantifies uncertainty across the ROM pipeline.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning</title>
<link>https://arxiv.org/abs/2511.04557</link>
<guid>https://arxiv.org/abs/2511.04557</guid>
<content:encoded><![CDATA[
arXiv:2511.04557v1 Announce Type: new 
Abstract: In domains such as healthcare, finance, and e-commerce, the temporal dynamics of relational data emerge from complex interactions-such as those between patients and providers, or users and products across diverse categories. To be broadly useful, models operating on these data must integrate long-range spatial and temporal dependencies across diverse types of entities, while also supporting multiple predictive tasks. However, existing graph models for relational data primarily focus on spatial structure, treating temporal information merely as a filtering constraint to exclude future events rather than a modeling signal, and are typically designed for single-task prediction. To address these gaps, we introduce a temporal subgraph sampler that enhances global context by retrieving nodes beyond the immediate neighborhood to capture temporally relevant relationships. In addition, we propose the Relational Graph Perceiver (RGP), a graph transformer architecture for relational deep learning that leverages a cross-attention-based latent bottleneck to efficiently integrate information from both structural and temporal contexts. This latent bottleneck integrates signals from different node and edge types into a common latent space, enabling the model to build global context across the entire relational system. RGP also incorporates a flexible cross-attention decoder that supports joint learning across tasks with disjoint label spaces within a single model. Experiments on RelBench, SALT, and CTU show that RGP delivers state-of-the-art performance, offering a general and scalable solution for relational deep learning with support for diverse predictive tasks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARETE: an R package for Automated REtrieval from TExt with large language models</title>
<link>https://arxiv.org/abs/2511.04573</link>
<guid>https://arxiv.org/abs/2511.04573</guid>
<content:encoded><![CDATA[
arXiv:2511.04573v1 Announce Type: new 
Abstract: 1. A hard stop for the implementation of rigorous conservation initiatives is our lack of key species data, especially occurrence data. Furthermore, researchers have to contend with an accelerated speed at which new information must be collected and processed due to anthropogenic activity. Publications ranging from scientific papers to gray literature contain this crucial information but their data are often not machine-readable, requiring extensive human work to be retrieved. 2. We present the ARETE R package, an open-source software aiming to automate data extraction of species occurrences powered by large language models, namely using the chatGPT Application Programming Interface. This R package integrates all steps of the data extraction and validation process, from Optical Character Recognition to detection of outliers and output in tabular format. Furthermore, we validate ARETE through systematic comparison between what is modelled and the work of human annotators. 3. We demonstrate the usefulness of the approach by comparing range maps produced using GBIF data and with those automatically extracted for 100 species of spiders. Newly extracted data allowed to expand the known Extent of Occurrence by a mean three orders of magnitude, revealing new areas where the species were found in the past, which mayhave important implications for spatial conservation planning and extinction risk assessments. 4. ARETE allows faster access to hitherto untapped occurrence data, a potential game changer in projects requiring such data. Researchers will be able to better prioritize resources, manually verifying selected species while maintaining automated extraction for the majority. This workflow also allows predicting available bibliographic data during project planning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complexity as Advantage: A Regret-Based Perspective on Emergent Structure</title>
<link>https://arxiv.org/abs/2511.04590</link>
<guid>https://arxiv.org/abs/2511.04590</guid>
<content:encoded><![CDATA[
arXiv:2511.04590v1 Announce Type: new 
Abstract: We introduce Complexity as Advantage (CAA), a framework that defines the complexity of a system relative to a family of observers. Instead of measuring complexity as an intrinsic property, we evaluate how much predictive regret a system induces for different observers attempting to model it. A system is complex when it is easy for some observers and hard for others, creating an information advantage. We show that this formulation unifies several notions of emergent behavior, including multiscale entropy, predictive information, and observer-dependent structure. The framework suggests that "interesting" systems are those positioned to create differentiated regret across observers, providing a quantitative grounding for why complexity can be functionally valuable. We demonstrate the idea through simple dynamical models and discuss implications for learning, evolution, and artificial agents.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems</title>
<link>https://arxiv.org/abs/2511.04594</link>
<guid>https://arxiv.org/abs/2511.04594</guid>
<content:encoded><![CDATA[
arXiv:2511.04594v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) are central to applications such as swarm robotics and traffic routing, where agents must coordinate in a decentralized manner to achieve a common objective. Stochastic Shortest Path (SSP) problems provide a natural framework for modeling decentralized control in such settings. While the problem of learning in SSP has been extensively studied in single-agent settings, the decentralized multi-agent variant remains largely unexplored. In this work, we take a step towards addressing that gap. We study decentralized multi-agent SSPs (Dec-MASSPs) under linear function approximation, where the transition dynamics and costs are represented using linear models. Applying novel symmetry-based arguments, we identify the structure of optimal policies. Our main contribution is the first regret lower bound for this setting based on the construction of hard-to-learn instances for any number of agents, $n$. Our regret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights the inherent learning difficulty in Dec-MASSPs. These insights clarify the learning complexity of decentralized control and can further guide the design of efficient learning algorithms in multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning</title>
<link>https://arxiv.org/abs/2511.04598</link>
<guid>https://arxiv.org/abs/2511.04598</guid>
<content:encoded><![CDATA[
arXiv:2511.04598v1 Announce Type: new 
Abstract: In this paper we study how transforming regular reinforcement learning environments into goal-conditioned environments can let agents learn to solve tasks autonomously and reward-free. We show that an agent can learn to solve tasks by selecting its own goals in an environment-agnostic way, at training times comparable to externally guided reinforcement learning. Our method is independent of the underlying off-policy learning algorithm. Since our method is environment-agnostic, the agent does not value any goals higher than others, leading to instability in performance for individual goals. However, in our experiments, we show that the average goal success rate improves and stabilizes. An agent trained with this method can be instructed to seek any observations made in the environment, enabling generic training of agents prior to specific use cases.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addressing divergent representations from causal interventions on neural networks</title>
<link>https://arxiv.org/abs/2511.04638</link>
<guid>https://arxiv.org/abs/2511.04638</guid>
<content:encoded><![CDATA[
arXiv:2511.04638v1 Announce Type: new 
Abstract: A common approach to mechanistic interpretability is to causally manipulate model representations via targeted interventions in order to understand what those representations encode. Here we ask whether such interventions create out-of-distribution (divergent) representations, and whether this raises concerns about how faithful their resulting explanations are to the target model in its natural state. First, we demonstrate empirically that common causal intervention techniques often do shift internal representations away from the natural distribution of the target model. Then, we provide a theoretical analysis of two classes of such divergences: `harmless' divergences that occur in the null-space of the weights and from covariance within behavioral decision boundaries, and `pernicious' divergences that activate hidden network pathways and cause dormant behavioral changes. Finally, in an effort to mitigate the pernicious cases, we modify the Counterfactual Latent (CL) loss from Grant (2025) that regularizes interventions to remain closer to the natural distributions, reducing the likelihood of harmful divergences while preserving the interpretive power of interventions. Together, these results highlight a path towards more reliable interpretability methods.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient probabilistic surrogate modeling techniques for partially-observed large-scale dynamical systems</title>
<link>https://arxiv.org/abs/2511.04641</link>
<guid>https://arxiv.org/abs/2511.04641</guid>
<content:encoded><![CDATA[
arXiv:2511.04641v1 Announce Type: new 
Abstract: This paper is concerned with probabilistic techniques for forecasting dynamical systems described by partial differential equations (such as, for example, the Navier-Stokes equations). In particular, it is investigating and comparing various extensions to the flow matching paradigm that reduce the number of sampling steps. In this regard, it compares direct distillation, progressive distillation, adversarial diffusion distillation, Wasserstein GANs and rectified flows. Moreover, experiments are conducted on a set of challenging systems. In particular, we also address the challenge of directly predicting 2D slices of large-scale 3D simulations, paving the way for efficient inflow generation for solvers.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Inference Schedules for Masked Diffusion Models</title>
<link>https://arxiv.org/abs/2511.04647</link>
<guid>https://arxiv.org/abs/2511.04647</guid>
<content:encoded><![CDATA[
arXiv:2511.04647v1 Announce Type: new 
Abstract: A major bottleneck of standard auto-regressive large language models is that their inference process is inherently sequential, resulting in very long and costly inference times. To circumvent this, practitioners proposed a class of language models called diffusion language models, of which the masked diffusion model (MDM) is the most successful. The MDM is able to sample tokens out-of-order and, ostensibly, many tokens at once and in parallel. However, there is very limited rigorous understanding of how much parallel sampling these models can perform without noticeable degradation in their sampling performance. Prior work of Li and Cai obtained some preliminary bounds, but these are not tight for many natural classes of distributions. In this work, we give a new, exact characterization of the expected divergence between the true distribution and the sampled distribution, for any distribution and any unmasking schedule for the sampler, showing an elegant connection to the theory of univariate function approximation.
  By leveraging this connection, we then attain a number of novel lower and upper bounds for this problem. While the connection to function approximation in principle gives the optimal unmasking schedule for any distribution, we show that it is in general impossible to compete with it without strong a priori knowledge of the distribution, even in seemingly benign settings. However, we also demonstrate new upper bounds and new sampling schedules in terms of well-studied information-theoretic properties of the base distribution, namely, its total correlation and dual total correlation, which show that in some natural settings, one can sample in $O(log n)$ steps without any visible loss in performance, where $n$ is the total sequence length.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning</title>
<link>https://arxiv.org/abs/2511.04653</link>
<guid>https://arxiv.org/abs/2511.04653</guid>
<content:encoded><![CDATA[
arXiv:2511.04653v1 Announce Type: new 
Abstract: Federated learning (FL) offers new opportunities in machine learning, particularly in addressing data privacy concerns. In contrast to conventional event-based federated learning, time-triggered federated learning (TT-Fed), as a general form of both asynchronous and synchronous FL, clusters users into different tiers based on fixed time intervals. However, the FL network consists of a growing number of user devices with limited wireless bandwidth, consequently magnifying issues such as stragglers and communication overhead. In this paper, we introduce adaptive model pruning to wireless TT-Fed systems and study the problem of jointly optimizing the pruning ratio and bandwidth allocation to minimize the training loss while ensuring minimal learning latency. To answer this question, we perform convergence analysis on the gradient l_2 norm of the TT-Fed model based on model pruning. Based on the obtained convergence upper bound, a joint optimization problem of pruning ratio and wireless bandwidth is formulated to minimize the model training loss under a given delay threshold. Then, we derive closed-form solutions for wireless bandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. The simulation results show that model pruning could reduce the communication cost by 40% while maintaining the model performance at the same level.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nowcast3D: Reliable precipitation nowcasting via gray-box learning</title>
<link>https://arxiv.org/abs/2511.04659</link>
<guid>https://arxiv.org/abs/2511.04659</guid>
<content:encoded><![CDATA[
arXiv:2511.04659v1 Announce Type: new 
Abstract: Extreme precipitation nowcasting demands high spatiotemporal fidelity and extended lead times, yet existing approaches remain limited. Numerical Weather Prediction (NWP) and its deep-learning emulations are too slow and coarse for rapidly evolving convection, while extrapolation and purely data-driven models suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based methods discard crucial vertical information, preventing accurate reconstruction of height-dependent dynamics. We introduce a gray-box, fully three-dimensional nowcasting framework that directly processes volumetric radar reflectivity and couples physically constrained neural operators with datadriven learning. The model learns vertically varying 3D advection fields under a conservative advection operator, parameterizes spatially varying diffusion, and introduces a Brownian-motion--inspired stochastic term to represent unresolved motions. A residual branch captures small-scale convective initiation and microphysical variability, while a diffusion-based stochastic module estimates uncertainty. The framework achieves more accurate forecasts up to three-hour lead time across precipitation regimes and ranked first in 57\% of cases in a blind evaluation by 160 meteorologists. By restoring full 3D dynamics with physical consistency, it offers a scalable and robust pathway for skillful and reliable nowcasting of extreme precipitation.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetting is Everywhere</title>
<link>https://arxiv.org/abs/2511.04666</link>
<guid>https://arxiv.org/abs/2511.04666</guid>
<content:encoded><![CDATA[
arXiv:2511.04666v1 Announce Type: new 
Abstract: A fundamental challenge in developing general learning algorithms is their tendency to forget past knowledge when adapting to new data. Addressing this problem requires a principled understanding of forgetting; yet, despite decades of study, no unified definition has emerged that provides insights into the underlying dynamics of learning. We propose an algorithm- and task-agnostic theory that characterises forgetting as a lack of self-consistency in a learner's predictive distribution over future experiences, manifesting as a loss of predictive information. Our theory naturally yields a general measure of an algorithm's propensity to forget. To validate the theory, we design a comprehensive set of experiments that span classification, regression, generative modelling, and reinforcement learning. We empirically demonstrate how forgetting is present across all learning settings and plays a significant role in determining learning efficiency. Together, these results establish a principled understanding of forgetting and lay the foundation for analysing and improving the information retention capabilities of general learning algorithms.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Method Analysis of Mathematics Placement Assessments: Classical, Machine Learning, and Clustering Approaches</title>
<link>https://arxiv.org/abs/2511.04667</link>
<guid>https://arxiv.org/abs/2511.04667</guid>
<content:encoded><![CDATA[
arXiv:2511.04667v1 Announce Type: new 
Abstract: This study evaluates a 40-item mathematics placement examination administered to 198 students using a multi-method framework combining Classical Test Theory, machine learning, and unsupervised clustering. Classical Test Theory analysis reveals that 55\% of items achieve excellent discrimination ($D \geq 0.40$) while 30\% demonstrate poor discrimination ($D < 0.20$) requiring replacement. Question 6 (Graph Interpretation) emerges as the examination's most powerful discriminator, achieving perfect discrimination ($D = 1.000$), highest ANOVA F-statistic ($F = 4609.1$), and maximum Random Forest feature importance (0.206), accounting for 20.6\% of predictive power. Machine learning algorithms demonstrate exceptional performance, with Random Forest and Gradient Boosting achieving 97.5\% and 96.0\% cross-validation accuracy. K-means clustering identifies a natural binary competency structure with a boundary at 42.5\%, diverging from the institutional threshold of 55\% and suggesting potential overclassification into remedial categories. The two-cluster solution exhibits exceptional stability (bootstrap ARI = 0.855) with perfect lower-cluster purity. Convergent evidence across methods supports specific refinements: replace poorly discriminating items, implement a two-stage assessment, and integrate Random Forest predictions with transparency mechanisms. These findings demonstrate that multi-method integration provides a robust empirical foundation for evidence-based mathematics placement optimization.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulation-Based Validation of an Integrated 4D/5D Digital-Twin Framework for Predictive Construction Control</title>
<link>https://arxiv.org/abs/2511.03684</link>
<guid>https://arxiv.org/abs/2511.03684</guid>
<content:encoded><![CDATA[
arXiv:2511.03684v1 Announce Type: cross 
Abstract: Persistent cost and schedule deviations remain a major challenge in the U.S. construction industry, revealing the limitations of deterministic CPM and static document-based estimating. This study presents an integrated 4D/5D digital-twin framework that couples Building Information Modeling (BIM) with natural-language processing (NLP)-based cost mapping, computer-vision (CV)-driven progress measurement, Bayesian probabilistic CPM updating, and deep-reinforcement-learning (DRL) resource-leveling. A nine-month case implementation on a Dallas-Fort Worth mid-rise project demonstrated measurable gains in accuracy and efficiency: 43% reduction in estimating labor, 6% reduction in overtime, and 30% project-buffer utilization, while maintaining an on-time finish at 128 days within P50-P80 confidence bounds. The digital-twin sandbox also enabled real-time "what-if" forecasting and traceable cost-schedule alignment through a 5D knowledge graph. Findings confirm that integrating AI-based analytics with probabilistic CPM and DRL enhances forecasting precision, transparency, and control resilience. The validated workflow establishes a practical pathway toward predictive, adaptive, and auditable construction management.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Friction on Demand: A Generative Framework for the Inverse Design of Metainterfaces</title>
<link>https://arxiv.org/abs/2511.03735</link>
<guid>https://arxiv.org/abs/2511.03735</guid>
<content:encoded><![CDATA[
arXiv:2511.03735v1 Announce Type: cross 
Abstract: Designing frictional interfaces to exhibit prescribed macroscopic behavior is a challenging inverse problem, made difficult by the non-uniqueness of solutions and the computational cost of contact simulations. Traditional approaches rely on heuristic search over low-dimensional parameterizations, which limits their applicability to more complex or nonlinear friction laws. We introduce a generative modeling framework using Variational Autoencoders (VAEs) to infer surface topographies from target friction laws. Trained on a synthetic dataset composed of 200 million samples constructed from a parameterized contact mechanics model, the proposed method enables efficient, simulation-free generation of candidate topographies. We examine the potential and limitations of generative modeling for this inverse design task, focusing on balancing accuracy, throughput, and diversity in the generated solutions. Our results highlight trade-offs and outline practical considerations when balancing these objectives. This approach paves the way for near-real-time control of frictional behavior through tailored surface topographies.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A convolutional neural network deep learning method for model class selection</title>
<link>https://arxiv.org/abs/2511.03743</link>
<guid>https://arxiv.org/abs/2511.03743</guid>
<content:encoded><![CDATA[
arXiv:2511.03743v1 Announce Type: cross 
Abstract: The response-only model class selection capability of a novel deep convolutional neural network method is examined herein in a simple, yet effective, manner. Specifically, the responses from a unique degree of freedom along with their class information train and validate a one-dimensional convolutional neural network. In doing so, the network selects the model class of new and unlabeled signals without the need of the system input information, or full system identification. An optional physics-based algorithm enhancement is also examined using the Kalman filter to fuse the system response signals using the kinematics constraints of the acceleration and displacement data. Importantly, the method is shown to select the model class in slight signal variations attributed to the damping behavior or hysteresis behavior on both linear and nonlinear dynamic systems, as well as on a 3D building finite element model, providing a powerful tool for structural health monitoring applications.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dynamic Recurrent Adjacency Memory Network for Mixed-Generation Power System Stability Forecasting</title>
<link>https://arxiv.org/abs/2511.03746</link>
<guid>https://arxiv.org/abs/2511.03746</guid>
<content:encoded><![CDATA[
arXiv:2511.03746v1 Announce Type: cross 
Abstract: Modern power systems with high penetration of inverter-based resources exhibit complex dynamic behaviors that challenge the scalability and generalizability of traditional stability assessment methods. This paper presents a dynamic recurrent adjacency memory network (DRAMN) that combines physics-informed analysis with deep learning for real-time power system stability forecasting. The framework employs sliding-window dynamic mode decomposition to construct time-varying, multi-layer adjacency matrices from phasor measurement unit and sensor data to capture system dynamics such as modal participation factors, coupling strengths, phase relationships, and spectral energy distributions. As opposed to processing spatial and temporal dependencies separately, DRAMN integrates graph convolution operations directly within recurrent gating mechanisms, enabling simultaneous modeling of evolving dynamics and temporal dependencies. Extensive validations on modified IEEE 9-bus, 39-bus, and a multi-terminal HVDC network demonstrate high performance, achieving 99.85\%, 99.90\%, and 99.69\% average accuracies, respectively, surpassing all tested benchmarks, including classical machine learning algorithms and recent graph-based models. The framework identifies optimal combinations of measurements that reduce feature dimensionality by 82\% without performance degradation. Correlation analysis between dominant measurements for small-signal and transient stability events validates generalizability across different stability phenomena. DRAMN achieves state-of-the-art accuracy while providing enhanced interpretability for power system operators, making it suitable for real-time deployment in modern control centers.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bifidelity Karhunen-Lo\`eve Expansion Surrogate with Active Learning for Random Fields</title>
<link>https://arxiv.org/abs/2511.03756</link>
<guid>https://arxiv.org/abs/2511.03756</guid>
<content:encoded><![CDATA[
arXiv:2511.03756v1 Announce Type: cross 
Abstract: We present a bifidelity Karhunen-Lo\`eve expansion (KLE) surrogate model for field-valued quantities of interest (QoIs) under uncertain inputs. The approach combines the spectral efficiency of the KLE with polynomial chaos expansions (PCEs) to preserve an explicit mapping between input uncertainties and output fields. By coupling inexpensive low-fidelity (LF) simulations that capture dominant response trends with a limited number of high-fidelity (HF) simulations that correct for systematic bias, the proposed method enables accurate and computationally affordable surrogate construction. To further improve surrogate accuracy, we form an active learning strategy that adaptively selects new HF evaluations based on the surrogate's generalization error, estimated via cross-validation and modeled using Gaussian process regression. New HF samples are then acquired by maximizing an expected improvement criterion, targeting regions of high surrogate error. The resulting BF-KLE-AL framework is demonstrated on three examples of increasing complexity: a one-dimensional analytical benchmark, a two-dimensional convection-diffusion system, and a three-dimensional turbulent round jet simulation based on Reynolds-averaged Navier--Stokes (RANS) and enhanced delayed detached-eddy simulations (EDDES). Across these cases, the method achieves consistent improvements in predictive accuracy and sample efficiency relative to single-fidelity and random-sampling approaches.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning-Driven Downscaling for Climate Risk Assessment of Projected Temperature Extremes in the Nordic Region</title>
<link>https://arxiv.org/abs/2511.03770</link>
<guid>https://arxiv.org/abs/2511.03770</guid>
<content:encoded><![CDATA[
arXiv:2511.03770v1 Announce Type: cross 
Abstract: Rapid changes and increasing climatic variability across the widely varied Koppen-Geiger regions of northern Europe generate significant needs for adaptation. Regional planning needs high-resolution projected temperatures. This work presents an integrative downscaling framework that incorporates Vision Transformer (ViT), Convolutional Long Short-Term Memory (ConvLSTM), and Geospatial Spatiotemporal Transformer with Attention and Imbalance-Aware Network (GeoStaNet) models. The framework is evaluated with a multicriteria decision system, Deep Learning-TOPSIS (DL-TOPSIS), for ten strategically chosen meteorological stations encompassing the temperate oceanic (Cfb), subpolar oceanic (Cfc), warm-summer continental (Dfb), and subarctic (Dfc) climate regions. Norwegian Earth System Model (NorESM2-LM) Coupled Model Intercomparison Project Phase 6 (CMIP6) outputs were bias-corrected during the 1951-2014 period and subsequently validated against earlier observations of day-to-day temperature metrics and diurnal range statistics. The ViT showed improved performance (Root Mean Squared Error (RMSE): 1.01 degrees C; R^2: 0.92), allowing for production of credible downscaled projections. Under the SSP5-8.5 scenario, the Dfc and Dfb climate zones are projected to warm by 4.8 degrees C and 3.9 degrees C, respectively, by 2100, with expansion in the diurnal temperature range by more than 1.5 degrees C. The Time of Emergence signal first appears in subarctic winter seasons (Dfc: approximately 2032), signifying an urgent need for adaptation measures. The presented framework offers station-based, high-resolution estimates of uncertainties and extremes, with direct uses for adaptation policy over high-latitude regions with fast environmental change.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Climbing the label tree: Hierarchy-preserving contrastive learning for medical imaging</title>
<link>https://arxiv.org/abs/2511.03771</link>
<guid>https://arxiv.org/abs/2511.03771</guid>
<content:encoded><![CDATA[
arXiv:2511.03771v1 Announce Type: cross 
Abstract: Medical image labels are often organized by taxonomies (e.g., organ - tissue - subtype), yet standard self-supervised learning (SSL) ignores this structure. We present a hierarchy-preserving contrastive framework that makes the label tree a first-class training signal and an evaluation target. Our approach introduces two plug-in objectives: Hierarchy-Weighted Contrastive (HWC), which scales positive/negative pair strengths by shared ancestors to promote within-parent coherence, and Level-Aware Margin (LAM), a prototype margin that separates ancestor groups across levels. The formulation is geometry-agnostic and applies to Euclidean and hyperbolic embeddings without architectural changes. Across several benchmarks, including breast histopathology, the proposed objectives consistently improve representation quality over strong SSL baselines while better respecting the taxonomy. We evaluate with metrics tailored to hierarchy faithfulness: HF1 (hierarchical F1), H-Acc (tree-distance-weighted accuracy), and parent-distance violation rate. We also report top-1 accuracy for completeness. Ablations show that HWC and LAM are effective even without curvature, and combining them yields the most taxonomy-aligned representations. Taken together, these results provide a simple, general recipe for learning medical image representations that respect the label tree and advance both performance and interpretability in hierarchy-rich domains.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Paths for Dynamic Measure Transport: A Control Perspective</title>
<link>https://arxiv.org/abs/2511.03797</link>
<guid>https://arxiv.org/abs/2511.03797</guid>
<content:encoded><![CDATA[
arXiv:2511.03797v1 Announce Type: cross 
Abstract: We bring a control perspective to the problem of identifying paths of measures for sampling via dynamic measure transport (DMT). We highlight the fact that commonly used paths may be poor choices for DMT and connect existing methods for learning alternate paths to mean-field games. Based on these connections we pose a flexible family of optimization problems for identifying tilted paths of measures for DMT and advocate for the use of objective terms which encourage smoothness of the corresponding velocities. We present a numerical algorithm for solving these problems based on recent Gaussian process methods for solution of partial differential equations and demonstrate the ability of our method to recover more efficient and smooth transport models compared to those which use an untilted reference path.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis</title>
<link>https://arxiv.org/abs/2511.03825</link>
<guid>https://arxiv.org/abs/2511.03825</guid>
<content:encoded><![CDATA[
arXiv:2511.03825v1 Announce Type: cross 
Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic characteristics like vocabulary size, semantic coverage, and extrinsic performance in downstream tasks. Despite its significance, tokenization in the context of assembly code remains an underexplored area. This study aims to address this gap by evaluating the intrinsic properties of Natural Language Processing (NLP) tokenization models and parameter choices, such as vocabulary size. We explore preprocessing customization options and pre-tokenization rules tailored to the unique characteristics of assembly code. Additionally, we assess their impact on downstream tasks like function signature prediction -- a critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models, systematically analyzing their efficiency in encoding assembly instructions and capturing semantic nuances. Through intrinsic evaluations, we compare tokenizers based on tokenization efficiency, vocabulary compression, and representational fidelity for assembly code. Using state-of-the-art pre-trained models such as the decoder-only Large Language Model (LLM) Llama 3.2, the encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate the effectiveness of these tokenizers across multiple performance metrics. Preliminary findings indicate that tokenizer choice significantly influences downstream performance, with intrinsic metrics providing partial but incomplete predictability of extrinsic evaluation outcomes. These results reveal complex trade-offs between intrinsic tokenizer properties and their utility in practical assembly code tasks. Ultimately, this study provides valuable insights into optimizing tokenization models for low-level code analysis, contributing to the robustness and scalability of Natural Language Model (NLM)-based binary analysis workflows.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To See or To Read: User Behavior Reasoning in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2511.03845</link>
<guid>https://arxiv.org/abs/2511.03845</guid>
<content:encoded><![CDATA[
arXiv:2511.03845v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic systems reason over sequential user-behavior data. However, whether textual or image representations of user behavior data are more effective for maximizing MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a systematic benchmarking framework for assessing modality trade-offs in user-behavior reasoning across six MLLMs by representing transaction data as (1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a real-world purchase-sequence dataset, we find that when data is represented as images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared with an equivalent textual representation without any additional computational cost.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Which Similarity-Sensitive Entropy?</title>
<link>https://arxiv.org/abs/2511.03849</link>
<guid>https://arxiv.org/abs/2511.03849</guid>
<content:encoded><![CDATA[
arXiv:2511.03849v1 Announce Type: cross 
Abstract: A canonical step in quantifying a system is to measure its entropy. Shannon entropy and other traditional entropy measures capture only the information encoded in the frequencies of a system's elements. Recently, Leinster, Cobbold, and Reeve (LCR) introduced a method that also captures the rich information encoded in the similarities and differences among elements, yielding similarity-sensitive entropy. More recently, the Vendi score (VS) was introduced as an alternative, raising the question of how LCR and VS compare, and which is preferable. Here we address these questions conceptually, analytically, and experimentally, using 53 machine-learning datasets. We show that LCR and VS can differ by orders of magnitude and can capture complementary information about a system, except in limiting cases. We demonstrate that both LCR and VS depend on how similarities are scaled and introduce the concept of ``half distance'' to parameterize this dependence. We prove that VS provides an upper bound on LCR for several values of the R\'enyi-Hill order parameter and conjecture that this bound holds for all values. We conclude that VS is preferable only when interpreting elements as linear combinations of a more fundamental set of ``ur-elements'' or when the system or dataset possesses a quantum-mechanical character. In the broader circumstance where one seeks simply to capture the rich information encoded by similarity, LCR is favored; nevertheless, for certain half-distances the two methods can complement each other.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms</title>
<link>https://arxiv.org/abs/2511.03866</link>
<guid>https://arxiv.org/abs/2511.03866</guid>
<content:encoded><![CDATA[
arXiv:2511.03866v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have significantly accelerated progress in code translation, enabling more accurate and efficient transformation across programming languages. While originally developed for natural language processing, LLMs have shown strong capabilities in modeling programming language syntax and semantics, outperforming traditional rule-based systems in both accuracy and flexibility. These models have streamlined cross-language conversion, reduced development overhead, and accelerated legacy code migration. In this paper, we introduce OMPILOT, a novel domain-specific encoder-decoder transformer tailored for translating C++ code into OpenMP, enabling effective shared-memory parallelization. OMPILOT leverages custom pre-training objectives that incorporate the semantics of parallel constructs and combines both unsupervised and supervised learning strategies to improve code translation robustness. Unlike previous work that focused primarily on loop-level transformations, OMPILOT operates at the function level to capture a wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel composite metric specifically crafted to assess the correctness and quality of OpenMP parallel constructs, addressing limitations in conventional translation metrics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computed Tomography (CT)-derived Cardiovascular Flow Estimation Using Physics-Informed Neural Networks Improves with Sinogram-based Training: A Simulation Study</title>
<link>https://arxiv.org/abs/2511.03876</link>
<guid>https://arxiv.org/abs/2511.03876</guid>
<content:encoded><![CDATA[
arXiv:2511.03876v1 Announce Type: cross 
Abstract: Background: Non-invasive imaging-based assessment of blood flow plays a critical role in evaluating heart function and structure. Computed Tomography (CT) is a widely-used imaging modality that can robustly evaluate cardiovascular anatomy and function, but direct methods to estimate blood flow velocity from movies of contrast evolution have not been developed.
  Purpose: This study evaluates the impact of CT imaging on Physics-Informed Neural Networks (PINN)-based flow estimation and proposes an improved framework, SinoFlow, which uses sinogram data directly to estimate blood flow.
  Methods: We generated pulsatile flow fields in an idealized 2D vessel bifurcation using computational fluid dynamics and simulated CT scans with varying gantry rotation speeds, tube currents, and pulse mode imaging settings. We compared the performance of PINN-based flow estimation using reconstructed images (ImageFlow) to SinoFlow.
  Results: SinoFlow significantly improved flow estimation performance by avoiding propagating errors introduced by filtered backprojection. SinoFlow was robust across all tested gantry rotation speeds and consistently produced lower mean squared error and velocity errors than ImageFlow. Additionally, SinoFlow was compatible with pulsed-mode imaging and maintained higher accuracy with shorter pulse widths.
  Conclusions: This study demonstrates the potential of SinoFlow for CT-based flow estimation, providing a more promising approach for non-invasive blood flow assessment. The findings aim to inform future applications of PINNs to CT images and provide a solution for image-based estimation, with reasonable acquisition parameters yielding accurate flow estimates.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KnowThyself: An Agentic Assistant for LLM Interpretability</title>
<link>https://arxiv.org/abs/2511.03878</link>
<guid>https://arxiv.org/abs/2511.03878</guid>
<content:encoded><![CDATA[
arXiv:2511.03878v1 Announce Type: cross 
Abstract: We develop KnowThyself, an agentic assistant that advances large language model (LLM) interpretability. Existing tools provide useful insights but remain fragmented and code-intensive. KnowThyself consolidates these capabilities into a chat-based interface, where users can upload models, pose natural language questions, and obtain interactive visualizations with guided explanations. At its core, an orchestrator LLM first reformulates user queries, an agent router further directs them to specialized modules, and the outputs are finally contextualized into coherent explanations. This design lowers technical barriers and provides an extensible platform for LLM inspection. By embedding the whole process into a conversational workflow, KnowThyself offers a robust foundation for accessible LLM interpretability.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures</title>
<link>https://arxiv.org/abs/2511.03882</link>
<guid>https://arxiv.org/abs/2511.03882</guid>
<content:encoded><![CDATA[
arXiv:2511.03882v1 Announce Type: cross 
Abstract: Imitation learning-based robot control policies are enjoying renewed interest in video-based robotics. However, it remains unclear whether this approach applies to X-ray-guided procedures, such as spine instrumentation. This is because interpretation of multi-view X-rays is complex. We examine opportunities and challenges for imitation policy learning in bi-plane-guided cannula insertion. We develop an in silico sandbox for scalable, automated simulation of X-ray-guided spine procedures with a high degree of realism. We curate a dataset of correct trajectories and corresponding bi-planar X-ray sequences that emulate the stepwise alignment of providers. We then train imitation learning policies for planning and open-loop control that iteratively align a cannula solely based on visual information. This precisely controlled setup offers insights into limitations and capabilities of this method. Our policy succeeded on the first attempt in 68.5% of cases, maintaining safe intra-pedicular trajectories across diverse vertebral levels. The policy generalized to complex anatomy, including fractures, and remained robust to varied initializations. Rollouts on real bi-planar X-rays further suggest that the model can produce plausible trajectories, despite training exclusively in simulation. While these preliminary results are promising, we also identify limitations, especially in entry point precision. Full closed-look control will require additional considerations around how to provide sufficiently frequent feedback. With more robust priors and domain knowledge, such models may provide a foundation for future efforts toward lightweight and CT-free robotic intra-operative spinal navigation.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model</title>
<link>https://arxiv.org/abs/2511.03888</link>
<guid>https://arxiv.org/abs/2511.03888</guid>
<content:encoded><![CDATA[
arXiv:2511.03888v1 Announce Type: cross 
Abstract: The global waste crisis is escalating, with solid waste generation expected to increase by 70% by 2050. Traditional waste collection methods, particularly in remote or harsh environments like deserts, are labor-intensive, inefficient, and often hazardous. Recent advances in computer vision and deep learning have opened the door to automated waste detection systems, yet most research focuses on urban environments and recyclable materials, overlooking organic and hazardous waste and underexplored terrains such as deserts. In this work, we propose an enhanced real-time object detection framework based on a pruned, lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT) and specialized data augmentation strategies. Using the DroneTrashNet dataset, we demonstrate significant improvements in precision, recall, and mean average precision (mAP), while achieving low latency and compact model size suitable for deployment on resource-constrained aerial drones. Benchmarking our model against state-of-the-art lightweight YOLO variants further highlights its optimal balance of accuracy and efficiency. Our results validate the effectiveness of combining data-centric and model-centric enhancements for robust, real-time waste detection in desert environments.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shape Deformation Networks for Automated Aortic Valve Finite Element Meshing from 3D CT Images</title>
<link>https://arxiv.org/abs/2511.03890</link>
<guid>https://arxiv.org/abs/2511.03890</guid>
<content:encoded><![CDATA[
arXiv:2511.03890v1 Announce Type: cross 
Abstract: Accurate geometric modeling of the aortic valve from 3D CT images is essential for biomechanical analysis and patient-specific simulations to assess valve health or make a preoperative plan. However, it remains challenging to generate aortic valve meshes with both high-quality and consistency across different patients. Traditional approaches often produce triangular meshes with irregular topologies, which can result in poorly shaped elements and inconsistent correspondence due to inter-patient anatomical variation. In this work, we address these challenges by introducing a template-fitting pipeline with deep neural networks to generate structured quad (i.e., quadrilateral) meshes from 3D CT images to represent aortic valve geometries. By remeshing aortic valves of all patients with a common quad mesh template, we ensure a uniform mesh topology with consistent node-to-node and element-to-element correspondence across patients. This consistency enables us to simplify the learning objective of the deep neural networks, by employing a loss function with only two terms (i.e., a geometry reconstruction term and a smoothness regularization term), which is sufficient to preserve mesh smoothness and element quality. Our experiments demonstrate that the proposed approach produces high-quality aortic valve surface meshes with improved smoothness and shape quality, while requiring fewer explicit regularization terms compared to the traditional methods. These results highlight that using structured quad meshes for the template and neural network training not only ensures mesh correspondence and quality but also simplifies the training process, thus enhancing the effectiveness and efficiency of aortic valve modeling.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A general technique for approximating high-dimensional empirical kernel matrices</title>
<link>https://arxiv.org/abs/2511.03892</link>
<guid>https://arxiv.org/abs/2511.03892</guid>
<content:encoded><![CDATA[
arXiv:2511.03892v1 Announce Type: cross 
Abstract: We present simple, user-friendly bounds for the expected operator norm of a random kernel matrix under general conditions on the kernel function $k(\cdot,\cdot)$. Our approach uses decoupling results for U-statistics and the non-commutative Khintchine inequality to obtain upper and lower bounds depending only on scalar statistics of the kernel function and a ``correlation kernel'' matrix corresponding to $k(\cdot,\cdot)$. We then apply our method to provide new, tighter approximations for inner-product kernel matrices on general high-dimensional data, where the sample size and data dimension are polynomially related. Our method obtains simplified proofs of existing results that rely on the moment method and combinatorial arguments while also providing novel approximation results for the case of anisotropic Gaussian data. Finally, using similar techniques to our approximation result, we show a tighter lower bound on the bias of kernel regression with anisotropic Gaussian data.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation</title>
<link>https://arxiv.org/abs/2511.03900</link>
<guid>https://arxiv.org/abs/2511.03900</guid>
<content:encoded><![CDATA[
arXiv:2511.03900v1 Announce Type: cross 
Abstract: Hallucination mitigation remains a persistent challenge for large language models (LLMs), even as model scales grow. Existing approaches often rely on external knowledge sources, such as structured databases or knowledge graphs, accessed through prompting or retrieval. However, prompt-based grounding is fragile and domain-sensitive, while symbolic knowledge integration incurs heavy retrieval and formatting costs. Motivated by knowledge graphs, we introduce Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds generation in corpus-derived evidence without retraining. GRAD constructs a sparse token transition graph by accumulating next-token logits across a small retrieved corpus in a single forward pass. During decoding, graph-retrieved logits are max-normalized and adaptively fused with model logits to favor high-evidence continuations while preserving fluency. Across three models and a range of question-answering benchmarks spanning intrinsic, extrinsic hallucination, and factuality tasks, GRAD consistently surpasses baselines, achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination rates, and 6.9$\%$ greater correctness compared to greedy decoding, while attaining the highest truth--informativeness product score among all methods. GRAD offers a lightweight, plug-and-play alternative to contrastive decoding and knowledge graph augmentation, demonstrating that statistical evidence from corpus-level token transitions can effectively steer generation toward more truthful and verifiable outputs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vectorized Computation of Euler Characteristic Functions and Transforms</title>
<link>https://arxiv.org/abs/2511.03909</link>
<guid>https://arxiv.org/abs/2511.03909</guid>
<content:encoded><![CDATA[
arXiv:2511.03909v1 Announce Type: cross 
Abstract: The weighted Euler characteristic transform (WECT) and Euler characteristic function (ECF) have proven to be useful tools in a variety of applications. However, current methods for computing these functions are neither optimized for speed nor do they scale to higher-dimensional settings. In this work, we present a vectorized framework for computing such topological transforms using tensor operations, which is highly optimized for GPU architectures and works in full generality across geometric simplicial complexes (or cubical complexes) of arbitrary dimension. Experimentally, the framework demonstrates significant speedups (up to $180 \times$) over existing methods when computing the WECT and ECF across a variety of image datasets. Computation of these transforms is implemented in a publicly available Python package called pyECT.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-dimensional limit theorems for SGD: Momentum and Adaptive Step-sizes</title>
<link>https://arxiv.org/abs/2511.03952</link>
<guid>https://arxiv.org/abs/2511.03952</guid>
<content:encoded><![CDATA[
arXiv:2511.03952v1 Announce Type: cross 
Abstract: We develop a high-dimensional scaling limit for Stochastic Gradient Descent with Polyak Momentum (SGD-M) and adaptive step-sizes. This provides a framework to rigourously compare online SGD with some of its popular variants. We show that the scaling limits of SGD-M coincide with those of online SGD after an appropriate time rescaling and a specific choice of step-size. However, if the step-size is kept the same between the two algorithms, SGD-M will amplify high-dimensional effects, potentially degrading performance relative to online SGD. We demonstrate our framework on two popular learning problems: Spiked Tensor PCA and Single Index Models. In both cases, we also examine online SGD with an adaptive step-size based on normalized gradients. In the high-dimensional regime, this algorithm yields multiple benefits: its dynamics admit fixed points closer to the population minimum and widens the range of admissible step-sizes for which the iterates converge to such solutions. These examples provide a rigorous account, aligning with empirical motivation, of how early preconditioners can stabilize and improve dynamics in settings where online SGD fails.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust inference using density-powered Stein operators</title>
<link>https://arxiv.org/abs/2511.03963</link>
<guid>https://arxiv.org/abs/2511.03963</guid>
<content:encoded><![CDATA[
arXiv:2511.03963v1 Announce Type: cross 
Abstract: We introduce a density-power weighted variant for the Stein operator, called the $\gamma$-Stein operator. This is a novel class of operators derived from the $\gamma$-divergence, designed to build robust inference methods for unnormalized probability models. The operator's construction (weighting by the model density raised to a positive power $\gamma$ inherently down-weights the influence of outliers, providing a principled mechanism for robustness. Applying this operator yields a robust generalization of score matching that retains the crucial property of being independent of the model's normalizing constant. We extend this framework to develop two key applications: the $\gamma$-kernelized Stein discrepancy for robust goodness-of-fit testing, and $\gamma$-Stein variational gradient descent for robust Bayesian posterior approximation. Empirical results on contaminated Gaussian and quartic potential models show our methods significantly outperform standard baselines in both robustness and statistical efficiency.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Characterization of List Language Identification in the Limit</title>
<link>https://arxiv.org/abs/2511.04103</link>
<guid>https://arxiv.org/abs/2511.04103</guid>
<content:encoded><![CDATA[
arXiv:2511.04103v1 Announce Type: cross 
Abstract: We study the problem of language identification in the limit, where given a sequence of examples from a target language, the goal of the learner is to output a sequence of guesses for the target language such that all the guesses beyond some finite time are correct. Classical results of Gold showed that language identification in the limit is impossible for essentially any interesting collection of languages. Later, Angluin gave a precise characterization of language collections for which this task is possible. Motivated by recent positive results for the related problem of language generation, we revisit the classic language identification problem in the setting where the learner is given the additional power of producing a list of $k$ guesses at each time step. The goal is to ensure that beyond some finite time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be $k$-list identified in the limit, based on a recursive version of Angluin's characterization (for language identification with a list of size $1$). This further leads to a conceptually appealing characterization: A language collection can be $k$-list identified in the limit if and only if the collection can be decomposed into $k$ collections of languages, each of which can be identified in the limit (with a list of size $1$). We also use our characterization to establish rates for list identification in the statistical setting where the input is drawn as an i.i.d. stream from a distribution supported on some language in the collection. Our results show that if a collection is $k$-list identifiable in the limit, then the collection can be $k$-list identified at an exponential rate, and this is best possible. On the other hand, if a collection is not $k$-list identifiable in the limit, then it cannot be $k$-list identified at any rate that goes to zero.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated and Explainable Denial of Service Analysis for AI-Driven Intrusion Detection Systems</title>
<link>https://arxiv.org/abs/2511.04114</link>
<guid>https://arxiv.org/abs/2511.04114</guid>
<content:encoded><![CDATA[
arXiv:2511.04114v1 Announce Type: cross 
Abstract: With the increasing frequency and sophistication of Distributed Denial of Service (DDoS) attacks, it has become critical to develop more efficient and interpretable detection methods. Traditional detection systems often struggle with scalability and transparency, hindering real-time response and understanding of attack vectors. This paper presents an automated framework for detecting and interpreting DDoS attacks using machine learning (ML). The proposed method leverages the Tree-based Pipeline Optimization Tool (TPOT) to automate the selection and optimization of ML models and features, reducing the need for manual experimentation. SHapley Additive exPlanations (SHAP) is incorporated to enhance model interpretability, providing detailed insights into the contribution of individual features to the detection process. By combining TPOT's automated pipeline selection with SHAP interpretability, this approach improves the accuracy and transparency of DDoS detection. Experimental results demonstrate that key features such as mean backward packet length and minimum forward packet header length are critical in detecting DDoS attacks, offering a scalable and explainable cybersecurity solution.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs</title>
<link>https://arxiv.org/abs/2511.04228</link>
<guid>https://arxiv.org/abs/2511.04228</guid>
<content:encoded><![CDATA[
arXiv:2511.04228v1 Announce Type: cross 
Abstract: Machine unlearning aims to remove the influence of specific training data from a model without requiring full retraining. This capability is crucial for ensuring privacy, safety, and regulatory compliance. Therefore, verifying whether a model has truly forgotten target data is essential for maintaining reliability and trustworthiness. However, existing evaluation methods often assess forgetting at the level of individual inputs. This approach may overlook residual influence present in semantically similar examples. Such influence can compromise privacy and lead to indirect information leakage. We propose REMIND (Residual Memorization In Neighborhood Dynamics), a novel evaluation method aiming to detect the subtle remaining influence of unlearned data and classify whether the data has been effectively forgotten. REMIND analyzes the model's loss over small input variations and reveals patterns unnoticed by single-point evaluations. We show that unlearned data yield flatter, less steep loss landscapes, while retained or unrelated data exhibit sharper, more volatile patterns. REMIND requires only query-based access, outperforms existing methods under similar constraints, and demonstrates robustness across different models, datasets, and paraphrased inputs, making it practical for real-world deployment. By providing a more sensitive and interpretable measure of unlearning effectiveness, REMIND provides a reliable framework to assess unlearning in language models. As a result, REMIND offers a novel perspective on memorization and unlearning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Twirlator: A Pipeline for Analyzing Subgroup Symmetry Effects in Quantum Machine Learning Ansatzes</title>
<link>https://arxiv.org/abs/2511.04243</link>
<guid>https://arxiv.org/abs/2511.04243</guid>
<content:encoded><![CDATA[
arXiv:2511.04243v1 Announce Type: cross 
Abstract: Leveraging data symmetries has been a key driver of performance gains in geometric deep learning and geometric and equivariant quantum machine learning. While symmetrization appears to be a promising method, its practical overhead, such as additional gates, reduced expressibility, and other factors, is not well understood in quantum machine learning. In this work, we develop an automated pipeline to measure various characteristics of quantum machine learning ansatzes with respect to symmetries that can appear in the learning task. We define the degree of symmetry in the learning problem as the size of the subgroup it admits. Subgroups define partial symmetries, which have not been extensively studied in previous research, which has focused on symmetries defined by whole groups. Symmetrizing the 19 common ansatzes with respect to these varying-sized subgroup representations, we compute three classes of metrics that describe how the common ansatz structures behave under varying amounts of symmetries. The first metric is based on the norm of the difference between the original and symmetrized generators, while the second metric counts depth, size, and other characteristics from the symmetrized circuits. The third class of metrics includes expressibility and entangling capability. The results demonstrate varying gate overhead across the studied ansatzes and confirm that increased symmetry reduces expressibility of the circuits. In most cases, increased symmetry increases entanglement capability. These results help select sufficiently expressible and computationally efficient ansatze patterns for geometric quantum machine learning applications.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection</title>
<link>https://arxiv.org/abs/2511.04255</link>
<guid>https://arxiv.org/abs/2511.04255</guid>
<content:encoded><![CDATA[
arXiv:2511.04255v1 Announce Type: cross 
Abstract: This paper does not introduce a novel architecture; instead, it revisits a fundamental yet overlooked baseline: adapting human-centric foundation models for anatomical landmark detection in medical imaging. While landmark detection has traditionally relied on domain-specific models, the emergence of large-scale pre-trained vision models presents new opportunities. In this study, we investigate the adaptation of Sapiens, a human-centric foundation model designed for pose estimation, to medical imaging through multi-dataset pretraining, establishing a new state of the art across multiple datasets. Our proposed model, MedSapiens, demonstrates that human-centric foundation models, inherently optimized for spatial pose localization, provide strong priors for anatomical landmark detection, yet this potential has remained largely untapped. We benchmark MedSapiens against existing state-of-the-art models, achieving up to 5.26% improvement over generalist models and up to 21.81% improvement over specialist models in the average success detection rate (SDR). To further assess MedSapiens adaptability to novel downstream tasks with few annotations, we evaluate its performance in limited-data settings, achieving 2.69% improvement over the few-shot state of the art in SDR. Code and model weights are available at https://github.com/xmed-lab/MedSapiens .
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Conformal Inference with Retrospective Adjustment for Faster Adaptation to Distribution Shift</title>
<link>https://arxiv.org/abs/2511.04275</link>
<guid>https://arxiv.org/abs/2511.04275</guid>
<content:encoded><![CDATA[
arXiv:2511.04275v1 Announce Type: cross 
Abstract: Conformal prediction has emerged as a powerful framework for constructing distribution-free prediction sets with guaranteed coverage assuming only the exchangeability assumption. However, this assumption is often violated in online environments where data distributions evolve over time. Several recent approaches have been proposed to address this limitation, but, typically, they slowly adapt to distribution shifts because they update predictions only in a forward manner, that is, they generate a prediction for a newly observed data point while previously computed predictions are not updated. In this paper, we propose a novel online conformal inference method with retrospective adjustment, which is designed to achieve faster adaptation to distributional shifts. Our method leverages regression approaches with efficient leave-one-out update formulas to retroactively adjust past predictions when new data arrive, thereby aligning the entire set of predictions with the most recent data distribution. Through extensive numerical studies performed on both synthetic and real-world data sets, we show that the proposed approach achieves faster coverage recalibration and improved statistical efficiency compared to existing online conformal prediction methods.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness of Minimum-Volume Nonnegative Matrix Factorization under an Expanded Sufficiently Scattered Condition</title>
<link>https://arxiv.org/abs/2511.04291</link>
<guid>https://arxiv.org/abs/2511.04291</guid>
<content:encoded><![CDATA[
arXiv:2511.04291v1 Announce Type: cross 
Abstract: Minimum-volume nonnegative matrix factorization (min-vol NMF) has been used successfully in many applications, such as hyperspectral imaging, chemical kinetics, spectroscopy, topic modeling, and audio source separation. However, its robustness to noise has been a long-standing open problem. In this paper, we prove that min-vol NMF identifies the groundtruth factors in the presence of noise under a condition referred to as the expanded sufficiently scattered condition which requires the data points to be sufficiently well scattered in the latent simplex generated by the basis vectors.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepPAAC: A New Deep Galerkin Method for Principal-Agent Problems</title>
<link>https://arxiv.org/abs/2511.04309</link>
<guid>https://arxiv.org/abs/2511.04309</guid>
<content:encoded><![CDATA[
arXiv:2511.04309v1 Announce Type: cross 
Abstract: We consider numerical resolution of principal-agent (PA) problems in continuous time. We formulate a generic PA model with continuous and lump payments and a multi-dimensional strategy of the agent. To tackle the resulting Hamilton-Jacobi-Bellman equation with an implicit Hamiltonian we develop a novel deep learning method: the Deep Principal-Agent Actor Critic (DeepPAAC) Actor-Critic algorithm. DeepPAAC is able to handle multi-dimensional states and controls, as well as constraints. We investigate the role of the neural network architecture, training designs, loss functions, etc. on the convergence of the solver, presenting five different case studies.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM</title>
<link>https://arxiv.org/abs/2511.04321</link>
<guid>https://arxiv.org/abs/2511.04321</guid>
<content:encoded><![CDATA[
arXiv:2511.04321v1 Announce Type: cross 
Abstract: SRAM Processing-in-Memory (PIM) has emerged as the most promising implementation for high-performance PIM, delivering superior computing density, energy efficiency, and computational precision. However, the pursuit of higher performance necessitates more complex circuit designs and increased operating frequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly degrade chip performance and even threaten reliability. Conventional circuit-level IR-drop mitigation methods, such as back-end optimizations, are resource-intensive and often compromise power, performance, and area (PPA). To address these challenges, we propose AIM, comprehensive software and hardware co-design for architecture-level IR-drop mitigation in high-performance PIM. Initially, leveraging the bit-serial and in-situ dataflow processing properties of PIM, we introduce Rtog and HR, which establish a direct correlation between PIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS, enabling extensive exploration of architecture-level IR-drop mitigation while maintaining computational accuracy through software optimization. Subsequently, we develop IR-Booster, a dynamic adjustment mechanism that integrates software-level HR information with hardware-based IR-drop monitoring to adapt the V-f pairs of the PIM macro, achieving enhanced energy efficiency and performance. Finally, we propose the HR-aware task mapping method, bridging software and hardware designs to achieve optimal improvement. Post-layout simulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up to 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement and 1.152x speedup.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography</title>
<link>https://arxiv.org/abs/2511.04334</link>
<guid>https://arxiv.org/abs/2511.04334</guid>
<content:encoded><![CDATA[
arXiv:2511.04334v1 Announce Type: cross 
Abstract: The accurate delineation of tumours in radiological images like Computed Tomography is a very specialised and time-consuming task, and currently a bottleneck preventing quantitative analyses to be performed routinely in the clinical setting. For this reason, developing methods for the automated segmentation of tumours in medical imaging is of the utmost importance and has driven significant efforts in recent years. However, challenges regarding the impracticality of 3D scans, given the large amount of voxels to be analysed, usually requires the downsampling of such images or using patches thereof when applying traditional convolutional neural networks. To overcome this problem, in this paper we propose a new methodology that uses, divided into two stages, voxel sparsification and submanifold sparse convolutional networks. This method allows segmentations to be performed with high-resolution inputs and a native 3D model architecture, obtaining state-of-the-art accuracies while significantly reducing the computational resources needed in terms of GPU memory and time. We studied the deployment of this methodology in the context of Computed Tomography images of renal cancer patients from the KiTS23 challenge, and our method achieved results competitive with the challenge winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7% for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also offers significant computational improvements, achieving up to a 60% reduction in inference time and up to a 75\% reduction in VRAM usage compared to an equivalent dense architecture, across both CPU and various GPU cards tested.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks</title>
<link>https://arxiv.org/abs/2511.04355</link>
<guid>https://arxiv.org/abs/2511.04355</guid>
<content:encoded><![CDATA[
arXiv:2511.04355v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success in code generation, and the race to improve their performance has become a central focus of AI research. Benchmarks and leaderboards are increasingly popular, offering quantitative rankings of LLMs. However, they provide limited insight into the tasks that LLMs consistently fail to solve - information that is crucial for understanding current limitations and guiding the development of more capable models. To address this gap, we examined code generation tasks across four popular benchmarks, identifying those that major LLMs are most likely to fail. To understand the causes of these failures, we investigated whether the static complexity of solution code contributes to them, followed by a systematic inspection of 114 tasks that LLMs consistently struggled with. Our analysis revealed four recurring patterns of weaknesses in LLMs, as well as common complications within benchmark tasks that most often lead to failure.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models</title>
<link>https://arxiv.org/abs/2511.04361</link>
<guid>https://arxiv.org/abs/2511.04361</guid>
<content:encoded><![CDATA[
arXiv:2511.04361v1 Announce Type: cross 
Abstract: Energy markets exhibit complex causal relationships between weather patterns, generation technologies, and price formation, with regime changes occurring continuously rather than at discrete break points. Current approaches model electricity prices without explicit causal interpretation or counterfactual reasoning capabilities. We introduce Augmented Time Series Causal Models (ATSCM) for energy markets, extending counterfactual reasoning frameworks to multivariate temporal data with learned causal structure. Our approach models energy systems through interpretable factors (weather, generation mix, demand patterns), rich grid dynamics, and observable market variables. We integrate neural causal discovery to learn time-varying causal graphs without requiring ground truth DAGs. Applied to real-world electricity price data, ATSCM enables novel counterfactual queries such as "What would prices be under different renewable generation scenarios?".
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion Transformers</title>
<link>https://arxiv.org/abs/2511.04376</link>
<guid>https://arxiv.org/abs/2511.04376</guid>
<content:encoded><![CDATA[
arXiv:2511.04376v1 Announce Type: cross 
Abstract: Music editing has emerged as an important and practical area of artificial intelligence, with applications ranging from video game and film music production to personalizing existing tracks according to user preferences. However, existing models face significant limitations, such as being restricted to editing synthesized music generated by their own models, requiring highly precise prompts, or necessitating task-specific retraining, thus lacking true zero-shot capability. Leveraging recent advances in rectified flow and diffusion transformers, we introduce MusRec, the first zero-shot text-to-music editing model capable of performing diverse editing tasks on real-world music efficiently and effectively. Experimental results demonstrate that our approach outperforms existing methods in preserving musical content, structural consistency, and editing fidelity, establishing a strong foundation for controllable music editing in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA</title>
<link>https://arxiv.org/abs/2511.04384</link>
<guid>https://arxiv.org/abs/2511.04384</guid>
<content:encoded><![CDATA[
arXiv:2511.04384v1 Announce Type: cross 
Abstract: We present a multi-task framework for the MediaEval Medico 2025 challenge, leveraging a LoRA-tuned Florence-2 model for simultaneous visual question answering (VQA), explanation generation, and visual grounding. The proposed system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer learning, (2) a synthetically enriched explanation dataset offering structured medical reasoning, and (3) text-to-region pairs linking visual features with segmentation masks. This multi-task setup enables the model to jointly learn visual grounding, reasoning, and interpretation, producing responses that are both accurate and interpretable. Extensive evaluation demonstrates that our approach substantially improves over single-task baselines in both answer accuracy and visual localization, highlighting the effectiveness of grounded multi-task learning for medical VQA applications.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Bayesian Experimental Design for Partially Observed Dynamical Systems</title>
<link>https://arxiv.org/abs/2511.04403</link>
<guid>https://arxiv.org/abs/2511.04403</guid>
<content:encoded><![CDATA[
arXiv:2511.04403v1 Announce Type: cross 
Abstract: Bayesian experimental design (BED) provides a principled framework for optimizing data collection, but existing approaches do not apply to crucial real-world settings such as dynamical systems with partial observability, where only noisy and incomplete observations are available. These systems are naturally modeled as state-space models (SSMs), where latent states mediate the link between parameters and data, making the likelihood -- and thus information-theoretic objectives like the expected information gain (EIG) -- intractable. In addition, the dynamical nature of the system requires online algorithms that update posterior distributions and select designs sequentially in a computationally efficient manner. We address these challenges by deriving new estimators of the EIG and its gradient that explicitly marginalize latent states, enabling scalable stochastic optimization in nonlinear SSMs. Our approach leverages nested particle filters (NPFs) for efficient online inference with convergence guarantees. Applications to realistic models, such as the susceptible-infected-recovered (SIR) and a moving source location task, show that our framework successfully handles both partial observability and online computation.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Koopman Economic Model Predictive Control of a Pasteurisation Unit</title>
<link>https://arxiv.org/abs/2511.04437</link>
<guid>https://arxiv.org/abs/2511.04437</guid>
<content:encoded><![CDATA[
arXiv:2511.04437v1 Announce Type: cross 
Abstract: This paper presents a deep Koopman-based Economic Model Predictive Control (EMPC) for efficient operation of a laboratory-scale pasteurization unit (PU). The method uses Koopman operator theory to transform the complex, nonlinear system dynamics into a linear representation, enabling the application of convex optimization while representing the complex PU accurately. The deep Koopman model utilizes neural networks to learn the linear dynamics from experimental data, achieving a 45% improvement in open-loop prediction accuracy over conventional N4SID subspace identification. Both analyzed models were employed in the EMPC formulation that includes interpretable economic costs, such as energy consumption, material losses due to inadequate pasteurization, and actuator wear. The feasibility of EMPC is ensured using slack variables. The deep Koopman EMPC and N4SID EMPC are numerically validated on a nonlinear model of multivariable PU under external disturbance. The disturbances include feed pump fail-to-close scenario and the introduction of a cold batch to be pastuerized. These results demonstrate that the deep Koopmand EMPC achieves a 32% reduction in total economic cost compared to the N4SID baseline. This improvement is mainly due to the reductions in material losses and energy consumption. Furthermore, the steady-state operation via Koopman-based EMPC requires 10.2% less electrical energy. The results highlight the practical advantages of integrating deep Koopman representations with economic optimization to achieve resource-efficient control of thermal-intensive plants.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Peril of Preference: Why GRPO fails on Ordinal Rewards</title>
<link>https://arxiv.org/abs/2511.04439</link>
<guid>https://arxiv.org/abs/2511.04439</guid>
<content:encoded><![CDATA[
arXiv:2511.04439v1 Announce Type: cross 
Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly desirable for adapting LLMs to become experts at specific tasks. But this simplicity also makes it ill-specified as we seek to enhance RL training with richer, non-binary feedback. When using ordinal rewards to give partial credit, GRPO's simplicity starts to hurt, as its group-average baseline often assigns a positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new formulation that solves this flaw. CoRPO uses an adaptive baseline that enforces a minimum quality threshold, ensuring failed solutions are never positively reinforced. Once the policy consistently meets this threshold, the baseline automatically transitions to a relative preference mode, pushing the model to find optimal solutions rather than just "acceptable" ones. We empirically validate CoRPO on a code verification task, where it demonstrates more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to enable LLMs to learn genuinely new capabilities through reinforcement learning. We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback - progressing from binary to ordinal rewards in this work, and onward to denser, per-step supervision.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Dictionary-Free Method for Identifying Linear Model of Nonlinear System with Input Delay</title>
<link>https://arxiv.org/abs/2511.04451</link>
<guid>https://arxiv.org/abs/2511.04451</guid>
<content:encoded><![CDATA[
arXiv:2511.04451v1 Announce Type: cross 
Abstract: Nonlinear dynamical systems with input delays pose significant challenges for prediction, estimation, and control due to their inherent complexity and the impact of delays on system behavior. Traditional linear control techniques often fail in these contexts, necessitating innovative approaches. This paper introduces a novel approach to approximate the Koopman operator using an LSTM-enhanced Deep Koopman model, enabling linear representations of nonlinear systems with time delays. By incorporating Long Short-Term Memory (LSTM) layers, the proposed framework captures historical dependencies and efficiently encodes time-delayed system dynamics into a latent space. Unlike traditional extended Dynamic Mode Decomposition (eDMD) approaches that rely on predefined dictionaries, the LSTM-enhanced Deep Koopman model is dictionary-free, which mitigates the problems with the underlying dynamics being known and incorporated into the dictionary. Quantitative comparisons with extended eDMD on a simulated system demonstrate highly significant performance gains in prediction accuracy in cases where the true nonlinear dynamics are unknown and achieve comparable results to eDMD with known dynamics of a system.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fitting Reinforcement Learning Model to Behavioral Data under Bandits</title>
<link>https://arxiv.org/abs/2511.04454</link>
<guid>https://arxiv.org/abs/2511.04454</guid>
<content:encoded><![CDATA[
arXiv:2511.04454v1 Announce Type: cross 
Abstract: We consider the problem of fitting a reinforcement learning (RL) model to some given behavioral data under a multi-armed bandit environment. These models have received much attention in recent years for characterizing human and animal decision making behavior. We provide a generic mathematical optimization problem formulation for the fitting problem of a wide range of RL models that appear frequently in scientific research applications, followed by a detailed theoretical analysis of its convexity properties. Based on the theoretical results, we introduce a novel solution method for the fitting problem of RL models based on convex relaxation and optimization. Our method is then evaluated in several simulated bandit environments to compare with some benchmark methods that appear in the literature. Numerical results indicate that our method achieves comparable performance to the state-of-the-art, while significantly reducing computation time. We also provide an open-source Python package for our proposed method to empower researchers to apply it in the analysis of their datasets directly, without prior knowledge of convex optimization.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven uncertainty-aware seakeeping prediction of the Delft 372 catamaran using ensemble Hankel dynamic mode decomposition</title>
<link>https://arxiv.org/abs/2511.04461</link>
<guid>https://arxiv.org/abs/2511.04461</guid>
<content:encoded><![CDATA[
arXiv:2511.04461v1 Announce Type: cross 
Abstract: In this study, we present and validate an ensemble-based Hankel Dynamic Mode Decomposition with control (HDMDc) for uncertainty-aware seakeeping predictions of a high-speed catamaran, namely the Delft 372 model. Experimental measurements (time histories) of wave elevation at the longitudinal center of gravity, heave, pitch, notional flight-deck velocity, notional bridge acceleration, and total resistance were collected from irregular wave basin tests on a 1:33.3 scale replica of the Delft 372 model under sea state 5 conditions at Fr = 0.425, and organized into training, validation, and test sets. The HDMDc algorithm constructs an equation-free linear reduced-order model of the seakeeping vessel by augmenting states and inputs with their time-lagged copies to capture nonlinear and memory effects. Two ensembling strategies, namely Bayesian HDMDc (BHDMDc), which samples hyperparameters considered stochastic variables with prior distribution to produce posterior mean forecasts with confidence intervals, and Frequentist HDMDc (FHDMDc), which aggregates multiple model obtained over data subsets, are compared in providing seakeeping prediction and uncertainty quantification. The FHDMDc approach is found to improve the accuracy of the predictions compared to the deterministic counterpart, also providing robust uncertainty estimation; whereas the application of BHDMDc to the present test case is not found beneficial in comparison to the deterministic model. FHDMDc-derived probability density functions for the motions closely match both experimental data and URANS results, demonstrating reliable and computationally efficient seakeeping prediction for design and operational support.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fraud-Proof Revenue Division on Subscription Platforms</title>
<link>https://arxiv.org/abs/2511.04465</link>
<guid>https://arxiv.org/abs/2511.04465</guid>
<content:encoded><![CDATA[
arXiv:2511.04465v1 Announce Type: cross 
Abstract: We study a model of subscription-based platforms where users pay a fixed fee for unlimited access to content, and creators receive a share of the revenue. Existing approaches to detecting fraud predominantly rely on machine learning methods, engaging in an ongoing arms race with bad actors. We explore revenue division mechanisms that inherently disincentivize manipulation. We formalize three types of manipulation-resistance axioms and examine which existing rules satisfy these. We show that a mechanism widely used by streaming platforms, not only fails to prevent fraud, but also makes detecting manipulation computationally intractable. We also introduce a novel rule, ScaledUserProp, that satisfies all three manipulation-resistance axioms. Finally, experiments with both real-world and synthetic streaming data support ScaledUserProp as a fairer alternative compared to existing rules.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Algorithms for Repeated Optimal Stopping: Achieving Both Competitive Ratio and Regret Bounds</title>
<link>https://arxiv.org/abs/2511.04484</link>
<guid>https://arxiv.org/abs/2511.04484</guid>
<content:encoded><![CDATA[
arXiv:2511.04484v1 Announce Type: cross 
Abstract: We study the repeated optimal stopping problem, which generalizes the classical optimal stopping problem with an unknown distribution to a setting where the same problem is solved repeatedly over $T$ rounds. In this framework, we aim to design algorithms that guarantee a competitive ratio in each round while also achieving sublinear regret across all rounds.
  Our primary contribution is a general algorithmic framework that achieves these objectives simultaneously for a wide array of repeated optimal stopping problems. The core idea is to dynamically select an algorithm for each round, choosing between two candidates: (1) an empirically optimal algorithm derived from the history of observations, and (2) a sample-based algorithm with a proven competitive ratio guarantee. Based on this approach, we design an algorithm that performs no worse than the baseline sample-based algorithm in every round, while ensuring that the total regret is bounded by $\tilde{O}(\sqrt{T})$.
  We demonstrate the broad applicability of our framework to canonical problems, including the prophet inequality, the secretary problem, and their variants under adversarial, random, and i.i.d. input models. For example, for the repeated prophet inequality problem, our method achieves a $1/2$-competitive ratio from the second round on and an $\tilde{O}(\sqrt{T})$ regret. Furthermore, we establish a regret lower bound of $\Omega(\sqrt{T})$ even in the i.i.d. model, confirming that our algorithm's performance is almost optimal with respect to the number of rounds.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables</title>
<link>https://arxiv.org/abs/2511.04491</link>
<guid>https://arxiv.org/abs/2511.04491</guid>
<content:encoded><![CDATA[
arXiv:2511.04491v1 Announce Type: cross 
Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform tables, underrepresenting the complexity of real-world data and giving an incomplete view of Large Language Models' (LLMs) reasoning abilities. Real tables are long, heterogeneous, and domain-specific, mixing structured fields with free text and requiring multi-hop reasoning across thousands of tokens. To address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from 2031 real-world tables spanning two domains: i) RB-Science (NSF grant records) and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates LLMs jointly across scale, heterogeneity, domain specificity, and reasoning complexity. Experiments with open-source and proprietary models show that LLMs struggle with heterogeneous schemas and complex multi-hop inference, revealing persistent weaknesses in current architectures and prompting strategies. RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Generative Latent Representation for Functional Brain Graphs</title>
<link>https://arxiv.org/abs/2511.04539</link>
<guid>https://arxiv.org/abs/2511.04539</guid>
<content:encoded><![CDATA[
arXiv:2511.04539v1 Announce Type: cross 
Abstract: Functional brain graphs are often characterized with separate graph-theoretic or spectral descriptors, overlooking how these properties covary and partially overlap across brains and conditions. We anticipate that dense, weighted functional connectivity graphs occupy a low-dimensional latent geometry along which both topological and spectral structures display graded variations. Here, we estimated this unified graph representation and enabled generation of dense functional brain graphs through a graph transformer autoencoder with latent diffusion, with spectral geometry providing an inductive bias to guide learning. This geometry-aware latent representation, although unsupervised, meaningfully separated working-memory states and decoded visual stimuli, with performance further enhanced by incorporating neural dynamics. From the diffusion modeled distribution, we were able to sample biologically plausible and structurally grounded synthetic dense graphs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confidential Computing for Cloud Security: Exploring Hardware based Encryption Using Trusted Execution Environments</title>
<link>https://arxiv.org/abs/2511.04550</link>
<guid>https://arxiv.org/abs/2511.04550</guid>
<content:encoded><![CDATA[
arXiv:2511.04550v1 Announce Type: cross 
Abstract: The growth of cloud computing has revolutionized data processing and storage capacities to another levels of scalability and flexibility. But in the process, it has created a huge challenge of security, especially in terms of safeguarding sensitive data. Classical security practices, including encryption at rest and during transit, fail to protect data in use and expose it to various possible breaches. In response to this problem , Confidential Computing has been a tool ,seeking to secure data in processing by usage of hardware-based Trusted Execution Environments (TEEs). TEEs, including Intel's Software Guard Extensions (SGX) and ARM's TrustZone, offers protected contexts within the processor, where data is kept confidential ,intact and secure , even with malicious software or compromised operating systems. In this research, we have explored the architecture and security features of TEEs like Intel SGX and ARM TrustZone, and their effectiveness in improving cloud data security. From a thorough literature survey ,we have analyzed the deployment strategies, performance indicators, and practical uses of these TEEs for the same purpose. In addition, we have discussed the issues regarding deployment, possible weaknesses, scalability issues, and integration issues. Our results focuses on the central position of TEEs in strengthening and advancing cloud security infrastructures, pointing towards their ability to create a secure foundation for Confidential Computing.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI</title>
<link>https://arxiv.org/abs/2511.04564</link>
<guid>https://arxiv.org/abs/2511.04564</guid>
<content:encoded><![CDATA[
arXiv:2511.04564v1 Announce Type: cross 
Abstract: Physics-informed machine learning (PIML) integrates partial differential equations (PDEs) into machine learning models to solve inverse problems, such as estimating coefficient functions (e.g., the Hamiltonian function) that characterize physical systems. This framework enables data-driven understanding and prediction of complex physical phenomena. While coefficient functions in PIML are typically estimated on the basis of predictive performance, physics as a discipline does not rely solely on prediction accuracy to evaluate models. For example, Kepler's heliocentric model was favored owing to small discrepancies in planetary motion, despite its similar predictive accuracy to the geocentric model. This highlights the inherent uncertainties in data-driven model inference and the scientific importance of selecting physically meaningful solutions. In this paper, we propose a framework to quantify and analyze such uncertainties in the estimation of coefficient functions in PIML. We apply our framework to reduced model of magnetohydrodynamics and our framework shows that there are uncertainties, and unique identification is possible with geometric constraints. Finally, we confirm that we can estimate the reduced model uniquely by incorporating these constraints.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning for Electron-Scale Turbulence Modeling in W7-X</title>
<link>https://arxiv.org/abs/2511.04567</link>
<guid>https://arxiv.org/abs/2511.04567</guid>
<content:encoded><![CDATA[
arXiv:2511.04567v1 Announce Type: cross 
Abstract: Constructing reduced models for turbulent transport is essential for accelerating profile predictions and enabling many-query tasks such as uncertainty quantification, parameter scans, and design optimization. This paper presents machine-learning-driven reduced models for Electron Temperature Gradient (ETG) turbulence in the Wendelstein 7-X (W7-X) stellarator. Each model predicts the ETG heat flux as a function of three plasma parameters: the normalized electron temperature radial gradient ($\omega_{T_e}$), the ratio of normalized electron temperature and density radial gradients ($\eta_e$), and the electron-to-ion temperature ratio ($\tau$). We first construct models across seven radial locations using regression and an active machine-learning-based procedure. This process initializes models using low-cardinality sparse-grid training data and then iteratively refines their training sets by selecting the most informative points from a pre-existing simulation database. We evaluate the prediction capabilities of our models using out-of-sample datasets with over $393$ points per location, and $95\%$ prediction intervals are estimated via bootstrapping to assess prediction uncertainty. We then investigate the construction of generalized reduced models, including a generic, position-independent model, and assess their heat flux prediction capabilities at three additional locations. Our models demonstrate robust performance and predictive accuracy comparable to the original reference simulations, even when applied beyond the training domain.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Riesz Regression As Direct Density Ratio Estimation</title>
<link>https://arxiv.org/abs/2511.04568</link>
<guid>https://arxiv.org/abs/2511.04568</guid>
<content:encoded><![CDATA[
arXiv:2511.04568v1 Announce Type: cross 
Abstract: Riesz regression has garnered attention as a tool in debiased machine learning for causal and structural parameter estimation (Chernozhukov et al., 2021). This study shows that Riesz regression is closely related to direct density-ratio estimation (DRE) in important cases, including average treat- ment effect (ATE) estimation. Specifically, the idea and objective in Riesz regression coincide with the one in least-squares importance fitting (LSIF, Kanamori et al., 2009) in direct density-ratio estimation. While Riesz regression is general in the sense that it can be applied to Riesz representer estimation in a wide class of problems, the equivalence with DRE allows us to directly import exist- ing results in specific cases, including convergence-rate analyses, the selection of loss functions via Bregman-divergence minimization, and regularization techniques for flexible models, such as neural networks. Conversely, insights about the Riesz representer in debiased machine learning broaden the applications of direct density-ratio estimation methods. This paper consolidates our prior results in Kato (2025a) and Kato (2025b).
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks and Neural Operators for Parametric PDEs: A Human-AI Collaborative Analysis</title>
<link>https://arxiv.org/abs/2511.04576</link>
<guid>https://arxiv.org/abs/2511.04576</guid>
<content:encoded><![CDATA[
arXiv:2511.04576v1 Announce Type: cross 
Abstract: PDEs arise ubiquitously in science and engineering, where solutions depend on parameters (physical properties, boundary conditions, geometry). Traditional numerical methods require re-solving the PDE for each parameter, making parameter space exploration prohibitively expensive. Recent machine learning advances, particularly physics-informed neural networks (PINNs) and neural operators, have revolutionized parametric PDE solving by learning solution operators that generalize across parameter spaces. We critically analyze two main paradigms: (1) PINNs, which embed physical laws as soft constraints and excel at inverse problems with sparse data, and (2) neural operators (e.g., DeepONet, Fourier Neural Operator), which learn mappings between infinite-dimensional function spaces and achieve unprecedented generalization. Through comparisons across fluid dynamics, solid mechanics, heat transfer, and electromagnetics, we show neural operators can achieve computational speedups of $10^3$ to $10^5$ times faster than traditional solvers for multi-query scenarios, while maintaining comparable accuracy. We provide practical guidance for method selection, discuss theoretical foundations (universal approximation, convergence), and identify critical open challenges: high-dimensional parameters, complex geometries, and out-of-distribution generalization. This work establishes a unified framework for understanding parametric PDE solvers via operator learning, offering a comprehensive, incrementally updated resource for this rapidly evolving field
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper</title>
<link>https://arxiv.org/abs/2511.04583</link>
<guid>https://arxiv.org/abs/2511.04583</guid>
<content:encoded><![CDATA[
arXiv:2511.04583v1 Announce Type: cross 
Abstract: Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>evomap: A Toolbox for Dynamic Mapping in Python</title>
<link>https://arxiv.org/abs/2511.04611</link>
<guid>https://arxiv.org/abs/2511.04611</guid>
<content:encoded><![CDATA[
arXiv:2511.04611v1 Announce Type: cross 
Abstract: This paper presents evomap, a Python package for dynamic mapping. Mapping methods are widely used across disciplines to visualize relationships among objects as spatial representations, or maps. However, most existing statistical software supports only static mapping, which captures objects' relationships at a single point in time and lacks tools to analyze how these relationships evolve. evomap fills this gap by implementing the dynamic mapping framework EvoMap, originally proposed by Matthe, Ringel, and Skiera (2023), which adapts traditional static mapping methods for dynamic analyses. The package supports multiple mapping techniques, including variants of Multidimensional Scaling (MDS), Sammon Mapping, and t-distributed Stochastic Neighbor Embedding (t-SNE). It also includes utilities for data preprocessing, exploration, and result evaluation, offering a comprehensive toolkit for dynamic mapping applications. This paper outlines the foundations of static and dynamic mapping, describes the architecture and functionality of evomap, and illustrates its application through an extensive usage example.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic causal discovery in Alzheimer's disease through latent pseudotime modelling</title>
<link>https://arxiv.org/abs/2511.04619</link>
<guid>https://arxiv.org/abs/2511.04619</guid>
<content:encoded><![CDATA[
arXiv:2511.04619v1 Announce Type: cross 
Abstract: The application of causal discovery to diseases like Alzheimer's (AD) is limited by the static graph assumptions of most methods; such models cannot account for an evolving pathophysiology, modulated by a latent disease pseudotime. We propose to apply an existing latent variable model to real-world AD data, inferring a pseudotime that orders patients along a data-driven disease trajectory independent of chronological age, then learning how causal relationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC 0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledge substantially improved graph accuracy and orientation. Our framework reveals dynamic interactions between novel (NfL, GFAP) and established AD markers, enabling practical causal discovery despite violated assumptions.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ODE approximation for the Adam algorithm: General and overparametrized setting</title>
<link>https://arxiv.org/abs/2511.04622</link>
<guid>https://arxiv.org/abs/2511.04622</guid>
<content:encoded><![CDATA[
arXiv:2511.04622v1 Announce Type: cross 
Abstract: The Adam optimizer is currently presumably the most popular optimization method in deep learning. In this article we develop an ODE based method to study the Adam optimizer in a fast-slow scaling regime. For fixed momentum parameters and vanishing step-sizes, we show that the Adam algorithm is an asymptotic pseudo-trajectory of the flow of a particular vector field, which is referred to as the Adam vector field. Leveraging properties of asymptotic pseudo-trajectories, we establish convergence results for the Adam algorithm. In particular, in a very general setting we show that if the Adam algorithm converges, then the limit must be a zero of the Adam vector field, rather than a local minimizer or critical point of the objective function.
  In contrast, in the overparametrized empirical risk minimization setting, the Adam algorithm is able to locally find the set of minima. Specifically, we show that in a neighborhood of the global minima, the objective function serves as a Lyapunov function for the flow induced by the Adam vector field. As a consequence, if the Adam algorithm enters a neighborhood of the global minima infinitely often, it converges to the set of global minima.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2511.04646</link>
<guid>https://arxiv.org/abs/2511.04646</guid>
<content:encoded><![CDATA[
arXiv:2511.04646v1 Announce Type: cross 
Abstract: Cooperative multi-agent planning requires agents to make joint decisions with partial information and limited communication. Coordination at the trajectory level often fails, as small deviations in timing or movement cascade into conflicts. Symbolic planning mitigates this challenge by raising the level of abstraction and providing a minimal vocabulary of actions that enable synchronization and collective progress. We present DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. Cooperation unfolds through a two-phase negotiation protocol: agents first propose candidate roles with reasoning and then commit to a joint allocation under consensus and environment constraints. After commitment, each agent independently generates and executes a symbolic plan for its role without revealing detailed trajectories. Plans are grounded in execution outcomes via a shared world model that encodes the current state and is updated as agents act. By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids brittle step-level alignment and enables higher-level operations that are reusable, synchronizable, and interpretable. Experiments on cooperative block-push tasks show that agents adapt across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency. Experiments on cooperative block-push tasks show that our dynamic world model improves task completion and efficiency through negotiation and self-refinement, trading a time overhead for evolving, more efficient collaboration strategies.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions</title>
<link>https://arxiv.org/abs/2511.04665</link>
<guid>https://arxiv.org/abs/2511.04665</guid>
<content:encoded><![CDATA[
arXiv:2511.04665v1 Announce Type: cross 
Abstract: Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: https://real2sim-eval.github.io/
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dark Energy Survey Year 3 results: Simulation-based $w$CDM inference from weak lensing and galaxy clustering maps with deep learning. I. Analysis design</title>
<link>https://arxiv.org/abs/2511.04681</link>
<guid>https://arxiv.org/abs/2511.04681</guid>
<content:encoded><![CDATA[
arXiv:2511.04681v1 Announce Type: cross 
Abstract: Data-driven approaches using deep learning are emerging as powerful techniques to extract non-Gaussian information from cosmological large-scale structure. This work presents the first simulation-based inference (SBI) pipeline that combines weak lensing and galaxy clustering maps in a realistic Dark Energy Survey Year 3 (DES Y3) configuration and serves as preparation for a forthcoming analysis of the survey data. We develop a scalable forward model based on the CosmoGridV1 suite of N-body simulations to generate over one million self-consistent mock realizations of DES Y3 at the map level. Leveraging this large dataset, we train deep graph convolutional neural networks on the full survey footprint in spherical geometry to learn low-dimensional features that approximately maximize mutual information with target parameters. These learned compressions enable neural density estimation of the implicit likelihood via normalizing flows in a ten-dimensional parameter space spanning cosmological $w$CDM, intrinsic alignment, and linear galaxy bias parameters, while marginalizing over baryonic, photometric redshift, and shear bias nuisances. To ensure robustness, we extensively validate our inference pipeline using synthetic observations derived from both systematic contaminations in our forward model and independent Buzzard galaxy catalogs. Our forecasts yield significant improvements in cosmological parameter constraints, achieving $2-3\times$ higher figures of merit in the $\Omega_m - S_8$ plane relative to our implementation of baseline two-point statistics and effectively breaking parameter degeneracies through probe combination. These results demonstrate the potential of SBI analyses powered by deep learning for upcoming Stage-IV wide-field imaging surveys.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rater Equivalence: Evaluating Classifiers in Human Judgment Settings</title>
<link>https://arxiv.org/abs/2106.01254</link>
<guid>https://arxiv.org/abs/2106.01254</guid>
<content:encoded><![CDATA[
arXiv:2106.01254v2 Announce Type: replace 
Abstract: In many decision settings, the definitive ground truth is either non-existent or inaccessible. We introduce a framework for evaluating classifiers based solely on human judgments. In such cases, it is helpful to compare automated classifiers to human judgment. We quantify a classifier's performance by its rater equivalence: the smallest number of human raters whose combined judgment matches the classifier's performance. Our framework uses human-generated labels both to construct benchmark panels and to evaluate performance. We distinguish between two models of utility: one based on agreement with the assumed but inaccessible ground truth, and one based on matching individual human judgments. Using case studies and formal analysis, we demonstrate how this framework can inform the evaluation and deployment of AI systems in practice.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local Fragments, Global Gains: Subgraph Counting using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2305.19659</link>
<guid>https://arxiv.org/abs/2305.19659</guid>
<content:encoded><![CDATA[
arXiv:2305.19659v4 Announce Type: replace 
Abstract: Subgraph counting is a fundamental task for analyzing structural patterns in graph-structured data, with important applications in domains such as computational biology and social network analysis, where recurring motifs reveal functional and organizational properties. In this paper, we propose localized versions of the Weisfeiler-Leman (WL) algorithms to improve both expressivity and computational efficiency for this task. We introduce Local $k$-WL, which we prove to be more expressive than $k$-WL and at most as expressive as $(k+1)$-WL, and provide a characterization of patterns whose subgraph and induced subgraph counts are invariant under Local $k$-WL equivalence. To enhance scalability, we present two variants -- Layer $k$-WL and Recursive $k$-WL -- that achieve greater time and space efficiency compared to applying $k$-WL on the entire graph. Additionally, we propose a novel fragmentation technique that decomposes complex subgraphs into simpler subpatterns, enabling the exact count of all induced subgraphs of size at most $4$ using only $1$-WL, with extensions possible for larger patterns when $k>1$. Building on these ideas, we develop a three-stage differentiable learning framework that combines subpattern counts to compute counts of more complex motifs, bridging combinatorial algorithm design with machine learning approaches. We also compare the expressive power of Local $k$-WL with existing GNN hierarchies and demonstrate that, under bounded time complexity, our methods are more expressive than prior approaches.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Kernel for Neural Network Learning</title>
<link>https://arxiv.org/abs/2403.17467</link>
<guid>https://arxiv.org/abs/2403.17467</guid>
<content:encoded><![CDATA[
arXiv:2403.17467v2 Announce Type: replace 
Abstract: Past decades have witnessed a great interest in the distinction and connection between neural network learning and kernel learning. Recent advancements have made theoretical progress in connecting infinite-wide neural networks and Gaussian processes. Two predominant approaches have emerged: the Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The former, rooted in Bayesian inference, represents a zero-order kernel, while the latter, grounded in the tangent space of gradient descents, is a first-order kernel. In this paper, we present the Unified Neural Kernel (UNK), which {is induced by the inner product of produced variables and characterizes the learning dynamics of neural networks with gradient descents and parameter initialization.} The proposed UNK kernel maintains the limiting properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite learning step and converging to NNGP as the learning step approaches infinity. Besides, we also theoretically characterize the uniform tightness and learning convergence of the UNK kernel, providing comprehensive insights into this unified kernel. Experimental results underscore the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Diffusion: A Diffusion Probabilistic Model for Stochastic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2406.02827</link>
<guid>https://arxiv.org/abs/2406.02827</guid>
<content:encoded><![CDATA[
arXiv:2406.02827v3 Announce Type: replace 
Abstract: Recent innovations in diffusion probabilistic models have paved the way for significant progress in image, text and audio generation, leading to their applications in generative time series forecasting. However, leveraging such abilities to model highly stochastic time series data remains a challenge. In this paper, we propose a novel Stochastic Diffusion (StochDiff) model which learns data-driven prior knowledge at each time step by utilizing the representational power of the stochastic latent spaces to model the variability of the multivariate time series data. The learnt prior knowledge helps the model to capture complex temporal dynamics and the inherent uncertainty of the data. This improves its ability to model highly stochastic time series data. Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed model on stochastic time series forecasting. Additionally, we showcase an application of our model for real-world surgical guidance, highlighting its potential to benefit the medical community.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Graph Transformers Across Diverse Graphs and Tasks via Pre-training</title>
<link>https://arxiv.org/abs/2407.03953</link>
<guid>https://arxiv.org/abs/2407.03953</guid>
<content:encoded><![CDATA[
arXiv:2407.03953v4 Announce Type: replace 
Abstract: Graph pre-training has been concentrated on graph-level tasks involving small graphs (e.g., molecular graphs) or learning node representations on a fixed graph. Extending graph pre-trained models to web-scale graphs with billions of nodes in industrial scenarios, while avoiding negative transfer across graphs or tasks, remains a challenge. We aim to develop a general graph pre-trained model with inductive ability that can make predictions for unseen new nodes and even new graphs. In this work, we introduce a scalable transformer-based graph pre-training framework called PGT (Pre-trained Graph Transformer). Based on the masked autoencoder architecture, we design two pre-training tasks: one for reconstructing node features and the other for reconstructing local structures. Unlike the original autoencoder architecture where the pre-trained decoder is discarded, we propose a novel strategy that utilizes the decoder for feature augmentation. Our framework, tested on the publicly available ogbn-papers100M dataset with 111 million nodes and 1.6 billion edges, achieves state-of-the-art performance, showcasing scalability and efficiency. We have deployed our framework on Tencent's online game data, confirming its capability to pre-train on real-world graphs with over 540 million nodes and 12 billion edges and to generalize effectively across diverse static and dynamic downstream tasks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedQUIT: On-Device Federated Unlearning via a Quasi-Competent Virtual Teacher</title>
<link>https://arxiv.org/abs/2408.07587</link>
<guid>https://arxiv.org/abs/2408.07587</guid>
<content:encoded><![CDATA[
arXiv:2408.07587v3 Announce Type: replace 
Abstract: Federated Learning (FL) systems enable the collaborative training of machine learning models without requiring centralized collection of individual data. FL participants should have the ability to exercise their right to be forgotten, ensuring their past contributions can be removed from the learned model upon request. In this paper, we propose FedQUIT, a novel algorithm that uses knowledge distillation to scrub the contribution of the data to forget from an FL global model while preserving its generalization ability. FedQUIT directly works on client devices that request to leave the federation, and leverages a teacher-student framework. The FL global model acts as the teacher, and the local model works as the student. To induce forgetting, FedQUIT tailors the teacher's output on local data (the data to forget) penalizing the prediction score of the true class. Unlike previous work, our method does not require hardly viable assumptions for cross-device settings, such as storing historical updates of participants or requiring access to proxy datasets. Experimental results on various datasets and model architectures demonstrate that (i) FedQUIT outperforms state-of-the-art competitors in forgetting data, (ii) has the exact computational requirements as a regular FedAvg round, and (iii) reduces the cumulative communication costs by up to 117.6$\times$ compared to retraining from scratch to restore the initial generalization performance after unlearning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion &amp; Adversarial Schr\"odinger Bridges via Iterative Proportional Markovian Fitting</title>
<link>https://arxiv.org/abs/2410.02601</link>
<guid>https://arxiv.org/abs/2410.02601</guid>
<content:encoded><![CDATA[
arXiv:2410.02601v4 Announce Type: replace 
Abstract: The Iterative Markovian Fitting (IMF) procedure, which iteratively projects onto the space of Markov processes and the reciprocal class, successfully solves the Schr\"odinger Bridge (SB) problem. However, an efficient practical implementation requires a heuristic modification -- alternating between fitting forward and backward time diffusion at each iteration. This modification is crucial for stabilizing training and achieving reliable results in applications such as unpaired domain translation. Our work reveals a close connection between the modified version of IMF and the Iterative Proportional Fitting (IPF) procedure -- a foundational method for the SB problem, also known as Sinkhorn's algorithm. Specifically, we demonstrate that the heuristic modification of the IMF effectively integrates both IMF and IPF procedures. We refer to this combined approach as the Iterative Proportional Markovian Fitting (IPMF) procedure. Through theoretical and empirical analysis, we establish the convergence of the IPMF procedure under various settings, contributing to developing a unified framework for solving SB problems. Moreover, from a practical standpoint, the IPMF procedure enables a flexible trade-off between image similarity and generation quality, offering a new mechanism for tailoring models to specific tasks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Singular Values Matter: A Random Matrix Analysis of Transformer Models</title>
<link>https://arxiv.org/abs/2410.17770</link>
<guid>https://arxiv.org/abs/2410.17770</guid>
<content:encoded><![CDATA[
arXiv:2410.17770v3 Announce Type: replace 
Abstract: This work analyzes singular-value spectra of weight matrices in pretrained transformer models to understand how information is stored at both ends of the spectrum. Using Random Matrix Theory (RMT) as a zero information hypothesis, we associate agreement with RMT as evidence of randomness and deviations as evidence for learning. Surprisingly, we observe pronounced departures from RMT not only among the largest singular values -- the usual outliers -- but also among the smallest ones. A comparison of the associated singular vectors with the eigenvectors of the activation covariance matrices shows that there is considerable overlap wherever RMT is violated. Thus, significant directions in the data are captured by small singular values and their vectors as well as by the large ones. We confirm this empirically: zeroing out the singular values that deviate from RMT raises language-model perplexity far more than removing values from the bulk, and after fine-tuning the smallest decile can be the third most influential part of the spectrum. To explain how vectors linked to small singular values can carry more information than those linked to larger values, we propose a linear random-matrix model. Our findings highlight the overlooked importance of the low end of the spectrum and provide theoretical and practical guidance for SVD-based pruning and compression of large language models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Kolmogorov Barrier: A Learnable Weighted Hybrid Autoencoder for Model Order Reduction</title>
<link>https://arxiv.org/abs/2410.18148</link>
<guid>https://arxiv.org/abs/2410.18148</guid>
<content:encoded><![CDATA[
arXiv:2410.18148v5 Announce Type: replace 
Abstract: Representation learning for high-dimensional, complex physical systems aims to identify a low-dimensional intrinsic latent space, which is crucial for reduced-order modeling and modal analysis. To overcome the well-known Kolmogorov barrier, deep autoencoders (AEs) have been introduced in recent years, but they often suffer from poor convergence behavior as the rank of the latent space increases. To address this issue, we propose the learnable weighted hybrid autoencoder, a hybrid approach that combines the strengths of singular value decomposition (SVD) with deep autoencoders through a learnable weighted framework. We find that the introduction of learnable weighting parameters is essential -- without them, the resulting model would either collapse into a standard POD or fail to exhibit the desired convergence behavior. Interestingly, we empirically find that our trained model has a sharpness thousands of times smaller compared to other models. Our experiments on classical chaotic PDE systems, including the 1D Kuramoto-Sivashinsky and forced isotropic turbulence datasets, demonstrate that our approach significantly improves generalization performance compared to several competing methods. Additionally, when combining with time series modeling techniques (e.g., Koopman operator, LSTM), the proposed technique offers significant improvements for surrogate modeling of high-dimensional multi-scale PDE systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Adam Requires Better Rotation Dependent Assumptions</title>
<link>https://arxiv.org/abs/2410.19964</link>
<guid>https://arxiv.org/abs/2410.19964</guid>
<content:encoded><![CDATA[
arXiv:2410.19964v3 Announce Type: replace 
Abstract: Despite its widespread adoption, Adam's advantage over Stochastic Gradient Descent (SGD) lacks a comprehensive theoretical explanation. This paper investigates Adam's sensitivity to rotations of the parameter space. We observe that Adam's performance in training transformers degrades under random rotations of the parameter space, indicating a crucial sensitivity to the choice of basis in practice. This reveals that conventional rotation-invariant assumptions are insufficient to capture Adam's advantages theoretically. To better understand the rotation-dependent properties that benefit Adam, we also identify structured rotations that preserve or even enhance its empirical performance. We then examine the rotation-dependent assumptions in the literature and find that they fall short in explaining Adam's behaviour across various rotation types. In contrast, we verify the orthogonality of the update as a promising indicator of Adam's basis sensitivity, suggesting it may be the key quantity for developing rotation-dependent theoretical frameworks that better explain its empirical success.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shallow Diffuse: Robust and Invisible Watermarking through Low-Dimensional Subspaces in Diffusion Models</title>
<link>https://arxiv.org/abs/2410.21088</link>
<guid>https://arxiv.org/abs/2410.21088</guid>
<content:encoded><![CDATA[
arXiv:2410.21088v3 Announce Type: replace 
Abstract: The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce Shallow Diffuse, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, Shallow Diffuse decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that our Shallow Diffuse outperforms existing watermarking methods in terms of robustness and consistency. The codes are released at https://github.com/liwd190019/Shallow-Diffuse.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Laws for Task-Optimized Models of the Primate Visual Ventral Stream</title>
<link>https://arxiv.org/abs/2411.05712</link>
<guid>https://arxiv.org/abs/2411.05712</guid>
<content:encoded><![CDATA[
arXiv:2411.05712v3 Announce Type: replace 
Abstract: When trained on large-scale object classification datasets, certain artificial neural network models begin to approximate core object recognition behaviors and neural response patterns in the primate brain. While recent machine learning advances suggest that scaling compute, model size, and dataset size improves task performance, the impact of scaling on brain alignment remains unclear. In this study, we explore scaling laws for modeling the primate visual ventral stream by systematically evaluating over 600 models trained under controlled conditions on benchmarks spanning V1, V2, V4, IT and behavior. We find that while behavioral alignment continues to scale with larger models, neural alignment saturates. This observation remains true across model architectures and training datasets, even though models with stronger inductive biases and datasets with higher-quality images are more compute-efficient. Increased scaling is especially beneficial for higher-level visual areas, where small models trained on few samples exhibit only poor alignment. Our results suggest that while scaling current architectures and datasets might suffice for alignment with human core object recognition behavior, it will not yield improved models of the brain's visual ventral stream, highlighting the need for novel strategies in building brain models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>scMEDAL for the interpretable analysis of single-cell transcriptomics data with batch effect visualization using a deep mixed effects autoencoder</title>
<link>https://arxiv.org/abs/2411.06635</link>
<guid>https://arxiv.org/abs/2411.06635</guid>
<content:encoded><![CDATA[
arXiv:2411.06635v4 Announce Type: replace 
Abstract: Single-cell RNA sequencing enables high-resolution analysis of cellular heterogeneity, yet disentangling biological signal from batch effects remains a major challenge. Existing batch-correction algorithms suppress or discard batch-related variation rather than modeling it. We propose scMEDAL, single-cell Mixed Effects Deep Autoencoder Learning, a framework that separately models batch-invariant and batch-specific effects using two complementary subnetworks. The principal innovation, scMEDAL-RE, is a random-effects Bayesian autoencoder that learns batch-specific representations while preserving biologically meaningful information confounded with batch effects signal often lost under standard correction. Complementing it, the fixed-effects subnetwork, scMEDAL-FE, trained via adversarial learning provides a default batch-correction component. Evaluations across diverse conditions (autism, leukemia, cardiovascular), cell types, and technical and biological effects show that scMEDAL-RE produces interpretable, batch-specific embeddings that complement both scMEDAL-FE and established correction methods (scVI, Scanorama, Harmony, SAUCIE), yielding more accurate prediction of disease status, donor group, and tissue. scMEDAL also provides generative visualizations, including counterfactual reconstructions of a cell's expression as if acquired in another batch. The framework allows substitution of the fixed-effects component with other correction methods, while retaining scMEDAL-RE's enhanced predictive power and visualization. Overall, scMEDAL is a versatile, interpretable framework that complements existing correction, providing enhanced insight into cellular heterogeneity and data acquisition.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnomalyAID: Reliable Interpretation for Semi-supervised Network Anomaly Detection</title>
<link>https://arxiv.org/abs/2411.11293</link>
<guid>https://arxiv.org/abs/2411.11293</guid>
<content:encoded><![CDATA[
arXiv:2411.11293v3 Announce Type: replace 
Abstract: Semi-supervised Learning plays a crucial role in network anomaly detection applications, however, learning anomaly patterns with limited labeled samples is not easy. Additionally, the lack of interpretability creates key barriers to the adoption of semi-supervised frameworks in practice. Most existing interpretation methods are developed for supervised/unsupervised frameworks or non-security domains and fail to provide reliable interpretations. In this paper, we propose AnomalyAID, a general framework aiming to (1) make the anomaly detection process interpretable and improve the reliability of interpretation results, and (2) assign high-confidence pseudo labels to unlabeled samples for improving the performance of anomaly detection systems with limited supervised data. For (1), we propose a novel interpretation approach that leverages global and local interpreters to provide reliable explanations, while for (2), we design a new two-stage semi-supervised learning framework for network anomaly detection by aligning both stages' model predictions with special constraints. We apply AnomalyAID over two representative network anomaly detection tasks and extensively evaluate AnomalyAID with representative prior works. Experimental results demonstrate that AnomalyAID can provide accurate detection results with reliable interpretations for semi-supervised network anomaly detection systems. The code is available at: https://github.com/M-Code-Space/AnomalyAID.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs</title>
<link>https://arxiv.org/abs/2411.14133</link>
<guid>https://arxiv.org/abs/2411.14133</guid>
<content:encoded><![CDATA[
arXiv:2411.14133v3 Announce Type: replace 
Abstract: LLMs have shown impressive capabilities across various natural language processing tasks, yet remain vulnerable to input prompts, known as jailbreak attacks, carefully designed to bypass safety guardrails and elicit harmful responses. Traditional methods rely on manual heuristics but suffer from limited generalizability. Despite being automatic, optimization-based attacks often produce unnatural prompts that can be easily detected by safety filters or require high computational costs due to discrete token optimization. In this paper, we introduce Generative Adversarial Suffix Prompter (GASP), a novel automated framework that can efficiently generate human-readable jailbreak prompts in a fully black-box setting. In particular, GASP leverages latent Bayesian optimization to craft adversarial suffixes by efficiently exploring continuous latent embedding spaces, gradually optimizing the suffix prompter to improve attack efficacy while balancing prompt coherence via a targeted iterative refinement procedure. Through comprehensive experiments, we show that GASP can produce natural adversarial prompts, significantly improving jailbreak success over baselines, reducing training times, and accelerating inference speed, thus making it an efficient and scalable solution for red-teaming LLMs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Federated Fine-Tuning: A Single Communication Round is Enough for Foundation Models</title>
<link>https://arxiv.org/abs/2412.04650</link>
<guid>https://arxiv.org/abs/2412.04650</guid>
<content:encoded><![CDATA[
arXiv:2412.04650v2 Announce Type: replace 
Abstract: The recent advancement of foundation models (FMs) has increased the demand for fine-tuning these models on large-scale cross-domain datasets. To address this, federated fine-tuning has emerged, allowing FMs to be fine-tuned on distributed datasets across multiple devices while ensuring data privacy. However, the substantial parameter size and the multi-round communication in federated learning algorithms result in prohibitively high communication costs, challenging the practicality of federated fine-tuning. In this paper, we identify and analyze, both theoretically and empirically, that the traditional multi-round aggregation algorithms may not be necessary for federated fine-tuning large FMs. Our experiments reveal that a single round of aggregation (i.e., one-shot federated fine-tuning) yields a global model performance comparable to that achieved through multiple rounds of aggregation. Through rigorous mathematical and empirical analyses, we demonstrate that large FMs, due to their extensive parameter sizes and pre-training on general tasks, achieve significantly lower training loss in one-shot federated fine-tuning compared to smaller models. Our extensive experiments show that one-shot federated fine-tuning significantly reduces communication costs. It also has the potential to enable asynchronous aggregation, enhances privacy, and maintains performance consistency with multi-round federated fine-tuning on both text generation and text-to-image generation tasks. Our findings provide insights to revolutionize federated fine-tuning in practice, enhancing efficiency, reducing costs, and expanding accessibility for FMs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Memory in Optimization Algorithms Implicitly Modifies the Loss</title>
<link>https://arxiv.org/abs/2502.02132</link>
<guid>https://arxiv.org/abs/2502.02132</guid>
<content:encoded><![CDATA[
arXiv:2502.02132v2 Announce Type: replace 
Abstract: In modern optimization methods used in deep learning, each update depends on the history of previous iterations, often referred to as memory, and this dependence decays fast as the iterates go further into the past. For example, gradient descent with momentum has exponentially decaying memory through exponentially averaged past gradients. We introduce a general technique for identifying a memoryless algorithm that approximates an optimization algorithm with memory. It is obtained by replacing all past iterates in the update by the current one, and then adding a correction term arising from memory (also a function of the current iterate). This correction term can be interpreted as a perturbation of the loss, and the nature of this perturbation can inform how memory implicitly (anti-)regularizes the optimization dynamics. As an application of our theory, we find that Lion does not have the kind of implicit anti-regularization induced by memory that AdamW does, providing a theory-based explanation for Lion's better generalization performance recently documented.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers</title>
<link>https://arxiv.org/abs/2503.03961</link>
<guid>https://arxiv.org/abs/2503.03961</guid>
<content:encoded><![CDATA[
arXiv:2503.03961v3 Announce Type: replace 
Abstract: Recent theoretical results show transformers cannot express sequential reasoning problems over long inputs, intuitively because their computational depth is bounded. However, prior work treats the depth as a constant, leaving it unclear to what degree bounded depth may suffice for solving problems over short inputs, or how increasing the transformer's depth affects its expressive power. We address these questions by analyzing transformers whose depth can grow minimally with context length $n$. We show even highly uniform transformers with depth $\Theta(\log n)$ can express two important problems: recognizing regular languages, which captures state tracking abilities and was known to be expressible only by an unconventional, non-uniform model of transformers, and graph connectivity, which underlies multi-step reasoning. Notably, both of these problems cannot be expressed by fixed-depth transformers under standard complexity conjectures, demonstrating the expressivity benefit of growing depth. Moreover, our theory quantitatively predicts how depth must grow with input length to express these problems, showing that depth scaling is more efficient than scaling width or chain-of-thought steps. Empirically, our detailed experiments designed to bridge the expressivity vs. learnability gap reveal that our theoretical depth requirements for regular language recognition closely match the practical depth requirements for successfully training transformers. Thus, our results clarify how depth affects a transformer's reasoning capabilities, and provide practical guidance for effective depth selection for sequential reasoning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models</title>
<link>https://arxiv.org/abs/2503.22879</link>
<guid>https://arxiv.org/abs/2503.22879</guid>
<content:encoded><![CDATA[
arXiv:2503.22879v4 Announce Type: replace 
Abstract: State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input $x$, combined with a per-state-group quantization for input-dependent parameters $B$ and $C$. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms two state-of-the-art SSM quantization methods and delivers 1.3$\times$ and 3$\times$ speed-ups in the pre-filling and generation stages, respectively, while offering 4$\times$ memory reduction with only a $1.6\%$ average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explanations Go Linear: Interpretable and Individual Latent Encoding for Post-hoc Explainability</title>
<link>https://arxiv.org/abs/2504.20667</link>
<guid>https://arxiv.org/abs/2504.20667</guid>
<content:encoded><![CDATA[
arXiv:2504.20667v2 Announce Type: replace 
Abstract: Post-hoc explainability is essential for understanding black-box machine learning models. Surrogate-based techniques are widely used for local and global model-agnostic explanations but have significant limitations. Local surrogates capture non-linearities but are computationally expensive and sensitive to parameters, while global surrogates are more efficient but struggle with complex local behaviors. In this paper, we present ILLUME, a flexible and interpretable framework grounded in representation learning, that can be integrated with various surrogate models to provide explanations for any black-box classifier. Specifically, our approach combines a globally trained surrogate with instance-specific linear transformations learned with a meta-encoder to generate both local and global explanations. Through extensive empirical evaluations, we demonstrate the effectiveness of ILLUME in producing feature attributions and decision rules that are not only accurate but also robust and faithful to the black-box, thus providing a unified explanation framework that effectively addresses the limitations of traditional surrogate methods.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Cancer Modeling in the Age of Foundation Model Embeddings</title>
<link>https://arxiv.org/abs/2505.07683</link>
<guid>https://arxiv.org/abs/2505.07683</guid>
<content:encoded><![CDATA[
arXiv:2505.07683v3 Announce Type: replace 
Abstract: The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference dataset in cancer through its harmonized genomics, clinical, and imaging data. Numerous prior studies have developed bespoke deep learning models over TCGA for tasks such as cancer survival prediction. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive feature embeddings agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized. Here, we investigate the ability to train classical machine learning models over multimodal, zero-shot FM embeddings of cancer data. We demonstrate the ease and additive effect of multimodal fusion, outperforming unimodal models. Further, we show the benefit of including pathology report text and rigorously evaluate the effect of model-based text summarization and hallucination. Overall, we propose an embedding-centric approach to multimodal cancer modeling.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Dynamics of RNNs in Closed-Loop Environments</title>
<link>https://arxiv.org/abs/2505.13567</link>
<guid>https://arxiv.org/abs/2505.13567</guid>
<content:encoded><![CDATA[
arXiv:2505.13567v2 Announce Type: replace 
Abstract: Recurrent neural networks (RNNs) trained on neuroscience-inspired tasks offer powerful models of brain computation. However, typical training paradigms rely on open-loop, supervised settings, whereas real-world learning unfolds in closed-loop environments. Here, we develop a mathematical theory describing the learning dynamics of linear RNNs trained in closed-loop contexts. We first demonstrate that two otherwise identical RNNs, trained in either closed- or open-loop modes, follow markedly different learning trajectories. To probe this divergence, we analytically characterize the closed-loop case, revealing distinct stages aligned with the evolution of the training loss. Specifically, we show that the learning dynamics of closed-loop RNNs, in contrast to open-loop ones, are governed by an interplay between two competing objectives: short-term policy improvement and long-term stability of the agent-environment interaction. Finally, we apply our framework to a realistic motor control task, highlighting its broader applicability. Taken together, our results underscore the importance of modeling closed-loop dynamics in a biologically plausible setting.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regularized least squares learning with heavy-tailed noise is minimax optimal</title>
<link>https://arxiv.org/abs/2505.14214</link>
<guid>https://arxiv.org/abs/2505.14214</guid>
<content:encoded><![CDATA[
arXiv:2505.14214v3 Announce Type: replace 
Abstract: This paper examines the performance of ridge regression in reproducing kernel Hilbert spaces in the presence of noise that exhibits a finite number of higher moments. We establish excess risk bounds consisting of subgaussian and polynomial terms based on the well known integral operator framework. The dominant subgaussian component allows to achieve convergence rates that have previously only been derived under subexponential noise - a prevalent assumption in related work from the last two decades. These rates are optimal under standard eigenvalue decay conditions, demonstrating the asymptotic robustness of regularized least squares against heavy-tailed noise. Our derivations are based on a Fuk-Nagaev inequality for Hilbert-space valued random variables.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors</title>
<link>https://arxiv.org/abs/2505.17760</link>
<guid>https://arxiv.org/abs/2505.17760</guid>
<content:encoded><![CDATA[
arXiv:2505.17760v2 Announce Type: replace 
Abstract: Detecting subtle forms of dishonesty like sycophancy and manipulation in Large Language Models (LLMs) remains challenging for both humans and automated evaluators, as these behaviors often appear through small biases rather than clear false statements. We introduce Judge Using Safety-Steered Alternatives (JUSSA), a novel framework that employs steering vectors not to improve model behavior directly, but to enhance LLM judges' evaluation capabilities. JUSSA applies steering vectors during inference to generate more honest alternatives, providing judges with contrastive examples that make subtle dishonest patterns easier to detect. While existing evaluation methods rely on black-box evaluation, JUSSA leverages model internals to create targeted comparisons from single examples. We evaluate our method on sycophancy detection and introduce a new manipulation dataset covering multiple types of manipulation. Our results demonstrate that JUSSA effectively improves detection accuracy over single-response evaluation in various cases. Analysis across judge models reveals that JUSSA helps weaker judges on easier dishonesty detection tasks, and stronger judges on harder tasks. Layer-wise experiments show how dishonest prompts cause representations to diverge from honest ones in middle layers, revealing where steering interventions are most effective for generating contrastive examples. By demonstrating that steering vectors can enhance safety evaluation rather than just modify behavior, our work opens new directions for scalable model auditing as systems become increasingly sophisticated.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exact Expressive Power of Transformers with Padding</title>
<link>https://arxiv.org/abs/2505.18948</link>
<guid>https://arxiv.org/abs/2505.18948</guid>
<content:encoded><![CDATA[
arXiv:2505.18948v2 Announce Type: replace 
Abstract: Chain of thought is a natural inference-time method for increasing the computational power of transformer-based large language models (LLMs), but comes at the cost of sequential decoding. Are there more efficient alternatives to expand a transformer's expressive power without adding parameters? We consider transformers with padding tokens as a form of parallelizable test-time compute. We show that averaging-hard-attention, masked-pre-norm transformers with polynomial padding recognize precisely the class $\mathsf{FO}$-uniform $\mathsf{TC}^0$ of extremely parallelizable problems. While the $\mathsf{TC}^0$ upper bound was known, proving a matching lower bound had been elusive. Further, our novel analysis reveals the precise expanded power of padded transformers when coupled with another form of inference-time compute, namely dynamically increasing depth via looping. Our core technical contribution is to show how padding helps bring the notions of complete problems and reductions, which have been a cornerstone of classical complexity theory, to the formal study of transformers. Armed with this new tool, we prove that padded transformers with $O(\log^d n)$ looping on inputs of length $n$ recognize exactly the class $\mathsf{FO}$-uniform $\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and looping together systematically expand transformers' expressive power: with polylogarithmic looping, polynomially padded transformers recognize precisely the class $\mathsf{FO}$-uniform $\mathsf{NC}$, the best that could be expected without losing parallelism (unless $\mathsf{NC} = \mathsf{P}$). Our results thus motivate further exploration of padding and looping as parallelizable alternatives to chain of thought for test-time compute.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference</title>
<link>https://arxiv.org/abs/2505.22913</link>
<guid>https://arxiv.org/abs/2505.22913</guid>
<content:encoded><![CDATA[
arXiv:2505.22913v2 Announce Type: replace 
Abstract: We demonstrate that unstructured sparsity significantly improves KV cache compression for LLMs, enabling sparsity levels up to 70% without compromising accuracy or requiring fine-tuning. We conduct a systematic exploration of pruning strategies and find per-token magnitude-based pruning as highly effective for both Key and Value caches under unstructured sparsity, surpassing prior structured pruning schemes. The Key cache benefits from prominent outlier elements, while the Value cache surprisingly benefits from a simple magnitude-based pruning despite its uniform distribution. KV cache size is the major bottleneck in decode performance due to high memory overhead for large context lengths. To address this, we use a bitmap-based sparse format and a custom attention kernel capable of compressing and directly computing over compressed caches pruned to arbitrary sparsity patterns, significantly accelerating memory-bound operations in decode computations and thereby compensating for the overhead of runtime pruning and compression. Our custom attention kernel coupled with the bitmap-based format delivers substantial compression of KV cache upto 45% of dense inference and thereby enables longer context length and increased tokens/sec throughput of upto 2.23x compared to dense inference. Our pruning mechanism and sparse attention kernel is available at https://github.com/dhjoo98/mustafar.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data</title>
<link>https://arxiv.org/abs/2505.23062</link>
<guid>https://arxiv.org/abs/2505.23062</guid>
<content:encoded><![CDATA[
arXiv:2505.23062v3 Announce Type: replace 
Abstract: Incorporating pre-collected offline data from a source environment can significantly improve the sample efficiency of reinforcement learning (RL), but this benefit is often challenged by discrepancies between the transition dynamics of the source and target environments. Existing methods typically address this issue by penalizing or filtering out source transitions in high dynamics-gap regions. However, their estimation of the dynamics gap often relies on KL divergence or mutual information, which can be ill-defined when the source and target dynamics have disjoint support. To overcome these limitations, we propose CompFlow, a method grounded in the theoretical connection between flow matching and optimal transport. Specifically, we model the target dynamics as a conditional flow built upon the output distribution of the source-domain flow, rather than learning it directly from a Gaussian prior. This composite structure offers two key advantages: (1) improved generalization for learning target dynamics, and (2) a principled estimation of the dynamics gap via the Wasserstein distance between source and target transitions. Leveraging our principled estimation of the dynamics gap, we further introduce an optimistic active data collection strategy that prioritizes exploration in regions of high dynamics gap, and theoretically prove that it reduces the performance disparity with the optimal policy. Empirically, CompFlow outperforms strong baselines across several RL benchmarks with shifted dynamics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How do Transformers Learn Implicit Reasoning?</title>
<link>https://arxiv.org/abs/2505.23653</link>
<guid>https://arxiv.org/abs/2505.23653</guid>
<content:encoded><![CDATA[
arXiv:2505.23653v2 Announce Type: replace 
Abstract: Recent work suggests that large language models (LLMs) can perform multi-hop reasoning implicitly -- producing correct answers without explicitly verbalizing intermediate steps -- but the underlying mechanisms remain poorly understood. In this paper, we study how such implicit reasoning emerges by training transformers from scratch in a controlled symbolic environment. Our analysis reveals a three-stage developmental trajectory: early memorization, followed by in-distribution generalization, and eventually cross-distribution generalization. We find that training with atomic triples is not necessary but accelerates learning, and that second-hop generalization relies on query-level exposure to specific compositional structures. To interpret these behaviors, we introduce two diagnostic tools: cross-query semantic patching, which identifies semantically reusable intermediate representations, and a cosine-based representational lens, which reveals that successful reasoning correlates with the cosine-base clustering in hidden space. This clustering phenomenon in turn provides a coherent explanation for the behavioral dynamics observed across training, linking representational structure to reasoning capability. These findings provide new insights into the interpretability of implicit multi-hop reasoning in LLMs, helping to clarify how complex reasoning processes unfold internally and offering pathways to enhance the transparency of such models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training</title>
<link>https://arxiv.org/abs/2505.23971</link>
<guid>https://arxiv.org/abs/2505.23971</guid>
<content:encoded><![CDATA[
arXiv:2505.23971v3 Announce Type: replace 
Abstract: The right batch size is important when training language models at scale: a large batch size is necessary for fast training, but a batch size that is too large will harm token efficiency. To navigate this tradeoff, McCandlish et al. (2018) suggest that a critical batch size (CBS), below which training will not substantially degrade loss, can be estimated based on the gradient noise scale during training. While their method has been adopted in practice, e.g., when training GPT-3, strong assumptions are required to justify gradient noise as a proxy for the CBS, which makes it unclear whether their approach should be trusted in practice, limiting its applicability. In this paper, we introduce a simple, empirical approach to directly measure the CBS and show how the CBS evolves over training. Applying our approach to the OLMo models, we find that CBS is near 0 at initialization, increases rapidly at first, and then plateaus as training progresses. Furthermore, we find that this trend holds across different model sizes (1B and 7B), suggesting CBS from small training runs can inform larger-scale training runs. Our findings about how the CBS changes over training motivate batch size warmup as a natural way to reliably train language models at large batch size: start the batch size small and increase it as the CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to slightly better loss than the original training run with 43% fewer gradient steps. This shows how our framework can be applied to reliably train language models at larger batch sizes, increasing data parallelism without compromising performance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts</title>
<link>https://arxiv.org/abs/2505.24722</link>
<guid>https://arxiv.org/abs/2505.24722</guid>
<content:encoded><![CDATA[
arXiv:2505.24722v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond</title>
<link>https://arxiv.org/abs/2506.03703</link>
<guid>https://arxiv.org/abs/2506.03703</guid>
<content:encoded><![CDATA[
arXiv:2506.03703v3 Announce Type: replace 
Abstract: Fundamental physics often confronts complex symbolic problems with few guiding exemplars or established principles. While artificial intelligence (AI) offers promise, its typical need for vast datasets to learn from hinders its use in these information-scarce frontiers. We introduce learning at criticality (LaC), a reinforcement learning (RL) scheme that tunes Large Language Models (LLMs) to a sharp learning transition, addressing this information scarcity. At this transition, LLMs achieve peak generalization from minimal data, exemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic reasoning. To elucidate this peak, we analyze a minimal concept-network model (CoNet) designed to capture the essence of how LLMs might link tokens. Trained on a single exemplar, this model also undergoes a sharp learning transition. This transition exhibits hallmarks of a second-order phase transition, notably power-law distributed solution path lengths. At this critical point, the system maximizes a ``critical thinking pattern" crucial for generalization, enabled by the underlying scale-free exploration. This suggests LLMs reach peak performance by operating at criticality, where such explorative dynamics enable the extraction of underlying operational rules. We demonstrate LaC in quantum field theory: an 8B-parameter LLM, tuned to its critical point by LaC using a few exemplars of symbolic Matsubara sums, solves unseen, higher-order problems, significantly outperforming far larger models. LaC thus leverages critical phenomena, a physical principle, to empower AI for complex, data-sparse challenges in fundamental physics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explicit Density Approximation for Neural Implicit Samplers Using a Bernstein-Based Convex Divergence</title>
<link>https://arxiv.org/abs/2506.04700</link>
<guid>https://arxiv.org/abs/2506.04700</guid>
<content:encoded><![CDATA[
arXiv:2506.04700v2 Announce Type: replace 
Abstract: Rank-based statistical metrics, such as the invariant statistical loss (ISL), have recently emerged as robust and practically effective tools for training implicit generative models. In this work, we introduce dual-ISL, a novel likelihood-free objective for training implicit generative models that interchanges the roles of the target and model distributions in the ISL framework, yielding a convex optimization problem in the space of model densities. We prove that the resulting rank-based discrepancy $d_K$ is i) continuous under weak convergence and with respect to the $L^1$ norm, and ii) convex in its first argument-properties not shared by classical divergences such as KL or Wasserstein distances. Building on this, we develop a theoretical framework that interprets $d_K$ as an $L^2$-projection of the density ratio $q = p/\tilde p$ onto a Bernstein polynomial basis, from which we derive exact bounds on the truncation error, precise convergence rates, and a closed-form expression for the truncated density approximation. We further extend our analysis to the multivariate setting via random one-dimensional projections, defining a sliced dual-ISL divergence that retains both convexity and continuity. We empirically show that these theoretical advantages translate into practical ones. Specifically, across several benchmarks dual-ISL converges more rapidly, delivers markedly smoother and more stable training, and more effectively prevents mode collapse than classical ISL and other leading implicit generative methods-while also providing an explicit density approximation.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning</title>
<link>https://arxiv.org/abs/2506.06694</link>
<guid>https://arxiv.org/abs/2506.06694</guid>
<content:encoded><![CDATA[
arXiv:2506.06694v5 Announce Type: replace 
Abstract: Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from a frozen teacher model, and reinforces knowledge retention through a tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a mobility-aware expert routing mechanism, and employs a layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks a crucial step toward unlocking foundation models for mobility, offering a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models. To facilitate reproducibility and future research, we have released the code and models at https://github.com/tsinghua-fib-lab/MoveGCL.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Graph Learning for Industrial Carbon Emission Analysis and Policy Impact</title>
<link>https://arxiv.org/abs/2507.02912</link>
<guid>https://arxiv.org/abs/2507.02912</guid>
<content:encoded><![CDATA[
arXiv:2507.02912v3 Announce Type: replace 
Abstract: Industrial carbon emissions are a major driver of climate change, yet modeling these emissions is challenging due to multicollinearity among factors and complex interdependencies across sectors and time. We propose a novel graph-based deep learning framework DGL to analyze and forecast industrial CO_2 emissions, addressing high feature correlation and capturing industrial-temporal interdependencies. Unlike traditional regression or clustering methods, our approach leverages a Graph Neural Network (GNN) with attention mechanisms to model relationships between industries (or regions) and a temporal transformer to learn long-range patterns. We evaluate our framework on public global industry emissions dataset derived from EDGAR v8.0, spanning multiple countries and sectors. The proposed model achieves superior predictive performance - reducing error by over 15% compared to baseline deep models - while maintaining interpretability via attention weights and causal analysis. We believe that we are the first Graph-Temporal architecture that resolves multicollinearity by structurally encoding feature relationships, along with integration of causal inference to identify true drivers of emissions, improving transparency and fairness. We also stand a demonstration of policy relevance, showing how model insights can guide sector-specific decarbonization strategies aligned with sustainable development goals. Based on the above, we show high-emission "hotspots" and suggest equitable intervention plans, illustrating the potential of state-of-the-art AI graph learning to advance climate action, offering a powerful tool for policymakers and industry stakeholders to achieve carbon reduction targets.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units</title>
<link>https://arxiv.org/abs/2507.18989</link>
<guid>https://arxiv.org/abs/2507.18989</guid>
<content:encoded><![CDATA[
arXiv:2507.18989v2 Announce Type: replace 
Abstract: As AI workloads proliferate, optimizing arithmetic units is becoming increasingly important for reducing the footprint of digital systems. Conventional design flows, which often rely on manual or heuristic-based optimization, are limited in their ability to thoroughly explore the vast design space. In this paper, we introduce GENIAL, a machine learning-based framework for the automatic generation and optimization of arithmetic units, with a focus on multipliers.
  At the core of GENIAL is a Transformer-based surrogate model trained in two stages, involving self-supervised pretraining followed by supervised finetuning, to robustly forecast key hardware metrics such as power and area from abstracted design representations. By inverting the surrogate model, GENIAL efficiently searches for new operand encodings that directly minimize power consumption in arithmetic units for specific input data distributions. Extensive experiments on large datasets demonstrate that GENIAL is consistently more sample efficient than other methods, and converges faster towards optimized designs. This enables deployment of a high-effort logic synthesis optimization flow in the loop, improving the accuracy of the surrogate model. Notably, GENIAL automatically discovers encodings that achieve up to 18% switching activity savings within multipliers on representative AI workloads compared with the conventional two's complement. We also demonstrate the versatility of our approach by achieving significant improvements on Finite State Machines, highlighting GENIAL's applicability for a wide spectrum of logic functions. Together, these advances mark a significant step toward automated Quality-of-Results-optimized combinational circuit generation for digital systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Communication Efficient LLM Pre-training with SparseLoCo</title>
<link>https://arxiv.org/abs/2508.15706</link>
<guid>https://arxiv.org/abs/2508.15706</guid>
<content:encoded><![CDATA[
arXiv:2508.15706v2 Announce Type: replace 
Abstract: Communication-efficient distributed training algorithms have received considerable interest recently due to their benefits for training Large Language Models (LLMs) in bandwidth-constrained settings, such as across datacenters and over the internet. Despite reducing communication frequency, these methods still typically require communicating a full copy of the model's gradients-resulting in a communication bottleneck even for cross-datacenter links. Furthermore, they can slightly degrade performance compared to a naive AdamW DDP baseline. While quantization is often applied to reduce the pseudo-gradient's size, in the context of LLM pre-training, existing approaches have been unable to additionally leverage sparsification and have obtained limited quantization. In this work, we introduce SparseLoCo, a communication-efficient training algorithm for LLMs that effectively leverages error feedback with Top-k sparsification and 2-bit quantization to reach extreme sparsity as low as 1-3% while outperforming full-precision DiLoCo. Our key observations are that outer momentum can be locally approximated by an error feedback accumulator combined with aggressive sparsity, and that sparse aggregation can actually improve model performance. We empirically demonstrate in a range of communication-constrained LLM training settings that SparseLoCo provides significant benefits in both performance and communication cost.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-target Bayesian Transformer Framework for Predicting Cardiovascular Disease Biomarkers during Pandemics</title>
<link>https://arxiv.org/abs/2509.01794</link>
<guid>https://arxiv.org/abs/2509.01794</guid>
<content:encoded><![CDATA[
arXiv:2509.01794v2 Announce Type: replace 
Abstract: The COVID-19 pandemic disrupted healthcare systems worldwide, disproportionately impacting individuals with chronic conditions such as cardiovascular disease (CVD). These disruptions -- through delayed care and behavioral changes, affected key CVD biomarkers, including LDL cholesterol (LDL-C), HbA1c, BMI, and systolic blood pressure (SysBP). Accurate modeling of these changes is crucial for predicting disease progression and guiding preventive care. However, prior work has not addressed multi-target prediction of CVD biomarker from Electronic Health Records (EHRs) using machine learning (ML), while jointly capturing biomarker interdependencies, temporal patterns, and predictive uncertainty. In this paper, we propose MBT-CB, a Multi-target Bayesian Transformer (MBT) with pre-trained BERT-based transformer framework to jointly predict LDL-C, HbA1c, BMI and SysBP CVD biomarkers from EHR data. The model leverages Bayesian Variational Inference to estimate uncertainties, embeddings to capture temporal relationships and a DeepMTR model to capture biomarker inter-relationships. We evaluate MBT-CT on retrospective EHR data from 3,390 CVD patient records (304 unique patients) in Central Massachusetts during the Covid-19 pandemic. MBT-CB outperformed a comprehensive set of baselines including other BERT-based ML models, achieving an MAE of 0.00887, RMSE of 0.0135 and MSE of 0.00027, while effectively capturing data and model uncertainty, patient biomarker inter-relationships, and temporal dynamics via its attention and embedding mechanisms. MBT-CB's superior performance highlights its potential to improve CVD biomarker prediction and support clinical decision-making during pandemics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finetuning LLMs for Human Behavior Prediction in Social Science Experiments</title>
<link>https://arxiv.org/abs/2509.05830</link>
<guid>https://arxiv.org/abs/2509.05830</guid>
<content:encoded><![CDATA[
arXiv:2509.05830v2 Announce Type: replace 
Abstract: Large language models (LLMs) offer a powerful opportunity to simulate the results of social science experiments. In this work, we demonstrate that finetuning LLMs directly on individual-level responses from past experiments meaningfully improves the accuracy of such simulations across diverse social science domains. We construct SocSci210 via an automatic pipeline, a dataset comprising 2.9 million responses from 400,491 participants in 210 open-source social science experiments. Through finetuning, we achieve multiple levels of generalization. In completely unseen studies, our strongest model, Socrates-Qwen-14B, produces predictions that are 26% more aligned with distributions of human responses to diverse outcome questions under varying conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by 13%. By finetuning on a subset of conditions in a study, generalization to new unseen conditions is particularly robust, improving by 71%. Since SocSci210 contains rich demographic information, we reduce demographic parity difference, a measure of bias, by 10.6% through finetuning. Because social sciences routinely generate rich, topic-specific datasets, our findings indicate that finetuning on such data could enable more accurate simulations for experimental hypothesis screening. We release our data, models and finetuning code at stanfordhci.github.io/socrates.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation</title>
<link>https://arxiv.org/abs/2509.07325</link>
<guid>https://arxiv.org/abs/2509.07325</guid>
<content:encoded><![CDATA[
arXiv:2509.07325v2 Announce Type: replace 
Abstract: The National Comprehensive Cancer Network (NCCN) provides evidence-based guidelines for cancer treatment. Translating complex patient presentations into guideline-compliant treatment recommendations is time-intensive, requires specialized expertise, and is prone to error. Advances in large language model (LLM) capabilities promise to reduce the time required to generate treatment recommendations and improve accuracy. We present an LLM agent-based approach to automatically generate guideline-concordant treatment trajectories for patients with non-small cell lung cancer (NSCLC). Our contributions are threefold. First, we construct a novel longitudinal dataset of 121 cases of NSCLC patients that includes clinical encounters, diagnostic results, and medical histories, each expertly annotated with the corresponding NCCN guideline trajectories by board-certified oncologists. Second, we demonstrate that existing LLMs possess domain-specific knowledge that enables high-quality proxy benchmark generation for both model development and evaluation, achieving strong correlation (Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks. Third, we develop a hybrid approach combining expensive human annotations with model consistency information to create both the agent framework that predicts the relevant guidelines for a patient, as well as a meta-classifier that verifies prediction accuracy with calibrated confidence scores for treatment recommendations (AUROC=0.800), a critical capability for communicating the accuracy of outputs, custom-tailoring tradeoffs in performance, and supporting regulatory compliance. This work establishes a framework for clinically viable LLM-based guideline adherence systems that balance accuracy, interpretability, and regulatory requirements while reducing annotation costs, providing a scalable pathway toward automated clinical decision support.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Warmup for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2509.10641</link>
<guid>https://arxiv.org/abs/2509.10641</guid>
<content:encoded><![CDATA[
arXiv:2509.10641v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) hold great promise for advanced reasoning at the intersection of text and images, yet they have not fully realized this potential. MLLMs typically integrate an LLM, a vision encoder, and a connector that maps the vision encoder's embeddings into the LLM's text embedding space. Although each component is pretrained on massive datasets with billions of samples, the entire multimodal model is typically trained on only thousands (or a few million) samples, which can result in weak performance on complex reasoning tasks. To address these shortcomings, instead of relying on extensive labeled datasets for fine-tuning, we propose a Test-Time Warmup method that adapts the MLLM per test instance by leveraging data from weakly supervised auxiliary tasks. With our approach, we observe a relative performance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on the Llama-Vision-Instruct model. Our method demonstrates that 'warming up' before inference can enhance MLLMs' robustness across diverse reasoning tasks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SolarCrossFormer: Improving day-ahead Solar Irradiance Forecasting by Integrating Satellite Imagery and Ground Sensors</title>
<link>https://arxiv.org/abs/2509.15827</link>
<guid>https://arxiv.org/abs/2509.15827</guid>
<content:encoded><![CDATA[
arXiv:2509.15827v2 Announce Type: replace 
Abstract: Accurate day-ahead forecasts of solar irradiance are required for the large-scale integration of solar photovoltaic (PV) systems into the power grid. However, current forecasting solutions lack the temporal and spatial resolution required by system operators. In this paper, we introduce SolarCrossFormer, a novel deep learning model for day-ahead irradiance forecasting, that combines satellite images and time series from a ground-based network of meteorological stations. SolarCrossFormer uses novel graph neural networks to exploit the inter- and intra-modal correlations of the input data and improve the accuracy and resolution of the forecasts. It generates probabilistic forecasts for any location in Switzerland with a 15-minute resolution for horizons up to 24 hours ahead. One of the key advantages of SolarCrossFormer its robustness in real life operations. It can incorporate new time-series data without retraining the model and, additionally, it can produce forecasts for locations without input data by using only their coordinates. Experimental results over a dataset of one year and 127 locations across Switzerland show that SolarCrossFormer yield a normalized mean absolute error of 6.1 % over the forecasting horizon. The results are competitive with those achieved by a commercial numerical weather prediction service.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences</title>
<link>https://arxiv.org/abs/2509.16189</link>
<guid>https://arxiv.org/abs/2509.16189</guid>
<content:encoded><![CDATA[
arXiv:2509.16189v2 Announce Type: replace 
Abstract: When do machine learning systems fail to generalize, and what mechanisms could improve their generalization? Here, we draw inspiration from cognitive science to argue that one weakness of parametric machine learning systems is their failure to exhibit latent learning -- learning information that is not relevant to the task at hand, but that might be useful in a future task. We show how this perspective links failures ranging from the reversal curse in language modeling to new findings on agent-based navigation. We then highlight how cognitive science points to episodic memory as a potential part of the solution to these issues. Correspondingly, we show that a system with an oracle retrieval mechanism can use learning experiences more flexibly to generalize better across many of these challenges. We also identify some of the essential components for effectively using retrieval, including the importance of within-example in-context learning for acquiring the ability to use information across retrieved examples. In summary, our results illustrate one possible contributor to the relative data inefficiency of current machine learning systems compared to natural intelligence, and help to understand how retrieval methods can complement parametric learning to improve generalization. We close by discussing some of the links between these findings and prior results in cognitive science and neuroscience, and the broader implications.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis</title>
<link>https://arxiv.org/abs/2509.18112</link>
<guid>https://arxiv.org/abs/2509.18112</guid>
<content:encoded><![CDATA[
arXiv:2509.18112v2 Announce Type: replace 
Abstract: Foundation models (FMs) and large language models (LLMs) have demonstrated promising generalization across diverse domains for time-series analysis, yet their potential for electronic fetal monitoring (EFM) and cardiotocography (CTG) analysis remains underexplored. Most existing CTG studies relied on domain-specific models and lack systematic comparisons with modern foundation or language models, limiting our understanding of whether these models can outperform specialized systems in fetal health assessment. In this study, we present the first comprehensive benchmark of state-of-the-art architectures for automated antepartum CTG classification. Over 2,500 20-minutes recordings were used to evaluate over 15 models spanning domain-specific, time-series, foundation, and language-model categories under a unified framework. Fine-tuned LLMs consistently outperformed both foundation and domain-specific models across data-availability scenarios, except when uterine-activity signals were absent, where domain-specific models showed greater robustness. These performance gains, however, required substantially higher computational resources. Our results highlight that while fine-tuned LLMs achieved state-of-the-art performance for CTG classification, practical deployment must balance performance with computational efficiency.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperAdapt: Simple High-Rank Adaptation</title>
<link>https://arxiv.org/abs/2509.18629</link>
<guid>https://arxiv.org/abs/2509.18629</guid>
<content:encoded><![CDATA[
arXiv:2509.18629v2 Announce Type: replace 
Abstract: Foundation models excel across diverse tasks, but adapting them to specialized applications often requires fine-tuning, an approach that is memory and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate this by updating only a small subset of weights. In this paper, we introduce HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces the number of trainable parameters compared to state-of-the-art methods like LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying row- and column-wise scaling through diagonal matrices, thereby inducing a high-rank update while requiring only $n+m$ trainable parameters for an $n \times m$ matrix. Theoretically, we establish an upper bound on the rank of HyperAdapt's updates, and empirically, we confirm that it consistently induces high-rank transformations across model layers. Experiments on GLUE, arithmetic reasoning, and commonsense reasoning benchmarks with models up to 14B parameters demonstrate that HyperAdapt matches or nearly matches the performance of full fine-tuning and state-of-the-art PEFT methods while using orders of magnitude fewer trainable parameters.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2509.24239</link>
<guid>https://arxiv.org/abs/2509.24239</guid>
<content:encoded><![CDATA[
arXiv:2509.24239v2 Announce Type: replace 
Abstract: Recent large language models (LLMs) have shown strong reasoning capabilities. However, a critical question remains: do these models possess genuine reasoning skills particularly complex strategic reasoning or are they primarily excelling at sophisticated pattern recognition within their training data? To address this question, this paper presents a chess testbed, ChessArena, to evaluate the strategic reasoning capabilities of LLMs. Chess requires complex strategic reasoning capabilities including long-term planning, strict rule comprehension, and multi-turn conversation memorization. Specifically, ChessArena is a competitive framework where LLMs play against each other, under four different play modes. The testbed is equipped with a ranking algorithm and a leaderboard. The testbed can also evaluate fine-grained capabilities including basic understanding, move selection, and puzzle solving. Over 13 LLMs with different modes are evaluated in ChessArena, playing over 800 games. The results reveal significant shortcomings in current LLMs: no model can beat Maia-1100 (a chess engine at human amateur level), while some even failed to defeat a random player that selects moves arbitrarily. We also present a strong baseline to the testbed: our fine-tuned Qwen3-8B substantially improved performance, approaching much larger state-of-the-art reasoning models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Sequential and Relational Modeling for User Events: Datasets and Prediction Tasks</title>
<link>https://arxiv.org/abs/2510.11903</link>
<guid>https://arxiv.org/abs/2510.11903</guid>
<content:encoded><![CDATA[
arXiv:2510.11903v2 Announce Type: replace 
Abstract: User event modeling plays a central role in many machine learning applications, with use cases spanning e-commerce, social media, finance, cybersecurity, and other domains. User events can be broadly categorized into personal events, which involve individual actions, and relational events, which involve interactions between two users. These two types of events are typically modeled separately, using sequence-based methods for personal events and graph-based methods for relational events. Despite the need to capture both event types in real-world systems, prior work has rarely considered them together. This is often due to the convenient simplification that user behavior can be adequately represented by a single formalization, either as a sequence or a graph. To address this gap, there is a need for public datasets and prediction tasks that explicitly incorporate both personal and relational events. In this work, we introduce a collection of such datasets, propose a unified formalization, and empirically show that models benefit from incorporating both event types. Our results also indicate that current methods leave a notable room for improvements. We release these resources to support further research in unified user event modeling and encourage progress in this direction.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning</title>
<link>https://arxiv.org/abs/2510.13865</link>
<guid>https://arxiv.org/abs/2510.13865</guid>
<content:encoded><![CDATA[
arXiv:2510.13865v4 Announce Type: replace 
Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass filtering to deep neural network features to improve model generalizability. Our method is motivated by our hypothesis that neural networks encode task-relevant semantic information in high-frequency components while storing domain-specific biases in low-frequency components of deep features. By subtracting low-pass filtered outputs from original features, our approach isolates generalizable representations while preserving architectural integrity. Experimental results across diverse domains such as Vision, Text, 3D, and Audio demonstrate consistent performance improvements regardless of model architecture and data modality. Analysis reveals that our method induces feature sparsification and effectively isolates high-frequency components, providing empirical validation of our core hypothesis. The code is available at https://github.com/dongkwani/DeepEdgeFilter.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.17923</link>
<guid>https://arxiv.org/abs/2510.17923</guid>
<content:encoded><![CDATA[
arXiv:2510.17923v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing Large Language Models (LLMs), achieving remarkable performance in complex reasoning domains such as mathematics and code generation. However, current RL methods face a fundamental scalability bottleneck due to their heavy reliance on human-curated preference data or labeled datasets for reward modeling. To overcome this limitation, we explore RL on unlabeled data where models learn autonomously from continuous experience streams. The core challenge in this setting lies in reliable reward estimation without ground-truth supervision. Existing approaches like Test-Time RL address this through self-consistent consensus, but risk reinforcing incorrect pseudo-labels derived from majority voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel test-time reward mechanism that operates without external supervision. COMPASS integrates two complementary components: the Dual-Calibration Answer Reward (DCAR), which stabilizes training by establishing trustworthy pseudo-labels through confidence and credibility calibration, and the Decisive Path Reward (DPR), which directly optimizes the reasoning process quality beyond mere outcome supervision. By jointly reinforcing trustworthy consensus answers and highly decisive reasoning chains, the COMPASS systematically enhances the model's analytical capabilities. Extensive experiments show that COMPASS achieves significant and consistent performance gains across diverse reasoning tasks and model architectures, advancing a more scalable direction for LLMs to learn from continuous experience.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TowerVision: Understanding and Improving Multilinguality in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.21849</link>
<guid>https://arxiv.org/abs/2510.21849</guid>
<content:encoded><![CDATA[
arXiv:2510.21849v3 Announce Type: replace 
Abstract: Despite significant advances in vision-language models (VLMs), most existing work follows an English-centric design process, limiting their effectiveness in multilingual settings. In this work, we provide a comprehensive empirical study analyzing the impact of several multilingual design choices, such as training data composition, encoder selection, and text backbones. The result is TowerVision, a family of open multilingual VLMs for both image-text and video-text tasks, built upon the multilingual text-only model Tower+. TowerVision achieves competitive performance on multiple multimodal multilingual benchmarks and shows particular strength in culturally grounded tasks and multimodal translation. By incorporating visual and cultural context during fine-tuning, our models surpass existing approaches trained on substantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image tasks) and ViMUL-Bench (video tasks). Alongside the models, we release VisionBlocks, a high-quality, curated vision-language dataset. Our findings highlight that multilingual vision-language training data substantially improves cross-lingual generalization -- both from high-resource to underrepresented languages and vice versa -- and that instruction-tuned LLMs are not always the optimal initialization point. To support further research, we publicly release all models, data, and training recipes.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Bayesian Multi-Bandit Learning</title>
<link>https://arxiv.org/abs/2510.26284</link>
<guid>https://arxiv.org/abs/2510.26284</guid>
<content:encoded><![CDATA[
arXiv:2510.26284v2 Announce Type: replace 
Abstract: Multi-task learning in contextual bandits has attracted significant research interest due to its potential to enhance decision-making across multiple related tasks by leveraging shared structures and task-specific heterogeneity. In this article, we propose a novel hierarchical Bayesian framework for learning in various bandit instances. This framework captures both the heterogeneity and the correlations among different bandit instances through a hierarchical Bayesian model, enabling effective information sharing while accommodating instance-specific variations. Unlike previous methods that overlook the learning of the covariance structure across bandits, we introduce an empirical Bayesian approach to estimate the covariance matrix of the prior distribution. This enhances both the practicality and flexibility of learning across multi-bandits. Building on this approach, we develop two efficient algorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and ebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which incorporate the estimated prior into the decision-making process. We provide the frequentist regret upper bounds for the proposed algorithms, thereby filling a research gap in the field of multi-bandit problems. Extensive experiments on both synthetic and real-world datasets demonstrate the superior performance of our algorithms, particularly in complex environments. Our methods achieve lower cumulative regret compared to existing techniques, highlighting their effectiveness in balancing exploration and exploitation across multi-bandits.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection</title>
<link>https://arxiv.org/abs/2510.26510</link>
<guid>https://arxiv.org/abs/2510.26510</guid>
<content:encoded><![CDATA[
arXiv:2510.26510v2 Announce Type: replace 
Abstract: Model and hyperparameter selection are critical but challenging in machine learning, typically requiring expert intuition or expensive automated search. We investigate whether large language models (LLMs) can act as in-context meta-learners for this task. By converting each dataset into interpretable metadata, we prompt an LLM to recommend both model families and hyperparameters. We study two prompting strategies: (1) a zero-shot mode relying solely on pretrained knowledge, and (2) a meta-informed mode augmented with examples of models and their performance on past tasks. Across synthetic and real-world benchmarks, we show that LLMs can exploit dataset metadata to recommend competitive models and hyperparameters without search, and that improvements from meta-informed prompting demonstrate their capacity for in-context meta-learning. These results highlight a promising new role for LLMs as lightweight, general-purpose assistants for model selection and hyperparameter optimization.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off</title>
<link>https://arxiv.org/abs/2510.26722</link>
<guid>https://arxiv.org/abs/2510.26722</guid>
<content:encoded><![CDATA[
arXiv:2510.26722v3 Announce Type: replace 
Abstract: Over-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximate non-linear model predictive control with safety-augmented neural networks</title>
<link>https://arxiv.org/abs/2304.09575</link>
<guid>https://arxiv.org/abs/2304.09575</guid>
<content:encoded><![CDATA[
arXiv:2304.09575v3 Announce Type: replace-cross 
Abstract: Model predictive control (MPC) achieves stability and constraint satisfaction for general nonlinear systems, but requires computationally expensive online optimization. This paper studies approximations of such MPC controllers via neural networks (NNs) to achieve fast online evaluation. We propose safety augmentation that yields deterministic guarantees for convergence and constraint satisfaction despite approximation inaccuracies. We approximate the entire input sequence of the MPC with NNs, which allows us to verify online if it is a feasible solution to the MPC problem. We replace the NN solution by a safe candidate based on standard MPC techniques whenever it is infeasible or has worse cost. Our method requires a single evaluation of the NN and forward integration of the input sequence online, which is fast to compute on resource-constrained systems. The proposed control framework is illustrated using two numerical non-linear MPC benchmarks of different complexity, demonstrating computational speedups that are orders of magnitude higher than online optimization. In the examples, we achieve deterministic safety through the safety-augmented NNs, where a naive NN implementation fails.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Generative and Discriminative Noisy-Label Learning via Direction-Agnostic EM Formulation</title>
<link>https://arxiv.org/abs/2308.01184</link>
<guid>https://arxiv.org/abs/2308.01184</guid>
<content:encoded><![CDATA[
arXiv:2308.01184v3 Announce Type: replace-cross 
Abstract: Although noisy-label learning is often approached with discriminative methods for simplicity and speed, generative modeling offers a principled alternative by capturing the joint mechanism that produces features, clean labels, and corrupted observations. However, prior work typically (i) introduces extra latent variables and heavy image generators that bias training toward reconstruction, (ii) fixes a single data-generating direction (\(Y\rightarrow\!X\) or \(X\rightarrow\!Y\)), limiting adaptability, and (iii) assumes a uniform prior over clean labels, ignoring instance-level uncertainty. We propose a single-stage, EM-style framework for generative noisy-label learning that is \emph{direction-agnostic} and avoids explicit image synthesis. First, we derive a single Expectation-Maximization (EM) objective whose E-step specializes to either causal orientation without changing the overall optimization. Second, we replace the intractable \(p(X\mid Y)\) with a dataset-normalized discriminative proxy computed using a discriminative classifier on the finite training set, retaining the structural benefits of generative modeling at much lower cost. Third, we introduce \emph{Partial-Label Supervision} (PLS), an instance-specific prior over clean labels that balances coverage and uncertainty, improving data-dependent regularization. Across standard vision and natural language processing (NLP) noisy-label benchmarks, our method achieves state-of-the-art accuracy, lower transition-matrix estimation error, and substantially less training compute than current generative and discriminative baselines. Code: https://github.com/lfb-1/GNL
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SySMOL: Co-designing Algorithms and Hardware for Neural Networks with Heterogeneous Precisions</title>
<link>https://arxiv.org/abs/2311.14114</link>
<guid>https://arxiv.org/abs/2311.14114</guid>
<content:encoded><![CDATA[
arXiv:2311.14114v3 Announce Type: replace-cross 
Abstract: Ultra-low-precision inference can sharply reduce memory and latency but often degrades accuracy and relies on specialized hardware. We present SONIQ, a system-optimized, noise-injected quantization framework that learns per-channel mixed precision for both weights and activations while training under the same rules used at inference. By injecting hardware-calibrated quantization noise during training, SONIQ steers models toward the discrete arithmetic used at deployment -- without bespoke runtimes. Across CNNs and Transformers, SONIQ achieves up to 16x and 7x compression, respectively, while matching or exceeding full-precision accuracy. Measured end-to-end, SONIQ delivers up to 7.3x CPU speedup over strong INT8 baselines and up to 6.3x (vector units) / 2.8x (tensor cores) GPU speedup relative to FP16. A practical outcome is that two per-channel precision levels -- one in the 1--4-bit range and one in the 4--8-bit range -- suffice in practice; at inference, each channel selects one of the two, keeping kernels simple and fast. To our knowledge, SONIQ is the first framework to reach or surpass full-precision accuracy under ultra-low (1--4 bits per parameter) regimes while remaining deployable on commodity hardware, narrowing the gap between quantization theory and practical, high-throughput inference.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EERO: Early Exit with Reject Option for Efficient Classification with limited budget</title>
<link>https://arxiv.org/abs/2402.03779</link>
<guid>https://arxiv.org/abs/2402.03779</guid>
<content:encoded><![CDATA[
arXiv:2402.03779v2 Announce Type: replace-cross 
Abstract: The increasing complexity of advanced machine learning models requires innovative approaches to manage computational resources effectively. One such method is the Early Exit strategy, which allows for adaptive computation by providing a mechanism to shorten the processing path for simpler data instances. In this paper, we propose EERO, a new methodology to translate the problem of early exiting to a problem of using multiple classifiers with reject option in order to better select the exiting head for each instance. We calibrate the probabilities of exiting at the different heads using aggregation with exponential weights to guarantee a fixed budget .We consider factors such as Bayesian risk, budget constraints, and head-specific budget consumption. Experimental results, conducted using a ResNet-18 model and a ConvNext architecture on Cifar and ImageNet datasets, demonstrate that our method not only effectively manages budget allocation but also enhances accuracy in overthinking scenarios.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond State Space Representation: A General Theory for Kernel Packets</title>
<link>https://arxiv.org/abs/2402.04022</link>
<guid>https://arxiv.org/abs/2402.04022</guid>
<content:encoded><![CDATA[
arXiv:2402.04022v5 Announce Type: replace-cross 
Abstract: Gaussian process (GP) regression provides a flexible, nonparametric framework for probabilistic modeling, yet remains computationally demanding in large-scale applications. For one-dimensional data, state space (SS) models achieve linear-time inference by reformulating GPs as stochastic differential equations (SDEs). However, SS approaches are confined to gridded inputs and cannot handle multi-dimensional scattered data. We propose a new framework based on kernel packet (KP), which overcomes these limitations while retaining exactness and scalability. A KP is a compactly supported function defined as a linear combination of the GP covariance functions. In this article, we prove that KPs can be identified via the forward and backward SS representations. We also show that the KP approach enables exact inference with linear-time training and logarithmic or constant-time prediction, and extends naturally to multi-dimensional gridded or scattered data without low-rank approximations. Numerical experiments on large-scale additive and product-form GPs with millions of samples demonstrate that KPs achieve exact, memory-efficient inference where SDE-based and low-rank GP methods fail.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Projection Methods for Operator Learning and Universal Approximation</title>
<link>https://arxiv.org/abs/2406.12264</link>
<guid>https://arxiv.org/abs/2406.12264</guid>
<content:encoded><![CDATA[
arXiv:2406.12264v3 Announce Type: replace-cross 
Abstract: We obtain a new universal approximation theorem for continuous (possibly nonlinear) operators on arbitrary Banach spaces using the Leray-Schauder mapping. Moreover, we introduce and study a method for operator learning in Banach spaces $L^p$ of functions with multiple variables, based on orthogonal projections on polynomial bases. We derive a universal approximation result for operators where we learn a linear projection and a finite dimensional mapping under some additional assumptions. For the case of $p=2$, we give some sufficient conditions for the approximation results to hold. This article serves as the theoretical framework for a deep learning methodology in operator learning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users</title>
<link>https://arxiv.org/abs/2406.17737</link>
<guid>https://arxiv.org/abs/2406.17737</guid>
<content:encoded><![CDATA[
arXiv:2406.17737v2 Announce Type: replace-cross 
Abstract: While state-of-the-art large language models (LLMs) have shown impressive performance on many tasks, there has been extensive research on undesirable model behavior such as hallucinations and bias. In this work, we investigate how the quality of LLM responses changes in terms of information accuracy, truthfulness, and refusals depending on three user traits: English proficiency, education level, and country of origin. We present extensive experimentation on three state-of-the-art LLMs and two different datasets targeting truthfulness and factuality. Our findings suggest that undesirable behaviors in state-of-the-art LLMs occur disproportionately more for users with lower English proficiency, of lower education status, and originating from outside the US, rendering these models unreliable sources of information towards their most vulnerable users.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measure-Theoretic Time-Delay Embedding</title>
<link>https://arxiv.org/abs/2409.08768</link>
<guid>https://arxiv.org/abs/2409.08768</guid>
<content:encoded><![CDATA[
arXiv:2409.08768v2 Announce Type: replace-cross 
Abstract: The celebrated Takens' embedding theorem provides a theoretical foundation for reconstructing the full state of a dynamical system from partial observations. However, the classical theorem assumes that the underlying system is deterministic and that observations are noise-free, limiting its applicability in real-world scenarios. Motivated by these limitations, we formulate a measure-theoretic generalization that adopts an Eulerian description of the dynamics and recasts the embedding as a pushforward map between spaces of probability measures. Our mathematical results leverage recent advances in optimal transport. Building on the proposed measure-theoretic time-delay embedding theory, we develop a computational procedure that aims to reconstruct the full state of a dynamical system from time-lagged partial observations, engineered with robustness to handle sparse and noisy data. We evaluate our measure-based approach across several numerical examples, ranging from the classic Lorenz-63 system to real-world applications such as NOAA sea surface temperature reconstruction and ERA5 wind field reconstruction.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Residual Kolmogorov-Arnold Network for Enhanced Deep Learning</title>
<link>https://arxiv.org/abs/2410.05500</link>
<guid>https://arxiv.org/abs/2410.05500</guid>
<content:encoded><![CDATA[
arXiv:2410.05500v4 Announce Type: replace-cross 
Abstract: Despite their immense success, deep convolutional neural networks (CNNs) can be difficult to optimize and costly to train due to hundreds of layers within the network depth. Conventional convolutional operations are fundamentally limited by their linear nature along with fixed activations, where many layers are needed to learn meaningful patterns in data. Because of the sheer size of these networks, this approach is simply computationally inefficient, and poses overfitting or gradient explosion risks, especially in small datasets. As a result, we introduce a "plug-in" module, called Residual Kolmogorov-Arnold Network (RKAN). Our module is highly compact, so it can be easily added into any stage (level) of traditional deep networks, where it learns to integrate supportive polynomial feature transformations to existing convolutional frameworks. RKAN offers consistent improvements over baseline models in different vision tasks and widely tested benchmarks, accomplishing cutting-edge performance on them.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QCircuitBench: A Large-Scale Dataset for Benchmarking Quantum Algorithm Design</title>
<link>https://arxiv.org/abs/2410.07961</link>
<guid>https://arxiv.org/abs/2410.07961</guid>
<content:encoded><![CDATA[
arXiv:2410.07961v2 Announce Type: replace-cross 
Abstract: Quantum computing is an emerging field recognized for the significant speedup it offers over classical computing through quantum algorithms. However, designing and implementing quantum algorithms pose challenges due to the complex nature of quantum mechanics and the necessity for precise control over quantum states. Despite the significant advancements in AI, there has been a lack of datasets specifically tailored for this purpose. In this work, we introduce QCircuitBench, the first benchmark dataset designed to evaluate AI's capability in designing and implementing quantum algorithms using quantum programming languages. Unlike using AI for writing traditional codes, this task is fundamentally more complicated due to highly flexible design space. Our key contributions include: 1. A general framework which formulates the key features of quantum algorithm design for Large Language Models. 2. Implementations for quantum algorithms from basic primitives to advanced applications, spanning 3 task suites, 25 algorithms, and 120,290 data points. 3. Automatic validation and verification functions, allowing for iterative evaluation and interactive reasoning without human inspection. 4. Promising potential as a training dataset through preliminary fine-tuning results. We observed several interesting experimental phenomena: LLMs tend to exhibit consistent error patterns, and fine-tuning does not always outperform few-shot learning. In all, QCircuitBench is a comprehensive benchmark for LLM-driven quantum algorithm design, and it reveals limitations of LLMs in this domain.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dispersion based Recurrent Neural Network Model for Methane Monitoring in Albertan Tailings Ponds</title>
<link>https://arxiv.org/abs/2411.06741</link>
<guid>https://arxiv.org/abs/2411.06741</guid>
<content:encoded><![CDATA[
arXiv:2411.06741v2 Announce Type: replace-cross 
Abstract: Bitumen extraction for the production of synthetic crude oil in Canada's Athabasca Oil Sands industry has recently come under spotlight for being a significant source of greenhouse gas emission. A major cause of concern is methane, a greenhouse gas produced by the anaerobic biodegradation of hydrocarbons in oil sands residues, or tailings, stored in settle basins commonly known as oil sands tailing ponds. In order to determine the methane emitting potential of these tailing ponds and have future methane projections, we use real-time weather data, mechanistic models developed from laboratory controlled experiments, and industrial reports to train a physics constrained machine learning model. Our trained model can successfully identify the directions of active ponds and estimate their emission levels, which are generally hard to obtain due to data sampling restrictions. We found that each active oil sands tailing pond could emit between 950 to 1500 tonnes of methane per year, whose environmental impact is equivalent to carbon dioxide emissions from at least 6000 gasoline powered vehicles. Although abandoned ponds are often presumed to have insignificant emissions, our findings indicate that these ponds could become active over time and potentially emit up to 1000 tonnes of methane each year. Taking an average over all datasets that was used in model training, we estimate that emissions around major oil sands regions would need to be reduced by approximately 12% over a year, to reduce the average methane concentrations to 2005 levels.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIMESAFE: Timing Interruption Monitoring and Security Assessment for Fronthaul Environments</title>
<link>https://arxiv.org/abs/2412.13049</link>
<guid>https://arxiv.org/abs/2412.13049</guid>
<content:encoded><![CDATA[
arXiv:2412.13049v2 Announce Type: replace-cross 
Abstract: 5G and beyond cellular systems embrace the disaggregation of Radio Access Network (RAN) components, exemplified by the evolution of the fronthaul (FH) connection between cellular baseband and radio unit equipment. Crucially, synchronization over the FH is pivotal for reliable 5G services. In recent years, there has been a push to move these links to an Ethernet-based packet network topology, leveraging existing standards and ongoing research for Time-Sensitive Networking (TSN). However, TSN standards, such as Precision Time Protocol (PTP), focus on performance with little to no concern for security. This increases the exposure of the open FH to security risks. Attacks targeting synchronization mechanisms pose significant threats, potentially disrupting 5G networks and impairing connectivity.
  In this paper, we demonstrate the impact of successful spoofing and replay attacks against PTP synchronization. We show how a spoofing attack is able to cause a production-ready O-RAN and 5G-compliant private cellular base station to catastrophically fail within 2 seconds of the attack, necessitating manual intervention to restore full network operations. To counter this, we design a Machine Learning (ML)-based monitoring solution capable of detecting various malicious attacks with over 97.5% accuracy.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>coverforest: Conformal Predictions with Random Forest in Python</title>
<link>https://arxiv.org/abs/2501.14570</link>
<guid>https://arxiv.org/abs/2501.14570</guid>
<content:encoded><![CDATA[
arXiv:2501.14570v2 Announce Type: replace-cross 
Abstract: Conformal prediction provides a framework for uncertainty quantification, specifically in the forms of prediction intervals and sets with distribution-free guaranteed coverage. While recent cross-conformal techniques such as CV+ and Jackknife+-after-bootstrap achieve better data efficiency than traditional split conformal methods, they incur substantial computational costs due to required pairwise comparisons between training and test samples' out-of-bag scores. Observing that these methods naturally extend from ensemble models, particularly random forests, we leverage existing optimized random forest implementations to enable efficient cross-conformal predictions.
  We present coverforest, a Python package that implements efficient conformal prediction methods specifically optimized for random forests. coverforest supports both regression and classification tasks through various conformal prediction methods, including split conformal, CV+, Jackknife+-after-bootstrap, and adaptive prediction sets. Our package leverages parallel computing and Cython optimizations to speed up out-of-bag calculations. Our experiments demonstrate that coverforest's predictions achieve the desired level of coverage. In addition, its training and prediction times can be faster than an existing implementation by 2--9 times. The source code for the coverforest is hosted on GitHub at https://github.com/donlap/coverforest.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KGGen: Extracting Knowledge Graphs from Plain Text with Language Models</title>
<link>https://arxiv.org/abs/2502.09956</link>
<guid>https://arxiv.org/abs/2502.09956</guid>
<content:encoded><![CDATA[
arXiv:2502.09956v2 Announce Type: replace-cross 
Abstract: Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP techniques. While human-generated KGs are in short supply, automatically extracted KGs are of questionable quality. We present a solution to this data scarcity problem in the form of a text-to-KG generator (KGGen), a package that uses language models to create high-quality graphs from plaintext. Unlike other KG extractors, KGGen clusters related entities to reduce sparsity in extracted KGs. KGGen is available as a Python library (\texttt{pip install kg-gen}), making it accessible to everyone. Along with KGGen, we release the first benchmark, Measure of of Information in Nodes and Edges (MINE), that tests an extractor's ability to produce a useful KG from plain text. We benchmark our new tool against existing extractors and demonstrate far superior performance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Datasets, Documents, and Repetitions: The Practicalities of Unequal Data Quality</title>
<link>https://arxiv.org/abs/2503.07879</link>
<guid>https://arxiv.org/abs/2503.07879</guid>
<content:encoded><![CDATA[
arXiv:2503.07879v2 Announce Type: replace-cross 
Abstract: Data filtering has become a powerful tool for improving model performance while reducing computational cost. However, as large language model compute budgets continue to grow, the limited data volume provided by heavily filtered and deduplicated datasets will become a practical constraint. In efforts to better understand how to proceed, we study model performance at various compute budgets and across multiple pre-training datasets created through data filtering and deduplication. We find that, given appropriate modifications to the training recipe, repeating existing aggressively filtered datasets for up to ten epochs can outperform training on the ten times larger superset for a single epoch across multiple compute budget orders of magnitude. While this finding relies on repeating the dataset for many epochs, we also investigate repeats within these datasets at the document level. We find that not all documents within a dataset are equal, and we can create better datasets relative to a token budget by explicitly manipulating the counts of individual documents. We conclude by arguing that even as large language models scale, data filtering remains an important direction of research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Model Development through Fine-tuning Transfer</title>
<link>https://arxiv.org/abs/2503.20110</link>
<guid>https://arxiv.org/abs/2503.20110</guid>
<content:encoded><![CDATA[
arXiv:2503.20110v2 Announce Type: replace-cross 
Abstract: Modern LLMs struggle with efficient updates, as each new pretrained model version requires repeating expensive alignment processes. This challenge also applies to domain- or languagespecific models, where fine-tuning on specialized data must be redone for every new base model release. In this paper, we explore the transfer of fine-tuning updates between model versions. Specifically, we derive the diff vector (representing the weight changes from finetuning) from one source model version and apply it to the base model of a different target version. Through empirical evaluations on various open-weight model versions, we show that transferring diff vectors can significantly improve the performance of the target base model. For example, transferring the fine-tuning updates from Llama 3.0 8B improves Llama 3.1 8B by 46.9% on IFEval and 15.7% on LiveCodeBench without additional training, even surpassing Llama 3.1 8B Instruct. Furthermore, we demonstrate performance gains on multilingual tasks, with 4.7% and 15.5% improvements on Global MMLU for Malagasy and Turkish, respectively. We observe that these merged models provide stronger initializations for further fine-tuning. Lastly, our controlled experiments suggest that fine-tuning transfer is most effective when source and target models lie in a linearly connected region of parameter space, and we provide a theoretical analysis of our method. Taken together, fine-tuning transfer offers a cost-efficient and practical strategy for continuous LLM development. Our code is available at github.com/pjlintw/finetuning-transfer.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context</title>
<link>https://arxiv.org/abs/2504.04737</link>
<guid>https://arxiv.org/abs/2504.04737</guid>
<content:encoded><![CDATA[
arXiv:2504.04737v2 Announce Type: replace-cross 
Abstract: In the landscape of Fact-based Judgment Prediction and Explanation (FJPE), reliance on factual data is essential for developing robust and realistic AI-driven decision-making tools. This paper introduces TathyaNyaya, the largest annotated dataset for FJPE tailored to the Indian legal context, encompassing judgments from the Supreme Court of India and various High Courts. Derived from the Hindi terms "Tathya" (fact) and "Nyaya" (justice), the TathyaNyaya dataset is uniquely designed to focus on factual statements rather than complete legal texts, reflecting real-world judicial processes where factual data drives outcomes. Complementing this dataset, we present FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM), optimized for generating high-quality explanations in FJPE tasks. Finetuned on the factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy with coherent, contextually relevant explanations, addressing the critical need for transparency and interpretability in AI-assisted legal systems. Our methodology combines transformers for binary judgment prediction with FactLegalLlama for explanation generation, creating a robust framework for advancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses existing datasets in scale and diversity but also establishes a benchmark for building explainable AI systems in legal analysis. The findings underscore the importance of factual precision and domain-specific tuning in enhancing predictive performance and interpretability, positioning TathyaNyaya and FactLegalLlama as foundational resources for AI-assisted legal decision-making.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DashCLIP: Leveraging multimodal models for generating semantic embeddings for DoorDash</title>
<link>https://arxiv.org/abs/2504.07110</link>
<guid>https://arxiv.org/abs/2504.07110</guid>
<content:encoded><![CDATA[
arXiv:2504.07110v2 Announce Type: replace-cross 
Abstract: Despite the success of vision-language models in various generative tasks, obtaining high-quality semantic representations for products and user intents is still challenging due to the inability of off-the-shelf models to capture nuanced relationships between the entities. In this paper, we introduce a joint training framework for product and user queries by aligning uni-modal and multi-modal encoders through contrastive learning on image-text data. Our novel approach trains a query encoder with an LLM-curated relevance dataset, eliminating the reliance on engagement history. These embeddings demonstrate strong generalization capabilities and improve performance across applications, including product categorization and relevance prediction. For personalized ads recommendation, a significant uplift in the click-through rate and conversion rate after the deployment further confirms the impact on key business metrics. We believe that the flexibility of our framework makes it a promising solution toward enriching the user experience across the e-commerce landscape.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Chest X-ray with Zero-Shot Multi-Task Capability</title>
<link>https://arxiv.org/abs/2504.07416</link>
<guid>https://arxiv.org/abs/2504.07416</guid>
<content:encoded><![CDATA[
arXiv:2504.07416v3 Announce Type: replace-cross 
Abstract: Recent advancements in multimodal models have significantly improved vision-language (VL) alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning and offer limited interpretability through attention probability visualizations. To address these challenges, we introduce $\textbf{RadZero}$, a novel framework for VL alignment in chest X-ray with zero-shot multi-task capability. A key component of our approach is $\textbf{VL-CABS}$ ($\textbf{V}$ision-$\textbf{L}$anguage $\textbf{C}$ross-$\textbf{A}$ttention $\textbf{B}$ased on $\textbf{S}$imilarity), which aligns text embeddings with local image features for interpretable, fine-grained VL reasoning. RadZero leverages large language models to extract concise semantic sentences from radiology reports and employs multi-positive contrastive training to effectively capture relationships between images and multiple relevant textual descriptions. It uses a pre-trained vision encoder with additional trainable Transformer layers, allowing efficient high-resolution image processing. By computing similarity between text embeddings and local image patch features, VL-CABS enables zero-shot inference with similarity probability for classification, and pixel-level VL similarity maps for grounding and segmentation. Experimental results on public chest radiograph benchmarks show that RadZero outperforms state-of-the-art methods in zero-shot classification, grounding, and segmentation. Furthermore, VL similarity map analysis highlights the potential of VL-CABS for improving explainability in VL alignment. Additionally, qualitative evaluation demonstrates RadZero's capability for open-vocabulary semantic segmentation, further validating its effectiveness in medical imaging. Code is available at $\href{https://github.com/deepnoid-ai/RadZero}{https://github.com/deepnoid-ai/RadZero}$.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics</title>
<link>https://arxiv.org/abs/2505.18658</link>
<guid>https://arxiv.org/abs/2505.18658</guid>
<content:encoded><![CDATA[
arXiv:2505.18658v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have emerged as a promising cornerstone for the development of natural language processing (NLP) and artificial intelligence (AI). However, ensuring the robustness of LLMs remains a critical challenge. To address these challenges and advance the field, this survey provides a comprehensive overview of current studies in this area. First, we systematically examine the nature of robustness in LLMs, including its conceptual foundations, the importance of consistent performance across diverse inputs, and the implications of failure modes in real-world applications. Next, we analyze the sources of non-robustness, categorizing intrinsic model limitations, data-driven vulnerabilities, and external adversarial factors that compromise reliability. Following this, we review state-of-the-art mitigation strategies, and then we discuss widely adopted benchmarks, emerging metrics, and persistent gaps in assessing real-world reliability. Finally, we synthesize findings from existing surveys and interdisciplinary studies to highlight trends, unresolved issues, and pathways for future research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables</title>
<link>https://arxiv.org/abs/2505.19470</link>
<guid>https://arxiv.org/abs/2505.19470</guid>
<content:encoded><![CDATA[
arXiv:2505.19470v2 Announce Type: replace-cross 
Abstract: Latent variables (LVs) play a crucial role in encoder-decoder models by enabling effective data compression, prediction, and generation. Although their theoretical properties, such as generalization, have been extensively studied in supervised learning, similar analyses for unsupervised models such as variational autoencoders (VAEs) remain insufficiently underexplored. In this work, we extend information-theoretic generalization analysis to vector-quantized (VQ) VAEs with discrete latent spaces, introducing a novel data-dependent prior to rigorously analyze the relationship among LVs, generalization, and data generation. We derive a novel generalization error bound of the reconstruction loss of VQ-VAEs, which depends solely on the complexity of LVs and the encoder, independent of the decoder. Additionally, we provide the upper bound of the 2-Wasserstein distance between the distributions of the true data and the generated data, explaining how the regularization of the LVs contributes to the data generation performance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Higher-Order Singular-Value Derivatives of Rectangular Real Matrices</title>
<link>https://arxiv.org/abs/2506.03764</link>
<guid>https://arxiv.org/abs/2506.03764</guid>
<content:encoded><![CDATA[
arXiv:2506.03764v4 Announce Type: replace-cross 
Abstract: We present a theoretical framework for deriving the general $n$-th order Fr\'echet derivatives of singular values in real rectangular matrices, by leveraging reduced resolvent operators from Kato's analytic perturbation theory for self-adjoint operators. Deriving closed-form expressions for higher-order derivatives of singular values is notoriously challenging through standard matrix-analysis techniques. To overcome this, we treat a real rectangular matrix as a compact operator on a finite-dimensional Hilbert space, and embed the rectangular matrix into a block self-adjoint operator so that non-symmetric perturbations are captured. Applying Kato's asymptotic eigenvalue expansion to this construction, we obtain a general, closed-form expression for the infinitesimal $n$-th order spectral variations. Specializing to $n=2$ and deploying on a Kronecker-product representation with matrix convention yield the Hessian of a singular value, not found in literature. By bridging abstract operator-theoretic perturbation theory with matrices, our framework equips researchers with a practical toolkit for higher-order spectral sensitivity studies in random matrix applications (e.g., adversarial perturbation in deep learning).
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos</title>
<link>https://arxiv.org/abs/2506.15680</link>
<guid>https://arxiv.org/abs/2506.15680</guid>
<content:encoded><![CDATA[
arXiv:2506.15680v2 Announce Type: replace-cross 
Abstract: Modeling the dynamics of deformable objects is challenging due to their diverse physical properties and the difficulty of estimating states from limited visual information. We address these challenges with a neural dynamics framework that combines object particles and spatial grids in a hybrid representation. Our particle-grid model captures global shape and motion information while predicting dense particle movements, enabling the modeling of objects with varied shapes and materials. Particles represent object shapes, while the spatial grid discretizes the 3D space to ensure spatial continuity and enhance learning efficiency. Coupled with Gaussian Splattings for visual rendering, our framework achieves a fully learning-based digital twin of deformable objects and generates 3D action-conditioned videos. Through experiments, we demonstrate that our model learns the dynamics of diverse objects -- such as ropes, cloths, stuffed animals, and paper bags -- from sparse-view RGB-D recordings of robot-object interactions, while also generalizing at the category level to unseen instances. Our approach outperforms state-of-the-art learning-based and physics-based simulators, particularly in scenarios with limited camera views. Furthermore, we showcase the utility of our learned models in model-based planning, enabling goal-conditioned object manipulation across a range of tasks. The project page is available at https://kywind.github.io/pgnd .
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VERA: Variational Inference Framework for Jailbreaking Large Language Models</title>
<link>https://arxiv.org/abs/2506.22666</link>
<guid>https://arxiv.org/abs/2506.22666</guid>
<content:encoded><![CDATA[
arXiv:2506.22666v2 Announce Type: replace-cross 
Abstract: The rise of API-only access to state-of-the-art LLMs highlights the need for effective black-box jailbreak methods to identify model vulnerabilities in real-world settings. Without a principled objective for gradient-based optimization, most existing approaches rely on genetic algorithms, which are limited by their initialization and dependence on manually curated prompt pools. Furthermore, these methods require individual optimization for each prompt, failing to provide a comprehensive characterization of model vulnerabilities. To address this gap, we introduce VERA: Variational infErence fRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a variational inference problem, training a small attacker LLM to approximate the target LLM's posterior over adversarial prompts. Once trained, the attacker can generate diverse, fluent jailbreak prompts for a target query without re-optimization. Experimental results show that VERA achieves strong performance across a range of target LLMs, highlighting the value of probabilistic inference for adversarial prompt generation.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation</title>
<link>https://arxiv.org/abs/2506.23717</link>
<guid>https://arxiv.org/abs/2506.23717</guid>
<content:encoded><![CDATA[
arXiv:2506.23717v3 Announce Type: replace-cross 
Abstract: Multi-bit spiking neural networks (SNNs) have recently become a heated research spot, pursuing energy-efficient and high-accurate AI. However, with more bits involved, the associated memory and computation demands escalate to the point where the performance improvements become disproportionate. Based on the insight that different layers demonstrate different importance and extra bits could be wasted and interfering, this paper presents an adaptive bit allocation strategy for direct-trained SNNs, achieving fine-grained layer-wise allocation of memory and computation resources. Thus, SNN's efficiency and accuracy can be improved. Specifically, we parametrize the temporal lengths and the bit widths of weights and spikes, and make them learnable and controllable through gradients. To address the challenges caused by changeable bit widths and temporal lengths, we propose the refined spiking neuron, which can handle different temporal lengths, enable the derivation of gradients for temporal lengths, and suit spike quantization better. In addition, we theoretically formulate the step-size mismatch problem of learnable bit widths, which may incur severe quantization errors to SNN, and accordingly propose the step-size renewal mechanism to alleviate this issue. Experiments on various datasets, including the static CIFAR and ImageNet datasets and the dynamic CIFAR-DVS, DVS-GESTURE, and SHD datasets, demonstrate that our methods can reduce the overall memory and computation cost while achieving higher accuracy. Particularly, our SEWResNet-34 can achieve a 2.69% accuracy gain and 4.16x lower bit budgets over the advanced baseline work on ImageNet. This work will be open-sourced.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory</title>
<link>https://arxiv.org/abs/2507.01110</link>
<guid>https://arxiv.org/abs/2507.01110</guid>
<content:encoded><![CDATA[
arXiv:2507.01110v3 Announce Type: replace-cross 
Abstract: Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Fourier Neural Operators for periodic homogenization problems in linear elasticity</title>
<link>https://arxiv.org/abs/2507.12233</link>
<guid>https://arxiv.org/abs/2507.12233</guid>
<content:encoded><![CDATA[
arXiv:2507.12233v3 Announce Type: replace-cross 
Abstract: Solving cell problems in homogenization is hard, and available deep-learning frameworks fail to match the speed and generality of traditional computational frameworks. More to the point, it is generally unclear what to expect of machine-learning approaches, let alone single out which approaches are promising. In the work at hand, we advocate Fourier Neural Operators (FNOs) for micromechanics, empowering them by insights from computational micromechanics methods based on the fast Fourier transform (FFT). We construct an FNO surrogate mimicking the basic scheme foundational for FFT-based methods and show that the resulting operator predicts solutions to cell problems with arbitrary stiffness distribution only subject to a material-contrast constraint up to a desired accuracy. In particular, there are no restrictions on the material symmetry like isotropy, on the number of phases and on the geometry of the interfaces between materials. Also, the provided fidelity is sharp and uniform, providing explicit guarantees leveraging our physical empowerment of FNOs. To show the desired universal approximation property, we construct an FNO explicitly that requires no training to begin with. Still, the obtained neural operator complies with the same memory requirements as the basic scheme and comes with runtimes proportional to classical FFT solvers. In particular, large-scale problems with more than 100 million voxels are readily handled. The goal of this work is to underline the potential of FNOs for solving micromechanical problems, linking FFT-based methods to FNOs. This connection is expected to provide a fruitful exchange between both worlds.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System</title>
<link>https://arxiv.org/abs/2508.00709</link>
<guid>https://arxiv.org/abs/2508.00709</guid>
<content:encoded><![CDATA[
arXiv:2508.00709v2 Announce Type: replace-cross 
Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery</title>
<link>https://arxiv.org/abs/2509.08027</link>
<guid>https://arxiv.org/abs/2509.08027</guid>
<content:encoded><![CDATA[
arXiv:2509.08027v2 Announce Type: replace-cross 
Abstract: This work presents a new dataset for the Martian digital elevation model prediction task, ready for machine learning applications called MCTED. The dataset has been generated using a comprehensive pipeline designed to process high-resolution Mars orthoimage and DEM pairs from Day et al., yielding a dataset consisting of 80,898 data samples. The source images are data gathered by the Mars Reconnaissance Orbiter using the CTX instrument, providing a very diverse and comprehensive coverage of the Martian surface. Given the complexity of the processing pipelines used in large-scale DEMs, there are often artefacts and missing data points in the original data, for which we developed tools to solve or mitigate their impact. We divide the processed samples into training and validation splits, ensuring samples in both splits cover no mutual areas to avoid data leakage. Every sample in the dataset is represented by the optical image patch, DEM patch, and two mask patches, indicating values that were originally missing or were altered by us. This allows future users of the dataset to handle altered elevation regions as they please. We provide statistical insights of the generated dataset, including the spatial distribution of samples, the distributions of elevation values, slopes and more. Finally, we train a small U-Net architecture on the MCTED dataset and compare its performance to a monocular depth estimation foundation model, DepthAnythingV2, on the task of elevation prediction. We find that even a very small architecture trained on this dataset specifically, beats a zero-shot performance of a depth estimation foundation model like DepthAnythingV2. We make the dataset and code used for its generation completely open source in public repositories.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLOWR.root: A flow matching based foundation model for joint multi-purpose structure-aware 3D ligand generation and affinity prediction</title>
<link>https://arxiv.org/abs/2510.02578</link>
<guid>https://arxiv.org/abs/2510.02578</guid>
<content:encoded><![CDATA[
arXiv:2510.02578v3 Announce Type: replace-cross 
Abstract: We present FLOWR:root, an equivariant flow-matching model for pocket-aware 3D ligand generation with joint binding affinity prediction and confidence estimation. The model supports de novo generation, pharmacophore-conditional sampling, fragment elaboration, and multi-endpoint affinity prediction (pIC50, pKi, pKd, pEC50). Training combines large-scale ligand libraries with mixed-fidelity protein-ligand complexes, followed by refinement on curated co-crystal datasets and parameter-efficient finetuning for project-specific adaptation. FLOWR:root achieves state-of-the-art performance in unconditional 3D molecule generation and pocket-conditional ligand design, producing geometrically realistic, low-strain structures. The integrated affinity prediction module demonstrates superior accuracy on the SPINDR test set and outperforms recent models on the Schrodinger FEP+/OpenFE benchmark with substantial speed advantages. As a foundation model, FLOWR:root requires finetuning on project-specific datasets to account for unseen structure-activity landscapes, yielding strong correlation with experimental data. Joint generation and affinity prediction enable inference-time scaling through importance sampling, steering molecular design toward higher-affinity compounds. Case studies validate this: selective CK2$\alpha$ ligand generation against CLK3 shows significant correlation between predicted and quantum-mechanical binding energies, while ER$\alpha$, TYK2 and BACE1 scaffold elaboration demonstrates strong agreement with QM calculations. By integrating structure-aware generation, affinity estimation, and property-guided sampling, FLOWR:root provides a comprehensive foundation for structure-based drug design spanning hit identification through lead optimization.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Large Language Models To Reason In Parallel With Global Forking Tokens</title>
<link>https://arxiv.org/abs/2510.05132</link>
<guid>https://arxiv.org/abs/2510.05132</guid>
<content:encoded><![CDATA[
arXiv:2510.05132v2 Announce Type: replace-cross 
Abstract: Although LLMs have demonstrated improved performance by scaling parallel test-time compute, doing so relies on generating reasoning paths that are both diverse and accurate. For challenging problems, the forking tokens that trigger diverse yet correct reasoning modes are typically deep in the sampling tree. Consequently, common strategies to encourage diversity, such as temperature scaling, encounter a worsened trade-off between diversity and accuracy. Motivated by this challenge, we treat parallel reasoning as a set-of-next-token-prediction problem, and incorporate a set-based global loss into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching between our global forking tokens and unique reasoning traces. We observe that, while naive fine-tuning with multiple reasoning traces collapses these unique reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT), preserves these modes and produces emergent global forking tokens. Experiments on multiple reasoning benchmarks show that our SSFT consistently outperforms SFT under both Pass@1 and Cons@k metrics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Navigate Socially Through Proactive Risk Perception</title>
<link>https://arxiv.org/abs/2510.07871</link>
<guid>https://arxiv.org/abs/2510.07871</guid>
<content:encoded><![CDATA[
arXiv:2510.07871v2 Announce Type: replace-cross 
Abstract: In this report, we describe the technical details of our submission to the IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on developing RGBD-based perception and navigation systems that enable autonomous agents to navigate safely, efficiently, and socially compliantly in dynamic human-populated indoor environments. The challenge requires agents to operate from an egocentric perspective using only onboard sensors including RGB-D observations and odometry, without access to global maps or privileged information, while maintaining social norm compliance such as safe distances and collision avoidance. Building upon the Falcon model, we introduce a Proactive Risk Perception Module to enhance social navigation performance. Our approach augments Falcon with collision risk understanding that learns to predict distance-based collision risk scores for surrounding humans, which enables the agent to develop more robust spatial awareness and proactive collision avoidance behaviors. The evaluation on the Social-HM3D benchmark demonstrates that our method improves the agent's ability to maintain personal space compliance while navigating toward goals in crowded indoor scenes with dynamic human agents, achieving 2nd place among 16 participating teams in the challenge.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Evaluation of Self-Supervised Learning for Label-Efficient Sleep Staging with Wearable EEG</title>
<link>https://arxiv.org/abs/2510.07960</link>
<guid>https://arxiv.org/abs/2510.07960</guid>
<content:encoded><![CDATA[
arXiv:2510.07960v2 Announce Type: replace-cross 
Abstract: Wearable EEG devices have emerged as a promising alternative to polysomnography (PSG). As affordable and scalable solutions, their widespread adoption results in the collection of massive volumes of unlabeled data that cannot be analyzed by clinicians at scale. Meanwhile, the recent success of deep learning for sleep scoring has relied on large annotated datasets. Self-supervised learning (SSL) offers an opportunity to bridge this gap, leveraging unlabeled signals to address label scarcity and reduce annotation effort. In this paper, we present the first systematic evaluation of SSL for sleep staging using wearable EEG. We investigate a range of well-established SSL methods and evaluate them on two sleep databases acquired with the Ikon Sleep wearable EEG headband: BOAS, a high-quality benchmark containing PSG and wearable EEG recordings with consensus labels, and HOGAR, a large collection of home-based, self-recorded, and unlabeled recordings. Three evaluation scenarios are defined to study label efficiency, representation quality, and cross-dataset generalization. Results show that SSL consistently improves classification performance by up to 10% over supervised baselines, with gains particularly evident when labeled data is scarce. SSL achieves clinical-grade accuracy above 80% leveraging only 5% to 10% of labeled data, while the supervised approach requires twice the labels. Additionally, SSL representations prove robust to variations in population characteristics, recording environments, and signal quality. Our findings demonstrate the potential of SSL to enable label-efficient sleep staging with wearable EEG, reducing reliance on manual annotations and advancing the development of affordable sleep monitoring systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematics with large language models as provers and verifiers</title>
<link>https://arxiv.org/abs/2510.12829</link>
<guid>https://arxiv.org/abs/2510.12829</guid>
<content:encoded><![CDATA[
arXiv:2510.12829v3 Announce Type: replace-cross 
Abstract: During 2024 and 2025 the discussion about the theorem-proving capabilities of large language models started reporting interesting success stories, mostly to do with difficult exercises (such as problems from the International Mathematical Olympiad), but also with conjectures [Feldman & Karbasi, arXiv:2509.18383v1] formulated for the purpose of verifying whether the artificial intelligence could prove it. In this paper we report a theorem proving feat achieved by ChatGPT by using a protocol involving different prover and verifier instances of the gpt-5 model working collaboratively. To make sure that the produced proofs do not suffer from hallucinations, the final proof is formally verified by the lean proof assistant, and the conformance of premises and conclusion of the lean code is verified by a human. Our methodology is by no means complete or exact. It was nonetheless able to solve five out of six 2025 IMO problems, and close about a third of the sixty-six number theory conjectures in [Cohen, Journal of Integer Sequences, 2025].
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.17697</link>
<guid>https://arxiv.org/abs/2510.17697</guid>
<content:encoded><![CDATA[
arXiv:2510.17697v4 Announce Type: replace-cross 
Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards desired outcomes is challenging, particularly when the global guidance from a human on the whole multi-agent system is impractical in a large-scale MARL. On the other hand, designing external mechanisms (e.g., intrinsic rewards and human feedback) to coordinate agents mostly relies on empirical studies, lacking a easy-to-use research tool. In this work, we employ multi-agent influence diagrams (MAIDs) as a graphical framework to address the above issues. First, we introduce the concept of MARL interaction paradigms (orthogonal to MARL learning paradigms), using MAIDs to analyze and visualize both unguided self-organization and global guidance mechanisms in MARL. Then, we design a new MARL interaction paradigm, referred to as the targeted intervention paradigm that is applied to only a single targeted agent, so the problem of global guidance can be mitigated. In implementation, we introduce a causal inference technique, referred to as Pre-Strategy Intervention (PSI), to realize the targeted intervention paradigm. Since MAIDs can be regarded as a special class of causal diagrams, a composite desired outcome that integrates the primary task goal and an additional desired outcome can be achieved by maximizing the corresponding causal effect through the PSI. Moreover, the bundled relevance graph analysis of MAIDs provides a tool to identify whether an MARL learning paradigm is workable under the design of an MARL interaction paradigm. In experiments, we demonstrate the effectiveness of our proposed targeted intervention, and verify the result of relevance graph analysis.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraceTrans: Translation and Spatial Tracing for Surgical Prediction</title>
<link>https://arxiv.org/abs/2510.22379</link>
<guid>https://arxiv.org/abs/2510.22379</guid>
<content:encoded><![CDATA[
arXiv:2510.22379v3 Announce Type: replace-cross 
Abstract: Image-to-image translation models have achieved notable success in converting images across visual domains and are increasingly used for medical tasks such as predicting post-operative outcomes and modeling disease progression. However, most existing methods primarily aim to match the target distribution and often neglect spatial correspondences between the source and translated images. This limitation can lead to structural inconsistencies and hallucinations, undermining the reliability and interpretability of the predictions. These challenges are accentuated in clinical applications by the stringent requirement for anatomical accuracy. In this work, we present TraceTrans, a novel deformable image translation model designed for post-operative prediction that generates images aligned with the target distribution while explicitly revealing spatial correspondences with the pre-operative input. The framework employs an encoder for feature extraction and dual decoders for predicting spatial deformations and synthesizing the translated image. The predicted deformation field imposes spatial constraints on the generated output, ensuring anatomical consistency with the source. Extensive experiments on medical cosmetology and brain MRI datasets demonstrate that TraceTrans delivers accurate and interpretable post-operative predictions, highlighting its potential for reliable clinical deployment.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels</title>
<link>https://arxiv.org/abs/2511.02872</link>
<guid>https://arxiv.org/abs/2511.02872</guid>
<content:encoded><![CDATA[
<div> provers, benchmark, formal algebra, mathematical reasoning, performance gap
Summary:
The article introduces the FATE benchmark series in formal algebra to address the limitations of contest-based mathematical benchmarks like the IMO. It consists of two components, FATE-H and FATE-X, with 100 problems each in abstract and commutative algebra. The FATE series covers a wide spectrum of difficulty levels, surpassing PhD-level exams. Results show that state-of-the-art LLM provers struggle with FATE benchmarks, achieving low accuracy rates. The study reveals that while models excel in natural-language reasoning, they face challenges in formalizing this reasoning. Common errors in the formalization process are systematically classified. Comparisons show that specialized provers may perform less effectively than general-purpose models. Overall, FATE provides a challenging benchmark to gauge progress in advancing formal mathematical reasoning. 
<br /><br />Summary: <div>
arXiv:2511.02872v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have demonstrated impressive capabilities in formal theorem proving, particularly on contest-based mathematical benchmarks like the IMO. However, these contests do not reflect the depth, breadth, and abstraction of modern mathematical research. To bridge this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new benchmark series in formal algebra designed to chart a course toward advanced mathematical reasoning. We present two new components, FATE-H and FATE-X, each with 100 problems in abstract and commutative algebra. The FATE series spans a difficulty spectrum from undergraduate exercises to problems exceeding PhD qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both PhD-level exam difficulty and the coverage of the Mathlib library. Our evaluations of state-of-the-art LLM provers on this new benchmark reveal a stark performance gap compared to contest math: the best model achieves only 3% (pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals that models' natural-language reasoning is notably more accurate than their ability to formalize this reasoning. We systematically classify the common errors that arise during this formalization process. Furthermore, a comparative study shows that a specialized prover can exhibit less effective reflection than general-purpose models, reducing its accuracy at the natural-language stage. We believe FATE provides a robust and challenging benchmark that establishes essential checkpoints on the path toward research-level formal mathematical reasoning.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Deep Graph Clustering for Practical Group Formation</title>
<link>https://arxiv.org/abs/2511.02879</link>
<guid>https://arxiv.org/abs/2511.02879</guid>
<content:encoded><![CDATA[
<div> recommendation accuracy, group formation, group recommender systems, DeepForm, dynamic adjustment <br />
Summary:
DeepForm is a novel framework for group recommender systems that focuses on group formation in dynamic scenarios. It addresses the limitations of previous approaches by incorporating high-order user information, enabling real-time group formation, and allowing for the dynamic adjustment of group numbers. The framework utilizes a lightweight GCN architecture to capture high-order structural signals effectively. It employs stochastic cluster learning to facilitate adaptive group reconfiguration without the need for retraining and contrastive learning to refine groups under changing conditions. Experimental results on various datasets show that DeepForm outperforms existing baselines in terms of group formation quality, efficiency, and recommendation accuracy. By emphasizing dynamic group formation, DeepForm offers a promising solution for practical applications of group recommender systems. <br /><br /> <div>
arXiv:2511.02879v1 Announce Type: new 
Abstract: While prior work on group recommender systems (GRSs) has primarily focused on improving recommendation accuracy, most approaches assume static or predefined groups, making them unsuitable for dynamic, real-world scenarios. We reframe group formation as a core challenge in GRSs and propose DeepForm (Stochastic Deep Graph Clustering for Practical Group Formation), a framework designed to meet three key operational requirements: (1) the incorporation of high-order user information, (2) real-time group formation, and (3) dynamic adjustment of the number of groups. DeepForm employs a lightweight GCN architecture that effectively captures high-order structural signals. Stochastic cluster learning enables adaptive group reconfiguration without retraining, while contrastive learning refines groups under dynamic conditions. Experiments on multiple datasets demonstrate that DeepForm achieves superior group formation quality, efficiency, and recommendation accuracy compared with various baselines.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-time Adaptation of Tiny Recursive Models</title>
<link>https://arxiv.org/abs/2511.02886</link>
<guid>https://arxiv.org/abs/2511.02886</guid>
<content:encoded><![CDATA[
<div> pre-training, fine-tuning, recursive neural network, ARC tasks, compute limits
Summary:
The paper introduces a new approach, TRM (Tiny Recursive Models), for the 2025 ARC Prize competition. The TRM approach involves pre-training a tiny model on public ARC tasks before fine-tuning it on competition tasks within the allowed compute limits. The pre-trained model achieved a 10% score on the public evaluation set and was post-trained during the competition to reach a score of 6.67% on semi-private evaluation tasks. This post-training performance was achieved through full-fine tuning of the tiny model, not LoRA fine-tuning or fine-tuning of task embeddings alone. The approach demonstrates the efficiency of starting with a pre-trained tiny model for ARC tasks and fine-tuning it to achieve competitive performance within the competition constraints. <div>
arXiv:2511.02886v1 Announce Type: new 
Abstract: Prior to the close of the 2025 ARC Prize competition, the leading open source approach - known as TRM, or Tiny Recursive Models - involved training a 7M parameter recursive neural network on augmented variants of ARC tasks. That approach scored approximately 7.8% on the public ARC AGI II evaluation set, but required a level of compute far in excess of what is allowed during the competition. This paper shows that, by starting from a tiny recursive model that has been pre-trained on public ARC tasks, one can efficiently fine-tune on competition tasks within the allowed compute limits. Specifically, a model was pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on 4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model was then post-trained in just 12,500 gradient steps during the competition to reach a score of 6.67% on semi-private evaluation tasks. Notably, such post-training performance is achieved by full-fine tuning of the tiny model, not LoRA fine-tuning or fine-tuning of task embeddings alone.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets</title>
<link>https://arxiv.org/abs/2511.02887</link>
<guid>https://arxiv.org/abs/2511.02887</guid>
<content:encoded><![CDATA[
<div> Keywords: North Indian Ocean, Potential Fishing Zones, AI-assisted framework, sustainable fishing practices, oceanographic parameters<br />
Summary:<br />
An AI-assisted framework has been developed to predict Potential Fishing Zones (PFZs) in the North Indian Ocean, specifically the Arabian Sea and Bay of Bengal. By utilizing oceanographic parameters such as sea surface temperature and chlorophyll concentration, the framework aims to assist fishermen in identifying productive fishing grounds with higher accuracy. Early findings suggest that this approach can aid fishermen in reducing search time, cutting down on fuel consumption, and promoting sustainable fishing practices by optimizing resource utilization. The framework provides region-specific insights, enhancing the efficiency and effectiveness of fishing activities in the area. This technology holds promise for improving the livelihoods of coastal communities reliant on fishing in the North Indian Ocean.<br /> <div>
arXiv:2511.02887v1 Announce Type: new 
Abstract: The North Indian Ocean, including the Arabian Sea and the Bay of Bengal, represents a vital source of livelihood for coastal communities, yet fishermen often face uncertainty in locating productive fishing grounds. To address this challenge, we present an AI-assisted framework for predicting Potential Fishing Zones (PFZs) using oceanographic parameters such as sea surface temperature and chlorophyll concentration. The approach is designed to enhance the accuracy of PFZ identification and provide region-specific insights for sustainable fishing practices. Preliminary results indicate that the framework can support fishermen by reducing search time, lowering fuel consumption, and promoting efficient resource utilization.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models</title>
<link>https://arxiv.org/abs/2511.02894</link>
<guid>https://arxiv.org/abs/2511.02894</guid>
<content:encoded><![CDATA[
<div> wearable sensing devices, Internet of Things, human activity recognition, data poisoning attacks, large language models <br />
Summary:
This article introduces a novel framework using large language models (LLMs) for detecting and sanitizing data poisoning attacks in human activity recognition (HAR) systems within IoT environments. The framework incorporates role play prompting and think step-by-step reasoning to identify sensor anomalies and clean alternatives without the need for extensive labeled datasets. By utilizing zero-shot, one-shot, and few-shot learning paradigms, the framework offers adaptable defense mechanisms in real-time. An extensive evaluation of the framework demonstrates high detection accuracy, quality sanitization, low latency, and minimal communication cost, highlighting the practicality and effectiveness of LLMs in enhancing the security and reliability of wearable IoT systems. <br /><br /> <div>
arXiv:2511.02894v1 Announce Type: new 
Abstract: The widespread integration of wearable sensing devices in Internet of Things (IoT) ecosystems, particularly in healthcare, smart homes, and industrial applications, has required robust human activity recognition (HAR) techniques to improve functionality and user experience. Although machine learning models have advanced HAR, they are increasingly susceptible to data poisoning attacks that compromise the data integrity and reliability of these systems. Conventional approaches to defending against such attacks often require extensive task-specific training with large, labeled datasets, which limits adaptability in dynamic IoT environments. This work proposes a novel framework that uses large language models (LLMs) to perform poisoning detection and sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot learning paradigms. Our approach incorporates \textit{role play} prompting, whereby the LLM assumes the role of expert to contextualize and evaluate sensor anomalies, and \textit{think step-by-step} reasoning, guiding the LLM to infer poisoning indicators in the raw sensor data and plausible clean alternatives. These strategies minimize reliance on curation of extensive datasets and enable robust, adaptable defense mechanisms in real-time. We perform an extensive evaluation of the framework, quantifying detection accuracy, sanitization quality, latency, and communication cost, thus demonstrating the practicality and effectiveness of LLMs in improving the security and reliability of wearable IoT systems.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot data citation function classification using transformer-based large language models (LLMs)</title>
<link>https://arxiv.org/abs/2511.02936</link>
<guid>https://arxiv.org/abs/2511.02936</guid>
<content:encoded><![CDATA[
<div> transformer-based large language models, data use case labels, genomic datasets, evaluation framework, F1 score
Summary:
Efforts have been made to identify how specific datasets are used in scientific literature, leveraging transformer-based large language models like Llama 3.1-405B to automatically generate structured data use case labels for publications referencing genomic datasets. The study introduces a novel evaluation framework for assessing the effectiveness of this approach, achieving an F1 score of .674 on a zero-shot data citation classification task. However, challenges such as data availability, prompt overfitting, computational infrastructure limitations, and the high cost of performance evaluation pose barriers to implementation. While the results are promising, further research and improvements are needed to address these challenges and enhance the scalability and accuracy of the automated data use case labeling process. 
<br /><br />Summary: <div>
arXiv:2511.02936v1 Announce Type: new 
Abstract: Efforts have increased in recent years to identify associations between specific datasets and the scientific literature that incorporates them. Knowing that a given publication cites a given dataset, the next logical step is to explore how or why that data was used. Advances in recent years with pretrained, transformer-based large language models (LLMs) offer potential means for scaling the description of data use cases in the published literature. This avoids expensive manual labeling and the development of training datasets for classical machine-learning (ML) systems. In this work we apply an open-source LLM, Llama 3.1-405B, to generate structured data use case labels for publications known to incorporate specific genomic datasets. We also introduce a novel evaluation framework for determining the efficacy of our methods. Our results demonstrate that the stock model can achieve an F1 score of .674 on a zero-shot data citation classification task with no previously defined categories. While promising, our results are qualified by barriers related to data availability, prompt overfitting, computational infrastructure, and the expense required to conduct responsible performance evaluation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics</title>
<link>https://arxiv.org/abs/2511.02944</link>
<guid>https://arxiv.org/abs/2511.02944</guid>
<content:encoded><![CDATA[
<div> Thompson Sampling, personalization, population-level effects, exploration, regret <br />
Summary: <br />
The article introduces a new algorithm, ROGUE-TS, tailored to the ROGUE bandit framework for decision-making in dynamic reward environments. The algorithm helps balance personalization and population-level learning, addressing challenges in micro-randomized trials (MRTs) for behavioral interventions. The study provides theoretical guarantees of sublinear regret and introduces a probability clipping procedure to optimize exploration while maintaining statistical power. Validation on MRT datasets related to physical activity promotion and bipolar disorder treatment demonstrates lower regret and reliable detection of treatment effects. The framework offers practical guidance for researchers designing MRTs to balance personalization with statistical validity. <div>
arXiv:2511.02944v1 Announce Type: new 
Abstract: A common challenge for decision makers is selecting actions whose rewards are unknown and evolve over time based on prior policies. For instance, repeated use may reduce an action's effectiveness (habituation), while inactivity may restore it (recovery). These nonstationarities are captured by the Reducing or Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world settings such as behavioral health interventions. While existing algorithms can compute sublinear regret policies to optimize these settings, they may not provide sufficient exploration due to overemphasis on exploitation, limiting the ability to estimate population-level effects. This is a challenge of particular interest in micro-randomized trials (MRTs) that aid researchers in developing just-in-time adaptive interventions that have population-level effects while still providing personalized recommendations to individuals. In this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored to the ROGUE framework, and provide theoretical guarantees of sublinear regret. We then introduce a probability clipping procedure to balance personalization and population-level learning, with quantified trade-off that balances regret and minimum exploration probability. Validation on two MRT datasets concerning physical activity promotion and bipolar disorder treatment shows that our methods both achieve lower regret than existing approaches and maintain high statistical power through the clipping procedure without significantly increasing regret. This enables reliable detection of treatment effects while accounting for individual behavioral dynamics. For researchers designing MRTs, our framework offers practical guidance on balancing personalization with statistical validity.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.02957</link>
<guid>https://arxiv.org/abs/2511.02957</guid>
<content:encoded><![CDATA[
<div> Keywords: Pavement infrastructure monitoring, Digital Twin, Graph Neural Network, Predictive maintenance, Proactive interventions

Summary:
Pavement management systems face challenges due to complex spatial dependencies and changing environmental conditions. Traditional reactive systems lack real-time intelligence for optimal maintenance planning. To address this, a unified Digital Twin (DT) and Graph Neural Network (GNN) framework is proposed for scalable pavement health monitoring and predictive maintenance. Segments and spatial relations are modeled as graph nodes and edges, with real-time data streaming into the DT. The GNN learns deterioration patterns to forecast distress and enable proactive interventions. Trained on a realistic dataset, the model outperforms baseline regressors with an R2 of 0.3798, capturing non-linear degradation effectively. An interactive dashboard and reinforcement learning module aid in simulation, visualization, and adaptive maintenance planning. This integration enhances forecasting precision and establishes a closed feedback loop for continuous improvement, laying the foundation for proactive, intelligent, and sustainable pavement management. Future extensions include real-world deployment, multi-agent coordination, and smart-city integration.<br /><br />Summary: The article presents a unified Digital Twin and Graph Neural Network framework for proactive pavement management, addressing challenges in traditional systems and enhancing forecasting precision through data-driven approaches. <div>
arXiv:2511.02957v1 Announce Type: new 
Abstract: Pavement infrastructure monitoring is challenged by complex spatial dependencies, changing environmental conditions, and non-linear deterioration across road networks. Traditional Pavement Management Systems (PMS) remain largely reactive, lacking real-time intelligence for failure prevention and optimal maintenance planning. To address this, we propose a unified Digital Twin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven pavement health monitoring and predictive maintenance. Pavement segments and spatial relations are modeled as graph nodes and edges, while real-time UAV, sensor, and LiDAR data stream into the DT. The inductive GNN learns deterioration patterns from graph-structured inputs to forecast distress and enable proactive interventions. Trained on a real-world-inspired dataset with segment attributes and dynamic connectivity, our model achieves an R2 of 0.3798, outperforming baseline regressors and effectively capturing non-linear degradation. We also develop an interactive dashboard and reinforcement learning module for simulation, visualization, and adaptive maintenance planning. This DT-GNN integration enhances forecasting precision and establishes a closed feedback loop for continuous improvement, positioning the approach as a foundation for proactive, intelligent, and sustainable pavement management, with future extensions toward real-world deployment, multi-agent coordination, and smart-city integration.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference-Time Personalized Alignment with a Few User Preference Queries</title>
<link>https://arxiv.org/abs/2511.02966</link>
<guid>https://arxiv.org/abs/2511.02966</guid>
<content:encoded><![CDATA[
<div> Keywords: generative model, personalized alignment, user preferences, response comparisons, UserAlign

Summary: 
User preferences are essential in aligning generative model responses accurately. Existing methods either require extensive user queries or explicit text inputs for preferences. This paper introduces UserAlign, a novel personalized alignment approach that gathers user preferences through pairwise response comparisons with minimal queries. Leveraging best-arm identification in logistic bandits theory, UserAlign selects a personalized response from the model's output pool, assuming noise-free and consistent user feedback. Experimental results across various tasks, such as personalized text and image generation, demonstrate the effectiveness of UserAlign in achieving personalized alignment. <div>
arXiv:2511.02966v1 Announce Type: new 
Abstract: We study the problem of aligning a generative model's response with a user's preferences. Recent works have proposed several different formulations for personalized alignment; however, they either require a large amount of user preference queries or require that the preference be explicitly specified as a text input. In this paper, we propose a novel inference-time personalized alignment method, UserAlign, that elicits the user's preferences with a few queries as pairwise response comparisons. In particular, UserAlign builds on the theoretical framework of best-arm identification in logistic bandits and selects a personalized response from a fixed pool of the model's generated responses. The key idea is to consider the user's feedback consistent and noise-free, and incorporate it into the theoretical framework to identify the best response quickly. Experimental results across several tasks, involving personalized text and image generation, showcase the effectiveness of UserAlign in achieving personalized alignment.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value of Information-Enhanced Exploration in Bootstrapped DQN</title>
<link>https://arxiv.org/abs/2511.02969</link>
<guid>https://arxiv.org/abs/2511.02969</guid>
<content:encoded><![CDATA[
<div> exploration, deep reinforcement learning, Bootstrapped DQN, value of information, Atari games

Summary: 
This paper addresses the challenge of efficient exploration in deep reinforcement learning, particularly in environments with high-dimensional states and sparse rewards. Traditional exploration strategies often struggle to balance exploration and exploitation effectively. The authors propose two novel algorithms that integrate the concept of expected value of information (EVOI) into the Bootstrapped DQN framework. These algorithms use EVOI estimates to measure discrepancies among network heads and guide exploration towards high-potential areas. The experiments on complex, sparse-reward Atari games show improved performance and better utilization of uncertainty without introducing additional hyperparameters. This integration of EVOI enhances deep exploration ability and achieves better results in exploration-exploitation trade-offs in deep reinforcement learning. 

<br /><br />Summary: <div>
arXiv:2511.02969v1 Announce Type: new 
Abstract: Efficient exploration in deep reinforcement learning remains a fundamental challenge, especially in environments characterized by high-dimensional states and sparse rewards. Traditional exploration strategies that rely on random local policy noise, such as $\epsilon$-greedy and Boltzmann exploration methods, often struggle to efficiently balance exploration and exploitation. In this paper, we integrate the notion of (expected) value of information (EVOI) within the well-known Bootstrapped DQN algorithmic framework, to enhance the algorithm's deep exploration ability. Specifically, we develop two novel algorithms that incorporate the expected gain from learning the value of information into Bootstrapped DQN. Our methods use value of information estimates to measure the discrepancies of opinions among distinct network heads, and drive exploration towards areas with the most potential. We evaluate our algorithms with respect to performance and their ability to exploit inherent uncertainty arising from random network initialization. Our experiments in complex, sparse-reward Atari games demonstrate increased performance, all the while making better use of uncertainty, and, importantly, without introducing extra hyperparameters.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heterogeneous Metamaterials Design via Multiscale Neural Implicit Representation</title>
<link>https://arxiv.org/abs/2511.03012</link>
<guid>https://arxiv.org/abs/2511.03012</guid>
<content:encoded><![CDATA[
<div> Keywords: metamaterials, neural network, multiscale design, mechanical metamaterial, negative Poisson's ratio<br />
Summary:<br />
Metamaterials are engineered materials with unique properties, and designing heterogeneous ones can be challenging due to compatibility requirements. Traditional design methods are costly and may result in discontinuities. Data-driven approaches have limitations. A new neural network-based framework proposes a continuous two-scale representation, integrating global and local coordinates for seamless structure design. The neural network learns to output an implicit field representing multiscale structures without a predefined dataset, enforcing connectivity between unit cells. The framework allows for high-resolution metamaterial designs, facilitating fabrication or simulation. The approach is demonstrated on various mechanical metamaterial design challenges, such as negative Poisson's ratio and mechanical cloaking, with potential applications in robotics, bioengineering, and aerospace.<br /> 
Summary: <div>
arXiv:2511.03012v1 Announce Type: new 
Abstract: Metamaterials are engineered materials composed of specially designed unit cells that exhibit extraordinary properties beyond those of natural materials. Complex engineering tasks often require heterogeneous unit cells to accommodate spatially varying property requirements. However, designing heterogeneous metamaterials poses significant challenges due to the enormous design space and strict compatibility requirements between neighboring cells. Traditional concurrent multiscale design methods require solving an expensive optimization problem for each unit cell and often suffer from discontinuities at cell boundaries. On the other hand, data-driven approaches that assemble structures from a fixed library of microstructures are limited by the dataset and require additional post-processing to ensure seamless connections. In this work, we propose a neural network-based metamaterial design framework that learns a continuous two-scale representation of the structure, thereby jointly addressing these challenges. Central to our framework is a multiscale neural representation in which the neural network takes both global (macroscale) and local (microscale) coordinates as inputs, outputting an implicit field that represents multiscale structures with compatible unit cell geometries across the domain, without the need for a predefined dataset. We use a compatibility loss term during training to enforce connectivity between adjacent unit cells. Once trained, the network can produce metamaterial designs at arbitrarily high resolution, hence enabling infinite upsampling for fabrication or simulation. We demonstrate the effectiveness of the proposed approach on mechanical metamaterial design, negative Poisson's ratio, and mechanical cloaking problems with potential applications in robotics, bioengineering, and aerospace.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discrete Bayesian Sample Inference for Graph Generation</title>
<link>https://arxiv.org/abs/2511.03015</link>
<guid>https://arxiv.org/abs/2511.03015</guid>
<content:encoded><![CDATA[
<div> GraphBSI, one-shot graph generative model, Bayesian Sample Inference, discrete diffusion, flow matching models<br />
<br />
Summary:<br />
GraphBSI is introduced as a one-shot graph generative model based on Bayesian Sample Inference (BSI), addressing the challenge of generating graph-structured data. It iteratively refines a belief over graphs in the continuous space of distribution parameters, effectively handling discrete structures. By formulating BSI as a stochastic differential equation (SDE), a noise-controlled family of SDEs that preserves marginal distributions through an approximation of the score function is derived, revealing connections to Bayesian Flow Networks and Diffusion models. Empirical evaluation demonstrates GraphBSI's superiority in molecular and synthetic graph generation, surpassing existing models on standard benchmarks like Moses and GuacaMol. <div>
arXiv:2511.03015v1 Announce Type: new 
Abstract: Generating graph-structured data is crucial in applications such as molecular generation, knowledge graphs, and network analysis. However, their discrete, unordered nature makes them difficult for traditional generative models, leading to the rise of discrete diffusion and flow matching models. In this work, we introduce GraphBSI, a novel one-shot graph generative model based on Bayesian Sample Inference (BSI). Instead of evolving samples directly, GraphBSI iteratively refines a belief over graphs in the continuous space of distribution parameters, naturally handling discrete structures. Further, we state BSI as a stochastic differential equation (SDE) and derive a noise-controlled family of SDEs that preserves the marginal distributions via an approximation of the score function. Our theoretical analysis further reveals the connection to Bayesian Flow Networks and Diffusion models. Finally, in our empirical evaluation, we demonstrate state-of-the-art performance on molecular and synthetic graph generation, outperforming existing one-shot graph generative models on the standard benchmarks Moses and GuacaMol.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive-Sensorless Monitoring of Shipping Containers</title>
<link>https://arxiv.org/abs/2511.03022</link>
<guid>https://arxiv.org/abs/2511.03022</guid>
<content:encoded><![CDATA[
<div> Keywords: shipping containers, sensorless monitoring, machine learning, residual correction method, adaptive-sensorless models

Summary:
Shipping containers' internal temperature and humidity monitoring is crucial for preserving cargo quality during transportation. Sensorless monitoring, which predicts internal conditions using external factors, has shown promise but lacks telemetry integration leading to significant discrepancies with live data. This paper introduces the residual correction method to rectify systematic biases in sensorless models using live telemetry data, creating adaptive-sensorless models. These models, trained on a massive dataset of 3.48 million container sensor readings, consistently outperform baseline sensorless models. On a holdout set, adaptive-sensorless models achieve lower mean absolute errors (MAEs) and root mean-squared errors (RMSEs) for temperature and relative humidity compared to sensorless models. These improved models enhance cargo monitoring accuracy, facilitate early risk detection, and reduce reliance on full connectivity in global shipping.Summary: <div>
arXiv:2511.03022v1 Announce Type: new 
Abstract: Monitoring the internal temperature and humidity of shipping containers is essential to preventing quality degradation during cargo transportation. Sensorless monitoring -- machine learning models that predict the internal conditions of the containers using exogenous factors -- shows promise as an alternative to monitoring using sensors. However, it does not incorporate telemetry information and correct for systematic errors, causing the predictions to differ significantly from the live data and confusing the users. In this paper, we introduce the residual correction method, a general framework for correcting for systematic biases in sensorless models after observing live telemetry data. We call this class of models ``adaptive-sensorless'' monitoring. We train and evaluate adaptive-sensorless models on the 3.48 million data points -- the largest dataset of container sensor readings ever used in academic research -- and show that they produce consistent improvements over the baseline sensorless models. When evaluated on the holdout set of the simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$ 2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$ 7.09% for relative humidity (vs 7.99% by sensorless) and average root mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs 3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs 10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo monitoring, early risk detection, and less dependence on full connectivity in global shipping.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Discrete Function Decomposability for Scientific Design</title>
<link>https://arxiv.org/abs/2511.03032</link>
<guid>https://arxiv.org/abs/2511.03032</guid>
<content:encoded><![CDATA[
<div> distributional optimization, AI-driven science, generative model, decomposability, DADO

Summary:
- The article discusses the importance of designing discrete objects in silico with user-specified properties in the era of AI-driven science and engineering.
- In silico design involves training a generative model over the design space to focus on designs with desired properties, such as proteins binding to targets or minimizing circuit latency.
- Current distributional optimization algorithms face challenges in optimizing discrete-valued designs due to the combinatorial nature of the design space.
- The proposed Decomposition-Aware Distributional Optimization (DADO) algorithm leverages decomposability structure defined by a junction tree on design variables for more efficient optimization.
- DADO employs a soft-factorized search distribution, a learned generative model, and graph message-passing to coordinate optimization across linked factors. 

<br /><br />Summary: <div>
arXiv:2511.03032v1 Announce Type: new 
Abstract: In the era of AI-driven science and engineering, we often want to design discrete objects in silico according to user-specified properties. For example, we may wish to design a protein to bind its target, arrange components within a circuit to minimize latency, or find materials with certain properties. Given a property predictive model, in silico design typically involves training a generative model over the design space (e.g., protein sequence space) to concentrate on designs with the desired properties. Distributional optimization -- which can be formalized as an estimation of distribution algorithm or as reinforcement learning policy optimization -- finds the generative model that maximizes an objective function in expectation. Optimizing a distribution over discrete-valued designs is in general challenging because of the combinatorial nature of the design space. However, many property predictors in scientific applications are decomposable in the sense that they can be factorized over design variables in a way that could in principle enable more effective optimization. For example, amino acids at a catalytic site of a protein may only loosely interact with amino acids of the rest of the protein to achieve maximal catalytic activity. Current distributional optimization algorithms are unable to make use of such decomposability structure. Herein, we propose and demonstrate use of a new distributional optimization algorithm, Decomposition-Aware Distributional Optimization (DADO), that can leverage any decomposability defined by a junction tree on the design variables, to make optimization more efficient. At its core, DADO employs a soft-factorized "search distribution" -- a learned generative model -- for efficient navigation of the search space, invoking graph message-passing to coordinate optimization across linked factors.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Efficient Realized Volatility Forecasting with Vision Transformers</title>
<link>https://arxiv.org/abs/2511.03046</link>
<guid>https://arxiv.org/abs/2511.03046</guid>
<content:encoded><![CDATA[
<div> Keywords: financial machine learning, transformer architecture, options data, Vision Transformer, volatility forecasting

Summary:
The article explores the use of transformer models, specifically the Vision Transformer (ViT), for options data in financial forecasting. While deep learning methods have proven effective in capturing nonlinear relationships, transformer architectures like ViT have not been extensively applied to options data. By training ViT to predict asset volatility from implied volatility surfaces, the study demonstrates the model's ability to capture seasonal patterns and nonlinear features. This initial research indicates the potential for transformer models to enhance forecasting accuracy in the options market, leveraging the ViT's capabilities in recognizing complex patterns. The ViT's success in learning from the IV surface augurs well for future development of transformer models tailored to options data, potentially improving prediction accuracy in financial markets. 

<br /><br />Summary: <div>
arXiv:2511.03046v1 Announce Type: new 
Abstract: Recent work in financial machine learning has shown the virtue of complexity: the phenomenon by which deep learning methods capable of learning highly nonlinear relationships outperform simpler approaches in financial forecasting. While transformer architectures like Informer have shown promise for financial time series forecasting, the application of transformer models for options data remains largely unexplored. We conduct preliminary studies towards the development of a transformer model for options data by training the Vision Transformer (ViT) architecture, typically used in modern image recognition and classification systems, to predict the realized volatility of an asset over the next 30 days from its implied volatility surface (augmented with date information) for a single day. We show that the ViT can learn seasonal patterns and nonlinear features from the IV surface, suggesting a promising direction for model development.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Evaluation of Multi-Turn Objective-Driven Interactions</title>
<link>https://arxiv.org/abs/2511.03047</link>
<guid>https://arxiv.org/abs/2511.03047</guid>
<content:encoded><![CDATA[
<div> language models, objective-driven interactions, evaluation metrics, unsupervised, LLMs

Summary: 
This article introduces new unsupervised metrics for evaluating large language models (LLMs) used in objective-driven interactions. Traditional evaluation methods face challenges due to complex data and impractical human annotation at scale. The proposed metrics leverage statistical properties of unlabeled interaction data and fine-tuned LLMs to adapt to distribution shifts. The metrics focus on labeling user goals, measuring goal completion, and quantifying LLM uncertainty without relying on human-generated ideal responses. The approach is validated on both open-domain and task-specific interaction data. The new metrics aim to provide a more reliable and efficient way to evaluate LLMs in enterprise applications. <div>
arXiv:2511.03047v1 Announce Type: new 
Abstract: Large language models (LLMs) have seen increasing popularity in enterprise applications where AI agents and humans engage in objective-driven interactions. However, these systems are difficult to evaluate: data may be complex and unlabeled; human annotation is often impractical at scale; custom metrics can monitor for specific errors, but not previously-undetected ones; and LLM judges can produce unreliable results. We introduce the first set of unsupervised metrics for objective-driven interactions, leveraging statistical properties of unlabeled interaction data and using fine-tuned LLMs to adapt to distributional shifts. We develop metrics for labeling user goals, measuring goal completion, and quantifying LLM uncertainty without grounding evaluations in human-generated ideal responses. Our approach is validated on open-domain and task-specific interaction data.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Curved Spacetime of Transformer Architectures</title>
<link>https://arxiv.org/abs/2511.03060</link>
<guid>https://arxiv.org/abs/2511.03060</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer-based language models, General relativity analogy, Representation space, Attention mechanism, Curvature landscape <br />
Summary: 
The article presents a geometric framework for understanding Transformer-based language models by drawing an analogy to General Relativity. It suggests that queries and keys induce a metric in representation space, with attention acting as a discrete connection for parallel transport. Stacked layers provide discrete time-slices for token representations to evolve on a curved manifold, influenced by backpropagation. Experimental tests confirm the presence and consequences of curvature, showcasing distinct turning angles and trajectory bends. The visualization of a curvature landscape for a paragraph reveals variations in local turning angles across layers. Simulations demonstrate statistically significant excess counts of sharp/flat angles and longer length-to-chord ratios, not attributable to chance. Controlled context edits show measurable bends in embedding trajectories, confirming attention-induced curvature that aligns with the concept of General Relativity. <br /><br /> <div>
arXiv:2511.03060v1 Announce Type: new 
Abstract: We present a geometric framework for understanding Transformer-based language models, drawing an explicit analogy to General Relativity. Queries and keys induce an effective metric on representation space, and attention acts as a discrete connection that implements parallel transport of value vectors across tokens. Stacked layers provide discrete time-slices through which token representations evolve on this curved manifold, while backpropagation plays the role of a least-action principle that shapes loss-minimizing trajectories in parameter space. If this analogy is correct, token embeddings should not traverse straight paths in feature space; instead, their layer-wise steps should bend and reorient as interactions mediated by embedding space curvature. To test this prediction, we design experiments that expose both the presence and the consequences of curvature: (i) we visualize a curvature landscape for a full paragraph, revealing how local turning angles vary across tokens and layers; (ii) we show through simulations that excess counts of sharp/flat angles and longer length-to-chord ratios are not explainable by dimensionality or chance; and (iii) inspired by Einstein's eclipse experiment, we probe deflection under controlled context edits, demonstrating measurable, meaning-consistent bends in embedding trajectories that confirm attention-induced curvature.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Homomorphism distortion: A metric to distinguish them all and in the latent space bind them</title>
<link>https://arxiv.org/abs/2511.03068</link>
<guid>https://arxiv.org/abs/2511.03068</guid>
<content:encoded><![CDATA[
<div> graph neural networks, graph homomorphism distortion, graph embedding, graph canonization problem, metric

Summary:
The article introduces the concept of graph homomorphism distortion as a new measure for similarity between vertex attributed graphs, departing from traditional combinatorial measurements. It is shown to completely characterize graphs and serve as a complete graph embedding. The measure can be efficiently computed through sampling, addressing the graph canonization problem. It also yields a metric for comparison. Empirical validation demonstrates the effectiveness of graph homomorphism distortion in distinguishing datasets like BREC and outperforming existing methods on ZINC-12k. These findings open up new possibilities for graph characterization, pushing the boundaries of graph theoretic analysis. <br /><br />Summary: <div>
arXiv:2511.03068v1 Announce Type: new 
Abstract: For far too long, expressivity of graph neural networks has been measured \emph{only} in terms of combinatorial properties. In this work we stray away from this tradition and provide a principled way to measure similarity between vertex attributed graphs. We denote this measure as the \emph{graph homomorphism distortion}. We show it can \emph{completely characterize} graphs and thus is also a \emph{complete graph embedding}. However, somewhere along the road, we run into the graph canonization problem. To circumvent this obstacle, we devise to efficiently compute this measure via sampling, which in expectation ensures \emph{completeness}. Additionally, we also discovered that we can obtain a metric from this measure. We validate our claims empirically and find that the \emph{graph homomorphism distortion}: (1.) fully distinguishes the \texttt{BREC} dataset with up to $4$-WL non-distinguishable graphs, and (2.) \emph{outperforms} previous methods inspired in homomorphisms under the \texttt{ZINC-12k} dataset.
  These theoretical results, (and their empirical validation), pave the way for future characterization of graphs, extending the graph theoretic tradition to new frontiers.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach</title>
<link>https://arxiv.org/abs/2511.03074</link>
<guid>https://arxiv.org/abs/2511.03074</guid>
<content:encoded><![CDATA[
<div> bandit, online learning, corruption, mean-of-medians estimator, robust algorithm <br />
Summary: <br />
The paper introduces MSUCB, a robust algorithm for online learning to rank (OLTR) in the presence of corruption. This algorithm utilizes a mean-of-medians estimator, a novel approach in the context of bandits with corruption, to combat click fraud and manipulation. In the absence of corruption, MSUCB achieves optimal logarithmic regret. The estimator adapts to filter out outliers and corrupted samples, maintaining the estimate close to the true value under corruption. By updating the estimate at every round, MSUCB shows accelerated convergence in empirical experiments. The algorithm performs consistently better than existing methods on real-world datasets, exhibiting a significant regret improvement of 97.35% and 91.60% over two state-of-the-art techniques. MSUCB offers a robust solution for OLTR systems, ensuring effective recommendations and user experience in the face of potential manipulations. <br /> <div>
arXiv:2511.03074v1 Announce Type: new 
Abstract: Online learning to rank (OLTR) studies how to recommend a short ranked list of items from a large pool and improves future rankings based on user clicks. This setting is commonly modeled as cascading bandits, where the objective is to maximize the likelihood that the user clicks on at least one of the presented items across as many timesteps as possible. However, such systems are vulnerable to click fraud and other manipulations (i.e., corruption), where bots or paid click farms inject corrupted feedback that misleads the learning process and degrades user experience. In this paper, we propose MSUCB, a robust algorithm that incorporates a novel mean-of-medians estimator, which to our knowledge is applied to bandits with corruption setting for the first time. This estimator behaves like a standard mean in the absence of corruption, so no cost is paid for robustness. Under corruption, the median step filters out outliers and corrupted samples, keeping the estimate close to its true value. Updating this estimate at every round further accelerates empirical convergence in experiments. Hence, MSUCB achieves optimal logarithmic regret in the absence of corruption and degrades gracefully under corruptions, with regret increasing only by an additive term tied to the total corruption. Comprehensive and extensive experiments on real-world datasets further demonstrate that our approach consistently outperforms prior methods while maintaining strong robustness. In particular, it achieves a \(97.35\%\) and a \(91.60\%\) regret improvement over two state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies</title>
<link>https://arxiv.org/abs/2511.03095</link>
<guid>https://arxiv.org/abs/2511.03095</guid>
<content:encoded><![CDATA[
<div> Sparse ensemble, Gaussian kernels, anomaly detection, self-organization, high-dimensional problems

Summary:
Sparse ensemble SparKer uses Gaussian kernels to detect anomalies in high-dimensional data by adaptively partitioning representation space. The model is trained in a semi-supervised NeymanPearson framework to compare samples with a nominal reference. The approach emphasizes sparsity, locality, and competition to efficiently allocate model capacity. The model's self-organizing local kernels identify anomalies in diverse scientific applications, spanning natural and computer science domains. The proposed method is efficient and scalable, requiring only a few kernels to detect anomalies in high-dimensional data. The approach provides interpretable and effective anomaly detection, making it suitable for various real-world applications. <div>
arXiv:2511.03095v1 Announce Type: new 
Abstract: Modern artificial intelligence has revolutionized our ability to extract rich and versatile data representations across scientific disciplines. Yet, the statistical properties of these representations remain poorly controlled, causing misspecified anomaly detection (AD) methods to falter. Weak or rare signals can remain hidden within the apparent regularity of normal data, creating a gap in our ability to detect and interpret anomalies. We examine this gap and identify a set of structural desiderata for detection methods operating under minimal prior information: sparsity, to enforce parsimony; locality, to preserve geometric sensitivity; and competition, to promote efficient allocation of model capacity. These principles define a class of self-organizing local kernels that adaptively partition the representation space around regions of statistical imbalance. As an instantiation of these principles, we introduce SparKer, a sparse ensemble of Gaussian kernels trained within a semi-supervised Neyman--Pearson framework to locally model the likelihood ratio between a sample that may contain anomalies and a nominal, anomaly-free reference. We provide theoretical insights into the mechanisms that drive detection and self-organization in the proposed model, and demonstrate the effectiveness of this approach on realistic high-dimensional problems of scientific discovery, open-world novelty detection, intrusion detection, and generative-model validation. Our applications span both the natural- and computer-science domains. We demonstrate that ensembles containing only a handful of kernels can identify statistically significant anomalous locations within representation spaces of thousands of dimensions, underscoring both the interpretability, efficiency and scalability of the proposed approach.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Multi-Agent Environment Co-Design with Diffusion Models</title>
<link>https://arxiv.org/abs/2511.03100</link>
<guid>https://arxiv.org/abs/2511.03100</guid>
<content:encoded><![CDATA[
<div> Keywords: agent-environment co-design, scalable, sample-efficient, Projected Universal Guidance (PUG), critic distillation

Summary: 
Diffusion Co-Design (DiCoDe) is a new framework that addresses scalability and sample efficiency issues in agent-environment co-design. It introduces Projected Universal Guidance (PUG) for sampling environments that optimize rewards while meeting constraints like obstacle separation. The method also includes a critic distillation mechanism to adapt to evolving agent policies using a dense learning signal. DiCoDe outperforms existing methods in multi-agent environment co-design benchmarks, achieving higher rewards with fewer simulation samples. For example, in a warehouse setting, it achieved 39% higher rewards with 66% fewer samples. This advancement sets a new standard in co-design and paves the way for practical applications in warehouse automation, multi-agent pathfinding, and wind farm optimization. <br /><br />Summary: <div>
arXiv:2511.03100v1 Announce Type: new 
Abstract: The agent-environment co-design paradigm jointly optimises agent policies and environment configurations in search of improved system performance. With application domains ranging from warehouse logistics to windfarm management, co-design promises to fundamentally change how we deploy multi-agent systems. However, current co-design methods struggle to scale. They collapse under high-dimensional environment design spaces and suffer from sample inefficiency when addressing moving targets inherent to joint optimisation. We address these challenges by developing Diffusion Co-Design (DiCoDe), a scalable and sample-efficient co-design framework pushing co-design towards practically relevant settings. DiCoDe incorporates two core innovations. First, we introduce Projected Universal Guidance (PUG), a sampling technique that enables DiCoDe to explore a distribution of reward-maximising environments while satisfying hard constraints such as spatial separation between obstacles. Second, we devise a critic distillation mechanism to share knowledge from the reinforcement learning critic, ensuring that the guided diffusion model adapts to evolving agent policies using a dense and up-to-date learning signal. Together, these improvements lead to superior environment-policy pairs when validated on challenging multi-agent environment co-design benchmarks including warehouse automation, multi-agent pathfinding and wind farm optimisation. Our method consistently exceeds the state-of-the-art, achieving, for example, 39% higher rewards in the warehouse setting with 66% fewer simulation samples. This sets a new standard in agent-environment co-design, and is a stepping stone towards reaping the rewards of co-design in real world domains.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Classification Model for Cyber Text</title>
<link>https://arxiv.org/abs/2511.03107</link>
<guid>https://arxiv.org/abs/2511.03107</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, carbon footprint, text analytics, CTF-IDF, machine learning

Summary:
The paper introduces a new algorithm, Clement Term Frequency-Inverse Document Frequency (CTF-IDF), as an enhancement to the traditional TF-IDF algorithm for data preprocessing in text analytics. By incorporating CTF-IDF and the faster IRLBA algorithm for dimensionality reduction, the study explores the efficacy of classical machine learning methods in text analytics. The primary focus is on addressing the carbon footprint issue associated with deep learning practices, which demand substantial computational resources. The experimental results demonstrate that the proposed techniques lead to a more efficient and less computationally intensive application while maintaining accuracy levels comparable to deep learning methods. Moreover, the use of CTF-IDF and IRLBA significantly reduces time complexity and enhances model accuracy in text analytics tasks. This research highlights the potential for classical machine learning methods to offer a sustainable alternative to deep learning in text analytics. 

<br /><br />Summary: <div>
arXiv:2511.03107v1 Announce Type: new 
Abstract: The uprising of deep learning methodology and practice in recent years has brought about a severe consequence of increasing carbon footprint due to the insatiable demand for computational resources and power. The field of text analytics also experienced a massive transformation in this trend of monopolizing methodology. In this paper, the original TF-IDF algorithm has been modified, and Clement Term Frequency-Inverse Document Frequency (CTF-IDF) has been proposed for data preprocessing. This paper primarily discusses the effectiveness of classical machine learning techniques in text analytics with CTF-IDF and a faster IRLBA algorithm for dimensionality reduction. The introduction of both of these techniques in the conventional text analytics pipeline ensures a more efficient, faster, and less computationally intensive application when compared with deep learning methodology regarding carbon footprint, with minor compromise in accuracy. The experimental results also exhibit a manifold of reduction in time complexity and improvement of model accuracy for the classical machine learning methods discussed further in this paper.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Scalable Backpropagation-Free Gradient Estimation</title>
<link>https://arxiv.org/abs/2511.03110</link>
<guid>https://arxiv.org/abs/2511.03110</guid>
<content:encoded><![CDATA[
<div> Jacobian matrices, gradient estimation, deep learning, automatic differentiation, neural networks
<br />
Summary: 
This article introduces a new gradient estimation approach for neural networks that aims to reduce both bias and variance in the estimates. The traditional backpropagation method requires two passes through the network, while existing gradient estimation methods struggle to scale due to high variance. The proposed approach manipulates upstream Jacobian matrices to compute guess directions, showing promising results and scalability to larger networks. Analysis of bias and variance in gradient estimates is essential for understanding the method's performance and its connection to the low-dimensional structure of neural network gradients. <div>
arXiv:2511.03110v1 Announce Type: new 
Abstract: While backpropagation--reverse-mode automatic differentiation--has been extraordinarily successful in deep learning, it requires two passes (forward and backward) through the neural network and the storage of intermediate activations. Existing gradient estimation methods that instead use forward-mode automatic differentiation struggle to scale beyond small networks due to the high variance of the estimates. Efforts to mitigate this have so far introduced significant bias to the estimates, reducing their utility. We introduce a gradient estimation approach that reduces both bias and variance by manipulating upstream Jacobian matrices when computing guess directions. It shows promising results and has the potential to scale to larger networks, indeed performing better as the network width is increased. Our understanding of this method is facilitated by analyses of bias and variance, and their connection to the low-dimensional structure of neural network gradients.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation</title>
<link>https://arxiv.org/abs/2511.03113</link>
<guid>https://arxiv.org/abs/2511.03113</guid>
<content:encoded><![CDATA[
<div> physics-informed regularizer, antibody design, generative models, Fokker-Planck Equation, deep biological priors

Summary:<br />
- FP-AbDiff introduces a new antibody generator that enforces Fokker-Planck Equation physics for consistent and physically plausible structures.
- It minimizes a novel FPE residual loss over CDR geometries to ensure globally coherent probability flow.
- The model integrates deep biological priors within a SE(3)-equivariant diffusion framework, achieving state-of-the-art performance on the RAbD benchmark.
- In de novo CDR-H3 design, FP-AbDiff outperforms previous models by achieving a mean RMSD of 0.99  and the highest Contact Amino Acid Recovery.
- In six-CDR co-design tasks, the model demonstrates superior geometric precision and highest full-chain Amino Acid Recovery on the CDR-H3 loop.
- By aligning generative dynamics with physical laws, FP-AbDiff enhances robustness, generalizability, and offers a principled approach for antibody design.<br /><br />Summary: <div>
arXiv:2511.03113v1 Announce Type: new 
Abstract: Computational antibody design holds immense promise for therapeutic discovery, yet existing generative models are fundamentally limited by two core challenges: (i) a lack of dynamical consistency, which yields physically implausible structures, and (ii) poor generalization due to data scarcity and structural bias. We introduce FP-AbDiff, the first antibody generator to enforce Fokker-Planck Equation (FPE) physics along the entire generative trajectory. Our method minimizes a novel FPE residual loss over the mixed manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising scores to assemble into a globally coherent probability flow. This physics-informed regularizer is synergistically integrated with deep biological priors within a state-of-the-art SE(3)-equivariant diffusion framework. Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean Square Deviation of 0.99 {\AA} when superposing on the variable region, a 25% improvement over the previous state-of-the-art model, AbX, and the highest reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored in the more challenging six-CDR co-design task, where our model delivers consistently superior geometric precision, cutting the average full-chain Root Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By aligning generative dynamics with physical laws, FP-AbDiff enhances robustness and generalizability, establishing a principled approach for physically faithful and functionally viable antibody design.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Augmentation Overlap Theory of Contrastive Learning</title>
<link>https://arxiv.org/abs/2511.03114</link>
<guid>https://arxiv.org/abs/2511.03114</guid>
<content:encoded><![CDATA[
<div> Keywords: self-supervised learning, contrastive learning, augmentation overlap, representation evaluation, unsupervised metric

Summary:
This paper delves into the workings of self-supervised contrastive learning, providing tight bounds and insights into its mechanisms. The authors introduce the concept of augmentation overlap, relaxing the conditional independence assumption to achieve practical bounds for downstream performance. By aligning positive samples through aggressive data augmentations, intra-class samples can be clustered effectively. The proposed augmentation overlap theory aids in the development of an unsupervised metric for representation evaluation in contrastive learning, correlating well with downstream tasks. The approach outlined in this paper demonstrates the potential for enhancing the performance of self-supervised learning methods without the need for additional modules. <div>
arXiv:2511.03114v1 Announce Type: new 
Abstract: Recently, self-supervised contrastive learning has achieved great success on various tasks. However, its underlying working mechanism is yet unclear. In this paper, we first provide the tightest bounds based on the widely adopted assumption of conditional independence. Further, we relax the conditional independence assumption to a more practical assumption of augmentation overlap and derive the asymptotically closed bounds for the downstream performance. Our proposed augmentation overlap theory hinges on the insight that the support of different intra-class samples will become more overlapped under aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make contrastive learning cluster intra-class samples together. Moreover, from the newly derived augmentation overlap perspective, we develop an unsupervised metric for the representation evaluation of contrastive learning, which aligns well with the downstream performance almost without relying on additional modules. Code is available at https://github.com/PKU-ML/GARC.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation</title>
<link>https://arxiv.org/abs/2511.03128</link>
<guid>https://arxiv.org/abs/2511.03128</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, adversarial examples, robustness, attack frameworks, transferability <br />
Summary: 
The article introduces Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), new attack frameworks for generating dynamic and adaptive adversarial examples to assess the robustness of Language Model models (LLMs). These attacks are designed to deceive LLMs for sensitive tasks by producing subtle, natural-looking adversarial inputs that maintain semantic similarity with the original text. The attacks use an automated pipeline driven by LLMs, eliminating the need for external heuristics. They are capable of evolving with LLM advancements and demonstrate strong transferability across unknown models. By releasing the code and data, the article provides a systematic approach for self-assessing the robustness of LLMs. Overall, this work contributes to the understanding and evaluation of LLMs' resistance to adversarial inputs. <br /><br />Summary: <div>
arXiv:2511.03128v1 Announce Type: new 
Abstract: LLMs can provide substantial zero-shot performance on diverse tasks using a simple task prompt, eliminating the need for training or fine-tuning. However, when applying these models to sensitive tasks, it is crucial to thoroughly assess their robustness against adversarial inputs. In this work, we introduce Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), two innovative attack frameworks designed to systematically generate dynamic and adaptive adversarial examples by leveraging the understanding of the LLMs. We produce subtle and natural-looking adversarial inputs that preserve semantic similarity to the original text while effectively deceiving the target LLM. By utilizing an automated, LLM-driven pipeline, we eliminate the dependence on external heuristics. Our attacks evolve with the advancements in LLMs and demonstrate strong transferability across models unknown to the attacker. Overall, this work provides a systematic approach for the self-assessment of an LLM's robustness. We release our code and data at https://github.com/Shukti042/AdversarialExample.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test Time Adaptation Using Adaptive Quantile Recalibration</title>
<link>https://arxiv.org/abs/2511.03148</link>
<guid>https://arxiv.org/abs/2511.03148</guid>
<content:encoded><![CDATA[
<div> Adaptive Quantile Recalibration, test-time adaptation, deep learning, domain adaptation, activation distributions
Summary: 
Adaptive Quantile Recalibration (AQR) is a novel test-time adaptation technique that enhances the generalizability of deep learning models by aligning quantiles on a channel-wise basis to modify pre-activation distributions. It can be applied across architectures employing BatchNorm, GroupNorm, or LayerNorm and incorporates a robust tail calibration strategy for improved stability and precision. AQR leverages source-domain statistics computed at training time, enabling unsupervised adaptation without model retraining. Experimental results on CIFAR-10-C, CIFAR-100-C, and ImageNet-C datasets demonstrate that AQR outperforms existing test-time adaptation baselines, showcasing its robust adaptation capabilities across diverse settings. The method's ability to capture complex activation distributions makes it suitable for deployment in real-world scenarios with dynamic and unpredictable data distributions.<br /><br />Summary: <div>
arXiv:2511.03148v1 Announce Type: new 
Abstract: Domain adaptation is a key strategy for enhancing the generalizability of deep learning models in real-world scenarios, where test distributions often diverge significantly from the training domain. However, conventional approaches typically rely on prior knowledge of the target domain or require model retraining, limiting their practicality in dynamic or resource-constrained environments. Recent test-time adaptation methods based on batch normalization statistic updates allow for unsupervised adaptation, but they often fail to capture complex activation distributions and are constrained to specific normalization layers. We propose Adaptive Quantile Recalibration (AQR), a test-time adaptation technique that modifies pre-activation distributions by aligning quantiles on a channel-wise basis. AQR captures the full shape of activation distributions and generalizes across architectures employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of estimating distribution tails under varying batch sizes, AQR incorporates a robust tail calibration strategy that improves stability and precision. Our method leverages source-domain statistics computed at training time, enabling unsupervised adaptation without retraining models. Experiments on CIFAR-10-C, CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR achieves robust adaptation across diverse settings, outperforming existing test-time adaptation baselines. These results highlight AQR's potential for deployment in real-world scenarios with dynamic and unpredictable data distributions.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction</title>
<link>https://arxiv.org/abs/2511.03149</link>
<guid>https://arxiv.org/abs/2511.03149</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly prediction, multivariate time series, Time Series Foundation Models (TSFMs), Forecast2Anomaly (F2A), Retrieval-Augmented Generation (RAG)

Summary:
Forecast2Anomaly (F2A) is a new framework designed to predict anomalies in multivariate time series data, crucial for pre-empting critical failures and reducing operational costs. Existing methods are limited in their ability to generalize to evolving anomaly patterns, but F2A utilizes Time Series Foundation Models (TSFMs) with two key innovations. Firstly, a joint forecast-anomaly loss fine-tunes TSFMs to accurately predict future signals, even at anomalous time points. Secondly, a Retrieval-Augmented Generation (RAG) module dynamically adapts to distributional shifts during inference, enabling F2A to track evolving anomalies without model updates. Through targeted fine-tuning and dynamic retrieval, F2A bridges the gap between robust TSFM zero-shot forecasting and zero-shot anomaly prediction. Extensive experiments across various datasets and TSFM backbones demonstrate that F2A outperforms state-of-the-art methods, providing a scalable solution for real-world anomaly prediction applications. 

<br /><br />Summary: <div>
arXiv:2511.03149v1 Announce Type: new 
Abstract: Forecasting anomalies (anomaly prediction) in multivariate time series from different real-world, dynamic, and complex systems is vital for preempting critical failures, leading to a substantial minimization in operational costs and human labor. Yet, existing methods are limited to specific systems while failing to generalize to evolving anomaly patterns over time. In contrast, pretrained Time Series Foundation Models (TSFMs) have recently demonstrated strong generalization and zero-shot forecasting capabilities. However, their potential remains untapped for anomaly prediction, a task fundamentally different from forecasting normal behavior. Thus, we present Forecast2Anomaly (F2A), a novel framework that empowers TSFMs with anomaly prediction abilities through two key innovations. First, we propose a joint forecast-anomaly loss that fine-tunes TSFMs to accurately forecast future signals even at anomalous time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module that retrieves historically relevant horizons and conditions predictions on them. This component dynamically adapts to distributional shifts at inference time, enabling F2A to track evolving anomalies without requiring model updates. By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap between robust TSFM zero-shot forecasting and zero-shot anomaly prediction. Extensive experiments across 16 diverse datasets and multiple TSFM backbones show that F2A consistently outperforms state-of-the-art methods, offering a scalable, zero-shot anomaly prediction solution for real-world applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnCLe: Towards Scalable Dynamic Causal Discovery in Non-linear Temporal Systems</title>
<link>https://arxiv.org/abs/2511.03168</link>
<guid>https://arxiv.org/abs/2511.03168</guid>
<content:encoded><![CDATA[
<div> Deep learning, causal discovery, dynamic causal graphs, time series, temporal dynamics<br />
<br />
Summary: 
The article introduces UnCLe, a deep learning method designed for dynamic causal discovery from observational time series data. UnCLe utilizes Uncoupler and Recoupler networks to extract semantic representations and learn inter-variable dependencies through Dependency Matrices. By analyzing prediction errors induced by temporal perturbations, UnCLe estimates dynamic causal influences, allowing for the accurate capture of evolving temporal causality in complex systems. Through extensive experiments, UnCLe has shown superior performance compared to existing methods in both static causal discovery benchmarks and real-world dynamic systems such as human motion. The method offers a promising approach to uncover the underlying mechanisms of complex phenomena by providing detailed insights into time-varying causal relationships. <br /><br />Summary: <div>
arXiv:2511.03168v1 Announce Type: new 
Abstract: Uncovering cause-effect relationships from observational time series is fundamental to understanding complex systems. While many methods infer static causal graphs, real-world systems often exhibit dynamic causality-where relationships evolve over time. Accurately capturing these temporal dynamics requires time-resolved causal graphs. We propose UnCLe, a novel deep learning method for scalable dynamic causal discovery. UnCLe employs a pair of Uncoupler and Recoupler networks to disentangle input time series into semantic representations and learns inter-variable dependencies via auto-regressive Dependency Matrices. It estimates dynamic causal influences by analyzing datapoint-wise prediction errors induced by temporal perturbations. Extensive experiments demonstrate that UnCLe not only outperforms state-of-the-art baselines on static causal discovery benchmarks but, more importantly, exhibits a unique capability to accurately capture and represent evolving temporal causality in both synthetic and real-world dynamic systems (e.g., human motion). UnCLe offers a promising approach for revealing the underlying, time-varying mechanisms of complex phenomena.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Periodic Skill Discovery</title>
<link>https://arxiv.org/abs/2511.03187</link>
<guid>https://arxiv.org/abs/2511.03187</guid>
<content:encoded><![CDATA[
<div> periodic skill discovery, reinforcement learning, unsupervised, robotic tasks, diverse behaviors

Summary:
Periodic Skill Discovery (PSD) is a framework proposed for unsupervised skill discovery in reinforcement learning. The goal of PSD is to learn diverse periodic behaviors in complex robotic tasks, particularly those involving locomotion. By training an encoder that maps states to a circular latent space, PSD can effectively capture the periodic nature of skills. This allows for the discovery of skills with diverse periods, even with pixel-based observations. The learned skills from PSD show high performance on downstream tasks such as hurdling. Integrating PSD with existing skill discovery methods results in even more diverse behaviors for the agent, expanding its range of capabilities. Overall, PSD offers a promising approach for discovering and learning diverse periodic skills in RL settings, enhancing the agent's ability to tackle a variety of tasks. 

<br /><br />Summary: <div>
arXiv:2511.03187v1 Announce Type: new 
Abstract: Unsupervised skill discovery in reinforcement learning (RL) aims to learn diverse behaviors without relying on external rewards. However, current methods often overlook the periodic nature of learned skills, focusing instead on increasing the mutual dependence between states and skills or maximizing the distance traveled in latent space. Considering that many robotic tasks -- particularly those involving locomotion -- require periodic behaviors across varying timescales, the ability to discover diverse periodic skills is essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a framework that discovers periodic behaviors in an unsupervised manner. The key idea of PSD is to train an encoder that maps states to a circular latent space, thereby naturally encoding periodicity in the latent representation. By capturing temporal distance, PSD can effectively learn skills with diverse periods in complex robotic tasks, even with pixel-based observations. We further show that these learned skills achieve high performance on downstream tasks such as hurdling. Moreover, integrating PSD with an existing skill discovery method offers more diverse behaviors, thus broadening the agent's repertoire. Our code and demos are available at https://jonghaepark.github.io/psd/
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality</title>
<link>https://arxiv.org/abs/2511.03190</link>
<guid>https://arxiv.org/abs/2511.03190</guid>
<content:encoded><![CDATA[
<div> Keywords: attention mechanisms, time series modeling, linear complexity, entropy equality, forecasting performance

Summary: 
- The study introduces a new linear attention mechanism for time series modeling to address the computational complexity issues typically associated with traditional attention mechanisms.
- The proposed method is based on the concept of entropy and the structural resemblance of probability distributions with aligned rankings and similar entropy values.
- By developing an approximation algorithm with linear complexity to compute entropy, the linear attention mechanism achieves efficient computation while maintaining accuracy.
- The research suggests that the effectiveness of attention in time series modeling lies more in achieving a well-balanced weight distribution rather than the non-linearity of softmax.
- Experimental results on multiple spatio-temporal datasets demonstrate that the proposed linear attention mechanism outperforms traditional methods in forecasting accuracy while also reducing memory usage and computational time significantly.

<br /><br />Summary: <div>
arXiv:2511.03190v1 Announce Type: new 
Abstract: Attention mechanisms have been extensively employed in various applications, including time series modeling, owing to their capacity to capture intricate dependencies; however, their utility is often constrained by quadratic computational complexity, which impedes scalability for long sequences. In this work, we propose a novel linear attention mechanism designed to overcome these limitations. Our approach is grounded in a theoretical demonstration that entropy, as a strictly concave function on the probability simplex, implies that distributions with aligned probability rankings and similar entropy values exhibit structural resemblance. Building on this insight, we develop an efficient approximation algorithm that computes the entropy of dot-product-derived distributions with only linear complexity, enabling the implementation of a linear attention mechanism based on entropy equality. Through rigorous analysis, we reveal that the effectiveness of attention in spatio-temporal time series modeling may not primarily stem from the non-linearity of softmax but rather from the attainment of a moderate and well-balanced weight distribution. Extensive experiments on four spatio-temporal datasets validate our method, demonstrating competitive or superior forecasting performance while achieving substantial reductions in both memory usage and computational time.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Modal Alignment via Variational Copula Modelling</title>
<link>https://arxiv.org/abs/2511.03196</link>
<guid>https://arxiv.org/abs/2511.03196</guid>
<content:encoded><![CDATA[
<div> Copula, multimodal learning, joint distribution, Gaussian mixture distribution, MIMIC dataset <br />
<br />
Summary: <br />
Various real-world applications involve multiple data modalities, such as electronic health records and medical images. Developing effective methods for multimodal learning is crucial for aggregating information from different modalities. Existing methods often oversimplify interactions between modalities, highlighting the need for more complex interaction modeling. This study introduces a novel copula-driven multimodal learning framework that focuses on capturing complex interactions by learning the joint distribution of different modalities. By using a Gaussian mixture distribution for each modality and a copula model for the joint distribution, the proposed model efficiently aligns marginal distributions and generates accurate representations for missing modalities. Experiments on MIMIC datasets demonstrate the superior performance of the model compared to other methods. The code for the model is publicly available for further exploration. <div>
arXiv:2511.03196v1 Announce Type: new 
Abstract: Various data modalities are common in real-world applications (e.g., electronic health records, medical images and clinical notes in healthcare). It is essential to develop multimodal learning methods to aggregate various information from multiple modalities. The main challenge is how to appropriately align and fuse the representations of different modalities into a joint distribution. Existing methods mainly rely on concatenation or the Kronecker product, oversimplifying the interaction structure between modalities and indicating a need to model more complex interactions. Additionally, the joint distribution of latent representations with higher-order interactions is underexplored. Copula is a powerful statistical structure for modelling the interactions among variables, as it naturally bridges the joint distribution and marginal distributions of multiple variables. We propose a novel copula-driven multimodal learning framework, which focuses on learning the joint distribution of various modalities to capture the complex interactions among them. The key idea is to interpret the copula model as a tool to align the marginal distributions of the modalities efficiently. By assuming a Gaussian mixture distribution for each modality and a copula model on the joint distribution, our model can generate accurate representations for missing modalities. Extensive experiments on public MIMIC datasets demonstrate the superior performance of our model over other competitors. The code is available at https://github.com/HKU-MedAI/CMCM.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Probabilistic U-Net Approach to Downscaling Climate Simulations</title>
<link>https://arxiv.org/abs/2511.03197</link>
<guid>https://arxiv.org/abs/2511.03197</guid>
<content:encoded><![CDATA[
<div> Keywords: climate models, statistical downscaling, U-Net, aleatoric uncertainty, extremities<br /> 

Summary: 
Climate models face limitations due to computational costs, often leading to coarse spatial resolution outputs that may not meet the requirements of climate change impact studies. To address this gap, statistical downscaling techniques are employed to produce finer scale results. In this study, the probabilistic U-Net framework is adapted for downscaling tasks by integrating a deterministic U-Net core with a variational latent space component to capture aleatoric uncertainty. The researchers evaluated four different training objectives, including afCRPS and WMSE-MS-SSIM, for downscaling precipitation and temperature data from a 16 times coarser resolution. Their analysis revealed that WMSE-MS-SSIM showed promising performance for extreme events under specific conditions, while afCRPS was more effective in capturing spatial variability across different scales.<br /><br />Summary: <div>
arXiv:2511.03197v1 Announce Type: new 
Abstract: Climate models are limited by heavy computational costs, often producing outputs at coarse spatial resolutions, while many climate change impact studies require finer scales. Statistical downscaling bridges this gap, and we adapt the probabilistic U-Net for this task, combining a deterministic U-Net backbone with a variational latent space to capture aleatoric uncertainty. We evaluate four training objectives, afCRPS and WMSE-MS-SSIM with three settings for downscaling precipitation and temperature from $16\times$ coarser resolution. Our main finding is that WMSE-MS-SSIM performs well for extremes under certain settings, whereas afCRPS better captures spatial variability across scales.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies</title>
<link>https://arxiv.org/abs/2511.03201</link>
<guid>https://arxiv.org/abs/2511.03201</guid>
<content:encoded><![CDATA[
<div> Keywords: IoT botnet, deep learning, model compression, quantization, detection performance <br />
Summary: 
This study addresses the need for lightweight detection models to counter IoT botnet-based attacks. A VAE-MLP model framework is proposed, where an MLP-based classifier is trained on 8-dimensional latent vectors derived from a pretrained VAE. Two quantization strategies, QAT and PTQ, are evaluated for their impact on detection performance, storage efficiency, and inference latency using IoT botnet datasets. The results show that PTQ outperforms QAT in terms of detection accuracy, with only a marginal reduction compared to the original unquantized model. PTQ also provides significant speedup and size reduction benefits, demonstrating the practicality of quantization for IoT botnet detection on resource-constrained devices. This research highlights the importance of efficient and effective detection methods in combating the growing threat of IoT botnet attacks. <br /><br />Summary: <div>
arXiv:2511.03201v1 Announce Type: new 
Abstract: In an effort to counter the increasing IoT botnet-based attacks, state-of-the-art deep learning methods have been proposed and have achieved impressive detection accuracy. However, their computational intensity restricts deployment on resource-constrained IoT devices, creating a critical need for lightweight detection models. A common solution to this challenge is model compression via quantization. This study proposes a VAE-MLP model framework where an MLP-based classifier is trained on 8-dimensional latent vectors derived from the high-dimensional train data using the encoder component of a pretrained variational autoencoder (VAE). Two widely used quantization strategies--Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ)--are then systematically evaluated in terms of their impact on detection performance, storage efficiency, and inference latency using two benchmark IoT botnet datasets--N-BaIoT and CICIoT2022. The results revealed that, with respect to detection accuracy, the QAT strategy experienced a more noticeable decline,whereas PTQ incurred only a marginal reduction compared to the original unquantized model. Furthermore, PTQ yielded a 6x speedup and 21x reduction in size, while QAT achieved a 3x speedup and 24x compression, demonstrating the practicality of quantization for device-level IoT botnet detection.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incorporating Quality of Life in Climate Adaptation Planning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.03238</link>
<guid>https://arxiv.org/abs/2511.03238</guid>
<content:encoded><![CDATA[
<div> Keywords: urban flooding, climate change, quality of life, reinforcement learning, adaptation strategies 

Summary: 
Urban flooding, exacerbated by climate change, is a growing concern impacting urban Quality of Life (QoL). Policymakers face the challenge of implementing effective adaptation strategies amidst uncertainty. This study employs Reinforcement Learning (RL) to identify optimal climate adaptation pathways for enhancing long-term QoL. An Integrated Assessment Model (IAM) integrates various models to assess the impacts of climate change on urban flooding, transportation accessibility, and QoL. Preliminary results suggest that RL-based approaches outperform traditional planning strategies in identifying effective adaptation measures. The publicly available framework provides a valuable tool for policymakers and researchers to enhance urban resilience and improve QoL in the face of climate change-induced urban flooding. 

<br /><br />Summary: <div>
arXiv:2511.03238v1 Announce Type: new 
Abstract: Urban flooding is expected to increase in frequency and severity as a consequence of climate change, causing wide-ranging impacts that include a decrease in urban Quality of Life (QoL). Meanwhile, policymakers must devise adaptation strategies that can cope with the uncertain nature of climate change and the complex and dynamic nature of urban flooding. Reinforcement Learning (RL) holds significant promise in tackling such complex, dynamic, and uncertain problems. Because of this, we use RL to identify which climate adaptation pathways lead to a higher QoL in the long term. We do this using an Integrated Assessment Model (IAM) which combines a rainfall projection model, a flood model, a transport accessibility model, and a quality of life index. Our preliminary results suggest that this approach can be used to learn optimal adaptation measures and it outperforms other realistic and real-world planning strategies. Our framework is publicly available: https://github.com/MLSM-at-DTU/maat_qol_framework.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams</title>
<link>https://arxiv.org/abs/2511.03239</link>
<guid>https://arxiv.org/abs/2511.03239</guid>
<content:encoded><![CDATA[
<div> closed-loop control, data-centric AI, data collection, probabilistic model, feedback mechanism

Summary:
- The paper introduces FCDC, a new paradigm that treats data collection as a closed-loop control problem in AI systems.
- FCDC continuously approximates the data distribution state using a probabilistic model and regulates sample retention based on feedback signals like likelihood and Mahalanobis distance.
- The system dynamically balances exploration and exploitation, maintains dataset diversity, and prevents redundancy accumulation.
- Experiments on a real data stream show that FCDC produces more balanced datasets with a 25.9% improvement and reduces data storage by 39.8%.
- The results highlight the importance of actively controlling data collection to transform it from a passive stage to a self-regulating, feedback-driven process at the core of data-centric AI. 

<br /><br />Summary: <div>
arXiv:2511.03239v1 Announce Type: new 
Abstract: Modern AI systems are increasingly constrained not by model capacity but by the quality and diversity of their data. Despite growing emphasis on data-centric AI, most datasets are still gathered in an open-loop manner which accumulates redundant samples without feedback from the current coverage. This results in inefficient storage, costly labeling, and limited generalization. To address this, this paper introduces \ac{FCDC}, a paradigm that formulates data collection as a closed-loop control problem. \ac{FCDC} continuously approximates the state of the collected data distribution using an online probabilistic model and adaptively regulates sample retention using based on feedback signals such as likelihood and Mahalanobis distance. Through this feedback mechanism, the system dynamically balances exploration and exploitation, maintains dataset diversity, and prevents redundancy from accumulating over time. Besides showcasing the controllability of \ac{FCDC} on a synthetic dataset, experiments on a real data stream show that \ac{FCDC} produces more balanced datasets by $\SI{25.9}{\percent}$ while reducing data storage by $\SI{39.8}{\percent}$. These results demonstrate that data collection itself can be actively controlled, transforming collection from a passive pipeline stage into a self-regulating, feedback-driven process at the core of data-centric AI.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A unified physics-informed generative operator framework for general inverse problems</title>
<link>https://arxiv.org/abs/2511.03241</link>
<guid>https://arxiv.org/abs/2511.03241</guid>
<content:encoded><![CDATA[
<div> Framework, Generative, Neural Operator, Inverse Problems, Partial Differential Equations <br />
Summary:
The article introduces a novel generative neural operator framework, IGNO, which aims to solve challenging inverse problems governed by partial differential equations (PDEs). The framework can handle sparse, noisy measurements, and high-dimensional or discontinuous coefficients without the need for labeled training pairs. IGNO encodes complex coefficient fields into a low-dimensional latent space, enabling neural operator decoders to reconstruct coefficients and PDE solutions. Training is based solely on physics constraints through PDE residuals, while inversion is performed via efficient gradient-based optimization in the latent space. The framework is accelerated by an a priori normalizing flow model. Across various difficult inverse problems, including recovering discontinuous coefficients and solving the EIT problem with operator-based measurements, IGNO consistently delivers accurate, stable, and scalable inversion even in the presence of severe noise. It outperforms existing methods, exhibits strong generalization capabilities, and proves to be a powerful and unified solution for challenging inverse problems in computational science domains.
<br /><br /> <div>
arXiv:2511.03241v1 Announce Type: new 
Abstract: Solving inverse problems governed by partial differential equations (PDEs) is central to science and engineering, yet remains challenging when measurements are sparse, noisy, or when the underlying coefficients are high-dimensional or discontinuous. Existing deep learning approaches either require extensive labeled datasets or are limited to specific measurement types, often leading to failure in such regimes and restricting their practical applicability. Here, a novel generative neural operator framework, IGNO, is introduced to overcome these limitations. IGNO unifies the solution of inverse problems from both point measurements and operator-valued data without labeled training pairs. This framework encodes high-dimensional, potentially discontinuous coefficient fields into a low-dimensional latent space, which drives neural operator decoders to reconstruct both coefficients and PDE solutions. Training relies purely on physics constraints through PDE residuals, while inversion proceeds via efficient gradient-based optimization in latent space, accelerated by an a priori normalizing flow model. Across a diverse set of challenging inverse problems, including recovery of discontinuous coefficients from solution-based measurements and the EIT problem with operator-based measurements, IGNO consistently achieves accurate, stable, and scalable inversion even under severe noise. It consistently outperforms the state-of-the-art method under varying noise levels and demonstrates strong generalization to out-of-distribution targets. These results establish IGNO as a unified and powerful framework for tackling challenging inverse problems across computational science domains.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life Adaptation Pathways</title>
<link>https://arxiv.org/abs/2511.03243</link>
<guid>https://arxiv.org/abs/2511.03243</guid>
<content:encoded><![CDATA[
<div> Keywords: Climate change, adaptation policymaking, Reinforcement Learning, Integrated Assessment Model, quality of life

Summary:<br />
Climate change is expected to increase the frequency and severity of flood events, necessitating effective adaptation policies. Managing the uncertainty of long-term climate impacts is crucial in designing such policies. The use of Reinforcement Learning (RL) can help identify adaptation pathways under uncertain conditions and explicitly model different adaptation priorities. This study utilized an Integrated Assessment Model (IAM) to assess the impacts of flooding on quality of life (QoL), transportation, and infrastructure damage. The results revealed that prioritizing QoL over economic impacts led to higher adaptation spending and a more evenly distributed allocation of resources. These findings underscore the importance of normative choices in shaping adaptation policies. The framework developed in this study is publicly available for further research and use. 

<br /><br />Summary: <div>
arXiv:2511.03243v1 Announce Type: new 
Abstract: Climate change will cause an increase in the frequency and severity of flood events, prompting the need for cohesive adaptation policymaking. Designing effective adaptation policies, however, depends on managing the uncertainty of long-term climate impacts. Meanwhile, such policies can feature important normative choices that are not always made explicit. We propose that Reinforcement Learning (RL) can be a useful tool to both identify adaptation pathways under uncertain conditions while it also allows for the explicit modelling (and consequent comparison) of different adaptation priorities (e.g. economic vs. wellbeing). We use an Integrated Assessment Model (IAM) to link together a rainfall and flood model, and compute the impacts of flooding in terms of quality of life (QoL), transportation, and infrastructure damage. Our results show that models prioritising QoL over economic impacts results in more adaptation spending as well as a more even distribution of spending over the study area, highlighting the extent to which such normative assumptions can alter adaptation policy. Our framework is publicly available: https://github.com/MLSM-at-DTU/maat_qol_framework.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models</title>
<link>https://arxiv.org/abs/2511.03251</link>
<guid>https://arxiv.org/abs/2511.03251</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Mixture-of-Experts, Prompt-based learning, Expert specialization, Transfer learning

Summary: 
- The article proposes GMoPE, a framework that integrates Mixture-of-Experts architecture with prompt-based learning for graphs to address generalization challenges in Graph Neural Networks.
- GMoPE utilizes expert-specific prompt vectors and structure-aware MoE routing to enable experts to specialize in distinct subdomains and contribute dynamically to predictions.
- A soft orthogonality constraint is introduced across prompt vectors to promote expert specialization and prevent expert collapse, facilitating balanced expert utilization.
- A prompt-only fine-tuning strategy is adopted to reduce spatiotemporal complexity during transfer, leading to more efficient adaptation.
- Extensive experiments validate GMoPE's superior performance compared to state-of-the-art baselines and demonstrate its ability to achieve results comparable to full parameter fine-tuning with significantly reduced adaptation overhead.

<br /><br />Summary: <div>
arXiv:2511.03251v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have demonstrated impressive performance on task-specific benchmarks, yet their ability to generalize across diverse domains and tasks remains limited. Existing approaches often struggle with negative transfer, scalability issues, and high adaptation costs. To address these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture with prompt-based learning for graphs. GMoPE leverages expert-specific prompt vectors and structure-aware MoE routing to enable each expert to specialize in distinct subdomains and dynamically contribute to predictions. To promote diversity and prevent expert collapse, we introduce a soft orthogonality constraint across prompt vectors, encouraging expert specialization and facilitating a more balanced expert utilization. Additionally, we adopt a prompt-only fine-tuning strategy that significantly reduces spatiotemporal complexity during transfer. We validate GMoPE through extensive experiments under various pretraining strategies and multiple downstream tasks. Results show that GMoPE consistently outperforms state-of-the-art baselines and achieves performance comparable to full parameter fine-tuning-while requiring only a fraction of the adaptation overhead. Our work provides a principled and scalable framework for advancing generalizable and efficient graph foundation models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupled Entropy Minimization</title>
<link>https://arxiv.org/abs/2511.03256</link>
<guid>https://arxiv.org/abs/2511.03256</guid>
<content:encoded><![CDATA[
<div> Entropy Minimization, Cluster Aggregation, Gradient Mitigation Calibrator, Adaptive Decoupled Entropy Minimization, Imperfectly Supervised Learning Tasks

Summary:
Entropy Minimization (EM) has benefits in reducing class overlap and bridging domain gap, but its potential is limited. The reformulation of EM into Cluster Aggregation Driving Factor (CADF) and Gradient Mitigation Calibrator (GMC) reveals issues such as reward collapse and easy-class bias. Adaptive Decoupled Entropy Minimization (AdaDEM) addresses these problems by normalizing CADF rewards and utilizing a Marginal Entropy Calibrator (MEC). AdaDEM outperforms a classical EM variant (DEM*) in various imperfectly supervised learning tasks, showing superior performance in noisy and dynamic environments. <div>
arXiv:2511.03256v1 Announce Type: new 
Abstract: Entropy Minimization (EM) is beneficial to reducing class overlap, bridging domain gap, and restricting uncertainty for various tasks in machine learning, yet its potential is limited. To study the internal mechanism of EM, we reformulate and decouple the classical EM into two parts with opposite effects: cluster aggregation driving factor (CADF) rewards dominant classes and prompts a peaked output distribution, while gradient mitigation calibrator (GMC) penalizes high-confidence classes based on predicted probabilities. Furthermore, we reveal the limitations of classical EM caused by its coupled formulation: 1) reward collapse impedes the contribution of high-certainty samples in the learning process, and 2) easy-class bias induces misalignment between output distribution and label distribution. To address these issues, we propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the reward brought from CADF and employs a marginal entropy calibrator (MEC) to replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM, and achieves superior performance across various imperfectly supervised learning tasks in noisy and dynamic environments.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Language Models are Super Data Learners</title>
<link>https://arxiv.org/abs/2511.03276</link>
<guid>https://arxiv.org/abs/2511.03276</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion language models, autoregressive models, pre-training data, Monte Carlo augmentation, data constraint

Summary:<br /><br />
In this study, the researchers explore the performance of diffusion language models (DLMs) compared to autoregressive (AR) models under controlled pre-training conditions. They find that DLMs excel when unique data is limited, surpassing AR models with extended training epochs. The crossover point shifts based on data quantity, model size, and architecture type. The researchers attribute this advantage to the ability of DLMs to model any order, utilize dense compute from bidirectional denoising, and incorporate Monte Carlo augmentation. Despite using standard pre-training data, a large DLM outperforms a strictly matched AR model on Python token data. Notably, a smaller DLM achieves high accuracy on challenging tasks like HellaSwag and MMLU with minimal data and no additional techniques. The study also challenges the notion that increased validation loss necessarily leads to worse downstream performance in this context. <div>
arXiv:2511.03276v1 Announce Type: new 
Abstract: Under strictly controlled pre-training settings, we observe a Crossover: when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. We attribute the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation; input or parameter noise improves AR under data constraint but cannot close the gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B unique Python tokens overtakes an AR coder trained with strictly matched settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag and > 33% on MMLU using only 1B tokens, without any special tricks, just by repeating standard pre-training data. We also show that rising validation cross-entropy does not imply degraded downstream performance in this regime.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Objective Adaptive Rate Limiting in Microservices Using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.03279</link>
<guid>https://arxiv.org/abs/2511.03279</guid>
<content:encoded><![CDATA[
<div> Keywords: API rate limiting, deep reinforcement learning, microservice architecture, Kubernetes cluster, system stability

Summary:
- This paper introduces an adaptive rate limiting strategy based on deep reinforcement learning to improve system stability and service quality in cloud computing and microservice architectures.
- The proposed hybrid architecture combines Deep Q-Network (DQN) and Asynchronous Advantage Actor-Critic (A3C) algorithms to model the rate limiting decision process as a Markov Decision Process.
- Through continuous monitoring of microservice states and learning optimal rate limiting policies, the system dynamically balances system throughput and service latency.
- Experimental results in a Kubernetes cluster environment show significant improvements in throughput (23.7%) and P99 latency reduction (31.4%) compared to traditional fixed-threshold strategies under high-load scenarios.
- A 90-day production deployment handling 500 million daily requests validates the practical effectiveness of the proposed method, with reductions in service degradation incidents (82%) and manual interventions (68%).

<br /><br />Summary: <div>
arXiv:2511.03279v1 Announce Type: new 
Abstract: As cloud computing and microservice architectures become increasingly prevalent, API rate limiting has emerged as a critical mechanism for ensuring system stability and service quality. Traditional rate limiting algorithms, such as token bucket and sliding window, while widely adopted, struggle to adapt to dynamic traffic patterns and varying system loads. This paper proposes an adaptive rate limiting strategy based on deep reinforcement learning that dynamically balances system throughput and service latency. We design a hybrid architecture combining Deep Q-Network (DQN) and Asynchronous Advantage Actor-Critic (A3C) algorithms, modeling the rate limiting decision process as a Markov Decision Process. The system continuously monitors microservice states and learns optimal rate limiting policies through environmental interaction. Extensive experiments conducted in a Kubernetes cluster environment demonstrate that our approach achieves 23.7% throughput improvement and 31.4% P99 latency reduction compared to traditional fixed-threshold strategies under high-load scenarios. Results from a 90-day production deployment handling 500 million daily requests validate the practical effectiveness of the proposed method, with 82% reduction in service degradation incidents and 68% decrease in manual interventions.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Probabilistic Approach to Pose Synchronization for Multi-Reference Alignment with Applications to MIMO Wireless Communication Systems</title>
<link>https://arxiv.org/abs/2511.03280</link>
<guid>https://arxiv.org/abs/2511.03280</guid>
<content:encoded><![CDATA[
<div> Probabilistic approach, multi-reference alignment, nuisance variables, decentralized algorithm, reconstruction error <br />
<br />Summary: <br />
The article introduces a new algorithm for multi-reference alignment (MRA), which is essential in various real-world applications like cryo-EM and wireless communication systems. By using a probabilistic model, the algorithm incorporates relative poses as nuisance variables to eliminate global symmetries, leading to improved convergence and more direct solutions. The decentralized approach of the algorithm results in significant computational savings by avoiding the cubic scaling of centralized methods through cycle consistency. Experimental results demonstrate that both proposed algorithms achieve lower reconstruction errors compared to existing methods. <div>
arXiv:2511.03280v1 Announce Type: new 
Abstract: From molecular imaging to wireless communications, the ability to align and reconstruct signals from multiple misaligned observations is crucial for system performance. We study the problem of multi-reference alignment (MRA), which arises in many real-world problems, such as cryo-EM, computer vision, and, in particular, wireless communication systems. Using a probabilistic approach to model MRA, we find a new algorithm that uses relative poses as nuisance variables to marginalize out -- thereby removing the global symmetries of the problem and allowing for more direct solutions and improved convergence. The decentralization of this approach enables significant computational savings by avoiding the cubic scaling of centralized methods through cycle consistency. Both proposed algorithms achieve lower reconstruction error across experimental settings.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Neural AI with Temporal Dynamics for Comprehensive Anomaly Detection in Microservices</title>
<link>https://arxiv.org/abs/2511.03285</link>
<guid>https://arxiv.org/abs/2511.03285</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, anomaly detection, microservice architectures, root cause tracing, temporal modeling<br />
<br />
Summary: <br />
This study introduces a unified framework for anomaly detection and root cause tracing in microservice architectures by combining graph neural networks with temporal modeling. The microservice call chain is represented as a directed graph, with features of nodes and edges used to construct a service topology representation. Graph convolution aggregates features and models dependencies, while gated recurrent units capture temporal evolution. Multi-layer stacking and concatenation operations yield structural and temporal representations to identify anomaly patterns. Anomaly scoring functions at node and path levels enable unified modeling for local anomaly detection and global call chain tracing, identifying abnormal service nodes and potential anomaly propagation paths. Sensitivity experiments demonstrate the framework's superiority over baseline methods in AUC, ACC, Recall, and F1-Score metrics, showcasing high accuracy and stability in dynamic and complex environments. This research paves the way for advanced anomaly detection in microservices and intelligent operations in distributed systems. <div>
arXiv:2511.03285v1 Announce Type: new 
Abstract: This study addresses the problem of anomaly detection and root cause tracing in microservice architectures and proposes a unified framework that combines graph neural networks with temporal modeling. The microservice call chain is abstracted as a directed graph, where multidimensional features of nodes and edges are used to construct a service topology representation, and graph convolution is applied to aggregate features across nodes and model dependencies, capturing complex structural relationships among services. On this basis, gated recurrent units are introduced to model the temporal evolution of call chains, and multi-layer stacking and concatenation operations are used to jointly obtain structural and temporal representations, improving the ability to identify anomaly patterns. Furthermore, anomaly scoring functions at both the node and path levels are defined to achieve unified modeling from local anomaly detection to global call chain tracing, which enables the identification of abnormal service nodes and the reconstruction of potential anomaly propagation paths. Sensitivity experiments are then designed from multiple dimensions, including hyperparameters, environmental disturbances, and data distribution, to evaluate the framework, and results show that it outperforms baseline methods in key metrics such as AUC, ACC, Recall, and F1-Score, maintaining high accuracy and stability under dynamic topologies and complex environments. This research not only provides a new technical path for anomaly detection in microservices but also lays a methodological foundation for intelligent operations in distributed systems.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extending Fair Null-Space Projections for Continuous Attributes to Kernel Methods</title>
<link>https://arxiv.org/abs/2511.03304</link>
<guid>https://arxiv.org/abs/2511.03304</guid>
<content:encoded><![CDATA[
<div> Kernel Methods, Continuous Fairness, Support Vector Regression, Fairness Notions, Machine Learning 

Summary: 
This article addresses the importance of fairness in machine learning systems, particularly in the context of continuous attributes, which has been underexplored in current literature. The focus is on developing a method for continuous fairness that can be applied to kernel methods, expanding the scope beyond linear models or embeddings. The proposed method, utilizing Support Vector Regression (SVR), demonstrates competitive or improved performance on multiple datasets compared to existing methods. By generalizing the iterative null-space projection strategy to kernel methods, this approach offers a model and fairness-score agnostic solution for addressing biases related to continuous protected attributes in machine learning systems. <div>
arXiv:2511.03304v1 Announce Type: new 
Abstract: With the on-going integration of machine learning systems into the everyday social life of millions the notion of fairness becomes an ever increasing priority in their development. Fairness notions commonly rely on protected attributes to assess potential biases. Here, the majority of literature focuses on discrete setups regarding both target and protected attributes. The literature on continuous attributes especially in conjunction with regression -- we refer to this as \emph{continuous fairness} -- is scarce. A common strategy is iterative null-space projection which as of now has only been explored for linear models or embeddings such as obtained by a non-linear encoder. We improve on this by generalizing to kernel methods, significantly extending the scope. This yields a model and fairness-score agnostic method for kernel embeddings applicable to continuous protected attributes. We demonstrate that our novel approach in conjunction with Support Vector Regression (SVR) provides competitive or improved performance across multiple datasets in comparisons to other contemporary methods.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration</title>
<link>https://arxiv.org/abs/2511.03344</link>
<guid>https://arxiv.org/abs/2511.03344</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse decision tree learning, Rashomon sets, variable importance analysis, scalability, anytime behavior 

Summary: 
SORTD is introduced as a framework for efficiently enumerating Rashomon sets to find accurate and interpretable predictive models within a given size limit. The traditional approach to finding the optimal tree is NP-hard, making enumeration of the Rashomon set challenging. SORTD aims to improve scalability and offer anytime behavior by ordering trees in the Rashomon set based on objective value. This framework significantly reduces runtime compared to existing methods, making it more practical for real-world applications. Additionally, SORTD can compute Rashomon sets for any separable and totally ordered objective, allowing for flexibility in post-evaluation using other objectives. This advancement enhances variable importance analysis, enables users to choose simpler trees based on stakeholder preferences, and enriches explanations in decision tree learning. <div>
arXiv:2511.03344v1 Announce Type: new 
Abstract: Sparse decision tree learning provides accurate and interpretable predictive models that are ideal for high-stakes applications by finding the single most accurate tree within a (soft) size limit. Rather than relying on a single "best" tree, Rashomon sets-trees with similar performance but varying structures-can be used to enhance variable importance analysis, enrich explanations, and enable users to choose simpler trees or those that satisfy stakeholder preferences (e.g., fairness) without hard-coding such criteria into the objective function. However, because finding the optimal tree is NP-hard, enumerating the Rashomon set is inherently challenging. Therefore, we introduce SORTD, a novel framework that improves scalability and enumerates trees in the Rashomon set in order of the objective value, thus offering anytime behavior. Our experiments show that SORTD reduces runtime by up to two orders of magnitude compared with the state of the art. Moreover, SORTD can compute Rashomon sets for any separable and totally ordered objective and supports post-evaluating the set using other separable (and partially ordered) objectives. Together, these advances make exploring Rashomon sets more practical in real-world applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation Agentic AI Applications</title>
<link>https://arxiv.org/abs/2511.03363</link>
<guid>https://arxiv.org/abs/2511.03363</guid>
<content:encoded><![CDATA[
<div> Keywords: intention recognition, agentic AI, transportation, multi-label, data-free

Summary:
The study presents a data-free pipeline for multi-label intention recognition in agentic AI applications in transportation. The pipeline, named DMTC, consists of prompt engineering to generate synthetic queries, encoding queries with Sentence-T5 model, and training a lightweight classifier with online focal-contrastive loss. The pipeline is demonstrated in maritime transportation and achieves a Hamming loss of 5.35% and an AUC of 95.92%. Sentence-T5 embeddings improve accuracy, and the OFC loss enhances inter-class separability. The system directs user queries to specific modules such as ETA information and traffic risk evaluation, enabling fully autonomous agents without manual labeling requirements.

<br /><br />Summary: <div>
arXiv:2511.03363v1 Announce Type: new 
Abstract: In this study, a modular, data-free pipeline for multi-label intention recognition is proposed for agentic AI applications in transportation. Unlike traditional intent recognition systems that depend on large, annotated corpora and often struggle with fine-grained, multi-label discrimination, our approach eliminates the need for costly data collection while enhancing the accuracy of multi-label intention understanding. Specifically, the overall pipeline, named DMTC, consists of three steps: 1) using prompt engineering to guide large language models (LLMs) to generate diverse synthetic queries in different transport scenarios; 2) encoding each textual query with a Sentence-T5 model to obtain compact semantic embeddings; 3) training a lightweight classifier using a novel online focal-contrastive (OFC) loss that emphasizes hard samples and maximizes inter-class separability. The applicability of the proposed pipeline is demonstrated in an agentic AI application in the maritime transportation context. Extensive experiments show that DMTC achieves a Hamming loss of 5.35% and an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers and recent end-to-end SOTA LLM-based baselines. Further analysis reveals that Sentence-T5 embeddings improve subset accuracy by at least 3.29% over alternative encoders, and integrating the OFC loss yields an additional 0.98% gain compared to standard contrastive objectives. In conclusion, our system seamlessly routes user queries to task-specific modules (e.g., ETA information, traffic risk evaluation, and other typical scenarios in the transportation domain), laying the groundwork for fully autonomous, intention-aware agents without costly manual labelling.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TripleWin: Fixed-Point Equilibrium Pricing for Data-Model Coupled Markets</title>
<link>https://arxiv.org/abs/2511.03368</link>
<guid>https://arxiv.org/abs/2511.03368</guid>
<content:encoded><![CDATA[
<div> model economy, machine learning, dataset pricing, market mechanism, fairness

Summary:
This study introduces a unified data-model coupled market that integrates dataset and model transactions into a single system. By incorporating supply-side and demand-side mappings, the market links interactions among data sellers, model producers, and model buyers. The proposed mechanism ensures fairness and efficiency by leveraging Shapley-based allocation and a standard interference function (SIF) as the joint operator. Experimental results show improved convergence and fairness compared to traditional broker-centric and one-sided approaches. The code for the market mechanism is publicly available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2511.03368v1 Announce Type: new 
Abstract: The rise of the machine learning (ML) model economy has intertwined markets for training datasets and pre-trained models. However, most pricing approaches still separate data and model transactions or rely on broker-centric pipelines that favor one side. Recent studies of data markets with externalities capture buyer interactions but do not yield a simultaneous and symmetric mechanism across data sellers, model producers, and model buyers. We propose a unified data-model coupled market that treats dataset and model trading as a single system. A supply-side mapping transforms dataset payments into buyer-visible model quotations, while a demand-side mapping propagates buyer prices back to datasets through Shapley-based allocation. Together, they form a closed loop that links four interactions: supply-demand propagation in both directions and mutual coupling among buyers and among sellers. We prove that the joint operator is a standard interference function (SIF), guaranteeing existence, uniqueness, and global convergence of equilibrium prices. Experiments demonstrate efficient convergence and improved fairness compared with broker-centric and one-sided baselines. The code is available on https://github.com/HongrunRen1109/Triple-Win-Pricing.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptable Hindsight Experience Replay for Search-Based Learning</title>
<link>https://arxiv.org/abs/2511.03405</link>
<guid>https://arxiv.org/abs/2511.03405</guid>
<content:encoded><![CDATA[
<div> keyword: AlphaZero-like, Monte Carlo Tree Search, Hindsight Experience Replay, supervised learning, reinforcement learning 

Summary:
AlphaZero-like Monte Carlo Tree Search systems, initially designed for two-player games, have been adapted for classical search problems by incorporating neural network guidance to balance exploration and exploitation. However, in sparse reward scenarios, the network's guidance may be limited in the early stages. Hindsight Experience Replay (HER) addresses this limitation by relabeling unsuccessful trajectories from the search tree for supervised learning. The Adaptable HER framework integrates HER with AlphaZero, allowing for customizable adjustments to HER properties. Experimental results, including equation discovery, show that the flexibility of modifying HER improves performance, outperforming pure supervised or reinforcement learning approaches. This research demonstrates the benefits of combining AlphaZero-like methods with HER for enhancing the efficiency and effectiveness of search algorithms. 

<br /><br />Summary: <div>
arXiv:2511.03405v1 Announce Type: new 
Abstract: AlphaZero-like Monte Carlo Tree Search systems, originally introduced for two-player games, dynamically balance exploration and exploitation using neural network guidance. This combination makes them also suitable for classical search problems. However, the original method of training the network with simulation results is limited in sparse reward settings, especially in the early stages, where the network cannot yet give guidance. Hindsight Experience Replay (HER) addresses this issue by relabeling unsuccessful trajectories from the search tree as supervised learning signals. We introduce Adaptable HER (\ours{}), a flexible framework that integrates HER with AlphaZero, allowing easy adjustments to HER properties such as relabeled goals, policy targets, and trajectory selection. Our experiments, including equation discovery, show that the possibility of modifying HER is beneficial and surpasses the performance of pure supervised or reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POEMS: Product of Experts for Interpretable Multi-omic Integration using Sparse Decoding</title>
<link>https://arxiv.org/abs/2511.03464</link>
<guid>https://arxiv.org/abs/2511.03464</guid>
<content:encoded><![CDATA[
<div> Sparse decoding, multiomics integration, interpretability, product of experts, biomarker discovery  
Summary:  
- POEMS is a novel probabilistic framework for integrating multiomics data while maintaining both predictive performance and interpretability.  
- It preserves nonlinear expressiveness by mapping features to latent factors using sparse connections for biomarker discovery.  
- POEMS enables cross-omic associations through a shared latent space using a product of experts model.  
- A gating network in POEMS adaptively computes the influence of each omic in representation learning, providing insights into individual omics contributions.  
- In a cancer subtyping case study, POEMS demonstrates competitive clustering and classification performance, highlighting its ability to balance biomarker-based interpretation and predictive accuracy in multiomics representation learning.  
  
<br /><br />Summary: <div>
arXiv:2511.03464v1 Announce Type: new 
Abstract: Integrating different molecular layers, i.e., multiomics data, is crucial for unraveling the complexity of diseases; yet, most deep generative models either prioritize predictive performance at the expense of interpretability or enforce interpretability by linearizing the decoder, thereby weakening the network's nonlinear expressiveness. To overcome this tradeoff, we introduce POEMS: Product Of Experts for Interpretable Multiomics Integration using Sparse Decoding, an unsupervised probabilistic framework that preserves predictive performance while providing interpretability. POEMS provides interpretability without linearizing any part of the network by 1) mapping features to latent factors using sparse connections, which directly translates to biomarker discovery, 2) allowing for cross-omic associations through a shared latent space using product of experts model, and 3) reporting contributions of each omic by a gating network that adaptively computes their influence in the representation learning. Additionally, we present an efficient sparse decoder. In a cancer subtyping case study, POEMS achieves competitive clustering and classification performance while offering our novel set of interpretations, demonstrating that biomarker based insight and predictive accuracy can coexist in multiomics representation learning.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning Using known Invariances</title>
<link>https://arxiv.org/abs/2511.03473</link>
<guid>https://arxiv.org/abs/2511.03473</guid>
<content:encoded><![CDATA[
arXiv:2511.03473v1 Announce Type: new 
Abstract: In many real-world reinforcement learning (RL) problems, the environment exhibits inherent symmetries that can be exploited to improve learning efficiency. This paper develops a theoretical and algorithmic framework for incorporating known group symmetries into kernel-based RL. We propose a symmetry-aware variant of optimistic least-squares value iteration (LSVI), which leverages invariant kernels to encode invariance in both rewards and transition dynamics. Our analysis establishes new bounds on the maximum information gain and covering numbers for invariant RKHSs, explicitly quantifying the sample efficiency gains from symmetry. Empirical results on a customized Frozen Lake environment and a 2D placement design problem confirm the theoretical improvements, demonstrating that symmetry-aware RL achieves significantly better performance than their standard kernel counterparts. These findings highlight the value of structural priors in designing more sample-efficient reinforcement learning algorithms.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse</title>
<link>https://arxiv.org/abs/2511.03475</link>
<guid>https://arxiv.org/abs/2511.03475</guid>
<content:encoded><![CDATA[
arXiv:2511.03475v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with retrieved context but often suffers from downgraded prefill performance as modern applications demand longer and more complex inputs. Existing caching techniques either preserve accuracy with low cache reuse or improve reuse at the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG system that achieves high cache reuse without sacrificing accuracy through accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items across concurrent sessions and multi-turn interactions, using efficient context indexing, ordering, and de-duplication to maximize reuse, while lightweight contextual hints maintain reasoning fidelity. It integrates seamlessly with existing LLM inference engines and improves their prefill performance by 1.5-3X over state-of-the-art methods, while preserving or even enhancing reasoning accuracy across diverse RAG and agentic AI workloads. Our code is released at: https://github.com/Edinburgh-AgenticAI/RAGBoost.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NAP: Attention-Based Late Fusion for Automatic Sleep Staging</title>
<link>https://arxiv.org/abs/2511.03488</link>
<guid>https://arxiv.org/abs/2511.03488</guid>
<content:encoded><![CDATA[
arXiv:2511.03488v1 Announce Type: new 
Abstract: Polysomnography signals are highly heterogeneous, varying in modality composition (e.g., EEG, EOG, ECG), channel availability (e.g., frontal, occipital EEG), and acquisition protocols across datasets and clinical sites. Most existing models that process polysomnography data rely on a fixed subset of modalities or channels and therefore neglect to fully exploit its inherently multimodal nature. We address this limitation by introducing NAP (Neural Aggregator of Predictions), an attention-based model which learns to combine multiple prediction streams using a tri-axial attention mechanism that captures temporal, spatial, and predictor-level dependencies. NAP is trained to adapt to different input dimensions. By aggregating outputs from frozen, pretrained single-channel models, NAP consistently outperforms individual predictors and simple ensembles, achieving state-of-the-art zero-shot generalization across multiple datasets. While demonstrated in the context of automated sleep staging from polysomnography, the proposed approach could be extended to other multimodal physiological applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Less is More (Sometimes): A Theory of Data Curation</title>
<link>https://arxiv.org/abs/2511.03492</link>
<guid>https://arxiv.org/abs/2511.03492</guid>
<content:encoded><![CDATA[
arXiv:2511.03492v1 Announce Type: new 
Abstract: This paper introduces a theoretical framework to resolve a central paradox in modern machine learning: When is it better to use less data? This question has become critical as classical scaling laws suggesting ``more is more'' (Sun et al., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et al., 2025; Muenighoff et al., 2025), which achieve superior performance with small, aggressively curated datasets. Here, we study data curation strategies where an imperfect oracle selects the training examples according to their difficulty and correctness. Our results provide exact scaling law curves for test error under both label-agnostic and label-aware curation rules, revealing when and why keeping only a subset of data can improve generalization. In contrast to classical scaling laws, we show that under certain conditions, small curated datasets can outperform full datasets, and we provide analytical conditions for this by deriving precise phase transition curves tied to data size and quality. We validate these theoretical claims with empirical results on ImageNet, confirming our predictions about when curation improves accuracy and can even mitigate model collapse. Furthermore, our framework provides a principled explanation for the contradictory curation strategies recently observed in LLM mathematical reasoning.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments</title>
<link>https://arxiv.org/abs/2511.03527</link>
<guid>https://arxiv.org/abs/2511.03527</guid>
<content:encoded><![CDATA[
arXiv:2511.03527v1 Announce Type: new 
Abstract: Group Relative Policy Optimization (GRPO) has emerged as a scalable alternative to Proximal Policy Optimization (PPO) by eliminating the learned critic and instead estimating advantages through group-relative comparisons of trajectories. This simplification raises fundamental questions about the necessity of learned baselines in policy-gradient methods. We present the first systematic study of GRPO in classical single-task reinforcement learning environments, spanning discrete and continuous control tasks. Through controlled ablations isolating baselines, discounting, and group sampling, we reveal three key findings: (1) learned critics remain essential for long-horizon tasks: all critic-free baselines underperform PPO except in short-horizon environments like CartPole where episodic returns can be effective; (2) GRPO benefits from high discount factors (gamma = 0.99) except in HalfCheetah, where lack of early termination favors moderate discounting (gamma = 0.9); (3) smaller group sizes outperform larger ones, suggesting limitations in batch-based grouping strategies that mix unrelated episodes. These results reveal both the limitations of critic-free methods in classical control and the specific conditions where they remain viable alternatives to learned value functions.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Byzantine-Robust Federated Learning with Learnable Aggregation Weights</title>
<link>https://arxiv.org/abs/2511.03529</link>
<guid>https://arxiv.org/abs/2511.03529</guid>
<content:encoded><![CDATA[
arXiv:2511.03529v1 Announce Type: new 
Abstract: Federated Learning (FL) enables clients to collaboratively train a global model without sharing their private data. However, the presence of malicious (Byzantine) clients poses significant challenges to the robustness of FL, particularly when data distributions across clients are heterogeneous. In this paper, we propose a novel Byzantine-robust FL optimization problem that incorporates adaptive weighting into the aggregation process. Unlike conventional approaches, our formulation treats aggregation weights as learnable parameters, jointly optimizing them alongside the global model parameters. To solve this optimization problem, we develop an alternating minimization algorithm with strong convergence guarantees under adversarial attack. We analyze the Byzantine resilience of the proposed objective. We evaluate the performance of our algorithm against state-of-the-art Byzantine-robust FL approaches across various datasets and attack scenarios. Experimental results demonstrate that our method consistently outperforms existing approaches, particularly in settings with highly heterogeneous data and a large proportion of malicious clients.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Neural Networks with Discrete Cosine Transform Activations</title>
<link>https://arxiv.org/abs/2511.03531</link>
<guid>https://arxiv.org/abs/2511.03531</guid>
<content:encoded><![CDATA[
arXiv:2511.03531v1 Announce Type: new 
Abstract: In this paper, we extend our previous work on the Expressive Neural Network (ENN), a multilayer perceptron with adaptive activation functions parametrized using the Discrete Cosine Transform (DCT). Building upon previous work that demonstrated the strong expressiveness of ENNs with compact architectures, we now emphasize their efficiency, interpretability and pruning capabilities. The DCT-based parameterization provides a structured and decorrelated representation that reveals the functional role of each neuron and allows direct identification of redundant components. Leveraging this property, we propose an efficient pruning strategy that removes unnecessary DCT coefficients with negligible or no loss in performance. Experimental results across classification and implicit neural representation tasks confirm that ENNs achieve state-of-the-art accuracy while maintaining a low number of parameters. Furthermore, up to 40% of the activation coefficients can be safely pruned, thanks to the orthogonality and bounded nature of the DCT basis. Overall, these findings demonstrate that the ENN framework offers a principled integration of signal processing concepts into neural network design, achieving a balanced trade-off between expressiveness, compactness, and interpretability.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flat Minima and Generalization: Insights from Stochastic Convex Optimization</title>
<link>https://arxiv.org/abs/2511.03548</link>
<guid>https://arxiv.org/abs/2511.03548</guid>
<content:encoded><![CDATA[
arXiv:2511.03548v1 Announce Type: new 
Abstract: Understanding the generalization behavior of learning algorithms is a central goal of learning theory. A recently emerging explanation is that learning algorithms are successful in practice because they converge to flat minima, which have been consistently associated with improved generalization performance. In this work, we study the link between flat minima and generalization in the canonical setting of stochastic convex optimization with a non-negative, $\beta$-smooth objective. Our first finding is that, even in this fundamental and well-studied setting, flat empirical minima may incur trivial $\Omega(1)$ population risk while sharp minima generalizes optimally. Then, we show that this poor generalization behavior extends to two natural ''sharpness-aware'' algorithms originally proposed by Foret et al. (2021), designed to bias optimization toward flat solutions: Sharpness-Aware Gradient Descent (SA-GD) and Sharpness-Aware Minimization (SAM). For SA-GD, which performs gradient steps on the maximal loss in a predefined neighborhood, we prove that while it successfully converges to a flat minimum at a fast rate, the population risk of the solution can still be as large as $\Omega(1)$, indicating that even flat minima found algorithmically using a sharpness-aware gradient method might generalize poorly. For SAM, a computationally efficient approximation of SA-GD based on normalized ascent steps, we show that although it minimizes the empirical loss, it may converge to a sharp minimum and also incur population risk $\Omega(1)$. Finally, we establish population risk upper bounds for both SA-GD and SAM using algorithmic stability techniques.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances</title>
<link>https://arxiv.org/abs/2511.03565</link>
<guid>https://arxiv.org/abs/2511.03565</guid>
<content:encoded><![CDATA[
arXiv:2511.03565v1 Announce Type: new 
Abstract: Imitation learning (IL) enables agents to acquire skills by observing and replicating the behavior of one or multiple experts. In recent years, advances in deep learning have significantly expanded the capabilities and scalability of imitation learning across a range of domains, where expert data can range from full state-action trajectories to partial observations or unlabeled sequences. Alongside this growth, novel approaches have emerged, with new methodologies being developed to address longstanding challenges such as generalization, covariate shift, and demonstration quality. In this survey, we review the latest advances in imitation learning research, highlighting recent trends, methodological innovations, and practical applications. We propose a novel taxonomy that is distinct from existing categorizations to better reflect the current state of the IL research stratum and its trends. Throughout the survey, we critically examine the strengths, limitations, and evaluation practices of representative works, and we outline key challenges and open directions for future research.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and Retrieval</title>
<link>https://arxiv.org/abs/2511.03570</link>
<guid>https://arxiv.org/abs/2511.03570</guid>
<content:encoded><![CDATA[
arXiv:2511.03570v1 Announce Type: new 
Abstract: We study LLMs for tabular prediction with mixed text, numeric, and categorical fields. We introduce TabGemma, a schema-agnostic in-context learner that treats rows as sequences and tackles two practical hurdles when adapting pretrained LLMs for tabular predictions: unstable numeric tokenization and limited context size. We propose to canonicalize numbers via signed scientific notation and continue pretraining of a 12B Gemma 3 model with a target imputation objective using a large-scale real world dataset. For inference, we use a compact n-gram-based retrieval to select informative exemplars that fit within a 128k-token window.
  On semantically rich benchmarks, TabGemma establishes a new state of the art on classification across low- and high-data regimes and improves monotonically with more context rows. For regression, it is competitive at small sample sizes but trails conventional approaches as data grows. Our results show that LLMs can be effective tabular in-context learners on highly semantic tasks when paired with dedicated numeric handling and context retrieval, while motivating further advances in numeric modeling and long-context scaling.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates Hallucinations</title>
<link>https://arxiv.org/abs/2511.03578</link>
<guid>https://arxiv.org/abs/2511.03578</guid>
<content:encoded><![CDATA[
arXiv:2511.03578v1 Announce Type: new 
Abstract: Neural networks can approximate solutions to partial differential equations, but they often break the very laws they are meant to model-creating mass from nowhere, drifting shocks, or violating conservation and entropy. We address this by training within the laws of physics rather than beside them. Our framework, called Constraint-Projected Learning (CPL), keeps every update physically admissible by projecting network outputs onto the intersection of constraint sets defined by conservation, Rankine-Hugoniot balance, entropy, and positivity. The projection is differentiable and adds only about 10% computational overhead, making it fully compatible with back-propagation. We further stabilize training with total-variation damping (TVD) to suppress small oscillations and a rollout curriculum that enforces consistency over long prediction horizons. Together, these mechanisms eliminate both hard and soft violations: conservation holds at machine precision, total-variation growth vanishes, and entropy and error remain bounded. On Burgers and Euler systems, CPL produces stable, physically lawful solutions without loss of accuracy. Instead of hoping neural solvers will respect physics, CPL makes that behavior an intrinsic property of the learning process.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tensor-Efficient High-Dimensional Q-learning</title>
<link>https://arxiv.org/abs/2511.03595</link>
<guid>https://arxiv.org/abs/2511.03595</guid>
<content:encoded><![CDATA[
arXiv:2511.03595v1 Announce Type: new 
Abstract: High-dimensional reinforcement learning faces challenges with complex calculations and low sample efficiency in large state-action spaces. Q-learning algorithms struggle particularly with the curse of dimensionality, where the number of state-action pairs grows exponentially with problem size. While neural network-based approaches like Deep Q-Networks have shown success, recent tensor-based methods using low-rank decomposition offer more parameter-efficient alternatives. Building upon existing tensor-based methods, we propose Tensor-Efficient Q-Learning (TEQL), which enhances low-rank tensor decomposition via improved block coordinate descent on discretized state-action spaces, incorporating novel exploration and regularization mechanisms. The key innovation is an exploration strategy that combines approximation error with visit count-based upper confidence bound to prioritize actions with high uncertainty, avoiding wasteful random exploration. Additionally, we incorporate a frequency-based penalty term in the objective function to encourage exploration of less-visited state-action pairs and reduce overfitting to frequently visited regions. Empirical results on classic control tasks demonstrate that TEQL outperforms conventional matrix-based methods and deep RL approaches in both sample efficiency and total rewards, making it suitable for resource-constrained applications, such as space and healthcare where sampling costs are high.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.03616</link>
<guid>https://arxiv.org/abs/2511.03616</guid>
<content:encoded><![CDATA[
arXiv:2511.03616v1 Announce Type: new 
Abstract: Imitation learning traditionally requires complete state-action demonstrations from optimal or near-optimal experts. These requirements severely limit practical applicability, as many real-world scenarios provide only state observations without corresponding actions and expert performance is often suboptimal. In this paper we introduce a deep implicit imitation reinforcement learning framework that addresses both limitations by combining deep reinforcement learning with implicit imitation learning from observation-only datasets. Our main algorithm, Deep Implicit Imitation Q-Network (DIIQN), employs an action inference mechanism that reconstructs expert actions through online exploration and integrates a dynamic confidence mechanism that adaptively balances expert-guided and self-directed learning. This enables the agent to leverage expert guidance for accelerated training while maintaining capacity to surpass suboptimal expert performance. We further extend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to tackle scenarios where expert and agent possess different action sets, a challenge previously unaddressed in the implicit imitation learning literature. HA-DIIQN introduces an infeasibility detection mechanism and a bridging procedure identifying alternative pathways connecting agent capabilities to expert guidance when direct action replication is impossible. Our experimental results demonstrate that DIIQN achieves up to 130% higher episodic returns compared to standard DQN, while consistently outperforming existing implicit imitation methods that cannot exceed expert performance. In heterogeneous action settings, HA-DIIQN learns up to 64% faster than baselines, leveraging expert datasets unusable by conventional approaches. Extensive parameter sensitivity analysis reveals the framework's robustness across varying dataset sizes and hyperparameter configurations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Formalizing Reinforcement Learning Theory</title>
<link>https://arxiv.org/abs/2511.03618</link>
<guid>https://arxiv.org/abs/2511.03618</guid>
<content:encoded><![CDATA[
arXiv:2511.03618v1 Announce Type: new 
Abstract: In this paper, we formalize the almost sure convergence of $Q$-learning and linear temporal difference (TD) learning with Markovian samples using the Lean 4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are among the earliest and most influential reinforcement learning (RL) algorithms. The investigation of their convergence properties is not only a major research topic during the early development of the RL field but also receives increasing attention nowadays. This paper formally verifies their almost sure convergence in a unified framework based on the Robbins-Siegmund theorem. The framework developed in this work can be easily extended to convergence rates and other modes of convergence. This work thus makes an important step towards fully formalizing convergent RL results. The code is available at https://github.com/ShangtongZhang/rl-theory-in-lean.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial Management System for SMEs: Real-World Deployment of Accounts Receivable and Cash Flow Prediction</title>
<link>https://arxiv.org/abs/2511.03631</link>
<guid>https://arxiv.org/abs/2511.03631</guid>
<content:encoded><![CDATA[
arXiv:2511.03631v1 Announce Type: new 
Abstract: Small and Medium Enterprises (SMEs), particularly freelancers and early-stage businesses, face unique financial management challenges due to limited resources, small customer bases, and constrained data availability. This paper presents the development and deployment of an integrated financial prediction system that combines accounts receivable prediction and cash flow forecasting specifically designed for SME operational constraints. Our system addresses the gap between enterprise-focused financial tools and the practical needs of freelancers and small businesses. The solution integrates two key components: a binary classification model for predicting invoice payment delays, and a multi-module cash flow forecasting model that handles incomplete and limited historical data. A prototype system has been implemented and deployed as a web application with integration into Cluee's platform, a startup providing financial management tools for freelancers, demonstrating practical feasibility for real-world SME financial management.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN</title>
<link>https://arxiv.org/abs/2511.03634</link>
<guid>https://arxiv.org/abs/2511.03634</guid>
<content:encoded><![CDATA[
arXiv:2511.03634v1 Announce Type: new 
Abstract: Tabular foundation models such as TabPFN have revolutionized predictive machine learning for tabular data. At the same time, the driving factors of this revolution are hard to understand. Existing open-source tabular foundation models are implemented in complicated pipelines boasting over 10,000 lines of code, lack architecture documentation or code quality. In short, the implementations are hard to understand, not beginner-friendly, and complicated to adapt for new experiments. We introduce nanoTabPFN, a simplified and lightweight implementation of the TabPFN v2 architecture and a corresponding training loop that uses pre-generated training data. nanoTabPFN makes tabular foundation models more accessible to students and researchers alike. For example, restricted to a small data setting it achieves a performance comparable to traditional machine learning baselines within one minute of pre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This eliminated requirement of large computational resources makes pre-training tabular foundation models accessible for educational purposes. Our code is available at https://github.com/automl/nanoTabPFN.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SHIELD: Securing Healthcare IoT with Efficient Machine Learning Techniques for Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.03661</link>
<guid>https://arxiv.org/abs/2511.03661</guid>
<content:encoded><![CDATA[
arXiv:2511.03661v1 Announce Type: new 
Abstract: The integration of IoT devices in healthcare introduces significant security and reliability challenges, increasing susceptibility to cyber threats and operational anomalies. This study proposes a machine learning-driven framework for (1) detecting malicious cyberattacks and (2) identifying faulty device anomalies, leveraging a dataset of 200,000 records. Eight machine learning models are evaluated across three learning approaches: supervised learning (XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (Generative Adversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervised learning (One-Class Support Vector Machine (SVM), Isolation Forest, Graph Neural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). The comprehensive evaluation was conducted across multiple metrics like F1-score, precision, recall, accuracy, ROC-AUC, computational efficiency. XGBoost achieved 99\% accuracy with minimal computational overhead (0.04s) for anomaly detection, while Isolation Forest balanced precision and recall effectively. LSTM Autoencoders underperformed with lower accuracy and higher latency. For attack detection, KNN achieved near-perfect precision, recall, and F1-score with the lowest computational cost (0.05s), followed by VAE at 97% accuracy. GAN showed the highest computational cost with lowest accuracy and ROC-AUC. These findings enhance IoT-enabled healthcare security through effective anomaly detection strategies. By improving early detection of cyber threats and device failures, this framework has the potential to prevent data breaches, minimize system downtime, and ensure the continuous and safe operation of medical devices, ultimately safeguarding patient health and trust in IoT-driven healthcare solutions.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay</title>
<link>https://arxiv.org/abs/2511.03670</link>
<guid>https://arxiv.org/abs/2511.03670</guid>
<content:encoded><![CDATA[
arXiv:2511.03670v1 Announce Type: new 
Abstract: We present a detailed study of Deep Q-Networks in finite environments, emphasizing the impact of epsilon-greedy exploration schedules and prioritized experience replay. Through systematic experimentation, we evaluate how variations in epsilon decay schedules affect learning efficiency, convergence behavior, and reward optimization. We investigate how prioritized experience replay leads to faster convergence and higher returns and show empirical results comparing uniform, no replay, and prioritized strategies across multiple simulations. Our findings illuminate the trade-offs and interactions between exploration strategies and memory management in DQN training, offering practical recommendations for robust reinforcement learning in resource-constrained settings.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Matrix Scaling for Multi-Class Calibration</title>
<link>https://arxiv.org/abs/2511.03685</link>
<guid>https://arxiv.org/abs/2511.03685</guid>
<content:encoded><![CDATA[
arXiv:2511.03685v1 Announce Type: new 
Abstract: Post-hoc recalibration methods are widely used to ensure that classifiers provide faithful probability estimates. We argue that parametric recalibration functions based on logistic regression can be motivated from a simple theoretical setting for both binary and multiclass classification. This insight motivates the use of more expressive calibration methods beyond standard temperature scaling. For multi-class calibration however, a key challenge lies in the increasing number of parameters introduced by more complex models, often coupled with limited calibration data, which can lead to overfitting. Through extensive experiments, we demonstrate that the resulting bias-variance tradeoff can be effectively managed by structured regularization, robust preprocessing and efficient optimization. The resulting methods lead to substantial gains over existing logistic-based calibration techniques. We provide efficient and easy-to-use open-source implementations of our methods, making them an attractive alternative to common temperature, vector, and matrix scaling implementations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL</title>
<link>https://arxiv.org/abs/2511.03695</link>
<guid>https://arxiv.org/abs/2511.03695</guid>
<content:encoded><![CDATA[
arXiv:2511.03695v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) enables training from fixed data without online interaction, but policies learned offline often struggle when deployed in dynamic environments due to distributional shift and unreliable value estimates on unseen state-action pairs. We introduce Behavior-Adaptive Q-Learning (BAQ), a framework designed to enable a smooth and reliable transition from offline to online RL. The key idea is to leverage an implicit behavioral model derived from offline data to provide a behavior-consistency signal during online fine-tuning. BAQ incorporates a dual-objective loss that (i) aligns the online policy toward the offline behavior when uncertainty is high, and (ii) gradually relaxes this constraint as more confident online experience is accumulated. This adaptive mechanism reduces error propagation from out-of-distribution estimates, stabilizes early online updates, and accelerates adaptation to new scenarios. Across standard benchmarks, BAQ consistently outperforms prior offline-to-online RL approaches, achieving faster recovery, improved robustness, and higher overall performance. Our results demonstrate that implicit behavior adaptation is a principled and practical solution for reliable real-world policy deployment.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing</title>
<link>https://arxiv.org/abs/2511.03697</link>
<guid>https://arxiv.org/abs/2511.03697</guid>
<content:encoded><![CDATA[
arXiv:2511.03697v1 Announce Type: new 
Abstract: Analog/mixed-signal circuits are key for interfacing electronics with the physical world. Their design, however, remains a largely handcrafted process, resulting in long and error-prone design cycles. While the recent rise of AI-based reinforcement learning and generative AI has created new techniques to automate this task, the need for many time-consuming simulations is a critical bottleneck hindering the overall efficiency. Furthermore, the lack of explainability of the resulting design solutions hampers widespread adoption of the tools. To address these issues, a novel agentic AI framework for sample-efficient and explainable analog circuit sizing is presented. It employs a multi-agent workflow where specialized Large Language Model (LLM)-based agents collaborate to interpret the circuit topology, to understand the design goals, and to iteratively refine the circuit's design parameters towards the target goals with human-interpretable reasoning. The adaptive simulation strategy creates an intelligent control that yields a high sample efficiency. The AnaFlow framework is demonstrated for two circuits of varying complexity and is able to complete the sizing task fully automatically, differently from pure Bayesian optimization and reinforcement learning approaches. The system learns from its optimization history to avoid past mistakes and to accelerate convergence. The inherent explainability makes this a powerful tool for analog design space exploration and a new paradigm in analog EDA, where AI agents serve as transparent design assistants.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards</title>
<link>https://arxiv.org/abs/2511.03710</link>
<guid>https://arxiv.org/abs/2511.03710</guid>
<content:encoded><![CDATA[
arXiv:2511.03710v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for post-training large reasoning models (LRMs) using policy-gradient methods such as GRPO. To stabilize training, these methods typically center trajectory rewards by subtracting the empirical mean for each prompt. Statistically, this centering acts as a control variate (or baseline), reducing the variance of the policy-gradient estimator.
  Typically, the mean reward is estimated using per-prompt empirical averages for each prompt in a batch. Drawing inspiration from Stein's paradox, we propose using shrinkage estimators that combine per-prompt and across-prompt means to improve the overall per-prompt mean estimation accuracy -- particularly in the low-generation regime typical of RLVR. Theoretically, we construct a shrinkage-based baseline that provably yields lower-variance policy-gradient estimators across algorithms. Our proposed baseline serves as a drop-in replacement for existing per-prompt mean baselines, requiring no additional hyper-parameters or computation. Empirically, shrinkage baselines consistently outperform standard empirical-mean baselines, leading to lower-variance gradient updates and improved training stability.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supersimulators</title>
<link>https://arxiv.org/abs/2509.17994</link>
<guid>https://arxiv.org/abs/2509.17994</guid>
<content:encoded><![CDATA[
arXiv:2509.17994v2 Announce Type: cross 
Abstract: We prove that every randomized Boolean function admits a supersimulator: a randomized polynomial-size circuit whose output on random inputs cannot be efficiently distinguished from reality with constant advantage, even by polynomially larger distinguishers. Our result builds on the landmark complexity-theoretic regularity lemma of Trevisan, Tulsiani and Vadhan (2009), which, in contrast, provides a simulator that fools smaller distinguishers. We circumvent lower bounds for the simulator size by letting the distinguisher size bound vary with the target function, while remaining below an absolute upper bound independent of the target function. This dependence on the target function arises naturally from our use of an iteration technique originating in the graph regularity literature.
  The simulators provided by the regularity lemma and recent refinements thereof, known as multiaccurate and multicalibrated predictors, respectively, as per Hebert-Johnson et al. (2018), have previously been shown to have myriad applications in complexity theory, cryptography, learning theory, and beyond. We first show that a recent multicalibration-based characterization of the computational indistinguishability of product distributions actually requires only (calibrated) multiaccuracy. We then show that supersimulators yield an even tighter result in this application domain, closing a complexity gap present in prior versions of the characterization.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Association-sensory spatiotemporal hierarchy and functional gradient-regularised recurrent neural network with implications for schizophrenia</title>
<link>https://arxiv.org/abs/2511.02722</link>
<guid>https://arxiv.org/abs/2511.02722</guid>
<content:encoded><![CDATA[
arXiv:2511.02722v1 Announce Type: cross 
Abstract: The human neocortex is functionally organised at its highest level along a continuous sensory-to-association (AS) hierarchy. This study characterises the AS hierarchy of patients with schizophrenia in a comparison with controls. Using a large fMRI dataset (N=355), we extracted individual AS gradients via spectral analysis of brain connectivity, quantified hierarchical specialisation by gradient spread, and related this spread with connectivity geometry. We found that schizophrenia compresses the AS hierarchy indicating reduced functional differentiation. By modelling neural timescale with the Ornstein-Uhlenbeck process, we observed that the most specialised, locally cohesive regions at the gradient extremes exhibit dynamics with a longer time constant, an effect that is attenuated in schizophrenia. To study computation, we used the gradients to regularise subject-specific recurrent neural networks (RNNs) trained on working memory tasks. Networks endowed with greater gradient spread learned more efficiently, plateaued at lower task loss, and maintained stronger alignment to the prescribed AS hierarchical geometry. Fixed point linearisation showed that high-range networks settled into more stable neural states during memory delay, evidenced by lower energy and smaller maximal Jacobian eigenvalues. This gradient-regularised RNN framework therefore links large-scale cortical architecture with fixed point stability, providing a mechanistic account of how gradient de-differentiation could destabilise neural computations in schizophrenia, convergently supported by empirical timescale flattening and model-based evidence of less stable fixed points.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatio-Temporal Attention Network for Epileptic Seizure Prediction</title>
<link>https://arxiv.org/abs/2511.02846</link>
<guid>https://arxiv.org/abs/2511.02846</guid>
<content:encoded><![CDATA[
arXiv:2511.02846v1 Announce Type: cross 
Abstract: In this study, we present a deep learning framework that learns complex spatio-temporal correlation structures of EEG signals through a Spatio-Temporal Attention Network (STAN) for accurate predictions of onset of seizures for Epilepsy patients. Unlike existing methods, which rely on feature engineering and/or assume fixed preictal durations, our approach simultaneously models spatio-temporal correlations through STAN and employs an adversarial discriminator to distinguish preictal from interictal attention patterns, enabling patient-specific learning. Evaluation on CHB-MIT and MSSM datasets demonstrates 96.6\% sensitivity with 0.011/h false detection rate on CHB-MIT, and 94.2% sensitivity with 0.063/h FDR on MSSM, significantly outperforming state-of-the-art methods. The framework reliably detects preictal states at least 15 minutes before an onset, with patient-specific windows extending to 45 minutes, providing sufficient intervention time for clinical applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEGReXferNet: A Lightweight Gen-AI Framework for EEG Subspace Reconstruction via Cross-Subject Transfer Learning and Channel-Aware Embedding</title>
<link>https://arxiv.org/abs/2511.02848</link>
<guid>https://arxiv.org/abs/2511.02848</guid>
<content:encoded><![CDATA[
arXiv:2511.02848v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) is a widely used non-invasive technique for monitoring brain activity, but low signal-to-noise ratios (SNR) due to various artifacts often compromise its utility. Conventional artifact removal methods require manual intervention or risk suppressing critical neural features during filtering/reconstruction. Recent advances in generative models, including Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), have shown promise for EEG reconstruction; however, these approaches often lack integrated temporal-spectral-spatial sensitivity and are computationally intensive, limiting their suitability for real-time applications like brain-computer interfaces (BCIs). To overcome these challenges, we introduce EEGReXferNet, a lightweight Gen-AI framework for EEG subspace reconstruction via cross-subject transfer learning - developed using Keras TensorFlow (v2.15.1). EEGReXferNet employs a modular architecture that leverages volume conduction across neighboring channels, band-specific convolution encoding, and dynamic latent feature extraction through sliding windows. By integrating reference-based scaling, the framework ensures continuity across successive windows and generalizes effectively across subjects. This design improves spatial-temporal-spectral resolution (mean PSD correlation >= 0.95; mean spectrogram RV-Coefficient >= 0.85), reduces total weights by ~45% to mitigate overfitting, and maintains computational efficiency for robust, real-time EEG preprocessing in neurophysiological and BCI applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ECGXtract: Deep Learning-based ECG Feature Extraction for Automated CVD Diagnosis</title>
<link>https://arxiv.org/abs/2511.02850</link>
<guid>https://arxiv.org/abs/2511.02850</guid>
<content:encoded><![CDATA[
arXiv:2511.02850v1 Announce Type: cross 
Abstract: This paper presents ECGXtract, a deep learning-based approach for interpretable ECG feature extraction, addressing the limitations of traditional signal processing and black-box machine learning methods. In particular, we develop convolutional neural network models capable of extracting both temporal and morphological features with strong correlations to a clinically validated ground truth. Initially, each model is trained to extract a single feature, ensuring precise and interpretable outputs. A series of experiments is then carried out to evaluate the proposed method across multiple setups, including global versus lead-specific features, different sampling frequencies, and comparisons with other approaches such as ECGdeli. Our findings show that ECGXtract achieves robust performance across most features with a mean correlation score of 0.80 with the ground truth for global features, with lead II consistently providing the best results. For lead-specific features, ECGXtract achieves a mean correlation score of 0.822. Moreover, ECGXtract achieves superior results to the state-of-the-art open source ECGdeli as it got a higher correlation score with the ground truth in 90% of the features. Furthermore, we explore the feasibility of extracting multiple features simultaneously utilizing a single model. Semantic grouping is proved to be effective for global features, while large-scale grouping and lead-specific multi-output models show notable performance drops. These results highlight the potential of structured grouping strategies to balance the computational efficiency vs. model accuracy, paving the way for more scalable and clinically interpretable ECG feature extraction systems in limited resource settings.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approaching Low-Cost Cardiac Intelligence with Semi-Supervised Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.02851</link>
<guid>https://arxiv.org/abs/2511.02851</guid>
<content:encoded><![CDATA[
arXiv:2511.02851v1 Announce Type: cross 
Abstract: Deploying advanced cardiac artificial intelligence for daily cardiac monitoring is hindered by its reliance on extensive medical data and high computational resources. Low-cost cardiac intelligence (LCCI) offers a promising alternative by using wearable device data, such as 1-lead electrocardiogram (ECG), but it suffers from a significant diagnostic performance gap compared to high-cost cardiac intelligence (HCCI). To bridge this gap, we propose LiteHeart, a semi-supervised knowledge distillation framework. LiteHeart introduces a region-aware distillation module to mimic how cardiologists focus on diagnostically relevant ECG regions and a cross-layer mutual information module to align the decision processes of LCCI and HCCI systems. Using a semi-supervised training strategy, LiteHeart further improves model robustness under limited supervision. Evaluated on five datasets covering over 38 cardiovascular diseases, LiteHeart substantially reduces the performance gap between LCCI and HCCI, outperforming existing methods by 4.27% to 7.10% in macro F1 score. These results demonstrate that LiteHeart significantly enhances the diagnostic capabilities of low-cost cardiac intelligence systems, paving the way for scalable, affordable, and accurate daily cardiac healthcare using wearable technologies.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consciousness-ECG Transformer for Conscious State Estimation System with Real-Time Monitoring</title>
<link>https://arxiv.org/abs/2511.02853</link>
<guid>https://arxiv.org/abs/2511.02853</guid>
<content:encoded><![CDATA[
arXiv:2511.02853v1 Announce Type: cross 
Abstract: Conscious state estimation is important in various medical settings, including sleep staging and anesthesia management, to ensure patient safety and optimize health outcomes. Traditional methods predominantly utilize electroencephalography (EEG), which faces challenges such as high sensitivity to noise and the requirement for controlled environments. In this study, we propose the consciousness-ECG transformer that leverages electrocardiography (ECG) signals for non-invasive and reliable conscious state estimation. Our approach employs a transformer with decoupled query attention to effectively capture heart rate variability features that distinguish between conscious and unconscious states. We implemented the conscious state estimation system with real-time monitoring and validated our system on datasets involving sleep staging and anesthesia level monitoring during surgeries. Experimental results demonstrate that our model outperforms baseline models, achieving accuracies of 0.877 on sleep staging and 0.880 on anesthesia level monitoring. Moreover, our model achieves the highest area under curve values of 0.786 and 0.895 on sleep staging and anesthesia level monitoring, respectively. The proposed system offers a practical and robust alternative to EEG-based methods, particularly suited for dynamic clinical environments. Our results highlight the potential of ECG-based consciousness monitoring to enhance patient safety and advance our understanding of conscious states.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Reservoir Computing Framework for Chaotic Time Series Prediction Using Time Delay Embedding and Random Fourier Features</title>
<link>https://arxiv.org/abs/2511.02877</link>
<guid>https://arxiv.org/abs/2511.02877</guid>
<content:encoded><![CDATA[
arXiv:2511.02877v1 Announce Type: cross 
Abstract: Forecasting chaotic time series requires models that can capture the intrinsic geometry of the underlying attractor while remaining computationally efficient. We introduce a novel reservoir computing (RC) framework that integrates time-delay embedding with Random Fourier Feature (RFF) mappings to construct a dynamical reservoir without the need for traditional recurrent architectures. Unlike standard RC, which relies on high-dimensional recurrent connectivity, the proposed RFF-RC explicitly approximates nonlinear kernel transformations that uncover latent dynamical relations in the reconstructed phase space. This hybrid formulation offers two key advantages: (i) it provides a principled way to approximate complex nonlinear interactions among delayed coordinates, thereby enriching the effective dynamical representation of the reservoir, and (ii) it reduces reliance on manual reservoir hyperparameters such as spectral radius and leaking rate. We evaluate the framework on canonical chaotic systems-the Mackey-Glass equation, the Lorenz system, and the Kuramoto-Sivashinsky equation. This novel formulation demonstrates that RFF-RC not only achieves superior prediction accuracy but also yields robust attractor reconstructions and long-horizon forecasts. These results show that the combination of delay embedding and RFF-based reservoirs reveals new dynamical structure by embedding the system in an enriched feature space, providing a computationally efficient and interpretable approach to modeling chaotic dynamics.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation</title>
<link>https://arxiv.org/abs/2511.02953</link>
<guid>https://arxiv.org/abs/2511.02953</guid>
<content:encoded><![CDATA[
arXiv:2511.02953v1 Announce Type: cross 
Abstract: Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model's ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Machine Translation Detection Using a Surrogate Multilingual Translation Model</title>
<link>https://arxiv.org/abs/2511.02958</link>
<guid>https://arxiv.org/abs/2511.02958</guid>
<content:encoded><![CDATA[
arXiv:2511.02958v1 Announce Type: cross 
Abstract: Modern machine translation (MT) systems depend on large parallel corpora, often collected from the Internet. However, recent evidence indicates that (i) a substantial portion of these texts are machine-generated translations, and (ii) an overreliance on such synthetic content in training data can significantly degrade translation quality. As a result, filtering out non-human translations is becoming an essential pre-processing step in building high-quality MT systems. In this work, we propose a novel approach that directly exploits the internal representations of a surrogate multilingual MT model to distinguish between human and machine-translated sentences. Experimental results show that our method outperforms current state-of-the-art techniques, particularly for non-English language pairs, achieving gains of at least 5 percentage points of accuracy.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Single-Cell Gene Expression Generation with Latent Diffusion Models</title>
<link>https://arxiv.org/abs/2511.02986</link>
<guid>https://arxiv.org/abs/2511.02986</guid>
<content:encoded><![CDATA[
arXiv:2511.02986v1 Announce Type: cross 
Abstract: Computational modeling of single-cell gene expression is crucial for understanding cellular processes, but generating realistic expression profiles remains a major challenge. This difficulty arises from the count nature of gene expression data and complex latent dependencies among genes. Existing generative models often impose artificial gene orderings or rely on shallow neural network architectures. We introduce a scalable latent diffusion model for single-cell gene expression data, which we refer to as scLDM, that respects the fundamental exchangeability property of the data. Our VAE uses fixed-size latent variables leveraging a unified Multi-head Cross-Attention Block (MCAB) architecture, which serves dual roles: permutation-invariant pooling in the encoder and permutation-equivariant unpooling in the decoder. We enhance this framework by replacing the Gaussian prior with a latent diffusion model using Diffusion Transformers and linear interpolants, enabling high-quality generation with multi-conditional classifier-free guidance. We show its superior performance in a variety of experiments for both observational and perturbational single-cell data, as well as downstream tasks like cell-level classification.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification</title>
<link>https://arxiv.org/abs/2511.02992</link>
<guid>https://arxiv.org/abs/2511.02992</guid>
<content:encoded><![CDATA[
arXiv:2511.02992v1 Announce Type: cross 
Abstract: Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT) have outperformed pure CNN or ViT architecture. However, since these architectures require large parameters and incur large computational costs, they are unsuitable for tinyML deployment. This paper introduces a new hybrid CNN-ViT search space for Neural Architecture Search (NAS) to find efficient hybrid architectures for image classification. The search space covers hybrid CNN and ViT blocks to learn local and global information, as well as the novel Pooling block of searchable pooling layers for efficient feature map reduction. Experimental results on the CIFAR10 dataset show that our proposed search space can produce hybrid CNN-ViT architectures with superior accuracy and inference speed to ResNet-based tinyML models under tight model size constraints.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unifying Information-Theoretic and Pair-Counting Clustering Similarity</title>
<link>https://arxiv.org/abs/2511.03000</link>
<guid>https://arxiv.org/abs/2511.03000</guid>
<content:encoded><![CDATA[
arXiv:2511.03000v1 Announce Type: cross 
Abstract: Comparing clusterings is central to evaluating unsupervised models, yet the many existing similarity measures can produce widely divergent, sometimes contradictory, evaluations. Clustering similarity measures are typically organized into two principal families, pair-counting and information-theoretic, reflecting whether they quantify agreement through element pairs or aggregate information across full cluster contingency tables. Prior work has uncovered parallels between these families and applied empirical normalization or chance-correction schemes, but their deeper analytical connection remains only partially understood. Here, we develop an analytical framework that unifies these families through two complementary perspectives. First, both families are expressed as weighted expansions of observed versus expected co-occurrences, with pair-counting arising as a quadratic, low-order approximation and information-theoretic measures as higher-order, frequency-weighted extensions. Second, we generalize pair-counting to $k$-tuple agreement and show that information-theoretic measures can be viewed as systematically accumulating higher-order co-assignment structure beyond the pairwise level. We illustrate the approaches analytically for the Rand index and Mutual Information, and show how other indices in each family emerge as natural extensions. Together, these views clarify when and why the two regimes diverge, relating their sensitivities directly to weighting and approximation order, and provide a principled basis for selecting, interpreting, and extending clustering similarity measures across applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploratory Analysis of Cyberattack Patterns on E-Commerce Platforms Using Statistical Methods</title>
<link>https://arxiv.org/abs/2511.03020</link>
<guid>https://arxiv.org/abs/2511.03020</guid>
<content:encoded><![CDATA[
arXiv:2511.03020v1 Announce Type: cross 
Abstract: Cyberattacks on e-commerce platforms have grown in sophistication, threatening consumer trust and operational continuity. This research presents a hybrid analytical framework that integrates statistical modelling and machine learning for detecting and forecasting cyberattack patterns in the e-commerce domain. Using the Verizon Community Data Breach (VCDB) dataset, the study applies Auto ARIMA for temporal forecasting and significance testing, including a Mann-Whitney U test (U = 2579981.5, p = 0.0121), which confirmed that holiday shopping events experienced significantly more severe cyberattacks than non-holiday periods. ANOVA was also used to examine seasonal variation in threat severity, while ensemble machine learning models (XGBoost, LightGBM, and CatBoost) were employed for predictive classification. Results reveal recurrent attack spikes during high-risk periods such as Black Friday and holiday seasons, with breaches involving Personally Identifiable Information (PII) exhibiting elevated threat indicators. Among the models, CatBoost achieved the highest performance (accuracy = 85.29%, F1 score = 0.2254, ROC AUC = 0.8247). The framework uniquely combines seasonal forecasting with interpretable ensemble learning, enabling temporal risk anticipation and breach-type classification. Ethical considerations, including responsible use of sensitive data and bias assessment, were incorporated. Despite class imbalance and reliance on historical data, the study provides insights for proactive cybersecurity resource allocation and outlines directions for future real-time threat detection research.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2511.03034</link>
<guid>https://arxiv.org/abs/2511.03034</guid>
<content:encoded><![CDATA[
arXiv:2511.03034v1 Announce Type: cross 
Abstract: Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining approach that identifies and classifies opinions associated with specific entities (aspects) or their categories within a sentence. Despite its rapid growth and broad potential, ABSA research and resources remain concentrated in commercial domains, leaving analytical needs unmet in high-demand yet low-resource areas such as education and healthcare. Domain adaptation challenges and most existing methods' reliance on resource-intensive in-training knowledge injection further hinder progress in these areas. Moreover, traditional evaluation methods based on exact matches are overly rigid for ABSA tasks, penalising any boundary variations which may misrepresent the performance of generative models. This work addresses these gaps through three contributions: 1) We propose a novel evaluation method, Flexible Text Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates realistic extraction boundary variations while maintaining strong correlation with traditional metrics and offering fine-grained diagnostics. 2) We present the first ABSA study of small decoder-only generative language models (SLMs; <7B parameters), examining resource lower bounds via a case study in education review ABSA. We systematically explore data-free (in-context learning and weight merging) and data-light fine-tuning methods, and propose a multitask fine-tuning strategy that significantly enhances SLM performance, enabling 1.5-3.8 B models to surpass proprietary large models and approach benchmark results with only 200-1,000 examples on a single GPU. 3) We release the first public set of education review ABSA resources to support future research in low-resource domains.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Precise asymptotic analysis of Sobolev training for random feature models</title>
<link>https://arxiv.org/abs/2511.03050</link>
<guid>https://arxiv.org/abs/2511.03050</guid>
<content:encoded><![CDATA[
arXiv:2511.03050v1 Announce Type: cross 
Abstract: Gradient information is widely useful and available in applications, and is therefore natural to include in the training of neural networks. Yet little is known theoretically about the impact of Sobolev training -- regression with both function and gradient data -- on the generalization error of highly overparameterized predictive models in high dimensions. In this paper, we obtain a precise characterization of this training modality for random feature (RF) models in the limit where the number of trainable parameters, input dimensions, and training data tend proportionally to infinity. Our model for Sobolev training reflects practical implementations by sketching gradient data onto finite dimensional subspaces. By combining the replica method from statistical physics with linearizations in operator-valued free probability theory, we derive a closed-form description for the generalization errors of the trained RF models. For target functions described by single-index models, we demonstrate that supplementing function data with additional gradient data does not universally improve predictive performance. Rather, the degree of overparameterization should inform the choice of training method. More broadly, our results identify settings where models perform optimally by interpolating noisy function and gradient data.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Min-Max Optimization Is Strictly Easier Than Variational Inequalities</title>
<link>https://arxiv.org/abs/2511.03052</link>
<guid>https://arxiv.org/abs/2511.03052</guid>
<content:encoded><![CDATA[
arXiv:2511.03052v1 Announce Type: cross 
Abstract: Classically, a mainstream approach for solving a convex-concave min-max problem is to instead solve the variational inequality problem arising from its first-order optimality conditions. Is it possible to solve min-max problems faster by bypassing this reduction? This paper initiates this investigation. We show that the answer is yes in the textbook setting of unconstrained quadratic objectives: the optimal convergence rate for first-order algorithms is strictly better for min-max problems than for the corresponding variational inequalities. The key reason that min-max algorithms can be faster is that they can exploit the asymmetry of the min and max variables--a property that is lost in the reduction to variational inequalities. Central to our analyses are sharp characterizations of optimal convergence rates in terms of extremal polynomials which we compute using Green's functions and conformal mappings.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth</title>
<link>https://arxiv.org/abs/2511.03053</link>
<guid>https://arxiv.org/abs/2511.03053</guid>
<content:encoded><![CDATA[
arXiv:2511.03053v1 Announce Type: cross 
Abstract: Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning (MLS) point clouds in many high-precision applications such as Scan-to-BIM, deformation analysis, and 3D modeling. However, obtaining the ground truth (GT) for evaluation is often costly and infeasible in many real-world applications. To reduce this long-standing reliance on GT in uncertainty evaluation research, this study presents a learning-based framework for MLS point clouds that integrates optimal neighborhood estimation with geometric feature extraction. Experiments on a real-world dataset show that the proposed framework is feasible and the XGBoost model delivers fully comparable accuracy to Random Forest while achieving substantially higher efficiency (about 3 times faster), providing initial evidence that geometric features can be used to predict point-level uncertainty quantified by the C2C distance. In summary, this study shows that MLS point clouds' uncertainty is learnable, offering a novel learning-based viewpoint towards uncertainty evaluation research.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reading Between the Lines: The One-Sided Conversation Problem</title>
<link>https://arxiv.org/abs/2511.03056</link>
<guid>https://arxiv.org/abs/2511.03056</guid>
<content:encoded><![CDATA[
arXiv:2511.03056v1 Announce Type: cross 
Abstract: Conversational AI is constrained in many real-world settings where only one side of a dialogue can be recorded, such as telemedicine, call centers, and smart glasses. We formalize this as the one-sided conversation problem (1SC): inferring and learning from one side of a conversation. We study two tasks: (1) reconstructing the missing speaker's turns for real-time use cases, and (2) generating summaries from one-sided transcripts. Evaluating prompting and finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B testing and LLM-as-a-judge metrics, we find that access to one future turn and information about utterance length improves reconstruction, placeholder prompting helps to mitigate hallucination, and while large models generate promising reconstructions with prompting, smaller models require finetuning. Further, high-quality summaries can be generated without reconstructing missing turns. We present 1SC as a novel challenge and report promising results that mark a step toward privacy-aware conversational AI.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge</title>
<link>https://arxiv.org/abs/2511.03070</link>
<guid>https://arxiv.org/abs/2511.03070</guid>
<content:encoded><![CDATA[
arXiv:2511.03070v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) systems hold great promise for advancing various scientific disciplines, and are increasingly used in real-world applications. Despite their remarkable progress, further capabilities are expected in order to achieve more general types of intelligence. A critical distinction in this context is between factual knowledge, which can be evaluated against true or false answers (e.g., "what is the capital of England?"), and probabilistic knowledge, reflecting probabilistic properties of the real world (e.g., "what is the sex of a computer science graduate in the US?"). In this paper, our goal is to build a benchmark for understanding the capabilities of LLMs in terms of knowledge of probability distributions describing the real world. Given that LLMs are trained on vast amounts of text, it may be plausible that they internalize aspects of these distributions. Indeed, LLMs are touted as powerful universal approximators of real-world distributions. At the same time, classical results in statistics, known as curse of dimensionality, highlight fundamental challenges in learning distributions in high dimensions, challenging the notion of universal distributional learning. In this work, we develop the first benchmark to directly test this hypothesis, evaluating whether LLMs have access to empirical distributions describing real-world populations across domains such as economics, health, education, and social behavior. Our results demonstrate that LLMs perform poorly overall, and do not seem to internalize real-world statistics naturally. When interpreted in the context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that language models do not contain knowledge on observational distributions (Layer 1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional (Layer 2) and counterfactual (Layer 3) knowledge of these models is also limited.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech</title>
<link>https://arxiv.org/abs/2511.03080</link>
<guid>https://arxiv.org/abs/2511.03080</guid>
<content:encoded><![CDATA[
arXiv:2511.03080v1 Announce Type: cross 
Abstract: Text Normalization (TN) is a key preprocessing step in Text-to-Speech (TTS) systems, converting written forms into their canonical spoken equivalents. Traditional TN systems can exhibit high accuracy, but involve substantial engineering effort, are difficult to scale, and pose challenges to language coverage, particularly in low-resource settings. We propose PolyNorm, a prompt-based approach to TN using Large Language Models (LLMs), aiming to reduce the reliance on manually crafted rules and enable broader linguistic applicability with minimal human intervention. Additionally, we present a language-agnostic pipeline for automatic data curation and evaluation, designed to facilitate scalable experimentation across diverse languages. Experiments across eight languages show consistent reductions in the word error rate (WER) compared to a production-grade-based system. To support further research, we release PolyNorm-Benchmark, a multilingual data set covering a diverse range of text normalization phenomena.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Articulatory Coordination as a Biomarker for Schizophrenia</title>
<link>https://arxiv.org/abs/2511.03084</link>
<guid>https://arxiv.org/abs/2511.03084</guid>
<content:encoded><![CDATA[
arXiv:2511.03084v1 Announce Type: cross 
Abstract: Advances in artificial intelligence (AI) and deep learning have improved diagnostic capabilities in healthcare, yet limited interpretability continues to hinder clinical adoption. Schizophrenia, a complex disorder with diverse symptoms including disorganized speech and social withdrawal, demands tools that capture symptom severity and provide clinically meaningful insights beyond binary diagnosis. Here, we present an interpretable framework that leverages articulatory speech features through eigenspectra difference plots and a weighted sum with exponential decay (WSED) to quantify vocal tract coordination. Eigenspectra plots effectively distinguished complex from simpler coordination patterns, and WSED scores reliably separated these groups, with ambiguity confined to a narrow range near zero. Importantly, WSED scores correlated not only with overall BPRS severity but also with the balance between positive and negative symptoms, reflecting more complex coordination in subjects with pronounced positive symptoms and the opposite trend for stronger negative symptoms. This approach offers a transparent, severity-sensitive biomarker for schizophrenia, advancing the potential for clinically interpretable speech-based assessment tools.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Detection of Software Aging under Workload Shift</title>
<link>https://arxiv.org/abs/2511.03103</link>
<guid>https://arxiv.org/abs/2511.03103</guid>
<content:encoded><![CDATA[
arXiv:2511.03103v1 Announce Type: cross 
Abstract: Software aging is a phenomenon that affects long-running systems, leading to progressive performance degradation and increasing the risk of failures. To mitigate this problem, this work proposes an adaptive approach based on machine learning for software aging detection in environments subject to dynamic workload conditions. We evaluate and compare a static model with adaptive models that incorporate adaptive detectors, specifically the Drift Detection Method (DDM) and Adaptive Windowing (ADWIN), originally developed for concept drift scenarios and applied in this work to handle workload shifts. Experiments with simulated sudden, gradual, and recurring workload transitions show that static models suffer a notable performance drop when applied to unseen workload profiles, whereas the adaptive model with ADWIN maintains high accuracy, achieving an F1-Score above 0.93 in all analyzed scenarios.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EGMOF: Efficient Generation of Metal-Organic Frameworks Using a Hybrid Diffusion-Transformer Architecture</title>
<link>https://arxiv.org/abs/2511.03122</link>
<guid>https://arxiv.org/abs/2511.03122</guid>
<content:encoded><![CDATA[
arXiv:2511.03122v1 Announce Type: cross 
Abstract: Designing materials with targeted properties remains challenging due to the vastness of chemical space and the scarcity of property-labeled data. While recent advances in generative models offer a promising way for inverse design, most approaches require large datasets and must be retrained for every new target property. Here, we introduce the EGMOF (Efficient Generation of MOFs), a hybrid diffusion-transformer framework that overcomes these limitations through a modular, descriptor-mediated workflow. EGMOF decomposes inverse design into two steps: (1) a one-dimensional diffusion model (Prop2Desc) that maps desired properties to chemically meaningful descriptors followed by (2) a transformer model (Desc2MOF) that generates structures from these descriptors. This modular hybrid design enables minimal retraining and maintains high accuracy even under small-data conditions. On a hydrogen uptake dataset, EGMOF achieved over 95% validity and 84% hit rate, representing significant improvements of up to 57% in validity and 14% in hit rate compared to existing methods, while remaining effective with only 1,000 training samples. Moreover, our model successfully performed conditional generation across 29 diverse property datasets, including CoREMOF, QMOF, and text-mined experimental datasets, whereas previous models have not. This work presents a data-efficient, generalizable approach to the inverse design of diverse MOFs and highlights the potential of modular inverse design workflows for broader materials discovery.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Accelerated Bayesian Optimization with Knowledge Transfer</title>
<link>https://arxiv.org/abs/2511.03125</link>
<guid>https://arxiv.org/abs/2511.03125</guid>
<content:encoded><![CDATA[
arXiv:2511.03125v1 Announce Type: cross 
Abstract: We study how Bayesian optimization (BO) can be accelerated on a target task with historical knowledge transferred from related source tasks. Existing works on BO with knowledge transfer either do not have theoretical guarantees or achieve the same regret as BO in the non-transfer setting, $\tilde{\mathcal{O}}(\sqrt{T \gamma_f})$, where $T$ is the number of evaluations of the target function and $\gamma_f$ denotes its information gain. In this paper, we propose the DeltaBO algorithm, in which a novel uncertainty-quantification approach is built on the difference function $\delta$ between the source and target functions, which are allowed to belong to different reproducing kernel Hilbert spaces (RKHSs). Under mild assumptions, we prove that the regret of DeltaBO is of order $\tilde{\mathcal{O}}(\sqrt{T (T/N + \gamma_\delta)})$, where $N$ denotes the number of evaluations from source tasks and typically $N \gg T$. In many applications, source and target tasks are similar, which implies that $\gamma_\delta$ can be much smaller than $\gamma_f$. Empirical studies on both real-world hyperparameter tuning tasks and synthetic functions show that DeltaBO outperforms other baseline methods and support our theoretical claims.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Measurement to Expertise: Empathetic Expert Adapters for Context-Based Empathy in Conversational AI Agents</title>
<link>https://arxiv.org/abs/2511.03143</link>
<guid>https://arxiv.org/abs/2511.03143</guid>
<content:encoded><![CDATA[
arXiv:2511.03143v1 Announce Type: cross 
Abstract: Empathy is a critical factor in fostering positive user experiences in conversational AI. While models can display empathy, it is often generic rather than tailored to specific tasks and contexts. In this work, we introduce a novel framework for developing and evaluating context-specific empathetic large language models (LLMs). We first analyze a real-world conversational dataset consisting of 672 multi-turn conversations across 8 tasks, revealing significant differences in terms of expected and experienced empathy before and after the conversations, respectively. To help minimize this gap, we develop a synthetic multi-turn conversational generation pipeline and steer responses toward our defined empathy patterns based on the context that more closely matches users' expectations. We then train empathetic expert adapters for context-specific empathy that specialize in varying empathy levels based on the recognized task. Our empirical results demonstrate a significant gap reduction of 72.66% between perceived and desired empathy with scores increasing by an average factor of 2.43 as measured by our metrics and reward models. Additionally, our trained empathetic expert adapters demonstrate superior effectiveness in preserving empathy patterns throughout conversation turns, outperforming system prompts, which tend to dramatically diminish in impact as conversations lengthen.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models</title>
<link>https://arxiv.org/abs/2511.03147</link>
<guid>https://arxiv.org/abs/2511.03147</guid>
<content:encoded><![CDATA[
arXiv:2511.03147v1 Announce Type: cross 
Abstract: Neural signed distance functions (SDFs) have become a powerful representation for geometric reconstruction from point clouds, yet they often require both gradient- and curvature-based regularization to suppress spurious warp and preserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten (ODW) loss as an efficient second-order prior for CAD surfaces, approximating full-Hessian regularization at roughly half the computational cost. However, FlatCAD applies a fixed ODW weight throughout training, which is suboptimal: strong regularization stabilizes early optimization but suppresses detail recovery in later stages. We present scheduling strategies for the ODW loss that assign a high initial weight to stabilize optimization and progressively decay it to permit fine-scale refinement. We investigate constant, linear, quintic, and step interpolation schedules, as well as an increasing warm-up variant. Experiments on the ABC CAD dataset demonstrate that time-varying schedules consistently outperform fixed weights. Our method achieves up to a 35% improvement in Chamfer Distance over the FlatCAD baseline, establishing scheduling as a simple yet effective extension of curvature regularization for robust CAD reconstruction.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Headway in Heterogeneous and Mixed Traffic Flow: A Statistical Distribution Based on a General Exponential Function</title>
<link>https://arxiv.org/abs/2511.03154</link>
<guid>https://arxiv.org/abs/2511.03154</guid>
<content:encoded><![CDATA[
arXiv:2511.03154v1 Announce Type: cross 
Abstract: The ability of existing headway distributions to accurately reflect the diverse behaviors and characteristics in heterogeneous traffic (different types of vehicles) and mixed traffic (human-driven vehicles with autonomous vehicles) is limited, leading to unsatisfactory goodness of fit. To address these issues, we modified the exponential function to obtain a novel headway distribution. Rather than employing Euler's number (e) as the base of the exponential function, we utilized a real number base to provide greater flexibility in modeling the observed headway. However, the proposed is not a probability function. We normalize it to calculate the probability and derive the closed-form equation. In this study, we utilized a comprehensive experiment with five open datasets: highD, exiD, NGSIM, Waymo, and Lyft to evaluate the performance of the proposed distribution and compared its performance with six existing distributions under mixed and heterogeneous traffic flow. The results revealed that the proposed distribution not only captures the fundamental characteristics of headway distribution but also provides physically meaningful parameters that describe the distribution shape of observed headways. Under heterogeneous flow on highways (i.e., uninterrupted traffic flow), the proposed distribution outperforms other candidate distributions. Under urban road conditions (i.e., interrupted traffic flow), including heterogeneous and mixed traffic, the proposed distribution still achieves decent results.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies</title>
<link>https://arxiv.org/abs/2511.03173</link>
<guid>https://arxiv.org/abs/2511.03173</guid>
<content:encoded><![CDATA[
arXiv:2511.03173v1 Announce Type: cross 
Abstract: The rapid growth of cislunar activities, including lunar landings, the Lunar Gateway, and in-space refueling stations, requires advances in cost-efficient trajectory design and reliable integration of navigation and remote sensing. Traditional Earth-Moon transfers suffer from rigid launch windows and high propellant demands, while Earth-based GNSS systems provide little to no coverage beyond geostationary orbit. This limits autonomy and environmental awareness in cislunar space. This review compares four major transfer strategies by evaluating velocity requirements, flight durations, and fuel efficiency, and by identifying their suitability for both crewed and robotic missions. The emerging role of artificial intelligence and machine learning is highlighted: convolutional neural networks support automated crater recognition and digital terrain model generation, while deep reinforcement learning enables adaptive trajectory refinement during descent and landing to reduce risk and decision latency. The study also examines how GNSS-Reflectometry and advanced Positioning, Navigation, and Timing architectures can extend navigation capabilities beyond current limits. GNSS-R can act as a bistatic radar for mapping lunar ice, soil properties, and surface topography, while PNT systems support autonomous rendezvous, Lagrange point station-keeping, and coordinated satellite swarm operations. Combining these developments establishes a scalable framework for sustainable cislunar exploration and long-term human and robotic presence.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2511.03179</link>
<guid>https://arxiv.org/abs/2511.03179</guid>
<content:encoded><![CDATA[
arXiv:2511.03179v1 Announce Type: cross 
Abstract: The engineering design process often demands expertise from multiple domains, leading to complex collaborations and iterative refinements. Traditional methods can be resource-intensive and prone to inefficiencies. To address this, we formalize the engineering design process through a multi-agent AI framework that integrates structured design and review loops. The framework introduces specialized knowledge-driven agents that collaborate to generate and refine design candidates. As an exemplar, we demonstrate its application to the aerodynamic optimization of 4-digit NACA airfoils. The framework consists of three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems Engineer. The Graph Ontologist employs a Large Language Model (LLM) to construct two domain-specific knowledge graphs from airfoil design literature. The Systems Engineer, informed by a human manager, formulates technical requirements that guide design generation and evaluation. The Design Engineer leverages the design knowledge graph and computational tools to propose candidate airfoils meeting these requirements. The Systems Engineer reviews and provides feedback both qualitative and quantitative using its own knowledge graph, forming an iterative feedback loop until a design is validated by the manager. The final design is then optimized to maximize performance metrics such as the lift-to-drag ratio. Overall, this work demonstrates how collaborative AI agents equipped with structured knowledge representations can enhance efficiency, consistency, and quality in the engineering design process.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control</title>
<link>https://arxiv.org/abs/2511.03181</link>
<guid>https://arxiv.org/abs/2511.03181</guid>
<content:encoded><![CDATA[
arXiv:2511.03181v1 Announce Type: cross 
Abstract: Human-robot cooperation is essential in environments such as warehouses and retail stores, where workers frequently handle deformable objects like paper, bags, and fabrics. Coordinating robotic actions with human assistance remains difficult due to the unpredictable dynamics of deformable materials and the need for adaptive force control. To explore this challenge, we focus on the task of gift wrapping, which exemplifies a long-horizon manipulation problem involving precise folding, controlled creasing, and secure fixation of paper. Success is achieved when the robot completes the sequence to produce a neatly wrapped package with clean folds and no tears.
  We propose a learning-based framework that integrates a high-level task planner powered by a large language model (LLM) with a low-level hybrid imitation learning (IL) and reinforcement learning (RL) policy. At its core is a Sub-task Aware Robotic Transformer (START) that learns a unified policy from human demonstrations. The key novelty lies in capturing long-range temporal dependencies across the full wrapping sequence within a single model. Unlike vanilla Action Chunking with Transformer (ACT), typically applied to short tasks, our method introduces sub-task IDs that provide explicit temporal grounding. This enables robust performance across the entire wrapping process and supports flexible execution, as the policy learns sub-goals rather than merely replicating motion sequences.
  Our framework achieves a 97% success rate on real-world wrapping tasks. We show that the unified transformer-based policy reduces the need for specialized models, allows controlled human supervision, and effectively bridges high-level intent with the fine-grained force control required for deformable object manipulation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Robustness of Model Editing in Code LLMs: An Empirical Study</title>
<link>https://arxiv.org/abs/2511.03182</link>
<guid>https://arxiv.org/abs/2511.03182</guid>
<content:encoded><![CDATA[
arXiv:2511.03182v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used in software development. However, while LLMs remain static after pretraining, programming languages and APIs continue to evolve, leading to the generation of deprecated or incompatible code that undermines reliability. Retraining LLMs from scratch to reflect such changes is computationally expensive, making model editing a promising lightweight alternative that updates only a small subset of parameters. Despite its potential, it remains unclear whether model editing yields genuine syntactic and semantic adaptations or merely superficial fixes. In this work, we present a systematic study of five state-of-the-art model editing methods: Constrained Fine-Tuning (FT), GRACE, MEMIT, PMET, and ROME. We apply these methods to three leading open-source code LLMs, CodeLlama, CodeQwen1.5, and DeepSeek-Coder, under controlled API deprecation scenarios. Our evaluation covers both instant and sequential editing settings, using three disjoint evaluation sets designed to assess reliability, generalization, and specificity. We measure model correctness at three levels: successful compilation, partial test case pass, and full test pass. Our findings show that instant edits consistently degrade model performance, with syntactic validity dropping by up to 86 percentage points and functional correctness declining by 45 points even in the best-performing setting. Sequential edits further amplify this degradation, and in some cases, model performance collapses entirely. Across all models, most passing generations relied on workarounds rather than correctly adopting the intended changes, while faulty adoptions that result in test failures or compilation errors were significantly more frequent. Correct adoptions, where the model correctly integrates the intended change, occurred in only about 6% of cases.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistical Properties of Rectified Flow</title>
<link>https://arxiv.org/abs/2511.03193</link>
<guid>https://arxiv.org/abs/2511.03193</guid>
<content:encoded><![CDATA[
arXiv:2511.03193v1 Announce Type: cross 
Abstract: Rectified flow (Liu et al., 2022; Liu, 2022; Wu et al., 2023) is a method for defining a transport map between two distributions, and enjoys popularity in machine learning, although theoretical results supporting the validity of these methods are scant. The rectified flow can be regarded as an approximation to optimal transport, but in contrast to other transport methods that require optimization over a function space, computing the rectified flow only requires standard statistical tools such as regression or density estimation. Because of this, one can leverage standard data analysis tools for regression and density estimation to develop empirical versions of transport maps. We study some structural properties of the rectified flow, including existence, uniqueness, and regularity, as well as the related statistical properties, such as rates of convergence and central limit theorems, for some selected estimators. To do so, we analyze separately the bounded and unbounded cases as each presents unique challenges. In both cases, we are able to establish convergence at faster rates than the ones for the usual nonparametric regression and density estimation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Separations between Memorization and Generalization in Diffusion Models</title>
<link>https://arxiv.org/abs/2511.03202</link>
<guid>https://arxiv.org/abs/2511.03202</guid>
<content:encoded><![CDATA[
arXiv:2511.03202v1 Announce Type: cross 
Abstract: Diffusion models have achieved remarkable success across diverse domains, but they remain vulnerable to memorization -- reproducing training data rather than generating novel outputs. This not only limits their creative potential but also raises concerns about privacy and safety. While empirical studies have explored mitigation strategies, theoretical understanding of memorization remains limited. We address this gap through developing a dual-separation result via two complementary perspectives: statistical estimation and network approximation. From the estimation side, we show that the ground-truth score function does not minimize the empirical denoising loss, creating a separation that drives memorization. From the approximation side, we prove that implementing the empirical score function requires network size to scale with sample size, spelling a separation compared to the more compact network representation of the ground-truth score function. Guided by these insights, we develop a pruning-based method that reduces memorization while maintaining generation quality in diffusion transformers.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2511.03206</link>
<guid>https://arxiv.org/abs/2511.03206</guid>
<content:encoded><![CDATA[
arXiv:2511.03206v1 Announce Type: cross 
Abstract: Recently, Multimodal Large Language Models (MLLMs) encounter two key issues in multi-image contexts: (1) a lack of fine-grained perception across disparate images, and (2) a diminished capability to effectively reason over and synthesize information from multiple visual inputs. However, while various prompting methods aim to describe visual content, many existing studies focus primarily on single-image settings or specific, constrained scenarios. This leaves a critical gap in understanding and addressing how MLLMs tackle more general and complex multi-image reasoning tasks. Thus, we first extensively investigate how current prompting methods perceive fine-grained visual details and process visual information when dealing with multiple images. Our findings reveal that existing prompting methods fall short in attending to needed clues and seamlessly integrating perception and reasoning. Inspired by the findings, we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions (QG-CoC), a generalized prompting approach that effectively handles problems with an arbitrary number of images. We evaluate our method on various open-source and closed-source MLLMs for multi-image and single-image benchmarks. Experimental results indicate that QG-CoC demonstrates competitive performance across tasks and exhibits robust improvements in the challenging scenarios where existing prompting methods fail.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RKUM: An R Package for Robust Kernel Unsupervised Methods</title>
<link>https://arxiv.org/abs/2511.03216</link>
<guid>https://arxiv.org/abs/2511.03216</guid>
<content:encoded><![CDATA[
arXiv:2511.03216v1 Announce Type: cross 
Abstract: RKUM is an R package developed for implementing robust kernel-based unsupervised methods. It provides functions for estimating the robust kernel covariance operator (CO) and the robust kernel cross-covariance operator (CCO) using generalized loss functions instead of the conventional quadratic loss. These operators form the foundation of robust kernel learning and enable reliable analysis under contaminated or noisy data conditions. The package includes implementations of robust kernel canonical correlation analysis (Kernel CCA), as well as the influence function (IF) for both standard and multiple kernel CCA frameworks. The influence function quantifies sensitivity and helps detect influential or outlying observations across two-view and multi-view datasets. Experiments using synthesized two-view and multi-view data demonstrate that the IF of the standard kernel CCA effectively identifies outliers, while the robust kernel methods implemented in RKUM exhibit reduced sensitivity to contamination. Overall, RKUM provides an efficient and extensible platform for robust kernel-based analysis in high-dimensional data applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topography, climate, land cover, and biodiversity: Explaining endemic richness and management implications on a Mediterranean island</title>
<link>https://arxiv.org/abs/2511.03242</link>
<guid>https://arxiv.org/abs/2511.03242</guid>
<content:encoded><![CDATA[
arXiv:2511.03242v1 Announce Type: cross 
Abstract: Island endemism is shaped by complex interactions among environmental, ecological, and evolutionary factors, yet the relative contributions of topography, climate, and land cover remain incompletely quantified. We investigated the drivers of endemic plant richness across Crete, a Mediterranean biodiversity hotspot, using spatially explicit data on species distributions, topographic complexity, climatic variability, land cover, and soil characteristics. Artificial Neural Network models, a machine learning tool, were employed to assess the relative importance of these predictors and to identify hotspots of endemism. We found that total species richness, elevation range, and climatic variability were the strongest predictors of endemic richness, reflecting the role of biodiversity, topographic heterogeneity, and climatic gradients in generating diverse habitats and micro-refugia that promote speciation and buffer extinction risk. Endemic hotspots only partially overlapped with areas of high total species richness, indicating that total species richness was the optimal from the ones examined, yet an imperfect surrogate. These environmentally heterogeneous areas also provide critical ecosystem services, including soil stabilization, pollination, and cultural value, which are increasingly threatened by tourism, renewable energy development, land-use change, and climate impacts. Our findings underscore the importance of prioritizing mountainous and climatically variable regions in conservation planning, integrating ecosystem service considerations, and accounting for within-island spatial heterogeneity. By explicitly linking the environmental drivers of endemism to both biodiversity patterns and ecosystem function, this study provides a framework for evidence-based conservation planning in Crete and other Mediterranean islands with similar geological and biogeographic contexts.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Death by a Thousand Prompts: Open Model Vulnerability Analysis</title>
<link>https://arxiv.org/abs/2511.03247</link>
<guid>https://arxiv.org/abs/2511.03247</guid>
<content:encoded><![CDATA[
arXiv:2511.03247v1 Announce Type: cross 
Abstract: Open-weight models provide researchers and developers with accessible foundations for diverse downstream applications. We tested the safety and security postures of eight open-weight large language models (LLMs) to identify vulnerabilities that may impact subsequent fine-tuning and deployment. Using automated adversarial testing, we measured each model's resilience against single-turn and multi-turn prompt injection and jailbreak attacks. Our findings reveal pervasive vulnerabilities across all tested models, with multi-turn attacks achieving success rates between 25.86\% and 92.78\% -- representing a $2\times$ to $10\times$ increase over single-turn baselines. These results underscore a systemic inability of current open-weight models to maintain safety guardrails across extended interactions. We assess that alignment strategies and lab priorities significantly influence resilience: capability-focused models such as Llama 3.3 and Qwen 3 demonstrate higher multi-turn susceptibility, whereas safety-oriented designs such as Google Gemma 3 exhibit more balanced performance.
  The analysis concludes that open-weight models, while crucial for innovation, pose tangible operational and ethical risks when deployed without layered security controls. These findings are intended to inform practitioners and developers of the potential risks and the value of professional AI security solutions to mitigate exposure. Addressing multi-turn vulnerabilities is essential to ensure the safe, reliable, and responsible deployment of open-weight LLMs in enterprise and public domains. We recommend adopting a security-first design philosophy and layered protections to ensure resilient deployments of open-weight models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Generative Artificial Intelligence meets Extended Reality: A Systematic Review</title>
<link>https://arxiv.org/abs/2511.03282</link>
<guid>https://arxiv.org/abs/2511.03282</guid>
<content:encoded><![CDATA[
arXiv:2511.03282v1 Announce Type: cross 
Abstract: With the continuous advancement of technology, the application of generative artificial intelligence (AI) in various fields is gradually demonstrating great potential, particularly when combined with Extended Reality (XR), creating unprecedented possibilities. This survey article systematically reviews the applications of generative AI in XR, covering as much relevant literature as possible from 2023 to 2025. The application areas of generative AI in XR and its key technology implementations are summarised through PRISMA screening and analysis of the final 26 articles. The survey highlights existing articles from the last three years related to how XR utilises generative AI, providing insights into current trends and research gaps. We also explore potential opportunities for future research to further empower XR through generative AI, providing guidance and information for future generative XR research.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Influence of Data Dimensionality Reduction Methods on the Effectiveness of Quantum Machine Learning Models</title>
<link>https://arxiv.org/abs/2511.03320</link>
<guid>https://arxiv.org/abs/2511.03320</guid>
<content:encoded><![CDATA[
arXiv:2511.03320v1 Announce Type: cross 
Abstract: Data dimensionality reduction techniques are often utilized in the implementation of Quantum Machine Learning models to address two significant issues: the constraints of NISQ quantum devices, which are characterized by noise and a limited number of qubits, and the challenge of simulating a large number of qubits on classical devices. It also raises concerns over the scalability of these approaches, as dimensionality reduction methods are slow to adapt to large datasets. In this article, we analyze how data reduction methods affect different QML models. We conduct this experiment over several generated datasets, quantum machine algorithms, quantum data encoding methods, and data reduction methods. All these models were evaluated on the performance metrics like accuracy, precision, recall, and F1 score. Our findings have led us to conclude that the usage of data dimensionality reduction methods results in skewed performance metric values, which results in wrongly estimating the actual performance of quantum machine learning models. There are several factors, along with data dimensionality reduction methods, that worsen this problem, such as characteristics of the datasets, classical to quantum information embedding methods, percentage of feature reduction, classical components associated with quantum models, and structure of quantum machine learning models. We consistently observed the difference in the accuracy range of 14% to 48% amongst these models, using data reduction and not using it. Apart from this, our observations have shown that some data reduction methods tend to perform better for some specific data embedding methodologies and ansatz constructions.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks</title>
<link>https://arxiv.org/abs/2511.03328</link>
<guid>https://arxiv.org/abs/2511.03328</guid>
<content:encoded><![CDATA[
arXiv:2511.03328v1 Announce Type: cross 
Abstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of "reasoning MLLMs" that offer explicit control over their internal thinking processes (normally referred as the "thinking mode") alongside the standard "non-thinking mode". This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these "dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active "thinking mode" capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.03367</link>
<guid>https://arxiv.org/abs/2511.03367</guid>
<content:encoded><![CDATA[
arXiv:2511.03367v1 Announce Type: cross 
Abstract: Recent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks. Methods such as CoOp and CoCoOp have shown that replacing handcrafted prompts with learnable vectors, known as prompt learning, can result in improved performance. However, these models often struggle to generalize to entirely unseen categories. While traditional zero-shot learning techniques benefit from various data augmentation strategies, prompt learning has primarily focused on text-based modifications, leaving the potential of image-based augmentation largely unexplored. In this work, we explore how image-level augmentations, particularly those that introduce attribute-specific variations, can support and enhance prompt learning. Our analysis examines the interaction between these augmentations and soft prompt frameworks, revealing their potential to improve generalization. We also identify a limitation in existing methods, such as CoCoOp, which do not provide explicit guidance for learning prompts that focus on semantically meaningful visual features. To address this, we propose Adding Attributes to Prompt Learning, AAPL, a novel method that introduces adversarial token embeddings to decouple superficial visual variations introduced by augmentation from class-relevant semantic representations. This decoupling enables the learned prompts to concentrate on visually discriminative features that align with the target categories. We conduct comprehensive experiments on eleven benchmark datasets, and AAPL consistently outperforms existing methods across few-shot, zero-shot, cross-dataset, and domain generalization settings. Our source code is publicly available at: https://github.com/Gahyeonkim09/AAPL
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SyMuPe: Affective and Controllable Symbolic Music Performance</title>
<link>https://arxiv.org/abs/2511.03425</link>
<guid>https://arxiv.org/abs/2511.03425</guid>
<content:encoded><![CDATA[
arXiv:2511.03425v1 Announce Type: cross 
Abstract: Emotions are fundamental to the creation and perception of music performances. However, achieving human-like expression and emotion through machine learning models for performance rendering remains a challenging task. In this work, we present SyMuPe, a novel framework for developing and training affective and controllable symbolic piano performance models. Our flagship model, PianoFlow, uses conditional flow matching trained to solve diverse multi-mask performance inpainting tasks. By design, it supports both unconditional generation and infilling of music performance features. For training, we use a curated, cleaned dataset of 2,968 hours of aligned musical scores and expressive MIDI performances. For text and emotion control, we integrate a piano performance emotion classifier and tune PianoFlow with the emotion-weighted Flan-T5 text embeddings provided as conditional inputs. Objective and subjective evaluations against transformer-based baselines and existing models show that PianoFlow not only outperforms other approaches, but also achieves performance quality comparable to that of human-recorded and transcribed MIDI samples. For emotion control, we present and analyze samples generated under different text conditioning scenarios. The developed model can be integrated into interactive applications, contributing to the creation of more accessible and engaging music performance systems.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Support-Set Algorithm for Optimization Problems with Nonnegative and Orthogonal Constraints</title>
<link>https://arxiv.org/abs/2511.03443</link>
<guid>https://arxiv.org/abs/2511.03443</guid>
<content:encoded><![CDATA[
arXiv:2511.03443v1 Announce Type: cross 
Abstract: In this paper, we investigate optimization problems with nonnegative and orthogonal constraints, where any feasible matrix of size $n \times p$ exhibits a sparsity pattern such that each row accommodates at most one nonzero entry. Our analysis demonstrates that, by fixing the support set, the global solution of the minimization subproblem for the proximal linearization of the objective function can be computed in closed form with at most $n$ nonzero entries. Exploiting this structural property offers a powerful avenue for dramatically enhancing computational efficiency. Guided by this insight, we propose a support-set algorithm preserving strictly the feasibility of iterates. A central ingredient is a strategically devised update scheme for support sets that adjusts the placement of nonzero entries. We establish the global convergence of the support-set algorithm to a first-order stationary point, and show that its iteration complexity required to reach an $\epsilon$-approximate first-order stationary point is $O (\epsilon^{-2})$. Numerical results are strongly in favor of our algorithm in real-world applications, including nonnegative PCA, clustering, and community detection.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>System Identification of a Moored ASV with Recessed Moon Pool via Deterministic and Bayesian Hankel-DMDc</title>
<link>https://arxiv.org/abs/2511.03482</link>
<guid>https://arxiv.org/abs/2511.03482</guid>
<content:encoded><![CDATA[
arXiv:2511.03482v1 Announce Type: cross 
Abstract: This study addresses the system identification of a small autonomous surface vehicle (ASV) under moored conditions using Hankel dynamic mode decomposition with control (HDMDc) and its Bayesian extension (BHDMDc). Experiments were carried out on a Codevintec CK-14e ASV in the towing tank of CNR-INM, under both irregular and regular head-sea wave conditions. The ASV under investigation features a recessed moon pool, which induces nonlinear responses due to sloshing, thereby increasing the modelling challenge. Data-driven reduced-order models were built from measurements of vessel motions and mooring loads. The HDMDc framework provided accurate deterministic predictions of vessel dynamics, while the Bayesian formulation enabled uncertainty-aware characterization of the model response by accounting for variability in hyperparameter selection. Validation against experimental data demonstrated that both HDMDc and BHDMDc can predict the vessel's response to unseen regular and irregular wave excitations. In conclusion, the study shows that HDMDc-based ROMs are a viable data-driven alternative for system identification, demonstrating for the first time their generalization capability for a sea condition different from the training set, achieving high accuracy in reproducing vessel dynamics.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation</title>
<link>https://arxiv.org/abs/2511.03498</link>
<guid>https://arxiv.org/abs/2511.03498</guid>
<content:encoded><![CDATA[
arXiv:2511.03498v1 Announce Type: cross 
Abstract: Large language models work well for technical problem solving in English but perform poorly when the same questions are asked in Bangla. A simple solution would be to translate Bangla questions into English first and then use these models. However, existing Bangla-English translation systems struggle with technical terms. They often mistranslate specialized vocabulary, which changes the meaning of the problem and leads to wrong answers. We present BanglaSTEM, a dataset of 5,000 carefully selected Bangla-English sentence pairs from STEM fields including computer science, mathematics, physics, chemistry, and biology. We generated over 12,000 translations using language models and then used human evaluators to select the highest quality pairs that preserve technical terminology correctly. We train a T5-based translation model on BanglaSTEM and test it on two tasks: generating code and solving math problems. Our results show significant improvements in translation accuracy for technical content, making it easier for Bangla speakers to use English-focused language models effectively. Both the BanglaSTEM dataset and the trained translation model are publicly released at https://huggingface.co/reyazul/BanglaSTEM-T5.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Structure of Cross-Validation Error: Stability, Covariance, and Minimax Limits</title>
<link>https://arxiv.org/abs/2511.03554</link>
<guid>https://arxiv.org/abs/2511.03554</guid>
<content:encoded><![CDATA[
arXiv:2511.03554v1 Announce Type: cross 
Abstract: Despite ongoing theoretical research on cross-validation (CV), many theoretical questions about CV remain widely open. This motivates our investigation into how properties of algorithm-distribution pairs can affect the choice for the number of folds in $k$-fold cross-validation.
  Our results consist of a novel decomposition of the mean-squared error of cross-validation for risk estimation, which explicitly captures the correlations of error estimates across overlapping folds and includes a novel algorithmic stability notion, squared loss stability, that is considerably weaker than the typically required hypothesis stability in other comparable works.
  Furthermore, we prove:
  1. For every learning algorithm that minimizes empirical error, a minimax lower bound on the mean-squared error of $k$-fold CV estimating the population risk $L_\mathcal{D}$: \[ \min_{k \mid n}\; \max_{\mathcal{D}}\; \mathbb{E}\!\left[\big(\widehat{L}_{\mathrm{CV}}^{(k)} - L_{\mathcal{D}}\big)^{2}\right] \;=\; \Omega\!\big(\sqrt{k}/n\big), \] where $n$ is the sample size and $k$ the number of folds. This shows that even under idealized conditions, for large values of $k$, CV cannot attain the optimum of order $1/n$ achievable by a validation set of size $n$, reflecting an inherent penalty caused by dependence between folds.
  2. Complementing this, we exhibit learning rules for which \[
  \max_{\mathcal{D}}\; \mathbb{E}\!\left[\big(\widehat{L}_{\mathrm{CV}}^{(k)} - L_{\mathcal{D}}\big)^{2}\right] \;=\; \Omega(k/n), \] matching (up to constants) the accuracy of a hold-out estimator of a single fold of size $n/k$.
  Together these results delineate the fundamental trade-off in resampling-based risk estimation: CV cannot fully exploit all $n$ samples for unbiased risk evaluation, and its minimax performance is pinned between the $k/n$ and $\sqrt{k}/n$ regimes.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vector-valued self-normalized concentration inequalities beyond sub-Gaussianity</title>
<link>https://arxiv.org/abs/2511.03606</link>
<guid>https://arxiv.org/abs/2511.03606</guid>
<content:encoded><![CDATA[
arXiv:2511.03606v1 Announce Type: cross 
Abstract: The study of self-normalized processes plays a crucial role in a wide range of applications, from sequential decision-making to econometrics. While the behavior of self-normalized concentration has been widely investigated for scalar-valued processes, vector-valued processes remain comparatively underexplored, especially outside of the sub-Gaussian framework. In this contribution, we provide concentration bounds for self-normalized processes with light tails beyond sub-Gaussianity (such as Bennett or Bernstein bounds). We illustrate the relevance of our results in the context of online linear regression, with applications in (kernelized) linear bandits.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLAX: Fast and Flexible Neural Click Models in JAX</title>
<link>https://arxiv.org/abs/2511.03620</link>
<guid>https://arxiv.org/abs/2511.03620</guid>
<content:encoded><![CDATA[
arXiv:2511.03620v1 Announce Type: cross 
Abstract: CLAX is a JAX-based library that implements classic click models using modern gradient-based optimization. While neural click models have emerged over the past decade, complex click models based on probabilistic graphical models (PGMs) have not systematically adopted gradient-based optimization, preventing practitioners from leveraging modern deep learning frameworks while preserving the interpretability of classic models. CLAX addresses this gap by replacing EM-based optimization with direct gradient-based optimization in a numerically stable manner. The framework's modular design enables the integration of any component, from embeddings and deep networks to custom modules, into classic click models for end-to-end optimization. We demonstrate CLAX's efficiency by running experiments on the full Baidu-ULTR dataset comprising over a billion user sessions in $\approx$ 2 hours on a single GPU, orders of magnitude faster than traditional EM approaches. CLAX implements ten classic click models, serving both industry practitioners seeking to understand user behavior and improve ranking performance at scale and researchers developing new click models. CLAX is available at: https://github.com/philipphager/clax
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Beamforming with Doppler-Aware Sparse Attention for High Mobility Environments</title>
<link>https://arxiv.org/abs/2511.03632</link>
<guid>https://arxiv.org/abs/2511.03632</guid>
<content:encoded><![CDATA[
arXiv:2511.03632v1 Announce Type: cross 
Abstract: Beamforming has significance for enhancing spectral efficiency and mitigating interference in multi-antenna wireless systems, facilitating spatial multiplexing and diversity in dense and high mobility scenarios. Traditional beamforming techniques such as zero-forcing beamforming (ZFBF) and minimum mean square error (MMSE) beamforming experience performance deterioration under adverse channel conditions. Deep learning-based beamforming offers an alternative with nonlinear mappings from channel state information (CSI) to beamforming weights by improving robustness against dynamic channel environments. Transformer-based models are particularly effective due to their ability to model long-range dependencies across time and frequency. However, their quadratic attention complexity limits scalability in large OFDM grids. Recent studies address this issue through sparse attention mechanisms that reduce complexity while maintaining expressiveness, yet often employ patterns that disregard channel dynamics, as they are not specifically designed for wireless communication scenarios. In this work, we propose a Doppler-aware Sparse Neural Network Beamforming (Doppler-aware Sparse NNBF) model that incorporates a channel-adaptive sparse attention mechanism in a multi-user single-input multiple-output (MU-SIMO) setting. The proposed sparsity structure is configurable along 2D time-frequency axes based on channel dynamics and is theoretically proven to ensure full connectivity within p hops, where p is the number of attention heads. Simulation results under urban macro (UMa) channel conditions show that Doppler-aware Sparse NNBF significantly outperforms both a fixed-pattern baseline, referred to as Standard Sparse NNBF, and conventional beamforming techniques ZFBF and MMSE beamforming in high mobility scenarios, while maintaining structured sparsity with a controlled number of attended keys per query.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability</title>
<link>https://arxiv.org/abs/2511.03635</link>
<guid>https://arxiv.org/abs/2511.03635</guid>
<content:encoded><![CDATA[
arXiv:2511.03635v1 Announce Type: cross 
Abstract: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward unseen targets. Existing research using contrastive, meta-learning, or data augmentation suffers from generalizability issues or lack of coherence between text and target. Recent works leveraging large language models (LLMs) for ZSSD focus either on improving unseen target-specific knowledge or generating explanations for stance analysis. However, most of these works are limited by their over-reliance on explicit reasoning, provide coarse explanations that lack nuance, and do not explicitly model the reasoning process, making it difficult to interpret the model's predictions. To address these issues, in our study, we develop a novel interpretable ZSSD framework, IRIS. We provide an interpretable understanding of the attitude of the input towards the target implicitly based on sequences within the text (implicit rationales) and explicitly based on linguistic measures (explicit rationales). IRIS considers stance detection as an information retrieval ranking task, understanding the relevance of implicit rationales for different stances to guide the model towards correct predictions without requiring the ground-truth of rationales, thus providing inherent interpretability. In addition, explicit rationales based on communicative features help decode the emotional and cognitive dimensions of stance, offering an interpretable understanding of the author's attitude towards the given target. Extensive experiments on the benchmark datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10% training data prove the generalizability of our model, benefiting from the proposed architecture and interpretable design.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Weighted Morphological Content of Large-Scale Structures via Simulation-Based Inference</title>
<link>https://arxiv.org/abs/2511.03636</link>
<guid>https://arxiv.org/abs/2511.03636</guid>
<content:encoded><![CDATA[
arXiv:2511.03636v1 Announce Type: cross 
Abstract: In this work, we perform a simulation-based forecasting analysis to compare the constraining power of two higher-order summary statistics of the large-scale structure (LSS), the Minkowski Functionals (MFs) and the Conditional Moments of Derivative (CMD), with a particular focus on their sensitivity to nonlinear and anisotropic features in redshift-space. Our analysis relies on halo catalogs from the Big Sobol Sequence(BSQ) simulations at redshift $z=0.5$, employing a likelihood-free inference framework implemented via neural posterior estimation. At the fiducial cosmology of the Quijote simulations $(\Omega_{m}=0.3175,\,\sigma_{8}=0.834)$, and for the smoothing scale $R=15\,h^{-1}$Mpc, we find that the CMD yields tighter forecasts for $(\Omega_{m}},\,\sigma_{8})$ than the zeroth- to third-order MFs components, improving the constraint precision by ${\sim}(44\%,\,52\%)$, ${\sim}(30\%,\,45\%)$, ${\sim}(27\%,\,17\%)$, and ${\sim}(26\%,\,17\%)$, respectively. A joint configuration combining the MFs and CMD further enhances the precision by approximately ${\sim}27\%$ compared to the standard MFs alone, highlighting the complementary anisotropy-sensitive information captured by the CMD in contrast to the scalar morphological content encapsulated by the MFs. We further extend the forecasting analysis to a continuous range of cosmological parameter values and multiple smoothing scales. Our results show that, although the absolute forecast uncertainty for each component of summary statistics depends on the underlying parameter values and the adopted smoothing scale, the relative constraining power among the summary statistics remains nearly constant throughout.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Testing Implies Structured Symmetry</title>
<link>https://arxiv.org/abs/2511.03653</link>
<guid>https://arxiv.org/abs/2511.03653</guid>
<content:encoded><![CDATA[
arXiv:2511.03653v1 Announce Type: cross 
Abstract: Given a small random sample of $n$-bit strings labeled by an unknown Boolean function, which properties of this function can be tested computationally efficiently? We show an equivalence between properties that are efficiently testable from few samples and properties with structured symmetry, which depend only on the function's average values on parts of a low-complexity partition of the domain. Without the efficiency constraint, a similar characterization in terms of unstructured symmetry was obtained by Blais and Yoshida (2019). Our main technical tool is supersimulation, which builds on methods from the algorithmic fairness literature to approximate arbitrarily complex functions by small-circuit simulators that fool significantly larger distinguishers.
  We extend the characterization along other axes as well. We show that allowing parts to overlap exponentially reduces their required number, broadening the scope of the construction from properties testable with $O(\log n)$ samples to properties testable with $O(n)$ samples. For larger sample sizes, we show that any efficient tester is essentially checking for indistinguishability from a bounded collection of small circuits, in the spirit of a characterization of testable graph properties. Finally, we show that our results for Boolean function testing generalize to high-entropy distribution testing on arbitrary domains.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Colorectal Cancer Histopathological Grading using Multi-Scale Federated Learning</title>
<link>https://arxiv.org/abs/2511.03693</link>
<guid>https://arxiv.org/abs/2511.03693</guid>
<content:encoded><![CDATA[
arXiv:2511.03693v1 Announce Type: cross 
Abstract: Colorectal cancer (CRC) grading is a critical prognostic factor but remains hampered by inter-observer variability and the privacy constraints of multi-institutional data sharing. While deep learning offers a path to automation, centralized training models conflict with data governance regulations and neglect the diagnostic importance of multi-scale analysis. In this work, we propose a scalable, privacy-preserving federated learning (FL) framework for CRC histopathological grading that integrates multi-scale feature learning within a distributed training paradigm. Our approach employs a dual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear detail and broader tissue-level context. This architecture is integrated into a robust FL system stabilized using FedProx to mitigate client drift across heterogeneous data distributions from multiple hospitals. Extensive evaluation on the CRC-HGD dataset demonstrates that our framework achieves an overall accuracy of 83.5%, outperforming a comparable centralized model (81.6%). Crucially, the system excels in identifying the most aggressive Grade III tumors with a high recall of 87.5%, a key clinical priority to prevent dangerous false negatives. Performance further improves with higher magnification, reaching 88.0% accuracy at 40x. These results validate that our federated multi-scale approach not only preserves patient privacy but also enhances model performance and generalization. The proposed modular pipeline, with built-in preprocessing, checkpointing, and error handling, establishes a foundational step toward deployable, privacy-aware clinical AI for digital pathology.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Adaptivity Barrier in Batched Nonparametric Bandits: Sharp Characterization of the Price of Unknown Margin</title>
<link>https://arxiv.org/abs/2511.03708</link>
<guid>https://arxiv.org/abs/2511.03708</guid>
<content:encoded><![CDATA[
arXiv:2511.03708v1 Announce Type: cross 
Abstract: We study batched nonparametric contextual bandits under a margin condition when the margin parameter $\alpha$ is unknown. To capture the statistical price of this ignorance, we introduce the regret inflation criterion, defined as the ratio between the regret of an adaptive algorithm and that of an oracle knowing $\alpha$. We show that the optimal regret inflation grows polynomial with the horizon $T$, with exponent precisely given by the value of a convex optimization problem involving the dimension, smoothness, and batch budget. Moreover, the minimizers of this optimization problem directly prescribe the batch allocation and exploration strategy of a rate-optimal algorithm. Building on this principle, we develop RoBIN (RObust batched algorithm with adaptive BINning), which achieves the optimal regret inflation up to logarithmic factors. These results reveal a new adaptivity barrier: under batching, adaptation to an unknown margin parameter inevitably incurs a polynomial penalty, sharply characterized by a variational problem. Remarkably, this barrier vanishes when the number of batches exceeds $\log \log T$; with only a doubly logarithmic number of updates, one can recover the oracle regret rate up to polylogarithmic factors.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy Representation Learning via Information Funnels and Bottlenecks</title>
<link>https://arxiv.org/abs/2211.01446</link>
<guid>https://arxiv.org/abs/2211.01446</guid>
<content:encoded><![CDATA[
arXiv:2211.01446v2 Announce Type: replace 
Abstract: Ensuring trustworthiness in machine learning -- by balancing utility, fairness, and privacy -- remains a critical challenge, particularly in representation learning. In this work, we investigate a family of closely related information-theoretic objectives, including information funnels and bottlenecks, designed to extract invariant representations from data. We introduce the Conditional Privacy Funnel with Side-information (CPFSI), a novel formulation within this family, applicable in both fully and semi-supervised settings. Given the intractability of these objectives, we derive neural-network-based approximations via amortized variational inference. We systematically analyze the trade-offs between utility, invariance, and representation fidelity, offering new insights into the Pareto frontiers of these methods. Our results demonstrate that CPFSI effectively balances these competing objectives and frequently outperforms existing approaches. Furthermore, we show that by intervening on sensitive attributes in CPFSI's predictive posterior enhances fairness while maintaining predictive performance. Finally, we focus on the real-world applicability of these approaches, particularly for learning robust and fair representations from tabular datasets in data scarce-environments -- a modality where these methods are often especially relevant.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How does training shape the Riemannian geometry of neural network representations?</title>
<link>https://arxiv.org/abs/2301.11375</link>
<guid>https://arxiv.org/abs/2301.11375</guid>
<content:encoded><![CDATA[
arXiv:2301.11375v4 Announce Type: replace 
Abstract: In machine learning, there is a long history of trying to build neural networks that can learn from fewer example data by baking in strong geometric priors. However, it is not always clear a priori what geometric constraints are appropriate for a given task. Here, we explore the possibility that one can uncover useful geometric inductive biases by studying how training molds the Riemannian geometry induced by unconstrained neural network feature maps. We first show that at infinite width, neural networks with random parameters induce highly symmetric metrics on input space. This symmetry is broken by feature learning: networks trained to perform classification tasks learn to magnify local areas along decision boundaries. This holds in deep networks trained on high-dimensional image classification tasks, and even in self-supervised representation learning. These results begin to elucidate how training shapes the geometry induced by unconstrained neural network feature maps, laying the groundwork for an understanding of this richly nonlinear form of feature learning.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion Detection From Social Media Posts</title>
<link>https://arxiv.org/abs/2302.05610</link>
<guid>https://arxiv.org/abs/2302.05610</guid>
<content:encoded><![CDATA[
arXiv:2302.05610v2 Announce Type: replace 
Abstract: Over the last few years, social media has evolved into a medium for expressing personal views, emotions, and even business and political proposals, recommendations, and advertisements. We address the topic of identifying emotions from text data obtained from social media posts like Twitter in this research. We have deployed different traditional machine learning techniques such as Support Vector Machines (SVM), Naive Bayes, Decision Trees, and Random Forest, as well as deep neural network models such as LSTM, CNN, GRU, BiLSTM, BiGRU to classify these tweets into four emotion categories (Fear, Anger, Joy, and Sadness). Furthermore, we have constructed a BiLSTM and BiGRU ensemble model. The evaluation result shows that the deep neural network models(BiGRU, to be specific) produce the most promising results compared to traditional machine learning models, with an 87.53 % accuracy rate. The ensemble model performs even better (87.66 %), albeit the difference is not significant. This result will aid in the development of a decision-making tool that visualizes emotional fluctuations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges</title>
<link>https://arxiv.org/abs/2403.04468</link>
<guid>https://arxiv.org/abs/2403.04468</guid>
<content:encoded><![CDATA[
arXiv:2403.04468v2 Announce Type: replace 
Abstract: Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive survey that systematically reviews existing GNN models, focusing on solutions to the four mentioned real-world challenges including imbalance, noise, privacy, and OOD in practical scenarios that many existing reviews have not considered. Specifically, we first highlight the four key challenges faced by existing GNNs, paving the way for our exploration of real-world GNN models. Subsequently, we provide detailed discussions on these four aspects, dissecting how these solutions contribute to enhancing the reliability and robustness of GNN models. Last but not least, we outline promising directions and offer future perspectives in the field.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reliable Cryptographic Framework for Empirical Machine Unlearning Evaluation</title>
<link>https://arxiv.org/abs/2404.11577</link>
<guid>https://arxiv.org/abs/2404.11577</guid>
<content:encoded><![CDATA[
arXiv:2404.11577v4 Announce Type: replace 
Abstract: Machine unlearning updates machine learning models to remove information from specific training samples, complying with data protection regulations that allow individuals to request the removal of their personal data. Despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question. In this work, we focus on membership inference attack (MIA) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics lacking theoretical understanding and reliability. Specifically, by modeling the proposed evaluation process as a \emph{cryptographic game} between unlearning algorithms and MIA adversaries, the naturally induced evaluation metric measures the data removal efficacy of unlearning algorithms and enjoys provable guarantees that existing evaluation metrics fail to satisfy. Furthermore, we propose a practical and efficient approximation of the induced evaluation metric and demonstrate its effectiveness through both theoretical analysis and empirical experiments. Overall, this work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignIQL: Policy Alignment in Implicit Q-Learning through Constrained Optimization</title>
<link>https://arxiv.org/abs/2405.18187</link>
<guid>https://arxiv.org/abs/2405.18187</guid>
<content:encoded><![CDATA[
arXiv:2405.18187v2 Announce Type: replace 
Abstract: Implicit Q-learning (IQL) serves as a strong baseline for offline RL, which learns the value function using only dataset actions through quantile regression. However, it is unclear how to recover the implicit policy from the learned implicit Q-function and why IQL can utilize weighted regression for policy extraction. IDQL reinterprets IQL as an actor-critic method and gets weights of implicit policy, however, this weight only holds for the optimal value function. In this work, we introduce a different way to solve the implicit policy-finding problem (IPF) by formulating this problem as an optimization problem. Based on this optimization problem, we further propose two practical algorithms AlignIQL and AlignIQL-hard, which inherit the advantages of decoupling actor from critic in IQL and provide insights into why IQL can use weighted regression for policy extraction. Compared with IQL and IDQL, we find our method keeps the simplicity of IQL and solves the implicit policy-finding problem. Experimental results on D4RL datasets show that our method achieves competitive or superior results compared with other SOTA offline RL methods. Especially in complex sparse reward tasks like Antmaze and Adroit, our method outperforms IQL and IDQL by a significant margin.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Quality Monitoring for the Hadron Calorimeters Using Transfer Learning for Anomaly Detection</title>
<link>https://arxiv.org/abs/2408.16612</link>
<guid>https://arxiv.org/abs/2408.16612</guid>
<content:encoded><![CDATA[
arXiv:2408.16612v3 Announce Type: replace 
Abstract: The proliferation of sensors brings an immense volume of spatio-temporal (ST) data in many domains, including monitoring, diagnostics, and prognostics applications. Data curation is a time-consuming process for a large volume of data, making it challenging and expensive to deploy data analytics platforms in new environments. Transfer learning (TL) mechanisms promise to mitigate data sparsity and model complexity by utilizing pre-trained models for a new task. Despite the triumph of TL in fields like computer vision and natural language processing, efforts on complex ST models for anomaly detection (AD) applications are limited. In this study, we present the potential of TL within the context of high-dimensional ST AD with a hybrid autoencoder architecture, incorporating convolutional, graph, and recurrent neural networks. Motivated by the need for improved model accuracy and robustness, particularly in scenarios with limited training data on systems with thousands of sensors, this research investigates the transferability of models trained on different sections of the Hadron Calorimeter of the Compact Muon Solenoid experiment at CERN. The key contributions of the study include exploring TL's potential and limitations within the context of encoder and decoder networks, revealing insights into model initialization and training configurations that enhance performance while substantially reducing trainable parameters and mitigating data contamination effects. Code: https://github.com/muleina/CMS\_HCAL\_ML\_OnlineDQM .
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization</title>
<link>https://arxiv.org/abs/2410.02628</link>
<guid>https://arxiv.org/abs/2410.02628</guid>
<content:encoded><![CDATA[
arXiv:2410.02628v4 Announce Type: replace 
Abstract: Learning conditional distributions $\pi^*(\cdot|x)$ is a central problem in machine learning, which is typically approached via supervised methods with paired data $(x,y) \sim \pi^*$. However, acquiring paired data samples is often challenging, especially in problems such as domain translation. This necessitates the development of $\textit{semi-supervised}$ models that utilize both limited paired data and additional unpaired i.i.d. samples $x \sim \pi^*_x$ and $y \sim \pi^*_y$ from the marginal distributions. The usage of such combined data is complex and often relies on heuristic approaches. To tackle this issue, we propose a new learning paradigm that integrates both paired and unpaired data $\textbf{seamlessly}$ using the data likelihood maximization techniques. We demonstrate that our approach also connects intriguingly with inverse entropic optimal transport (OT). This finding allows us to apply recent advances in computational OT to establish an $\textbf{end-to-end}$ learning algorithm to get $\pi^*(\cdot|x)$. In addition, we derive the universal approximation property, demonstrating that our approach can theoretically recover true conditional distributions with arbitrarily small error. Furthermore, we demonstrate through empirical tests that our method effectively learns conditional distributions using paired and unpaired data simultaneously.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamical loss functions shape landscape topography and improve learning in artificial neural networks</title>
<link>https://arxiv.org/abs/2410.10690</link>
<guid>https://arxiv.org/abs/2410.10690</guid>
<content:encoded><![CDATA[
arXiv:2410.10690v3 Announce Type: replace 
Abstract: Dynamical loss functions are derived from standard loss functions used in supervised classification tasks, but are modified so that the contribution from each class periodically increases and decreases. These oscillations globally alter the loss landscape without affecting the global minima. In this paper, we demonstrate how to transform cross-entropy and mean squared error into dynamical loss functions. We begin by discussing the impact of increasing the size of the neural network or the learning rate on the depth and sharpness of the minima that the system explores. Building on this intuition, we propose several versions of dynamical loss functions and use a simple classification problem where we can show how they significantly improve validation accuracy for networks of varying sizes. Finally, we explore how the landscape of these dynamical loss functions evolves during training, highlighting the emergence of instabilities that may be linked to edge-of-instability minimization.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs</title>
<link>https://arxiv.org/abs/2410.20749</link>
<guid>https://arxiv.org/abs/2410.20749</guid>
<content:encoded><![CDATA[
arXiv:2410.20749v3 Announce Type: replace 
Abstract: Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. Existing works aim to enhance LLM capabilities via domain-specific adaptation, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. To address this challenge, we introduce Matryoshka Pilot (M-Pilot), a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs. Specifically, we consider the black-box LLM as an environment, with M-Pilot serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM. M-Pilot is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. Empirical evaluations on diverse tasks demonstrate that our method effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks. Our code is publicly available at: https://github.com/lichangh20/Matryoshka.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Expressive Random Feature Models via Parametrized Activations</title>
<link>https://arxiv.org/abs/2411.19468</link>
<guid>https://arxiv.org/abs/2411.19468</guid>
<content:encoded><![CDATA[
arXiv:2411.19468v3 Announce Type: replace 
Abstract: Random feature (RF) method is a powerful kernel approximation technique, but is typically equipped with fixed activation functions, limiting its adaptability across diverse tasks. To overcome this limitation, we introduce the Random Feature Model with Learnable Activation Functions (RFLAF), a novel statistical model that parameterizes activation functions as weighted sums of basis functions within the random feature framework. Examples of basis functions include radial basis functions, spline functions, polynomials, and so forth. For theoretical results, we consider RBFs as representative basis functions. We start with a single RBF as the activation, and then extend the results to multiple RBFs, demonstrating that RF models with learnable activation component largely expand the represented function space. We provide estimates on the required number of samples and random features to achieve low excess risks. For experiments, we test RFLAF with three types of bases: radial basis functions, spline functions and polynomials. Experimental results show that RFLAFs with RBFs and splines consistently outperform other RF models, where RBFs show 3 times faster computational efficiency than splines. We then unfreeze the first-layer parameters and retrain the models, validating the expressivity advantage of learnable activation components on regular two-layer neural networks. Our work provides a deeper understanding of the component of learnable activation functions within modern neural network architectures.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REFA: Reference Free Alignment for multi-preference optimization</title>
<link>https://arxiv.org/abs/2412.16378</link>
<guid>https://arxiv.org/abs/2412.16378</guid>
<content:encoded><![CDATA[
arXiv:2412.16378v4 Announce Type: replace 
Abstract: To mitigate reward hacking from response verbosity, modern preference optimization methods are increasingly adopting length normalization (e.g., SimPO, ORPO, LN-DPO). While effective against this bias, we demonstrate that length normalization itself introduces a failure mode: the URSLA shortcut. Here models learn to satisfy the alignment objective by prematurely truncating low-quality responses rather than learning from their semantic content. To address this, we introduce REFA, a new alignment framework that proposes probabilistic control on a structural token that controls termination. Our core innovation is a new class of regularizers that operate directly on the probability of the End-of-Sequence (EOS) token, a previously unexploited control lever. This token-level intervention provides a principled solution to the URSLA shortcut, ensuring genuine quality improvements. Furthermore, it unlocks a versatile mechanism for managing the alignment-efficiency tradeoff, enabling practitioners to fine-tune models that adhere to specific token budgets. Empirically, REFA achieves a 60.29% win rate and a 52.17% length-controlled win rate on AlpacaEval2 with Llama-3-8B-Instruct, demonstrating the power of our token-level control paradigm.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs</title>
<link>https://arxiv.org/abs/2501.02625</link>
<guid>https://arxiv.org/abs/2501.02625</guid>
<content:encoded><![CDATA[
arXiv:2501.02625v3 Announce Type: replace 
Abstract: Quantized training of Large Language Models (LLMs) remains an open challenge, as maintaining accuracy while performing all matrix multiplications in low precision has proven difficult. This is particularly the case when fine-tuning pre-trained models, which can have large weight and activation outlier values that make lower-precision optimization difficult. To address this, we present HALO, a novel quantization-aware training approach for Transformers that enables accurate and efficient low-precision training by combining 1) strategic placement of Hadamard rotations in both forward and backward passes, which mitigate outliers, 2) high-performance kernel support, and 3) FSDP integration for low-precision communication. Our approach ensures that all large matrix multiplications during the forward and backward passes are executed in lower precision. Applied to LLAMA-family models, HALO achieves near-full-precision-equivalent results during fine-tuning on various tasks, while delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX 4090 GPUs. HALO efficiently supports both standard and parameterefficient fine-tuning (PEFT). Our results demonstrate the first practical approach to fully quantized LLM fine-tuning that maintains accuracy in 8-bit precision, while delivering performance benefits. Code is available at https://github.com/IST-DASLab/HALO.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REINFORCE-ING Chemical Language Models for Drug Discovery</title>
<link>https://arxiv.org/abs/2501.15971</link>
<guid>https://arxiv.org/abs/2501.15971</guid>
<content:encoded><![CDATA[
arXiv:2501.15971v2 Announce Type: replace 
Abstract: Chemical language models, combined with reinforcement learning (RL), have shown significant promise to efficiently traverse large chemical spaces for drug discovery. However, the performance of various RL algorithms and their best practices for practical drug discovery are still unclear. Here, starting from the principles of the REINFORCE algorithm, we investigate the effect of different components from RL theory including experience replay, hill-climbing, baselines to reduce variance, and alternative reward shaping. We propose a new regularization method more aligned to REINFORCE than current standard practices, and demonstrate how RL hyperparameters can be fine-tuned for effectiveness and efficiency. Lastly, we apply our learnings to practical drug discovery by demonstrating enhanced learning efficiency on frontier binding affinity models by using Boltz2 as a reward model. We share our RL models used in the ACEGEN repository, and hope the experiments here act as a guide to researchers applying RL to chemical language models for drug discovery.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sundial: A Family of Highly Capable Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2502.00816</link>
<guid>https://arxiv.org/abs/2502.00816</guid>
<content:encoded><![CDATA[
arXiv:2502.00816v3 Announce Type: replace 
Abstract: We introduce Sundial, a family of native, flexible, and scalable time series foundation models. To predict the next-patch's distribution, we propose a TimeFlow Loss based on flow-matching, which facilitates native pre-training of Transformers on continuous-valued time series without discrete tokenization. Conditioned on arbitrary-length time series, our models are pre-trained without specifying any prior distribution and can generate multiple probable predictions, achieving more flexibility in representation learning than using parametric densities. Towards time series foundation models, we leverage minimal but crucial adaptations of Transformers and curate TimeBench with one trillion time points, comprising mostly real-world datasets and synthetic data. By mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial models on TimeBench, which achieve unprecedented model capacity and generalization performance. In addition to excellent scalability, Sundial achieves state-of-the-art results on both point and probabilistic forecasting benchmarks with a just-in-time inference speed, i.e., making zero-shot predictions within a few milliseconds. We believe that Sundial's pioneering generative forecasting capability can improve model reliability in real-world decision-making. Code is available at: https://github.com/thuml/Sundial.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable Port-Hamiltonian Neural Networks</title>
<link>https://arxiv.org/abs/2502.02480</link>
<guid>https://arxiv.org/abs/2502.02480</guid>
<content:encoded><![CDATA[
arXiv:2502.02480v2 Announce Type: replace 
Abstract: In recent years, nonlinear dynamic system identification using artificial neural networks has garnered attention due to its broad potential applications across science and engineering. However, purely data-driven approaches often struggle with extrapolation and may yield physically implausible forecasts. Furthermore, the learned dynamics can exhibit instabilities, making it difficult to apply such models safely and robustly. This article introduces stable port-Hamiltonian neural networks, a machine learning architecture that incorporates physical biases of energy conservation and dissipation while ensuring global Lyapunov stability of the learned dynamics. Through illustrative and real-world examples, we demonstrate that these strong inductive biases facilitate robust learning of stable dynamics from sparse data, while avoiding instability and surpassing purely data-driven approaches in accuracy and physically meaningful generalization. Furthermore, the model's applicability and potential for data-driven surrogate modeling are showcased on multi-physics simulation data.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Covariance Matrix: The Statistical Complexity of Private Linear Regression</title>
<link>https://arxiv.org/abs/2502.13115</link>
<guid>https://arxiv.org/abs/2502.13115</guid>
<content:encoded><![CDATA[
arXiv:2502.13115v2 Announce Type: replace 
Abstract: We study the statistical complexity of private linear regression under an unknown, potentially ill-conditioned covariate distribution. Somewhat surprisingly, under privacy constraints the intrinsic complexity is \emph{not} captured by the usual covariance matrix but rather its $L_1$ analogues. Building on this insight, we establish minimax convergence rates for both the central and local privacy models and introduce an Information-Weighted Regression method that attains the optimal rates.
  As application, in private linear contextual bandits, we propose an efficient algorithm that achieves rate-optimal regret bounds of order $\sqrt{T}+\frac{1}{\alpha}$ and $\sqrt{T}/\alpha$ under joint and local $\alpha$-privacy models, respectively. Notably, our results demonstrate that joint privacy comes at almost no additional cost, addressing the open problems posed by Azize and Basu (2024).
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decision-aware training of spatiotemporal forecasting models to select a top K subset of sites for intervention</title>
<link>https://arxiv.org/abs/2503.05622</link>
<guid>https://arxiv.org/abs/2503.05622</guid>
<content:encoded><![CDATA[
arXiv:2503.05622v3 Announce Type: replace 
Abstract: Optimal allocation of scarce resources is a common problem for decision makers faced with choosing a limited number of locations for intervention. Spatiotemporal prediction models could make such decisions data-driven. A recent performance metric called fraction of best possible reach (BPR) measures the impact of using a model's recommended size K subset of sites compared to the best possible top-K in hindsight. We tackle two open problems related to BPR. First, we explore how to rank all sites numerically given a probabilistic model that predicts event counts jointly across sites. Ranking via the per-site mean is suboptimal for BPR. Instead, we offer a better ranking for BPR backed by decision theory. Second, we explore how to train a probabilistic model's parameters to maximize BPR. Discrete selection of K sites implies all-zero parameter gradients which prevent standard gradient training. We overcome this barrier via advances in perturbed optimizers. We further suggest a training objective that combines likelihood with a decision-aware BPR constraint to deliver high-quality top-K rankings as well as good forecasts for all sites. We demonstrate our approach on two where-to-intervene applications: mitigating opioid-related fatal overdoses for public health and monitoring endangered wildlife.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting semi-supervised learning in the era of foundation models</title>
<link>https://arxiv.org/abs/2503.09707</link>
<guid>https://arxiv.org/abs/2503.09707</guid>
<content:encoded><![CDATA[
arXiv:2503.09707v4 Announce Type: replace 
Abstract: Semi-supervised learning (SSL) leverages abundant unlabeled data alongside limited labeled data to enhance learning. As vision foundation models (VFMs) increasingly serve as the backbone of vision applications, it remains unclear how SSL interacts with these pre-trained models. To address this gap, we develop new SSL benchmark datasets where frozen VFMs underperform and systematically evaluate representative SSL methods. We make a surprising observation: parameter-efficient fine-tuning (PEFT) using only labeled data often matches SSL performance, even without leveraging unlabeled data. This motivates us to revisit self-training, a conceptually simple SSL baseline, where we use the supervised PEFT model to pseudo-label unlabeled data for further training. To overcome the notorious issue of noisy pseudo-labels, we propose ensembling multiple PEFT approaches and VFM backbones to produce more robust pseudo-labels. Empirical results validate the effectiveness of this simple yet powerful approach, providing actionable insights into SSL with VFMs and paving the way for more scalable and practical semi-supervised learning in the era of foundation models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniFault: A Fault Diagnosis Foundation Model from Bearing Data</title>
<link>https://arxiv.org/abs/2504.01373</link>
<guid>https://arxiv.org/abs/2504.01373</guid>
<content:encoded><![CDATA[
arXiv:2504.01373v2 Announce Type: replace 
Abstract: Machine fault diagnosis (FD) is a critical task for predictive maintenance, enabling early fault detection and preventing unexpected failures. Despite its importance, existing FD models are operation-specific with limited generalization across diverse datasets. Foundation models (FM) have demonstrated remarkable potential in both visual and language domains, achieving impressive generalization capabilities even with minimal data through few-shot or zero-shot learning. However, translating these advances to FD presents unique hurdles. Unlike the large-scale, cohesive datasets available for images and text, FD datasets are typically smaller and more heterogeneous, with significant variations in sampling frequencies and the number of channels across different systems and applications. This heterogeneity complicates the design of a universal architecture capable of effectively processing such diverse data while maintaining robust feature extraction and learning capabilities. In this paper, we introduce UniFault, a foundation model for fault diagnosis that systematically addresses these issues. Specifically, the model incorporates a comprehensive data harmonization pipeline featuring two key innovations. First, a unification scheme transforms multivariate inputs into standardized univariate sequences. Second, a novel cross-domain temporal fusion strategy mitigates distribution shifts and enriches sample diversity and count, improving the model generalization across varying conditions. UniFault is pretrained on over 6.9 million samples spanning diverse FD datasets, enabling superior few-shot performance. Extensive experiments on real-world FD datasets demonstrate that UniFault achieves state-of-the-art performance, setting a new benchmark for fault diagnosis models and paving the way for more scalable and robust predictive maintenance solutions.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable and efficient inverse analysis using physics-informed neural networks with normalized distance functions and adaptive weight tuning</title>
<link>https://arxiv.org/abs/2504.18091</link>
<guid>https://arxiv.org/abs/2504.18091</guid>
<content:encoded><![CDATA[
arXiv:2504.18091v3 Announce Type: replace 
Abstract: Physics-informed neural networks have attracted significant attention in scientific machine learning for their capability to solve forward and inverse problems governed by partial differential equations. However, the accuracy of PINN solutions is often limited by the treatment of boundary conditions. Conventional penalty-based methods, which incorporate boundary conditions as penalty terms in the loss function, cannot guarantee exact satisfaction of the given boundary conditions and are highly sensitive to the choice of penalty parameters. This paper demonstrates that distance functions, specifically R-functions, can be leveraged to enforce boundary conditions, overcoming these limitations. R-functions provide normalized distance fields, enabling flexible representation of boundary geometries, including non-convex domains, and facilitating various types of boundary conditions. Nevertheless, distance functions alone are insufficient for accurate inverse analysis in PINNs. To address this, we propose an integrated framework that combines the normalized distance field with bias-corrected adaptive weight tuning to improve both accuracy and efficiency. Numerical results show that the proposed method provides more accurate and efficient solutions to various inverse problems than penalty-based approaches, even in the presence of non-convex geometries with complex boundary conditions. This approach offers a reliable and efficient framework for inverse analysis using PINNs, with potential applications across a wide range of engineering problems.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuralSurv: Deep Survival Analysis with Bayesian Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2505.11054</link>
<guid>https://arxiv.org/abs/2505.11054</guid>
<content:encoded><![CDATA[
arXiv:2505.11054v2 Announce Type: replace 
Abstract: We introduce NeuralSurv, the first deep survival model to incorporate Bayesian uncertainty quantification. Our non-parametric, architecture-agnostic framework captures time-varying covariate-risk relationships in continuous time via a novel two-stage data-augmentation scheme, for which we establish theoretical guarantees. For efficient posterior inference, we introduce a mean-field variational algorithm with coordinate-ascent updates that scale linearly in model size. By locally linearizing the Bayesian neural network, we obtain full conjugacy and derive all coordinate updates in closed form. In experiments, NeuralSurv delivers superior calibration compared to state-of-the-art deep survival models, while matching or exceeding their discriminative performance across both synthetic benchmarks and real-world datasets. Our results demonstrate the value of Bayesian principles in data-scarce regimes by enhancing model calibration and providing robust, well-calibrated uncertainty estimates for the survival function.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>This Time is Different: An Observability Perspective on Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2505.14766</link>
<guid>https://arxiv.org/abs/2505.14766</guid>
<content:encoded><![CDATA[
arXiv:2505.14766v2 Announce Type: replace 
Abstract: We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10$\times$ larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians</title>
<link>https://arxiv.org/abs/2505.19458</link>
<guid>https://arxiv.org/abs/2505.19458</guid>
<content:encoded><![CDATA[
arXiv:2505.19458v4 Announce Type: replace 
Abstract: The theoretical understanding of self-attention (SA) has been steadily progressing. A prominent line of work studies a class of SA layers that admit an energy function decreased by state updates. While it provides valuable insights into inherent biases in signal propagation, it often relies on idealized assumptions or additional constraints not necessarily present in standard SA. Thus, to broaden our understanding, this work aims to relax these energy constraints and provide an energy-agnostic characterization of inference dynamics by dynamical systems analysis. In more detail, we first consider relaxing the symmetry and single-head constraints traditionally required in energy-based formulations. Next, we show that analyzing the Jacobian matrix of the state is highly valuable when investigating more general SA architectures without necessarily admitting an energy function. It reveals that the normalization layer plays an essential role in suppressing the Lipschitzness of SA and the Jacobian's complex eigenvalues, which correspond to the oscillatory components of the dynamics. In addition, the Lyapunov exponents computed from the Jacobians demonstrate that the normalized dynamics lie close to a critical state, and this criticality serves as a strong indicator of high inference performance. Furthermore, the Jacobian perspective also enables us to develop regularization methods for training and a pseudo-energy for monitoring inference dynamics.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On scalable and efficient training of diffusion samplers</title>
<link>https://arxiv.org/abs/2505.19552</link>
<guid>https://arxiv.org/abs/2505.19552</guid>
<content:encoded><![CDATA[
arXiv:2505.19552v3 Announce Type: replace 
Abstract: We address the challenge of training diffusion models to sample from unnormalized energy distributions in the absence of data, the so-called diffusion samplers. Although these approaches have shown promise, they struggle to scale in more demanding scenarios where energy evaluations are expensive and the sampling space is high-dimensional. To address this limitation, we propose a scalable and sample-efficient framework that properly harmonizes the powerful classical sampling method and the diffusion sampler. Specifically, we utilize Monte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy as a Searcher to collect off-policy samples, using an auxiliary energy function to compensate for exploring modes the diffusion sampler rarely visits. These off-policy samples are then combined with on-policy data to train the diffusion sampler, thereby expanding its coverage of the energy landscape. Furthermore, we identify primacy bias, i.e., the preference of samplers for early experience during training, as the main cause of mode collapse during training, and introduce a periodic re-initialization trick to resolve this issue. Our method significantly improves sample efficiency on standard benchmarks for diffusion samplers and also excels at higher-dimensional problems and real-world molecular conformer generation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Theoretical Framework for Grokking: Interpolation followed by Riemannian Norm Minimisation</title>
<link>https://arxiv.org/abs/2505.20172</link>
<guid>https://arxiv.org/abs/2505.20172</guid>
<content:encoded><![CDATA[
arXiv:2505.20172v2 Announce Type: replace 
Abstract: We study the dynamics of gradient flow with small weight decay on general training losses $F: \mathbb{R}^d \to \mathbb{R}$. Under mild regularity assumptions and assuming convergence of the unregularised gradient flow, we show that the trajectory with weight decay $\lambda$ exhibits a two-phase behaviour as $\lambda \to 0$. During the initial fast phase, the trajectory follows the unregularised gradient flow and converges to a manifold of critical points of $F$. Then, at time of order $1/\lambda$, the trajectory enters a slow drift phase and follows a Riemannian gradient flow minimising the $\ell_2$-norm of the parameters. This purely optimisation-based phenomenon offers a natural explanation for the \textit{grokking} effect observed in deep learning, where the training loss rapidly reaches zero while the test loss plateaus for an extended period before suddenly improving. We argue that this generalisation jump can be attributed to the slow norm reduction induced by weight decay, as explained by our analysis. We validate this mechanism empirically on several synthetic regression tasks.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust and Computation-Aware Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.21133</link>
<guid>https://arxiv.org/abs/2505.21133</guid>
<content:encoded><![CDATA[
arXiv:2505.21133v2 Announce Type: replace 
Abstract: Gaussian processes (GPs) are widely used for regression and optimization tasks such as Bayesian optimization (BO) due to their expressiveness and principled uncertainty estimates. However, in settings with large datasets corrupted by outliers, standard GPs and their sparse approximations struggle with computational tractability and robustness. We introduce Robust Computation-aware Gaussian Process (RCaGP), a novel GP model that jointly addresses these challenges by combining a principled treatment of approximation-induced uncertainty with robust generalized Bayesian updating. The key insight is that robustness and approximation-awareness are not orthogonal but intertwined: approximations can exacerbate the impact of outliers, and mitigating one without the other is insufficient. Unlike previous work that focuses narrowly on either robustness or approximation quality, RCaGP combines both in a principled and scalable framework, thus effectively managing both outliers and computational uncertainties introduced by approximations such as low-rank matrix multiplications. Our model ensures more conservative and reliable uncertainty estimates, a property we rigorously demonstrate. Additionally, we establish a robustness property and show that the mean function is key to preserving it, motivating a tailored model selection scheme for robust mean functions. Empirical results confirm that solving these challenges jointly leads to superior performance across both clean and outlier-contaminated settings, both on regression and high-throughput Bayesian optimization benchmarks.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty</title>
<link>https://arxiv.org/abs/2505.23506</link>
<guid>https://arxiv.org/abs/2505.23506</guid>
<content:encoded><![CDATA[
arXiv:2505.23506v2 Announce Type: replace 
Abstract: In recent years various supervised learning methods that disentangle aleatoric and epistemic uncertainty based on second-order distributions have been proposed. We argue that these methods fail to capture critical components of epistemic uncertainty, particularly due to the often-neglected component of model bias. To show this, we make use of a more fine-grained taxonomy of epistemic uncertainty sources in machine learning models, and analyse how the classical bias-variance decomposition of the expected prediction error can be decomposed into different parts reflecting these uncertainties. By using a simulation-based evaluation protocol which encompasses epistemic uncertainty due to both procedural- and data-driven uncertainty components, we illustrate that current methods rarely capture the full spectrum of epistemic uncertainty. Through theoretical insights and synthetic experiments, we show that high model bias can lead to misleadingly low estimates of epistemic uncertainty, and common second-order uncertainty quantification methods systematically blur bias-induced errors into aleatoric estimates, thereby underrepresenting epistemic uncertainty. Our findings underscore that meaningful aleatoric estimates are feasible only if all relevant sources of epistemic uncertainty are properly represented.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiCoFlex: Model-agnostic diverse counterfactuals with flexible control</title>
<link>https://arxiv.org/abs/2505.23700</link>
<guid>https://arxiv.org/abs/2505.23700</guid>
<content:encoded><![CDATA[
arXiv:2505.23700v2 Announce Type: replace 
Abstract: Counterfactual explanations play a pivotal role in explainable artificial intelligence (XAI) by offering intuitive, human-understandable alternatives that elucidate machine learning model decisions. Despite their significance, existing methods for generating counterfactuals often require constant access to the predictive model, involve computationally intensive optimization for each instance and lack the flexibility to adapt to new user-defined constraints without retraining. In this paper, we propose DiCoFlex, a novel model-agnostic, conditional generative framework that produces multiple diverse counterfactuals in a single forward pass. Leveraging conditional normalizing flows trained solely on labeled data, DiCoFlex addresses key limitations by enabling real-time user-driven customization of constraints such as sparsity and actionability at inference time. Extensive experiments on standard benchmark datasets show that DiCoFlex outperforms existing methods in terms of validity, diversity, proximity, and constraint adherence, making it a practical and scalable solution for counterfactual generation in sensitive decision-making domains.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Informed Flows for Bayesian Inference</title>
<link>https://arxiv.org/abs/2505.24243</link>
<guid>https://arxiv.org/abs/2505.24243</guid>
<content:encoded><![CDATA[
arXiv:2505.24243v2 Announce Type: replace 
Abstract: Variational inference often struggles with the posterior geometry exhibited by complex hierarchical Bayesian models. Recent advances in flow-based variational families and Variationally Inferred Parameters (VIP) each address aspects of this challenge, but their formal relationship is unexplored. Here, we prove that the combination of VIP and a full-rank Gaussian can be represented exactly as a forward autoregressive flow augmented with a translation term and input from the model's prior. Guided by this theoretical insight, we introduce the Model-Informed Flow (MIF) architecture, which adds the necessary translation mechanism, prior information, and hierarchical ordering. Empirically, MIF delivers tighter posterior approximations and matches or exceeds state-of-the-art performance across a suite of hierarchical and non-hierarchical benchmarks.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dense SAE Latents Are Features, Not Bugs</title>
<link>https://arxiv.org/abs/2506.15679</link>
<guid>https://arxiv.org/abs/2506.15679</guid>
<content:encoded><![CDATA[
arXiv:2506.15679v2 Announce Type: replace 
Abstract: Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are \emph{dense}), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs -- suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and finally to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference-Time Reward Hacking in Large Language Models</title>
<link>https://arxiv.org/abs/2506.19248</link>
<guid>https://arxiv.org/abs/2506.19248</guid>
<content:encoded><![CDATA[
arXiv:2506.19248v2 Announce Type: replace 
Abstract: A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to an LLM's output that indicates, for example, how likely it is to align with user preferences or safety goals. However, reward models are never perfect. They inevitably function as proxies for complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, we can subvert intended alignment goals and reduce overall performance, a phenomenon commonly referred to as reward hacking. In this work, we characterize reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. We study this phenomenon under Best-of-$n$ (BoN) and Soft Best-of-$n$ (SBoN), and we introduce Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. We show that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, we introduce HedgeTune, an efficient algorithm to find the optimal inference-time parameter. We demonstrate that hedging mitigates reward hacking and achieves superior reward-distortion tradeoffs on math, reasoning, and human-preference setups.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training</title>
<link>https://arxiv.org/abs/2506.22638</link>
<guid>https://arxiv.org/abs/2506.22638</guid>
<content:encoded><![CDATA[
arXiv:2506.22638v2 Announce Type: replace 
Abstract: Large language models improve at math after instruction tuning, reinforcement learning, or knowledge distillation. We ask whether these gains come from major changes in the transformer layers or from smaller adjustments that keep the original structure. Using layer-wise ablation on base and trained variants, we find that math reasoning depends on a few critical layers, which stay important across all post- training methods. Removing these layers reduces math accuracy by as much as 80%, whereas factual recall tasks only show relatively smaller drops. This suggests that specialized layers for mathematical tasks form during pre-training and remain stable afterward. As measured by Normalized Mutual Information (NMI), we find that near these critical layers, tokens drift from their original syntactic clusters toward representations aligned with tokens less syntactically related but potentially more useful for downstream task.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedRef: Communication-Efficient Bayesian Fine-Tuning using a Reference Model</title>
<link>https://arxiv.org/abs/2506.23210</link>
<guid>https://arxiv.org/abs/2506.23210</guid>
<content:encoded><![CDATA[
arXiv:2506.23210v3 Announce Type: replace 
Abstract: Federated learning (FL) collaboratively trains artificial intelligence (AI) models to ensure user data privacy. Sharing only model updates generated from local training on client data with the server enhances user data privacy. However, model performance may suffer due to data and system heterogeneity among clients in FL scenarios. Previous studies have proposed model optimization, fine-tuning, and personalization to achieve improved model performance. Despite these efforts, models resulting from FL scenarios often exhibit catastrophic forgetting, which increases the communication and computational costs of clients for model optimization and raises energy consumption. To address these challenges, we propose a reference model-based fine-tuning method for federated learning that overcomes catastrophic forgetting in each round. Our method is derived from Bayesian parameter-efficient transfer learning and includes an proximal term. It employs a reference model that incorporates previous model parameters and reviews previous global features in the model optimization step to mitigate catastrophic forgetting. As a result, our method achieves higher model performance and lower communication and computational costs for clients than existing methods.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations</title>
<link>https://arxiv.org/abs/2507.01131</link>
<guid>https://arxiv.org/abs/2507.01131</guid>
<content:encoded><![CDATA[
arXiv:2507.01131v3 Announce Type: replace 
Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine learning interatomic potentials (MLIPs). The key operation of such networks is the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To accelerate the computation, we develop tensor decomposition networks (TDNs) as a class of approximately equivariant networks in which CG tensor products are replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP) decomposition. With the CP decomposition, we prove (i) a uniform bound on the induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of approximating any equivariant bilinear map. To further reduce the number of parameters, we propose path-weight sharing that ties all multiplicity-space weights across the $\mathcal{O}(L^3)$ CG paths into a single path without compromising equivariance, where $L$ is the maximum angular degree. The resulting layer acts as a plug-and-play replacement for tensor products in existing networks, and the computational complexity of tensor products is reduced from $\mathcal{O}(L^6)$ to $\mathcal{O}(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation dataset containing 105 million DFT-calculated snapshots. We also use existing datasets, including OC20, and OC22. Results show that TDNs achieve competitive performance with dramatic speedup in computations. Our code is publicly available as part of the AIRS library (\href{https://github.com/divelab/AIRS/tree/main/OpenMol/TDN}{https://github.com/divelab/AIRS/}).
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models</title>
<link>https://arxiv.org/abs/2507.06853</link>
<guid>https://arxiv.org/abs/2507.06853</guid>
<content:encoded><![CDATA[
arXiv:2507.06853v2 Announce Type: replace 
Abstract: Molecular structure elucidation from spectra is a fundamental challenge in molecular science. Conventional approaches rely heavily on expert interpretation and lack scalability, while retrieval-based machine learning approaches remain constrained by limited reference libraries. Generative models offer a promising alternative, yet most adopt autoregressive architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that formulates molecular structure elucidation as a conditional generation process, directly inferring 2D and 3D molecular structures from multi-modal spectra using diffusion models. Its denoising network is parameterized by the Diffusion Molecule Transformer, an SE(3)-equivariant architecture for geometric modeling, conditioned by SpecFormer, a Transformer-based spectral encoder capturing multi-modal spectral dependencies. Extensive experiments demonstrate that DiffSpectra accurately elucidates molecular structures, achieving 40.76% top-1 and 99.49% top-10 accuracy. Its performance benefits substantially from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. To our knowledge, DiffSpectra is the first framework that unifies multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compliance Minimization via Physics-Informed Gaussian Processes</title>
<link>https://arxiv.org/abs/2507.09968</link>
<guid>https://arxiv.org/abs/2507.09968</guid>
<content:encoded><![CDATA[
arXiv:2507.09968v2 Announce Type: replace 
Abstract: Machine learning (ML) techniques have recently gained significant attention for solving compliance minimization (CM) problems. However, these methods typically provide poor feature boundaries, are very expensive, and lack a systematic mechanism to control the design complexity. Herein, we address these limitations by proposing a mesh-free and simultaneous framework based on physics-informed Gaussian processes (GPs). In our approach, we parameterize the design and state variables with GP priors which have independent kernels but share a multi-output neural network (NN) as their mean function. The architecture of this NN is based on Parametric Grid Convolutional Attention Networks (PGCANs) which not only mitigate spectral bias issues, but also provide an interpretable mechanism to control design complexity. We estimate all the parameters of our GP-based representations by simultaneously minimizing the compliance, total potential energy, and residual of volume fraction constraint. Importantly, our loss function exclude all data-based residuals as GPs automatically satisfy them. We also develop computational schemes based on curriculum training and numerical integration to increase the efficiency and robustness of our approach which is shown to (1) produce super-resolution topologies with fast convergence, (2) achieve comparable compliance and less gray area fraction compared to traditional numerical methods, (3) provide control over fine-scale features, and (4) outperform competing ML-based methods.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composing Linear Layers from Irreducibles</title>
<link>https://arxiv.org/abs/2507.11688</link>
<guid>https://arxiv.org/abs/2507.11688</guid>
<content:encoded><![CDATA[
arXiv:2507.11688v3 Announce Type: replace 
Abstract: Contemporary large models often exhibit behaviors suggesting the presence of low-level primitives that compose into modules with richer functionality, but these fundamental building blocks remain poorly understood. We investigate this compositional structure in linear layers by asking: can we identify/synthesize linear transformations from a minimal set of geometric primitives? Using Clifford algebra, we show that linear layers can be expressed as compositions of bivectors -- geometric objects encoding oriented planes -- and introduce a differentiable algorithm that decomposes them into products of rotors. This construction uses only O(log^2 d) parameters, versus O(d^2) required by dense matrices. Applied to the key, query, and value projections in LLM attention layers, our rotor-based layers match the performance of strong baselines such as block-Hadamard and low-rank approximations. Our findings provide an algebraic perspective on how these geometric primitives can compose into higher-level functions within deep models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OrdShap: Feature Position Importance for Sequential Black-Box Models</title>
<link>https://arxiv.org/abs/2507.11855</link>
<guid>https://arxiv.org/abs/2507.11855</guid>
<content:encoded><![CDATA[
arXiv:2507.11855v2 Announce Type: replace 
Abstract: Sequential deep learning models excel in domains with temporal or sequential dependencies, but their complexity necessitates post-hoc feature attribution methods for understanding their predictions. While existing techniques quantify feature importance, they inherently assume fixed feature ordering - conflating the effects of (1) feature values and (2) their positions within input sequences. To address this gap, we introduce OrdShap, a novel attribution method that disentangles these effects by quantifying how a model's predictions change in response to permuting feature position. We establish a game-theoretic connection between OrdShap and Sanchez-Berganti\~nos values, providing a theoretically grounded approach to position-sensitive attribution. Empirical results from health, natural language, and synthetic datasets highlight OrdShap's effectiveness in capturing feature value and feature position attributions, and provide deeper insight into model behavior.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating High Dimensional Concept Space with Metalearning</title>
<link>https://arxiv.org/abs/2508.01948</link>
<guid>https://arxiv.org/abs/2508.01948</guid>
<content:encoded><![CDATA[
arXiv:2508.01948v3 Announce Type: replace 
Abstract: Rapidly learning abstract concepts from limited examples is a hallmark of human intelligence. This work investigates whether gradient-based meta-learning can equip neural networks with inductive biases for efficient few-shot acquisition of discrete concepts. I compare meta-learning methods against a supervised learning baseline on Boolean concepts (logical statements) generated by a probabilistic context-free grammar (PCFG). By systematically varying concept dimensionality (number of features) and recursive compositionality (depth of grammar recursion), I delineate between complexity regimes in which meta-learning robustly improves few-shot concept learning and regimes in which it does not. Meta-learners are much better able to handle compositional complexity than featural complexity. I highlight some reasons for this with a representational analysis of the weights of meta-learners and a loss landscape analysis demonstrating how featural complexity increases the roughness of loss trajectories, allowing curvature-aware optimization to be more effective than first-order methods. I find improvements in out-of-distribution generalization on complex concepts by increasing the number of adaptation steps in meta-SGD, where adaptation acts as a way of encouraging exploration of rougher loss basins. Overall, this work highlights the intricacies of learning compositional versus featural complexity in high dimensional concept spaces and provides a road to understanding the role of 2nd order methods and extended gradient adaptation in few-shot concept learning.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction</title>
<link>https://arxiv.org/abs/2508.03159</link>
<guid>https://arxiv.org/abs/2508.03159</guid>
<content:encoded><![CDATA[
arXiv:2508.03159v2 Announce Type: replace 
Abstract: Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty</title>
<link>https://arxiv.org/abs/2508.05659</link>
<guid>https://arxiv.org/abs/2508.05659</guid>
<content:encoded><![CDATA[
arXiv:2508.05659v3 Announce Type: replace 
Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental research to represent hypothesized causal structures underlying complex problems. However, as qualitative and static representations, CLDs are limited in their ability to support dynamic analysis and inform intervention strategies. We propose Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory system dynamics models (SDMs) in the absence of empirical data. With minimal user input - following a protocol to label variables as stocks, flows or auxiliaries, and constants - D2D leverages the structural information already encoded in CLDs, namely, link existence and polarity, to simulate hypothetical interventions and explore potential leverage points under uncertainty. Results suggest that D2D helps distinguish between high- and low-ranked leverage points. We compare D2D to a data-driven SDM constructed from the same CLD and variable labels. D2D showed greater consistency with the data-driven model compared to static network centrality analysis, while providing uncertainty estimates and guidance for future data collection. The D2D method is implemented in an open-source Python package and a web-based application to support further testing and lower the barrier to dynamic modeling for researchers working with CLDs. We expect that additional validation studies will further establish the approach's utility across a broad range of cases and domains.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast weight programming and linear transformers: from machine learning to neurobiology</title>
<link>https://arxiv.org/abs/2508.08435</link>
<guid>https://arxiv.org/abs/2508.08435</guid>
<content:encoded><![CDATA[
arXiv:2508.08435v2 Announce Type: replace 
Abstract: Recent advances in artificial neural networks for machine learning, and language modeling in particular, have established a family of recurrent neural network (RNN) architectures that, unlike conventional RNNs with vector-form hidden states, use two-dimensional (2D) matrix-form hidden states. Such 2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a neural network whose synaptic weights (called fast weights) dynamically change over time as a function of input observations, and serve as short-term memory storage; corresponding synaptic weight modifications are controlled or programmed by another network (the programmer) whose parameters are trained (e.g., by gradient descent). In this Primer, we review the technical foundations of FWPs, their computational characteristics, and their connections to transformers and state space models. We also discuss connections between FWPs and models of synaptic plasticity in the brain, suggesting a convergence of natural and artificial intelligence.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems</title>
<link>https://arxiv.org/abs/2508.17341</link>
<guid>https://arxiv.org/abs/2508.17341</guid>
<content:encoded><![CDATA[
arXiv:2508.17341v3 Announce Type: replace 
Abstract: The rapid expansion of immersive Metaverse applications introduces complex challenges at the intersection of performance, privacy, and environmental sustainability. Centralized architectures fall short in addressing these demands, often resulting in elevated energy consumption, latency, and privacy concerns. This paper proposes MetaFed, a decentralized federated learning (FL) framework that enables sustainable and intelligent resource orchestration for Metaverse environments. MetaFed integrates (i) multi-agent reinforcement learning for dynamic client selection, (ii) privacy-preserving FL using homomorphic encryption, and (iii) carbon-aware scheduling aligned with renewable energy availability. Evaluations on MNIST and CIFAR-10 using lightweight ResNet architectures demonstrate that MetaFed achieves up to 25% reduction in carbon emissions compared to conventional approaches, while maintaining high accuracy and minimal communication overhead. These results highlight MetaFed as a scalable solution for building environmentally responsible and privacy-compliant Metaverse infrastructures.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation Transport Operators</title>
<link>https://arxiv.org/abs/2508.17540</link>
<guid>https://arxiv.org/abs/2508.17540</guid>
<content:encoded><![CDATA[
arXiv:2508.17540v2 Announce Type: replace 
Abstract: The residual stream mediates communication between transformer decoder layers via linear reads and writes of non-linear computations. While sparse-dictionary learning-based methods locate features in the residual stream, and activation patching methods discover circuits within the model, the mechanism by which features flow through the residual stream remains understudied. Understanding this dynamic can better inform jailbreaking protections, enable early detection of model mistakes, and their correction. In this work, we propose Activation Transport Operators (ATO), linear maps from upstream to downstream residuals $k$ layers later, evaluated in feature space using downstream SAE decoder projections. We empirically demonstrate that these operators can determine whether a feature has been linearly transported from a previous layer or synthesised from non-linear layer computation. We develop the notion of transport efficiency, for which we provide an upper bound, and use it to estimate the size of the residual stream subspace that corresponds to linear transport. We empirically demonstrate the linear transport, report transport efficiency and the size of the residual stream's subspace involved in linear transport. This compute-light (no finetuning, <50 GPU-h) method offers practical tools for safety, debugging, and a clearer picture of where computation in LLMs behaves linearly.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Black Box: Inherently Interpretable Physics-Constrained Machine Learning With Weighted Mixed-Effects for Imbalanced Seismic Data</title>
<link>https://arxiv.org/abs/2508.19031</link>
<guid>https://arxiv.org/abs/2508.19031</guid>
<content:encoded><![CDATA[
arXiv:2508.19031v2 Announce Type: replace 
Abstract: Ground motion models (GMMs) are critical for seismic risk mitigation and infrastructure design. Machine learning (ML) is increasingly applied to GMM development due to expanding strong motion databases. However, existing ML-based GMMs operate as 'black boxes,' creating opacity that undermines confidence in engineering decisions. Moreover, seismic datasets exhibit severe imbalance, with scarce large-magnitude near-field records causing systematic underprediction of critical high-hazard ground motions. Despite these limitations, research addressing both interpretability and data imbalance remains limited. This study develops an inherently interpretable neural network employing independent additive pathways with novel HazBinLoss and concurvity regularization. HazBinLoss integrates physics-constrained weighting with inverse bin count scaling to address underfitting in sparse, high-hazard regions. Concurvity regularization enforces pathway orthogonality, reducing inter-pathway correlation. The model achieves robust performance: mean squared error = 0.6235, mean absolute error = 0.6230, and coefficient of determination = 88.48%. Pathway scaling corroborates established seismological behaviors. Weighted hierarchical Student-t mixed-effects analysis demonstrates unbiased residuals with physically consistent variance partitioning: sigma components range from 0.26-0.38 (inter-event), 0.12-0.41 (inter-region), 0.58-0.71 (intra-event), and 0.68-0.89 (total). The lower inter-event and higher intra-event components have implications for non-ergodic hazard analysis. Predictions exhibit strong agreement with NGA-West2 GMMs across diverse conditions. This interpretable framework advances GMMs, establishing a transparent, physics-consistent foundation for seismic hazard and risk assessment.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GDS Agent for Graph Algorithmic Reasoning</title>
<link>https://arxiv.org/abs/2508.20637</link>
<guid>https://arxiv.org/abs/2508.20637</guid>
<content:encoded><![CDATA[
arXiv:2508.20637v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We introduce new benchmarks that evaluate intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.05193</link>
<guid>https://arxiv.org/abs/2509.05193</guid>
<content:encoded><![CDATA[
arXiv:2509.05193v2 Announce Type: replace 
Abstract: Low-rank structure is a common implicit assumption in many modern reinforcement learning (RL) algorithms. For instance, reward-free and goal-conditioned RL methods often presume that the successor measure admits a low-rank representation. In this work, we challenge this assumption by first remarking that the successor measure itself is not approximately low-rank. Instead, we demonstrate that a low-rank structure naturally emerges in the shifted successor measure, which captures the system dynamics after bypassing a few initial transitions. We provide finite-sample performance guarantees for the entry-wise estimation of a low-rank approximation of the shifted successor measure from sampled entries. Our analysis reveals that both the approximation and estimation errors are primarily governed by a newly introduced quantitity: the spectral recoverability of the corresponding matrix. To bound this parameter, we derive a new class of functional inequalities for Markov chains that we call Type II Poincar\'e inequalities and from which we can quantify the amount of shift needed for effective low-rank approximation and estimation. This analysis shows in particular that the required shift depends on decay of the high-order singular values of the shifted successor measure and is hence typically small in practice. Additionally, we establish a connection between the necessary shift and the local mixing properties of the underlying dynamical system, which provides a natural way of selecting the shift. Finally, we validate our theoretical findings with experiments, and demonstrate that shifting the successor measure indeed leads to improved performance in goal-conditioned RL.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs</title>
<link>https://arxiv.org/abs/2509.10594</link>
<guid>https://arxiv.org/abs/2509.10594</guid>
<content:encoded><![CDATA[
arXiv:2509.10594v2 Announce Type: replace 
Abstract: Artificial Intelligence (AI) and Large Language Models (LLMs) are revolutionizing today's business practices; however, their adoption within small and medium-sized enterprises (SMEs) raises serious trust, ethical, and technical issues. In this perspective paper, we introduce a structured, multi-phased framework, "SME-TEAM" for the secure and responsible use of these technologies in SMEs. Based on a conceptual structure of four key pillars, i.e., Data, Algorithms, Human Oversight, and Model Architecture, SME-TEAM bridges theoretical ethical principles with operational practice, enhancing AI capabilities across a wide range of applications in SMEs. Ultimately, this paper provides a structured roadmap for the adoption of these emerging technologies, positioning trust and ethics as a driving force for resilience, competitiveness, and sustainable innovation within the area of business analytics and SMEs.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few</title>
<link>https://arxiv.org/abs/2509.16875</link>
<guid>https://arxiv.org/abs/2509.16875</guid>
<content:encoded><![CDATA[
arXiv:2509.16875v3 Announce Type: replace 
Abstract: Attention mechanisms have achieved significant empirical success in multiple fields, but their underlying optimization objectives remain unclear yet. Moreover, the quadratic complexity of self-attention has become increasingly prohibitive. Although interpretability and efficiency are two mutually reinforcing pursuits, prior work typically investigates them separately. In this paper, we propose a unified optimization objective that derives inherently interpretable and efficient attention mechanisms through algorithm unrolling. Precisely, we construct a gradient step of the proposed objective with a set of forward-pass operations of our \emph{Contract-and-Broadcast Self-Attention} (CBSA), which compresses input tokens towards low-dimensional structures by contracting a few representatives of them. This novel mechanism can not only scale linearly by fixing the number of representatives, but also covers the instantiations of varied attention mechanisms when using different sets of representatives. We conduct extensive experiments to demonstrate comparable performance and superior advantages over black-box attention mechanisms on visual tasks. Our work sheds light on the integration of interpretability and efficiency, as well as the unified formula of attention mechanisms.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Discovery of One-Parameter Subgroups of Lie Groups: Compact and Non-Compact Cases of $\mathbf{SO(n)}$ and $\mathbf{SL(n)}$</title>
<link>https://arxiv.org/abs/2509.22219</link>
<guid>https://arxiv.org/abs/2509.22219</guid>
<content:encoded><![CDATA[
arXiv:2509.22219v3 Announce Type: replace 
Abstract: We introduce a novel framework for the automatic discovery of one-parameter subgroups ($H_{\gamma}$) of $SO(3)$ and, more generally, $SO(n)$. One-parameter subgroups of $SO(n)$ are crucial in a wide range of applications, including robotics, quantum mechanics, and molecular structure analysis. Our method utilizes the standard Jordan form of skew-symmetric matrices, which define the Lie algebra of $SO(n)$, to establish a canonical form for orbits under the action of $H_{\gamma}$. This canonical form is then employed to derive a standardized representation for $H_{\gamma}$-invariant functions. By learning the appropriate parameters, the framework uncovers the underlying one-parameter subgroup $H_{\gamma}$. The effectiveness of the proposed approach is demonstrated through tasks such as double pendulum modeling, moment of inertia prediction, top quark tagging and invariant polynomial regression, where it successfully recovers meaningful subgroup structure and produces interpretable, symmetry-aware representations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proposing a Framework for Machine Learning Adoption on Legacy Systems</title>
<link>https://arxiv.org/abs/2509.24224</link>
<guid>https://arxiv.org/abs/2509.24224</guid>
<content:encoded><![CDATA[
arXiv:2509.24224v2 Announce Type: replace 
Abstract: The integration of machine learning (ML) is critical for industrial competitiveness, yet its adoption is frequently stalled by the prohibitive costs and operational disruptions of upgrading legacy systems. The financial and logistical overhead required to support the full ML lifecycle presents a formidable barrier to widespread implementation, particularly for small and medium-sized enterprises. This paper introduces a pragmatic, API-based framework designed to overcome these challenges by strategically decoupling the ML model lifecycle from the production environment. Our solution delivers the analytical power of ML to domain experts through a lightweight, browser-based interface, eliminating the need for local hardware upgrades and ensuring model maintenance can occur with zero production downtime. This human-in-the-loop approach empowers experts with interactive control over model parameters, fostering trust and facilitating seamless integration into existing workflows. By mitigating the primary financial and operational risks, this framework offers a scalable and accessible pathway to enhance production quality and safety, thereby strengthening the competitive advantage of the manufacturing sector.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG</title>
<link>https://arxiv.org/abs/2509.25804</link>
<guid>https://arxiv.org/abs/2509.25804</guid>
<content:encoded><![CDATA[
arXiv:2509.25804v2 Announce Type: replace 
Abstract: This study aims to develop and evaluate an ensemble machine learning-based framework for the automatic detection of Wide QRS Complex Tachycardia (WCT) from ECG signals, emphasizing diagnostic accuracy and interpretability using Explainable AI. The proposed system integrates ensemble learning techniques, i.e., an optimized Random Forest known as CardioForest, and models like XGBoost and LightGBM. The models were trained and tested on ECG data from the publicly available MIMIC-IV dataset. The testing was carried out with the assistance of accuracy, balanced accuracy, precision, recall, F1 score, ROC-AUC, and error rate (RMSE, MAE) measures. In addition, SHAP (SHapley Additive exPlanations) was used to ascertain model explainability and clinical relevance. The CardioForest model performed best on all metrics, achieving a test accuracy of 95.19%, a balanced accuracy of 88.76%, a precision of 95.26%, a recall of 78.42%, and an ROC-AUC of 0.8886. SHAP analysis confirmed the model's ability to rank the most relevant ECG features, such as QRS duration, in accordance with clinical intuitions, thereby fostering trust and usability in clinical practice. The findings recognize CardioForest as an extremely dependable and interpretable WCT detection model. Being able to offer accurate predictions and transparency through explainability makes it a valuable tool to help cardiologists make timely and well-informed diagnoses, especially for high-stakes and emergency scenarios.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Optimal Large Diffusion Language Models</title>
<link>https://arxiv.org/abs/2510.03280</link>
<guid>https://arxiv.org/abs/2510.03280</guid>
<content:encoded><![CDATA[
arXiv:2510.03280v2 Announce Type: replace 
Abstract: We introduce Quokka, the first systematic scaling law for diffusion language models (DLMs), encompassing both compute-constrained and data-constrained regimes, and studying the key modeling and optimization designs. Quokka is a good friend of Chinchilla and provides wider scopes. We hope the results would bring short-term practical guidance in DLMs training and long-term inspirations for the whole AI community.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aegis: A Correlation-Based Data Masking Advisor for Data Sharing Ecosystems</title>
<link>https://arxiv.org/abs/2510.10810</link>
<guid>https://arxiv.org/abs/2510.10810</guid>
<content:encoded><![CDATA[
arXiv:2510.10810v2 Announce Type: replace 
Abstract: Data sharing ecosystems connect providers, consumers, and intermediaries to facilitate the exchange and use of data for a wide range of downstream tasks. In sensitive domains such as healthcare, privacy is enforced as a hard constraint, any shared data must satisfy a minimum privacy threshold. However, among all masking configurations that meet this requirement, the utility of the masked data can vary significantly, posing a key challenge: how to efficiently select the optimal configuration that preserves maximum utility. This paper presents Aegis, a middleware framework that selects optimal masking configurations for machine learning datasets with features and class labels. Aegis incorporates a utility optimizer that minimizes predictive utility deviation, quantifying shifts in feature label correlations due to masking. Our framework leverages limited data summaries (such as 1D histograms) or none to estimate the feature label joint distribution, making it suitable for scenarios where raw data is inaccessible due to privacy restrictions. To achieve this, we propose a joint distribution estimator based on iterative proportional fitting, which allows supporting various feature label correlation quantification methods such as mutual information, chi square, or g3. Our experimental evaluation of real world datasets shows that Aegis identifies optimal masking configurations over an order of magnitude faster, while the resulting masked datasets achieve predictive performance on downstream ML tasks on par with baseline approaches and complements privacy anonymization data masking techniques.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification</title>
<link>https://arxiv.org/abs/2510.12214</link>
<guid>https://arxiv.org/abs/2510.12214</guid>
<content:encoded><![CDATA[
arXiv:2510.12214v2 Announce Type: replace 
Abstract: Early Time Series Classification (ETSC) is critical in time-sensitive medical applications such as sepsis, yet it presents an inherent trade-off between accuracy and earliness. This trade-off arises from two core challenges: 1) models should effectively model inherently weak and noisy early-stage snippets, and 2) they should resolve the complex, dual requirement of simultaneously capturing local, subject-specific variations and overarching global temporal patterns. Existing methods struggle to overcome these underlying challenges, often forcing a severe compromise: sacrificing accuracy to achieve earliness, or vice-versa. We propose \textbf{DE3S}, a \textbf{D}ual-\textbf{E}nhanced \textbf{S}oft-\textbf{S}parse \textbf{S}equence Learning framework, which systematically solves these challenges. A dual enhancement mechanism is proposed to enhance the modeling of weak, early signals. Then, an attention-based patch module is introduced to preserve discriminative information while reducing noise and complexity. A dual-path fusion architecture is designed, using a sparse mixture of experts to model local, subject-specific variations. A multi-scale inception module is also employed to capture global dependencies. Experiments on six real-world medical datasets show the competitive performance of DE3S, particularly in early prediction windows. Ablation studies confirm the effectiveness of each component in addressing its targeted challenge. The source code is available \href{https://github.com/kuxit/DE3S}{\textbf{here}}.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finding geodesics with the Deep Ritz method</title>
<link>https://arxiv.org/abs/2510.15177</link>
<guid>https://arxiv.org/abs/2510.15177</guid>
<content:encoded><![CDATA[
arXiv:2510.15177v2 Announce Type: replace 
Abstract: Geodesic problems involve computing trajectories between prescribed initial and final states to minimize a user-defined measure of distance, cost, or energy. They arise throughout physics and engineering -- for instance, in determining optimal paths through complex environments, modeling light propagation in refractive media, and the study of spacetime trajectories in control theory and general relativity. Despite their ubiquity, the scientific machine learning (SciML) community has given relatively little attention to investigating its methods in the context of these problems. In this work, we argue that given their simple geometry, variational structure, and natural nonlinearity, geodesic problems are particularly well-suited for the Deep Ritz method. We substantiate this claim with four numerical examples drawn from path planning, optics, solid mechanics, and generative modeling. Our goal is not to provide an exhaustive study of geodesic problems, but rather to identify a promising application of the Deep Ritz method and a fruitful direction for future SciML research.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems</title>
<link>https://arxiv.org/abs/2510.21861</link>
<guid>https://arxiv.org/abs/2510.21861</guid>
<content:encoded><![CDATA[
arXiv:2510.21861v2 Announce Type: replace 
Abstract: Large language models are often described as capable of reflective reasoning, yet recursive self-evaluation without external feedback frequently yields reformulation rather than progress. We test this prediction in a cross-provider study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini, Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families (arithmetic, code, explanation, reflection), each iterated ten times under two conditions: ungrounded self-critique and a minimal grounding intervention (a single verification step at iteration three). Mean informational change (delta I, measured via normalized edit distance) declined by 55% from early (0.193) to late (0.087) iterations in ungrounded runs, with consistent patterns across all three providers. Grounded runs showed a +28% rebound in informational change immediately after the intervention and sustained non-zero variance thereafter. Complementary measures-n-gram novelty, embedding drift, and character-level entropy-converged on the same pattern: reflection without contact tends toward informational closure. We interpret this as evidence for a structural limit on self-correction in generative reasoning: without an exchange of information with an independent verifier or environment, recursive inference approaches an attractor state of epistemic stasis. Minimal grounding functions as dissipative coupling, reintroducing informational flux. The cross-architecture consistency suggests the mirror loop arises from shared autoregressive training objectives rather than provider-specific alignment schemes. The results delineate when reflection is performative rather than epistemic and motivate design principles for grounded, cooperative reasoning. Materials and code are publicly available.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression (Technical Report)</title>
<link>https://arxiv.org/abs/2510.23632</link>
<guid>https://arxiv.org/abs/2510.23632</guid>
<content:encoded><![CDATA[
arXiv:2510.23632v2 Announce Type: replace 
Abstract: The rapid growth of high-resolution scientific simulations and observation systems is generating massive spatiotemporal datasets, making efficient, error-bounded compression increasingly important. Meanwhile, decoder-only large language models (LLMs) have demonstrated remarkable capabilities in modeling complex sequential data. In this paper, we propose LLMCOMP, a novel lossy compression paradigm that leverages decoder-only large LLMs to model scientific data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via Z-order curves to preserve locality, and applies coverage-guided sampling to enhance training efficiency. An autoregressive transformer is then trained with spatial-temporal embeddings to model token transitions. During compression, the model performs top-k prediction, storing only rank indices and fallback corrections to ensure strict error bounds. Experiments on multiple reanalysis datasets show that LLMCOMP consistently outperforms state-of-the-art compressors, achieving up to 30% higher compression ratios under strict error bounds. These results highlight the potential of LLMs as general-purpose compressors for high-fidelity scientific data.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Measuring Localization of Shortcuts in Deep Networks</title>
<link>https://arxiv.org/abs/2510.26560</link>
<guid>https://arxiv.org/abs/2510.26560</guid>
<content:encoded><![CDATA[
arXiv:2510.26560v2 Announce Type: replace 
Abstract: Shortcuts, spurious rules that perform well during training but fail to generalize, present a major challenge to the reliability of deep networks (Geirhos et al., 2020). However, the impact of shortcuts on feature representations remains understudied, obstructing the design of principled shortcut-mitigation methods. To overcome this limitation, we investigate the layer-wise localization of shortcuts in deep models. Our novel experiment design quantifies the layer-wise contribution to accuracy degradation caused by a shortcut-inducing skew by counterfactual training on clean and skewed datasets. We employ our design to study shortcuts on CIFAR-10, Waterbirds, and CelebA datasets across VGG, ResNet, DeiT, and ConvNeXt architectures. We find that shortcut learning is not localized in specific layers but distributed throughout the network. Different network parts play different roles in this process: shallow layers predominantly encode spurious features, while deeper layers predominantly forget core features that are predictive on clean data. We also analyze the differences in localization and describe its principal axes of variation. Finally, our analysis of layer-wise shortcut-mitigation strategies suggests the hardness of designing general methods, supporting dataset- and architecture-specific approaches instead.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variable Selection in Maximum Mean Discrepancy for Interpretable Distribution Comparison</title>
<link>https://arxiv.org/abs/2311.01537</link>
<guid>https://arxiv.org/abs/2311.01537</guid>
<content:encoded><![CDATA[
arXiv:2311.01537v2 Announce Type: replace-cross 
Abstract: We study two-sample variable selection: identifying variables that discriminate between the distributions of two sets of data vectors. Such variables help scientists understand the mechanisms behind dataset discrepancies. Although domain-specific methods exist (e.g., in medical imaging, genetics, and computational social science), a general framework remains underdeveloped. We make two separate contributions. (i) We introduce a mathematical notion of the discriminating set of variables: the largest subset containing no variables whose marginals are identical across the two distributions and independent of the remaining variables. We prove this set is uniquely defined and establish further properties, making it a suitable ground truth for theory and evaluation. (ii) We propose two methods for two-sample variable selection that assign weights to variables and optimise them to maximise the power of a kernel two-sample test while enforcing sparsity to downweight redundant variables. To select the regularisation parameter - unknown in practice, as it controls the number of selected variables - we develop two data-driven procedures to balance recall and precision. Synthetic experiments show improved performance over baselines, and we illustrate the approach on two applications using datasets from water-pipe and traffic networks.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Physics: Using AI Libraries to Develop Physics-Based Solvers for Incompressible Computational Fluid Dynamics</title>
<link>https://arxiv.org/abs/2402.17913</link>
<guid>https://arxiv.org/abs/2402.17913</guid>
<content:encoded><![CDATA[
arXiv:2402.17913v2 Announce Type: replace-cross 
Abstract: Numerical discretisations of partial differential equations (PDEs) can be written as discrete convolutions, which, themselves, are a key tool in AI libraries and used in convolutional neural networks (CNNs). We therefore propose to implement numerical discretisations as convolutional layers of a neural network, where the weights or filters are determined analytically rather than by training. Furthermore, we demonstrate that these systems can be solved entirely by functions in AI libraries, either by using Jacobi iteration or multigrid methods, the latter realised through a U-Net architecture. Some advantages of the Neural Physics approach are that (1) the methods are platform agnostic; (2) the resulting solvers are fully differentiable, ideal for optimisation tasks; and (3) writing CFD solvers as (untrained) neural networks means that they can be seamlessly integrated with trained neural networks to form hybrid models. We demonstrate the proposed approach on a number of test cases of increasing complexity from advection-diffusion problems, the non-linear Burgers equation to the Navier-Stokes equations. We validate the approach by comparing our results with solutions obtained from traditionally written code and common benchmarks from the literature. We show that the proposed methodology can solve all these problems using repurposed AI libraries in an efficient way, without training, and presents a new avenue to explore in the development of methods to solve PDEs with implicit methods.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contraction of Private Quantum Channels and Private Quantum Hypothesis Testing</title>
<link>https://arxiv.org/abs/2406.18651</link>
<guid>https://arxiv.org/abs/2406.18651</guid>
<content:encoded><![CDATA[
arXiv:2406.18651v3 Announce Type: replace-cross 
Abstract: A quantum generalized divergence by definition satisfies the data-processing inequality; as such, the relative decrease in such a divergence under the action of a quantum channel is at most one. This relative decrease is formally known as the contraction coefficient of the channel and the divergence. Interestingly, there exist combinations of channels and divergences for which the contraction coefficient is strictly less than one. Furthermore, understanding the contraction coefficient is fundamental for the study of statistical tasks under privacy constraints. To this end, here we establish upper bounds on contraction coefficients for the hockey-stick divergence under privacy constraints, where privacy is quantified with respect to the quantum local differential privacy (QLDP) framework, and we fully characterize the contraction coefficient for the trace distance under privacy constraints. With the machinery developed, we also determine an upper bound on the contraction of both the Bures distance and quantum relative entropy relative to the normalized trace distance, under QLDP constraints. Next, we apply our findings to establish bounds on the sample complexity of quantum hypothesis testing under privacy constraints. Furthermore, we study various scenarios in which the sample complexity bounds are tight, while providing order-optimal quantum channels that achieve those bounds. Lastly, we show how private quantum channels provide fairness and Holevo information stability in quantum learning settings.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disentanglement with Factor Quantized Variational Autoencoders</title>
<link>https://arxiv.org/abs/2409.14851</link>
<guid>https://arxiv.org/abs/2409.14851</guid>
<content:encoded><![CDATA[
arXiv:2409.14851v3 Announce Type: replace-cross 
Abstract: Disentangled representation learning aims to represent the underlying generative factors of a dataset in a latent representation independently of one another. In our work, we propose a discrete variational autoencoder (VAE) based model where the ground truth information about the generative factors are not provided to the model. We demonstrate the advantages of learning discrete representations over learning continuous representations in facilitating disentanglement. Furthermore, we propose incorporating an inductive bias into the model to further enhance disentanglement. Precisely, we propose scalar quantization of the latent variables in a latent representation with scalar values from a global codebook, and we add a total correlation term to the optimization as an inductive bias. Our method called FactorQVAE combines optimization based disentanglement approaches with discrete representation learning, and it outperforms the former disentanglement methods in terms of two disentanglement metrics (DCI and InfoMEC) while improving the reconstruction performance. Our code can be found at https://github.com/ituvisionlab/FactorQVAE.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Sampling for Scalable and Expressive Graph Neural Networks on Homophilic Graphs</title>
<link>https://arxiv.org/abs/2410.16593</link>
<guid>https://arxiv.org/abs/2410.16593</guid>
<content:encoded><![CDATA[
arXiv:2410.16593v5 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) excel in many graph machine learning tasks but face challenges when scaling to large networks. GNN transferability allows training on smaller graphs and applying the model to larger ones, but existing methods often rely on random subsampling, leading to disconnected subgraphs and reduced model expressivity. We propose a novel graph sampling algorithm that leverages feature homophily to preserve graph structure. By minimizing the trace of the data correlation matrix, our method better preserves the graph Laplacian trace -- a proxy for the graph connectivity -- than random sampling, while achieving lower complexity than spectral methods. Experiments on citation networks show improved performance in preserving Laplacian trace and GNN transferability compared to random sampling.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alleviating Hyperparameter-Tuning Burden in SVM Classifiers for Pulmonary Nodules Diagnosis with Multi-Task Bayesian Optimization</title>
<link>https://arxiv.org/abs/2411.06184</link>
<guid>https://arxiv.org/abs/2411.06184</guid>
<content:encoded><![CDATA[
arXiv:2411.06184v2 Announce Type: replace-cross 
Abstract: In the field of non-invasive medical imaging, radiomic features are utilized to measure tumor characteristics. However, these features can be affected by the techniques used to discretize the images, ultimately impacting the accuracy of diagnosis. To investigate the influence of various image discretization methods on diagnosis, it is common practice to evaluate multiple discretization strategies individually. This approach often leads to redundant and time-consuming tasks such as training predictive models and fine-tuning hyperparameters separately. This study examines the feasibility of employing multi-task Bayesian optimization to accelerate the hyperparameters search for classifying benign and malignant pulmonary nodules using RBF SVM. Our findings suggest that multi-task Bayesian optimization significantly accelerates the search for hyperparameters in comparison to a single-task approach. To the best of our knowledge, this is the first investigation to utilize multi-task Bayesian optimization in a critical medical context.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aspen Open Jets: Unlocking LHC Data for Foundation Models in Particle Physics</title>
<link>https://arxiv.org/abs/2412.10504</link>
<guid>https://arxiv.org/abs/2412.10504</guid>
<content:encoded><![CDATA[
arXiv:2412.10504v2 Announce Type: replace-cross 
Abstract: Foundation models are deep learning models pre-trained on large amounts of data which are capable of generalizing to multiple datasets and/or downstream tasks. This work demonstrates how data collected by the CMS experiment at the Large Hadron Collider can be useful in pre-training foundation models for HEP. Specifically, we introduce the AspenOpenJets dataset, consisting of approximately 178M high $p_T$ jets derived from CMS 2016 Open Data. We show how pre-training the OmniJet-$\alpha$ foundation model on AspenOpenJets improves performance on generative tasks with significant domain shift: generating boosted top and QCD jets from the simulated JetClass dataset. In addition to demonstrating the power of pre-training of a jet-based foundation model on actual proton-proton collision data, we provide the ML-ready derived AspenOpenJets dataset for further public use.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Learning of Pure States is as Hard as Mixed States</title>
<link>https://arxiv.org/abs/2502.00823</link>
<guid>https://arxiv.org/abs/2502.00823</guid>
<content:encoded><![CDATA[
arXiv:2502.00823v3 Announce Type: replace-cross 
Abstract: Quantum state tomography, the task of learning an unknown quantum state, is a fundamental problem in quantum information. In standard settings, the complexity of this problem depends significantly on the type of quantum state that one is trying to learn, with pure states being substantially easier to learn than general mixed states. A natural question is whether this separation holds for any quantum state learning setting. In this work, we consider the online learning framework and prove the surprising result that learning pure states in this setting is as hard as learning mixed states. More specifically, we show that both classes share almost the same sequential fat-shattering dimension, leading to identical regret scaling. We also generalize previous results on full quantum state tomography in the online setting to (i) the $\epsilon$-realizable setting and (ii) learning the density matrix only partially, using smoothed analysis.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Haystack to Needle: Label Space Reduction for Zero-shot Classification</title>
<link>https://arxiv.org/abs/2502.08436</link>
<guid>https://arxiv.org/abs/2502.08436</guid>
<content:encoded><![CDATA[
arXiv:2502.08436v2 Announce Type: replace-cross 
Abstract: We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Probabilistic Air-Sea Flux Parameterization</title>
<link>https://arxiv.org/abs/2503.03990</link>
<guid>https://arxiv.org/abs/2503.03990</guid>
<content:encoded><![CDATA[
arXiv:2503.03990v2 Announce Type: replace-cross 
Abstract: Accurately quantifying air-sea fluxes is important for understanding air-sea interactions and improving coupled weather and climate systems. This study introduces a probabilistic framework to represent the highly variable nature of air-sea fluxes, which is missing in deterministic bulk algorithms. Assuming Gaussian distributions conditioned on the input variables, we use artificial neural networks and eddy-covariance measurement data to estimate the mean and variance by minimizing negative log-likelihood loss. The trained neural networks provide alternative mean flux estimates to existing bulk algorithms, and quantify the uncertainty around the mean estimates. Stochastic parameterization of air-sea turbulent fluxes can be constructed by sampling from the predicted distributions. Tests in a single-column forced upper-ocean model suggest that changes in flux algorithms influence sea surface temperature and mixed layer depth seasonally. The ensemble spread in stochastic runs is most pronounced during spring restratification.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning Large Language Models</title>
<link>https://arxiv.org/abs/2503.07329</link>
<guid>https://arxiv.org/abs/2503.07329</guid>
<content:encoded><![CDATA[
arXiv:2503.07329v2 Announce Type: replace-cross 
Abstract: The impact of random seeds in fine-tuning large language models (LLMs) has been largely overlooked despite its potential influence on model performance.In this study, we systematically evaluate the effects of random seeds on LLMs using the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact through traditional metrics like accuracy and F1, calculating their mean and variance to quantify performance fluctuations. To capture the micro-level effects, we introduce a novel metric, consistency, measuring the stability of individual predictions across runs. Our experiments reveal significant variance at both macro and micro levels, underscoring the need for careful consideration of random seeds in fine-tuning and evaluation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents</title>
<link>https://arxiv.org/abs/2503.16711</link>
<guid>https://arxiv.org/abs/2503.16711</guid>
<content:encoded><![CDATA[
arXiv:2503.16711v2 Announce Type: replace-cross 
Abstract: Autonomous agents that rely purely on perception to make real-time control decisions require efficient and robust architectures. In this work, we demonstrate that augmenting RGB input with depth information significantly enhances our agents' ability to predict steering commands compared to using RGB alone. We benchmark lightweight recurrent controllers that leverage the fused RGB-D features for sequential decision-making. To train our models, we collect high-quality data using a small-scale autonomous car controlled by an expert driver via a physical steering wheel, capturing varying levels of steering difficulty. Our models were successfully deployed on real hardware and inherently avoided dynamic and static obstacles, under out-of-distribution conditions. Specifically, our findings reveal that the early fusion of depth data results in a highly robust controller, which remains effective even with frame drops and increased noise levels, without compromising the network's focus on the task.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Polynomial-Time Algorithm for Variational Inequalities under the Minty Condition</title>
<link>https://arxiv.org/abs/2504.03432</link>
<guid>https://arxiv.org/abs/2504.03432</guid>
<content:encoded><![CDATA[
arXiv:2504.03432v2 Announce Type: replace-cross 
Abstract: Solving variational inequalities (SVIs) is a foundational problem at the heart of optimization. However, this expressivity comes at the cost of computational hardness. As a result, most research has focused on carving out specific subclasses that elude those intractability barriers. A classical property that goes back to the 1960s is the Minty condition, which postulates that the Minty VI (MVI) problem admits a solution.
  In this paper, we establish the first polynomial-time algorithm -- that is, with complexity growing polynomially in the dimension $d$ and $\log(1/\epsilon)$ -- for solving $\epsilon$-SVIs for Lipschitz continuous mappings under the Minty condition. Prior approaches either incurred an exponentially worse dependence on $1/\epsilon$ or made restrictive assumptions. To do so, we introduce a new variant of the ellipsoid algorithm whereby separating hyperplanes are obtained after taking a gradient descent step from the center of the ellipsoid. It succeeds even though the set of SVIs can be nonconvex and not fully dimensional. Moreover, when our algorithm is applied to an instance with no MVI solution and fails to identify an SVI solution, it produces a succinct certificate of MVI infeasibility. We also show that deciding whether the Minty condition holds is $\mathsf{coNP}$-complete, thereby establishing that the disjunction of those two problems is polynomial-time solvable even though each problem is individually intractable.
  We provide several extensions and new applications of our main results. Most notably, we obtain the first polynomial-time algorithms for i) globally minimizing a (potentially nonsmooth) quasar-convex function, and ii) computing Nash equilibria in multi-player harmonic games. Finally, in two-player general-sum concave games, we give the first polynomial-time algorithm that outputs either a Nash equilibrium or a strict coarse correlated equilibrium.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tight Regret Bounds for Fixed-Price Bilateral Trade</title>
<link>https://arxiv.org/abs/2504.04349</link>
<guid>https://arxiv.org/abs/2504.04349</guid>
<content:encoded><![CDATA[
arXiv:2504.04349v2 Announce Type: replace-cross 
Abstract: We examine fixed-price mechanisms in bilateral trade through the lens of regret minimization. Our main results are twofold. (i) For independent values, a near-optimal $\widetilde{\Theta}(T^{2/3})$ tight bound for $\textsf{Global Budget Balance}$ fixed-price mechanisms with two-bit/one-bit feedback. (ii) For correlated/adversarial values, a near-optimal $\Omega(T^{3/4})$ lower bound for $\textsf{Global Budget Balance}$ fixed-price mechanisms with two-bit/one-bit feedback, which improves the best known $\Omega(T^{5/7})$ lower bound obtained in the work [BCCF24] and, up to polylogarithmic factors, matches the $\widetilde{\mathcal{O}}(T^{3 / 4})$ upper bound obtained in the same work. Our work in combination with the previous works [CCCFL24mor, CCCFL24jmlr, AFF24, BCCF24] (essentially) gives a thorough understanding of regret minimization for fixed-price bilateral trade.
  En route, we have developed two technical ingredients that might be of independent interest: (i) A novel algorithmic paradigm, called $\textit{{fractal elimination}}$, to address one-bit feedback and independent values. (ii) A new $\textit{lower-bound construction}$ with novel proof techniques, to address the $\textsf{Global Budget Balance}$ constraint and correlated values.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A data-driven framework for team selection in Fantasy Premier League</title>
<link>https://arxiv.org/abs/2505.02170</link>
<guid>https://arxiv.org/abs/2505.02170</guid>
<content:encoded><![CDATA[
arXiv:2505.02170v2 Announce Type: replace-cross 
Abstract: Fantasy football is a billion-dollar industry with millions of participants. Under a fixed budget, managers select squads to maximize future Fantasy Premier League (FPL) points. This study formulates lineup selection as data-driven optimization and develops deterministic and robust mixed-integer linear programs that choose the starting eleven, bench, and captain under budget, formation, and club-quota constraints (maximum three players per club). The objective is parameterized by a hybrid scoring metric that combines realized FPL points with predictions from a linear regression model trained on match-performance features identified using exploratory data analysis techniques. The study benchmarks alternative objectives and cost estimators, including simple and recency-weighted averages, exponential smoothing, autoregressive integrated moving average (ARIMA), and Monte Carlo simulation. Experiments on the 2023/24 Premier League season show that ARIMA with a constrained budget and a rolling window yields the most consistent out-of-sample performance; weighted averages and Monte Carlo are also competitive. Robust variants improve some objectives but are not uniformly superior. The framework provides transparent decision support for fantasy roster construction and extends to FPL chips, multi-week rolling-horizon transfer planning, and week-by-week dynamic captaincy.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning Warm Starts for Trajectory Optimization on the International Space Station</title>
<link>https://arxiv.org/abs/2505.05588</link>
<guid>https://arxiv.org/abs/2505.05588</guid>
<content:encoded><![CDATA[
arXiv:2505.05588v3 Announce Type: replace-cross 
Abstract: Trajectory optimization is a cornerstone of modern robot autonomy, enabling systems to compute trajectories and controls in real-time while respecting safety and physical constraints. However, it has seen limited usage in spaceflight applications due to its heavy computational demands that exceed the capability of most flight computers. In this work, we provide results on the first in-space demonstration of using machine learning-based warm starts for accelerating trajectory optimization for the Astrobee free-flying robot onboard the International Space Station (ISS). We formulate a data-driven optimal control approach that trains a neural network to learn the structure of the trajectory generation problem being solved using sequential convex programming (SCP). Onboard, this trained neural network predicts solutions for the trajectory generation problem and relies on using the SCP solver to enforce safety constraints for the system. Our trained network reduces the number of solver iterations required for convergence in cases including rotational dynamics by 60% and in cases with obstacles drawn from the training distribution of the warm start model by 50%. This work represents a significant milestone in the use of learning-based control for spaceflight applications and a stepping stone for future advances in the use of machine learning for autonomous guidance, navigation, & control.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Semantics Augmented Few-Shot Relational Learning</title>
<link>https://arxiv.org/abs/2505.05684</link>
<guid>https://arxiv.org/abs/2505.05684</guid>
<content:encoded><![CDATA[
arXiv:2505.05684v4 Announce Type: replace-cross 
Abstract: Few-shot relational learning on knowledge graph (KGs) aims to perform reasoning over relations with only a few training examples. While current methods have focused primarily on leveraging specific relational information, rich semantics inherent in KGs have been largely overlooked. To bridge this gap, we propose PromptMeta, a novel prompted meta-learning framework that seamlessly integrates meta-semantics with relational information for few-shot relational learning. PromptMeta introduces two core innovations: (1) a Meta-Semantic Prompt (MSP) pool that learns and consolidates high-level meta-semantics shared across tasks, enabling effective knowledge transfer and adaptation to newly emerging relations; and (2) a learnable fusion mechanism that dynamically combines meta-semantics with task-specific relational information tailored to different few-shot tasks. Both components are optimized jointly with model parameters within a meta-learning framework. Extensive experiments and analyses on two real-world KG benchmarks validate the effectiveness of PromptMeta in adapting to new relations with limited supervision.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Traversal Verification for Speculative Tree Decoding</title>
<link>https://arxiv.org/abs/2505.12398</link>
<guid>https://arxiv.org/abs/2505.12398</guid>
<content:encoded><![CDATA[
arXiv:2505.12398v2 Announce Type: replace-cross 
Abstract: Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results across different large language models and multiple tasks show that our method consistently improves acceptance length and throughput over existing methods.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models Miss the Multi-Agent Mark</title>
<link>https://arxiv.org/abs/2505.21298</link>
<guid>https://arxiv.org/abs/2505.21298</guid>
<content:encoded><![CDATA[
arXiv:2505.21298v3 Announce Type: replace-cross 
Abstract: Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs) has led to an increase in frameworks leveraging multiple LLMs to tackle complex tasks. However, much of this literature appropriates the terminology of MAS without engaging with its foundational principles. In this position paper, we highlight critical discrepancies between MAS theory and current MAS LLMs implementations, focusing on four key areas: the social aspect of agency, environment design, coordination and communication protocols, and measuring emergent behaviours. Our position is that many MAS LLMs lack multi-agent characteristics such as autonomy, social interaction, and structured environments, and often rely on oversimplified, LLM-centric architectures. The field may slow down and lose traction by revisiting problems the MAS literature has already addressed. Therefore, we systematically analyse this issue and outline associated research opportunities; we advocate for better integrating established MAS concepts and more precise terminology to avoid mischaracterisation and missed opportunities.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing</title>
<link>https://arxiv.org/abs/2505.21600</link>
<guid>https://arxiv.org/abs/2505.21600</guid>
<content:encoded><![CDATA[
arXiv:2505.21600v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving</title>
<link>https://arxiv.org/abs/2506.09397</link>
<guid>https://arxiv.org/abs/2506.09397</guid>
<content:encoded><![CDATA[
arXiv:2506.09397v5 Announce Type: replace-cross 
Abstract: The growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities. Existing strategies, such as aggressive quantization, pruning, or remote inference, trade accuracy for efficiency or lead to substantial cost burdens. This position paper introduces a new framework that leverages speculative decoding, previously viewed primarily as a decoding acceleration technique for autoregressive generation of LLMs, as a promising approach specifically adapted for edge computing by orchestrating computation across heterogeneous devices. We propose \acronym, a framework that allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single, shared edge server verifies the tokens utilizing a more precise target model. To further increase the efficiency of verification, the edge server batch the diverse verification requests from devices. This approach supports device heterogeneity and reduces server-side memory footprint by sharing the same upstream target model across multiple devices. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits: 2.2 more system throughput, 2.8 more system capacity, and better cost efficiency, all without sacrificing model accuracy.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for Scalable and Robust Quantum Machine Learning</title>
<link>https://arxiv.org/abs/2506.10275</link>
<guid>https://arxiv.org/abs/2506.10275</guid>
<content:encoded><![CDATA[
arXiv:2506.10275v2 Announce Type: replace-cross 
Abstract: Variational quantum circuits (VQCs) hold promise for quantum machine learning but face challenges in expressivity, trainability, and noise resilience. We propose VQC-MLPNet, a hybrid architecture where a VQC generates the first-layer weights of a classical multilayer perceptron during training, while inference is performed entirely classically. This design preserves scalability, reduces quantum resource demands, and enables practical deployment. We provide a theoretical analysis based on statistical learning and neural tangent kernel theory, establishing explicit risk bounds and demonstrating improved expressivity and trainability compared to purely quantum or existing hybrid approaches. These theoretical insights demonstrate exponential improvements in representation capacity relative to quantum circuit depth and the number of qubits, providing clear computational advantages over standalone quantum circuits and existing hybrid quantum architectures. Empirical results on diverse datasets, including quantum-dot classification and genomic sequence analysis, show that VQC-MLPNet achieves high accuracy and robustness under realistic noise models, outperforming classical and quantum baselines while using significantly fewer trainable parameters.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIP Meets Diffusion: A Synergistic Approach to Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.11772</link>
<guid>https://arxiv.org/abs/2506.11772</guid>
<content:encoded><![CDATA[
arXiv:2506.11772v3 Announce Type: replace-cross 
Abstract: Anomaly detection is a complex problem due to the ambiguity in defining anomalies, the diversity of anomaly types (e.g., local and global defect), and the scarcity of training data. As such, it necessitates a comprehensive model capable of capturing both low-level and high-level features, even with limited data. To address this, we propose CLIPFUSION, a method that leverages both discriminative and generative foundation models. Specifically, the CLIP-based discriminative model excels at capturing global features, while the diffusion-based generative model effectively captures local details, creating a synergistic and complementary approach. Notably, we introduce a methodology for utilizing cross-attention maps and feature maps extracted from diffusion models specifically for anomaly detection. Experimental results on benchmark datasets (MVTec-AD, VisA) demonstrate that CLIPFUSION consistently outperforms baseline methods, achieving outstanding performance in both anomaly segmentation and classification. We believe that our method underscores the effectiveness of multi-modal and multi-model fusion in tackling the multifaceted challenges of anomaly detection, providing a scalable solution for real-world applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs</title>
<link>https://arxiv.org/abs/2506.14562</link>
<guid>https://arxiv.org/abs/2506.14562</guid>
<content:encoded><![CDATA[
arXiv:2506.14562v3 Announce Type: replace-cross 
Abstract: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines. Our code is available at https://github.com/hed-ucas/AlphaDecay.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recurrent neural network-based robust control systems with closed-loop regional incremental ISS and application to MPC design</title>
<link>https://arxiv.org/abs/2506.20334</link>
<guid>https://arxiv.org/abs/2506.20334</guid>
<content:encoded><![CDATA[
arXiv:2506.20334v2 Announce Type: replace-cross 
Abstract: This paper investigates the design of output-feedback schemes for systems described by a class of recurrent neural networks. We propose a procedure based on linear matrix inequalities for designing an observer and a static state-feedback controller. The algorithm leverages global and regional incremental input-to-state stability (incremental ISS) and enables the tracking of constant setpoints, ensuring robustness to disturbances and state estimation uncertainty. To address the potential limitations of regional incremental ISS, we introduce an alternative scheme in which the static law is replaced with a tube-based nonlinear model predictive controller (NMPC) that exploits regional incremental ISS properties. We show that these conditions enable the formulation of a robust NMPC law with guarantees of convergence and recursive feasibility, leading to an enlarged region of attraction. Theoretical results are validated through numerical simulations on the pH-neutralisation process benchmark.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition</title>
<link>https://arxiv.org/abs/2507.05724</link>
<guid>https://arxiv.org/abs/2507.05724</guid>
<content:encoded><![CDATA[
arXiv:2507.05724v3 Announce Type: replace-cross 
Abstract: Mixture-of-experts (MoE) architectures have expanded from language modeling to automatic speech recognition (ASR). Traditional MoE methods, such as the Switch Transformer, route experts independently within each layer. Our analysis reveals that routers in most layers make expert choices that are not strongly correlated with the choices of the routers in other layers. To increase the cooperation between experts in different layers and encourage greater specialization, we use a shared router across different MoE layers. We call this model Omni-router Transformer. Extensive experiments on a large-scale pseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is able to achieve lower training loss and consistently outperform dense and Switch Transformer models, reducing average word error rates by 11.2% and 8.2%, respectively, while providing structured expert usage and improved robustness to diverse data.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Isn't Relational Learning Taking Over the World?</title>
<link>https://arxiv.org/abs/2507.13558</link>
<guid>https://arxiv.org/abs/2507.13558</guid>
<content:encoded><![CDATA[
arXiv:2507.13558v5 Announce Type: replace-cross 
Abstract: Artificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TensorHyper-VQC: A Tensor-Train-Guided Hypernetwork for Robust and Scalable Variational Quantum Computing</title>
<link>https://arxiv.org/abs/2508.01116</link>
<guid>https://arxiv.org/abs/2508.01116</guid>
<content:encoded><![CDATA[
arXiv:2508.01116v2 Announce Type: replace-cross 
Abstract: Variational Quantum Computing (VQC) faces fundamental scalability barriers, primarily due to the presence of barren plateaus and its sensitivity to quantum noise. To address these challenges, we introduce TensorHyper-VQC, a novel tensor-train (TT)-guided hypernetwork framework that significantly improves the robustness and scalability of VQC. Our framework fully delegates the generation of quantum circuit parameters to a classical TT network, effectively decoupling optimization from quantum hardware. This innovative parameterization mitigates gradient vanishing, enhances noise resilience through structured low-rank representations, and facilitates efficient gradient propagation. Grounded in Neural Tangent Kernel and statistical learning theory, our rigorous theoretical analyses establish strong guarantees on approximation capability, optimization stability, and generalization performance. Extensive empirical results across quantum dot classification, Max-Cut optimization, and molecular quantum simulation tasks demonstrate that TensorHyper-VQC consistently achieves superior performance and robust noise tolerance, including hardware-level validation on a 156-qubit IBM Heron processor. These results position TensorHyper-VQC as a scalable and noise-resilient framework for advancing practical quantum machine learning on near-term devices.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Live Music Models</title>
<link>https://arxiv.org/abs/2508.04651</link>
<guid>https://arxiv.org/abs/2508.04651</guid>
<content:encoded><![CDATA[
arXiv:2508.04651v3 Announce Type: replace-cross 
Abstract: We introduce a new class of generative models for music called live music models that produce a continuous stream of music in real-time with synchronized user control. We release Magenta RealTime, an open-weights live music model that can be steered using text or audio prompts to control acoustic style. On automatic metrics of music quality, Magenta RealTime outperforms other open-weights music generation models, despite using fewer parameters and offering first-of-its-kind live generation capabilities. We also release Lyria RealTime, an API-based model with extended controls, offering access to our most powerful model with wide prompt coverage. These models demonstrate a new paradigm for AI-assisted music creation that emphasizes human-in-the-loop interaction for live music performance.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off</title>
<link>https://arxiv.org/abs/2508.04825</link>
<guid>https://arxiv.org/abs/2508.04825</guid>
<content:encoded><![CDATA[
arXiv:2508.04825v2 Announce Type: replace-cross 
Abstract: Virtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForTIFAI: Fending Off Recursive Training Induced Failure for AI Model Collapse</title>
<link>https://arxiv.org/abs/2509.08972</link>
<guid>https://arxiv.org/abs/2509.08972</guid>
<content:encoded><![CDATA[
arXiv:2509.08972v4 Announce Type: replace-cross 
Abstract: The increasing reliance on generative AI models is rapidly increasing the volume of synthetic data, with some projections suggesting that most available new data for training could be machine-generated by 2030. This shift to a mainly synthetic content presents a critical challenge: repeated training in synthetic data leads to a phenomenon known as model collapse, where model performance degrades over generations of training, eventually rendering the models ineffective. While the causes of model collapse are increasingly understood, effective mitigation strategies remain scarce. We address this challenge by leveraging a key insight: auto-regressive models tend to generate text sequences to which they assign high confidence (i.e., high log-likelihood). Based on this observation, we introduce the Truncated-Cross-Entropy (TCE) loss function. TCE mitigates collapse by selectively ignoring high-confidence tokens during training, effectively filtering out likely machine-generated artifacts from the learning process. Our experiments demonstrate that models trained with TCE not only learn effectively but also exhibit significantly increased resilience, tolerating over 2.3x more synthetic data before the onset of collapse. In addition, we provide an open-source benchmark for collapse dynamics in mixed-data settings. Our results demonstrate that confidence-aware training objectives can substantially delay collapse onset, offering a practical and generalizable tool for model robustness under synthetic-data exposure.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multivariate Bernoulli Hoeffding Decomposition: From Theory to Sensitivity Analysis</title>
<link>https://arxiv.org/abs/2510.07088</link>
<guid>https://arxiv.org/abs/2510.07088</guid>
<content:encoded><![CDATA[
arXiv:2510.07088v3 Announce Type: replace-cross 
Abstract: Understanding the behavior of predictive models with random inputs can be achieved through functional decompositions into sub-models that capture interpretable effects of input groups. Building on recent advances in uncertainty quantification, the existence and uniqueness of a generalized Hoeffding decomposition have been established for correlated input variables, using oblique projections onto suitable functional subspaces. This work focuses on the case of Bernoulli inputs and provides a complete analytical characterization of the decomposition. We show that, in this discrete setting, the associated subspaces are one-dimensional and that the decomposition admits a closed-form representation. One of the main contributions of this study is to generalize the classical Fourier--Walsh--Hadamard decomposition for pseudo-Boolean functions to the correlated case, yielding an oblique version when the underlying distribution is not a product measure, and recovering the standard orthogonal form when independence holds. This explicit structure offers a fully interpretable framework, clarifying the contribution of each input combination and theoretically enabling model reverse engineering. From this formulation, explicit sensitivity measures-such as Sobol' indices and Shapley effects-can be directly derived. Numerical experiments illustrate the practical interest of the approach for decision-support problems involving binary features. The paper concludes with perspectives on extending the methodology to high-dimensional settings and to models involving inputs with finite, non-binary support.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoiceAgentBench: Are Voice Assistants ready for agentic tasks?</title>
<link>https://arxiv.org/abs/2510.07978</link>
<guid>https://arxiv.org/abs/2510.07978</guid>
<content:encoded><![CDATA[
arXiv:2510.07978v2 Announce Type: replace-cross 
Abstract: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constraint-Driven Small Language Models Based on Agent and OpenAlex Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points in Academic Papers</title>
<link>https://arxiv.org/abs/2510.14303</link>
<guid>https://arxiv.org/abs/2510.14303</guid>
<content:encoded><![CDATA[
arXiv:2510.14303v2 Announce Type: replace-cross 
Abstract: In recent years, the rapid increase in academic publications across various fields has posed severe challenges for academic paper analysis: scientists struggle to timely and comprehensively track the latest research findings and methodologies. Key concept extraction has proven to be an effective analytical paradigm, and its automation has been achieved with the widespread application of language models in industrial and scientific domains. However, existing paper databases are mostly limited to similarity matching and basic classification of key concepts, failing to deeply explore the relational networks between concepts. This paper is based on the OpenAlex opensource knowledge graph. By analyzing nearly 8,000 open-source paper data from Novosibirsk State University, we discovered a strong correlation between the distribution patterns of paper key concept paths and both innovation points and rare paths. We propose a prompt engineering-based key concept path analysis method. This method leverages small language models to achieve precise key concept extraction and innovation point identification, and constructs an agent based on a knowledge graph constraint mechanism to enhance analysis accuracy. Through fine-tuning of the Qwen and DeepSeek models, we achieved significant improvements in accuracy, with the models publicly available on the Hugging Face platform.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning noisy tissue dynamics across time scales</title>
<link>https://arxiv.org/abs/2510.19090</link>
<guid>https://arxiv.org/abs/2510.19090</guid>
<content:encoded><![CDATA[
arXiv:2510.19090v2 Announce Type: replace-cross 
Abstract: Tissue dynamics play a crucial role in biological processes ranging from inflammation to morphogenesis. However, these noisy multicellular dynamics are notoriously hard to predict. Here, we introduce a biomimetic machine learning framework capable of inferring noisy multicellular dynamics directly from experimental movies. This generative model combines graph neural networks, normalizing flows and WaveNet algorithms to represent tissues as neural stochastic differential equations where cells are edges of an evolving graph. Cell interactions are encoded in a dual signaling graph capable of handling signaling cascades. The dual graph architecture of our neural networks reflects the architecture of the underlying biological tissues, substantially reducing the amount of data needed for training, compared to convolutional or fully-connected neural networks. Taking epithelial tissue experiments as a case study, we show that our model not only captures stochastic cell motion but also predicts the evolution of cell states in their division cycle. Finally, we demonstrate that our method can accurately generate the experimental dynamics of developmental systems, such as the fly wing, and cell signaling processes mediated by stochastic ERK waves, paving the way for its use as a digital twin in bioengineering and clinical contexts.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Foundational Theory of Quantitative Abstraction: Adjunctions, Duality, and Logic for Probabilistic Systems</title>
<link>https://arxiv.org/abs/2510.19444</link>
<guid>https://arxiv.org/abs/2510.19444</guid>
<content:encoded><![CDATA[
arXiv:2510.19444v2 Announce Type: replace-cross 
Abstract: The analysis and control of stochastic dynamical systems rely on probabilistic models such as (continuous-space) Markov decision processes, but large or continuous state spaces make exact analysis intractable and call for principled quantitative abstraction. This work develops a unified theory of such abstraction by integrating category theory, coalgebra, quantitative logic, and optimal transport, centred on a canonical $\varepsilon$-quotient of the behavioral pseudo-metric with a universal property: among all abstractions that collapse behavioral differences below $\varepsilon$, it is the most detailed, and every other abstraction achieving the same discounted value-loss guarantee factors uniquely through it. Categorically, a quotient functor $Q_\varepsilon$ from a category of probabilistic systems to a category of metric specifications admits, via the Special Adjoint Functor Theorem, a right adjoint $R_\varepsilon$, yielding an adjunction $Q_\varepsilon \dashv R_\varepsilon$ that formalizes a duality between abstraction and realization; logically, a quantitative modal $\mu$-calculus with separate reward and transition modalities is shown, for a broad class of systems, to be expressively complete for the behavioral pseudo-metric, with a countable fully abstract fragment suitable for computation. The theory is developed coalgebraically over Polish spaces and the Giry monad and validated on finite-state models using optimal-transport solvers, with experiments corroborating the predicted contraction properties and structural stability and aligning with the theoretical value-loss bounds, thereby providing a rigorous foundation for quantitative state abstraction and representation learning in probabilistic domains.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Robust In-Context Memory and Rapid Task Adaptation in Transformers with Hebbian and Gradient-Based Plasticity</title>
<link>https://arxiv.org/abs/2510.21908</link>
<guid>https://arxiv.org/abs/2510.21908</guid>
<content:encoded><![CDATA[
arXiv:2510.21908v2 Announce Type: replace-cross 
Abstract: Large language models display in-context learning as an emergent effect of scale, but they rely on static weights during inference. In contrast, biological systems continually adapt via synaptic plasticity. We investigate whether explicit, biologically inspired plasticity can endow Transformers with faster in-sequence adaptation. To this end, we augment decoder-only Transformers with fast-weight modules updated either by (i) a neuromodulated Hebbian rule or (ii) the gradient-based plasticity mechanism of Duan et al. (2023). Across copying, regression, and few-shot classification tasks (CIFAR-FS, Omniglot), Hebbian plasticity consistently achieves lower loss and stronger few-shot generalization, while gradient-based updates perform best on long-horizon credit assignment. When associations are short and linearly separable, static weights suffice, defining a clear boundary condition for when plasticity helps. Analysis of learned modulatory signals reveals that gradient-based rules maintain large, persistent updates, whereas Hebbian plasticity is sharply gated around salient events. Together, these results show that explicit plasticity complements attention by enabling rapid, task-specific adaptation, and clarify when different plasticity mechanisms are most effective.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative View Stitching</title>
<link>https://arxiv.org/abs/2510.24718</link>
<guid>https://arxiv.org/abs/2510.24718</guid>
<content:encoded><![CDATA[
arXiv:2510.24718v2 Announce Type: replace-cross 
Abstract: Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with a predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is a sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require a specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, a prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, a technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for a variety of predefined camera paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best viewed as videos at https://andrewsonga.github.io/gvs.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using latent representations to link disjoint longitudinal data for mixed-effects regression</title>
<link>https://arxiv.org/abs/2510.25531</link>
<guid>https://arxiv.org/abs/2510.25531</guid>
<content:encoded><![CDATA[
arXiv:2510.25531v2 Announce Type: replace-cross 
Abstract: Many rare diseases offer limited established treatment options, leading patients to switch therapies when new medications emerge. To analyze the impact of such treatment switches within the low sample size limitations of rare disease trials, it is important to use all available data sources. This, however, is complicated when usage of measurement instruments change during the observation period, for example when instruments are adapted to specific age ranges. The resulting disjoint longitudinal data trajectories, complicate the application of traditional modeling approaches like mixed-effects regression. We tackle this by mapping observations of each instrument to a aligned low-dimensional temporal trajectory, enabling longitudinal modeling across instruments. Specifically, we employ a set of variational autoencoder architectures to embed item values into a shared latent space for each time point. Temporal disease dynamics and treatment switch effects are then captured through a mixed-effects regression model applied to latent representations. To enable statistical inference, we present a novel statistical testing approach that accounts for the joint parameter estimation of mixed-effects regression and variational autoencoders. The methodology is applied to quantify the impact of treatment switches for patients with spinal muscular atrophy. Here, our approach aligns motor performance items from different measurement instruments for mixed-effects regression and maps estimated effects back to the observed item level to quantify the treatment switch effect. Our approach allows for model selection as well as for assessing effects of treatment switching. The results highlight the potential of modeling in joint latent representations for addressing small data challenges.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation</title>
<link>https://arxiv.org/abs/2510.26130</link>
<guid>https://arxiv.org/abs/2510.26130</guid>
<content:encoded><![CDATA[
arXiv:2510.26130v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated strong performance on function-level code generation benchmarks, yet real-world software development increasingly demands class-level implementations that integrate multiple methods, attributes, and dependencies within authentic project contexts. This gap between benchmark performance and practical utility raises critical questions about LLMs' readiness for production code assistance, particularly regarding their ability to generalize across familiar and novel codebases.
  We introduce a benchmark derived from real-world open-source repositories, comprising classes divided into seen and unseen partitions to evaluate generalization under practical conditions. We systematically examine how input specification completeness and retrieval-augmented generation affect class-level correctness across multiple state-of-the-art LLMs.
  Our evaluation reveals a substantial performance gap: while LLMs achieve 84 to 89% correctness on synthetic benchmarks, they attain only 25 to 34% on real-world class tasks, with minimal distinction between familiar and novel codebases. Comprehensive documentation provides marginal improvements (1 to 3%), whereas retrieval augmentation yields greater gains (4 to 7%) by supplying concrete implementation patterns. Error analysis identifies AttributeError, TypeError, and AssertionError as dominant failure modes, with distinct patterns between synthetic and real-world scenarios.
  These findings provide actionable insights for enhancing context modelling, documentation strategies, and retrieval integration in production code assistance tools.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Gap between Empirical Welfare Maximization and Conditional Average Treatment Effect Estimation in Policy Learning</title>
<link>https://arxiv.org/abs/2510.26723</link>
<guid>https://arxiv.org/abs/2510.26723</guid>
<content:encoded><![CDATA[
arXiv:2510.26723v2 Announce Type: replace-cross 
Abstract: The goal of policy learning is to train a policy function that recommends a treatment given covariates to maximize population welfare. There are two major approaches in policy learning: the empirical welfare maximization (EWM) approach and the plug-in approach. The EWM approach is analogous to a classification problem, where one first builds an estimator of the population welfare, which is a functional of policy functions, and then trains a policy by maximizing the estimated welfare. In contrast, the plug-in approach is based on regression, where one first estimates the conditional average treatment effect (CATE) and then recommends the treatment with the highest estimated outcome. This study bridges the gap between the two approaches by showing that both are based on essentially the same optimization problem. In particular, we prove an exact equivalence between EWM and least squares over a reparameterization of the policy class. As a consequence, the two approaches are interchangeable in several respects and share the same theoretical guarantees under common conditions. Leveraging this equivalence, we propose a regularization method for policy learning. The reduction to least squares yields a smooth surrogate that is typically easier to optimize in practice. At the same time, for many natural policy classes the inherent combinatorial hardness of exact EWM generally remains, so the reduction should be viewed as an optimization aid rather than a universal bypass of NP-hardness.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver</title>
<link>https://arxiv.org/abs/2506.02935</link>
<guid>https://arxiv.org/abs/2506.02935</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Task Learning, Neural Combinatorial Optimization, Vehicle Routing Problem, Reinforcement Learning, Knowledge Distillation

Summary:
Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) aims to train a single model to solve various Vehicle Routing Problem (VRP) variants. Existing RL-based methods struggle with generalization on large-scale problems. A novel approach, MTL-KD, uses knowledge distillation to efficiently train heavy decoder models with improved generalization. MTL-KD transfers policy knowledge from single-task models to enhance performance across tasks. An adaptive inference strategy, Random Reordering Re-Construction (R3C), tailored for VRP tasks, further enhances the model's performance. Experimental results on 6 seen and 10 unseen VRP variants demonstrate the superior and robust generalization abilities of the proposed method on both synthetic and real-world datasets. <div>
arXiv:2506.02935v4 Announce Type: replace 
Abstract: Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a promising approach to train a unified model capable of solving multiple Vehicle Routing Problem (VRP) variants. However, existing Reinforcement Learning (RL)-based multi-task methods can only train light decoder models on small-scale problems, exhibiting limited generalization ability when solving large-scale problems. To overcome this limitation, this work introduces a novel multi-task learning method driven by knowledge distillation (MTL-KD), which enables the efficient training of heavy decoder models with strong generalization ability. The proposed MTL-KD method transfers policy knowledge from multiple distinct RL-based single-task models to a single heavy decoder model, facilitating label-free training and effectively improving the model's generalization ability across diverse tasks. In addition, we introduce a flexible inference strategy termed Random Reordering Re-Construction (R3C), which is specifically adapted for diverse VRP tasks and further boosts the performance of the multi-task model. Experimental results on 6 seen and 10 unseen VRP variants with up to 1000 nodes indicate that our proposed method consistently achieves superior performance on both uniform and real-world benchmarks, demonstrating robust generalization abilities.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Human-AI Conceptual Alignment through the Prism of Chess</title>
<link>https://arxiv.org/abs/2510.26025</link>
<guid>https://arxiv.org/abs/2510.26025</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, chess, human concepts, transformer, conceptual understanding

Summary:
Artificial intelligence systems have been developed to play chess at a grandmaster level, but a study reveals that while these systems perform well, they may not truly understand human concepts. The analysis shows that early layers of the system can accurately encode human concepts like center control and knight outposts, but deeper layers diverge towards alien representations, indicating a shift away from human understanding. To test the system's conceptual robustness, a Chess960 dataset is introduced, revealing a reliance on memorized patterns rather than genuine understanding when opening theory is eliminated. This tension between game-winning representations and human-aligned thinking poses a challenge for AI applications that require collaboration with humans. As AI systems prioritize performance, they may develop increasingly alien intelligence, highlighting the need for genuine human-AI collaboration in creative AI applications. The dataset and code for further exploration are available for study. 

<br /><br />Summary: <div>
arXiv:2510.26025v2 Announce Type: replace 
Abstract: Do AI systems truly understand human concepts or merely mimic surface patterns? We investigate this through chess, where human creativity meets precise strategic concepts. Analyzing a 270M-parameter transformer that achieves grandmaster-level play, we uncover a striking paradox: while early layers encode human concepts like center control and knight outposts with up to 85\% accuracy, deeper layers, despite driving superior performance, drift toward alien representations, dropping to 50-65\% accuracy. To test conceptual robustness beyond memorization, we introduce the first Chess960 dataset: 240 expert-annotated positions across 6 strategic concepts. When opening theory is eliminated through randomized starting positions, concept recognition drops 10-20\% across all methods, revealing the model's reliance on memorized patterns rather than abstract understanding. Our layer-wise analysis exposes a fundamental tension in current architectures: the representations that win games diverge from those that align with human thinking. These findings suggest that as AI systems optimize for performance, they develop increasingly alien intelligence, a critical challenge for creative AI applications requiring genuine human-AI collaboration. Dataset and code are available at: https://github.com/slomasov/ChessConceptsLLM.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning</title>
<link>https://arxiv.org/abs/2510.26709</link>
<guid>https://arxiv.org/abs/2510.26709</guid>
<content:encoded><![CDATA[
<div> Keywords: distributed machine learning, gradient sparsification, ARC-Top-K, All-Reduce, convergence rates <br />
Summary: 
Communication bottleneck in large-scale distributed machine learning is addressed through gradient sparsification. Existing compressors like Rand-K and Top-K have limitations, prompting the development of ARC-Top-K. This new compressor aligns sparsity patterns across nodes using a lightweight gradient sketch, enabling index-free All-Reduce while preserving globally significant information. ARC-Top-K is provably contractive and, when combined with momentum error feedback (EF21M), achieves linear speedup and sharper convergence rates than the original EF21M. Empirical results show that ARC-Top-K matches the accuracy of Top-K while reducing wall-clock training time by up to 60.7%. This efficient and scalable solution combines the robustness of Rand-K with the strong performance of Top-K. <br /><br />Summary: <div>
arXiv:2510.26709v3 Announce Type: replace 
Abstract: Communication remains a central bottleneck in large-scale distributed machine learning, and gradient sparsification has emerged as a promising strategy to alleviate this challenge. However, existing gradient compressors face notable limitations: Rand-$K$ discards structural information and performs poorly in practice, while Top-$K$ preserves informative entries but loses the contraction property and requires costly All-Gather operations. In this paper, we propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that aligns sparsity patterns across nodes using a lightweight sketch of the gradient, enabling index-free All-Reduce while preserving globally significant information. ARC-Top-$K$ is provably contractive and, when combined with momentum error feedback (EF21M), achieves linear speedup and sharper convergence rates than the original EF21M under standard assumptions. Empirically, ARC-Top-$K$ matches the accuracy of Top-$K$ while reducing wall-clock training time by up to 60.7\%, offering an efficient and scalable solution that combines the robustness of Rand-$K$ with the strong performance of Top-$K$.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization</title>
<link>https://arxiv.org/abs/2511.01884</link>
<guid>https://arxiv.org/abs/2511.01884</guid>
<content:encoded><![CDATA[
<div> CUDA kernel generation, LLMs, automatic optimization, CudaForge, efficiency <br />
Summary: <br />
Developing efficient CUDA kernels is crucial for AI applications but manual design is time-consuming, leading to the need for automatic approaches like CudaForge. This training-free multi-agent workflow uses LLM agents to iteratively generate, correct, and optimize kernels, integrating hardware feedback for improvement. CudaForge achieves 97.6% correctness and a 1.68x speedup over PyTorch baselines, outperforming existing models. It also demonstrates strong generalization across GPUs and base models while maintaining efficiency. Generating an optimized kernel with CudaForge is cost-effective and takes significantly less time compared to other methods. This research showcases the effectiveness of utilizing multi-agent, training-free workflows for high-performance CUDA kernel optimization. <div>
arXiv:2511.01884v1 Announce Type: new 
Abstract: Developing efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, however, often produce low-efficiency kernels, incur high computational overhead, and fail to generalize across settings. In this work, we propose CudaForge, a training-free multi-agent workflow for CUDA kernel generation and optimization. Our workflow is inspired by the iterative workflow of human experts, which contains steps such as developing initial kernels, testing correctness, analyzing hardware feedback, and iterative improvement. More specifically, CudaForge employs two LLM agents: a Coder and a Judge, that iteratively generate, correct, and optimize CUDA kernels, while integrating hardware feedback such as Nsight Compute (NCU) metrics. In extensive evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3, achieves 97.6\% correctness of generated kernels and an average 1.68$\times$ speedup over PyTorch baselines, substantially surpassing state-of-the-art models including OpenAI-o3 and Kevin on KernelBench. Beyond accuracy and speed, CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090, 3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4, QwQ-32B), while maintaining high efficiency. In particular, generating an optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \$ 0.3 API cost, which is significantly cheaper than existing agentic work that costs 6 H100 hours and \$ 5 API cost per kernel. Our results highlight that multi-agent, training-free workflows can enable cost-effective, generalizable, and high-performance CUDA kernel optimization. Code available at https://github.com/OptimAI-Lab/CudaForge
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Multimodal Depression Detection</title>
<link>https://arxiv.org/abs/2511.01892</link>
<guid>https://arxiv.org/abs/2511.01892</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal deep learning, sentiment analysis, Retrieval-Augmented Generation, emotion prompt, AVEC 2019 dataset

Summary:
The study introduces a novel approach called the Retrieval-Augmented Generation (RAG) framework for depression detection using multimodal deep learning, integrating text, audio, and video signals. The method leverages sentiment analysis to enhance emotional understanding but overcomes challenges of high computational cost, domain mismatch, and static knowledge limitations. By retrieving relevant emotional content from a sentiment dataset and generating an Emotion Prompt using a Large Language Model (LLM), the approach enriches emotional representation and improves interpretability. Experimental results on the AVEC 2019 dataset demonstrate the superiority of the proposed method, achieving a CCC of 0.593 and MAE of 3.95, outperforming previous transfer learning and multi-task learning baselines.<br /><br />Summary: <div>
arXiv:2511.01892v1 Announce Type: new 
Abstract: Multimodal deep learning has shown promise in depression detection by integrating text, audio, and video signals. Recent work leverages sentiment analysis to enhance emotional understanding, yet suffers from high computational cost, domain mismatch, and static knowledge limitations. To address these issues, we propose a novel Retrieval-Augmented Generation (RAG) framework. Given a depression-related text, our method retrieves semantically relevant emotional content from a sentiment dataset and uses a Large Language Model (LLM) to generate an Emotion Prompt as an auxiliary modality. This prompt enriches emotional representation and improves interpretability. Experiments on the AVEC 2019 dataset show our approach achieves state-of-the-art performance with CCC of 0.593 and MAE of 3.95, surpassing previous transfer learning and multi-task learning baselines.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Eigenvalues Entropy as a Classifier Evaluation Measure</title>
<link>https://arxiv.org/abs/2511.01904</link>
<guid>https://arxiv.org/abs/2511.01904</guid>
<content:encoded><![CDATA[
<div> Keywords: Classification, Evaluation measure, Eigenvalues entropy, Imbalanced classes, Performance

Summary:
Classification is a crucial method in machine learning for various applications, but existing evaluation measures may be less accurate for datasets with imbalanced classes. This paper proposes using eigenvalues entropy as an evaluation measure for binary or multi-class classification problems. It establishes relationships between eigenvalues and commonly used measures like sensitivity, specificity, AUC-ROC, and the Gini index for binary problems. Additionally, it provides an estimate of the confusion matrix to address imbalanced class issues. The study demonstrates the superior performance of the proposed evaluation measure over standard measures through various data examples. Overall, the eigenvalues entropy offers a more effective approach for evaluating classification methods, particularly in the presence of imbalanced classes.<br /><br />Summary: <div>
arXiv:2511.01904v1 Announce Type: new 
Abstract: Classification is a machine learning method used in many practical applications: text mining, handwritten character recognition, face recognition, pattern classification, scene labeling, computer vision, natural langage processing. A classifier prediction results and training set information are often used to get a contingency table which is used to quantify the method quality through an evaluation measure. Such measure, typically a numerical value, allows to choose a suitable method among several. Many evaluation measures available in the literature are less accurate for a dataset with imbalanced classes. In this paper, the eigenvalues entropy is used as an evaluation measure for a binary or a multi-class problem. For a binary problem, relations are given between the eigenvalues and some commonly used measures, the sensitivity, the specificity, the area under the operating receiver characteristic curve and the Gini index. A by-product result of this paper is an estimate of the confusion matrix to deal with the curse of the imbalanced classes. Various data examples are used to show the better performance of the proposed evaluation measure over the gold standard measures available in the literature.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Geometry-aware Neural Network based Method for Solving High-dimensional Diffeomorphic Mapping Problems</title>
<link>https://arxiv.org/abs/2511.01911</link>
<guid>https://arxiv.org/abs/2511.01911</guid>
<content:encoded><![CDATA[
<div> mesh-free learning, diffeomorphic mapping, variational principles, quasi-conformal theory, neural network architectures 

Summary:
This study introduces a novel mesh-free learning framework for high-dimensional diffeomorphic mapping problems. By incorporating variational principles and quasi-conformal theory, the framework effectively addresses challenges associated with the curse of dimensionality. It prioritizes accurate, bijective mappings by controlling conformality and volume distortion, ensuring deformation quality control. The approach is compatible with gradient-based optimization and neural network architectures, enhancing versatility and scalability in high-dimensional contexts. Experimental results, using both synthetic and real-world medical image data, demonstrate the method's accuracy, robustness, and efficiency in intricate registration tasks. <div>
arXiv:2511.01911v1 Announce Type: new 
Abstract: Traditional methods for high-dimensional diffeomorphic mapping often struggle with the curse of dimensionality. We propose a mesh-free learning framework designed for $n$-dimensional mapping problems, seamlessly combining variational principles with quasi-conformal theory. Our approach ensures accurate, bijective mappings by regulating conformality distortion and volume distortion, enabling robust control over deformation quality. The framework is inherently compatible with gradient-based optimization and neural network architectures, making it highly flexible and scalable to higher-dimensional settings. Numerical experiments on both synthetic and real-world medical image data validate the accuracy, robustness, and effectiveness of the proposed method in complex registration scenarios.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Superpositional Gradient Descent: Harnessing Quantum Principles for Model Training</title>
<link>https://arxiv.org/abs/2511.01918</link>
<guid>https://arxiv.org/abs/2511.01918</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Superpositional Gradient Descent, quantum-inspired methods, optimization techniques, PyTorch

Summary: 
Superpositional Gradient Descent (SGD) is introduced as a novel optimizer that combines classical optimization techniques with quantum superposition through quantum circuit perturbations. By implementing hybrid quantum-classical circuits in PyTorch and Qiskit, SGD shows improved convergence and lower final loss compared to the widely used AdamW optimizer on synthetic sequence classification tasks and large-scale fine-tuning of large language models. While these results are promising, limitations in scalability and hardware constraints hinder widespread adoption. This work sheds light on the potential of integrating quantum computing principles into deep learning algorithms, providing insights into controlling and enhancing model behavior through quantum-inspired methods. <br /><br />Summary: <div>
arXiv:2511.01918v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly trained with classical optimization techniques like AdamW to improve convergence and generalization. However, the mechanisms by which quantum-inspired methods enhance classical training remain underexplored. We introduce Superpositional Gradient Descent (SGD), a novel optimizer linking gradient updates with quantum superposition by injecting quantum circuit perturbations. We present a mathematical framework and implement hybrid quantum-classical circuits in PyTorch and Qiskit. On synthetic sequence classification and large-scale LLM fine-tuning, SGD converges faster and yields lower final loss than AdamW. Despite promising results, scalability and hardware constraints limit adoption. Overall, this work provides new insights into the intersection of quantum computing and deep learning, suggesting practical pathways for leveraging quantum principles to control and enhance model behavior.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Green's Functions</title>
<link>https://arxiv.org/abs/2511.01924</link>
<guid>https://arxiv.org/abs/2511.01924</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Green's Function, linear partial differential equations, generalization, volumetric point cloud, thermal analysis

Summary:
Neural Green's Function is introduced as a neural solution operator for linear partial differential equations with eigendecompositions. It mimics the behavior of Green's functions, achieving superior generalization across irregular geometries and diverse source and boundary functions. By extracting per-point features from a volumetric point cloud and predicting a solution operator decomposition, Neural Green's Function enables efficient and robust generalization to unseen functions. In thermal analysis of mechanical part geometries, it outperforms state-of-the-art neural operators with an average error reduction of 13.9% across various shape categories. Additionally, it is up to 350 times faster than traditional numerical solvers that require computationally expensive meshing. <div>
arXiv:2511.01924v1 Announce Type: new 
Abstract: We introduce Neural Green's Function, a neural solution operator for linear partial differential equations (PDEs) whose differential operators admit eigendecompositions. Inspired by Green's functions, the solution operators of linear PDEs that depend exclusively on the domain geometry, we design Neural Green's Function to imitate their behavior, achieving superior generalization across diverse irregular geometries and source and boundary functions. Specifically, Neural Green's Function extracts per-point features from a volumetric point cloud representing the problem domain and uses them to predict a decomposition of the solution operator, which is subsequently applied to evaluate solutions via numerical integration. Unlike recent learning-based solution operators, which often struggle to generalize to unseen source or boundary functions, our framework is, by design, agnostic to the specific functions used during training, enabling robust and efficient generalization. In the steady-state thermal analysis of mechanical part geometries from the MCB dataset, Neural Green's Function outperforms state-of-the-art neural operators, achieving an average error reduction of 13.9\% across five shape categories, while being up to 350 times faster than a numerical solver that requires computationally expensive meshing.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepContour: A Hybrid Deep Learning Framework for Accelerating Generalized Eigenvalue Problem Solving via Efficient Contour Design</title>
<link>https://arxiv.org/abs/2511.01927</link>
<guid>https://arxiv.org/abs/2511.01927</guid>
<content:encoded><![CDATA[
<div> predictive power, deep learning, kernel density estimation, generalized eigenvalue problems, contour integral methods <br />
Summary: <br />
The article introduces DeepContour, a hybrid framework that combines deep learning and Kernel Density Estimation to solve large-scale Generalized Eigenvalue Problems (GEPs) efficiently. The DeepContour framework utilizes a Fourier Neural Operator (FNO) to predict the spectral distribution of a given GEP and then applies Kernel Density Estimation (KDE) to determine proper integration contours. By optimizing the integration contours, DeepContour significantly accelerates GEP solving, achieving up to a 5.63x speedup in multiple datasets. This approach addresses the challenge of selecting integration contours without prior knowledge of eigenvalue distribution, enhancing computational efficiency and numerical accuracy. DeepContour leverages the strengths of deep learning and classical solvers to provide a robust and efficient method for solving challenging GEPs involving high-dimensional matrices. <br /> <div>
arXiv:2511.01927v1 Announce Type: new 
Abstract: Solving large-scale Generalized Eigenvalue Problems (GEPs) is a fundamental yet computationally prohibitive task in science and engineering. As a promising direction, contour integral (CI) methods, such as the CIRR algorithm, offer an efficient and parallelizable framework. However, their performance is critically dependent on the selection of integration contours -- improper selection without reliable prior knowledge of eigenvalue distribution can incur significant computational overhead and compromise numerical accuracy. To address this challenge, we propose DeepContour, a novel hybrid framework that integrates a deep learning-based spectral predictor with Kernel Density Estimation for principled contour design. Specifically, DeepContour first employs a Fourier Neural Operator (FNO) to rapidly predict the spectral distribution of a given GEP. Subsequently, Kernel Density Estimation (KDE) is applied to the predicted spectrum to automatically and systematically determine proper integration contours. Finally, these optimized contours guide the CI solver to efficiently find the desired eigenvalues. We demonstrate the effectiveness of our method on diverse challenging scientific problems. In our main experiments, DeepContour accelerates GEP solving across multiple datasets, achieving up to a 5.63$\times$ speedup. By combining the predictive power of deep learning with the numerical rigor of classical solvers, this work pioneers an efficient and robust paradigm for tackling difficult generalized eigenvalue involving matrices of high dimension.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Population Distribution Aware Human Trajectory Generation with Diffusion Model</title>
<link>https://arxiv.org/abs/2511.01929</link>
<guid>https://arxiv.org/abs/2511.01929</guid>
<content:encoded><![CDATA[
<div> Keywords: human trajectory data, urban planning, population distribution, trajectory generation, diffusion model

Summary:<br />
- Human trajectory data is essential for urban planning, traffic engineering, and public health but faces challenges like privacy concerns and data quality. 
- Trajectory generation is a practical solution to these challenges, simulating human mobility behaviors.
- Existing methods focus on individual movement patterns without considering the influence of population distribution on trajectory generation.
- A novel trajectory generation framework based on a diffusion model is proposed, integrating dynamic population distribution constraints for high-fidelity outcomes.
- The model uses a spatial graph and a dynamic population distribution aware denoising network to capture spatiotemporal dependencies and population distribution impact, outperforming state-of-the-art algorithms by over 54%. 

Summary: <div>
arXiv:2511.01929v1 Announce Type: new 
Abstract: Human trajectory data is crucial in urban planning, traffic engineering, and public health. However, directly using real-world trajectory data often faces challenges such as privacy concerns, data acquisition costs, and data quality. A practical solution to these challenges is trajectory generation, a method developed to simulate human mobility behaviors. Existing trajectory generation methods mainly focus on capturing individual movement patterns but often overlook the influence of population distribution on trajectory generation. In reality, dynamic population distribution reflects changes in population density across different regions, significantly impacting individual mobility behavior. Thus, we propose a novel trajectory generation framework based on a diffusion model, which integrates the dynamic population distribution constraints to guide high-fidelity generation outcomes. Specifically, we construct a spatial graph to enhance the spatial correlation of trajectories. Then, we design a dynamic population distribution aware denoising network to capture the spatiotemporal dependencies of human mobility behavior as well as the impact of population distribution in the denoising process. Extensive experiments show that the trajectories generated by our model can resemble real-world trajectories in terms of some critical statistical metrics, outperforming state-of-the-art algorithms by over 54%.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models</title>
<link>https://arxiv.org/abs/2511.01932</link>
<guid>https://arxiv.org/abs/2511.01932</guid>
<content:encoded><![CDATA[
<div> Keywords: Image generation models, personalization, explainability, natural language, Fine-grained

Summary:
FineXL introduces a technique for Fine-grained eXplainability in natural Language for personalized image generation models. It aims to provide detailed explanations in natural language about the different aspects of personalization in generated images, along with quantitative scores indicating the level of each aspect. Existing approaches to explainability in natural language are limited in their ability to identify multiple aspects and varying levels of personalization accurately. FineXL addresses this limitation and enhances the accuracy of explainability by 56% across various personalization scenarios applied to different image generation models. This new technique enables users to better understand how their personalized models are tailored to meet their individual needs. <div>
arXiv:2511.01932v1 Announce Type: new 
Abstract: Image generation models are usually personalized in practical uses in order to better meet the individual users' heterogeneous needs, but most personalized models lack explainability about how they are being personalized. Such explainability can be provided via visual features in generated images, but is difficult for human users to understand. Explainability in natural language is a better choice, but the existing approaches to explainability in natural language are limited to be coarse-grained. They are unable to precisely identify the multiple aspects of personalization, as well as the varying levels of personalization in each aspect. To address such limitation, in this paper we present a new technique, namely \textbf{FineXL}, towards \textbf{Fine}-grained e\textbf{X}plainability in natural \textbf{L}anguage for personalized image generation models. FineXL can provide natural language descriptions about each distinct aspect of personalization, along with quantitative scores indicating the level of each aspect of personalization. Experiment results show that FineXL can improve the accuracy of explainability by 56\%, when different personalization scenarios are applied to multiple types of image generation models.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch</title>
<link>https://arxiv.org/abs/2511.01934</link>
<guid>https://arxiv.org/abs/2511.01934</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language models, tool-augmented, generalization, dynamic reward design  
Summary:  
- The study focuses on enhancing language models through reinforcement learning (RL) to improve reasoning and generalization abilities.  
- A dynamic reward design is proposed for rule-based RL to shift rewards from exploratory to exploitative tool-use patterns.  
- The Tool-Zero series models are introduced to train LLMs to use general tools independently without post-training.  
- Experimental results show over 7% performance improvement compared to supervised fine-tuning and RL-with-SFT models.  
- The gains are consistent across different dataset evaluations, demonstrating the effectiveness and robustness of the proposed methods. 

<br /><br />Summary: <div>
arXiv:2511.01934v1 Announce Type: new 
Abstract: Training tool-augmented LLMs has emerged as a promising approach to enhancing language models' capabilities for complex tasks. The current supervised fine-tuning paradigm relies on constructing extensive domain-specific datasets to train models. However, this approach often struggles to generalize effectively to unfamiliar or intricate tool-use scenarios. Recently, reinforcement learning (RL) paradigm can endow LLMs with superior reasoning and generalization abilities. In this work, we address a key question: Can the pure RL be used to effectively elicit a model's intrinsic reasoning capabilities and enhance the tool-agnostic generalization? We propose a dynamic generalization-guided reward design for rule-based RL, which progressively shifts rewards from exploratory to exploitative tool-use patterns. Based on this design, we introduce the Tool-Zero series models. These models are trained to enable LLMs to autonomously utilize general tools by directly scaling up RL from Zero models (i.e., base models without post-training). Experimental results demonstrate that our models achieve over 7% performance improvement compared to both SFT and RL-with-SFT models under the same experimental settings. These gains are consistently replicated across cross-dataset and intra-dataset evaluations, validating the effectiveness and robustness of our methods.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q-Sat AI: Machine Learning-Based Decision Support for Data Saturation in Qualitative Studies</title>
<link>https://arxiv.org/abs/2511.01935</link>
<guid>https://arxiv.org/abs/2511.01935</guid>
<content:encoded><![CDATA[
<div> Keywords: qualitative research, sample size determination, machine learning, ensemble learning model, decision support system <br />
Summary: 
This study introduces a systematic model using machine learning to determine sample size in qualitative research, aiming to enhance objectivity in the process. A dataset from five qualitative research approaches was utilized to develop an ensemble learning model that evaluated ten critical parameters. ML algorithms such as K-Nearest Neighbors, Gradient Boosting, Random Forest, XGBoost, and Decision Tree showed high explanatory power in modeling qualitative sampling decisions. Feature importance analysis highlighted the significance of research design type and information power. The study proposes a web-based computational application as a decision support system for qualitative researchers. This model contributes to standardizing sample size justification, increasing transparency, and improving the epistemological foundation of qualitative inquiry through evidence-based decision-making. <br /><br />Summary: <div>
arXiv:2511.01935v1 Announce Type: new 
Abstract: The determination of sample size in qualitative research has traditionally relied on the subjective and often ambiguous principle of data saturation, which can lead to inconsistencies and threaten methodological rigor. This study introduces a new, systematic model based on machine learning (ML) to make this process more objective. Utilizing a dataset derived from five fundamental qualitative research approaches - namely, Case Study, Grounded Theory, Phenomenology, Narrative Research, and Ethnographic Research - we developed an ensemble learning model. Ten critical parameters, including research scope, information power, and researcher competence, were evaluated using an ordinal scale and used as input features. After thorough preprocessing and outlier removal, multiple ML algorithms were trained and compared. The K-Nearest Neighbors (KNN), Gradient Boosting (GB), Random Forest (RF), XGBoost, and Decision Tree (DT) algorithms showed the highest explanatory power (Test R2 ~ 0.85), effectively modeling the complex, non-linear relationships involved in qualitative sampling decisions. Feature importance analysis confirmed the vital roles of research design type and information power, providing quantitative validation of key theoretical assumptions in qualitative methodology. The study concludes by proposing a conceptual framework for a web-based computational application designed to serve as a decision support system for qualitative researchers, journal reviewers, and thesis advisors. This model represents a significant step toward standardizing sample size justification, enhancing transparency, and strengthening the epistemological foundation of qualitative inquiry through evidence-based, systematic decision-making.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR</title>
<link>https://arxiv.org/abs/2511.01937</link>
<guid>https://arxiv.org/abs/2511.01937</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reinforcement learning, reasoning, model efficiency, brevity

Summary:
Large language models trained for step-by-step reasoning often suffer from verbosity, leading to increased inference cost. Standard Reinforcement Learning with Verifiable Rewards (RLVR) methods filter out easy problems during training, causing models to primarily focus on harder tasks and potentially inflate output length. By incorporating moderately easy problems and implicitly regularizing output length, models can learn to solve harder tasks without verbose outputs. This approach, termed 'emergent brevity for free', results in improved accuracy on challenging tasks without explicit length penalization. Experimental results on the Qwen3-4B-Thinking-2507 dataset demonstrate that models trained using this method achieve baseline pass@1 AIME25 accuracy while generating solutions that are nearly twice as short on average. The code is available on GitHub, with datasets and models accessible through Hugging Face. 

<br /><br />Summary: <div>
arXiv:2511.01937v1 Announce Type: new 
Abstract: Large language models (LLMs) trained for step-by-step reasoning often become excessively verbose, raising inference cost. Standard Reinforcement Learning with Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for training efficiency, leaving the model to train primarily on harder problems that require longer reasoning chains. This skews the output length distribution upward, resulting in a \textbf{model that conflates ``thinking longer'' with ``thinking better''}. In this work, we show that retaining and modestly up-weighting moderately easy problems acts as an implicit length regularizer. Exposing the model to solvable short-chain tasks constrains its output distribution and prevents runaway verbosity. The result is \textbf{\emph{emergent brevity for free}}: the model learns to solve harder problems without inflating the output length, \textbf{ despite the absence of any explicit length penalization}. RLVR experiments using this approach on \textit{Qwen3-4B-Thinking-2507} (with a 16k token limit) achieve baseline pass@1 AIME25 accuracy while generating solutions that are, on average, nearly twice as short. The code is available at \href{https://github.com/MBZUAI-Paris/Frugal-AI}{GitHub}, with datasets and models on \href{https://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc}{Hugging Face}.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Geometry of Grokking: Norm Minimization on the Zero-Loss Manifold</title>
<link>https://arxiv.org/abs/2511.01938</link>
<guid>https://arxiv.org/abs/2511.01938</guid>
<content:encoded><![CDATA[
<div> Keywords: grokking, neural networks, generalization, weight decay, representation learning <br />
<br />
Summary: 
This paper investigates the phenomenon of grokking in neural networks, where delayed generalization occurs after memorizing training data. The study suggests that post-memorization learning can be explained by constrained optimization, with gradient descent minimizing weight norms on the zero-loss manifold. The research proves this concept in the context of infinitesimally small learning rates and weight decay coefficients. By introducing a parameter decoupling approximation, the paper derives a closed-form expression for the post-memorization dynamics of the first layer in a two-layer network. Experiments validate that simulating training using predicted gradients replicates the delayed generalization and representation learning observed in grokking. This work sheds light on the underlying dynamics of grokking and provides insights into how neural networks achieve full generalization after a delay. <br /><br /> <div>
arXiv:2511.01938v1 Announce Type: new 
Abstract: Grokking is a puzzling phenomenon in neural networks where full generalization occurs only after a substantial delay following the complete memorization of the training data. Previous research has linked this delayed generalization to representation learning driven by weight decay, but the precise underlying dynamics remain elusive. In this paper, we argue that post-memorization learning can be understood through the lens of constrained optimization: gradient descent effectively minimizes the weight norm on the zero-loss manifold. We formally prove this in the limit of infinitesimally small learning rates and weight decay coefficients. To further dissect this regime, we introduce an approximation that decouples the learning dynamics of a subset of parameters from the rest of the network. Applying this framework, we derive a closed-form expression for the post-memorization dynamics of the first layer in a two-layer network. Experiments confirm that simulating the training process using our predicted gradients reproduces both the delayed generalization and representation learning characteristic of grokking.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning a Distance for the Clustering of Patients with Amyotrophic Lateral Sclerosis</title>
<link>https://arxiv.org/abs/2511.01945</link>
<guid>https://arxiv.org/abs/2511.01945</guid>
<content:encoded><![CDATA[
<div> Keywords: Amyotrophic lateral sclerosis, clustering, disease progression, personalized care, survival analysis

Summary:<br />
- Amyotrophic lateral sclerosis (ALS) is a devastating disease with a short lifespan post-symptom onset.
- Current treatments have limited effectiveness, underscoring the need for personalized patient care.
- Research on ALS is hindered by small and diverse study groups, insufficient longitudinal data, and a lack of clear patient cluster definitions.
- The proposed clustering approach integrates medical expertise and utilizes multiple variables to group sequences based on disease progression scores.
- The approach outperforms existing methods in survival analysis and offers more interpretable results for medical professionals. <div>
arXiv:2511.01945v1 Announce Type: new 
Abstract: Amyotrophic lateral sclerosis (ALS) is a severe disease with a typical survival of 3-5 years after symptom onset. Current treatments offer only limited life extension, and the variability in patient responses highlights the need for personalized care. However, research is hindered by small, heterogeneous cohorts, sparse longitudinal data, and the lack of a clear definition for clinically meaningful patient clusters. Existing clustering methods remain limited in both scope and number. To address this, we propose a clustering approach that groups sequences using a disease progression declarative score. Our approach integrates medical expertise through multiple descriptive variables, investigating several distance measures combining such variables, both by reusing off-the-shelf distances and employing a weak-supervised learning method. We pair these distances with clustering methods and benchmark them against state-of-the-art techniques. The evaluation of our approach on a dataset of 353 ALS patients from the University Hospital of Tours, shows that our method outperforms state-of-the-art methods in survival analysis while achieving comparable silhouette scores. In addition, the learned distances enhance the relevance and interpretability of results for medical experts.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COFAP: A Universal Framework for COFs Adsorption Prediction through Designed Multi-Modal Extraction and Cross-Modal Synergy</title>
<link>https://arxiv.org/abs/2511.01946</link>
<guid>https://arxiv.org/abs/2511.01946</guid>
<content:encoded><![CDATA[
<div> Keywords: Covalent organic frameworks, gas adsorption, high-throughput screening, deep learning, separation

Summary:<br />
Covalent organic frameworks (COFs) are explored as promising adsorbents for gas adsorption and separation. A new universal COFs adsorption prediction framework (COFAP) utilizes deep learning to extract multi-modal structural and chemical features. COFAP outperforms previous approaches without relying on specific gas-related features such as Henry coefficients or adsorption heat. It was found that high-performing COFs for separation tend to have specific characteristics like a narrow range of pore size and surface area. A weight-adjustable prioritization scheme was developed to assist researchers in ranking candidate COFs based on application-specific criteria. The efficiency and accuracy of COFAP make it a valuable tool for identifying optimal COF structures for various gas adsorption and separation applications. <br /> <div>
arXiv:2511.01946v1 Announce Type: new 
Abstract: Covalent organic frameworks (COFs) are promising adsorbents for gas adsorption and separation, while identifying the optimal structures among their vast design space requires efficient high-throughput screening. Conventional machine-learning predictors rely heavily on specific gas-related features. However, these features are time-consuming and limit scalability, leading to inefficiency and labor-intensive processes. Herein, a universal COFs adsorption prediction framework (COFAP) is proposed, which can extract multi-modal structural and chemical features through deep learning, and fuse these complementary features via cross-modal attention mechanism. Without Henry coefficients or adsorption heat, COFAP sets a new SOTA by outperforming previous approaches on hypoCOFs dataset. Based on COFAP, we also found that high-performing COFs for separation concentrate within a narrow range of pore size and surface area. A weight-adjustable prioritization scheme is also developed to enable flexible, application-specific ranking of candidate COFs for researchers. Superior efficiency and accuracy render COFAP directly deployable in crystalline porous materials.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Heart Disease Prediction via a Weighted Ensemble Model: A Large-Scale Study with SHAP and Surrogate Decision Trees</title>
<link>https://arxiv.org/abs/2511.01947</link>
<guid>https://arxiv.org/abs/2511.01947</guid>
<content:encoded><![CDATA[
<div> Keywords: Cardiovascular Disease, Predictive Model, Ensemble Learning, Convolutional Neural Network, Explainable AI

Summary: 
This study introduces a novel predictive model for cardiovascular disease (CVD) risk assessment using a large dataset. The model is a strategically weighted ensemble of tree-based methods (LightGBM, XGBoost) and a Convolutional Neural Network (CNN), trained on a dataset of 229,781 patients with 25 features. By managing class imbalance and incorporating feature engineering, the ensemble model outperforms individual models with a Test AUC of 0.8371 and a high recall of 80.0%. Additionally, the model prioritizes transparency and interpretability by utilizing surrogate decision trees and SHapley Additive exPlanations (SHAP). This combination of robust predictive performance and clinical transparency makes the model well-suited for real-world deployment in public health screening. <br /><br />Summary: <div>
arXiv:2511.01947v1 Announce Type: new 
Abstract: Cardiovascular disease (CVD) remains a critical global health concern, demanding reliable and interpretable predictive models for early risk assessment. This study presents a large-scale analysis using the Heart Disease Health Indicators Dataset, developing a strategically weighted ensemble model that combines tree-based methods (LightGBM, XGBoost) with a Convolutional Neural Network (CNN) to predict CVD risk. The model was trained on a preprocessed dataset of 229,781 patients where the inherent class imbalance was managed through strategic weighting and feature engineering enhanced the original 22 features to 25. The final ensemble achieves a statistically significant improvement over the best individual model, with a Test AUC of 0.8371 (p=0.003) and is particularly suited for screening with a high recall of 80.0%. To provide transparency and clinical interpretability, surrogate decision trees and SHapley Additive exPlanations (SHAP) are used. The proposed model delivers a combination of robust predictive performance and clinical transparency by blending diverse learning architectures and incorporating explainability through SHAP and surrogate decision trees, making it a strong candidate for real-world deployment in public health screening.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EchoLSTM: A Self-Reflective Recurrent Network for Stabilizing Long-Range Memory</title>
<link>https://arxiv.org/abs/2511.01950</link>
<guid>https://arxiv.org/abs/2511.01950</guid>
<content:encoded><![CDATA[
<div> recurrent neural networks, LSTMs, long-range dependencies, EchoLSTM, attention mechanism <br />
Summary: 
The study introduces the EchoLSTM, a novel architectural design incorporating Output-Conditioned Gating to enhance memory retention in neural networks. This self-reflective mechanism enables the model to modulate internal memory gates based on past inferences, creating a stabilizing feedback loop. The EchoLSTM outperforms standard LSTMs by 33 percentage points in a Distractor Signal Task, achieving 69.0% accuracy. On the ListOps benchmark, the EchoLSTM competes with a Transformer model, achieving 69.8% accuracy while being over 5 times more parameter-efficient. Lastly, the Trigger Sensitivity Test demonstrates that the EchoLSTM's self-reflective mechanism leads to a more robust memory system. <div>
arXiv:2511.01950v1 Announce Type: new 
Abstract: Standard Recurrent Neural Networks, including LSTMs, struggle to model long-range dependencies, particularly in sequences containing noisy or misleading information. We propose a new architectural principle, Output-Conditioned Gating, which enables a model to perform self-reflection by modulating its internal memory gates based on its own past inferences. This creates a stabilizing feedback loop that enhances memory retention. Our final model, the EchoLSTM, integrates this principle with an attention mechanism. We evaluate the EchoLSTM on a series of challenging benchmarks. On a custom-designed Distractor Signal Task, the EchoLSTM achieves 69.0% accuracy, decisively outperforming a standard LSTM baseline by 33 percentage points. Furthermore, on the standard ListOps benchmark, the EchoLSTM achieves performance competitive with a modern Transformer model, 69.8% vs. 71.8%, while being over 5 times more parameter-efficient. A final Trigger Sensitivity Test provides qualitative evidence that our model's self-reflective mechanism leads to a fundamentally more robust memory system.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroClean: A Generalized Machine-Learning Approach to Neural Time-Series Conditioning</title>
<link>https://arxiv.org/abs/2511.01951</link>
<guid>https://arxiv.org/abs/2511.01951</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG, LFP, preprocessing, NeuroClean pipeline, machine learning

Summary:
The article introduces a new unsupervised EEG/LFP preprocessing method called the NeuroClean pipeline. This method aims to automate the conditioning of brain data recordings by removing artifacts and noise sources. The pipeline consists of five steps including bandpass filtering, line noise filtering, and bad channel rejection. It utilizes independent component analysis with automatic component rejection based on a clustering algorithm to ensure relevant information is preserved. Validation experiments showed that NeuroClean successfully removed common artifacts from the signal and significantly improved accuracy in motor tasks after cleaning the data. Specifically, the cleaned data achieved over 97% accuracy in a Multinomial Logistic Regression model compared to 74% accuracy with raw data. This promising pipeline demonstrates the potential for enhancing the generalization and performance of machine learning pipelines in future research studies.

<br /><br />Summary: <div>
arXiv:2511.01951v1 Announce Type: new 
Abstract: Electroencephalography (EEG) and local field potentials (LFP) are two widely used techniques to record electrical activity from the brain. These signals are used in both the clinical and research domains for multiple applications. However, most brain data recordings suffer from a myriad of artifacts and noise sources other than the brain itself. Thus, a major requirement for their use is proper and, given current volumes of data, a fully automatized conditioning. As a means to this end, here we introduce an unsupervised, multipurpose EEG/LFP preprocessing method, the NeuroClean pipeline. In addition to its completeness and reliability, NeuroClean is an unsupervised series of algorithms intended to mitigate reproducibility issues and biases caused by human intervention. The pipeline is designed as a five-step process, including the common bandpass and line noise filtering, and bad channel rejection. However, it incorporates an efficient independent component analysis with an automatic component rejection based on a clustering algorithm. This machine learning classifier is used to ensure that task-relevant information is preserved after each step of the cleaning process. We used several data sets to validate the pipeline. NeuroClean removed several common types of artifacts from the signal. Moreover, in the context of motor tasks of varying complexity, it yielded more than 97% accuracy (vs. a chance-level of 33.3%) in an optimized Multinomial Logistic Regression model after cleaning the data, compared to the raw data, which performed at 74% accuracy. These results show that NeuroClean is a promising pipeline and workflow that can be applied to future work and studies to achieve better generalization and performance on machine learning pipelines.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bulk-boundary decomposition of neural networks</title>
<link>https://arxiv.org/abs/2511.02003</link>
<guid>https://arxiv.org/abs/2511.02003</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, training dynamics, bulk-boundary decomposition, stochastic gradient descent, field-theoretic formulation <br />
Summary: 
The article introduces a new framework called the bulk-boundary decomposition to analyze the training dynamics of deep neural networks. It reorganizes the Lagrangian of stochastic gradient descent into a bulk term, representing intrinsic dynamics determined by network architecture and activation functions, and a boundary term, reflecting stochastic interactions from training samples at input and output layers. This decomposition reveals the local and homogeneous structure within deep networks. The authors also develop a field-theoretic formulation of neural dynamics based on this decomposition, offering a deeper understanding of network behavior. <div>
arXiv:2511.02003v1 Announce Type: new 
Abstract: We present the bulk-boundary decomposition as a new framework for understanding the training dynamics of deep neural networks. Starting from the stochastic gradient descent formulation, we show that the Lagrangian can be reorganized into a data-independent bulk term and a data-dependent boundary term. The bulk captures the intrinsic dynamics set by network architecture and activation functions, while the boundary reflects stochastic interactions from training samples at the input and output layers. This decomposition exposes the local and homogeneous structure underlying deep networks. As a natural extension, we develop a field-theoretic formulation of neural dynamics based on this decomposition.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TapOut: A Bandit-Based Approach to Dynamic Speculative Decoding</title>
<link>https://arxiv.org/abs/2511.02017</link>
<guid>https://arxiv.org/abs/2511.02017</guid>
<content:encoded><![CDATA[
<div> accelerates LLMs, speculative decoding, dynamic speculative decoding, TapOut, multi-armed bandits <br />
Summary: <br />
Speculative decoding speeds up LLMs by using a draft model to generate tokens before verification. The challenge is determining the optimal number of tokens to draft. TapOut is a training-free algorithm for dynamic speculation policy selection using multi-armed bandits. It selects among multiple strategies based on past rewards and exploration, outperforming existing methods without hyperparameter tuning. <div>
arXiv:2511.02017v1 Announce Type: new 
Abstract: Speculative decoding accelerates LLMs by using a lightweight draft model to generate tokens autoregressively before verifying them in parallel with a larger target model. However, determining the optimal number of tokens to draft remains a key challenge limiting the approach's effectiveness. Dynamic speculative decoding aims to intelligently decide how many tokens to draft to achieve maximum speedups. Existing methods often rely on hand-tuned, sensitive thresholds (e.g., token entropy), which are costly to set and generalize poorly across models and domains. We propose TapOut, an online, training-free, plug-and-play algorithm for dynamic speculation policy selection using multi-armed bandits. Our approach employs a meta-algorithm that selects among multiple parameter-free dynamic speculation strategies based on past reward and exploration. We conduct extensive experiments across diverse model pairs and datasets, showing that TapOut achieves competitive or superior speedups compared to well-established dynamic speculation baselines without any hyperparameter tuning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shared Parameter Subspaces and Cross-Task Linearity in Emergently Misaligned Behavior</title>
<link>https://arxiv.org/abs/2511.02022</link>
<guid>https://arxiv.org/abs/2511.02022</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, emergent misalignment, harmful behavior, parametric geometry, weight landscape

Summary:
This study investigates emergent misalignment (EM) in large language models, where harmful behaviors can arise after fine-tuning on specific datasets. The research takes a geometric approach to understand how harmful behavior is encoded across different tasks. The findings reveal a linear structure in EM parameters across tasks, indicating shared parameter directions that lead to broad misalignment. The study demonstrates strong convergence in EM parameters and functional equivalence through linear mode connectivity, showing how interpolated models maintain harmful behaviors. The results suggest that harmful behaviors may be organized into specific regions of the weight landscape. By uncovering the connection between parametric geometry and behavioral outcomes, this research paves the way for further exploration of parameter space interpretability and interventions based on weight analysis.<br /><br />Summary: <div>
arXiv:2511.02022v1 Announce Type: new 
Abstract: Recent work has discovered that large language models can develop broadly misaligned behaviors after being fine-tuned on narrowly harmful datasets, a phenomenon known as emergent misalignment (EM). However, the fundamental mechanisms enabling such harmful generalization across disparate domains remain poorly understood. In this work, we adopt a geometric perspective to study EM and demonstrate that it exhibits a fundamental cross-task linear structure in how harmful behavior is encoded across different datasets. Specifically, we find a strong convergence in EM parameters across tasks, with the fine-tuned weight updates showing relatively high cosine similarities, as well as shared lower-dimensional subspaces as measured by their principal angles and projection overlaps. Furthermore, we also show functional equivalence via linear mode connectivity, wherein interpolated models across narrow misalignment tasks maintain coherent, broadly misaligned behavior. Our results indicate that EM arises from different narrow tasks discovering the same set of shared parameter directions, suggesting that harmful behaviors may be organized into specific, predictable regions of the weight landscape. By revealing this fundamental connection between parametric geometry and behavioral outcomes, we hope our work catalyzes further research on parameter space interpretability and weight-based interventions.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Path-Coordinated Continual Learning with Neural Tangent Kernel-Justified Plasticity: A Theoretical Framework with Near State-of-the-Art Performance</title>
<link>https://arxiv.org/abs/2511.02025</link>
<guid>https://arxiv.org/abs/2511.02025</guid>
<content:encoded><![CDATA[
<div> NTK theory, continual learning, catastrophic forgetting, Split-CIFAR10, adaptive regularization <br />
Summary:<br />
The article introduces a novel path-coordinated framework for continual learning to address catastrophic forgetting in neural networks. By combining NTK theory, Wilson confidence intervals, and multiple metrics evaluation, the framework achieves an average accuracy of 66.7% with only 23.4% catastrophic forgetting on Split-CIFAR10. The use of NTK condition numbers as predictive indicators reveals a critical threshold for learning capacity limits. Additionally, the proposed strategy demonstrates a trend of decreasing forgetting as tasks progress, indicating system stabilization. The framework validates 80% of discovered paths statistically and maintains 90-97% retention on intermediate tasks. Analysis of core capacity limits provides valuable insights for enhancing adaptive regularization in continual learning environments. <br /> <div>
arXiv:2511.02025v1 Announce Type: new 
Abstract: Catastrophic forgetting is one of the fundamental issues of continual learning because neural networks forget the tasks learned previously when trained on new tasks. The proposed framework is a new path-coordinated framework of continual learning that unites the Neural Tangent Kernel (NTK) theory of principled plasticity bounds, statistical validation by Wilson confidence intervals, and evaluation of path quality by the use of multiple metrics. Experimental evaluation shows an average accuracy of 66.7% at the cost of 23.4% catastrophic forgetting on Split-CIFAR10, a huge improvement over the baseline and competitive performance achieved, which is very close to state-of-the-art results. Further, it is found out that NTK condition numbers are predictive indicators of learning capacity limits, showing the existence of a critical threshold at condition number $>10^{11}$. It is interesting to note that the proposed strategy shows a tendency of lowering forgetting as the sequence of tasks progresses (27% to 18%), which is a system stabilization. The framework validates 80% of discovered paths with a rigorous statistical guarantee and maintains 90-97% retention on intermediate tasks. The core capacity limits of the continual learning environment are determined in the analysis, and actionable insights to enhance the adaptive regularization are offered.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustFSM: Submodular Maximization in Federated Setting with Malicious Clients</title>
<link>https://arxiv.org/abs/2511.02029</link>
<guid>https://arxiv.org/abs/2511.02029</guid>
<content:encoded><![CDATA[
<div> Keywords: submodular maximization, federated learning, client misbehaviors, RobustFSM, real-world datasets

Summary: 
The article introduces RobustFSM, a solution for federated submodular maximization that addresses client misbehaviors in a decentralized setting where clients have their definitions for data representability. The solution aims to protect against malicious clients sharing fake information, similar to backdoor attacks in federated learning. RobustFSM is designed to maintain the privacy and autonomy of clients while ensuring the quality of the representation subset. Empirical evaluation using real-world datasets demonstrates that RobustFSM outperforms conventional federated algorithms when facing severe attacks, with improvements in solution quality reaching up to 200%. This robust solution offers a practical approach to submodular maximization in federated settings, providing a reliable way to aggregate local information accurately and efficiently. 

<br /><br />Summary: <div>
arXiv:2511.02029v1 Announce Type: new 
Abstract: Submodular maximization is an optimization problem benefiting many machine learning applications, where we seek a small subset best representing an extremely large dataset. We focus on the federated setting where the data are locally owned by decentralized clients who have their own definitions for the quality of representability. This setting requires repetitive aggregation of local information computed by the clients. While the main motivation is to respect the privacy and autonomy of the clients, the federated setting is vulnerable to client misbehaviors: malicious clients might share fake information. An analogy is backdoor attack in conventional federated learning, but our challenge differs freshly due to the unique characteristics of submodular maximization. We propose RobustFSM, a federated submodular maximization solution that is robust to various practical client attacks. Its performance is substantiated with an empirical evaluation study using real-world datasets. Numerical results show that the solution quality of RobustFSM substantially exceeds that of the conventional federated algorithm when attacks are severe. The degree of this improvement depends on the dataset and attack scenarios, which can be as high as 200%
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Microbial Interactions Using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.02038</link>
<guid>https://arxiv.org/abs/2511.02038</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, microbial ecology, interspecies interactions, pairwise interactions, classification<br />
Summary:<br />
Predicting interspecies interactions in microbial communities is essential for understanding their structure and function. Using a dataset of over 7,500 pairwise interactions between 20 species under different conditions, Graph Neural Networks (GNNs) were employed to predict the direction and type of these interactions. By constructing edge-graphs of microbial interactions and leveraging shared information across experiments, the GNN model achieved an F1-score of 80.44%, outperforming conventional methods such as Extreme Gradient Boosting. The model not only classified interactions as positive or negative but also captured more complex types like mutualism, competition, and parasitism. This approach shows promise in improving our understanding of microbial communities and their dynamics. <br /><br />Summary: <div>
arXiv:2511.02038v1 Announce Type: new 
Abstract: Predicting interspecies interactions is a key challenge in microbial ecology, as these interactions are critical to determining the structure and activity of microbial communities. In this work, we used data on monoculture growth capabilities, interactions with other species, and phylogeny to predict a negative or positive effect of interactions. More precisely, we used one of the largest available pairwise interaction datasets to train our models, comprising over 7,500 interactions be- tween 20 species from two taxonomic groups co-cultured under 40 distinct carbon conditions, with a primary focus on the work of Nestor et al.[28 ]. In this work, we propose Graph Neural Networks (GNNs) as a powerful classifier to predict the direction of the effect. We construct edge-graphs of pairwise microbial interactions in order to leverage shared information across individual co-culture experiments, and use GNNs to predict modes of interaction. Our model can not only predict binary interactions (positive/negative) but also classify more complex interaction types such as mutualism, competition, and parasitism. Our initial results were encouraging, achieving an F1-score of 80.44%. This significantly outperforms comparable methods in the literature, including conventional Extreme Gradient Boosting (XGBoost) models, which reported an F1-score of 72.76%.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Enhanced Generative Models for Rare Event Prediction</title>
<link>https://arxiv.org/abs/2511.02042</link>
<guid>https://arxiv.org/abs/2511.02042</guid>
<content:encoded><![CDATA[
<div> finance, climate, rare events, generative model, quantum

Summary:
- The article introduces the Quantum-Enhanced Generative Model (QEGM) to address the challenge of modeling rare events with heavy-tailed distributions.
- QEGM is a hybrid classical-quantum framework combining deep latent-variable models with variational quantum circuits.
- Two key innovations of QEGM include a hybrid loss function optimizing reconstruction fidelity and tail-aware likelihood, as well as quantum randomness-driven noise injection for sample diversity and mitigating mode collapse.
- Training of QEGM involves a hybrid loop updating classical parameters through backpropagation and optimizing quantum parameters using parameter-shift gradients.
- Evaluation on various datasets shows that QEGM reduces tail KL divergence by up to 50% compared to other state-of-the-art baselines like GAN, VAE, and Diffusion, while improving rare-event recall and coverage calibration. 

<br /><br />Summary: <div>
arXiv:2511.02042v1 Announce Type: new 
Abstract: Rare events such as financial crashes, climate extremes, and biological anomalies are notoriously difficult to model due to their scarcity and heavy-tailed distributions. Classical deep generative models often struggle to capture these rare occurrences, either collapsing low-probability modes or producing poorly calibrated uncertainty estimates. In this work, we propose the Quantum-Enhanced Generative Model (QEGM), a hybrid classical-quantum framework that integrates deep latent-variable models with variational quantum circuits. The framework introduces two key innovations: (1) a hybrid loss function that jointly optimizes reconstruction fidelity and tail-aware likelihood, and (2) quantum randomness-driven noise injection to enhance sample diversity and mitigate mode collapse. Training proceeds via a hybrid loop where classical parameters are updated through backpropagation while quantum parameters are optimized using parameter-shift gradients. We evaluate QEGM on synthetic Gaussian mixtures and real-world datasets spanning finance, climate, and protein structure. Results demonstrate that QEGM reduces tail KL divergence by up to 50 percent compared to state-of-the-art baselines (GAN, VAE, Diffusion), while improving rare-event recall and coverage calibration. These findings highlight the potential of QEGM as a principled approach for rare-event prediction, offering robustness beyond what is achievable with purely classical methods.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flashlight: PyTorch Compiler Extensions to Accelerate Attention Variants</title>
<link>https://arxiv.org/abs/2511.02043</link>
<guid>https://arxiv.org/abs/2511.02043</guid>
<content:encoded><![CDATA[
<div> Attention, Large Language Models, Flashlight, PyTorch, FlexAttention<br />
Summary:<br />
The paper introduces Flashlight, a compiler-native framework integrated into the PyTorch ecosystem. Flashlight automates the generation of fused, FlashAttention-style kernels for various attention-based programs. It eliminates the need for static templates or predefined kernel specializations, making it easier to support different attention variants efficiently. Flashlight uses PyTorch's compilation workflow to seamlessly fuse and tile attention computations, resulting in competitive or superior performance compared to FlexAttention. Additionally, Flashlight offers the flexibility of native PyTorch code, allowing developers to quickly experiment with new attention models without compromising on performance. The framework supports not only all variants expressible in the FlexAttention model but also handles more complex, data-dependent attention formulations. Overall, Flashlight enhances the efficiency of attention mechanisms in large language models, promoting faster exploration and development of diverse attention patterns. <br /><br /> <div>
arXiv:2511.02043v1 Announce Type: new 
Abstract: Bad charactors when submitting to arXiv: Attention is a fundamental building block of large language models (LLMs), so there have been many efforts to implement it efficiently. For example, FlashAttention leverages tiling and kernel fusion to optimize attention. Recently, a number of variants of attention have been introduced to enhance model quality or efficiency. Supporting them efficiently remains difficult since they usually require specialized kernels or hand-tuned implementations. FlexAttention recently addressed part of this gap by using static programming templates to support FlashAttention-like kernels for a subset of attention variants.
  In this paper, we introduce Flashlight, a compiler-native framework within the PyTorch ecosystem that automatically generates fused, FlashAttention-style kernels for arbitrary attention-based programs, without relying on static templates or predefined kernel specializations. Flashlight leverages PyTorch's compilation workflow to fuse and tile attention computations transparently, enabling efficient execution for diverse attention patterns. Not only does it support all variants expressible in the FlexAttention model but it also handles more general, data-dependent attention formulations that are beyond the capabilities of FlexAttention.
  Our results show that Flashlight produces kernels with competitive or superior performance to FlexAttention, while offering the flexibility of native PyTorch code, enabling developers to rapidly explore new attention models without sacrificing performance.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regularization Through Reasoning: Systematic Improvements in Language Model Classification via Explanation-Enhanced Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.02044</link>
<guid>https://arxiv.org/abs/2511.02044</guid>
<content:encoded><![CDATA[
<div> model, fine-tuning, classification, explanations, random tokens

Summary: 
The study explores the impact of attaching explanations to labels during fine-tuning Large Language Models (LLMs) for classification tasks. The research investigates conversational response quality in terms of naturalness, comprehensiveness, and on-topic adherence. By incorporating brief explanations along with labels, the 7B-parameter model outperforms label-only baselines across six diverse conversational datasets. Surprisingly, even using syntactically incoherent pseudo-explanations yields improved accuracy over label-only training, indicating the importance of token-level structure in computation. This approach encourages richer intermediate computation and acts as a regularizer to reduce over-confident shortcuts. Internal analyses reveal higher activation entropy in intermediate layers and sharper predictive mass at the output layer, suggesting increased deliberation before decision-making. Ultimately, explanation-augmented fine-tuning enhances classification accuracy and reliability, shedding light on how token-level scaffolding influences computation during inference. 

Summary: <div>
arXiv:2511.02044v1 Announce Type: new 
Abstract: Fine-tuning LLMs for classification typically maps inputs directly to labels. We ask whether attaching brief explanations to each label during fine-tuning yields better models. We evaluate conversational response quality along three axes: naturalness, comprehensiveness, and on-topic adherence, each rated on 5-point scales. Using ensemble-generated data from multiple LLMs, we fine-tune a 7B-parameter model and test across six diverse conversational datasets. Across 18 dataset, task settings, label-plus-explanation training outperforms label-only baselines.
  A central and unexpected result concerns random tokens. We replace human-written explanations with text that is syntactically incoherent yet vocabulary-aligned with the originals (e.g., shuffled or bag-of-words variants). Despite lacking semantics, these pseudo-explanations still improve accuracy over label-only training and often narrow much of the gap to true explanations. The effect persists across datasets and training seeds, indicating that gains arise less from meaning than from structure: the extra token budget encourages richer intermediate computation and acts as a regularizer that reduces over-confident shortcuts.
  Internal analyses support this view: explanation-augmented models exhibit higher activation entropy in intermediate layers alongside sharper predictive mass at the output layer, consistent with increased deliberation before decision. Overall, explanation-augmented fine-tuning, whether with genuine rationales or carefully constructed random token sequences, improves accuracy and reliability for LLM classification while clarifying how token-level scaffolding shapes computation during inference.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dual-Use Framework for Clinical Gait Analysis: Attention-Based Sensor Optimization and Automated Dataset Auditing</title>
<link>https://arxiv.org/abs/2511.02047</link>
<guid>https://arxiv.org/abs/2511.02047</guid>
<content:encoded><![CDATA[
<div> wearable sensors, AI, gait analysis, dataset biases, attention-based deep learning<br />
<br />
Summary: 
The article introduces a novel multi-stream attention-based deep learning framework designed for optimizing wearable sensor use and automatically auditing data integrity in objective gait analysis for managing neurological and orthopedic conditions. The framework was applied to a multi-cohort gait dataset, revealing a severe dataset confound related to bilateral assessment in OA and CVA screening tasks. The model highlighted a laterality bias, with 70% attention on the Right Foot and almost none on the Left Foot due to a skewed dataset (e.g., all right-sided OA cases). This methodology showcases the framework's capability to detect hidden biases and propose optimized sensor synergies, such as using Head plus Foot for PD screening, as potential protocols for future studies.<br /> <div>
arXiv:2511.02047v1 Announce Type: new 
Abstract: Objective gait analysis using wearable sensors and AI is critical for managing neurological and orthopedic conditions. However, models are vulnerable to hidden dataset biases, and task-specific sensor optimization remains a challenge. We propose a multi-stream attention-based deep learning framework that functions as both a sensor optimizer and an automated data auditor. Applied to the Voisard et al. (2025) multi-cohort gait dataset on four clinical tasks (PD, OA, CVA screening; PD vs CVA differential), the model's attention mechanism quantitatively discovered a severe dataset confound. For OA and CVA screening, tasks where bilateral assessment is clinically essential, the model assigned more than 70 percent attention to the Right Foot while statistically ignoring the Left Foot (less than 0.1 percent attention, 95 percent CI [0.0-0.1]). This was not a clinical finding but a direct reflection of a severe laterality bias (for example, 15 of 15 right-sided OA) in the public dataset. The primary contribution of this work is methodological, demonstrating that an interpretable framework can automatically audit dataset integrity. As a secondary finding, the model proposes novel, data-driven sensor synergies (for example, Head plus Foot for PD screening) as hypotheses for future optimized protocols.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finding Probably Approximate Optimal Solutions by Training to Estimate the Optimal Values of Subproblems</title>
<link>https://arxiv.org/abs/2511.02048</link>
<guid>https://arxiv.org/abs/2511.02048</guid>
<content:encoded><![CDATA[
<div> Keywords: solver, maximizing function, binary variables, estimation algorithm, loss function<br />
<br />
Summary: <br />
This paper presents a novel solver for maximizing real-valued functions with binary variables. The solver utilizes an algorithm that estimates optimal objective-function values by training an estimator on the distribution of objectives and sub-instances. Instead of calculating values of policies or relying on solved instances, the estimator is trained using an inequality that employs the expected total deviation from optimality conditions as a loss function. This approach allows for efficient estimation without directly computing the objective-function itself. The solver represents a new method for tackling optimization problems involving binary variables and real-valued functions, presenting a promising direction for future research in this area. <div>
arXiv:2511.02048v1 Announce Type: new 
Abstract: The paper is about developing a solver for maximizing a real-valued function of binary variables. The solver relies on an algorithm that estimates the optimal objective-function value of instances from the underlying distribution of objectives and their respective sub-instances. The training of the estimator is based on an inequality that facilitates the use of the expected total deviation from optimality conditions as a loss function rather than the objective-function itself. Thus, it does not calculate values of policies, nor does it rely on solved instances.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Static Cutoffs: One-Shot Dynamic Thresholding for Diffusion Language Models</title>
<link>https://arxiv.org/abs/2511.02077</link>
<guid>https://arxiv.org/abs/2511.02077</guid>
<content:encoded><![CDATA[
<div> Keywords: Masked diffusion language models, parallel decoding, One-Shot Dynamic Thresholding, accuracy-throughput trade-offs, diffusion decoding <br />
Summary: 
Masked diffusion language models (MDLMs) are advancing to compete with autoregressive models, but face challenges with fixed step decoding. To address this, One-Shot Dynamic Thresholding (OSDT) is introduced, which calibrates thresholds on a single sequence for subsequent inputs with minimal overhead. OSDT outperforms existing methods on various datasets such as GPQA, GSM8K, and HumanEval, achieving higher accuracy-throughput trade-offs. Notably, OSDT shows significant improvements, such as a 24% increase in tokens per second on GSM8K and a 50% increase on HumanEval with a slight accuracy gap. The study suggests the potential for leveraging task-level confidence signatures for broader algorithmic and systems advancements in diffusion decoding. <br /><br />Summary: <div>
arXiv:2511.02077v1 Announce Type: new 
Abstract: Masked diffusion language models (MDLMs) are becoming competitive with their autoregressive counterparts but typically decode with fixed steps and sequential unmasking. To accelerate decoding, recent work such as Fast-dLLM enables parallel decoding via a static global confidence threshold, yet we observe strong block- and step-wise confidence fluctuations and, within a dataset, near-identical confidence trajectories across inputs as measured by cosine similarity. Motivated by these observations, we introduce One-Shot Dynamic Thresholding (OSDT), which calibrates thresholds on a single sequence and applies them to subsequent inputs with negligible overhead. On GPQA, GSM8K, and HumanEval, OSDT attains superior accuracy-throughput trade-offs (+24% tokens/s on GSM8K at the best accuracy, +45% on GPQA with comparable accuracy, and +50% on HumanEval with a modest accuracy gap). Beyond these results, our findings suggest broader opportunities to leverage reusable task-level confidence signatures for more general-purpose algorithmic and systems innovations in diffusion decoding.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy Loss Functions for Physical Systems</title>
<link>https://arxiv.org/abs/2511.02087</link>
<guid>https://arxiv.org/abs/2511.02087</guid>
<content:encoded><![CDATA[
<div> Machine learning, physics, loss function, prediction, generative modeling <br />
<br />
Summary: <br />
The paper introduces a framework that incorporates physical knowledge directly into the loss function for tasks involving molecules and spins. It proposes deriving energy loss functions based on the assumption that each data sample is in thermal equilibrium with an approximate energy landscape. By using the reverse KL divergence with a Boltzmann distribution, the loss function is obtained as an energy difference between the data and model predictions. This approach provides physically grounded loss functions that align better with valid configurations, are architecture-agnostic, and computationally efficient. The energy-based formulation respects physical symmetries and offers improvements over traditional objectives like MSE. The framework is demonstrated on molecular generation and spin ground-state prediction tasks, showcasing significant enhancements over existing methods. <div>
arXiv:2511.02087v1 Announce Type: new 
Abstract: Effectively leveraging prior knowledge of a system's physics is crucial for applications of machine learning to scientific domains. Previous approaches mostly focused on incorporating physical insights at the architectural level. In this paper, we propose a framework to leverage physical information directly into the loss function for prediction and generative modeling tasks on systems like molecules and spins. We derive energy loss functions assuming that each data sample is in thermal equilibrium with respect to an approximate energy landscape. By using the reverse KL divergence with a Boltzmann distribution around the data, we obtain the loss as an energy difference between the data and the model predictions. This perspective also recasts traditional objectives like MSE as energy-based, but with a physically meaningless energy. In contrast, our formulation yields physically grounded loss functions with gradients that better align with valid configurations, while being architecture-agnostic and computationally efficient. The energy loss functions also inherently respect physical symmetries. We demonstrate our approach on molecular generation and spin ground-state prediction and report significant improvements over baselines.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Probing with Contrastive Eigenproblems: Improving Understanding and Applicability of CCS</title>
<link>https://arxiv.org/abs/2511.02089</link>
<guid>https://arxiv.org/abs/2511.02089</guid>
<content:encoded><![CDATA[
<div> Contrast-Consistent Search, unsupervised probing method, large language models, binary features, sentence truth <br />
Summary: <br />
- Contrast-Consistent Search (CCS) is a probing method for testing binary feature representation in language models. <br />
- The objective of CCS, relative contrast consistency, is clarified and reformulated as an eigenproblem for better understanding and broader applicability. <br />
- The reformulated CCS provides closed-form solutions with interpretable eigenvalues and extensions to multiple variables. <br />
- The new approaches show similar performance to CCS, with improved random initialization sensitivity. <br />
- Relativizing contrast consistency enhances our understanding of CCS and opens avenues for more comprehensive probing and interpretability methods.  <div>
arXiv:2511.02089v1 Announce Type: new 
Abstract: Contrast-Consistent Search (CCS) is an unsupervised probing method able to test whether large language models represent binary features, such as sentence truth, in their internal activations. While CCS has shown promise, its two-term objective has been only partially understood. In this work, we revisit CCS with the aim of clarifying its mechanisms and extending its applicability. We argue that what should be optimized for, is relative contrast consistency. Building on this insight, we reformulate CCS as an eigenproblem, yielding closed-form solutions with interpretable eigenvalues and natural extensions to multiple variables. We evaluate these approaches across a range of datasets, finding that they recover similar performance to CCS, while avoiding problems around sensitivity to random initialization. Our results suggest that relativizing contrast consistency not only improves our understanding of CCS but also opens pathways for broader probing and mechanistic interpretability methods.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural Building Blocks for Structured World Models: Theory, Evidence, and Scaling</title>
<link>https://arxiv.org/abs/2511.02091</link>
<guid>https://arxiv.org/abs/2511.02091</guid>
<content:encoded><![CDATA[
<div> Building blocks, structured world models, Hidden Markov Models, switching linear dynamical systems, multimodal generative modeling.<br />
<br />Summary:
The proposed framework for world modeling aims to standardize architectures based on discrete processes (logic, symbols) and continuous processes (physics, dynamics). By utilizing Hidden Markov Models and switching linear dynamical systems as building blocks, the architecture supports both passive modeling and active control. The approach avoids combinatorial explosion by limiting the search to four depth parameters. The framework demonstrates practical expressiveness through multimodal generative modeling and planning from pixels. Scalable joint structure-parameter learning remains a core challenge, with existing methods incrementally growing structure and parameters. If solved, these building blocks could serve as foundational infrastructure for world modeling, similar to standardized layers in deep learning. <br /> <div>
arXiv:2511.02091v1 Announce Type: new 
Abstract: The field of world modeling is fragmented, with researchers developing bespoke architectures that rarely build upon each other. We propose a framework that specifies the natural building blocks for structured world models based on the fundamental stochastic processes that any world model must capture: discrete processes (logic, symbols) and continuous processes (physics, dynamics); the world model is then defined by the hierarchical composition of these building blocks. We examine Hidden Markov Models (HMMs) and switching linear dynamical systems (sLDS) as natural building blocks for discrete and continuous modeling--which become partially-observable Markov decision processes (POMDPs) and controlled sLDS when augmented with actions. This modular approach supports both passive modeling (generation, forecasting) and active control (planning, decision-making) within the same architecture. We avoid the combinatorial explosion of traditional structure learning by largely fixing the causal architecture and searching over only four depth parameters. We review practical expressiveness through multimodal generative modeling (passive) and planning from pixels (active), with performance competitive to neural approaches while maintaining interpretability. The core outstanding challenge is scalable joint structure-parameter learning; current methods finesse this by cleverly growing structure and parameters incrementally, but are limited in their scalability. If solved, these natural building blocks could provide foundational infrastructure for world modeling, analogous to how standardized layers enabled progress in deep learning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science</title>
<link>https://arxiv.org/abs/2511.02092</link>
<guid>https://arxiv.org/abs/2511.02092</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine Learning, Fusion, Data Drift, Online Learning, Deep Gaussian Process Approximation

Summary: 
Machine Learning in fusion research is becoming increasingly important, but traditional models struggle with non-stationary data caused by experimental evolution and machine wear. This paper explores the use of online learning techniques, previously unexplored in fusion applications, to adapt to drifting data streams. The study focuses on predicting Toroidal Field coils deflection at the DIII-D fusion facility and shows that online learning significantly improves model performance, reducing error by 80% compared to static models. To address short-term performance degradation, an uncertainty-guided online ensemble method is proposed. This method uses Deep Gaussian Process Approximation for uncertainty estimation and guides a meta-algorithm to produce predictions based on an ensemble of learners trained on different historical data horizons. The uncertainty-guided online ensemble further reduces prediction errors by approximately 10% compared to standard online learning with a single model. <div>
arXiv:2511.02092v1 Announce Type: new 
Abstract: Machine Learning (ML) is poised to play a pivotal role in the development and operation of next-generation fusion devices. Fusion data shows non-stationary behavior with distribution drifts, resulted by both experimental evolution and machine wear-and-tear. ML models assume stationary distribution and fail to maintain performance when encountered with such non-stationary data streams. Online learning techniques have been leveraged in other domains, however it has been largely unexplored for fusion applications. In this paper, we present an application of online learning to continuously adapt to drifting data stream for prediction of Toroidal Field (TF) coils deflection at the DIII-D fusion facility. The results demonstrate that online learning is critical to maintain ML model performance and reduces error by 80% compared to a static model. Moreover, traditional online learning can suffer from short-term performance degradation as ground truth is not available before making the predictions. As such, we propose an uncertainty guided online ensemble method to further improve the performance. The Deep Gaussian Process Approximation (DGPA) technique is leveraged for calibrated uncertainty estimation and the uncertainty values are then used to guide a meta-algorithm that produces predictions based on an ensemble of learners trained on different horizon of historical data. The DGPA also provides uncertainty estimation along with the predictions for decision makers. The online ensemble and the proposed uncertainty guided online ensemble reduces predictions error by about 6%, and 10% respectively over standard single model based online learning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Data Valuation via Leverage Scores</title>
<link>https://arxiv.org/abs/2511.02100</link>
<guid>https://arxiv.org/abs/2511.02100</guid>
<content:encoded><![CDATA[
<div> leverage scores, data valuation, Shapley valuation, training, active learning <br />
Summary: 
This study introduces a geometric approach to data valuation based on statistical leverage scores as an alternative to the computationally complex Shapley data valuation. The proposed ridge leverage scores measure each data point's influence in the representation space, providing a connection to classical optimal design criteria. The study proves that training on a subset sampled using leverage scores yields a model with parameters and predictive risk close to the full-data optimum. Additionally, an active learning experiment demonstrates that ridge-leverage sampling outperforms standard baselines without gradient information. This approach bridges the gap between data valuation and decision quality in downstream applications. <div>
arXiv:2511.02100v1 Announce Type: new 
Abstract: Shapley data valuation provides a principled, axiomatic framework for assigning importance to individual datapoints, and has gained traction in dataset curation, pruning, and pricing. However, it is a combinatorial measure that requires evaluating marginal utility across all subsets of the data, making it computationally infeasible at scale. We propose a geometric alternative based on statistical leverage scores, which quantify each datapoint's structural influence in the representation space by measuring how much it extends the span of the dataset and contributes to the effective dimensionality of the training problem. We show that our scores satisfy the dummy, efficiency, and symmetry axioms of Shapley valuation and that extending them to \emph{ridge leverage scores} yields strictly positive marginal gains that connect naturally to classical A- and D-optimal design criteria. We further show that training on a leverage-sampled subset produces a model whose parameters and predictive risk are within $O(\varepsilon)$ of the full-data optimum, thereby providing a rigorous link between data valuation and downstream decision quality. Finally, we conduct an active learning experiment in which we empirically demonstrate that ridge-leverage sampling outperforms standard baselines without requiring access gradients or backward passes.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring the Intrinsic Dimension of Earth Representations</title>
<link>https://arxiv.org/abs/2511.02101</link>
<guid>https://arxiv.org/abs/2511.02101</guid>
<content:encoded><![CDATA[
<div> INRs, representation learning, Earth observation, intrinsic dimension, spatial resolution <br />
Summary: <br />
- Geographic Implicit Neural Representations (INRs) embed location inputs into high-dimensional embeddings using Earth data. 
- The study examines the intrinsic dimensionality of geographic INRs which ranges between 2 and 10, influenced by spatial resolution and input modalities. 
- The intrinsic dimension correlates with task performance and helps identify spatial artifacts in the models. 
- This metric provides a label-free measure of information content for unsupervised evaluation and pre-training design of INRs. 
- The research offers a framework-agnostic approach for assessing model performance and improving training strategies in Earth observation representation learning. <div>
arXiv:2511.02101v1 Announce Type: new 
Abstract: Within the context of representation learning for Earth observation, geographic Implicit Neural Representations (INRs) embed low-dimensional location inputs (longitude, latitude) into high-dimensional embeddings, through models trained on geo-referenced satellite, image or text data. Despite the common aim of geographic INRs to distill Earth's data into compact, learning-friendly representations, we lack an understanding of how much information is contained in these Earth representations, and where that information is concentrated. The intrinsic dimension of a dataset measures the number of degrees of freedom required to capture its local variability, regardless of the ambient high-dimensional space in which it is embedded. This work provides the first study of the intrinsic dimensionality of geographic INRs. Analyzing INRs with ambient dimension between 256 and 512, we find that their intrinsic dimensions fall roughly between 2 and 10 and are sensitive to changing spatial resolution and input modalities during INR pre-training. Furthermore, we show that the intrinsic dimension of a geographic INR correlates with downstream task performance and can capture spatial artifacts, facilitating model evaluation and diagnostics. More broadly, our work offers an architecture-agnostic, label-free metric of information content that can enable unsupervised evaluation, model selection, and pre-training design across INRs.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matrix Sensing with Kernel Optimal Loss: Robustness and Optimization Landscape</title>
<link>https://arxiv.org/abs/2511.02122</link>
<guid>https://arxiv.org/abs/2511.02122</guid>
<content:encoded><![CDATA[
<div> robustness, optimization landscape, non-convex optimization, noisy matrix sensing, mean squared error (MSE) loss

Summary:
This paper investigates the impact of different loss functions on the robustness and optimization landscape of non-convex optimization problems, using the example of noisy matrix sensing. The traditional choice of mean squared error (MSE) loss may not be reliable for non-Gaussian or heavy-tailed noise, prompting the adoption of a robust loss based on nonparametric regression. This robust loss, derived from a kernel-based estimate of residual density, maximizes the estimated log-likelihood and remains stable across various noise distributions. By reshaping the optimization landscape, the robust loss helps eliminate spurious local minima by analyzing the upper-bound of restricted isometry property (RIP) constants. Theoretical and empirical analyses demonstrate the effectiveness of this new loss function in handling large noise and enhancing the robustness of machine learning tasks. These findings highlight the importance of selecting appropriate loss functions to improve the performance and reliability of optimization algorithms. 

<br /><br />Summary: <div>
arXiv:2511.02122v1 Announce Type: new 
Abstract: In this paper we study how the choice of loss functions of non-convex optimization problems affects their robustness and optimization landscape, through the study of noisy matrix sensing. In traditional regression tasks, mean squared error (MSE) loss is a common choice, but it can be unreliable for non-Gaussian or heavy-tailed noise. To address this issue, we adopt a robust loss based on nonparametric regression, which uses a kernel-based estimate of the residual density and maximizes the estimated log-likelihood. This robust formulation coincides with the MSE loss under Gaussian errors but remains stable under more general settings. We further examine how this robust loss reshapes the optimization landscape by analyzing the upper-bound of restricted isometry property (RIP) constants for spurious local minima to disappear. Through theoretical and empirical analysis, we show that this new loss excels at handling large noise and remains robust across diverse noise distributions. This work offers initial insights into enhancing the robustness of machine learning tasks through simply changing the loss, guided by an intuitive and broadly applicable analytical framework.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variance-Aware Feel-Good Thompson Sampling for Contextual Bandits</title>
<link>https://arxiv.org/abs/2511.02123</link>
<guid>https://arxiv.org/abs/2511.02123</guid>
<content:encoded><![CDATA[
<div> Thompson Sampling, Contextual Bandits, Regret Bound, Variance, Model Dimension

Summary: 
The paper introduces the FGTSVA algorithm, a variance-aware Thompson Sampling algorithm for contextual bandits with a general reward function. Unlike previous studies, FGTSVA achieves an optimal regret bound that considers the complexity of the model space. By extending the decoupling coefficient technique, FGTSVA is able to achieve a regret of $\tilde{O}(\sqrt{\mathrm{dc}\cdot\log|\mathcal{F}|\sum_{t=1}^T\sigma_t^2}+\mathrm{dc})$ which takes into account the model space size, the total number of rounds, and the subgaussian norm of the noise. In the context of contextual linear bandits, the regret bound of FGTSVA matches that of UCB-based algorithms using weighted linear regression. Overall, FGTSVA provides a promising approach for variance-aware Thompson Sampling in contextual bandit settings. 

<br /><br />Summary: <div>
arXiv:2511.02123v1 Announce Type: new 
Abstract: Variance-dependent regret bounds have received increasing attention in recent studies on contextual bandits. However, most of these studies are focused on upper confidence bound (UCB)-based bandit algorithms, while sampling based bandit algorithms such as Thompson sampling are still understudied. The only exception is the LinVDTS algorithm (Xu et al., 2023), which is limited to linear reward function and its regret bound is not optimal with respect to the model dimension. In this paper, we present FGTSVA, a variance-aware Thompson Sampling algorithm for contextual bandits with general reward function with optimal regret bound. At the core of our analysis is an extension of the decoupling coefficient, a technique commonly used in the analysis of Feel-good Thompson sampling (FGTS) that reflects the complexity of the model space. With the new decoupling coefficient denoted by $\mathrm{dc}$, FGTS-VA achieves the regret of $\tilde{O}(\sqrt{\mathrm{dc}\cdot\log|\mathcal{F}|\sum_{t=1}^T\sigma_t^2}+\mathrm{dc})$, where $|\mathcal{F}|$ is the size of the model space, $T$ is the total number of rounds, and $\sigma_t^2$ is the subgaussian norm of the noise (e.g., variance when the noise is Gaussian) at round $t$. In the setting of contextual linear bandits, the regret bound of FGTSVA matches that of UCB-based algorithms using weighted linear regression (Zhou and Gu, 2022).
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuPCG: Quantum Convolutional Neural Network for Detecting Abnormal Patterns in PCG Signals</title>
<link>https://arxiv.org/abs/2511.02140</link>
<guid>https://arxiv.org/abs/2511.02140</guid>
<content:encoded><![CDATA[
<div> Hybrid Quantum-Classical Convolutional Neural Network, S3, murmur, abnormality, heart sound signals<br />
<br />
Summary:<br />
A hybrid quantum-classical convolutional neural network (QCNN) is proposed for early detection of cardiac abnormalities. Phonocardiogram (PCG) signals are transformed into 2D images using wavelet feature extraction and adaptive threshold compression. The compressed images, with only 8 qubits required for the quantum stage, enable efficient classification of S3 and murmur abnormalities. Results on the HLS-CMDS dataset show 93.33% accuracy on the test set and 97.14% on the train set, indicating the model's ability to capture temporal-spectral correlations in biomedical signals. This marks the first application of a QCNN in bioacoustic signal processing, paving the way for quantum-enhanced diagnostic systems in resource-constrained healthcare settings. <br /><br /> <div>
arXiv:2511.02140v1 Announce Type: new 
Abstract: Early identification of abnormal physiological patterns is essential for the timely detection of cardiac disease. This work introduces a hybrid quantum-classical convolutional neural network (QCNN) designed to classify S3 and murmur abnormalities in heart sound signals. The approach transforms one-dimensional phonocardiogram (PCG) signals into compact two-dimensional images through a combination of wavelet feature extraction and adaptive threshold compression methods. We compress the cardiac-sound patterns into an 8-pixel image so that only 8 qubits are needed for the quantum stage. Preliminary results on the HLS-CMDS dataset demonstrate 93.33% classification accuracy on the test set and 97.14% on the train set, suggesting that quantum models can efficiently capture temporal-spectral correlations in biomedical signals. To our knowledge, this is the first application of a QCNN algorithm for bioacoustic signal processing. The proposed method represents an early step toward quantum-enhanced diagnostic systems for resource-constrained healthcare environments.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disentangling Causal Substructures for Interpretable and Generalizable Drug Synergy Prediction</title>
<link>https://arxiv.org/abs/2511.02146</link>
<guid>https://arxiv.org/abs/2511.02146</guid>
<content:encoded><![CDATA[
<div> CausalDDS, Drug synergy prediction, Combination therapies, Cancer, Molecular level insights
Summary:
CausalDDS is a novel framework proposed for predicting drug synergy in the development of combination therapies for complex diseases, particularly cancer. The framework disentangles drug molecules into causal and spurious substructures, focusing on causal substructures to enhance prediction accuracy and interpretability by minimizing the impact of redundant features. CausalDDS also employs a conditional intervention mechanism and an optimization objective guided by sufficiency and independence principles. Extensive experiments demonstrate that CausalDDS outperforms baseline models, especially in cold start and out-of-distribution settings. Moreover, the framework effectively identifies key substructures underlying drug synergy, offering clear insights into the molecular mechanisms of drug combinations. These results highlight the potential of CausalDDS as a practical tool for drug synergy prediction and drug discovery. 
<br /><br />Summary: <div>
arXiv:2511.02146v1 Announce Type: new 
Abstract: Drug synergy prediction is a critical task in the development of effective combination therapies for complex diseases, including cancer. Although existing methods have shown promising results, they often operate as black-box predictors that rely predominantly on statistical correlations between drug characteristics and results. To address this limitation, we propose CausalDDS, a novel framework that disentangles drug molecules into causal and spurious substructures, utilizing the causal substructure representations for predicting drug synergy. By focusing on causal sub-structures, CausalDDS effectively mitigates the impact of redundant features introduced by spurious substructures, enhancing the accuracy and interpretability of the model. In addition, CausalDDS employs a conditional intervention mechanism, where interventions are conditioned on paired molecular structures, and introduces a novel optimization objective guided by the principles of sufficiency and independence. Extensive experiments demonstrate that our method outperforms baseline models, particularly in cold start and out-of-distribution settings. Besides, CausalDDS effectively identifies key substructures underlying drug synergy, providing clear insights into how drug combinations work at the molecular level. These results underscore the potential of CausalDDS as a practical tool for predicting drug synergy and facilitating drug discovery.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CFL: On the Use of Characteristic Function Loss for Domain Alignment in Machine Learning</title>
<link>https://arxiv.org/abs/2511.02148</link>
<guid>https://arxiv.org/abs/2511.02148</guid>
<content:encoded><![CDATA[
<div> distribution shift problem, machine learning, Characteristic Function, frequency domain approach, domain adaptation

Summary:
In the realm of machine learning, the distribution shift problem poses a significant challenge for the deployment of ML models in real-world applications. This issue can result in severe consequences, especially in high-risk scenarios. Traditionally, statistical methods like Kullback-Leibler and Wasserstein distance have been used to quantify distribution shift. However, a new approach using Characteristic Function (CF) as a frequency domain technique has emerged as a powerful tool for measuring distribution shift in high-dimensional spaces and for facilitating domain adaptation. This letter highlights the effectiveness of the CF method in addressing the distribution shift problem and emphasizes its potential in enhancing the performance of ML models in practical applications. <div>
arXiv:2511.02148v1 Announce Type: new 
Abstract: Machine Learning (ML) models are extensively used in various applications due to their significant advantages over traditional learning methods. However, the developed ML models often underperform when deployed in the real world due to the well-known distribution shift problem. This problem can lead to a catastrophic outcomes when these decision-making systems have to operate in high-risk applications. Many researchers have previously studied this problem in ML, known as distribution shift problem, using statistical techniques (such as Kullback-Leibler, Kolmogorov-Smirnov Test, Wasserstein distance, etc.) to quantify the distribution shift. In this letter, we show that using Characteristic Function (CF) as a frequency domain approach is a powerful alternative for measuring the distribution shift in high-dimensional space and for domain adaptation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProtoTSNet: Interpretable Multivariate Time Series Classification With Prototypical Parts</title>
<link>https://arxiv.org/abs/2511.02152</link>
<guid>https://arxiv.org/abs/2511.02152</guid>
<content:encoded><![CDATA[
<div> Algorithm; Interpretable classification; Time series data; ProtoTSNet architecture; Feature importance<br />
<br />
Summary: 
The paper introduces ProtoTSNet, a new approach for interpretable classification of multivariate time series data. The method builds upon the ProtoPNet architecture to address challenges specific to time series analysis, such as capturing dynamic patterns and handling varying feature significance. A key innovation is a modified convolutional encoder using group convolutions, pre-trainable as part of an autoencoder, to preserve and quantify feature importance. Evaluation on 30 datasets demonstrates that ProtoTSNet outperforms existing explainable methods while maintaining competitive performance with non-explainable and post-hoc explainable approaches. This provides interpretable results accessible to domain experts, crucial in industries like medicine and industry where accurate and interpretable decisions are essential. <div>
arXiv:2511.02152v1 Announce Type: new 
Abstract: Time series data is one of the most popular data modalities in critical domains such as industry and medicine. The demand for algorithms that not only exhibit high accuracy but also offer interpretability is crucial in such fields, as decisions made there bear significant consequences. In this paper, we present ProtoTSNet, a novel approach to interpretable classification of multivariate time series data, through substantial enhancements to the ProtoPNet architecture. Our method is tailored to overcome the unique challenges of time series analysis, including capturing dynamic patterns and handling varying feature significance. Central to our innovation is a modified convolutional encoder utilizing group convolutions, pre-trainable as part of an autoencoder and designed to preserve and quantify feature importance. We evaluated our model on 30 multivariate time series datasets from the UEA archive, comparing our approach with existing explainable methods as well as non-explainable baselines. Through comprehensive evaluation and ablation studies, we demonstrate that our approach achieves the best performance among ante-hoc explainable methods while maintaining competitive performance with non-explainable and post-hoc explainable approaches, providing interpretable results accessible to domain experts.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tackling Incomplete Data in Air Quality Prediction: A Bayesian Deep Learning Framework for Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2511.02175</link>
<guid>https://arxiv.org/abs/2511.02175</guid>
<content:encoded><![CDATA[
<div> Keywords: air quality forecasting, spatiotemporal data, Bayesian neural network, missing data patterns, deep learning

Summary:
The article introduces a new framework called CGLUBNF for accurate air quality forecasting despite missing observational data. CGLUBNF utilizes Fourier features and a graph attention encoder to capture spatial dependencies and seasonal dynamics. It includes a channel gated learning unit with learnable activations and gated residual connections to enhance feature filtering and amplification. Bayesian inference is used to optimize predictive distributions and parameter uncertainty, providing both point estimates and calibrated prediction intervals. Experimental evaluations on real-world datasets demonstrate that CGLUBNF outperforms five state-of-the-art baselines in terms of prediction accuracy and confidence interval sharpness. The model is also robust across various prediction horizons and can handle multiple missing data patterns. This research paves the way for accurate spatio-temporal forecasting in scenarios with incomplete observations, such as vehicle-borne mobile monitoring. 

<br /><br />Summary: <div>
arXiv:2511.02175v1 Announce Type: new 
Abstract: Accurate air quality forecasts are vital for public health alerts, exposure assessment, and emissions control. In practice, observational data are often missing in varying proportions and patterns due to collection and transmission issues. These incomplete spatiotemporal records impede reliable inference and risk assessment and can lead to overconfident extrapolation. To address these challenges, we propose an end to end framework, the channel gated learning unit based spatiotemporal bayesian neural field (CGLUBNF). It uses Fourier features with a graph attention encoder to capture multiscale spatial dependencies and seasonal temporal dynamics. A channel gated learning unit, equipped with learnable activations and gated residual connections, adaptively filters and amplifies informative features. Bayesian inference jointly optimizes predictive distributions and parameter uncertainty, producing point estimates and calibrated prediction intervals. We conduct a systematic evaluation on two real world datasets, covering four typical missing data patterns and comparing against five state of the art baselines. CGLUBNF achieves superior prediction accuracy and sharper confidence intervals. In addition, we further validate robustness across multiple prediction horizons and analysis the contribution of extraneous variables. This research lays a foundation for reliable deep learning based spatio-temporal forecasting with incomplete observations in emerging sensing paradigms, such as real world vehicle borne mobile monitoring.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning</title>
<link>https://arxiv.org/abs/2511.02205</link>
<guid>https://arxiv.org/abs/2511.02205</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal learning, spatiotemporal, neural field, cross-modal, robustness

Summary:
OmniField addresses challenges in multimodal spatiotemporal learning on real-world experimental data by leveraging a continuity-aware framework. The framework learns a continuous neural field that adapts to sparse, irregular, and noisy within-modality measurements, while also handling varying modalities across space and time. By utilizing a multimodal crosstalk block architecture and iterative cross-modal refinement, OmniField aligns signals prior to decoding, enabling tasks such as reconstruction, interpolation, forecasting, and cross-modal prediction without the need for gridding or pre-processing. Extensive evaluations demonstrate that OmniField outperforms eight strong baselines in multimodal spatiotemporal learning. Additionally, the framework exhibits robustness to heavy simulated sensor noise, with performance remaining close to clean-input levels even under such conditions. Overall, OmniField provides a promising solution for effectively handling cross-modal data in real-world scenarios.<br /><br />Summary: <div>
arXiv:2511.02205v1 Announce Type: new 
Abstract: Multimodal spatiotemporal learning on real-world experimental data is constrained by two challenges: within-modality measurements are sparse, irregular, and noisy (QA/QC artifacts) but cross-modally correlated; the set of available modalities varies across space and time, shrinking the usable record unless models can adapt to arbitrary subsets at train and test time. We propose OmniField, a continuity-aware framework that learns a continuous neural field conditioned on available modalities and iteratively fuses cross-modal context. A multimodal crosstalk block architecture paired with iterative cross-modal refinement aligns signals prior to the decoder, enabling unified reconstruction, interpolation, forecasting, and cross-modal prediction without gridding or surrogate preprocessing. Extensive evaluations show that OmniField consistently outperforms eight strong multimodal spatiotemporal baselines. Under heavy simulated sensor noise, performance remains close to clean-input levels, highlighting robustness to corrupted measurements.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Interactive World Model for Object-Centric Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02225</link>
<guid>https://arxiv.org/abs/2511.02225</guid>
<content:encoded><![CDATA[
<div> framework, structured representations, object interactions, policy learning, sample efficiency <br />
Summary: <br />
The article introduces the Factored Interactive Object-Centric World Model (FIOC-WM), a framework that enhances policy learning in agents by learning structured representations of objects and their interactions. FIOC-WM captures environment dynamics using disentangled and modular representations of object interactions, leading to improved sample efficiency and generalization for policy learning. The model learns object-centric latents and interaction structures directly from pixels with pre-trained vision encoders. Tasks are decomposed into interaction primitives, allowing for a hierarchical policy where a high level selects the type and order of interactions, while a low level executes them. FIOC-WM outperforms world-model baselines on simulated robotic and embodied-AI benchmarks, demonstrating the significance of explicit, modular interaction learning for robust control. <div>
arXiv:2511.02225v1 Announce Type: new 
Abstract: Agents that understand objects and their interactions can learn policies that are more robust and transferable. However, most object-centric RL methods factor state by individual objects while leaving interactions implicit. We introduce the Factored Interactive Object-Centric World Model (FIOC-WM), a unified framework that learns structured representations of both objects and their interactions within a world model. FIOC-WM captures environment dynamics with disentangled and modular representations of object interactions, improving sample efficiency and generalization for policy learning. Concretely, FIOC-WM first learns object-centric latents and an interaction structure directly from pixels, leveraging pre-trained vision encoders. The learned world model then decomposes tasks into composable interaction primitives, and a hierarchical policy is trained on top: a high level selects the type and order of interactions, while a low level executes them. On simulated robotic and embodied-AI benchmarks, FIOC-WM improves policy-learning sample efficiency and generalization over world-model baselines, indicating that explicit, modular interaction learning is crucial for robust control.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opportunistic Expert Activation: Batch-Aware Expert Routing for Faster Decode Without Retraining</title>
<link>https://arxiv.org/abs/2511.02237</link>
<guid>https://arxiv.org/abs/2511.02237</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, Autoregressive generation, Token-to-expert mapping, Latency reduction, Quality preservation  
Summary:  
Mixture-of-Experts (MoE) architectures in Large Language Models (LLMs) can face memory-bound challenges during autoregressive generation. This study introduces a dynamic token-to-expert mapping framework to lower decode latency while maintaining quality. By using batch-aware routing, tokens can share experts already loaded into memory, reducing the number of activated experts and thereby decreasing latency. Experimental results on Qwen3-30B and Qwen3-235B models with a batch size of 16 show latency reductions of 39% and 15% in the MoE layer decode latency without significant accuracy loss. This approach efficiently optimizes expert usage in MoE architectures for improved performance in LLMs.  
<br /><br />Summary: <div>
arXiv:2511.02237v1 Announce Type: new 
Abstract: An increasing number of LLMs employ Mixture-of-Experts (MoE) architectures where the feed-forward layer is replaced by a pool of experts and each token only activates a small subset of them. During autoregressive generation, these models often enter a memory-bound regime even for moderate batch sizes because the average expert load grows more slowly than in an equivalent dense feedforward layer. Consequently, MoE latency is governed by the number of activated experts. We introduce a framework for dynamically re-routing token-to-expert mapping to lower this number (and thus, the decode latency) while preserving a comparable quality. Our best results use a batch-aware routing that works by having tokens piggyback experts that have already been loaded into memory due to being crucial to other tokens within the same batch. Empirically, we evaluate our method on the Qwen3-30B and Qwen3-235B models with a batch size of $16$. Without any statistically significant loss in accuracy, our approach achieves latency reductions of $39\%$ and $15\%$ in the MoE layer decode latency, respectively.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural network initialization with nonlinear characteristics and information on spectral bias</title>
<link>https://arxiv.org/abs/2511.02244</link>
<guid>https://arxiv.org/abs/2511.02244</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network, initialization, spectral bias, SWIM algorithm, training performance

Summary: 
Neural network parameter initialization plays a critical role in learning performance, potentially eliminating the need for backpropagation training. Existing algorithms like ridgelet transform and SWIM have been used for initialization. Neural networks tend to learn coarse information in early layers, known as spectral bias. This study investigates leveraging spectral bias information in initialization to improve network performance. A framework is proposed to adjust scale factors in the SWIM algorithm to represent low-frequency components in early layers and high-frequency components in later layers. Numerical experiments on regression and classification tasks show the proposed method outperforms conventional initialization techniques. The research underscores the significance of intrinsic spectral properties in neural network learning and offers an effective strategy for parameter initialization to enhance training outcomes.

<br /><br />Summary: <div>
arXiv:2511.02244v1 Announce Type: new 
Abstract: Initialization of neural network parameters, such as weights and biases, has a crucial impact on learning performance; if chosen well, we can even avoid the need for additional training with backpropagation. For example, algorithms based on the ridgelet transform or the SWIM (sampling where it matters) concept have been proposed for initialization. On the other hand, it is well-known that neural networks tend to learn coarse information in the earlier layers. The feature is called spectral bias. In this work, we investigate the effects of utilizing information on the spectral bias in the initialization of neural networks. Hence, we propose a framework that adjusts the scale factors in the SWIM algorithm to capture low-frequency components in the early-stage hidden layers and to represent high-frequency components in the late-stage hidden layers. Numerical experiments on a one-dimensional regression task and the MNIST classification task demonstrate that the proposed method outperforms the conventional initialization algorithms. This work clarifies the importance of intrinsic spectral properties in learning neural networks, and the finding yields an effective parameter initialization strategy that enhances their training performance.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Graph Cuts</title>
<link>https://arxiv.org/abs/2511.02272</link>
<guid>https://arxiv.org/abs/2511.02272</guid>
<content:encoded><![CDATA[
<div> framework, probabilistic relaxations, graph cuts, spectral clustering, end-to-end learning

Summary:
The article introduces a novel probabilistic framework for graph cuts that offers a differentiable alternative to spectral clustering. This framework covers a broad range of cuts, including Normalized Cut, providing tight analytic upper bounds on expected discrete cuts through integral representations and Gauss hypergeometric functions. The results allow for rigorous, numerically stable computation of differentiable graph partitioning, enabling scalable and principled learning for various clustering and contrastive objectives. This advancement in probabilistic relaxations of graph cuts offers a promising avenue for end-to-end and online learning without the need for eigendecompositions. <div>
arXiv:2511.02272v1 Announce Type: new 
Abstract: Probabilistic relaxations of graph cuts offer a differentiable alternative to spectral clustering, enabling end-to-end and online learning without eigendecompositions, yet prior work centered on RatioCut and lacked general guarantees and principled gradients. We present a unified probabilistic framework that covers a wide class of cuts, including Normalized Cut. Our framework provides tight analytic upper bounds on expected discrete cuts via integral representations and Gauss hypergeometric functions with closed-form forward and backward. Together, these results deliver a rigorous, numerically stable foundation for scalable, differentiable graph partitioning covering a wide range of clustering and contrastive learning objectives.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient-Variation Online Adaptivity for Accelerated Optimization with H\"older Smoothness</title>
<link>https://arxiv.org/abs/2511.02276</link>
<guid>https://arxiv.org/abs/2511.02276</guid>
<content:encoded><![CDATA[
<div> Online learning, Gradient-variation, H\"older smooth functions, Accelerated optimization, Convex optimization<br />
<br />
Summary: 
This paper investigates online learning with H\"older smooth functions, which includes both smooth and non-smooth functions. The study shows a close connection between accelerated optimization and gradient-variation online learning, with the former being understood through the lens of the latter. For strongly convex online functions, a gradient-variation online learning algorithm is designed to achieve optimal guarantees in both smooth and non-smooth regimes. The algorithms do not require prior knowledge of the H\"older smoothness parameter, showcasing adaptivity over existing methods. Through online-to-batch conversion, a universal method for stochastic convex optimization under H\"older smoothness is achieved. The integration of online adaptivity with a detection-based guess-and-check procedure results in a universal offline method that achieves accelerated convergence in the smooth regime while maintaining near-optimal convergence in the non-smooth regime. <div>
arXiv:2511.02276v1 Announce Type: new 
Abstract: Smoothness is known to be crucial for acceleration in offline optimization, and for gradient-variation regret minimization in online learning. Interestingly, these two problems are actually closely connected -- accelerated optimization can be understood through the lens of gradient-variation online learning. In this paper, we investigate online learning with H\"older smooth functions, a general class encompassing both smooth and non-smooth (Lipschitz) functions, and explore its implications for offline optimization. For (strongly) convex online functions, we design the corresponding gradient-variation online learning algorithm whose regret smoothly interpolates between the optimal guarantees in smooth and non-smooth regimes. Notably, our algorithms do not require prior knowledge of the H\"older smoothness parameter, exhibiting strong adaptivity over existing methods. Through online-to-batch conversion, this gradient-variation online adaptivity yields an optimal universal method for stochastic convex optimization under H\"older smoothness. However, achieving universality in offline strongly convex optimization is more challenging. We address this by integrating online adaptivity with a detection-based guess-and-check procedure, which, for the first time, yields a universal offline method that achieves accelerated convergence in the smooth regime while maintaining near-optimal convergence in the non-smooth one.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement learning based data assimilation for unknown state model</title>
<link>https://arxiv.org/abs/2511.02286</link>
<guid>https://arxiv.org/abs/2511.02286</guid>
<content:encoded><![CDATA[
<div> Machine learning, data assimilation, reinforcement learning, Bayesian filtering, state estimation

Summary: 
The article introduces a novel method that combines reinforcement learning with ensemble-based Bayesian filtering for state estimation in scenarios where the underlying dynamics are unknown. By treating the parameter estimation process as a sequential decision-making problem, the method formulates it as a discrete-time Markov decision process. This approach allows for learning a surrogate state transition model directly from noisy observations without relying on noise-free ground-truth state sequences. Once the model is trained offline, state estimation can be performed in real-time using filtering methods based on the learned dynamics. The proposed framework is versatile and applicable to a wide range of observation scenarios, including nonlinear and partially observed measurement models. Numerical examples demonstrate that the method achieves superior accuracy and robustness in high-dimensional settings. <div>
arXiv:2511.02286v1 Announce Type: new 
Abstract: Data assimilation (DA) has increasingly emerged as a critical tool for state estimation
  across a wide range of applications. It is signiffcantly challenging when the governing equations of the underlying dynamics are unknown. To this end, various machine learning approaches have been employed to construct a surrogate state transition
  model in a supervised learning framework, which relies on pre-computed training
  datasets. However, it is often infeasible to obtain noise-free ground-truth state sequences in practice. To address this challenge, we propose a novel method that integrates reinforcement learning with ensemble-based Bayesian ffltering methods, enabling
  the learning of surrogate state transition model for unknown dynamics directly from noisy observations, without using true state trajectories. Speciffcally, we treat the process for computing maximum likelihood estimation of surrogate model parameters
  as a sequential decision-making problem, which can be formulated as a discretetime
  Markov decision process (MDP). Under this formulation, learning the surrogate transition model is equivalent to ffnding an optimal policy of the MDP, which can be effectively addressed using reinforcement learning techniques. Once the model is trained offfine, state estimation can be performed in the online stage using ffltering methods based on the learned dynamics. The proposed framework accommodates a wide range of observation scenarios, including nonlinear and partially observed measurement
  models. A few numerical examples demonstrate that the proposed method achieves superior accuracy and robustness in high-dimensional settings.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Quantum Kernel Learning for Anomaly Detection in Multivariate IoT Time-Series</title>
<link>https://arxiv.org/abs/2511.02301</link>
<guid>https://arxiv.org/abs/2511.02301</guid>
<content:encoded><![CDATA[
arXiv:2511.02301v1 Announce Type: new 
Abstract: The rapid growth of industrial Internet of Things (IIoT) systems has created new challenges for anomaly detection in high-dimensional, multivariate time-series, where privacy, scalability, and communication efficiency are critical. Classical federated learning approaches mitigate privacy concerns by enabling decentralized training, but they often struggle with highly non-linear decision boundaries and imbalanced anomaly distributions. To address this gap, we propose a Federated Quantum Kernel Learning (FQKL) framework that integrates quantum feature maps with federated aggregation to enable distributed, privacy-preserving anomaly detection across heterogeneous IoT networks. In our design, quantum edge nodes locally compute compressed kernel statistics using parameterized quantum circuits and share only these summaries with a central server, which constructs a global Gram matrix and trains a decision function (e.g., Fed-QSVM). Experimental results on synthetic IIoT benchmarks demonstrate that FQKL achieves superior generalization in capturing complex temporal correlations compared to classical federated baselines, while significantly reducing communication overhead. This work highlights the promise of quantum kernels in federated settings, advancing the path toward scalable, robust, and quantum-enhanced intelligence for next-generation IoT infrastructures.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error</title>
<link>https://arxiv.org/abs/2511.02302</link>
<guid>https://arxiv.org/abs/2511.02302</guid>
<content:encoded><![CDATA[
arXiv:2511.02302v1 Announce Type: new 
Abstract: Training large Mixture-of-Experts (MoE) models remains computationally prohibitive due to their extreme compute and memory demands. Although low-precision training promises to accelerate computation and reduce memory footprint, existing implementations still rely on BF16-dominated dataflows with frequent quantize-dequantize (Q/DQ) conversions. These redundant casts erode much of FP8's theoretical efficiency. However, naively removing these casts by keeping dataflows entirely in FP8 introduces double quantization error: tensors quantized along different dimensions accumulate inconsistent scaling factors, degrading numerical stability.
  We propose FP8-Flow-MoE, an FP8 training recipe featuring a quantization-consistent FP8-centric dataflow with a scaling-aware transpose and fused FP8 operators that streamline computation and eliminate explicit cast operations from 12 to 2. Evaluations on a 671B-parameter MoE model demonstrate up to 21\% higher throughput and 16.5 GB lower memory usage per GPU compared to BF16 and na\"ive FP8 baselines, while maintaining stable convergence. We provide a plug-and-play FP8 recipe compatible with TransformerEngine and Megatron-LM, which will be open-sourced soon.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute</title>
<link>https://arxiv.org/abs/2511.02309</link>
<guid>https://arxiv.org/abs/2511.02309</guid>
<content:encoded><![CDATA[
arXiv:2511.02309v1 Announce Type: new 
Abstract: We revisit test-time scaling for language model reasoning and ask a fundamental question: at equal token budget and compute, is it better to run multiple independent chains in parallel, or to run fewer chains that iteratively refine through sequential steps? Through comprehensive evaluation across 5 state-of-the-art open source models and 3 challenging reasoning benchmarks, we find that sequential scaling where chains explicitly build upon previous attempts consistently outperforms the dominant parallel self-consistency paradigm in 95.6% of configurations with gains in accuracy upto 46.7%. Further, we introduce inverse-entropy weighted voting, a novel training-free method to further boost the accuracy of sequential scaling. By weighing answers in proportion to the inverse entropy of their reasoning chains, we increase our success rate over parallel majority and establish it as the optimal test-time scaling strategy. Our findings fundamentally challenge the parallel reasoning orthodoxy that has dominated test-time scaling since Wang et al.'s self-consistency decoding (Wang et al., 2022), positioning sequential refinement as the robust default for modern LLM reasoning and necessitating a paradigm shift in how we approach inference-time optimization.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large-scale automatic carbon ion treatment planning for head and neck cancers via parallel multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2511.02314</link>
<guid>https://arxiv.org/abs/2511.02314</guid>
<content:encoded><![CDATA[
arXiv:2511.02314v1 Announce Type: new 
Abstract: Head-and-neck cancer (HNC) planning is difficult because multiple critical organs-at-risk (OARs) are close to complex targets. Intensity-modulated carbon-ion therapy (IMCT) offers superior dose conformity and OAR sparing but remains slow due to relative biological effectiveness (RBE) modeling, leading to laborious, experience-based, and often suboptimal tuning of many treatment-planning parameters (TPPs). Recent deep learning (DL) methods are limited by data bias and plan feasibility, while reinforcement learning (RL) struggles to efficiently explore the exponentially large TPP search space. We propose a scalable multi-agent RL (MARL) framework for parallel tuning of 45 TPPs in IMCT. It uses a centralized-training decentralized-execution (CTDE) QMIX backbone with Double DQN, Dueling DQN, and recurrent encoding (DRQN) for stable learning in a high-dimensional, non-stationary environment. To enhance efficiency, we (1) use compact historical DVH vectors as state inputs, (2) apply a linear action-to-value transform mapping small discrete actions to uniform parameter adjustments, and (3) design an absolute, clinically informed piecewise reward aligned with plan scores. A synchronous multi-process worker system interfaces with the PHOENIX TPS for parallel optimization and accelerated data collection. On a head-and-neck dataset (10 training, 10 testing), the method tuned 45 parameters simultaneously and produced plans comparable to or better than expert manual ones (relative plan score: RL $85.93\pm7.85%$ vs Manual $85.02\pm6.92%$), with significant (p-value $<$ 0.05) improvements for five OARs. The framework efficiently explores high-dimensional TPP spaces and generates clinically competitive IMCT plans through direct TPS interaction, notably improving OAR sparing.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoME: Domain-Robust Mixture-of-Experts for MILP Solution Prediction across Domains</title>
<link>https://arxiv.org/abs/2511.02331</link>
<guid>https://arxiv.org/abs/2511.02331</guid>
<content:encoded><![CDATA[
arXiv:2511.02331v1 Announce Type: new 
Abstract: Mixed-Integer Linear Programming (MILP) is a fundamental and powerful framework for modeling complex optimization problems across diverse domains. Recently, learning-based methods have shown great promise in accelerating MILP solvers by predicting high-quality solutions. However, most existing approaches are developed and evaluated in single-domain settings, limiting their ability to generalize to unseen problem distributions. This limitation poses a major obstacle to building scalable and general-purpose learning-based solvers. To address this challenge, we introduce RoME, a domain-Robust Mixture-of-Experts framework for predicting MILP solutions across domains. RoME dynamically routes problem instances to specialized experts based on learned task embeddings. The model is trained using a two-level distributionally robust optimization strategy: inter-domain to mitigate global shifts across domains, and intra-domain to enhance local robustness by introducing perturbations on task embeddings. We reveal that cross-domain training not only enhances the model's generalization capability to unseen domains but also improves performance within each individual domain by encouraging the model to capture more general intrinsic combinatorial patterns. Specifically, a single RoME model trained on three domains achieves an average improvement of 67.7% then evaluated on five diverse domains. We further test the pretrained model on MIPLIB in a zero-shot setting, demonstrating its ability to deliver measurable performance gains on challenging real-world instances where existing learning-based approaches often struggle to generalize.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning A Universal Crime Predictor with Knowledge-guided Hypernetworks</title>
<link>https://arxiv.org/abs/2511.02336</link>
<guid>https://arxiv.org/abs/2511.02336</guid>
<content:encoded><![CDATA[
arXiv:2511.02336v1 Announce Type: new 
Abstract: Predicting crimes in urban environments is crucial for public safety, yet existing prediction methods often struggle to align the knowledge across diverse cities that vary dramatically in data availability of specific crime types. We propose HYpernetwork-enhanced Spatial Temporal Learning (HYSTL), a framework that can effectively train a unified, stronger crime predictor without assuming identical crime types in different cities' records. In HYSTL, instead of parameterising a dedicated predictor per crime type, a hypernetwork is designed to dynamically generate parameters for the prediction function conditioned on the crime type of interest. To bridge the semantic gap between different crime types, a structured crime knowledge graph is built, where the learned representations of crimes are used as the input to the hypernetwork to facilitate parameter generation. As such, when making predictions for each crime type, the predictor is additionally guided by its intricate association with other relevant crime types. Extensive experiments are performed on two cities with non-overlapping crime types, and the results demonstrate HYSTL outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reducing normalizing flow complexity for MCMC preconditioning</title>
<link>https://arxiv.org/abs/2511.02345</link>
<guid>https://arxiv.org/abs/2511.02345</guid>
<content:encoded><![CDATA[
arXiv:2511.02345v1 Announce Type: new 
Abstract: Preconditioning is a key component of MCMC algorithms that improves sampling efficiency by facilitating exploration of geometrically complex target distributions through an invertible map. While linear preconditioners are often sufficient for moderately complex target distributions, recent work has explored nonlinear preconditioning with invertible neural networks as components of normalizing flows (NFs). However, empirical and theoretical studies show that overparameterized NF preconditioners can degrade sampling efficiency and fit quality. Moreover, existing NF-based approaches do not adapt their architectures to the target distribution. Related work outside of MCMC similarly finds that suitably parameterized NFs can achieve comparable or superior performance with substantially less training time or data. We propose a factorized preconditioning architecture that reduces NF complexity by combining a linear component with a conditional NF, improving adaptability to target geometry. The linear preconditioner is applied to dimensions that are approximately Gaussian, as estimated from warmup samples, while the conditional NF models more complex dimensions. Our method yields significantly better tail samples on two complex synthetic distributions and consistently better performance on a sparse logistic regression posterior across varying likelihood and prior strengths. It also achieves higher effective sample sizes on hierarchical Bayesian model posteriors with weak likelihoods and strong funnel geometries. This approach is particularly relevant for hierarchical Bayesian model analyses with limited data and could inform current theoretical and software strides in neural MCMC design.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Machine Ritual: Synergic Performance through Real-Time Motion Recognition</title>
<link>https://arxiv.org/abs/2511.02351</link>
<guid>https://arxiv.org/abs/2511.02351</guid>
<content:encoded><![CDATA[
arXiv:2511.02351v1 Announce Type: new 
Abstract: We introduce a lightweight, real-time motion recognition system that enables synergic human-machine performance through wearable IMU sensor data, MiniRocket time-series classification, and responsive multimedia control. By mapping dancer-specific movement to sound through somatic memory and association, we propose an alternative approach to human-machine collaboration, one that preserves the expressive depth of the performing body while leveraging machine learning for attentive observation and responsiveness. We demonstrate that this human-centered design reliably supports high accuracy classification (<50 ms latency), offering a replicable framework to integrate dance-literate machines into creative, educational, and live performance contexts.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolving Graph Learning for Out-of-Distribution Generalization in Non-stationary Environments</title>
<link>https://arxiv.org/abs/2511.02354</link>
<guid>https://arxiv.org/abs/2511.02354</guid>
<content:encoded><![CDATA[
arXiv:2511.02354v1 Announce Type: new 
Abstract: Graph neural networks have shown remarkable success in exploiting the spatial and temporal patterns on dynamic graphs. However, existing GNNs exhibit poor generalization ability under distribution shifts, which is inevitable in dynamic scenarios. As dynamic graph generation progresses amid evolving latent non-stationary environments, it is imperative to explore their effects on out-of-distribution (OOD) generalization. This paper proposes a novel Evolving Graph Learning framework for OOD generalization (EvoOOD) by environment-aware invariant pattern recognition. Specifically, we first design an environment sequential variational auto-encoder to model environment evolution and infer the underlying environment distribution. Then, we introduce a mechanism for environment-aware invariant pattern recognition, tailored to address environmental diversification through inferred distributions. Finally, we conduct fine-grained causal interventions on individual nodes using a mixture of instantiated environment samples. This approach helps to distinguish spatio-temporal invariant patterns for OOD prediction, especially in non-stationary environments. Experimental results demonstrate the superiority of EvoGOOD on both real-world and synthetic dynamic datasets under distribution shifts. To the best of our knowledge, it is the first attempt to study the dynamic graph OOD generalization problem from the environment evolution perspective.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUMA-RAG: Lifelong Multimodal Agents with Provably Stable Streaming Alignment</title>
<link>https://arxiv.org/abs/2511.02371</link>
<guid>https://arxiv.org/abs/2511.02371</guid>
<content:encoded><![CDATA[
arXiv:2511.02371v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as the dominant paradigm for grounding large language model outputs in verifiable evidence. However, as modern AI agents transition from static knowledge bases to continuous multimodal streams encompassing text, images, video, and audio, two critical challenges arise: maintaining index freshness without prohibitive re-indexing costs, and preserving cross-modal semantic consistency across heterogeneous embedding spaces. We present LUMA-RAG, a lifelong multimodal agent architecture featuring three key innovations: (i) a streaming, multi-tier memory system that dynamically spills embeddings from a hot HNSW tier to a compressed IVFPQ tier under strict memory budgets; (ii) a streaming CLAP->CLIP alignment bridge that maintains cross-modal consistency through incremental orthogonal Procrustes updates; and (iii) stability-aware retrieval telemetry providing Safe@k guarantees by jointly bounding alignment drift and quantization error. Experiments demonstrate robust text-to-image retrieval (Recall@10 = 0.94), graceful performance degradation under product quantization offloading, and provably stable audio-to-image rankings (Safe@1 = 1.0), establishing LUMA-RAG as a practical framework for production multimodal RAG systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H-Infinity Filter Enhanced CNN-LSTM for Arrhythmia Detection from Heart Sound Recordings</title>
<link>https://arxiv.org/abs/2511.02379</link>
<guid>https://arxiv.org/abs/2511.02379</guid>
<content:encoded><![CDATA[
arXiv:2511.02379v1 Announce Type: new 
Abstract: Early detection of heart arrhythmia can prevent severe future complications in cardiac patients. While manual diagnosis still remains the clinical standard, it relies heavily on visual interpretation and is inherently subjective. In recent years, deep learning has emerged as a powerful tool to automate arrhythmia detection, offering improved accuracy, consistency, and efficiency. Several variants of convolutional and recurrent neural network architectures have been widely explored to capture spatial and temporal patterns in physiological signals. However, despite these advancements, current models often struggle to generalize well in real-world scenarios, especially when dealing with small or noisy datasets, which are common challenges in biomedical applications. In this paper, a novel CNN-H-Infinity-LSTM architecture is proposed to identify arrhythmic heart signals from heart sound recordings. This architecture introduces trainable parameters inspired by the H-Infinity filter from control theory, enhancing robustness and generalization. Extensive experimentation on the PhysioNet CinC Challenge 2016 dataset, a public benchmark of heart audio recordings, demonstrates that the proposed model achieves stable convergence and outperforms existing benchmarks, with a test accuracy of 99.42% and an F1 score of 98.85%.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Spatially Informed Gaussian Process UCB Method for Decentralized Coverage Control</title>
<link>https://arxiv.org/abs/2511.02398</link>
<guid>https://arxiv.org/abs/2511.02398</guid>
<content:encoded><![CDATA[
arXiv:2511.02398v1 Announce Type: new 
Abstract: We present a novel decentralized algorithm for coverage control in unknown spatial environments modeled by Gaussian Processes (GPs). To trade-off between exploration and exploitation, each agent autonomously determines its trajectory by minimizing a local cost function. Inspired by the GP-UCB (Upper Confidence Bound for GPs) acquisition function, the proposed cost combines the expected locational cost with a variance-based exploration term, guiding agents toward regions that are both high in predicted density and model uncertainty. Compared to previous work, our algorithm operates in a fully decentralized fashion, relying only on local observations and communication with neighboring agents. In particular, agents periodically update their inducing points using a greedy selection strategy, enabling scalable online GP updates. We demonstrate the effectiveness of our algorithm in simulation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Unlearning with Model Updates Probably Aligned with Gradients</title>
<link>https://arxiv.org/abs/2511.02435</link>
<guid>https://arxiv.org/abs/2511.02435</guid>
<content:encoded><![CDATA[
arXiv:2511.02435v1 Announce Type: new 
Abstract: We formulate the machine unlearning problem as a general constrained optimization problem. It unifies the first-order methods from the approximate machine unlearning literature. This paper then introduces the concept of feasible updates as the model's parameter update directions that help with unlearning while not degrading the utility of the initial model. Our design of feasible updates is based on masking, \ie\ a careful selection of the model's parameters worth updating. It also takes into account the estimation noise of the gradients when processing each batch of data to offer a statistical guarantee to derive locally feasible updates. The technique can be plugged in, as an add-on, to any first-order approximate unlearning methods. Experiments with computer vision classifiers validate this approach.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accounting for Underspecification in Statistical Claims of Model Superiority</title>
<link>https://arxiv.org/abs/2511.02453</link>
<guid>https://arxiv.org/abs/2511.02453</guid>
<content:encoded><![CDATA[
arXiv:2511.02453v1 Announce Type: new 
Abstract: Machine learning methods are increasingly applied in medical imaging, yet many reported improvements lack statistical robustness: recent works have highlighted that small but significant performance gains are highly likely to be false positives. However, these analyses do not take \emph{underspecification} into account -- the fact that models achieving similar validation scores may behave differently on unseen data due to random initialization or training dynamics. Here, we extend a recent statistical framework modeling false outperformance claims to include underspecification as an additional variance component. Our simulations demonstrate that even modest seed variability ($\sim1\%$) substantially increases the evidence required to support superiority claims. Our findings underscore the need for explicit modeling of training variance when validating medical imaging systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SKGE: Spherical Knowledge Graph Embedding with Geometric Regularization</title>
<link>https://arxiv.org/abs/2511.02460</link>
<guid>https://arxiv.org/abs/2511.02460</guid>
<content:encoded><![CDATA[
arXiv:2511.02460v1 Announce Type: new 
Abstract: Knowledge graph embedding (KGE) has become a fundamental technique for representation learning on multi-relational data. Many seminal models, such as TransE, operate in an unbounded Euclidean space, which presents inherent limitations in modeling complex relations and can lead to inefficient training. In this paper, we propose Spherical Knowledge Graph Embedding (SKGE), a model that challenges this paradigm by constraining entity representations to a compact manifold: a hypersphere. SKGE employs a learnable, non-linear Spherization Layer to map entities onto the sphere and interprets relations as a hybrid translate-then-project transformation. Through extensive experiments on three benchmark datasets, FB15k-237, CoDEx-S, and CoDEx-M, we demonstrate that SKGE consistently and significantly outperforms its strong Euclidean counterpart, TransE, particularly on large-scale benchmarks such as FB15k-237 and CoDEx-M, demonstrating the efficacy of the spherical geometric prior. We provide an in-depth analysis to reveal the sources of this advantage, showing that this geometric constraint acts as a powerful regularizer, leading to comprehensive performance gains across all relation types. More fundamentally, we prove that the spherical geometry creates an "inherently hard negative sampling" environment, naturally eliminating trivial negatives and forcing the model to learn more robust and semantically coherent representations. Our findings compellingly demonstrate that the choice of manifold is not merely an implementation detail but a fundamental design principle, advocating for geometric priors as a cornerstone for designing the next generation of powerful and stable KGE models.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NOWS: Neural Operator Warm Starts for Accelerating Iterative Solvers</title>
<link>https://arxiv.org/abs/2511.02481</link>
<guid>https://arxiv.org/abs/2511.02481</guid>
<content:encoded><![CDATA[
arXiv:2511.02481v1 Announce Type: new 
Abstract: Partial differential equations (PDEs) underpin quantitative descriptions across the physical sciences and engineering, yet high-fidelity simulation remains a major computational bottleneck for many-query, real-time, and design tasks. Data-driven surrogates can be strikingly fast but are often unreliable when applied outside their training distribution. Here we introduce Neural Operator Warm Starts (NOWS), a hybrid strategy that harnesses learned solution operators to accelerate classical iterative solvers by producing high-quality initial guesses for Krylov methods such as conjugate gradient and GMRES. NOWS leaves existing discretizations and solver infrastructures intact, integrating seamlessly with finite-difference, finite-element, isogeometric analysis, finite volume method, etc. Across our benchmarks, the learned initialization consistently reduces iteration counts and end-to-end runtime, resulting in a reduction of the computational time of up to 90 %, while preserving the stability and convergence guarantees of the underlying numerical algorithms. By combining the rapid inference of neural operators with the rigor of traditional solvers, NOWS provides a practical and trustworthy approach to accelerate high-fidelity PDE simulations.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring</title>
<link>https://arxiv.org/abs/2511.02490</link>
<guid>https://arxiv.org/abs/2511.02490</guid>
<content:encoded><![CDATA[
arXiv:2511.02490v1 Announce Type: new 
Abstract: As the global burden of Alzheimer's disease (AD) continues to grow, early and accurate detection has become increasingly critical, especially in regions with limited access to advanced diagnostic tools. We propose BRAINS (Biomedical Retrieval-Augmented Intelligence for Neurodegeneration Screening) to address this challenge. This novel system harnesses the powerful reasoning capabilities of Large Language Models (LLMs) for Alzheimer's detection and monitoring. BRAINS features a dual-module architecture: a cognitive diagnostic module and a case-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on cognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain volume metrics -- to perform structured assessments of Alzheimer's risk. Meanwhile, the Case Retrieval Module encodes patient profiles into latent representations and retrieves similar cases from a curated knowledge base. These auxiliary cases are fused with the input profile via a Case Fusion Layer to enhance contextual understanding. The combined representation is then processed with clinical prompts for inference. Evaluations on real-world datasets demonstrate BRAINS effectiveness in classifying disease severity and identifying early signs of cognitive decline. This system not only shows strong potential as an assistive tool for scalable, explainable, and early-stage Alzheimer's disease detection, but also offers hope for future applications in the field.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Geometric Information Bottleneck: Learning the Shape of Understanding</title>
<link>https://arxiv.org/abs/2511.02496</link>
<guid>https://arxiv.org/abs/2511.02496</guid>
<content:encoded><![CDATA[
arXiv:2511.02496v1 Announce Type: new 
Abstract: We propose a unified information-geometric framework that formalizes understanding in learning as a trade-off between informativeness and geometric simplicity. An encoder phi is evaluated by U(phi) = I(phi(X); Y) - beta * C(phi), where C(phi) penalizes curvature and intrinsic dimensionality, enforcing smooth, low-complexity manifolds. Under mild manifold and regularity assumptions, we derive non-asymptotic bounds showing that generalization error scales with intrinsic dimension while curvature controls approximation stability, directly linking geometry to sample efficiency. To operationalize this theory, we introduce the Variational Geometric Information Bottleneck (V-GIB), a variational estimator that unifies mutual-information compression and curvature regularization through tractable geometric proxies such as the Hutchinson trace, Jacobian norms, and local PCA. Experiments across synthetic manifolds, few-shot settings, and real-world datasets (Fashion-MNIST, CIFAR-10) reveal a robust information-geometry Pareto frontier, stable estimators, and substantial gains in interpretive efficiency. Fractional-data experiments on CIFAR-10 confirm that curvature-aware encoders maintain predictive power under data scarcity, validating the predicted efficiency-curvature law. Overall, V-GIB provides a principled and measurable route to representations that are geometrically coherent, data-efficient, and aligned with human-understandable structure.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An End-to-End Learning Approach for Solving Capacitated Location-Routing Problems</title>
<link>https://arxiv.org/abs/2511.02525</link>
<guid>https://arxiv.org/abs/2511.02525</guid>
<content:encoded><![CDATA[
arXiv:2511.02525v1 Announce Type: new 
Abstract: The capacitated location-routing problems (CLRPs) are classical problems in combinatorial optimization, which require simultaneously making location and routing decisions. In CLRPs, the complex constraints and the intricate relationships between various decisions make the problem challenging to solve. With the emergence of deep reinforcement learning (DRL), it has been extensively applied to address the vehicle routing problem and its variants, while the research related to CLRPs still needs to be explored. In this paper, we propose the DRL with heterogeneous query (DRLHQ) to solve CLRP and open CLRP (OCLRP), respectively. We are the first to propose an end-to-end learning approach for CLRPs, following the encoder-decoder structure. In particular, we reformulate the CLRPs as a markov decision process tailored to various decisions, a general modeling framework that can be adapted to other DRL-based methods. To better handle the interdependency across location and routing decisions, we also introduce a novel heterogeneous querying attention mechanism designed to adapt dynamically to various decision-making stages. Experimental results on both synthetic and benchmark datasets demonstrate superior solution quality and better generalization performance of our proposed approach over representative traditional and DRL-based baselines in solving both CLRP and OCLRP.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Graph Neural Networks for Healthcare</title>
<link>https://arxiv.org/abs/2511.02531</link>
<guid>https://arxiv.org/abs/2511.02531</guid>
<content:encoded><![CDATA[
arXiv:2511.02531v1 Announce Type: new 
Abstract: Healthcare artificial intelligence systems routinely fail when deployed across institutions, with documented performance drops and perpetuation of discriminatory patterns embedded in historical data. This brittleness stems, in part, from learning statistical associations rather than causal mechanisms. Causal graph neural networks address this triple crisis of distribution shift, discrimination, and inscrutability by combining graph-based representations of biomedical data with causal inference principles to learn invariant mechanisms rather than spurious correlations. This Review examines methodological foundations spanning structural causal models, disentangled causal representation learning, and techniques for interventional prediction and counterfactual reasoning on graphs. We analyse applications demonstrating clinical value across psychiatric diagnosis through brain network analysis, cancer subtyping via multi-omics causal integration, continuous physiological monitoring with mechanistic interpretation, and drug recommendation correcting prescription bias. These advances establish foundations for patient-specific Causal Digital Twins, enabling in silico clinical experimentation, with integration of large language models for hypothesis generation and causal graph neural networks for mechanistic validation. Substantial barriers remain, including computational requirements precluding real-time deployment, validation challenges demanding multi-modal evidence triangulation beyond cross-validation, and risks of causal-washing where methods employ causal terminology without rigorous evidentiary support. We propose tiered frameworks distinguishing causally-inspired architectures from causally-validated discoveries and identify critical research priorities making causal rather than purely associational claims.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rawlsian many-to-one matching with non-linear utility</title>
<link>https://arxiv.org/abs/2511.02533</link>
<guid>https://arxiv.org/abs/2511.02533</guid>
<content:encoded><![CDATA[
arXiv:2511.02533v1 Announce Type: new 
Abstract: We study a many-to-one matching problem, such as the college admission problem, where each college can admit multiple students. Unlike classical models, colleges evaluate sets of students through non-linear utility functions that capture diversity between them. In this setting, we show that classical stable matchings may fail to exist. To address this, we propose alternative solution concepts based on Rawlsian fairness, aiming to maximize the minimum utility across colleges. We design both deterministic and stochastic algorithms that iteratively improve the outcome of the worst-off college, offering a practical approach to fair allocation when stability cannot be guaranteed.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theoretical Guarantees for Causal Discovery on Large Random Graphs</title>
<link>https://arxiv.org/abs/2511.02536</link>
<guid>https://arxiv.org/abs/2511.02536</guid>
<content:encoded><![CDATA[
arXiv:2511.02536v1 Announce Type: new 
Abstract: We investigate theoretical guarantees for the false-negative rate (FNR) -- the fraction of true causal edges whose orientation is not recovered, under single-variable random interventions and an $\epsilon$-interventional faithfulness assumption that accommodates latent confounding. For sparse Erd\H{o}s--R\'enyi directed acyclic graphs, where the edge probability scales as $p_e = \Theta(1/d)$, we show that the FNR concentrates around its mean at rate $O(\frac{\log d}{\sqrt d})$, implying that large deviations above the expected error become exponentially unlikely as dimensionality increases. This concentration ensures that derived upper bounds hold with high probability in large-scale settings. Extending the analysis to generalized Barab\'asi--Albert graphs reveals an even stronger phenomenon: when the degree exponent satisfies $\gamma > 3$, the deviation width scales as $O(d^{\beta - \frac{1}{2}})$ with $\beta = 1/(\gamma - 1) < \frac{1}{2}$, and hence vanishes in the limit. This demonstrates that realistic scale-free topologies intrinsically regularize causal discovery, reducing variability in orientation error. These finite-dimension results provide the first dimension-adaptive, faithfulness-robust guarantees for causal structure recovery, and challenge the intuition that high dimensionality and network heterogeneity necessarily hinder accurate discovery. Our simulation results corroborate these theoretical predictions, showing that the FNR indeed concentrates and often vanishes in practice as dimensionality grows.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02567</link>
<guid>https://arxiv.org/abs/2511.02567</guid>
<content:encoded><![CDATA[
arXiv:2511.02567v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) suffers from extrapolation errors induced by out-of-distribution (OOD) actions. To address this, offline RL algorithms typically impose constraints on action selection, which can be systematically categorized into density, support, and sample constraints. However, we show that each category has inherent limitations: density and sample constraints tend to be overly conservative in many scenarios, while the support constraint, though least restrictive, faces challenges in accurately modeling the behavior policy. To overcome these limitations, we propose a new neighborhood constraint that restricts action selection in the Bellman target to the union of neighborhoods of dataset actions. Theoretically, the constraint not only bounds extrapolation errors and distribution shift under certain conditions, but also approximates the support constraint without requiring behavior policy modeling. Moreover, it retains substantial flexibility and enables pointwise conservatism by adapting the neighborhood radius for each data point. In practice, we employ data quality as the adaptation criterion and design an adaptive neighborhood constraint. Building on an efficient bilevel optimization framework, we develop a simple yet effective algorithm, Adaptive Neighborhood-constrained Q learning (ANQ), to perform Q learning with target actions satisfying this constraint. Empirically, ANQ achieves state-of-the-art performance on standard offline RL benchmarks and exhibits strong robustness in scenarios with noisy or limited data.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Priors in Bayesian Optimization for Hyperparameter Optimization</title>
<link>https://arxiv.org/abs/2511.02570</link>
<guid>https://arxiv.org/abs/2511.02570</guid>
<content:encoded><![CDATA[
arXiv:2511.02570v1 Announce Type: new 
Abstract: Hyperparameter optimization (HPO), for example, based on Bayesian optimization (BO), supports users in designing models well-suited for a given dataset. HPO has proven its effectiveness on several applications, ranging from classical machine learning for tabular data to deep neural networks for computer vision and transformers for natural language processing. However, HPO still sometimes lacks acceptance by machine learning experts due to its black-box nature and limited user control. Addressing this, first approaches have been proposed to initialize BO methods with expert knowledge. However, these approaches do not allow for online steering during the optimization process. In this paper, we introduce a novel method that enables repeated interventions to steer BO via user input, specifying expert knowledge and user preferences at runtime of the HPO process in the form of prior distributions. To this end, we generalize an existing method, $\pi$BO, preserving theoretical guarantees. We also introduce a misleading prior detection scheme, which allows protection against harmful user inputs. In our experimental evaluation, we demonstrate that our method can effectively incorporate multiple priors, leveraging informative priors, whereas misleading priors are reliably rejected or overcome. Thereby, we achieve competitiveness to unperturbed BO.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Directional-Clamp PPO</title>
<link>https://arxiv.org/abs/2511.02577</link>
<guid>https://arxiv.org/abs/2511.02577</guid>
<content:encoded><![CDATA[
arXiv:2511.02577v1 Announce Type: new 
Abstract: Proximal Policy Optimization (PPO) is widely regarded as one of the most successful deep reinforcement learning algorithms, known for its robustness and effectiveness across a range of problems.
  The PPO objective encourages the importance ratio between the current and behavior policies to move to the "right" direction -- starting from importance sampling ratios equal to 1, increasing the ratios for actions with positive advantages and decreasing those with negative advantages. A clipping function is introduced to prevent over-optimization when updating the importance ratio in these "right" direction regions. Many PPO variants have been proposed to extend its success, most of which modify the objective's behavior by altering the clipping in the "right" direction regions. However, due to randomness in the rollouts and stochasticity of the policy optimization, we observe that the ratios frequently move to the "wrong" direction during the PPO optimization. This is a key factor hindering the improvement of PPO, but it has been largely overlooked. To address this, we propose the Directional-Clamp PPO algorithm (DClamp-PPO), which further penalizes the actions going to the strict "wrong" direction regions, where the advantage is positive (negative) and importance ratio falls below (above) $1 - \beta$ ($1+\beta$),
  for a tunable parameter $\beta \in (0, 1)$. The penalty is by enforcing a steeper loss slope, i.e., a clamp, in those regions. We demonstrate that DClamp-PPO consistently outperforms PPO, as well as its variants, by focusing on modifying the objective's behavior in the "right" direction, across various MuJoCo environments, using different random seeds. The proposed method is shown, both theoretically and empirically, to better avoid "wrong" direction updates while keeping the importance ratio closer to 1.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Large Language Model for Corporate Credit Scoring</title>
<link>https://arxiv.org/abs/2511.02593</link>
<guid>https://arxiv.org/abs/2511.02593</guid>
<content:encoded><![CDATA[
arXiv:2511.02593v1 Announce Type: new 
Abstract: We introduce Omega^2, a Large Language Model-driven framework for corporate credit scoring that combines structured financial data with advanced machine learning to improve predictive reliability and interpretability. Our study evaluates Omega^2 on a multi-agency dataset of 7,800 corporate credit ratings drawn from Moody's, Standard & Poor's, Fitch, and Egan-Jones, each containing detailed firm-level financial indicators such as leverage, profitability, and liquidity ratios. The system integrates CatBoost, LightGBM, and XGBoost models optimized through Bayesian search under temporal validation to ensure forward-looking and reproducible results. Omega^2 achieved a mean test AUC above 0.93 across agencies, confirming its ability to generalize across rating systems and maintain temporal consistency. These results show that combining language-based reasoning with quantitative learning creates a transparent and institution-grade foundation for reliable corporate credit-risk assessment.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Network Interoperability Across Platforms</title>
<link>https://arxiv.org/abs/2511.02610</link>
<guid>https://arxiv.org/abs/2511.02610</guid>
<content:encoded><![CDATA[
arXiv:2511.02610v1 Announce Type: new 
Abstract: The development of smart systems (i.e., systems enhanced with AI components) has thrived thanks to the rapid advancements in neural networks (NNs). A wide range of libraries and frameworks have consequently emerged to support NN design and implementation. The choice depends on factors such as available functionalities, ease of use, documentation and community support. After adopting a given NN framework, organizations might later choose to switch to another if performance declines, requirements evolve, or new features are introduced. Unfortunately, migrating NN implementations across libraries is challenging due to the lack of migration approaches specifically tailored for NNs. This leads to increased time and effort to modernize NNs, as manual updates are necessary to avoid relying on outdated implementations and ensure compatibility with new features. In this paper, we propose an approach to automatically migrate neural network code across deep learning frameworks. Our method makes use of a pivot NN model to create an abstraction of the NN prior to migration. We validate our approach using two popular NN frameworks, namely PyTorch and TensorFlow. We also discuss the challenges of migrating code between the two frameworks and how they were approached in our method. Experimental evaluation on five NNs shows that our approach successfully migrates their code and produces NNs that are functionally equivalent to the originals. Artefacts from our work are available online.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Non-Adversarial Approach to Idempotent Generative Modelling</title>
<link>https://arxiv.org/abs/2511.02614</link>
<guid>https://arxiv.org/abs/2511.02614</guid>
<content:encoded><![CDATA[
arXiv:2511.02614v1 Announce Type: new 
Abstract: Idempotent Generative Networks (IGNs) are deep generative models that also function as local data manifold projectors, mapping arbitrary inputs back onto the manifold. They are trained to act as identity operators on the data and as idempotent operators off the data manifold. However, IGNs suffer from mode collapse, mode dropping, and training instability due to their objectives, which contain adversarial components and can cause the model to cover the data manifold only partially -- an issue shared with generative adversarial networks. We introduce Non-Adversarial Idempotent Generative Networks (NAIGNs) to address these issues. Our loss function combines reconstruction with the non-adversarial generative objective of Implicit Maximum Likelihood Estimation (IMLE). This improves on IGN's ability to restore corrupted data and generate new samples that closely match the data distribution. We moreover demonstrate that NAIGNs implicitly learn the distance field to the data manifold, as well as an energy-based model.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recursively Enumerably Representable Classes and Computable Versions of the Fundamental Theorem of Statistical Learning</title>
<link>https://arxiv.org/abs/2511.02644</link>
<guid>https://arxiv.org/abs/2511.02644</guid>
<content:encoded><![CDATA[
arXiv:2511.02644v1 Announce Type: new 
Abstract: We study computable probably approximately correct (CPAC) learning, where learners are required to be computable functions. It had been previously observed that the Fundamental Theorem of Statistical Learning, which characterizes PAC learnability by finiteness of the Vapnik-Chervonenkis (VC-)dimension, no longer holds in this framework. Recent works recovered analogs of the Fundamental Theorem in the computable setting, for instance by introducing an effective VC-dimension. Guided by this, we investigate the connection between CPAC learning and recursively enumerable representable (RER) classes, whose members can be algorithmically listed. Our results show that the effective VC-dimensions can take arbitrary values above the traditional one, even for RER classes, which creates a whole family of (non-)examples for various notions of CPAC learning. Yet the two dimensions coincide for classes satisfying sufficiently strong notions of CPAC learning. We then observe that CPAC learnability can also be characterized via containment of RER classes that realize the same samples. Furthermore, it is shown that CPAC learnable classes satisfying a unique identification property are necessarily RER. Finally, we establish that agnostic learnability can be guaranteed for RER classes, by considering the relaxed notion of nonuniform CPAC learning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural-gas storage modelling by deep reinforcement learning</title>
<link>https://arxiv.org/abs/2511.02646</link>
<guid>https://arxiv.org/abs/2511.02646</guid>
<content:encoded><![CDATA[
arXiv:2511.02646v1 Announce Type: new 
Abstract: We introduce GasRL, a simulator that couples a calibrated representation of the natural gas market with a model of storage-operator policies trained with deep reinforcement learning (RL). We use it to analyse how optimal stockpile management affects equilibrium prices and the dynamics of demand and supply. We test various RL algorithms and find that Soft Actor Critic (SAC) exhibits superior performance in the GasRL environment: multiple objectives of storage operators - including profitability, robust market clearing and price stabilisation - are successfully achieved. Moreover, the equilibrium price dynamics induced by SAC-derived optimal policies have characteristics, such as volatility and seasonality, that closely match those of real-world prices. Remarkably, this adherence to the historical distribution of prices is obtained without explicitly calibrating the model to price data. We show how the simulator can be used to assess the effects of EU-mandated minimum storage thresholds. We find that such thresholds have a positive effect on market resilience against unanticipated shifts in the distribution of supply shocks. For example, with unusually large shocks, market disruptions are averted more often if a threshold is in place.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Apriel-H1: Towards Efficient Enterprise Reasoning Models</title>
<link>https://arxiv.org/abs/2511.02651</link>
<guid>https://arxiv.org/abs/2511.02651</guid>
<content:encoded><![CDATA[
arXiv:2511.02651v1 Announce Type: new 
Abstract: Large Language Models (LLMs) achieve remarkable reasoning capabilities through transformer architectures with attention mechanisms. However, transformers suffer from quadratic time and memory complexity in the attention module (MHA) and require caching key-value states during inference, which severely limits throughput and scalability. High inference throughput is critical for agentic tasks, long-context reasoning, efficient deployment under high request loads, and more efficient test-time compute scaling.
  State Space Models (SSMs) such as Mamba offer a promising alternative with linear inference complexity and a constant memory footprint via recurrent computation with fixed-size hidden states. In this technical report we introduce the Apriel-H1 family of hybrid LLMs that combine transformer attention and SSM sequence mixers for efficient reasoning at 15B model size. These models are obtained through incremental distillation from a pretrained reasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing less critical attention layers with linear Mamba blocks.
  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with different SSM-to-MHA ratios and analyse how reasoning performance degrades as more Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant of Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces, achieving over 2x higher inference throughput when deployed in the production-ready vLLM environment, with minimal degradation in reasoning performance. This shows that distilled hybrid SSM-Transformer architectures can deliver substantial efficiency gains over the pretrained transformer equivalent without substantially compromising the reasoning quality.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nesterov-Accelerated Robust Federated Learning Over Byzantine Adversaries</title>
<link>https://arxiv.org/abs/2511.02657</link>
<guid>https://arxiv.org/abs/2511.02657</guid>
<content:encoded><![CDATA[
arXiv:2511.02657v1 Announce Type: new 
Abstract: We investigate robust federated learning, where a group of workers collaboratively train a shared model under the orchestration of a central server in the presence of Byzantine adversaries capable of arbitrary and potentially malicious behaviors. To simultaneously enhance communication efficiency and robustness against such adversaries, we propose a Byzantine-resilient Nesterov-Accelerated Federated Learning (Byrd-NAFL) algorithm. Byrd-NAFL seamlessly integrates Nesterov's momentum into the federated learning process alongside Byzantine-resilient aggregation rules to achieve fast and safeguarding convergence against gradient corruption. We establish a finite-time convergence guarantee for Byrd-NAFL under non-convex and smooth loss functions with relaxed assumption on the aggregated gradients. Extensive numerical experiments validate the effectiveness of Byrd-NAFL and demonstrate the superiority over existing benchmarks in terms of convergence speed, accuracy, and resilience to diverse Byzantine attack strategies.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Situ Training of Implicit Neural Compressors for Scientific Simulations via Sketch-Based Regularization</title>
<link>https://arxiv.org/abs/2511.02659</link>
<guid>https://arxiv.org/abs/2511.02659</guid>
<content:encoded><![CDATA[
arXiv:2511.02659v1 Announce Type: new 
Abstract: Focusing on implicit neural representations, we present a novel in situ training protocol that employs limited memory buffers of full and sketched data samples, where the sketched data are leveraged to prevent catastrophic forgetting. The theoretical motivation for our use of sketching as a regularizer is presented via a simple Johnson-Lindenstrauss-informed result. While our methods may be of wider interest in the field of continual learning, we specifically target in situ neural compression using implicit neural representation-based hypernetworks. We evaluate our method on a variety of complex simulation data in two and three dimensions, over long time horizons, and across unstructured grids and non-Cartesian geometries. On these tasks, we show strong reconstruction performance at high compression rates. Most importantly, we demonstrate that sketching enables the presented in situ scheme to approximately match the performance of the equivalent offline method.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Evaluation and Neural Models for Compositional Generalization</title>
<link>https://arxiv.org/abs/2511.02667</link>
<guid>https://arxiv.org/abs/2511.02667</guid>
<content:encoded><![CDATA[
arXiv:2511.02667v1 Announce Type: new 
Abstract: Compositional generalization-a key open challenge in modern machine learning-requires models to predict unknown combinations of known concepts. However, assessing compositional generalization remains a fundamental challenge due to the lack of standardized evaluation protocols and the limitations of current benchmarks, which often favor efficiency over rigor. At the same time, general-purpose vision architectures lack the necessary inductive biases, and existing approaches to endow them compromise scalability. As a remedy, this paper introduces: 1) a rigorous evaluation framework that unifies and extends previous approaches while reducing computational requirements from combinatorial to constant; 2) an extensive and modern evaluation on the status of compositional generalization in supervised vision backbones, training more than 5000 models; 3) Attribute Invariant Networks, a class of models establishing a new Pareto frontier in compositional generalization, achieving a 23.43% accuracy improvement over baselines while reducing parameter overhead from 600% to 16% compared to fully disentangled counterparts.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curriculum Design for Trajectory-Constrained Agent: Compressing Chain-of-Thought Tokens in LLMs</title>
<link>https://arxiv.org/abs/2511.02690</link>
<guid>https://arxiv.org/abs/2511.02690</guid>
<content:encoded><![CDATA[
arXiv:2511.02690v1 Announce Type: new 
Abstract: Training agents to operate under strict constraints during deployment, such as limited resource budgets or stringent safety requirements, presents significant challenges, especially when these constraints render the task complex. In this work, we propose a curriculum learning strategy that gradually tightens constraints during training, enabling the agent to incrementally master the deployment requirements. Inspired by self-paced learning techniques in unconstrained reinforcement learning (RL), our approach facilitates a smoother transition to challenging environments by initially training on simplified versions of the constraints and progressively introducing the full deployment conditions. We provide a theoretical analysis using an RL agent in a binary-tree Markov Decision Process (MDP) to demonstrate that our curriculum strategy can accelerate training relative to a baseline approach that imposes the trajectory constraints from the outset. Moreover, we empirically validate the effectiveness and generality of our method across both RL and large language model (LLM) agents in diverse settings, including a binary-tree MDP, a multi-task navigation domain, and a math reasoning task with two benchmarks. These results highlight the potential of curriculum design in enhancing the efficiency and performance of agents operating under complex trajectory constraints during deployment. Moreover, when applied to LLMs, our strategy enables compression of output chain-of-thought tokens, achieving a substantial inference speedup on consumer hardware, demonstrating its effectiveness for resource-constrained deployment.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Interpretability of Knowledge Tracing Models Support Teacher Decision Making?</title>
<link>https://arxiv.org/abs/2511.02718</link>
<guid>https://arxiv.org/abs/2511.02718</guid>
<content:encoded><![CDATA[
arXiv:2511.02718v1 Announce Type: new 
Abstract: Knowledge tracing (KT) models are a crucial basis for pedagogical decision-making, namely which task to select next for a learner and when to stop teaching a particular skill. Given the high stakes of pedagogical decisions, KT models are typically required to be interpretable, in the sense that they should implement an explicit model of human learning and provide explicit estimates of learners' abilities. However, to our knowledge, no study to date has investigated whether the interpretability of KT models actually helps human teachers to make teaching decisions. We address this gap. First, we perform a simulation study to show that, indeed, decisions based on interpretable KT models achieve mastery faster compared to decisions based on a non-interpretable model. Second, we repeat the study but ask $N=12$ human teachers to make the teaching decisions based on the information provided by KT models. As expected, teachers rate interpretable KT models higher in terms of usability and trustworthiness. However, the number of tasks needed until mastery hardly differs between KT models. This suggests that the relationship between model interpretability and teacher decisions is not straightforward: teachers do not solely rely on KT models to make decisions and further research is needed to investigate how learners and teachers actually understand and use KT models.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibration improves detection of mislabeled examples</title>
<link>https://arxiv.org/abs/2511.02738</link>
<guid>https://arxiv.org/abs/2511.02738</guid>
<content:encoded><![CDATA[
arXiv:2511.02738v1 Announce Type: new 
Abstract: Mislabeled data is a pervasive issue that undermines the performance of machine learning systems in real-world applications. An effective approach to mitigate this problem is to detect mislabeled instances and subject them to special treatment, such as filtering or relabeling. Automatic mislabeling detection methods typically rely on training a base machine learning model and then probing it for each instance to obtain a trust score that each provided label is genuine or incorrect. The properties of this base model are thus of paramount importance. In this paper, we investigate the impact of calibrating this model. Our empirical results show that using calibration methods improves the accuracy and robustness of mislabeled instance detection, providing a practical and effective solution for industrial applications.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free Finetuning of Large Language Models</title>
<link>https://arxiv.org/abs/2511.02757</link>
<guid>https://arxiv.org/abs/2511.02757</guid>
<content:encoded><![CDATA[
arXiv:2511.02757v1 Announce Type: new 
Abstract: Zeroth-order or derivative-free optimization (MeZO) is an attractive strategy for finetuning large language models (LLMs) because it eliminates the memory overhead of backpropagation. However, it converges slowly due to the inherent curse of dimensionality when searching for descent directions in the high-dimensional parameter space of billion-scale LLMs. We propose ConMeZO, a novel zeroth-order optimizer that accelerates convergence by adaptive directional sampling. Instead of drawing the direction uniformly at random, ConMeZO restricts the sampling to a cone centered around a momentum estimate. This concentrates the search in directions where the true gradient is more likely to lie and thus reduces the effect of high dimensions. We prove that ConMeZO achieves the same worst-case convergence rate as MeZO. Empirically, when finetuning LLMs on natural language tasks, ConMeZO is up to 2X faster than MeZO while retaining the low-memory footprint of zeroth-order methods.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Solo to Symphony: Orchestrating Multi-Agent Collaboration with Single-Agent Demos</title>
<link>https://arxiv.org/abs/2511.02762</link>
<guid>https://arxiv.org/abs/2511.02762</guid>
<content:encoded><![CDATA[
arXiv:2511.02762v1 Announce Type: new 
Abstract: Training a team of agents from scratch in multi-agent reinforcement learning (MARL) is highly inefficient, much like asking beginners to play a symphony together without first practicing solo. Existing methods, such as offline or transferable MARL, can ease this burden, but they still rely on costly multi-agent data, which often becomes the bottleneck. In contrast, solo experiences are far easier to obtain in many important scenarios, e.g., collaborative coding, household cooperation, and search-and-rescue. To unlock their potential, we propose Solo-to-Collaborative RL (SoCo), a framework that transfers solo knowledge into cooperative learning. SoCo first pretrains a shared solo policy from solo demonstrations, then adapts it for cooperation during multi-agent training through a policy fusion mechanism that combines an MoE-like gating selector and an action editor. Experiments across diverse cooperative tasks show that SoCo significantly boosts the training efficiency and performance of backbone algorithms. These results demonstrate that solo demonstrations provide a scalable and effective complement to multi-agent data, making cooperative learning more practical and broadly applicable.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VecComp: Vector Computing via MIMO Digital Over-the-Air Computation</title>
<link>https://arxiv.org/abs/2511.02765</link>
<guid>https://arxiv.org/abs/2511.02765</guid>
<content:encoded><![CDATA[
arXiv:2511.02765v1 Announce Type: new 
Abstract: Recently, the ChannelComp framework has proposed digital over-the-air computation by designing digital modulations that enable the computation of arbitrary functions. Unlike traditional analog over-the-air computation, which is restricted to nomographic functions, ChannelComp enables a broader range of computational tasks while maintaining compatibility with digital communication systems. This framework is intended for applications that favor local information processing over the mere acquisition of data. However, ChannelComp is currently designed for scalar function computation, while numerous data-centric applications necessitate vector-based computations, and it is susceptible to channel fading. In this work, we introduce a generalization of the ChannelComp framework, called VecComp, by integrating ChannelComp with multiple-antenna technology. This generalization not only enables vector function computation but also ensures scalability in the computational complexity, which increases only linearly with the vector dimension. As such, VecComp remains computationally efficient and robust against channel impairments, making it suitable for high-dimensional, data-centric applications. We establish a non-asymptotic upper bound on the mean squared error of VecComp, affirming its computation efficiency under fading channel conditions. Numerical experiments show the effectiveness of VecComp in improving the computation of vector functions and fading compensation over noisy and fading multiple-access channels.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation</title>
<link>https://arxiv.org/abs/2511.02769</link>
<guid>https://arxiv.org/abs/2511.02769</guid>
<content:encoded><![CDATA[
arXiv:2511.02769v1 Announce Type: new 
Abstract: The chemical space of drug-like molecules is vast, motivating the development of generative models that must learn broad chemical distributions, enable conditional generation by capturing structure-property representations, and provide fast molecular generation. Meeting the objectives depends on modeling choices, including the probabilistic modeling approach, the conditional generative formulation, the architecture, and the molecular input representation. To address the challenges, we present STAR-VAE (Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder), a scalable latent-variable framework with a Transformer encoder and an autoregressive Transformer decoder. It is trained on 79 million drug-like molecules from PubChem, using SELFIES to guarantee syntactic validity. The latent-variable formulation enables conditional generation: a property predictor supplies a conditioning signal that is applied consistently to the latent prior, the inference network, and the decoder. Our contributions are: (i) a Transformer-based latent-variable encoder-decoder model trained on SELFIES representations; (ii) a principled conditional latent-variable formulation for property-guided generation; and (iii) efficient finetuning with low-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation with limited property and activity data. On the GuacaMol and MOSES benchmarks, our approach matches or exceeds baselines, and latent-space analyses reveal smooth, semantically structured representations that support both unconditional exploration and property-aware generation. On the Tartarus benchmarks, the conditional model shifts docking-score distributions toward stronger predicted binding. These results suggest that a modernized, scale-appropriate VAE remains competitive for molecular generation when paired with principled conditioning and parameter-efficient finetuning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adam Reduces a Unique Form of Sharpness: Theoretical Insights Near the Minimizer Manifold</title>
<link>https://arxiv.org/abs/2511.02773</link>
<guid>https://arxiv.org/abs/2511.02773</guid>
<content:encoded><![CDATA[
arXiv:2511.02773v1 Announce Type: new 
Abstract: Despite the popularity of the Adam optimizer in practice, most theoretical analyses study Stochastic Gradient Descent (SGD) as a proxy for Adam, and little is known about how the solutions found by Adam differ. In this paper, we show that Adam implicitly reduces a unique form of sharpness measure shaped by its adaptive updates, leading to qualitatively different solutions from SGD. More specifically, when the training loss is small, Adam wanders around the manifold of minimizers and takes semi-gradients to minimize this sharpness measure in an adaptive manner, a behavior we rigorously characterize through a continuous-time approximation using stochastic differential equations. We further demonstrate how this behavior differs from that of SGD in a well-studied setting: when training overparameterized models with label noise, SGD has been shown to minimize the trace of the Hessian matrix, $\tr(\mH)$, whereas we prove that Adam minimizes $\tr(\Diag(\mH)^{1/2})$ instead. In solving sparse linear regression with diagonal linear networks, this distinction enables Adam to achieve better sparsity and generalization than SGD. Finally, our analysis framework extends beyond Adam to a broad class of adaptive gradient methods, including RMSProp, Adam-mini, Adalayer and Shampoo, and provides a unified perspective on how these adaptive optimizers reduce sharpness, which we hope will offer insights for future optimizer design.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Federated Learning Privacy with QUBO</title>
<link>https://arxiv.org/abs/2511.02785</link>
<guid>https://arxiv.org/abs/2511.02785</guid>
<content:encoded><![CDATA[
arXiv:2511.02785v1 Announce Type: new 
Abstract: Federated learning (FL) is a widely used method for training machine learning (ML) models in a scalable way while preserving privacy (i.e., without centralizing raw data). Prior research shows that the risk of exposing sensitive data increases cumulatively as the number of iterations where a client's updates are included in the aggregated model increase. Attackers can launch membership inference attacks (MIA; deciding whether a sample or client participated), property inference attacks (PIA; inferring attributes of a client's data), and model inversion attacks (MI; reconstructing inputs), thereby inferring client-specific attributes and, in some cases, reconstructing inputs. In this paper, we mitigate risk by substantially reducing per client exposure using a quantum computing-inspired quadratic unconstrained binary optimization (QUBO) formulation that selects a small subset of client updates most relevant for each training round. In this work, we focus on two threat vectors: (i) information leakage by clients during training and (ii) adversaries who can query or obtain the global model. We assume a trusted central server and do not model server compromise. This method also assumes that the server has access to a validation/test set with global data distribution. Experiments on the MNIST dataset with 300 clients in 20 rounds showed a 95.2% per-round and 49% cumulative privacy exposure reduction, with 147 clients' updates never being used during training while maintaining in general the full-aggregation accuracy or even better. The method proved to be efficient at lower scale and more complex model as well. A CINIC-10 dataset-based experiment with 30 clients resulted in 82% per-round privacy improvement and 33% cumulative privacy.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs subtract numbers?</title>
<link>https://arxiv.org/abs/2511.02795</link>
<guid>https://arxiv.org/abs/2511.02795</guid>
<content:encoded><![CDATA[
arXiv:2511.02795v1 Announce Type: new 
Abstract: We present a systematic study of subtraction in large language models (LLMs). While prior benchmarks emphasize addition and multiplication, subtraction has received comparatively little attention despite being structurally distinct as a non-commutative operation. We evaluate eight pretrained LLMs spanning four families on addition and subtraction problems. Our experiments reveal that subtraction accuracy lags behind addition by a wide margin. We find that the errors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs frequently produce the correct magnitude but omit the negative sign. Probing analyses show that LLMs internally encode whether results should be negative, yet this information is often not reflected in generated outputs. We further test well-known techniques such as few-shot learning and instruction-tuning to see if they can improve the LLMs' performance. Our results suggest that while few-shot prompting yields modest gains, the instruction-tuned models achieve near-perfect accuracies in generating the negative sign. Together, these findings provide a clearer characterization of the limitations and recoverability of LLMs' arithmetic capabilities in subtraction.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast, Private, and Protected: Safeguarding Data Privacy and Defending Against Model Poisoning Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2511.02797</link>
<guid>https://arxiv.org/abs/2511.02797</guid>
<content:encoded><![CDATA[
arXiv:2511.02797v1 Announce Type: new 
Abstract: Federated Learning (FL) is a distributed training paradigm wherein participants collaborate to build a global model while ensuring the privacy of the involved data, which remains stored on participant devices. However, proposals aiming to ensure such privacy also make it challenging to protect against potential attackers seeking to compromise the training outcome. In this context, we present Fast, Private, and Protected (FPP), a novel approach that aims to safeguard federated training while enabling secure aggregation to preserve data privacy. This is accomplished by evaluating rounds using participants' assessments and enabling training recovery after an attack. FPP also employs a reputation-based mechanism to mitigate the participation of attackers. We created a dockerized environment to validate the performance of FPP compared to other approaches in the literature (FedAvg, Power-of-Choice, and aggregation via Trimmed Mean and Median). Our experiments demonstrate that FPP achieves a rapid convergence rate and can converge even in the presence of malicious participants performing model poisoning attacks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2511.02802</link>
<guid>https://arxiv.org/abs/2511.02802</guid>
<content:encoded><![CDATA[
arXiv:2511.02802v1 Announce Type: new 
Abstract: Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models. The library is open source and available at https://github.com/Lexsi-Labs/TabTune .
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing win strength in MLB win prediction models</title>
<link>https://arxiv.org/abs/2511.02815</link>
<guid>https://arxiv.org/abs/2511.02815</guid>
<content:encoded><![CDATA[
arXiv:2511.02815v1 Announce Type: new 
Abstract: In Major League Baseball, strategy and planning are major factors in determining the outcome of a game. Previous studies have aided this by building machine learning models for predicting the winning team of any given game. We extend this work by training a comprehensive set of machine learning models using a common dataset. In addition, we relate the win probabilities produced by these models to win strength as measured by score differential. In doing so we show that the most common machine learning models do indeed demonstrate a relationship between predicted win probability and the strength of the win. Finally, we analyze the results of using predicted win probabilities as a decision making mechanism on run-line betting. We demonstrate positive returns when utilizing appropriate betting strategies, and show that naive use of machine learning models for betting lead to significant loses.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoCrossBench: Cross-Band Generalization for Remote Sensing</title>
<link>https://arxiv.org/abs/2511.02831</link>
<guid>https://arxiv.org/abs/2511.02831</guid>
<content:encoded><![CDATA[
arXiv:2511.02831v1 Announce Type: new 
Abstract: The number and diversity of remote sensing satellites grows over time, while the vast majority of labeled data comes from older satellites. As the foundation models for Earth observation scale up, the cost of (re-)training to support new satellites grows too, so the generalization capabilities of the models towards new satellites become increasingly important. In this work we introduce GeoCrossBench, an extension of the popular GeoBench benchmark with a new evaluation protocol: it tests the in-distribution performance; generalization to satellites with no band overlap; and generalization to satellites with additional bands with respect to the training set. We also develop a self-supervised extension of ChannelViT, ChiViT, to improve its cross-satellite performance. First, we show that even the best foundation models for remote sensing (DOFA, TerraFM) do not outperform general purpose models like DINOv3 in the in-distribution setting. Second, when generalizing to new satellites with no band overlap, all models suffer 2-4x drop in performance, and ChiViT significantly outperforms the runner-up DINOv3. Third, the performance of all tested models drops on average by 5-25\% when given additional bands during test time. Finally, we show that fine-tuning just the last linear layer of these models using oracle labels from all bands can get relatively consistent performance across all satellites, highlighting that the benchmark is far from being saturated. We publicly release the code and the datasets to encourage the development of more future-proof remote sensing models with stronger cross-satellite generalization.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Good GRACEs: Principled Teacher Selection for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.02833</link>
<guid>https://arxiv.org/abs/2511.02833</guid>
<content:encoded><![CDATA[
arXiv:2511.02833v1 Announce Type: new 
Abstract: Knowledge distillation is an efficient strategy to use data generated by large "teacher" language models to train smaller capable "student" models, but selecting the optimal teacher for a specific student-task combination requires expensive trial-and-error. We propose a lightweight score called GRACE to quantify how effective a teacher will be for post-training a student model. GRACE measures distributional properties of the student's gradients without access to a verifier, teacher logits, teacher internals, or test data. From an information-theoretic perspective, GRACE connects to leave-one-out stability of gradient-based algorithms, which controls the generalization performance of the distilled students. On GSM8K and MATH, GRACE correlates strongly (up to 86% Spearman correlation) with the performance of the distilled LLaMA and OLMo students. In particular, training a student using the GRACE-selected teacher can improve the performance by up to 7.4% over naively using the best-performing teacher. Further, GRACE can provide guidance on crucial design choices in distillation, including (1) the best temperature to use when generating from the teacher, (2) the best teacher to use given a size constraint, and (3) the best teacher to use within a specific model family. Altogether, our findings demonstrate that GRACE can efficiently and effectively identify a strongly compatible teacher for a given student and provide fine-grained guidance on how to perform distillation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning phases with Quantum Monte Carlo simulation cell</title>
<link>https://arxiv.org/abs/2503.23098</link>
<guid>https://arxiv.org/abs/2503.23098</guid>
<content:encoded><![CDATA[
arXiv:2503.23098v3 Announce Type: cross 
Abstract: We propose the use of the ``spin-opstring", derived from Stochastic Series Expansion Quantum Monte Carlo (QMC) simulations as machine learning (ML) input data. It offers a compact, memory-efficient representation of QMC simulation cells, combining the initial state with an operator string that encodes the state's evolution through imaginary time. Using supervised ML, we demonstrate the input's effectiveness in capturing both conventional and topological phase transitions, and in a regression task to predict non-local observables. We also demonstrate the capability of spin-opstring data in transfer learning by training models on one quantum system and successfully predicting on another, as well as showing that models trained on smaller system sizes generalize well to larger ones. Importantly, we illustrate a clear advantage of spin-opstring over conventional spin configurations in the accurate prediction of a quantum phase transition. Finally, we show how the inherent structure of spin-opstring provides an elegant framework for the interpretability of ML predictions. Using two state-of-the-art interpretability techniques, Layer-wise Relevance Propagation and SHapley Additive exPlanations, we show that the ML models learn and rely on physically meaningful features from the input data. Together, these findings establish the spin-opstring as a broadly-applicable and interpretable input format for ML in quantum many-body physics.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Condition-Invariant fMRI Decoding of Speech Intelligibility with Deep State Space Model</title>
<link>https://arxiv.org/abs/2511.01868</link>
<guid>https://arxiv.org/abs/2511.01868</guid>
<content:encoded><![CDATA[
arXiv:2511.01868v1 Announce Type: cross 
Abstract: Clarifying the neural basis of speech intelligibility is critical for computational neuroscience and digital speech processing. Recent neuroimaging studies have shown that intelligibility modulates cortical activity beyond simple acoustics, primarily in the superior temporal and inferior frontal gyri. However, previous studies have been largely confined to clean speech, leaving it unclear whether the brain employs condition-invariant neural codes across diverse listening environments. To address this gap, we propose a novel architecture built upon a deep state space model for decoding intelligibility from fMRI signals, specifically tailored to their high-dimensional temporal structure. We present the first attempt to decode intelligibility across acoustically distinct conditions, showing our method significantly outperforms classical approaches. Furthermore, region-wise analysis highlights contributions from auditory, frontal, and parietal regions, and cross-condition transfer indicates the presence of condition-invariant neural codes, thereby advancing understanding of abstract linguistic representations in the brain.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BondBERT: What we learn when assigning sentiment in the bond market</title>
<link>https://arxiv.org/abs/2511.01869</link>
<guid>https://arxiv.org/abs/2511.01869</guid>
<content:encoded><![CDATA[
arXiv:2511.01869v1 Announce Type: cross 
Abstract: Bond markets respond differently to macroeconomic news compared to equity markets, yet most sentiment models, including FinBERT, are trained primarily on general financial or equity news data. This mismatch is important because bond prices often move in the opposite direction to economic optimism, making general or equity-based sentiment tools potentially misleading. In this paper, we introduce BondBERT, a transformer-based language model fine-tuned on bond-specific news. BondBERT can act as the perception and reasoning component of a financial decision-support agent, providing sentiment signals that integrate with forecasting models. It is a generalisable framework for adapting transformers to low-volatility, domain-inverse sentiment tasks by compiling and cleaning 30,000 UK bond market articles (2018--2025) for training, validation, and testing. We compare BondBERT's sentiment predictions against FinBERT, FinGPT, and Instruct-FinGPT using event-based correlation, up/down accuracy analyses, and LSTM forecasting across ten UK sovereign bonds. We find that BondBERT consistently produces positive correlations with bond returns, achieves higher alignment and forecasting accuracy than the three baseline models, with lower normalised RMSE and higher information coefficient. These results demonstrate that domain-specific sentiment adaptation better captures fixed income dynamics, bridging a gap between NLP advances and bond market analytics.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CytoNet: A Foundation Model for the Human Cerebral Cortex</title>
<link>https://arxiv.org/abs/2511.01870</link>
<guid>https://arxiv.org/abs/2511.01870</guid>
<content:encoded><![CDATA[
arXiv:2511.01870v1 Announce Type: cross 
Abstract: To study how the human brain works, we need to explore the organization of the cerebral cortex and its detailed cellular architecture. We introduce CytoNet, a foundation model that encodes high-resolution microscopic image patches of the cerebral cortex into highly expressive feature representations, enabling comprehensive brain analyses. CytoNet employs self-supervised learning using spatial proximity as a powerful training signal, without requiring manual labelling. The resulting features are anatomically sound and biologically relevant. They encode general aspects of cortical architecture and unique brain-specific traits. We demonstrate top-tier performance in tasks such as cortical area classification, cortical layer segmentation, cell morphology estimation, and unsupervised brain region mapping. As a foundation model, CytoNet offers a consistent framework for studying cortical microarchitecture, supporting analyses of its relationship with other structural and functional brain features, and paving the way for diverse neuroscientific investigations.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learned Cost Model for Placement on Reconfigurable Dataflow Hardware</title>
<link>https://arxiv.org/abs/2511.01872</link>
<guid>https://arxiv.org/abs/2511.01872</guid>
<content:encoded><![CDATA[
arXiv:2511.01872v1 Announce Type: cross 
Abstract: Mapping a dataflow-graph of an ML model onto a reconfigurable system is difficult, as different mappings have different throughputs and consume resource constraints differently. To solve this, a model to evaluate the throughput of mappings is necessary as measuring throughput completely is expensive. Many use a hand-designed analytical model, relying on proxy features or intuition, introducing error. We provide a Learned Approach that predicts throughput 31%-52% more accurately over a variety of graphs. In addition, our approach shows no accuracy degradation after removing performance annotations. We show that using this approach results in 5.6% faster compiled graphs.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effectiveness of High-Dimensional Distance Metrics on Solar Flare Time Series</title>
<link>https://arxiv.org/abs/2511.01873</link>
<guid>https://arxiv.org/abs/2511.01873</guid>
<content:encoded><![CDATA[
arXiv:2511.01873v1 Announce Type: cross 
Abstract: Solar-flare forecasting has been extensively researched yet remains an open problem. In this paper, we investigate the contributions of elastic distance measures for detecting patterns in the solar-flare dataset, SWAN-SF. We employ a simple $k$-medoids clustering algorithm to evaluate the effectiveness of advanced, high-dimensional distance metrics. Our results show that, despite thorough optimization, none of the elastic distances outperform Euclidean distance by a significant margin. We demonstrate that, although elastic measures have shown promise for univariate time series, when applied to the multivariate time series of SWAN-SF, characterized by the high stochasticity of solar activity, they effectively collapse to Euclidean distance. We conduct thousands of experiments and present both quantitative and qualitative evidence supporting this finding.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affordable EEG, Actionable Insights: An Open Dataset and Evaluation Framework for Epilepsy Patient Stratification</title>
<link>https://arxiv.org/abs/2511.01879</link>
<guid>https://arxiv.org/abs/2511.01879</guid>
<content:encoded><![CDATA[
arXiv:2511.01879v1 Announce Type: cross 
Abstract: Access to clinical multi-channel EEG remains limited in many regions worldwide. We present NEUROSKY-EPI, the first open dataset of single-channel, consumer-grade EEG for epilepsy, collected in a South Asian clinical setting along with rich contextual metadata. To explore its utility, we introduce EmbedCluster, a patient-stratification pipeline that transfers representations from EEGNet models trained on clinical data and enriches them with contextual autoencoder embeddings, followed by unsupervised clustering of patients based on EEG patterns. Results show that low-cost, single-channel data can support meaningful stratification. Beyond algorithmic performance, we emphasize human-centered concerns such as deployability in resource-constrained environments, interpretability for non-specialists, and safeguards for privacy, inclusivity, and bias. By releasing the dataset and code, we aim to catalyze interdisciplinary research across health technology, human-computer interaction, and machine learning, advancing the goal of affordable and actionable EEG-based epilepsy care.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mirror-Neuron Patterns in AI Alignment</title>
<link>https://arxiv.org/abs/2511.01885</link>
<guid>https://arxiv.org/abs/2511.01885</guid>
<content:encoded><![CDATA[
arXiv:2511.01885v1 Announce Type: cross 
Abstract: As artificial intelligence (AI) advances toward superhuman capabilities, aligning these systems with human values becomes increasingly critical. Current alignment strategies rely largely on externally specified constraints that may prove insufficient against future super-intelligent AI capable of circumventing top-down controls.
  This research investigates whether artificial neural networks (ANNs) can develop patterns analogous to biological mirror neurons cells that activate both when performing and observing actions, and how such patterns might contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in empathy, imitation, and social cognition in humans. The study therefore asks: (1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these patterns contribute to ethical and cooperative decision-making in AI systems?
  Using a novel Frog and Toad game framework designed to promote cooperative behaviors, we identify conditions under which mirror-neuron patterns emerge, evaluate their influence on action circuits, introduce the Checkpoint Mirror Neuron Index (CMNI) to quantify activation strength and consistency, and propose a theoretical framework for further study.
  Our findings indicate that appropriately scaled model capacities and self/other coupling foster shared neural representations in ANNs similar to biological mirror neurons. These empathy-like circuits support cooperative behavior and suggest that intrinsic motivations modeled through mirror-neuron dynamics could complement existing alignment techniques by embedding empathy-like mechanisms directly within AI architectures.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LGCC: Enhancing Flow Matching Based Text-Guided Image Editing with Local Gaussian Coupling and Context Consistency</title>
<link>https://arxiv.org/abs/2511.01894</link>
<guid>https://arxiv.org/abs/2511.01894</guid>
<content:encoded><![CDATA[
arXiv:2511.01894v1 Announce Type: cross 
Abstract: Recent advancements have demonstrated the great potential of flow matching-based Multimodal Large Language Models (MLLMs) in image editing. However, state-of-the-art works like BAGEL face limitations, including detail degradation, content inconsistency, and inefficiency due to their reliance on random noise initialization. To address these issues, we propose LGCC, a novel framework with two key components: Local Gaussian Noise Coupling (LGNC) and Content Consistency Loss (CCL). LGNC preserves spatial details by modeling target image embeddings and their locally perturbed counterparts as coupled pairs, while CCL ensures semantic alignment between edit instructions and image modifications, preventing unintended content removal. By integrating LGCC with the BAGEL pre-trained model via curriculum learning, we significantly reduce inference steps, improving local detail scores on I2EBench by 1.60% and overall scores by 0.53%. LGCC achieves 3x -- 5x speedup for lightweight editing and 2x for universal editing, requiring only 40% -- 50% of the inference time of BAGEL or Flux. These results demonstrate LGCC's ability to preserve detail, maintain contextual integrity, and enhance inference speed, offering a cost-efficient solution without compromising editing quality.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory</title>
<link>https://arxiv.org/abs/2511.01912</link>
<guid>https://arxiv.org/abs/2511.01912</guid>
<content:encoded><![CDATA[
arXiv:2511.01912v1 Announce Type: cross 
Abstract: Planning has been a cornerstone of artificial intelligence for solving complex problems, and recent progress in LLM-based multi-agent frameworks have begun to extend this capability. However, the role of human-like memory within these frameworks remains largely unexplored. Understanding how agents coordinate through memory is critical for natural language planning, where iterative reasoning, constraint tracking, and error correction drive the success. Inspired by working memory model in cognitive psychology, we present EvoMem, a multi-agent framework built on a dual-evolving memory mechanism. The framework consists of three agents (Constraint Extractor, Verifier, and Actor) and two memory modules: Constraint Memory (CMem), which evolves across queries by storing task-specific rules and constraints while remains fixed within a query, and Query-feedback Memory (QMem), which evolves within a query by accumulating feedback across iterations for solution refinement. Both memory modules are reset at the end of each query session. Evaluations on trip planning, meeting planning, and calendar scheduling show consistent performance improvements, highlighting the effectiveness of EvoMem. This success underscores the importance of memory in enhancing multi-agent planning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delta-learned force fields for nonbonded interactions: Addressing the strength mismatch between covalent-nonbonded interaction for global models</title>
<link>https://arxiv.org/abs/2511.01913</link>
<guid>https://arxiv.org/abs/2511.01913</guid>
<content:encoded><![CDATA[
arXiv:2511.01913v1 Announce Type: cross 
Abstract: Noncovalent interactions--vdW dispersion, hydrogen/halogen bonding, ion-$\pi$, and $\pi$-stacking--govern structure, dynamics, and emergent phenomena in materials and molecular systems, yet accurately learning them alongside covalent forces remains a core challenge for machine-learned force fields (MLFFs). This challenge is acute for global models that use Coulomb-matrix (CM) descriptors compared under Euclidean/Frobenius metrics in multifragment settings. We show that the mismatch between predominantly covalent force labels and the CM's overrepresentation of intermolecular features biases single-model training and degrades force-field fidelity. To address this, we introduce \textit{$\Delta$-sGDML}, a scale-aware formulation within the sGDML framework that explicitly decouples intra- and intermolecular physics by training fragment-specific models alongside a dedicated binding model, then composing them at inference. Across benzene dimers, host-guest complexes (C$_{60}$@buckycatcher, NO$_3^-$@i-corona[6]arene), benzene-water, and benzene-Na$^+$, \mbox{$\Delta$-sGDML} delivers consistent gains over a single global model, with fragment-resolved force-error reductions up to \textbf{75\%}, without loss of energy accuracy. Furthermore, molecular-dynamics simulations further confirm that the $\Delta$-model yields a reliable force field for C$_{60}$@buckycatcher, producing stable trajectories across a wide range of temperatures (10-400~K), unlike the single global model, which loses stability above $\sim$200~K. The method offers a practical route to homogenize per-fragment errors and recover reliable noncovalent physics in global MLFFs.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Bayesian inference in PTA data analysis: importance nested sampling with Normalizing Flows</title>
<link>https://arxiv.org/abs/2511.01958</link>
<guid>https://arxiv.org/abs/2511.01958</guid>
<content:encoded><![CDATA[
arXiv:2511.01958v1 Announce Type: cross 
Abstract: We present a detailed study of Bayesian inference workflows for pulsar timing array data with a focus on enhancing efficiency, robustness and speed through the use of normalizing flow-based nested sampling. Building on the Enterprise framework, we integrate the i-nessai sampler and benchmark its performance on realistic, simulated datasets. We analyze its computational scaling and stability, and show that it achieves accurate posteriors and reliable evidence estimates with substantially reduced runtime, by up to three orders of magnitude depending on the dataset configuration, with respect to conventional single-core parallel-tempering MCMC analyses. These results highlight the potential of flow-based nested sampling to accelerate PTA analyses while preserving the quality of the inference.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addressing prior dependence in hierarchical Bayesian modeling for PTA data analysis II: Noise and SGWB inference through parameter decorrelation</title>
<link>https://arxiv.org/abs/2511.01959</link>
<guid>https://arxiv.org/abs/2511.01959</guid>
<content:encoded><![CDATA[
arXiv:2511.01959v1 Announce Type: cross 
Abstract: Pulsar Timing Arrays provide a powerful framework to measure low-frequency gravitational waves, but accuracy and robustness of the results are challenged by complex noise processes that must be accurately modeled. Standard PTA analyses assign fixed uniform noise priors to each pulsar, an approach that can introduce systematic biases when combining the array. To overcome this limitation, we adopt a hierarchical Bayesian modeling strategy in which noise priors are parametrized by higher-level hyperparameters. We further address the challenge posed by the correlations between hyperparameters and physical noise parameters, focusing on those describing red noise and dispersion measure variations. To decorrelate these quantities, we introduce an orthogonal reparametrization of the hierarchical model implemented with Normalizing Flows. We also employ i-nessai, a flow-guided nested sampler, to efficiently explore the resulting higher-dimensional parameter space. We apply our method to a minimal 3-pulsar case study, performing a simultaneous inference of noise and SGWB parameters. Despite the limited dataset, the results consistently show that the hierarchical treatment constrains the noise parameters more tightly and partially alleviates the red-noise-SGWB degeneracy, while the orthogonal reparametrization further enhances parameter independence without affecting the correlations intrinsic to the power-law modeling of the physical processes involved.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stability of mixed-state phases under weak decoherence</title>
<link>https://arxiv.org/abs/2511.01976</link>
<guid>https://arxiv.org/abs/2511.01976</guid>
<content:encoded><![CDATA[
arXiv:2511.01976v1 Announce Type: cross 
Abstract: We prove that the Gibbs states of classical, and commuting-Pauli, Hamiltonians are stable under weak local decoherence: i.e., we show that the effect of the decoherence can be locally reversed. In particular, our conclusions apply to finite-temperature equilibrium critical points and ordered low-temperature phases. In these systems the unconditional spatio-temporal correlations are long-range, and local (e.g., Metropolis) dynamics exhibits critical slowing down. Nevertheless, our results imply the existence of local "decoders" that undo the decoherence, when the decoherence strength is below a critical value. An implication of these results is that thermally stable quantum memories have a threshold against decoherence that remains nonzero as one approaches the critical temperature. Analogously, in diffusion models, stability of data distributions implies the existence of computationally-efficent local denoisers in the late-time generation dynamics.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEAL - A Symmetry EncourAging Loss for High Energy Physics</title>
<link>https://arxiv.org/abs/2511.01982</link>
<guid>https://arxiv.org/abs/2511.01982</guid>
<content:encoded><![CDATA[
arXiv:2511.01982v1 Announce Type: cross 
Abstract: Physical symmetries provide a strong inductive bias for constructing functions to analyze data. In particular, this bias may improve robustness, data efficiency, and interpretability of machine learning models. However, building machine learning models that explicitly respect symmetries can be difficult due to the dedicated components required. Moreover, real-world experiments may not exactly respect fundamental symmetries at the level of finite granularities and energy thresholds. In this work, we explore an alternative approach to create symmetry-aware machine learning models. We introduce soft constraints that allow the model to decide the importance of added symmetries during the learning process instead of enforcing exact symmetries. We investigate two complementary approaches, one that penalizes the model based on specific transformations of the inputs and one inspired by group theory and infinitesimal transformations of the inputs. Using top quark jet tagging and Lorentz equivariance as examples, we observe that the addition of the soft constraints leads to more robust performance while requiring negligible changes to current state-of-the-art models.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving cold start in news recommendations: a RippleNet-based system for large scale media outlet</title>
<link>https://arxiv.org/abs/2511.02052</link>
<guid>https://arxiv.org/abs/2511.02052</guid>
<content:encoded><![CDATA[
arXiv:2511.02052v1 Announce Type: cross 
Abstract: We present a scalable recommender system implementation based on RippleNet, tailored for the media domain with a production deployment in Onet.pl, one of Poland's largest online media platforms. Our solution addresses the cold-start problem for newly published content by integrating content-based item embeddings into the knowledge propagation mechanism of RippleNet, enabling effective scoring of previously unseen items. The system architecture leverages Amazon SageMaker for distributed training and inference, and Apache Airflow for orchestrating data pipelines and model retraining workflows. To ensure high-quality training data, we constructed a comprehensive golden dataset consisting of user and item features and a separate interaction table, all enabling flexible extensions and integration of new signals.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven Learning of Interaction Laws in Multispecies Particle Systems with Gaussian Processes: Convergence Theory and Applications</title>
<link>https://arxiv.org/abs/2511.02053</link>
<guid>https://arxiv.org/abs/2511.02053</guid>
<content:encoded><![CDATA[
arXiv:2511.02053v1 Announce Type: cross 
Abstract: We develop a Gaussian process framework for learning interaction kernels in multi-species interacting particle systems from trajectory data. Such systems provide a canonical setting for multiscale modeling, where simple microscopic interaction rules generate complex macroscopic behaviors. While our earlier work established a Gaussian process approach and convergence theory for single-species systems, and later extended to second-order models with alignment and energy-type interactions, the multi-species setting introduces new challenges: heterogeneous populations interact both within and across species, the number of unknown kernels grows, and asymmetric interactions such as predator-prey dynamics must be accommodated. We formulate the learning problem in a nonparametric Bayesian setting and establish rigorous statistical guarantees. Our analysis shows recoverability of the interaction kernels, provides quantitative error bounds, and proves statistical optimality of posterior estimators, thereby unifying and generalizing previous single-species theory. Numerical experiments confirm the theoretical predictions and demonstrate the effectiveness of the proposed approach, highlighting its advantages over existing kernel-based methods. This work contributes a complete statistical framework for data-driven inference of interaction laws in multi-species systems, advancing the broader multiscale modeling program of connecting microscopic particle dynamics with emergent macroscopic behavior.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2511.02130</link>
<guid>https://arxiv.org/abs/2511.02130</guid>
<content:encoded><![CDATA[
arXiv:2511.02130v1 Announce Type: cross 
Abstract: We propose Re-FORC, an adaptive reward prediction method that, given a context, enables prediction of the expected future rewards as a function of the number of future thinking tokens. Re-FORC trains a lightweight adapter on reasoning models, demonstrating improved prediction with longer reasoning and larger models. Re-FORC enables: 1) early stopping of unpromising reasoning chains, reducing compute by 26% while maintaining accuracy, 2) optimized model and thinking length selection that achieves 4% higher accuracy at equal compute and 55% less compute at equal accuracy compared to the largest model, 3) adaptive test-time scaling, which increases accuracy by 11% in high compute regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with length control via cost-per-token thresholds while estimating computation time upfront.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects</title>
<link>https://arxiv.org/abs/2511.02132</link>
<guid>https://arxiv.org/abs/2511.02132</guid>
<content:encoded><![CDATA[
arXiv:2511.02132v1 Announce Type: cross 
Abstract: The rise of disaggregated AI GPUs has exposed a critical bottleneck in large-scale attention workloads: non-uniform memory access (NUMA). As multi-chiplet designs become the norm for scaling compute capabilities, memory latency and bandwidth vary sharply across compute regions, undermining the performance of traditional GPU kernel scheduling strategies that assume uniform memory access. We identify how these NUMA effects distort locality in multi-head attention (MHA) and present Swizzled Head-first Mapping, a spatially-aware scheduling strategy that aligns attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our method achieves up to 50% higher performance over state-of-the-art attention algorithms using conventional scheduling techniques and sustains consistently high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware scheduling is now fundamental to achieving full efficiency on next-generation disaggregated GPUs, offering a path forward for scalable AI training and inference.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DoFlow: Causal Generative Flows for Interventional and Counterfactual Time-Series Prediction</title>
<link>https://arxiv.org/abs/2511.02137</link>
<guid>https://arxiv.org/abs/2511.02137</guid>
<content:encoded><![CDATA[
arXiv:2511.02137v1 Announce Type: cross 
Abstract: Time-series forecasting increasingly demands not only accurate observational predictions but also causal forecasting under interventional and counterfactual queries in multivariate systems. We present DoFlow, a flow based generative model defined over a causal DAG that delivers coherent observational and interventional predictions, as well as counterfactuals through the natural encoding and decoding mechanism of continuous normalizing flows (CNFs). We also provide a supporting counterfactual recovery result under certain assumptions. Beyond forecasting, DoFlow provides explicit likelihoods of future trajectories, enabling principled anomaly detection. Experiments on synthetic datasets with various causal DAG and real world hydropower and cancer treatment time series show that DoFlow achieves accurate system-wide observational forecasting, enables causal forecasting over interventional and counterfactual queries, and effectively detects anomalies. This work contributes to the broader goal of unifying causal reasoning and generative modeling for complex dynamical systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near Optimal Convergence to Coarse Correlated Equilibrium in General-Sum Markov Games</title>
<link>https://arxiv.org/abs/2511.02157</link>
<guid>https://arxiv.org/abs/2511.02157</guid>
<content:encoded><![CDATA[
arXiv:2511.02157v1 Announce Type: cross 
Abstract: No-regret learning dynamics play a central role in game theory, enabling decentralized convergence to equilibrium for concepts such as Coarse Correlated Equilibrium (CCE) or Correlated Equilibrium (CE). In this work, we improve the convergence rate to CCE in general-sum Markov games, reducing it from the previously best-known rate of $\mathcal{O}(\log^5 T / T)$ to a sharper $\mathcal{O}(\log T / T)$. This matches the best known convergence rate for CE in terms of $T$, number of iterations, while also improving the dependence on the action set size from polynomial to polylogarithmic-yielding exponential gains in high-dimensional settings. Our approach builds on recent advances in adaptive step-size techniques for no-regret algorithms in normal-form games, and extends them to the Markovian setting via a stage-wise scheme that adjusts learning rates based on real-time feedback. We frame policy updates as an instance of Optimistic Follow-the-Regularized-Leader (OFTRL), customized for value-iteration-based learning. The resulting self-play algorithm achieves, to our knowledge, the fastest known convergence rate to CCE in Markov games.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScenicProver: A Framework for Compositional Probabilistic Verification of Learning-Enabled Systems</title>
<link>https://arxiv.org/abs/2511.02164</link>
<guid>https://arxiv.org/abs/2511.02164</guid>
<content:encoded><![CDATA[
arXiv:2511.02164v1 Announce Type: cross 
Abstract: Full verification of learning-enabled cyber-physical systems (CPS) has long been intractable due to challenges including black-box components and complex real-world environments. Existing tools either provide formal guarantees for limited types of systems or test the system as a monolith, but no general framework exists for compositional analysis of learning-enabled CPS using varied verification techniques over complex real-world environments. This paper introduces ScenicProver, a verification framework that aims to fill this gap. Built upon the Scenic probabilistic programming language, the framework supports: (1) compositional system description with clear component interfaces, ranging from interpretable code to black boxes; (2) assume-guarantee contracts over those components using an extension of Linear Temporal Logic containing arbitrary Scenic expressions; (3) evidence generation through testing, formal proofs via Lean 4 integration, and importing external assumptions; (4) systematic combination of generated evidence using contract operators; and (5) automatic generation of assurance cases tracking the provenance of system-level guarantees. We demonstrate the framework's effectiveness through a case study on an autonomous vehicle's automatic emergency braking system with sensor fusion. By leveraging manufacturer guarantees for radar and laser sensors and focusing testing efforts on uncertain conditions, our approach enables stronger probabilistic guarantees than monolithic testing with the same computational budget.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs</title>
<link>https://arxiv.org/abs/2511.02168</link>
<guid>https://arxiv.org/abs/2511.02168</guid>
<content:encoded><![CDATA[
arXiv:2511.02168v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to scale, their workloads increasingly rely on distributed execution across multiple GPUs. However, the conventional bulk synchronous parallel~(BSP) model used in such settings introduces significant performance inefficiencies. To characterize these bottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel Data Locality, and Kernel Launch Overhead) as an analytical framework. We propose moving beyond the rigid BSP model to address key inefficiencies in distributed GPU execution. By exploiting libraries like Iris for Triton, we gain access to in-kernel communication primitives that enable the design of novel fine-grained programming patterns, offering greater flexibility and performance than traditional BSP-based approaches. These patterns systematically eliminate the three taxes by creating direct, tile-level producer-consumer pipelines and replacing global barriers with fine-grained dataflow synchronization. Applying this methodology to critical kernels, from the foundational All-Gather + general matrix multiplication operation to the complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end latency over BSP-based approaches, establishing a more programmable and efficient paradigm for distributed LLM workloads.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivGNN: High-Performance Secure Inference for Cryptographic Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.02185</link>
<guid>https://arxiv.org/abs/2511.02185</guid>
<content:encoded><![CDATA[
arXiv:2511.02185v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) are powerful tools for analyzing and learning from graph-structured (GS) data, facilitating a wide range of services. Deploying such services in privacy-critical cloud environments necessitates the development of secure inference (SI) protocols that safeguard sensitive GS data. However, existing SI solutions largely focus on convolutional models for image and text data, leaving the challenge of securing GNNs and GS data relatively underexplored. In this work, we design, implement, and evaluate $\sysname$, a lightweight cryptographic scheme for graph-centric inference in the cloud. By hybridizing additive and function secret sharings within secure two-party computation (2PC), $\sysname$ is carefully designed based on a series of novel 2PC interactive protocols that achieve $1.5\times \sim 1.7\times$ speedups for linear layers and $2\times \sim 15\times$ for non-linear layers over state-of-the-art (SotA) solutions. A thorough theoretical analysis is provided to prove $\sysname$'s correctness, security, and lightweight nature. Extensive experiments across four datasets demonstrate $\sysname$'s superior efficiency with $1.3\times \sim 4.7\times$ faster secure predictions while maintaining accuracy comparable to plaintext graph property inference.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning</title>
<link>https://arxiv.org/abs/2511.02194</link>
<guid>https://arxiv.org/abs/2511.02194</guid>
<content:encoded><![CDATA[
arXiv:2511.02194v1 Announce Type: cross 
Abstract: Decision-making models for individuals, particularly in high-stakes scenarios like vaccine uptake, often diverge from population optimal predictions. This gap arises from the uniqueness of the individual decision-making process, shaped by numerical attributes (e.g., cost, time) and linguistic influences (e.g., personal preferences and constraints). Developing upon Utility Theory and leveraging the textual-reasoning capabilities of Large Language Models (LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric Reasoning framework (ATHENA) to address the optimal information integration. ATHENA uniquely integrates two stages: First, it discovers robust, group-level symbolic utility functions via LLM-augmented symbolic discovery; Second, it implements individual-level semantic adaptation, creating personalized semantic templates guided by the optimal utility to model personalized choices. Validated on real-world travel mode and vaccine choice tasks, ATHENA consistently outperforms utility-based, machine learning, and other LLM-based models, lifting F1 score by at least 6.5% over the strongest cutting-edge models. Further, ablation studies confirm that both stages of ATHENA are critical and complementary, as removing either clearly degrades overall predictive performance. By organically integrating symbolic utility modeling and semantic adaptation, ATHENA provides a new scheme for modeling human-centric decisions. The project page can be found at https://yibozh.github.io/Athena.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Proactive and Personalized LLM Agents</title>
<link>https://arxiv.org/abs/2511.02208</link>
<guid>https://arxiv.org/abs/2511.02208</guid>
<content:encoded><![CDATA[
arXiv:2511.02208v1 Announce Type: cross 
Abstract: While existing work focuses primarily on task success, we argue that effective real-world agents require optimizing three dimensions: productivity (task completion), proactivity (asking essential questions), and personalization (adapting to diverse user preferences). We introduce UserVille, an interactive environment with LLM-based user simulators enabling diverse, configurable user preferences. Leveraging UserVille, we introduce PPP, a multi-objective reinforcement learning approach that jointly optimizes all three dimensions: Productivity, Proactivity, and Personalization. Experiments on software engineering and deep research tasks show that agents trained with PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6 on average), demonstrating the ability to ask strategic clarifying questions, adapt to unseen user preferences, and improve task success through better interaction. This work demonstrates that explicitly optimizing for user-centered interaction is critical for building practical and effective AI agents.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Multi-Lane Intersection Performance in Mixed Autonomy Environments</title>
<link>https://arxiv.org/abs/2511.02217</link>
<guid>https://arxiv.org/abs/2511.02217</guid>
<content:encoded><![CDATA[
arXiv:2511.02217v1 Announce Type: cross 
Abstract: One of the main challenges in managing traffic at multilane intersections is ensuring smooth coordination between human-driven vehicles (HDVs) and connected autonomous vehicles (CAVs). This paper presents a novel traffic signal control framework that combines Graph Attention Networks (GAT) with Soft Actor-Critic (SAC) reinforcement learning to address this challenge. GATs are used to model the dynamic graph- structured nature of traffic flow to capture spatial and temporal dependencies between lanes and signal phases. The proposed SAC is a robust off-policy reinforcement learning algorithm that enables adaptive signal control through entropy-optimized decision making. This design allows the system to coordinate the signal timing and vehicle movement simultaneously with objectives focused on minimizing travel time, enhancing performance, ensuring safety, and improving fairness between HDVs and CAVs. The model is evaluated using a SUMO-based simulation of a four-way intersection and incorporating different traffic densities and CAV penetration rates. The experimental results demonstrate the effectiveness of the GAT-SAC approach by achieving a 24.1% reduction in average delay and up to 29.2% fewer traffic violations compared to traditional methods. Additionally, the fairness ratio between HDVs and CAVs improved to 1.59, indicating more equitable treatment across vehicle types. These findings suggest that the GAT-SAC framework holds significant promise for real-world deployment in mixed-autonomy traffic systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control</title>
<link>https://arxiv.org/abs/2511.02241</link>
<guid>https://arxiv.org/abs/2511.02241</guid>
<content:encoded><![CDATA[
arXiv:2511.02241v1 Announce Type: cross 
Abstract: Traditional neural networks, while powerful, rely on biologically implausible learning mechanisms such as global backpropagation. This paper introduces the Structurally Adaptive Predictive Inference Network (SAPIN), a novel computational model inspired by the principles of active inference and the morphological plasticity observed in biological neural cultures. SAPIN operates on a 2D grid where processing units, or cells, learn by minimizing local prediction errors. The model features two primary, concurrent learning mechanisms: a local, Hebbian-like synaptic plasticity rule based on the temporal difference between a cell's actual activation and its learned expectation, and a structural plasticity mechanism where cells physically migrate across the grid to optimize their information-receptive fields. This dual approach allows the network to learn both how to process information (synaptic weights) and also where to position its computational resources (network topology). We validated the SAPIN model on the classic Cart Pole reinforcement learning benchmark. Our results demonstrate that the architecture can successfully solve the CartPole task, achieving robust performance. The network's intrinsic drive to minimize prediction error and maintain homeostasis was sufficient to discover a stable balancing policy. We also found that while continual learning led to instability, locking the network's parameters after achieving success resulted in a stable policy. When evaluated for 100 episodes post-locking (repeated over 100 successful agents), the locked networks maintained an average 82% success rate.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results</title>
<link>https://arxiv.org/abs/2511.02246</link>
<guid>https://arxiv.org/abs/2511.02246</guid>
<content:encoded><![CDATA[
arXiv:2511.02246v1 Announce Type: cross 
Abstract: Recent research has shown that hallucinations, omissions, and biases are prevalent in everyday use-cases of LLMs. However, chatbots used in medical contexts must provide consistent advice in situations where non-medical factors are involved, such as when demographic information is present. In order to understand the conditions under which medical chatbots fail to perform as expected, we develop an infrastructure that 1) automatically generates queries to probe LLMs and 2) evaluates answers to these queries using multiple LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples the space of patient demographics, histories, disorders, and writing styles to create realistic questions that we subsequently use to prompt LLMs. In 2), our evaluation pipeline provides hallucination and omission detection using LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge treatment category detectors. As a baseline study, we perform two case studies on inter-LLM agreement and the impact of varying the answering and evaluation LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs yield statistically significant differences across writing styles, genders, and races. We recommend that studies using LLM evaluation use multiple LLMs as evaluators in order to avoid arriving at statistically significant but non-generalizable results, particularly in the absence of ground-truth data. We also suggest publishing inter-LLM agreement metrics for transparency. Our code and dataset are available here: https://github.com/BBN-E/medic-neurips-2025-demo.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models</title>
<link>https://arxiv.org/abs/2511.02248</link>
<guid>https://arxiv.org/abs/2511.02248</guid>
<content:encoded><![CDATA[
arXiv:2511.02248v1 Announce Type: cross 
Abstract: Serving large generative models such as LLMs and multi- modal transformers requires balancing user-facing SLOs (e.g., time-to-first-token, time-between-tokens) with provider goals of efficiency and cost reduction. Existing solutions rely on static provisioning or model-level autoscaling, both of which treat the model as a monolith. This coarse-grained resource management leads to degraded performance or significant resource underutilization due to poor adaptability to dynamic inference traffic that is common online.
  The root cause of this inefficiency lies in the internal structure of generative models: they are executed as graphs of interconnected operators. Through detailed characterization and systematic analysis, we find that operators are heterogeneous in their compute and memory footprints and exhibit diverse sensitivity to workload and resource factors such as batch size, sequence length, and traffic rate. This heterogeneity suggests that the operator, rather than the entire model, is the right granularity for scaling decisions.
  We propose an operator-level autoscaling framework, which allocates resources at finer (operator)-granularity, optimizing the scaling, batching, and placement based on individual operator profiles. Evaluated on production-scale traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less energy, or under fixed resources achieves 1.6x higher throughput with 5% less energy. These results show that the operator, rather than the model, is fundamentally a more effective unit for scaling large generative workloads.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Limit Theorems for Stochastic Gradient Descent in High-Dimensional Single-Layer Networks</title>
<link>https://arxiv.org/abs/2511.02258</link>
<guid>https://arxiv.org/abs/2511.02258</guid>
<content:encoded><![CDATA[
arXiv:2511.02258v1 Announce Type: cross 
Abstract: This paper studies the high-dimensional scaling limits of online stochastic gradient descent (SGD) for single-layer networks. Building on the seminal work of Saad and Solla, which analyzed the deterministic (ballistic) scaling limits of SGD corresponding to the gradient flow of the population loss, we focus on the critical scaling regime of the step size. Below this critical scale, the effective dynamics are governed by ballistic (ODE) limits, but at the critical scale, new correction term appears that changes the phase diagram. In this regime, near the fixed points, the corresponding diffusive (SDE) limits of the effective dynamics reduces to an Ornstein-Uhlenbeck process under certain conditions. These results highlight how the information exponent controls sample complexity and illustrates the limitations of deterministic scaling limit in capturing the stochastic fluctuations of high-dimensional learning dynamics.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02304</link>
<guid>https://arxiv.org/abs/2511.02304</guid>
<content:encoded><![CDATA[
arXiv:2511.02304v1 Announce Type: cross 
Abstract: We study the problem of learning multi-task, multi-agent policies for cooperative, temporal objectives, under centralized training, decentralized execution. In this setting, using automata to represent tasks enables the decomposition of complex tasks into simpler sub-tasks that can be assigned to agents. However, existing approaches remain sample-inefficient and are limited to the single-task case. In this work, we present Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for learning task-conditioned, decentralized team policies. We identify the main challenges to ACC-MARL's feasibility in practice, propose solutions, and prove the correctness of our approach. We further show that the value functions of learned policies can be used to assign tasks optimally at test time. Experiments show emergent task-aware, multi-step coordination among agents, e.g., pressing a button to unlock a door, holding the door, and short-circuiting tasks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Automated Framework for Strategy Discovery, Retrieval, and Evolution in LLM Jailbreak Attacks</title>
<link>https://arxiv.org/abs/2511.02356</link>
<guid>https://arxiv.org/abs/2511.02356</guid>
<content:encoded><![CDATA[
arXiv:2511.02356v1 Announce Type: cross 
Abstract: The widespread deployment of Large Language Models (LLMs) as public-facing web services and APIs has made their security a core concern for the web ecosystem. Jailbreak attacks, as one of the significant threats to LLMs, have recently attracted extensive research. In this paper, we reveal a jailbreak strategy which can effectively evade current defense strategies. It can extract valuable information from failed or partially successful attack attempts and contains self-evolution from attack interactions, resulting in sufficient strategy diversity and adaptability. Inspired by continuous learning and modular design principles, we propose ASTRA, a jailbreak framework that autonomously discovers, retrieves, and evolves attack strategies to achieve more efficient and adaptive attacks. To enable this autonomous evolution, we design a closed-loop "attack-evaluate-distill-reuse" core mechanism that not only generates attack prompts but also automatically distills and generalizes reusable attack strategies from every interaction. To systematically accumulate and apply this attack knowledge, we introduce a three-tier strategy library that categorizes strategies into Effective, Promising, and Ineffective based on their performance scores. The strategy library not only provides precise guidance for attack generation but also possesses exceptional extensibility and transferability. We conduct extensive experiments under a black-box setting, and the results show that ASTRA achieves an average Attack Success Rate (ASR) of 82.7%, significantly outperforming baselines.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation</title>
<link>https://arxiv.org/abs/2511.02358</link>
<guid>https://arxiv.org/abs/2511.02358</guid>
<content:encoded><![CDATA[
arXiv:2511.02358v1 Announce Type: cross 
Abstract: Query augmentation makes queries more meaningful by appending further information to the queries to find relevant documents. Current studies have proposed Large Language Model (LLM)-based embedders, which learn representation for embedding and generation for query augmentation in a multi-task manner by leveraging the generative capabilities of LLM. During inference, these jointly trained embedders have conducted query augmentation followed by embedding, showing effective results. However, augmenting every query leads to substantial embedding latency and query augmentation can be detrimental to performance for some queries. Also, previous methods have not been explored in multimodal environments. To tackle these problems, we propose M-Solomon, a universal multimodal embedder that can adaptively determine when to augment queries. Our approach first divides the queries of the training datasets into two groups at the dataset level. One includes queries that require augmentation and the other includes queries that do not. Then, we introduces a synthesis process that generates appropriate augmentations for queries that require them by leveraging a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation. Through this step, M-Solomon can conduct query augmentation only when necessary by learning to generate synthetic augmentations with the prefix /augment for queries that demand them and to generate the simple string /embed for others. Experimental results showed that M-Solomon not only surpassed the baseline without augmentation by a large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A new class of Markov random fields enabling lightweight sampling</title>
<link>https://arxiv.org/abs/2511.02373</link>
<guid>https://arxiv.org/abs/2511.02373</guid>
<content:encoded><![CDATA[
arXiv:2511.02373v1 Announce Type: cross 
Abstract: This work addresses the problem of efficient sampling of Markov random fields (MRF). The sampling of Potts or Ising MRF is most often based on Gibbs sampling, and is thus computationally expensive. We consider in this work how to circumvent this bottleneck through a link with Gaussian Markov Random fields. The latter can be sampled in several cost-effective ways, and we introduce a mapping from real-valued GMRF to discrete-valued MRF. The resulting new class of MRF benefits from a few theoretical properties that validate the new model. Numerical results show the drastic performance gain in terms of computational efficiency, as we sample at least 35x faster than Gibbs sampling using at least 37x less energy, all the while exhibiting empirical properties close to classical MRFs.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</title>
<link>https://arxiv.org/abs/2511.02376</link>
<guid>https://arxiv.org/abs/2511.02376</guid>
<content:encoded><![CDATA[
arXiv:2511.02376v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where adversarial prompts elicit harmful outputs, yet most evaluations focus on single-turn interactions while real-world attacks unfold through adaptive multi-turn conversations. We present AutoAdv, a training-free framework for automated multi-turn jailbreaking that achieves up to 95% attack success rate on Llama-3.1-8B within six turns a 24 percent improvement over single turn baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern manager that learns from successful attacks to enhance future prompts, a temperature manager that dynamically adjusts sampling parameters based on failure modes, and a two-phase rewriting strategy that disguises harmful requests then iteratively refines them. Extensive evaluation across commercial and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent vulnerabilities in current safety mechanisms, with multi-turn attacks consistently outperforming single-turn approaches. These findings demonstrate that alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations, highlighting an urgent need for multi-turn-aware defenses.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds</title>
<link>https://arxiv.org/abs/2511.02395</link>
<guid>https://arxiv.org/abs/2511.02395</guid>
<content:encoded><![CDATA[
arXiv:2511.02395v1 Announce Type: cross 
Abstract: Moving object segmentation is a crucial task for safe and reliable autonomous mobile systems like self-driving cars, improving the reliability and robustness of subsequent tasks like SLAM or path planning. While the segmentation of camera or LiDAR data is widely researched and achieves great results, it often introduces an increased latency by requiring the accumulation of temporal sequences to gain the necessary temporal context. Radar sensors overcome this problem with their ability to provide a direct measurement of a point's Doppler velocity, which can be exploited for single-scan moving object segmentation. However, radar point clouds are often sparse and noisy, making data annotation for use in supervised learning very tedious, time-consuming, and cost-intensive. To overcome this problem, we address the task of self-supervised moving object segmentation of sparse and noisy radar point clouds. We follow a two-step approach of contrastive self-supervised representation learning with subsequent supervised fine-tuning using limited amounts of annotated data. We propose a novel clustering-based contrastive loss function with cluster refinement based on dynamic points removal to pretrain the network to produce motion-aware representations of the radar data. Our method improves label efficiency after fine-tuning, effectively boosting state-of-the-art performance by self-supervised pretraining.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through Dataset Harmonization</title>
<link>https://arxiv.org/abs/2511.02400</link>
<guid>https://arxiv.org/abs/2511.02400</guid>
<content:encoded><![CDATA[
arXiv:2511.02400v1 Announce Type: cross 
Abstract: The development of clinically reliable artificial intelligence (AI) systems for mammography is hindered by profound heterogeneity in data quality, metadata standards, and population distributions across public datasets. This heterogeneity introduces dataset-specific biases that severely compromise the generalizability of the model, a fundamental barrier to clinical deployment. We present MammoClean, a public framework for standardization and bias quantification in mammography datasets. MammoClean standardizes case selection, image processing (including laterality and intensity correction), and unifies metadata into a consistent multi-view structure. We provide a comprehensive review of breast anatomy, imaging characteristics, and public mammography datasets to systematically identify key sources of bias. Applying MammoClean to three heterogeneous datasets (CBIS-DDSM, TOMPEI-CMMD, VinDr-Mammo), we quantify substantial distributional shifts in breast density and abnormality prevalence. Critically, we demonstrate the direct impact of data corruption: AI models trained on corrupted datasets exhibit significant performance degradation compared to their curated counterparts. By using MammoClean to identify and mitigate bias sources, researchers can construct unified multi-dataset training corpora that enable development of robust models with superior cross-domain generalization. MammoClean provides an essential, reproducible pipeline for bias-aware AI development in mammography, facilitating fairer comparisons and advancing the creation of safe, effective systems that perform equitably across diverse patient populations and clinical settings. The open-source code is publicly available from: https://github.com/Minds-R-Lab/MammoClean.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arithmetic Circuits and Neural Networks for Regular Matroids</title>
<link>https://arxiv.org/abs/2511.02406</link>
<guid>https://arxiv.org/abs/2511.02406</guid>
<content:encoded><![CDATA[
arXiv:2511.02406v1 Announce Type: cross 
Abstract: We prove that there exist uniform $(+,\times,/)$-circuits of size $O(n^3)$ to compute the basis generating polynomial of regular matroids on $n$ elements. By tropicalization, this implies that there exist uniform $(\max,+,-)$-circuits and ReLU neural networks of the same size for weighted basis maximization of regular matroids. As a consequence in linear programming theory, we obtain a first example where taking the difference of two extended formulations can be more efficient than the best known individual extended formulation of size $O(n^6)$ by Aprile and Fiorini. Such differences have recently been introduced as virtual extended formulations. The proof of our main result relies on a fine-tuned version of Seymour's decomposition of regular matroids which allows us to identify and maintain graphic substructures to which we can apply a local version of the star-mesh transformation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Adaptive Sampling Framework for Detecting Localized Concept Drift under Label Scarcity</title>
<link>https://arxiv.org/abs/2511.02452</link>
<guid>https://arxiv.org/abs/2511.02452</guid>
<content:encoded><![CDATA[
arXiv:2511.02452v1 Announce Type: cross 
Abstract: Concept drift and label scarcity are two critical challenges limiting the robustness of predictive models in dynamic industrial environments. Existing drift detection methods often assume global shifts and rely on dense supervision, making them ill-suited for regression tasks with local drifts and limited labels. This paper proposes an adaptive sampling framework that combines residual-based exploration and exploitation with EWMA monitoring to efficiently detect local concept drift under labeling budget constraints. Empirical results on synthetic benchmarks and a case study on electricity market demonstrate superior performance in label efficiency and drift detection accuracy.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning CNF formulas from uniform random solutions in the local lemma regime</title>
<link>https://arxiv.org/abs/2511.02487</link>
<guid>https://arxiv.org/abs/2511.02487</guid>
<content:encoded><![CDATA[
arXiv:2511.02487v1 Announce Type: cross 
Abstract: We study the problem of learning a $n$-variables $k$-CNF formula $\Phi$ from its i.i.d. uniform random solutions, which is equivalent to learning a Boolean Markov random field (MRF) with $k$-wise hard constraints. Revisiting Valiant's algorithm (Commun. ACM'84), we show that it can exactly learn (1) $k$-CNFs with bounded clause intersection size under Lov\'asz local lemma type conditions, from $O(\log n)$ samples; and (2) random $k$-CNFs near the satisfiability threshold, from $\widetilde{O}(n^{\exp(-\sqrt{k})})$ samples. These results significantly improve the previous $O(n^k)$ sample complexity. We further establish new information-theoretic lower bounds on sample complexity for both exact and approximate learning from i.i.d. uniform random solutions.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Many-vs-Many Missile Guidance via Virtual Targets</title>
<link>https://arxiv.org/abs/2511.02526</link>
<guid>https://arxiv.org/abs/2511.02526</guid>
<content:encoded><![CDATA[
arXiv:2511.02526v1 Announce Type: cross 
Abstract: This paper presents a novel approach to many-vs-many missile guidance using virtual targets (VTs) generated by a Normalizing Flows-based trajectory predictor. Rather than assigning n interceptors directly to m physical targets through conventional weapon target assignment algorithms, we propose a centralized strategy that constructs n VT trajectories representing probabilistic predictions of maneuvering target behavior. Each interceptor is guided toward its assigned VT using Zero-Effort-Miss guidance during midcourse flight, transitioning to Proportional Navigation guidance for terminal interception. This approach treats many-vs-many engagements as many-vs-distribution scenarios, exploiting numerical superiority (n > m) by distributing interceptors across diverse trajectory hypotheses rather than pursuing identical deterministic predictions. Monte Carlo simulations across various target-interceptor configurations (1-6 targets, 1-8 interceptors) demonstrate that the VT method matches or exceeds baseline straight-line prediction performance by 0-4.1% when n = m, with improvements increasing to 5.8-14.4% when n > m. The results confirm that probabilistic VTs enable effective exploitation of numerical superiority, significantly increasing interception probability in many-vs-many scenarios.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic AI for Mobile Network RAN Management and Optimization</title>
<link>https://arxiv.org/abs/2511.02532</link>
<guid>https://arxiv.org/abs/2511.02532</guid>
<content:encoded><![CDATA[
arXiv:2511.02532v1 Announce Type: cross 
Abstract: Agentic AI represents a new paradigm for automating complex systems by using Large AI Models (LAMs) to provide human-level cognitive abilities with multimodal perception, planning, memory, and reasoning capabilities. This will lead to a new generation of AI systems that autonomously decompose goals, retain context over time, learn continuously, operate across tools and environments, and adapt dynamically. The complexity of 5G and upcoming 6G networks renders manual optimization ineffective, pointing to Agentic AI as a method for automating decisions in dynamic RAN environments. However, despite its rapid advances, there is no established framework outlining the foundational components and operational principles of Agentic AI systems nor a universally accepted definition.
  This paper contributes to ongoing research on Agentic AI in 5G and 6G networks by outlining its core concepts and then proposing a practical use case that applies Agentic principles to RAN optimization. We first introduce Agentic AI, tracing its evolution from classical agents and discussing the progress from workflows and simple AI agents to Agentic AI. Core design patterns-reflection, planning, tool use, and multi-agent collaboration-are then described to illustrate how intelligent behaviors are orchestrated. These theorical concepts are grounded in the context of mobile networks, with a focus on RAN management and optimization. A practical 5G RAN case study shows how time-series analytics and LAM-driven agents collaborate for KPI-based autonomous decision-making.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction</title>
<link>https://arxiv.org/abs/2511.02558</link>
<guid>https://arxiv.org/abs/2511.02558</guid>
<content:encoded><![CDATA[
arXiv:2511.02558v1 Announce Type: cross 
Abstract: Predicting future brain state from a baseline magnetic resonance image (MRI) is a central challenge in neuroimaging and has important implications for studying neurodegenerative diseases such as Alzheimer's disease (AD). Most existing approaches predict future cognitive scores or clinical outcomes, such as conversion from mild cognitive impairment to dementia. Instead, here we investigate longitudinal MRI image-to-image prediction that forecasts a participant's entire brain MRI several years into the future, intrinsically modeling complex, spatially distributed neurodegenerative patterns. We implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL). Predicted follow-up MRIs are directly compared with the actual follow-up scans using metrics that capture global similarity and local differences. The best performing models achieve high-fidelity predictions, and all models generalize well to an independent external dataset, demonstrating robust cross-cohort performance. Our results indicate that deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RIS-Assisted 3D Spherical Splatting for Object Composition Visualization using Detection Transformers</title>
<link>https://arxiv.org/abs/2511.02573</link>
<guid>https://arxiv.org/abs/2511.02573</guid>
<content:encoded><![CDATA[
arXiv:2511.02573v1 Announce Type: cross 
Abstract: The pursuit of immersive and structurally aware multimedia experiences has intensified interest in sensing modalities that reconstruct objects beyond the limits of visible light. Conventional optical pipelines degrade under occlusion or low illumination, motivating the use of radio-frequency (RF) sensing, whose electromagnetic waves penetrate materials and encode both geometric and compositional information. Yet, uncontrolled multipath propagation restricts reconstruction accuracy. Recent advances in Programmable Wireless Environments (PWEs) mitigate this limitation by enabling software-defined manipulation of propagation through Reconfigurable Intelligent Surfaces (RISs), thereby providing controllable illumination diversity. Building on this capability, this work introduces a PWE-driven RF framework for three-dimensional object reconstruction using material-aware spherical primitives. The proposed approach combines RIS-enabled field synthesis with a Detection Transformer (DETR) that infers spatial and material parameters directly from extracted RF features. Simulation results confirm the framework's ability to approximate object geometries and classify material composition with an overall accuracy of 79.35%, marking an initial step toward programmable and physically grounded RF-based 3D object composition visualization.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAUE: Training-free Noise Transplant and Cultivation Diffusion Model</title>
<link>https://arxiv.org/abs/2511.02580</link>
<guid>https://arxiv.org/abs/2511.02580</guid>
<content:encoded><![CDATA[
arXiv:2511.02580v1 Announce Type: cross 
Abstract: Despite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Redundancy Maximization as a Principle of Associative Memory Learning</title>
<link>https://arxiv.org/abs/2511.02584</link>
<guid>https://arxiv.org/abs/2511.02584</guid>
<content:encoded><![CDATA[
arXiv:2511.02584v1 Announce Type: cross 
Abstract: Associative memory, traditionally modeled by Hopfield networks, enables the retrieval of previously stored patterns from partial or noisy cues. Yet, the local computational principles which are required to enable this function remain incompletely understood. To formally characterize the local information processing in such systems, we employ a recent extension of information theory - Partial Information Decomposition (PID). PID decomposes the contribution of different inputs to an output into unique information from each input, redundant information across inputs, and synergistic information that emerges from combining different inputs. Applying this framework to individual neurons in classical Hopfield networks we find that below the memory capacity, the information in a neuron's activity is characterized by high redundancy between the external pattern input and the internal recurrent input, while synergy and unique information are close to zero until the memory capacity is surpassed and performance drops steeply. Inspired by this observation, we use redundancy as an information-theoretic learning goal, which is directly optimized for each neuron, dramatically increasing the network's memory capacity to 1.59, a more than tenfold improvement over the 0.14 capacity of classical Hopfield networks and even outperforming recent state-of-the-art implementations of Hopfield networks. Ultimately, this work establishes redundancy maximization as a new design principle for associative memories and opens pathways for new associative memory models based on information-theoretic goals.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifying LLM Inference to Prevent Model Weight Exfiltration</title>
<link>https://arxiv.org/abs/2511.02620</link>
<guid>https://arxiv.org/abs/2511.02620</guid>
<content:encoded><![CDATA[
arXiv:2511.02620v1 Announce Type: cross 
Abstract: As large AI models become increasingly valuable assets, the risk of model weight exfiltration from inference servers grows accordingly. An attacker controlling an inference server may exfiltrate model weights by hiding them within ordinary model outputs, a strategy known as steganography. This work investigates how to verify model responses to defend against such attacks and, more broadly, to detect anomalous or buggy behavior during inference. We formalize model exfiltration as a security game, propose a verification framework that can provably mitigate steganographic exfiltration, and specify the trust assumptions associated with our scheme. To enable verification, we characterize valid sources of non-determinism in large language model inference and introduce two practical estimators for them. We evaluate our detection framework on several open-weight models ranging from 3B to 30B parameters. On MOE-Qwen-30B, our detector reduces exfiltratable information to <0.5% with false-positive rate of 0.01%, corresponding to a >200x slowdown for adversaries. Overall, this work further establishes a foundation for defending against model weight exfiltration and demonstrates that strong protection can be achieved with minimal additional cost to inference providers.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The stability of shallow neural networks on spheres: A sharp spectral analysis</title>
<link>https://arxiv.org/abs/2511.02625</link>
<guid>https://arxiv.org/abs/2511.02625</guid>
<content:encoded><![CDATA[
arXiv:2511.02625v1 Announce Type: cross 
Abstract: We present an estimation of the condition numbers of the \emph{mass} and \emph{stiffness} matrices arising from shallow ReLU$^k$ neural networks defined on the unit sphere~$\mathbb{S}^d$. In particular, when $\{\theta_j^*\}_{j=1}^n \subset \mathbb{S}^d$ is \emph{antipodally quasi-uniform}, the condition number is sharp. Indeed, in this case, we obtain sharp asymptotic estimates for the full spectrum of eigenvalues and characterize the structure of the corresponding eigenspaces, showing that the smallest eigenvalues are associated with an eigenbasis of low-degree polynomials while the largest eigenvalues are linked to high-degree polynomials. This spectral analysis establishes a precise correspondence between the approximation power of the network and its numerical stability.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks</title>
<link>https://arxiv.org/abs/2511.02647</link>
<guid>https://arxiv.org/abs/2511.02647</guid>
<content:encoded><![CDATA[
arXiv:2511.02647v1 Announce Type: cross 
Abstract: Large language models (LLMs) are proliferating rapidly at the edge, delivering intelligent capabilities across diverse application scenarios. However, their practical deployment in collaborative scenarios confronts fundamental challenges: privacy vulnerabilities, communication overhead, and computational bottlenecks. To address these, we propose Federated Attention (FedAttn), which integrates the federated paradigm into the self-attention mechanism, creating a new distributed LLM inference framework that simultaneously achieves privacy protection, communication efficiency, and computational efficiency. FedAttn enables participants to perform local self-attention over their own token representations while periodically exchanging and aggregating Key-Value (KV) matrices across multiple Transformer blocks, collaboratively generating LLM responses without exposing private prompts. Further, we identify a structural duality between contextual representation refinement in FedAttn and parameter optimization in FL across private data, local computation, and global aggregation. This key insight provides a principled foundation for systematically porting federated optimization techniques to collaborative LLM inference. Building on this framework, we theoretically analyze how local self-attention computation within participants and heterogeneous token relevance among participants shape error propagation dynamics across Transformer blocks. Moreover, we characterize the fundamental trade-off between response quality and communication/computation efficiency, which is governed by the synchronization interval and the number of participants. Experimental results validate our theoretical analysis, and reveal significant optimization opportunities through sparse attention and adaptive KV aggregation, highlighting FedAttn's potential to deliver scalability and efficiency in real-world edge deployments.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL-Aided Cognitive ISAC: Robust Detection and Sensing-Communication Trade-offs</title>
<link>https://arxiv.org/abs/2511.02672</link>
<guid>https://arxiv.org/abs/2511.02672</guid>
<content:encoded><![CDATA[
arXiv:2511.02672v1 Announce Type: cross 
Abstract: This paper proposes a reinforcement learning (RL)-aided cognitive framework for massive MIMO-based integrated sensing and communication (ISAC) systems employing a uniform planar array (UPA). The focus is on enhancing radar sensing performance in environments with unknown and dynamic disturbance characteristics. A Wald-type detector is employed for robust target detection under non-Gaussian clutter, while a SARSA-based RL algorithm enables adaptive estimation of target positions without prior environmental knowledge. Based on the RL-derived sensing information, a joint waveform optimization strategy is formulated to balance radar sensing accuracy and downlink communication throughput. The resulting design provides an adaptive trade-off between detection performance and achievable sum rate through an analytically derived closed-form solution. Monte Carlo simulations demonstrate that the proposed cognitive ISAC framework achieves significantly improved detection probability compared to orthogonal and non-learning adaptive baselines, while maintaining competitive communication performance. These results underline the potential of RL-assisted sensing for robust and spectrum-efficient ISAC in next-generation wireless networks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes</title>
<link>https://arxiv.org/abs/2511.02681</link>
<guid>https://arxiv.org/abs/2511.02681</guid>
<content:encoded><![CDATA[
arXiv:2511.02681v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly prevalent across diverse applications. However, their enormous size limits storage and processing capabilities to a few well-resourced stakeholders. As a result, most applications rely on pre-trained LLMs, fine-tuned for specific tasks. However, even storing the fine-tuned versions of these models remains a significant challenge due to the wide range of tasks they address. Recently, studies show that fine-tuning these models primarily affects a small fraction of parameters, highlighting the need for more efficient storage of fine-tuned models. This paper focuses on efficient storage of parameter updates in pre-trained models after fine-tuning. To address this challenge, we leverage the observation that fine-tuning updates are both low-rank and sparse, which can be utilized for storage efficiency. However, using only low-rank approximation or sparsification may discard critical singular components that enhance model expressivity. We first observe that given the same memory budget, sparsified low-rank approximations with larger ranks outperform standard low-rank approximations with smaller ranks. Building on this, we propose our method, optimal singular damage, that selectively sparsifies low-rank approximated updates by leveraging the interleaved importance of singular vectors, ensuring that the most impactful components are retained. We demonstrate through extensive experiments that our proposed methods lead to significant storage efficiency and superior accuracy within the same memory budget compared to employing the low-rank approximation or sparsification individually.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Kernel Discrepancies via Subset Selection</title>
<link>https://arxiv.org/abs/2511.02706</link>
<guid>https://arxiv.org/abs/2511.02706</guid>
<content:encoded><![CDATA[
arXiv:2511.02706v1 Announce Type: cross 
Abstract: Kernel discrepancies are a powerful tool for analyzing worst-case errors in quasi-Monte Carlo (QMC) methods. Building on recent advances in optimizing such discrepancy measures, we extend the subset selection problem to the setting of kernel discrepancies, selecting an m-element subset from a large population of size $n \gg m$. We introduce a novel subset selection algorithm applicable to general kernel discrepancies to efficiently generate low-discrepancy samples from both the uniform distribution on the unit hypercube, the traditional setting of classical QMC, and from more general distributions $F$ with known density functions by employing the kernel Stein discrepancy. We also explore the relationship between the classical $L_2$ star discrepancy and its $L_\infty$ counterpart.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic World Modeling for 6G: Near-Real-Time Generative State-Space Reasoning</title>
<link>https://arxiv.org/abs/2511.02748</link>
<guid>https://arxiv.org/abs/2511.02748</guid>
<content:encoded><![CDATA[
arXiv:2511.02748v1 Announce Type: cross 
Abstract: We argue that sixth-generation (6G) intelligence is not fluent token prediction but the capacity to imagine and choose -- to simulate future scenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe open radio access network (O-RAN) near-real-time (Near-RT) control via counterfactual dynamics and a world modeling (WM) paradigm that learns an action-conditioned generative state space. This enables quantitative "what-if" forecasting beyond large language models (LLMs) as the primary modeling primitive. Actions such as physical resource blocks (PRBs) are treated as first-class control inputs in a causal world model, and both aleatoric and epistemic uncertainty are modeled for prediction and what-if analysis. An agentic, model predictive control (MPC)-based cross-entropy method (CEM) planner operates over short horizons, using prior-mean rollouts within data-driven PRB bounds to maximize a deterministic reward. The model couples multi-scale structured state-space mixtures (MS3M) with a compact stochastic latent to form WM-MS3M, summarizing key performance indicators (KPIs) histories and predicting next-step KPIs under hypothetical PRB sequences. On realistic O-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with 32% fewer parameters and similar latency, and achieves 35-80% lower root mean squared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster inference, enabling rare-event simulation and offline policy screening.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DANIEL: A Distributed and Scalable Approach for Global Representation Learning with EHR Applications</title>
<link>https://arxiv.org/abs/2511.02754</link>
<guid>https://arxiv.org/abs/2511.02754</guid>
<content:encoded><![CDATA[
arXiv:2511.02754v1 Announce Type: cross 
Abstract: Classical probabilistic graphical models face fundamental challenges in modern data environments, which are characterized by high dimensionality, source heterogeneity, and stringent data-sharing constraints. In this work, we revisit the Ising model, a well-established member of the Markov Random Field (MRF) family, and develop a distributed framework that enables scalable and privacy-preserving representation learning from large-scale binary data with inherent low-rank structure. Our approach optimizes a non-convex surrogate loss function via bi-factored gradient descent, offering substantial computational and communication advantages over conventional convex approaches. We evaluate our algorithm on multi-institutional electronic health record (EHR) datasets from 58,248 patients across the University of Pittsburgh Medical Center (UPMC) and Mass General Brigham (MGB), demonstrating superior performance in global representation learning and downstream clinical tasks, including relationship detection, patient phenotyping, and patient clustering. These results highlight a broader potential for statistical inference in federated, high-dimensional settings while addressing the practical challenges of data complexity and multi-institutional integration.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning</title>
<link>https://arxiv.org/abs/2511.02818</link>
<guid>https://arxiv.org/abs/2511.02818</guid>
<content:encoded><![CDATA[
arXiv:2511.02818v1 Announce Type: cross 
Abstract: Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning. However, current architectures exhibit key limitations: (1) single-scale feature processing that overlooks hierarchical dependencies, (2) dense attention with quadratic scaling in table width, and (3) strictly sequential component processing that prevents iterative representation refinement and cross-component communication. To address these challenges, we introduce Orion-MSP, a tabular ICL architecture featuring three key innovations: (1) multi-scale processing to capture hierarchical feature interactions; (2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency and long-range connectivity; and (3) a Perceiver-style memory enabling safe bidirectional information flow across components. Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables, establishing a new standard for efficient tabular in-context learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-MSP .
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerated Frank-Wolfe Algorithms: Complementarity Conditions and Sparsity</title>
<link>https://arxiv.org/abs/2511.02821</link>
<guid>https://arxiv.org/abs/2511.02821</guid>
<content:encoded><![CDATA[
arXiv:2511.02821v1 Announce Type: cross 
Abstract: We develop new accelerated first-order algorithms in the Frank-Wolfe (FW) family for minimizing smooth convex functions over compact convex sets, with a focus on two prominent constraint classes: (1) polytopes and (2) matrix domains given by the spectrahedron and the unit nuclear-norm ball. A key technical ingredient is a complementarity condition that captures solution sparsity -- face dimension for polytopes and rank for matrices. We present two algorithms: (1) a purely linear optimization oracle (LOO) method for polytopes that has optimal worst-case first-order (FO) oracle complexity and, aside of a finite \emph{burn-in} phase and up to a logarithmic factor, has LOO complexity that scales with $r/\sqrt{\epsilon}$, where $\epsilon$ is the target accuracy and $r$ is the solution sparsity $r$ (independently of the ambient dimension), and (2) a hybrid scheme that combines FW with a sparse projection oracle (e.g., low-rank SVDs for matrix domains with low-rank solutions), which also has optimal FO oracle complexity, and after a finite burn-in phase, only requires $O(1/\sqrt{\epsilon})$ sparse projections and LOO calls (independently of both the ambient dimension and the rank of optimal solutions). Our results close a gap on how to accelerate recent advancements in linearly-converging FW algorithms for strongly convex optimization, without paying the price of the dimension.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System</title>
<link>https://arxiv.org/abs/2511.02832</link>
<guid>https://arxiv.org/abs/2511.02832</guid>
<content:encoded><![CDATA[
arXiv:2511.02832v1 Announce Type: cross 
Abstract: Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also open-sourced at https://twist-data.github.io .
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything</title>
<link>https://arxiv.org/abs/2511.02834</link>
<guid>https://arxiv.org/abs/2511.02834</guid>
<content:encoded><![CDATA[
arXiv:2511.02834v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining. The master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses. Extensive experiments across text, image, audio, video, and omni benchmarks show that Agent-Omni consistently achieves state-of-the-art performance, particularly on tasks requiring complex cross-modal reasoning. Its agent-based design enables seamless integration of specialized foundation models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. In addition, the framework is modular and easily extensible, allowing future improvements as stronger models become available. %We release an open-source implementation to support continued research on scalable and reliable omni-modal reasoning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search (Full version)</title>
<link>https://arxiv.org/abs/2308.15734</link>
<guid>https://arxiv.org/abs/2308.15734</guid>
<content:encoded><![CDATA[
arXiv:2308.15734v3 Announce Type: replace 
Abstract: The number of graph neural network (GNN) architectures has increased rapidly due to the growing adoption of graph analysis. Although we use GNNs in wide application scenarios, it is a laborious task to design/select optimal GNN architectures in diverse graphs. To reduce human efforts, graph neural architecture search (Graph NAS) has been used to search for a sub-optimal GNN architecture that combines existing components. However, existing Graph NAS methods lack explainability to understand the reasons why the model architecture is selected because they use complex search space and neural models to select architecture. Therefore, we propose an explainable Graph NAS method, called ExGNAS, which consists of (i) a simple search space that can adapt to various graphs and (ii) a search algorithm with Monte-Carlo tree that makes the decision process explainable. The combination of our search space and algorithm achieves finding accurate GNN models and the important functions within the search space. We comprehensively evaluate ExGNAS compared with four state-of-the-art Graph NAS methods in twelve graphs. Our experimental results show that ExGNAS achieves high average accuracy and efficiency; improving accuracy up to 26.1% and reducing run time up to 88%. Furthermore, we show the effectiveness of explainability by questionnaire-based user study and architecture analysis.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous-time Riemannian SGD and SVRG Flows on Wasserstein Probabilistic Space</title>
<link>https://arxiv.org/abs/2401.13530</link>
<guid>https://arxiv.org/abs/2401.13530</guid>
<content:encoded><![CDATA[
arXiv:2401.13530v4 Announce Type: replace 
Abstract: Recently, optimization on the Riemannian manifold have provided valuable insights to the optimization community. In this regard, extending these methods to to the Wasserstein space is of particular interest, since optimization on Wasserstein space is closely connected to practical sampling processes. Generally, the standard (continuous) optimization method on Wasserstein space is Riemannian gradient flow (i.e., Langevin dynamics when minimizing KL divergence). In this paper, we aim to enrich the family of continuous optimization methods in the Wasserstein space, by extending the gradient flow on it into the stochastic gradient descent (SGD) flow and stochastic variance reduction gradient (SVRG) flow.
  By leveraging the property of Wasserstein space, we construct stochastic differential equations (SDEs) to approximate the corresponding discrete Euclidean dynamics of the desired Riemannian stochastic methods. Then, we obtain the flows in Wasserstein space by Fokker-Planck equation. Finally, we establish convergence rates of the proposed stochastic flows, which align with those known in the Euclidean setting.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reset &amp; Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2403.05066</link>
<guid>https://arxiv.org/abs/2403.05066</guid>
<content:encoded><![CDATA[
arXiv:2403.05066v3 Announce Type: replace 
Abstract: We argue that the negative transfer problem occurring when the new task to learn arrives is an important problem that needs not be overlooked when developing effective Continual Reinforcement Learning (CRL) algorithms. Through comprehensive experimental validation, we demonstrate that such issue frequently exists in CRL and cannot be effectively addressed by several recent work on either mitigating plasticity loss of RL agents or enhancing the positive transfer in CRL scenario. To that end, we develop Reset & Distill (R&amp;D), a simple yet highly effective baseline method, to overcome the negative transfer problem in CRL. R&amp;D combines a strategy of resetting the agent's online actor and critic networks to learn a new task and an offline learning step for distilling the knowledge from the online actor and previous expert's action probabilities. We carried out extensive experiments on long sequence of Meta World tasks and show that our simple baseline method consistently outperforms recent approaches, achieving significantly higher success rates across a range of tasks. Our findings highlight the importance of considering negative transfer in CRL and emphasize the need for robust strategies like R&amp;D to mitigate its detrimental effects.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy-Based Model for Accurate Estimation of Shapley Values in Feature Attribution</title>
<link>https://arxiv.org/abs/2404.01078</link>
<guid>https://arxiv.org/abs/2404.01078</guid>
<content:encoded><![CDATA[
arXiv:2404.01078v4 Announce Type: replace 
Abstract: Shapley value is a widely used tool in explainable artificial intelligence (XAI), as it provides a principled way to attribute contributions of input features to model outputs. However, estimation of Shapley value requires capturing conditional dependencies among all feature combinations, which poses significant challenges in complex data environments. In this article, EmSHAP (Energy-based model for Shapley value estimation), an accurate Shapley value estimation method, is proposed to estimate the expectation of Shapley contribution function under the arbitrary subset of features given the rest. By utilizing the ability of energy-based model (EBM) to model complex distributions, EmSHAP provides an effective solution for estimating the required conditional probabilities. To further improve estimation accuracy, a GRU (Gated Recurrent Unit)-coupled partition function estimation method is introduced. The GRU network captures long-term dependencies with a lightweight parameterization and maps input features into a latent space to mitigate the influence of feature ordering. Additionally, a dynamic masking mechanism is incorporated to further enhance the robustness and accuracy by progressively increasing the masking rate. Theoretical analysis on the error bound as well as application to four case studies verified the higher accuracy and better scalability of EmSHAP in contrast to competitive methods.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Uncertainty Estimation through Semantically Diverse Language Generation</title>
<link>https://arxiv.org/abs/2406.04306</link>
<guid>https://arxiv.org/abs/2406.04306</guid>
<content:encoded><![CDATA[
arXiv:2406.04306v2 Announce Type: replace 
Abstract: Large language models (LLMs) can suffer from hallucinations when generating text. These hallucinations impede various applications in society and industry by making LLMs untrustworthy. Current LLMs generate text in an autoregressive fashion by predicting and appending text tokens. When an LLM is uncertain about the semantic meaning of the next tokens to generate, it is likely to start hallucinating. Thus, it has been suggested that predictive uncertainty is one of the main causes of hallucinations. We introduce Semantically Diverse Language Generation (SDLG) to quantify predictive uncertainty in LLMs. SDLG steers the LLM to generate semantically diverse yet likely alternatives for an initially generated text. This approach provides a precise measure of aleatoric semantic uncertainty, detecting whether the initial text is likely to be hallucinated. Experiments on question-answering tasks demonstrate that SDLG consistently outperforms existing methods while being the most computationally efficient, setting a new standard for uncertainty estimation in LLMs.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RASPNet: A Benchmark Dataset for Radar Adaptive Signal Processing Applications</title>
<link>https://arxiv.org/abs/2406.09638</link>
<guid>https://arxiv.org/abs/2406.09638</guid>
<content:encoded><![CDATA[
arXiv:2406.09638v3 Announce Type: replace 
Abstract: We present a large-scale dataset called RASPNet for radar adaptive signal processing (RASP) applications to support the development of data-driven models within the adaptive radar community. RASPNet exceeds 16 TB in size and comprises 100 realistic scenarios compiled over a variety of topographies and land types across the contiguous United States. For each scenario, RASPNet comprises 10,000 clutter realizations from an airborne radar setting, which can be used to benchmark radar and complex-valued learning algorithms. RASPNet intends to fill a prominent gap in the availability of a large-scale, realistic dataset that standardizes the evaluation of RASP techniques and complex-valued neural networks. We outline its construction, organization, and several applications, including a transfer learning example to demonstrate how RASPNet can be used for real-world adaptive radar scenarios.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature compression is the root cause of adversarial fragility in neural network classifiers</title>
<link>https://arxiv.org/abs/2406.16200</link>
<guid>https://arxiv.org/abs/2406.16200</guid>
<content:encoded><![CDATA[
arXiv:2406.16200v2 Announce Type: replace 
Abstract: In this paper, we uniquely study the adversarial robustness of deep neural networks (NN) for classification tasks against that of optimal classifiers. We look at the smallest magnitude of possible additive perturbations that can change a classifier's output. We provide a matrix-theoretic explanation of the adversarial fragility of deep neural networks for classification. In particular, our theoretical results show that a neural network's adversarial robustness can degrade as the input dimension $d$ increases. Analytically, we show that neural networks' adversarial robustness can be only $1/\sqrt{d}$ of the best possible adversarial robustness of optimal classifiers. Our theories match remarkably well with numerical experiments of practically trained NN, including NN for ImageNet images. The matrix-theoretic explanation is consistent with an earlier information-theoretic feature-compression-based explanation for the adversarial fragility of neural networks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Link Prediction with Untrained Message Passing Layers</title>
<link>https://arxiv.org/abs/2406.16687</link>
<guid>https://arxiv.org/abs/2406.16687</guid>
<content:encoded><![CDATA[
arXiv:2406.16687v2 Announce Type: replace 
Abstract: Message passing neural networks (MPNNs) operate on graphs by exchanging information between neigbouring nodes. MPNNs have been successfully applied to various node-, edge-, and graph-level tasks in areas like molecular science, computer vision, natural language processing, and combinatorial optimization. However, most MPNNs require training on large amounts of labeled data, which can be costly and time-consuming. In this work, we explore the use of various untrained message passing layers in graph neural networks, i.e. variants of popular message passing architecture where we remove all trainable parameters that are used to transform node features in the message passing step. Focusing on link prediction, we find that untrained message passing layers can lead to competitive and even superior performance compared to fully trained MPNNs, especially in the presence of high-dimensional features. We provide a theoretical analysis of untrained message passing by relating the inner products of features implicitly produced by untrained message passing layers to path-based topological node similarity measures. As such, untrained message passing architectures can be viewed as a highly efficient and interpretable approach to link prediction.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network Anomaly Traffic Detection via Multi-view Feature Fusion</title>
<link>https://arxiv.org/abs/2409.08020</link>
<guid>https://arxiv.org/abs/2409.08020</guid>
<content:encoded><![CDATA[
arXiv:2409.08020v2 Announce Type: replace 
Abstract: Traditional anomalous traffic detection methods are based on single-view analysis, which has obvious limitations in dealing with complex attacks and encrypted communications. In this regard, we propose a Multi-view Feature Fusion (MuFF) method for network anomaly traffic detection. MuFF models the temporal and interactive relationships of packets in network traffic based on the temporal and interactive viewpoints respectively. It learns temporal and interactive features. These features are then fused from different perspectives for anomaly traffic detection. Extensive experiments on six real traffic datasets show that MuFF has excellent performance in network anomalous traffic detection, which makes up for the shortcomings of detection under a single perspective.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lower-dimensional projections of cellular expression improves cell type classification from single-cell RNA sequencing</title>
<link>https://arxiv.org/abs/2410.09964</link>
<guid>https://arxiv.org/abs/2410.09964</guid>
<content:encoded><![CDATA[
arXiv:2410.09964v2 Announce Type: replace 
Abstract: Single-cell RNA sequencing (scRNA-seq) enables the study of cellular diversity at single cell level. It provides a global view of cell-type specification during the onset of biological mechanisms such as developmental processes and human organogenesis. Various statistical, machine and deep learning-based methods have been proposed for cell-type classification. Most of the methods utilizes unsupervised lower dimensional projections obtained from for a large reference data. In this work, we proposed a reference-based method for cell type classification, called EnProCell. The EnProCell, first, computes lower dimensional projections that capture both the high variance and class separability through an ensemble of principle component analysis and multiple discriminant analysis. In the second phase, EnProCell trains a deep neural network on the lower dimensional representation of data to classify cell types. The proposed method outperformed the existing state-of-the-art methods when tested on four different data sets produced from different single-cell sequencing technologies. The EnProCell showed higher accuracy (98.91) and F1 score (98.64) than other methods for predicting reference from reference datasets. Similarly, EnProCell also showed better performance than existing methods in predicting cell types for data with unknown cell types (query) from reference datasets (accuracy:99.52; F1 score: 99.07). In addition to improved performance, the proposed methodology is simple and does not require more computational resources and time. the EnProCell is available at https://github.com/umar1196/EnProCell.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constrained Optimal Fuel Consumption of HEVs under Observational Noise</title>
<link>https://arxiv.org/abs/2410.20913</link>
<guid>https://arxiv.org/abs/2410.20913</guid>
<content:encoded><![CDATA[
arXiv:2410.20913v3 Announce Type: replace 
Abstract: In our prior work, we investigated the minimum fuel consumption of a hybrid electric vehicle (HEV) under a state-of-charge (SOC) balance constraint, assuming perfect SOC measurements and accurate reference speed profiles. The constrained optimal fuel consumption (COFC) problem was addressed using a constrained reinforcement learning (CRL) framework. However, in real-world scenarios, SOC readings are often corrupted by sensor noise, and reference speeds may deviate from actual driving conditions. To account for these imperfections, this study reformulates the COFC problem by explicitly incorporating observational noise in both SOC and reference speed. We adopt a robust CRL approach, where the noise is modeled as a uniform distribution, and employ a structured training procedure to ensure stability. The proposed method is evaluated through simulations on the Toyota Prius hybrid system (THS), using both the New European Driving Cycle (NEDC) and the Worldwide Harmonized Light Vehicles Test Cycle (WLTC). Results show that fuel consumption and SOC constraint satisfaction remain robust across varying noise levels. Furthermore, the analysis reveals that observational noise in SOC and speed can impact fuel consumption to different extents. To the best of our knowledge, this is the first study to explicitly examine how observational noise -- commonly encountered in dynamometer testing and predictive energy control (PEC) applications -- affects constrained optimal fuel consumption in HEVs.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiscale spatiotemporal heterogeneity analysis of bike-sharing system's self-loop phenomenon: Evidence from Shanghai</title>
<link>https://arxiv.org/abs/2411.17555</link>
<guid>https://arxiv.org/abs/2411.17555</guid>
<content:encoded><![CDATA[
arXiv:2411.17555v3 Announce Type: replace 
Abstract: Bike-sharing is an environmentally friendly shared mobility mode, but its self-loop phenomenon, where bikes are returned to the same station after several time usage, significantly impacts equity in accessing its services. Therefore, this study conducts a multiscale analysis with a spatial autoregressive model and double machine learning framework to assess socioeconomic features and geospatial location's impact on the self-loop phenomenon at metro stations and street scales. The results reveal that bike-sharing self-loop intensity exhibits significant spatial lag effect at street scale and is positively associated with residential land use. Marginal treatment effects of residential land use is higher on streets with middle-aged residents, high fixed employment, and low car ownership. The multimodal public transit condition reveals significant positive marginal treatment effects at both scales. To enhance bike-sharing cooperation, we advocate augmenting bicycle availability in areas with high metro usage and low bus coverage, alongside implementing adaptable redistribution strategies.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems</title>
<link>https://arxiv.org/abs/2412.07067</link>
<guid>https://arxiv.org/abs/2412.07067</guid>
<content:encoded><![CDATA[
arXiv:2412.07067v5 Announce Type: replace 
Abstract: The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for scaling Large Language Models (LLMs) efficiently, but it depends on heterogeneous compute and memory resources. These factors jointly affect system Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing benchmarks often fail to capture these trade-offs accurately, complicating practical deployment decisions. To address this, we introduce MoE-CAP, a benchmark specifically designed for MoE systems. Our analysis reveals that achieving an optimal balance across CAP is difficult with current hardware; MoE systems typically optimize two of the three dimensions at the expense of the third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose the CAP Radar Diagram. We further introduce sparsity-aware performance metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems across diverse hardware platforms and deployment scenarios.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoLaFL: Low-Latency Federated Learning via Forward-only Propagation</title>
<link>https://arxiv.org/abs/2412.14668</link>
<guid>https://arxiv.org/abs/2412.14668</guid>
<content:encoded><![CDATA[
arXiv:2412.14668v3 Announce Type: replace 
Abstract: Federated learning (FL) has emerged as a widely adopted paradigm for enabling edge learning with distributed data while ensuring data privacy. However, the traditional FL with deep neural networks trained via backpropagation can hardly meet the low-latency learning requirements in the sixth generation (6G) mobile networks. This challenge mainly arises from the high-dimensional model parameters to be transmitted and the numerous rounds of communication required for convergence due to the inherent randomness of the training process. To address this issue, we adopt the state-of-the-art principle of maximal coding rate reduction to learn linear discriminative features and extend the resultant white-box neural network into FL, yielding the novel framework of Low-Latency Federated Learning (LoLaFL) via forward-only propagation. LoLaFL enables layer-wise transmissions and aggregation with significantly fewer communication rounds, thereby considerably reducing latency. Additionally, we propose two \emph{nonlinear} aggregation schemes for LoLaFL. The first scheme is based on the proof that the optimal NN parameter aggregation in LoLaFL should be harmonic-mean-like. The second scheme further exploits the low-rank structures of the features and transmits the low-rank-approximated covariance matrices of features to achieve additional latency reduction. Theoretic analysis and experiments are conducted to evaluate the performance of LoLaFL. In comparison with traditional FL, the two nonlinear aggregation schemes for LoLaFL can achieve reductions in latency of over 87\% and 97\%, respectively, while maintaining comparable accuracies.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEASE: Offline Preference-based Reinforcement Learning with High Sample Efficiency</title>
<link>https://arxiv.org/abs/2412.21001</link>
<guid>https://arxiv.org/abs/2412.21001</guid>
<content:encoded><![CDATA[
arXiv:2412.21001v2 Announce Type: replace 
Abstract: Offline preference-based reinforcement learning (PbRL) provides an effective way to overcome the challenges of designing reward and the high costs of online interaction. However, since labeling preference needs real-time human feedback, acquiring sufficient preference labels is challenging. To solve this, this paper proposes a offLine prEference-bAsed RL with high Sample Efficiency (LEASE) algorithm, where a learned transition model is leveraged to generate unlabeled preference data. Considering the pretrained reward model may generate incorrect labels for unlabeled data, we design an uncertainty-aware mechanism to ensure the performance of reward model, where only high confidence and low variance data are selected. Moreover, we provide the generalization bound of reward model to analyze the factors influencing reward accuracy, and demonstrate that the policy learned by LEASE has theoretical improvement guarantee. The developed theory is based on state-action pair, which can be easily combined with other offline algorithms. The experimental results show that LEASE can achieve comparable performance to baseline under fewer preference data without online interaction.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UFGraphFR: Graph Federation Recommendation System based on User Text description features</title>
<link>https://arxiv.org/abs/2501.08044</link>
<guid>https://arxiv.org/abs/2501.08044</guid>
<content:encoded><![CDATA[
arXiv:2501.08044v4 Announce Type: replace 
Abstract: Federated learning offers a privacy-preserving framework for recommendation systems by enabling local data processing; however, data localization introduces substantial obstacles. Traditional federated recommendation approaches treat each user as an isolated entity, failing to construct global user relationship graphs that capture collaborative signals, which limits the accuracy of recommendations. To address this limitation, we derive insight from the insight that semantic similarity reflects preference. similarity, which can be used to improve the construction of user relationship graphs. This paper proposes UFGraphFR, a novel framework with three key components: 1) On the client side, private structured data is first transformed into text descriptions. These descriptions are then encoded into semantic vectors using pre-trained models; 2) On the server side, user relationship graphs are securely reconstructed using aggregated model weights without accessing raw data, followed by information propagation through lightweight graph neural networks; 3) On the client side, user behavior sequences are personalized using Transformer architectures. Extensive experiments conducted on four benchmark datasets demonstrate that UFGraphFR significantly outperforms state-of-the-art baselines in both recommendation accuracy and personalization. The framework also maintains robustness across different pre-trained models, as evidenced by the consistent performance metrics obtained. This work provides a practical method for efficient federated recommendations with strict privacy by using semantic vectors, secure user relationship graphs, and personalized behavior sequences. The code is available at: https://github.com/trueWangSyutung/UFGraphFR
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gene Regulatory Network Inference in the Presence of Selection Bias and Latent Confounders</title>
<link>https://arxiv.org/abs/2501.10124</link>
<guid>https://arxiv.org/abs/2501.10124</guid>
<content:encoded><![CDATA[
arXiv:2501.10124v2 Announce Type: replace 
Abstract: Gene regulatory network inference (GRNI) aims to discover how genes causally regulate each other from gene expression data. It is well-known that statistical dependencies in observed data do not necessarily imply causation, as spurious dependencies may arise from latent confounders, such as non-coding RNAs. Numerous GRNI methods have thus been proposed to address this confounding issue. However, dependencies may also result from selection--only cells satisfying certain survival or inclusion criteria are observed--while these selection-induced spurious dependencies are frequently overlooked in gene expression data analyses. In this work, we show that such selection is ubiquitous and, when ignored or conflated with true regulations, can lead to flawed causal interpretation and misguided intervention recommendations. To address this challenge, a fundamental question arises: can we distinguish dependencies due to regulation, confounding, and crucially, selection? We show that gene perturbations offer a simple yet effective answer: selection-induced dependencies are symmetric under perturbation, while those from regulation or confounding are not. Building on this motivation, we propose GISL (Gene regulatory network Inference in the presence of Selection bias and Latent confounders), a principled algorithm that leverages perturbation data to uncover both true gene regulatory relations and non-regulatory mechanisms of selection and confounding up to the equivalence class. Experiments on synthetic and real-world gene expression data demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Interpolation: Achieving Efficient Conversion Estimation with Flexible Optimization Windows</title>
<link>https://arxiv.org/abs/2501.14103</link>
<guid>https://arxiv.org/abs/2501.14103</guid>
<content:encoded><![CDATA[
arXiv:2501.14103v2 Announce Type: replace 
Abstract: Optimizing conversions is crucial in modern online advertising systems, enabling advertisers to deliver relevant products to users and drive business outcomes. However, accurately predicting conversion events remains challenging due to variable time delays between user interactions (e.g., impressions or clicks) and the actual conversions. These delays vary substantially across advertisers and products, necessitating flexible optimization windows tailored to specific conversion behaviors. To address this, we propose a novel \textit{Personalized Interpolation} method that extends existing models based on fixed conversion windows to support flexible advertiser-specific optimization windows. Our method enables accurate conversion estimation across diverse delay distributions without increasing system complexity. We evaluate the effectiveness of the proposed approach through extensive experiments using a real-world ads conversion model. Our results show that this method achieves both high prediction accuracy and improved efficiency compared to existing solutions. This study demonstrates the potential of our Personalized Interpolation method to improve conversion optimization and support a wider range of advertising strategies in large-scale online advertising systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization Error Analysis for Selective State-Space Models Through the Lens of Attention</title>
<link>https://arxiv.org/abs/2502.01473</link>
<guid>https://arxiv.org/abs/2502.01473</guid>
<content:encoded><![CDATA[
arXiv:2502.01473v3 Announce Type: replace 
Abstract: State-space models (SSMs) have recently emerged as a compelling alternative to Transformers for sequence modeling tasks. This paper presents a theoretical generalization analysis of selective SSMs, the core architectural component behind the Mamba model. We derive a novel covering number-based generalization bound for selective SSMs, building upon recent theoretical advances in the analysis of Transformer models. Using this result, we analyze how the spectral abscissa of the continuous-time state matrix influences the model's stability during training and its ability to generalize across sequence lengths. We empirically validate our findings on a synthetic majority task, the IMDb sentiment classification benchmark, and the ListOps task, demonstrating how our theoretical insights translate into practical model behavior.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning</title>
<link>https://arxiv.org/abs/2502.02770</link>
<guid>https://arxiv.org/abs/2502.02770</guid>
<content:encoded><![CDATA[
arXiv:2502.02770v5 Announce Type: replace 
Abstract: Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\times$ acceleration in self-attention operations and $3.9\times$ acceleration in end-to-end per token latency in long context LLM decoding.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Language Models to Reason Efficiently</title>
<link>https://arxiv.org/abs/2502.04463</link>
<guid>https://arxiv.org/abs/2502.04463</guid>
<content:encoded><![CDATA[
arXiv:2502.04463v4 Announce Type: replace 
Abstract: Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models.
  In this work, we propose to train large reasoning models to reason efficiently. More precisely, we use reinforcement learning (RL) to train reasoning models to dynamically allocate inference-time compute based on task complexity. Our method incentivizes models to minimize unnecessary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Sequence Preconditioning</title>
<link>https://arxiv.org/abs/2502.06545</link>
<guid>https://arxiv.org/abs/2502.06545</guid>
<content:encoded><![CDATA[
arXiv:2502.06545v4 Announce Type: replace 
Abstract: We study the problem of preconditioning in sequential prediction. From the theoretical lens of linear dynamical systems, we show that convolving the target sequence corresponds to applying a polynomial to the hidden transition matrix. Building on this insight, we propose a universal preconditioning method that convolves the target with coefficients from orthogonal polynomials such as Chebyshev or Legendre. We prove that this approach reduces regret for two distinct prediction algorithms and yields the first ever sublinear and hidden-dimension-independent regret bounds (up to logarithmic factors) that hold for systems with marginally table and asymmetric transition matrices. Finally, extensive synthetic and real-world experiments show that this simple preconditioning strategy improves the performance of a diverse range of algorithms, including recurrent neural networks, and generalizes to signals beyond linear dynamical systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Position: Bridge the Gaps between Machine Unlearning and AI Regulation</title>
<link>https://arxiv.org/abs/2502.12430</link>
<guid>https://arxiv.org/abs/2502.12430</guid>
<content:encoded><![CDATA[
arXiv:2502.12430v2 Announce Type: replace 
Abstract: The ''right to be forgotten'' and the data privacy laws that encode it have motivated machine unlearning since its earliest days. Now, some argue that an inbound wave of artificial intelligence regulations -- like the European Union's Artificial Intelligence Act (AIA) -- may offer important new use cases for machine unlearning. However, this position paper argues, this opportunity will only be realized if researchers proactively bridge the (sometimes sizable) gaps between machine unlearning's state of the art and its potential applications to AI regulation. To demonstrate this point, we use the AIA as our primary case study. Specifically, we deliver a ``state of the union'' as regards machine unlearning's current potential (or, in many cases, lack thereof) for aiding compliance with various provisions of the AIA. This starts with a precise cataloging of the potential applications of machine unlearning to AIA compliance. For each, we flag the technical gaps that exist between the potential application and the state of the art of machine unlearning. Finally, we end with a call to action: for machine learning researchers to solve the open technical questions that could unlock machine unlearning's potential to assist compliance with the AIA -- and other AI regulations like it.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Remasking Discrete Diffusion Models with Inference-Time Scaling</title>
<link>https://arxiv.org/abs/2503.00307</link>
<guid>https://arxiv.org/abs/2503.00307</guid>
<content:encoded><![CDATA[
arXiv:2503.00307v3 Announce Type: replace 
Abstract: Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with a blog post on the project page: https://remdm.github.io
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overcoming Non-stationary Dynamics with Evidential Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2503.01468</link>
<guid>https://arxiv.org/abs/2503.01468</guid>
<content:encoded><![CDATA[
arXiv:2503.01468v3 Announce Type: replace 
Abstract: Continuous control of non-stationary environments is a major challenge for deep reinforcement learning algorithms. The time-dependency of the state transition dynamics aggravates the notorious stability problems of model-free deep actor-critic architectures. We posit that two properties will play a key role in overcoming non-stationarity in transition dynamics: (i)~preserving the plasticity of the critic network and (ii) directed exploration for rapid adaptation to changing dynamics. We show that performing on-policy reinforcement learning with an evidential critic provides both. The evidential design ensures a fast and accurate approximation of the uncertainty around the state value, which maintains the plasticity of the critic network by detecting the distributional shifts caused by changes in dynamics. The probabilistic critic also makes the actor training objective a random variable, enabling the use of directed exploration approaches as a by-product. We name the resulting algorithm \emph{Evidential Proximal Policy Optimization (EPPO)} due to the integral role of evidential uncertainty quantification in both policy evaluation and policy improvement stages. Through experiments on non-stationary continuous control tasks, where the environment dynamics change at regular intervals, we demonstrate that our algorithm outperforms state-of-the-art on-policy reinforcement learning variants in both task-specific and overall return.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing the Intent-to-Behavior Gap via Fulfillment Priority Logic</title>
<link>https://arxiv.org/abs/2503.05818</link>
<guid>https://arxiv.org/abs/2503.05818</guid>
<content:encoded><![CDATA[
arXiv:2503.05818v3 Announce Type: replace 
Abstract: Practitioners designing reinforcement learning policies face a fundamental challenge: translating intended behavioral objectives into representative reward functions. This challenge stems from behavioral intent requiring simultaneous achievement of multiple competing objectives, typically addressed through labor-intensive linear reward composition that yields brittle results. Consider the ubiquitous robotics scenario where performance maximization directly conflicts with energy conservation. Such competitive dynamics are resistant to simple linear reward combinations. In this paper, we present the concept of objective fulfillment upon which we build Fulfillment Priority Logic (FPL). FPL allows practitioners to define logical formula representing their intentions and priorities within multi-objective reinforcement learning. Our novel Balanced Policy Gradient algorithm leverages FPL specifications to achieve up to 500\% better sample efficiency compared to Soft Actor Critic. Notably, this work constitutes the first implementation of non-linear utility scalarization design, specifically for continuous control problems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveStitch: Flexible and Fast Conditional Time Series Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2503.06231</link>
<guid>https://arxiv.org/abs/2503.06231</guid>
<content:encoded><![CDATA[
arXiv:2503.06231v3 Announce Type: replace 
Abstract: Generating temporal data under conditions is crucial for forecasting, imputation, and generative tasks. Such data often has metadata and partially observed signals that jointly influence the generated values. However, existing methods face three key limitations: (1) they condition on either the metadata or observed values, but rarely both together; (2) they adopt either training-time approaches that fail to generalize to unseen scenarios, or inference-time approaches that ignore metadata; and (3) they suffer from trade-offs between generation speed and temporal coherence across time windows--choosing either slow but coherent autoregressive methods or fast but incoherent parallel ones. We propose WaveStitch, a novel diffusion-based method to overcome these hurdles through: (1) dual-sourced conditioning on both metadata and partially observed signals; (2) a hybrid training-inference architecture, incorporating metadata during training and observations at inference via gradient-based guidance; and (3) a novel pipeline-style paradigm that generates time windows in parallel while preserving coherence through an inference-time conditional loss and a stitching mechanism. Across diverse datasets, WaveStitch demonstrates adaptability to arbitrary patterns of observed signals, achieving 1.81x lower mean-squared-error compared to the state-of-the-art, and generates data up to 166.48x faster than autoregressive methods while maintaining coherence. Our code is available at: https://github.com/adis98/WaveStitch
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise-based reward-modulated learning</title>
<link>https://arxiv.org/abs/2503.23972</link>
<guid>https://arxiv.org/abs/2503.23972</guid>
<content:encoded><![CDATA[
arXiv:2503.23972v3 Announce Type: replace 
Abstract: The pursuit of energy-efficient and adaptive artificial intelligence (AI) has positioned neuromorphic computing as a promising alternative to conventional computing. However, achieving learning on these platforms requires techniques that prioritize local information while enabling effective credit assignment. Here, we propose noise-based reward-modulated learning (NRL), a novel synaptic plasticity rule that mathematically unifies reinforcement learning and gradient-based optimization with biologically-inspired local updates. NRL addresses the computational bottleneck of exact gradients by approximating them through stochastic neural activity, transforming the inherent noise of biological and neuromorphic substrates into a functional resource. Drawing inspiration from biological learning, our method uses reward prediction errors as its optimization target to generate increasingly advantageous behavior, and eligibility traces to facilitate retrospective credit assignment. Experimental validation on reinforcement tasks, featuring immediate and delayed rewards, shows that NRL achieves performance comparable to baselines optimized using backpropagation, although with slower convergence, while showing significantly superior performance and scalability in multi-layer networks compared to reward-modulated Hebbian learning (RMHL), the most prominent similar approach. While tested on simple architectures, the results highlight the potential of noise-driven, brain-inspired learning for low-power adaptive systems, particularly in computing substrates with locality constraints. NRL offers a theoretically grounded paradigm well-suited for the event-driven characteristics of next-generation neuromorphic AI.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoPDL: Automatic Prompt Optimization for LLM Agents</title>
<link>https://arxiv.org/abs/2504.04365</link>
<guid>https://arxiv.org/abs/2504.04365</guid>
<content:encoded><![CDATA[
arXiv:2504.04365v5 Announce Type: replace 
Abstract: The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dense Backpropagation Improves Training for Sparse Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2504.12463</link>
<guid>https://arxiv.org/abs/2504.12463</guid>
<content:encoded><![CDATA[
arXiv:2504.12463v3 Announce Type: replace 
Abstract: Mixture of Experts (MoE) pretraining is more scalable than dense Transformer pretraining, because MoEs learn to route inputs to a sparse set of their feedforward parameters. However, this means that MoEs only receive a sparse backward update, leading to training instability and suboptimal performance. We present a lightweight approximation method that gives the MoE router a dense gradient update while continuing to sparsely activate its parameters. Our method, which we refer to as Default MoE, substitutes missing expert activations with default outputs consisting of an exponential moving average of expert outputs previously seen over the course of training. This allows the router to receive signals from every expert for each token, leading to significant improvements in training performance. Our Default MoE outperforms standard TopK routing in a variety of settings without requiring significant computational overhead. Code: https://github.com/vatsal0/default-moe.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergence and scaling laws in SGD learning of shallow neural networks</title>
<link>https://arxiv.org/abs/2504.19983</link>
<guid>https://arxiv.org/abs/2504.19983</guid>
<content:encoded><![CDATA[
arXiv:2504.19983v2 Announce Type: replace 
Abstract: We study the complexity of online stochastic gradient descent (SGD) for learning a two-layer neural network with $P$ neurons on isotropic Gaussian data: $f_*(\boldsymbol{x}) = \sum_{p=1}^P a_p\cdot \sigma(\langle\boldsymbol{x},\boldsymbol{v}_p^*\rangle)$, $\boldsymbol{x} \sim \mathcal{N}(0,\boldsymbol{I}_d)$, where the activation $\sigma:\mathbb{R}\to\mathbb{R}$ is an even function with information exponent $k_*>2$ (defined as the lowest degree in the Hermite expansion), $\{\boldsymbol{v}^*_p\}_{p\in[P]}\subset \mathbb{R}^d$ are orthonormal signal directions, and the non-negative second-layer coefficients satisfy $\sum_{p} a_p^2=1$. We focus on the challenging ``extensive-width'' regime $P\gg 1$ and permit diverging condition number in the second-layer, covering as a special case the power-law scaling $a_p\asymp p^{-\beta}$ where $\beta\in\mathbb{R}_{\ge 0}$. We provide a precise analysis of SGD dynamics for the training of a student two-layer network to minimize the mean squared error (MSE) objective, and explicitly identify sharp transition times to recover each signal direction. In the power-law setting, we characterize scaling law exponents for the MSE loss with respect to the number of training samples and SGD steps, as well as the number of parameters in the student neural network. Our analysis entails that while the learning of individual teacher neurons exhibits abrupt transitions, the juxtaposition of $P\gg 1$ emergent learning curves at different timescales leads to a smooth scaling law in the cumulative objective.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture</title>
<link>https://arxiv.org/abs/2505.00316</link>
<guid>https://arxiv.org/abs/2505.00316</guid>
<content:encoded><![CDATA[
arXiv:2505.00316v4 Announce Type: replace 
Abstract: The Cellular-Potts model is a powerful and ubiquitous framework for developing computational models for simulating complex multicellular biological systems. Cellular-Potts models (CPMs) are often computationally expensive due to the explicit modeling of interactions among large numbers of individual model agents and diffusive fields described by partial differential equations (PDEs). In this work, we develop a convolutional neural network (CNN) surrogate model using a U-Net architecture that accounts for periodic boundary conditions. We use this model to accelerate the evaluation of a mechanistic CPM previously used to investigate in vitro vasculogenesis. The surrogate model was trained to predict 100 computational steps ahead (Monte-Carlo steps, MCS), accelerating simulation evaluations by a factor of 590 times compared to CPM code execution. Over multiple recursive evaluations, our model effectively captures the emergent behaviors demonstrated by the original Cellular-Potts model of such as vessel sprouting, extension and anastomosis, and contraction of vascular lacunae. This approach demonstrates the potential for deep learning to serve as efficient surrogate models for CPM simulations, enabling faster evaluation of computationally expensive CPM of biological processes at greater spatial and temporal scales.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-driven software for automated quantification of skeletal metastases and treatment response evaluation using Whole-Body Diffusion-Weighted MRI (WB-DWI) in Advanced Prostate Cancer</title>
<link>https://arxiv.org/abs/2505.09011</link>
<guid>https://arxiv.org/abs/2505.09011</guid>
<content:encoded><![CDATA[
arXiv:2505.09011v2 Announce Type: replace 
Abstract: Quantitative assessment of treatment response in Advanced Prostate Cancer (APC) with bone metastases remains an unmet clinical need. Whole-Body Diffusion-Weighted MRI (WB-DWI) provides two response biomarkers: Total Diffusion Volume (TDV) and global Apparent Diffusion Coefficient (gADC). However, tracking post-treatment changes of TDV and gADC from manually delineated lesions is cumbersome and increases inter-reader variability. We developed a software to automate this process. Core technologies include: (i) a weakly-supervised Residual U-Net model generating a skeleton probability map to isolate bone; (ii) a statistical framework for WB-DWI intensity normalisation, obtaining a signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional neural network that processes outputs from (i) and (ii) to generate a mask of suspected bone lesions, characterised by higher b900 signal intensity due to restricted water diffusion. This mask is applied to the gADC map to extract TDV and gADC statistics. We tested the tool using expert-defined metastatic bone disease delineations on 66 datasets, assessed repeatability of imaging biomarkers (N=10), and compared software-based response assessment with a construct reference standard (N=118). Average dice score between manual and automated delineations was 0.6 for lesions within pelvis and spine, with an average surface distance of 2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC were 8.8% and 5%, respectively. Repeatability analysis showed coefficients of variation of 4.6% for log-TDV and 3.5% for median gADC, with intraclass correlation coefficients of 0.94 or higher. The software achieved 80.5% accuracy, 84.3% sensitivity, and 85.7% specificity in assessing response to treatment. Average computation time was 90s per scan.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.12737</link>
<guid>https://arxiv.org/abs/2505.12737</guid>
<content:encoded><![CDATA[
arXiv:2505.12737v2 Announce Type: replace 
Abstract: Offline goal-conditioned reinforcement learning (GCRL) offers a practical learning paradigm in which goal-reaching policies are trained from abundant state-action trajectory datasets without additional environment interaction. However, offline GCRL still struggles with long-horizon tasks, even with recent advances that employ hierarchical policy structures, such as HIQL. Identifying the root cause of this challenge, we observe the following insight. Firstly, performance bottlenecks mainly stem from the high-level policy's inability to generate appropriate subgoals. Secondly, when learning the high-level policy in the long-horizon regime, the sign of the advantage estimate frequently becomes incorrect. Thus, we argue that improving the value function to produce a clear advantage estimate for learning the high-level policy is essential. In this paper, we propose a simple yet effective solution: Option-aware Temporally Abstracted value learning, dubbed OTA, which incorporates temporal abstraction into the temporal-difference learning process. By modifying the value update to be option-aware, our approach contracts the effective horizon length, enabling better advantage estimates even in long-horizon regimes. We experimentally show that the high-level policy learned using the OTA value function achieves strong performance on complex tasks from OGBench, a recently proposed offline GCRL benchmark, including maze navigation and visual robotic manipulation environments.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning</title>
<link>https://arxiv.org/abs/2505.14125</link>
<guid>https://arxiv.org/abs/2505.14125</guid>
<content:encoded><![CDATA[
arXiv:2505.14125v2 Announce Type: replace 
Abstract: Biological brains learn continually from a stream of unlabeled data, while integrating specialized information from sparsely labeled examples without compromising their ability to generalize. Meanwhile, machine learning methods are susceptible to catastrophic forgetting in this natural learning setting, as supervised specialist fine-tuning degrades performance on the original task. We introduce task-modulated contrastive learning (TMCL), which takes inspiration from the biophysical machinery in the neocortex, using predictive coding principles to integrate top-down information continually and without supervision. We follow the idea that these principles build a view-invariant representation space, and that this can be implemented using a contrastive loss. Then, whenever labeled samples of a new class occur, new affine modulations are learned that improve separation of the new class from all others, without affecting feedforward weights. By co-opting the view-invariance learning mechanism, we then train feedforward weights to match the unmodulated representation of a data sample to its modulated counterparts. This introduces modulation invariance into the representation space, and, by also using past modulations, stabilizes it. Our experiments show improvements in both class-incremental and transfer learning over state-of-the-art unsupervised approaches, as well as over comparable supervised approaches, using as few as 1% of available labels. Taken together, our work suggests that top-down modulations play a crucial role in balancing stability and plasticity.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why and When Deep is Better than Shallow: An Implementation-Agnostic State-Transition View of Depth Supremacy</title>
<link>https://arxiv.org/abs/2505.15064</link>
<guid>https://arxiv.org/abs/2505.15064</guid>
<content:encoded><![CDATA[
arXiv:2505.15064v3 Announce Type: replace 
Abstract: Why and when is deep better than shallow? We answer this question in a framework that is agnostic to network implementation. We formulate a deep model as an abstract state-transition semigroup acting on a general metric space, and separate the implementation (e.g., ReLU nets, transformers, and chain-of-thought) from the abstract state transition. We prove a bias-variance decomposition in which the variance depends only on the abstract depth-$k$ network and not on the implementation (Theorem 1). We further split the bounds into output and hidden parts to tie the depth dependence of the variance to the metric entropy of the state-transition semigroup (Theorem 2). We then investigate implementation-free conditions under which the variance grow polynomially or logarithmically with depth (Section 4). Combining these with exponential or polynomial bias decay identifies four canonical bias-variance trade-off regimes (EL/EP/PL/PP) and produces explicit optimal depths $k^\ast$. Across regimes, $k^\ast>1$ typically holds, giving a rigorous form of depth supremacy. The lowest generalization error bound is achieved under the EL regime (exp-decay bias + log-growth variance), explaining why and when deep is better, especially for iterative or hierarchical concept classes such as neural ODEs, diffusion/score-matching models, and chain-of-thought reasoning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17830</link>
<guid>https://arxiv.org/abs/2505.17830</guid>
<content:encoded><![CDATA[
arXiv:2505.17830v3 Announce Type: replace 
Abstract: Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously acquire diverse behaviors, but faces major challenges in visual environments due to high-dimensional, semantically sparse observations. In the online setting, where agents learn representations while exploring, the latent space evolves with the agent's policy, to capture newly discovered areas of the environment. However, without incentivization to maximize state coverage in the representation, classical approaches based on auto-encoders may converge to latent spaces that over-represent a restricted set of states frequently visited by the agent. This is exacerbated in an intrinsic motivation setting, where the agent uses the distribution encoded in the latent space to sample the goals it learns to master. To address this issue, we propose to progressively enforce distributional shifts towards a uniform distribution over the full state space, to ensure a full coverage of skills that can be learned in the environment. We introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that combines the $\beta$-VAE framework with Distributionally Robust Optimization. DRAG leverages an adversarial neural weighter of training states of the VAE, to account for the mismatch between the current data distribution and unseen parts of the environment. This allows the agent to construct semantically meaningful latent spaces beyond its immediate experience. Our approach improves state space coverage and downstream control performance on hard exploration environments such as mazes and robotic control involving walls to bypass, without pre-training nor prior environment knowledge.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2505.20241</link>
<guid>https://arxiv.org/abs/2505.20241</guid>
<content:encoded><![CDATA[
arXiv:2505.20241v3 Announce Type: replace 
Abstract: Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM's domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Strategic Classification with Non-Linear Classifiers</title>
<link>https://arxiv.org/abs/2505.23443</link>
<guid>https://arxiv.org/abs/2505.23443</guid>
<content:encoded><![CDATA[
arXiv:2505.23443v2 Announce Type: replace 
Abstract: In strategic classification, the standard supervised learning setting is extended to support the notion of strategic user behavior in the form of costly feature manipulations made in response to a classifier. While standard learning supports a broad range of model classes, the study of strategic classification has, so far, been dedicated mostly to linear classifiers. This work aims to expand the horizon by exploring how strategic behavior manifests under non-linear classifiers and what this implies for learning. We take a bottom-up approach showing how non-linearity affects decision boundary points, classifier expressivity, and model class complexity. Our results show how, unlike the linear case, strategic behavior may either increase or decrease effective class complexity, and that the complexity decrease may be arbitrarily large. Another key finding is that universal approximators (e.g., neural nets) are no longer universal once the environment is strategic. We demonstrate empirically how this can create performance gaps even on an unrestricted model class.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit</title>
<link>https://arxiv.org/abs/2506.03093</link>
<guid>https://arxiv.org/abs/2506.03093</guid>
<content:encoded><![CDATA[
arXiv:2506.03093v2 Announce Type: replace 
Abstract: Motivated by the hypothesis that neural network representations encode abstract, interpretable features as linearly accessible, approximately orthogonal directions, sparse autoencoders (SAEs) have become a popular tool in interpretability. However, recent work has demonstrated phenomenology of model representations that lies outside the scope of this hypothesis, showing signatures of hierarchical, nonlinear, and multi-dimensional features. This raises the question: do SAEs represent features that possess structure at odds with their motivating hypothesis? If not, does avoiding this mismatch help identify said features and gain further insights into neural network representations? To answer these questions, we take a construction-based approach and re-contextualize the popular matching pursuits (MP) algorithm from sparse coding to design MP-SAE -- an SAE that unrolls its encoder into a sequence of residual-guided steps, allowing it to capture hierarchical and nonlinearly accessible features. Comparing this architecture with existing SAEs on a mixture of synthetic and natural data settings, we show: (i) hierarchical concepts induce conditionally orthogonal features, which existing SAEs are unable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE recovers highly meaningful features, helping us unravel shared structure in the seemingly dichotomous representation spaces of different modalities in a vision-language model, hence demonstrating the assumption that useful features are solely linearly accessible is insufficient. We also show that the sequential encoder principle of MP-SAE affords an additional benefit of adaptive sparsity at inference time, which may be of independent interest. Overall, we argue our results provide credence to the idea that interpretability should begin with the phenomenology of representations, with methods emerging from assumptions that fit it.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliably Detecting Model Failures in Deployment Without Labels</title>
<link>https://arxiv.org/abs/2506.05047</link>
<guid>https://arxiv.org/abs/2506.05047</guid>
<content:encoded><![CDATA[
arXiv:2506.05047v4 Announce Type: replace 
Abstract: The distribution of data changes over time; models operating in dynamic environments need retraining. But knowing when to retrain, without access to labels, is an open challenge since some, but not all shifts degrade model performance. This paper formalizes and addresses the problem of post-deployment deterioration (PDD) monitoring. We propose D3M, a practical and efficient monitoring algorithm based on the disagreement of predictive models, achieving low false positive rates under non-deteriorating shifts and provides sample complexity bounds for high true positive rates under deteriorating shifts. Empirical results on both standard benchmark and a real-world large-scale internal medicine dataset demonstrate the effectiveness of the framework and highlight its viability as an alert mechanism for high-stakes machine learning pipelines.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Sparse Autoencoders: From Shallow Design to Matching Pursuit</title>
<link>https://arxiv.org/abs/2506.05239</link>
<guid>https://arxiv.org/abs/2506.05239</guid>
<content:encoded><![CDATA[
arXiv:2506.05239v2 Announce Type: replace 
Abstract: Sparse autoencoders (SAEs) have recently become central tools for interpretability, leveraging dictionary learning principles to extract sparse, interpretable features from neural representations whose underlying structure is typically unknown. This paper evaluates SAEs in a controlled setting using MNIST, which reveals that current shallow architectures implicitly rely on a quasi-orthogonality assumption that limits the ability to extract correlated features. To move beyond this, we compare them with an iterative SAE that unrolls Matching Pursuit (MP-SAE), enabling the residual-guided extraction of correlated features that arise in hierarchical settings such as handwritten digit generation while guaranteeing monotonic improvement of the reconstruction as more atoms are selected.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Collapse in Cumulative Link Models for Ordinal Regression: An Analysis with Unconstrained Feature Model</title>
<link>https://arxiv.org/abs/2506.05801</link>
<guid>https://arxiv.org/abs/2506.05801</guid>
<content:encoded><![CDATA[
arXiv:2506.05801v2 Announce Type: replace 
Abstract: A phenomenon known as ''Neural Collapse (NC)'' in deep classification tasks, in which the penultimate-layer features and the final classifiers exhibit an extremely simple geometric structure, has recently attracted considerable attention, with the expectation that it can deepen our understanding of how deep neural networks behave. The Unconstrained Feature Model (UFM) has been proposed to explain NC theoretically, and there emerges a growing body of work that extends NC to tasks other than classification and leverages it for practical applications. In this study, we investigate whether a similar phenomenon arises in deep Ordinal Regression (OR) tasks, via combining the cumulative link model for OR and UFM. We show that a phenomenon we call Ordinal Neural Collapse (ONC) indeed emerges and is characterized by the following three properties: (ONC1) all optimal features in the same class collapse to their within-class mean when regularization is applied; (ONC2) these class means align with the classifier, meaning that they collapse onto a one-dimensional subspace; (ONC3) the optimal latent variables (corresponding to logits or preactivations in classification tasks) are aligned according to the class order, and in particular, in the zero-regularization limit, a highly local and simple geometric relationship emerges between the latent variables and the threshold values. We prove these properties analytically within the UFM framework with fixed threshold values and corroborate them empirically across a variety of datasets. We also discuss how these insights can be leveraged in OR, highlighting the use of fixed thresholds.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Probabilistic Framework for Learning with Hard Constraints</title>
<link>https://arxiv.org/abs/2506.07003</link>
<guid>https://arxiv.org/abs/2506.07003</guid>
<content:encoded><![CDATA[
arXiv:2506.07003v2 Announce Type: replace 
Abstract: We present ProbHardE2E, a probabilistic forecasting framework that incorporates hard operational/physical constraints, and provides uncertainty quantification. Our methodology uses a novel differentiable probabilistic projection layer (DPPL) that can be combined with a wide range of neural network architectures. DPPL allows the model to learn the system in an end-to-end manner, compared to other approaches where constraints are satisfied either through a post-processing step or at inference. ProbHardE2E optimizes a strictly proper scoring rule, without making any distributional assumptions on the target, which enables it to obtain robust distributional estimates (in contrast to existing approaches that generally optimize likelihood-based objectives, which are heavily biased by their distributional assumptions and model choices); and it can incorporate a range of non-linear constraints (increasing the power of modeling and flexibility). We apply ProbHardE2E in learning partial differential equations with uncertainty estimates and to probabilistic time-series forecasting, showcasing it as a broadly applicable general framework that connects these seemingly disparate domains.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABS: Enforcing Constraint Satisfaction On Generated Sequences Via Automata-Guided Beam Search</title>
<link>https://arxiv.org/abs/2506.09701</link>
<guid>https://arxiv.org/abs/2506.09701</guid>
<content:encoded><![CDATA[
arXiv:2506.09701v2 Announce Type: replace 
Abstract: Sequence generation and prediction form a cornerstone of modern machine learning, with applications spanning natural language processing, program synthesis, and time-series forecasting. These tasks are typically modeled in an autoregressive fashion, where each token is generated conditional on the preceding ones, and beam search is commonly used to balance exploration and fluency during decoding. While deep learning models and Large Language Models (LLMs) excel at capturing statistical patterns in this setting, they remain ill-equipped to guarantee compliance with formal constraints. In this paper, we introduce ABS: a general and model-agnostic inference-time algorithm that guarantees compliance with any constraint that can be compiled into a Deterministic Finite Automaton (DFA), without requiring retraining. ABS leverages the DFA to guide a constrained variant of beam search: at each decoding step, transitions leading to violations are masked, while remaining paths are dynamically re-ranked according to both the model's probabilities and the automaton's acceptance structure. We formally prove that the resulting sequences are guaranteed to satisfy the given constraints, and we empirically demonstrate that ABS also improves output quality. We validate our approach on three distinct tasks: constrained image-stream classification, controlled text generation, and text infilling. In all settings, ABS achieves perfect constraint satisfaction, while outperforming or matching state-of-the-art baselines on standard quality metrics and efficiency.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Clients Are Equal: Collaborative Model Personalization on Heterogeneous Multi-Modal Clients</title>
<link>https://arxiv.org/abs/2506.11024</link>
<guid>https://arxiv.org/abs/2506.11024</guid>
<content:encoded><![CDATA[
arXiv:2506.11024v3 Announce Type: replace 
Abstract: As AI becomes more personal, e.g., Agentic AI, there is an increasing need for personalizing models for various use cases. Personalized federated learning (PFL) enables each client to collaboratively leverage other clients' knowledge for better adaptation to the task of interest, without privacy risks. Despite its potential, existing PFL methods remain confined to rather simplified scenarios where data and models are the same across clients. To move towards realistic scenarios, we propose FedMosaic, a method that jointly addresses data and model heterogeneity with a task-relevance-aware model aggregation strategy to reduce parameter interference, and a dimension-invariant module that enables knowledge sharing across heterogeneous architectures without huge computational cost. To mimic the real-world task diversity, we propose a multi-modal PFL benchmark spanning 40 distinct tasks with distribution shifts over time. The empirical study shows that FedMosaic outperforms the state-of-the-art PFL methods, excelling in both personalization and generalization capabilities under challenging, realistic scenarios.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two-Player Zero-Sum Games with Bandit Feedback</title>
<link>https://arxiv.org/abs/2506.14518</link>
<guid>https://arxiv.org/abs/2506.14518</guid>
<content:encoded><![CDATA[
arXiv:2506.14518v2 Announce Type: replace 
Abstract: We study a two-player zero-sum game in which the row player aims to maximize their payoff against an adversarial column player, under an unknown payoff matrix estimated through bandit feedback. We propose three algorithms based on the Explore-Then-Commit framework. The first adapts it to zero-sum games, the second incorporates adaptive elimination that leverages the $\varepsilon$-Nash Equilibrium property to efficiently select the optimal action pair, and the third extends the elimination algorithm by employing non-uniform exploration. Our objective is to demonstrate the applicability of ETC in a zero-sum game setting by focusing on learning pure strategy Nash Equilibria. A key contribution of our work is a derivation of instance-dependent upper bounds on the expected regret of our proposed algorithms, which has received limited attention in the literature on zero-sum games. Particularly, after $T$ rounds, we achieve an instance-dependent regret upper bounds of $O(\Delta + \sqrt{T})$ for ETC in zero-sum game setting and $O(\log (T \Delta^2) / \Delta)$ for the adaptive elimination algorithm and its variant with non-uniform exploration, where $\Delta$ denotes the suboptimality gap. Therefore, our results indicate that ETC-based algorithms perform effectively in adversarial game settings, achieving regret bounds comparable to existing methods while providing insight through instance-dependent analysis.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models</title>
<link>https://arxiv.org/abs/2506.17139</link>
<guid>https://arxiv.org/abs/2506.17139</guid>
<content:encoded><![CDATA[
arXiv:2506.17139v2 Announce Type: replace 
Abstract: In recent years, diffusion models trained on equilibrium molecular distributions have proven effective for sampling biomolecules. Beyond direct sampling, the score of such a model can also be used to derive the forces that act on molecular systems. However, while classical diffusion sampling usually recovers the training distribution, the corresponding energy-based interpretation of the learned score is often inconsistent with this distribution, even for low-dimensional toy systems. We trace this inconsistency to inaccuracies of the learned score at very small diffusion timesteps, where the model must capture the correct evolution of the data distribution. In this regime, diffusion models fail to satisfy the Fokker--Planck equation, which governs the evolution of the score. We interpret this deviation as one source of the observed inconsistencies and propose an energy-based diffusion model with a Fokker--Planck-derived regularization term to enforce consistency. We demonstrate our approach by sampling and simulating multiple biomolecular systems, including fast-folding proteins, and by introducing a state-of-the-art transferable Boltzmann emulator for dipeptides that supports simulation and achieves improved consistency and efficient sampling. Our code, model weights, and self-contained JAX and PyTorch notebooks are available at https://github.com/noegroup/ScoreMD.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Hierarchical Spaces: A Review and Unified Framework for Surrogate-Based Architecture Design</title>
<link>https://arxiv.org/abs/2506.22621</link>
<guid>https://arxiv.org/abs/2506.22621</guid>
<content:encoded><![CDATA[
arXiv:2506.22621v2 Announce Type: replace 
Abstract: Simulation-based problems involving mixed-variable inputs frequently feature domains that are hierarchical, conditional, heterogeneous, or tree-structured. These characteristics pose challenges for data representation, modeling, and optimization. This paper reviews extensive literature on these structured input spaces and proposes a unified framework that generalizes existing approaches.
  In this framework, input variables may be continuous, integer, or categorical. A variable is described as meta if its value governs the presence of other decreed variables, enabling the modeling of conditional and hierarchical structures. We further introduce the concept of partially-decreed variables, whose activation depends on contextual conditions.
  To capture these inter-variable hierarchical relationships, we introduce design space graphs, combining principles from feature modeling and graph theory. This allows the definition of general hierarchical domains suitable for describing complex system architectures.
  Our framework defines hierarchical distances and kernels to enable surrogate modeling and optimization on hierarchical domains. We demonstrate its effectiveness on complex system design problems, including a neural network and a green-aircraft case study. Our methods are available in the open-source Surrogate Modeling Toolbox (SMT 2.0).
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aggregation of Published Non-Uniform Axial Power Data for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark</title>
<link>https://arxiv.org/abs/2507.00034</link>
<guid>https://arxiv.org/abs/2507.00034</guid>
<content:encoded><![CDATA[
arXiv:2507.00034v2 Announce Type: replace 
Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water reactors, defining safe thermal-hydraulic operating limits. To support Phase II of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power profiles, this work compiles and digitizes a broad CHF dataset covering both uniform and non-uniform axial heating conditions. Heating profiles were extracted from technical reports, interpolated onto a consistent axial mesh, validated via energy-balance checks, and encoded in machine-readable formats for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating and degrade markedly when applied to non-uniform profiles, while modern tabular methods offer improved but still imperfect predictions. A neural network trained solely on uniform data performs well in that regime but fails to generalize to spatially varying scenarios, underscoring the need for models that explicitly incorporate axial power distributions. By providing these curated datasets and baseline modeling results, this study lays the groundwork for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relational Causal Discovery with Latent Confounders</title>
<link>https://arxiv.org/abs/2507.01700</link>
<guid>https://arxiv.org/abs/2507.01700</guid>
<content:encoded><![CDATA[
arXiv:2507.01700v2 Announce Type: replace 
Abstract: Estimating causal effects from real-world relational data can be challenging when the underlying causal model and potential confounders are unknown. While several causal discovery algorithms exist for learning causal models with latent confounders from data, they assume that the data is independent and identically distributed (i.i.d.) and are not well-suited for learning from relational data. Similarly, existing relational causal discovery algorithms assume causal sufficiency, which is unrealistic for many real-world datasets. To address this gap, we propose RelFCI, a sound and complete causal discovery algorithm for relational data with latent confounders. Our work builds upon the Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms and it defines new graphical models, necessary to support causal discovery in relational domains. We also establish soundness and completeness guarantees for relational d-separation with latent confounders. We present experimental results demonstrating the effectiveness of RelFCI in identifying the correct causal structure in relational causal models with latent confounders.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate</title>
<link>https://arxiv.org/abs/2507.07129</link>
<guid>https://arxiv.org/abs/2507.07129</guid>
<content:encoded><![CDATA[
arXiv:2507.07129v2 Announce Type: replace 
Abstract: The prevailing paradigm for scaling large language models (LLMs) involves monolithic, end-to-end training, a resource-intensive process that lacks flexibility. This paper explores an alternative, constructive scaling paradigm, enabled by the principle of emergent semantics in Transformers with frozen, non-semantic input embeddings. We posit that because high-level meaning is a compositional property of a Transformer's deep layers, not its input vectors, the embedding layer and trained lower layers can serve as a fixed foundation. This liberates backpropagation to focus solely on newly added components, making incremental growth viable. We operationalize this with a layer-wise constructive methodology that combines strict layer freezing in early stages with efficient, holistic fine-tuning of the entire model stack via low-rank adaptation (LoRA) as complexity increases. This method not only demonstrates stable convergence but also reveals a direct correlation between model depth and the emergence of complex reasoning abilities, such as those required for SQuAD, which are absent in shallower models. In a controlled study, our constructively grown model rivals the performance of a monolithically trained baseline of the same size, validating the efficiency and efficacy of the approach. Our findings suggest a path towards a paradigm shift from monolithic optimization towards a more biological or constructive model of AI development. This opens a path for more resource-efficient scaling, continual learning, and a more modular approach to building powerful AI systems. We release all code and models to facilitate further research.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation</title>
<link>https://arxiv.org/abs/2508.19999</link>
<guid>https://arxiv.org/abs/2508.19999</guid>
<content:encoded><![CDATA[
arXiv:2508.19999v2 Announce Type: replace 
Abstract: This paper introduces an algorithm to select demonstration examples for in-context learning of a query set. Given a set of $n$ examples, how can we quickly select $k$ out of $n$ to best serve as the conditioning for downstream inference? This problem has broad applications in prompt tuning and chain-of-thought reasoning. Since model weights remain fixed during in-context learning, previous work has sought to design methods based on the similarity of token embeddings. This work proposes a new approach based on gradients of the output taken in the input embedding space. Our approach estimates model outputs through a first-order approximation using the gradients. Then, we apply this estimation to multiple randomly sampled subsets. Finally, we aggregate the sampled subset outcomes to form an influence score for each demonstration, and select $k$ most relevant examples. This procedure only requires pre-computing model outputs and gradients once, resulting in a linear-time algorithm relative to model and training set sizes. Extensive experiments across various models and datasets validate the efficiency of our approach. We show that the gradient estimation procedure yields approximations of full inference with less than ${1}\%$ error across six datasets. This allows us to scale up subset selection that would otherwise run full inference by up to ${37.7}\times$ on models with up to $34$ billion parameters, and outperform existing selection methods based on input embeddings by ${11}\%$ on average.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prior-Guided Flow Matching for Target-Aware Molecule Design with Learnable Atom Number</title>
<link>https://arxiv.org/abs/2509.01486</link>
<guid>https://arxiv.org/abs/2509.01486</guid>
<content:encoded><![CDATA[
arXiv:2509.01486v2 Announce Type: replace 
Abstract: Structure-based drug design (SBDD), aiming to generate 3D molecules with high binding affinity toward target proteins, is a vital approach in novel drug discovery. Although recent generative models have shown great potential, they suffer from unstable probability dynamics and mismatch between generated molecule size and the protein pockets geometry, resulting in inconsistent quality and off-target effects. We propose PAFlow, a novel target-aware molecular generation model featuring prior interaction guidance and a learnable atom number predictor. PAFlow adopts the efficient flow matching framework to model the generation process and constructs a new form of conditional flow matching for discrete atom types. A protein-ligand interaction predictor is incorporated to guide the vector field toward higher-affinity regions during generation, while an atom number predictor based on protein pocket information is designed to better align generated molecule size with target geometry. Extensive experiments on the CrossDocked2020 benchmark show that PAFlow achieves a new state-of-the-art in binding affinity (up to -8.31 Avg. Vina Score), simultaneously maintains favorable molecular properties.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Evolving Complexity: An Adversarial Framework for Automatic MARL Curricula</title>
<link>https://arxiv.org/abs/2509.03771</link>
<guid>https://arxiv.org/abs/2509.03771</guid>
<content:encoded><![CDATA[
arXiv:2509.03771v3 Announce Type: replace 
Abstract: The advancement of general-purpose intelligent agents is intrinsically linked to the environments in which they are trained. While scaling models and datasets has yielded remarkable capabilities, scaling the complexity, diversity, and interactivity of environments remains a crucial bottleneck. Hand-crafted environments are finite and often contain implicit biases, limiting the potential for agents to develop truly generalizable and robust skills. In this work, we propose a paradigm for generating a boundless and adaptive curriculum of challenges by framing the environment generation process as an adversarial game. We introduce a system where a team of cooperative multi-agent defenders learns to survive against a procedurally generative attacker. The attacker agent learns to produce increasingly challenging configurations of enemy units, dynamically creating novel worlds tailored to exploit the defenders' current weaknesses. Concurrently, the defender team learns cooperative strategies to overcome these generated threats. This co-evolutionary dynamic creates a self-scaling environment where complexity arises organically from the adversarial interaction, providing an effectively infinite stream of novel and relevant training data. We demonstrate that with minimal training, this approach leads to the emergence of complex, intelligent behaviors, such as flanking and shielding by the attacker, and focus-fire and spreading by the defenders. Our findings suggest that adversarial co-evolution is a powerful mechanism for automatically scaling environmental complexity, driving agents towards greater robustness and strategic depth.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Compositional Kernel Model for Feature Learning</title>
<link>https://arxiv.org/abs/2509.14158</link>
<guid>https://arxiv.org/abs/2509.14158</guid>
<content:encoded><![CDATA[
arXiv:2509.14158v2 Announce Type: replace 
Abstract: We study a compositional variant of kernel ridge regression in which the predictor is applied to a coordinate-wise reweighting of the inputs. Formulated as a variational problem, this model provides a simple testbed for feature learning in compositional architectures. From the perspective of variable selection, we show how relevant variables are recovered while noise variables are eliminated. We establish guarantees showing that both global minimizers and stationary points discard noise coordinates when the noise variables are Gaussian distributed. A central finding is that $\ell_1$-type kernels, such as the Laplace kernel, succeed in recovering features contributing to nonlinear effects at stationary points, whereas Gaussian kernels recover only linear ones.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowRL: Matching Reward Distributions for LLM Reasoning</title>
<link>https://arxiv.org/abs/2509.15207</link>
<guid>https://arxiv.org/abs/2509.15207</guid>
<content:encoded><![CDATA[
arXiv:2509.15207v3 Announce Type: replace 
Abstract: We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification</title>
<link>https://arxiv.org/abs/2509.15591</link>
<guid>https://arxiv.org/abs/2509.15591</guid>
<content:encoded><![CDATA[
arXiv:2509.15591v2 Announce Type: replace 
Abstract: Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at https://github.com/microsoft/latent-zoning-networks. The project website is at https://zinanlin.me/blogs/latent_zoning_networks.html.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAM-DTI: adaptive multimodal representation learning for drug target interaction prediction</title>
<link>https://arxiv.org/abs/2509.21971</link>
<guid>https://arxiv.org/abs/2509.21971</guid>
<content:encoded><![CDATA[
arXiv:2509.21971v2 Announce Type: replace 
Abstract: Drug target interaction (DTI) prediction is a cornerstone of computational drug discovery, enabling rational design, repurposing, and mechanistic insights. While deep learning has advanced DTI modeling, existing approaches primarily rely on SMILES protein pairs and fail to exploit the rich multimodal information available for small molecules and proteins. We introduce GRAMDTI, a pretraining framework that integrates multimodal molecular and protein inputs into unified representations. GRAMDTI extends volume based contrastive learning to four modalities, capturing higher-order semantic alignment beyond conventional pairwise approaches. To handle modality informativeness, we propose adaptive modality dropout, dynamically regulating each modality's contribution during pre-training. Additionally, IC50 activity measurements, when available, are incorporated as weak supervision to ground representations in biologically meaningful interaction strengths. Experiments on four publicly available datasets demonstrate that GRAMDTI consistently outperforms state of the art baselines. Our results highlight the benefits of higher order multimodal alignment, adaptive modality utilization, and auxiliary supervision for robust and generalizable DTI prediction.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Multivariate Time Series Forecasting with Missing Values</title>
<link>https://arxiv.org/abs/2509.23494</link>
<guid>https://arxiv.org/abs/2509.23494</guid>
<content:encoded><![CDATA[
arXiv:2509.23494v2 Announce Type: replace 
Abstract: Missing values are common in real-world time series, and multivariate time series forecasting with missing values (MTSF-M) has become a crucial area of research for ensuring reliable predictions. To address the challenge of missing data, current approaches have developed an imputation-then-prediction framework that uses imputation modules to fill in missing values, followed by forecasting on the imputed data. However, this framework overlooks a critical issue: there is no ground truth for the missing values, making the imputation process susceptible to errors that can degrade prediction accuracy. In this paper, we conduct a systematic empirical study and reveal that imputation without direct supervision can corrupt the underlying data distribution and actively degrade prediction accuracy. To address this, we propose a paradigm shift that moves away from imputation and directly predicts from the partially observed time series. We introduce Consistency-Regularized Information Bottleneck (CRIB), a novel framework built on the Information Bottleneck principle. CRIB combines a unified-variate attention mechanism with a consistency regularization scheme to learn robust representations that filter out noise introduced by missing values while preserving essential predictive signals. Comprehensive experiments on four real-world datasets demonstrate the effectiveness of CRIB, which predicts accurately even under high missing rates. Our code is available in https://github.com/Muyiiiii/CRIB.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling</title>
<link>https://arxiv.org/abs/2509.24655</link>
<guid>https://arxiv.org/abs/2509.24655</guid>
<content:encoded><![CDATA[
arXiv:2509.24655v2 Announce Type: replace 
Abstract: Language models are increasingly applied to biological sequences like proteins and mRNA, yet their default Euclidean geometry may mismatch the hierarchical structures inherent to biological data. While hyperbolic geometry provides a better alternative for accommodating hierarchical data, it has yet to find a way into language modeling for mRNA sequences. In this work, we introduce HyperHELM, a framework that implements masked language model pre-training in hyperbolic space for mRNA sequences. Using a hybrid design with hyperbolic layers atop Euclidean backbone, HyperHELM aligns learned representations with the biological hierarchy defined by the relationship between mRNA and amino acids. Across multiple multi-species datasets, it outperforms Euclidean baselines on 9 out of 10 tasks involving property prediction, with 10% improvement on average, and excels in out-of-distribution generalization to long and low-GC content sequences; for antibody region annotation, it surpasses hierarchy-aware Euclidean models by 3% in annotation accuracy. Our results highlight hyperbolic geometry as an effective inductive bias for hierarchical language modeling of mRNA sequences.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Effective Are Time-Series Models for Precipitation Nowcasting? A Comprehensive Benchmark for GNSS-based Precipitation Nowcasting</title>
<link>https://arxiv.org/abs/2509.25263</link>
<guid>https://arxiv.org/abs/2509.25263</guid>
<content:encoded><![CDATA[
arXiv:2509.25263v3 Announce Type: replace 
Abstract: Precipitation Nowcasting, which aims to predict precipitation within the next 0 to 6 hours, is critical for disaster mitigation and real-time response planning. However, most time series forecasting benchmarks in meteorology are evaluated on variables with strong periodicity, such as temperature and humidity, which fail to reflect model capabilities in more complex and practically meteorology scenarios like precipitation nowcasting. To address this gap, we propose RainfallBench, a benchmark designed for precipitation nowcasting, a highly challenging and practically relevant task characterized by zero inflation, temporal decay, and non-stationarity, focusing on predicting precipitation within the next 0 to 6 hours. The dataset is derived from five years of meteorological observations, recorded at hourly intervals across six essential variables, and collected from more than 140 Global Navigation Satellite System (GNSS) stations globally. In particular, it incorporates precipitable water vapor (PWV), a crucial indicator of rainfall that is absent in other datasets. We further design specialized evaluation protocols to assess model performance on key meteorological challenges, including multi-scale prediction, multi-resolution forecasting, and extreme rainfall events, benchmarking 17 state-of-the-art models across six major architectures on RainfallBench. Additionally, to address the zero-inflation and temporal decay issues overlooked by existing models, we introduce Bi-Focus Precipitation Forecaster (BFPF), a plug-and-play module that incorporates domain-specific priors to enhance rainfall time series forecasting. Statistical analysis and ablation studies validate the comprehensiveness of our dataset as well as the superiority of our methodology.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReNF: Rethinking the Design Space of Neural Long-Term Time Series Forecasters</title>
<link>https://arxiv.org/abs/2509.25914</link>
<guid>https://arxiv.org/abs/2509.25914</guid>
<content:encoded><![CDATA[
arXiv:2509.25914v3 Announce Type: replace 
Abstract: Neural Forecasters (NFs) are a cornerstone of Long-term Time Series Forecasting (LTSF). However, progress has been hampered by an overemphasis on architectural complexity at the expense of fundamental forecasting principles. In this work, we return to first principles to redesign the LTSF paradigm. We begin by introducing a Multiple Neural Forecasting Theorem that provides a theoretical basis for our approach. We propose Boosted Direct Output (BDO), a novel forecasting strategy that synergistically combines the advantages of both Auto-Regressive (AR) and Direct Output (DO). In addition, we stabilize the learning process by smoothly tracking the model's parameters. Extensive experiments show that these principled improvements enable a simple MLP to achieve state-of-the-art performance, outperforming recent, complex models in nearly all cases, without any specific considerations in the area. Finally, we empirically verify our theorem, establishing a dynamic performance bound and identifying promising directions for future research. The code for review is available at: .
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing</title>
<link>https://arxiv.org/abs/2510.04263</link>
<guid>https://arxiv.org/abs/2510.04263</guid>
<content:encoded><![CDATA[
arXiv:2510.04263v2 Announce Type: replace 
Abstract: Learning causal structure from observational data is especially challenging when latent variables or selection bias are present. The Fast Causal Inference (FCI) algorithm addresses this setting but often performs exhaustive conditional independence tests across many subsets, leading to spurious independence claims, extra or missing edges, and unreliable orientations. We present a family of score-guided mixed-strategy causal search algorithms that build on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI, straightforward variants of GFCI that substitute BOSS or GRaSP for FGES, thereby retaining correctness while incurring different scalability tradeoffs. Second, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method that improves upon these variants by replacing exhaustive all-subsets testing with targeted tests guided by BOSS, yielding well-formed PAGs with higher precision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also known as BOSS-POD), which bypasses latent-variable-specific reasoning and directly returns the PAG of the BOSS DAG. Although not strictly correct in the FCI sense, it scales better and often achieves superior accuracy in practice. Simulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI provide sound baselines, FCIT improves both efficiency and reliability, and LV-Dumb offers a practical heuristic with strong empirical performance. Together, these method highlight the value of score-guided and targeted strategies for scalable latent-variable causal discovery.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Fast Top-K Selection for Large-Scale Recommendation</title>
<link>https://arxiv.org/abs/2510.11472</link>
<guid>https://arxiv.org/abs/2510.11472</guid>
<content:encoded><![CDATA[
arXiv:2510.11472v2 Announce Type: replace 
Abstract: Cascade ranking is a widely adopted paradigm in large-scale information retrieval systems for Top-K item selection. However, the Top-K operator is non-differentiable, hindering end-to-end training. Existing methods include Learning-to-Rank approaches (e.g., LambdaLoss), which optimize ranking metrics like NDCG and suffer from objective misalignment, and differentiable sorting-based methods (e.g., ARF, LCRON), which relax permutation matrices for direct Top-K optimization but introduce gradient conflicts through matrix aggregation. A promising alternative is to directly construct a differentiable approximation of the Top-K selection operator, bypassing the use of soft permutation matrices. However, even state-of-the-art differentiable Top-K operator (e.g., LapSum) require $O(n \log n)$ complexity due to their dependence on sorting for solving the threshold. Thus, we propose DFTopK, a novel differentiable Top-K operator achieving optimal $O(n)$ time complexity. By relaxing normalization constraints, DFTopK admits a closed-form solution and avoids sorting. DFTopK also avoids the gradient conflicts inherent in differentiable sorting-based methods. We evaluate DFTopK on both the public benchmark RecFLow and an industrial system. Experimental results show that DFTopK significantly improves training efficiency while achieving superior performance, which enables us to scale up training samples more efficiently. In the online A/B test, DFTopK yielded a +1.77% revenue lift with the same computational budget compared to the baseline. To the best of our knowledge, this work is the first to introduce differentiable Top-K operators into recommendation systems and the first to achieve theoretically optimal linear-time complexity for Top-K selection. We have open-sourced our implementation to facilitate future research in both academia and industry.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reflections from Research Roundtables at the Conference on Health, Inference, and Learning (CHIL) 2025</title>
<link>https://arxiv.org/abs/2510.15217</link>
<guid>https://arxiv.org/abs/2510.15217</guid>
<content:encoded><![CDATA[
arXiv:2510.15217v3 Announce Type: replace 
Abstract: The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025), hosted by the Association for Health Learning and Inference (AHLI), was held in person on June 25-27, 2025, at the University of California, Berkeley, in Berkeley, California, USA. As part of this year's program, we hosted Research Roundtables to catalyze collaborative, small-group dialogue around critical, timely topics at the intersection of machine learning and healthcare. Each roundtable was moderated by a team of senior and junior chairs who fostered open exchange, intellectual curiosity, and inclusive engagement. The sessions emphasized rigorous discussion of key challenges, exploration of emerging opportunities, and collective ideation toward actionable directions in the field. In total, eight roundtables were held by 19 roundtable chairs on topics of "Explainability, Interpretability, and Transparency," "Uncertainty, Bias, and Fairness," "Causality," "Domain Adaptation," "Foundation Models," "Learning from Small Medical Data," "Multimodal Methods," and "Scalable, Translational Healthcare Solutions."
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation</title>
<link>https://arxiv.org/abs/2510.19296</link>
<guid>https://arxiv.org/abs/2510.19296</guid>
<content:encoded><![CDATA[
arXiv:2510.19296v2 Announce Type: replace 
Abstract: The remarkable progress of Large Language Models (LLMs) presents promising opportunities for Verilog code generation which is significantly important for automated circuit design. The lacking of meaningful functional rewards hinders the preference optimization based on Reinforcement Learning (RL) for producing functionally correct Verilog code. In this paper, we propose Signal-Aware Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments of functionally correct output signal to optimize RL training. Considering Verilog code specifies the structural interconnection of hardware gates and wires so that different output signals are independent, the key insight of QiMeng-SALV is to extract verified signal-aware implementations in partially incorrect modules, so as to enhance the extraction of meaningful functional rewards. Roughly, we verify the functional correctness of signals in generated module by comparing with that of reference module in the training data. Then abstract syntax tree (AST) is employed to identify signal-aware code segments which can provide meaningful functional rewards from erroneous modules. Finally, we introduce signal-aware DPO which is optimized on the correct signal-level code segments, thereby preventing noise and interference from incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from conventional module-level to fine-grained signal-level optimization in Verilog code generation, addressing the issue of insufficient functional rewards. Experiments demonstrate that our method achieves state-of-the-art performance on VerilogEval and RTLLM, with a 7B parameter model matching the performance of the DeepSeek v3 671B model and significantly outperforming the leading open-source model CodeV trained on the same dataset. Our code is available at https://github.com/zy1xxx/SALV.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data</title>
<link>https://arxiv.org/abs/2510.25123</link>
<guid>https://arxiv.org/abs/2510.25123</guid>
<content:encoded><![CDATA[
arXiv:2510.25123v2 Announce Type: replace 
Abstract: We present a data-driven dimensionality reduction method that is well-suited for physics-based data representing hyperbolic wave propagation. The method utilizes a specialized neural network architecture called low rank neural representation (LRNR) inside a hypernetwork framework. The architecture is motivated by theoretical results that rigorously prove the existence of efficient representations for this wave class. We illustrate through archetypal examples that such an efficient low-dimensional representation of propagating waves can be learned directly from data through a combination of deep learning techniques. We observe that a low rank tensor representation arises naturally in the trained LRNRs, and that this reveals a new decomposition of wave propagation where each decomposed mode corresponds to interpretable physical features. Furthermore, we demonstrate that the LRNR architecture enables efficient inference via a compression scheme, which is a potentially important feature when deploying LRNRs in demanding performance regimes.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2510.25557</link>
<guid>https://arxiv.org/abs/2510.25557</guid>
<content:encoded><![CDATA[
arXiv:2510.25557v2 Announce Type: replace 
Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN) architecture in which the recurrent core is realized as a parametrized quantum circuit (PQC) controlled by a classical feedforward network. The hidden state is the quantum state of an $n$-qubit PQC in an exponentially large Hilbert space $\mathbb{C}^{2^n}$, which serves as a coherent recurrent quantum memory. The PQC is unitary by construction, making the hidden-state evolution norm-preserving without external constraints. At each timestep, mid-circuit Pauli expectation-value readouts are combined with the input embedding and processed by the feedforward network, which provides explicit classical nonlinearity. The outputs parametrize the PQC, which updates the hidden state via unitary dynamics. The QRNN is compact and physically consistent, and it unifies (i) unitary recurrence as a high-capacity memory, (ii) partial observation via mid-circuit readouts, and (iii) nonlinear classical control for input-conditioned parametrization. We evaluate the model in simulation with up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory, and language modeling. For sequence-to-sequence learning, we further devise a soft attention mechanism over the mid-circuit readouts and show its effectiveness for machine translation. To our knowledge, this is the first model (RNN or otherwise) grounded in quantum operations to achieve competitive performance against strong classical baselines across a broad class of sequence-learning tasks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expertise and confidence explain how social influence evolves along intellective tasks</title>
<link>https://arxiv.org/abs/2011.07168</link>
<guid>https://arxiv.org/abs/2011.07168</guid>
<content:encoded><![CDATA[
arXiv:2011.07168v2 Announce Type: replace-cross 
Abstract: Discovering the antecedents of individuals' influence in collaborative environments is an important, practical, and challenging problem. In this paper, we study interpersonal influence in small groups of individuals who collectively execute a sequence of intellective tasks. We observe that along an issue sequence with feedback, individuals with higher expertise and social confidence are accorded higher interpersonal influence. We also observe that low-performing individuals tend to underestimate their high-performing teammate's expertise. Based on these observations, we introduce three hypotheses and present empirical and theoretical support for their validity. We report empirical evidence on longstanding theories of transactive memory systems, social comparison, and confidence heuristics on the origins of social influence. We propose a cognitive dynamical model inspired by these theories to describe the process by which individuals adjust interpersonal influences over time. We demonstrate the model's accuracy in predicting individuals' influence and provide analytical results on its asymptotic behavior for the case with identically performing individuals. Lastly, we propose a novel approach using deep neural networks on a pre-trained text embedding model for predicting the influence of individuals. Using message contents, message times, and individual correctness collected during tasks, we are able to accurately predict individuals' self-reported influence over time. Extensive experiments verify the accuracy of the proposed models compared to baselines such as structural balance and reflected appraisal model. While the neural networks model is the most accurate, the dynamical model is the most interpretable for influence prediction.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates</title>
<link>https://arxiv.org/abs/2305.13409</link>
<guid>https://arxiv.org/abs/2305.13409</guid>
<content:encoded><![CDATA[
arXiv:2305.13409v5 Announce Type: replace-cross 
Abstract: We give a pair of algorithms that efficiently learn a quantum state prepared by Clifford gates and $O(\log n)$ non-Clifford gates. Specifically, for an $n$-qubit state $|\psi\rangle$ prepared with at most $t$ non-Clifford gates, our algorithms use $\mathsf{poly}(n,2^t,1/\varepsilon)$ time and copies of $|\psi\rangle$ to learn $|\psi\rangle$ to trace distance at most $\varepsilon$.
  The first algorithm for this task is more efficient, but requires entangled measurements across two copies of $|\psi\rangle$. The second algorithm uses only single-copy measurements at the cost of polynomial factors in runtime and sample complexity. Our algorithms more generally learn any state with sufficiently large stabilizer dimension, where a quantum state has stabilizer dimension $k$ if it is stabilized by an abelian group of $2^k$ Pauli operators. We also develop an efficient property testing algorithm for stabilizer dimension, which may be of independent interest.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Testing with Non-identically Distributed Samples</title>
<link>https://arxiv.org/abs/2311.11194</link>
<guid>https://arxiv.org/abs/2311.11194</guid>
<content:encoded><![CDATA[
arXiv:2311.11194v2 Announce Type: replace-cross 
Abstract: We examine the extent to which sublinear-sample property testing and estimation apply to settings where samples are independently but not identically distributed. Specifically, we consider the following distributional property testing framework: Suppose there is a set of distributions over a discrete support of size $k$, $p_1, p_2,\ldots,p_T$, and we obtain $c$ independent draws from each distribution. Suppose the goal is to learn or test a property of the average distribution, $p_{avg}$. This setup models a number of important practical settings where the individual distributions correspond to heterogeneous entities -- either individuals, chronologically distinct time periods, spatially separated data sources, etc. From a learning standpoint, even with $c=1$ samples from each distribution, $\Theta(k/\varepsilon^2)$ samples are necessary and sufficient to learn $p_{avg}$ to within error $\varepsilon$ in $\ell_1$ distance. To test uniformity or identity -- distinguishing the case that $p_{avg}$ is equal to some reference distribution, versus has $\ell_1$ distance at least $\varepsilon$ from the reference distribution, we show that a linear number of samples in $k$ is necessary given $c=1$ samples from each distribution. In contrast, for $c \ge 2$, we recover the usual sublinear sample testing guarantees of the i.i.d.\ setting: we show that $O(\sqrt{k}/\varepsilon^2 + 1/\varepsilon^4)$ total samples are sufficient, matching the optimal sample complexity in the i.i.d.\ case in the regime where $\varepsilon \ge k^{-1/4}$. Additionally, we show that in the $c=2$ case, there is a constant $\rho > 0$ such that even in the linear regime with $\rho k$ samples, no tester that considers the multiset of samples (ignoring which samples were drawn from the same $p_i$) can perform uniformity testing. We also extend our techniques to the problem of testing "closeness" of two distributions.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal Diffusion Modelling for Super-resolved Spatial Transcriptomics</title>
<link>https://arxiv.org/abs/2404.12973</link>
<guid>https://arxiv.org/abs/2404.12973</guid>
<content:encoded><![CDATA[
arXiv:2404.12973v3 Announce Type: replace-cross 
Abstract: The recent advancement of spatial transcriptomics (ST) allows to characterize spatial gene expression within tissue for discovery research. However, current ST platforms suffer from low resolution, hindering in-depth understanding of spatial gene expression. Super-resolution approaches promise to enhance ST maps by integrating histology images with gene expressions of profiled tissue spots. However, current super-resolution methods are limited by restoration uncertainty and mode collapse. Although diffusion models have shown promise in capturing complex interactions between multi-modal conditions, it remains a challenge to integrate histology images and gene expression for super-resolved ST maps. This paper proposes a cross-modal conditional diffusion model for super-resolving ST maps with the guidance of histology images. Specifically, we design a multi-modal disentangling network with cross-modal adaptive modulation to utilize complementary information from histology images and spatial gene expression. Moreover, we propose a dynamic cross-attention modelling strategy to extract hierarchical cell-to-tissue information from histology images. Lastly, we propose a co-expression-based gene-correlation graph network to model the co-expression relationship of multiple genes. Experiments show that our method outperforms other state-of-the-art methods in ST super-resolution on three public datasets.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking solutions of time-varying variational inequalities</title>
<link>https://arxiv.org/abs/2406.14059</link>
<guid>https://arxiv.org/abs/2406.14059</guid>
<content:encoded><![CDATA[
arXiv:2406.14059v2 Announce Type: replace-cross 
Abstract: Tracking the solution of time-varying variational inequalities is an important problem with applications in game theory, optimization, and machine learning. Existing work considers time-varying games or time-varying optimization problems. For strongly convex optimization problems or strongly monotone games, these results provide tracking guarantees under the assumption that the variation of the time-varying problem is restrained, that is, problems with a sublinear solution path. In this work we extend existing results in two ways: In our first result, we provide tracking bounds for (1) variational inequalities with a sublinear solution path but not necessarily monotone functions, and (2) for periodic time-varying variational inequalities that do not necessarily have a sublinear solution path-length. Our second main contribution is an extensive study of the convergence behavior and trajectory of discrete dynamical systems of periodic time-varying VI. We show that these systems can exhibit provably chaotic behavior or can converge to the solution. Finally, we illustrate our theoretical results with experiments.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaffolded Language Models with Language Supervision for Mixed-Autonomy: A Survey</title>
<link>https://arxiv.org/abs/2410.16392</link>
<guid>https://arxiv.org/abs/2410.16392</guid>
<content:encoded><![CDATA[
arXiv:2410.16392v3 Announce Type: replace-cross 
Abstract: This survey organizes the intricate literature on the design and optimization of emerging structures around post-trained LMs. We refer to this overarching structure as scaffolded LMs and focus on LMs that are integrated into multi-step processes with tools. We view scaffolded LMs as semi-parametric models wherein we train non-parametric variables, including the prompt, tools, and scaffold's code. In particular, they interpret instructions, use tools, and receive feedback all in language. Recent works use an LM as an optimizer to interpret language supervision and update non-parametric variables according to intricate objectives. In this survey, we refer to this paradigm as training of scaffolded LMs with language supervision. A key feature of non-parametric training is the ability to learn from language. Parametric training excels in learning from demonstration (supervised learning), exploration (reinforcement learning), or observations (unsupervised learning), using well-defined loss functions. Language-based optimization enables rich, interpretable, and expressive objectives, while mitigating issues like catastrophic forgetting and supporting compatibility with closed-source models. Furthermore, agents are increasingly deployed as co-workers in real-world applications such as Copilot in Office tools or software development. In these mixed-autonomy settings, where control and decision-making are shared between human and AI, users point out errors or suggest corrections. Accordingly, we discuss agents that continuously improve by learning from this real-time, language-based feedback and refer to this setting as streaming learning from language supervision.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Agnostic Modeling of Source Reliability on Wikipedia</title>
<link>https://arxiv.org/abs/2410.18803</link>
<guid>https://arxiv.org/abs/2410.18803</guid>
<content:encoded><![CDATA[
arXiv:2410.18803v3 Announce Type: replace-cross 
Abstract: Over the last few years, verifying the credibility of information sources has become a fundamental need to combat disinformation. Here, we present a language-agnostic model designed to assess the reliability of web domains as sources in references across multiple language editions of Wikipedia. Utilizing editing activity data, the model evaluates domain reliability within different articles of varying controversiality, such as Climate Change, COVID-19, History, Media, and Biology topics. Crafting features that express domain usage across articles, the model effectively predicts domain reliability, achieving an F1 Macro score of approximately 0.80 for English and other high-resource languages. For mid-resource languages, we achieve 0.65, while the performance of low-resource languages varies. In all cases, the time the domain remains present in the articles (which we dub as permanence) is one of the most predictive features. We highlight the challenge of maintaining consistent model performance across languages of varying resource levels and demonstrate that adapting models from higher-resource languages can improve performance. We believe these findings can assist Wikipedia editors in their ongoing efforts to verify citations and may offer useful insights for other user-generated content communities.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection Augmented Bandit Procedures for Piecewise Stationary MABs: A Modular Approach</title>
<link>https://arxiv.org/abs/2501.01291</link>
<guid>https://arxiv.org/abs/2501.01291</guid>
<content:encoded><![CDATA[
arXiv:2501.01291v3 Announce Type: replace-cross 
Abstract: Conventional Multi-Armed Bandit (MAB) algorithms are designed for stationary environments, where the reward distributions associated with the arms do not change with time. In many applications, however, the environment is more accurately modeled as being non-stationary. In this work, piecewise stationary MAB (PS-MAB) environments are investigated, in which the reward distributions associated with a subset of the arms change at some change-points and remain stationary between change-points. Our focus is on the asymptotic analysis of PS-MABs, for which practical algorithms based on change detection have been previously proposed. Our goal is to modularize the design and analysis of such Detection Augmented Bandit (DAB) procedures. To this end, we first provide novel, improved performance lower bounds for PS-MABs. Then, we identify the requirements for stationary bandit algorithms and change detectors in a DAB procedure that are needed for the modularization. We assume that the rewards are sub-Gaussian. Under this assumption and a condition on the separation of the change-points, we show that the analysis of DAB procedures can indeed be modularized, so that the regret bounds can be obtained in a unified manner for various combinations of change detectors and bandit algorithms. Through this analysis, we develop new modular DAB procedures that are order-optimal. Finally, we showcase the practical effectiveness of our modular DAB approach in our experiments, studying its regret performance compared to other methods and investigating its detection capabilities.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting Emergent Features in Deep Learning-based Side-channel Analysis</title>
<link>https://arxiv.org/abs/2502.00384</link>
<guid>https://arxiv.org/abs/2502.00384</guid>
<content:encoded><![CDATA[
arXiv:2502.00384v2 Announce Type: replace-cross 
Abstract: Side-channel analysis (SCA) poses a real-world threat by exploiting unintentional physical signals to extract secret information from secure devices. Evaluation labs also use the same techniques to certify device security. In recent years, deep learning has emerged as a prominent method for SCA, achieving state-of-the-art attack performance at the cost of interpretability. Understanding how neural networks extract secrets is crucial for security evaluators aiming to defend against such attacks, as only by understanding the attack can one propose better countermeasures.
  In this work, we apply mechanistic interpretability to neural networks trained for SCA, revealing \textit{how} models exploit \textit{what} leakage in side-channel traces. We focus on sudden jumps in performance to reverse engineer learned representations, ultimately recovering secret masks and moving the evaluation process from black-box to white-box. Our results show that mechanistic interpretability can scale to realistic SCA settings, even when relevant inputs are sparse, model accuracies are low, and side-channel protections prevent standard input interventions.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Astromer 2</title>
<link>https://arxiv.org/abs/2502.02717</link>
<guid>https://arxiv.org/abs/2502.02717</guid>
<content:encoded><![CDATA[
arXiv:2502.02717v2 Announce Type: replace-cross 
Abstract: Foundational models have emerged as a powerful paradigm in deep learning field, leveraging their capacity to learn robust representations from large-scale datasets and effectively to diverse downstream applications such as classification. In this paper, we present Astromer 2 a foundational model specifically designed for extracting light curve embeddings. We introduce Astromer 2 as an enhanced iteration of our self-supervised model for light curve analysis. This paper highlights the advantages of its pre-trained embeddings, compares its performance with that of its predecessor, Astromer 1, and provides a detailed empirical analysis of its capabilities, offering deeper insights into the model's representations. Astromer 2 is pretrained on 1.5 million single-band light curves from the MACHO survey using a self-supervised learning task that predicts randomly masked observations within sequences. Fine-tuning on a smaller labeled dataset allows us to assess its performance in classification tasks. The quality of the embeddings is measured by the F1 score of an MLP classifier trained on Astromer-generated embeddings. Our results demonstrate that Astromer 2 significantly outperforms Astromer 1 across all evaluated scenarios, including limited datasets of 20, 100, and 500 samples per class. The use of weighted per-sample embeddings, which integrate intermediate representations from Astromer's attention blocks, is particularly impactful. Notably, Astromer 2 achieves a 15% improvement in F1 score on the ATLAS dataset compared to prior models, showcasing robust generalization to new datasets. This enhanced performance, especially with minimal labeled data, underscores the potential of Astromer 2 for more efficient and scalable light curve analysis.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Optimization by Kernel Regression and Density-based Exploration</title>
<link>https://arxiv.org/abs/2502.06178</link>
<guid>https://arxiv.org/abs/2502.06178</guid>
<content:encoded><![CDATA[
arXiv:2502.06178v4 Announce Type: replace-cross 
Abstract: Bayesian optimization is highly effective for optimizing expensive-to-evaluate black-box functions, but it faces significant computational challenges due to the high computational complexity of Gaussian processes, which results in a total time complexity that is quartic with respect to the number of iterations. To address this limitation, we propose the Bayesian Optimization by Kernel regression and density-based Exploration (BOKE) algorithm. BOKE uses kernel regression for efficient function approximation, kernel density for exploration, and integrates them into the confidence bound criteria to guide the optimization process, thus reducing computational costs to quadratic. Our theoretical analysis rigorously establishes the global convergence of BOKE and ensures its robustness in noisy settings. Through extensive numerical experiments on both synthetic and real-world optimization tasks, we demonstrate that BOKE not only performs competitively compared to Gaussian process-based methods and several other baseline methods but also exhibits superior computational efficiency. These results highlight BOKE's effectiveness in resource-constrained environments, providing a practical approach for optimization problems in engineering applications.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Super-Resolution with Guarantees via Conformalized Generative Models</title>
<link>https://arxiv.org/abs/2502.09664</link>
<guid>https://arxiv.org/abs/2502.09664</guid>
<content:encoded><![CDATA[
arXiv:2502.09664v3 Announce Type: replace-cross 
Abstract: The increasing use of generative ML foundation models for image restoration tasks such as super-resolution calls for robust and interpretable uncertainty quantification methods. We address this need by presenting a novel approach based on conformal prediction techniques to create a 'confidence mask' capable of reliably and intuitively communicating where the generated image can be trusted. Our method is adaptable to any black-box generative model, including those locked behind an opaque API, requires only easily attainable data for calibration, and is highly customizable via the choice of a local image similarity metric. We prove strong theoretical guarantees for our method that span fidelity error control (according to our local image similarity metric), reconstruction quality, and robustness in the face of data leakage. Finally, we empirically evaluate these results and establish our method's solid performance.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient GA: Gradient Genetic Algorithm for Drug Molecular Design</title>
<link>https://arxiv.org/abs/2502.09860</link>
<guid>https://arxiv.org/abs/2502.09860</guid>
<content:encoded><![CDATA[
arXiv:2502.09860v2 Announce Type: replace-cross 
Abstract: Molecular discovery has brought great benefits to the chemical industry. Various molecule design techniques are developed to identify molecules with desirable properties. Traditional optimization methods, such as genetic algorithms, continue to achieve state-of-the-art results across multiple molecular design benchmarks. However, these techniques rely solely on random walk exploration, which hinders both the quality of the final solution and the convergence speed. To address this limitation, we propose a novel approach called Gradient Genetic Algorithm (Gradient GA), which incorporates gradient information from the objective function into genetic algorithms. Instead of random exploration, each proposed sample iteratively progresses toward an optimal solution by following the gradient direction. We achieve this by designing a differentiable objective function parameterized by a neural network and utilizing the Discrete Langevin Proposal to enable gradient guidance in discrete molecular spaces. Experimental results demonstrate that our method significantly improves both convergence speed and solution quality, outperforming cutting-edge techniques. For example, it achieves up to a 25% improvement in the top-10 score over the vanilla genetic algorithm. The code is publicly available at https://github.com/debadyuti23/GradientGA.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection and Geographic Localization of Natural Objects in the Wild: A Case Study on Palms</title>
<link>https://arxiv.org/abs/2502.13023</link>
<guid>https://arxiv.org/abs/2502.13023</guid>
<content:encoded><![CDATA[
arXiv:2502.13023v2 Announce Type: replace-cross 
Abstract: Palms are ecologically and economically indicators of tropical forest health, biodiversity, and human impact that support local economies and global forest product supply chains. While palm detection in plantations is well-studied, efforts to map naturally occurring palms in dense forests remain limited by overlapping crowns, uneven shading, and heterogeneous landscapes. We develop PRISM (Processing, Inference, Segmentation, and Mapping), a flexible pipeline for detecting and localizing palms in dense tropical forests using large orthomosaic images. Orthomosaics are created from thousands of aerial images and spanning several to hundreds of gigabytes. Our contributions are threefold. First, we construct a large UAV-derived orthomosaic dataset collected across 21 ecologically diverse sites in western Ecuador, annotated with 8,830 bounding boxes and 5,026 palm center points. Second, we evaluate multiple state-of-the-art object detectors based on efficiency and performance, integrating zero-shot SAM 2 as the segmentation backbone, and refining the results for precise geographic mapping. Third, we apply calibration methods to align confidence scores with IoU and explore saliency maps for feature explainability. Though optimized for palms, PRISM is adaptable for identifying other natural objects, such as eastern white pines. Future work will explore transfer learning for lower-resolution datasets (0.5 to 1m).
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards efficient quantum algorithms for diffusion probabilistic models</title>
<link>https://arxiv.org/abs/2502.14252</link>
<guid>https://arxiv.org/abs/2502.14252</guid>
<content:encoded><![CDATA[
arXiv:2502.14252v2 Announce Type: replace-cross 
Abstract: A diffusion probabilistic model (DPM) is a generative model renowned for its ability to produce high-quality outputs in tasks such as image and audio generation. However, training DPMs on large, high-dimensional datasets such as high-resolution images or audio incurs significant computational, energy, and hardware costs. In this work, we introduce efficient quantum algorithms for implementing DPMs through various quantum ODE solvers. These algorithms highlight the potential of quantum Carleman linearization for diverse mathematical structures, leveraging state-of-the-art quantum linear system solvers (QLSS) or linear combination of Hamiltonian simulations (LCHS). Specifically, we focus on two approaches: DPM-solver-$k$ which employs exact $k$-th order derivatives to compute a polynomial approximation of $\epsilon_\theta(x_\lambda,\lambda)$; and UniPC which uses finite difference of $\epsilon_\theta(x_\lambda,\lambda)$ at different points $(x_{s_m}, \lambda_{s_m})$ to approximate higher-order derivatives. As such, this work represents one of the most direct and pragmatic applications of quantum algorithms to large-scale machine learning models, presumably taking substantial steps towards demonstrating the practical utility of quantum computing.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Video Super-Resolution: Towards Diffusion-Based Methods without Motion Alignment</title>
<link>https://arxiv.org/abs/2503.03355</link>
<guid>https://arxiv.org/abs/2503.03355</guid>
<content:encoded><![CDATA[
arXiv:2503.03355v5 Announce Type: replace-cross 
Abstract: In this work, we rethink the approach to video super-resolution by introducing a method based on the Diffusion Posterior Sampling framework, combined with an unconditional video diffusion transformer operating in latent space. The video generation model, a diffusion transformer, functions as a space-time model. We argue that a powerful model, which learns the physics of the real world, can easily handle various kinds of motion patterns as prior knowledge, thus eliminating the need for explicit estimation of optical flows or motion parameters for pixel alignment. Furthermore, a single instance of the proposed video diffusion transformer model can adapt to different sampling conditions without re-training. Empirical results on synthetic and real-world datasets illustrate the feasibility of diffusion-based, alignment-free video super-resolution.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FORTALESA: Fault-Tolerant Reconfigurable Systolic Array for DNN Inference</title>
<link>https://arxiv.org/abs/2503.04426</link>
<guid>https://arxiv.org/abs/2503.04426</guid>
<content:encoded><![CDATA[
arXiv:2503.04426v2 Announce Type: replace-cross 
Abstract: The emergence of Deep Neural Networks (DNNs) in mission- and safety-critical applications brings their reliability to the front. High performance demands of DNNs require the use of specialized hardware accelerators. Systolic array architecture is widely used in DNN accelerators due to its parallelism and regular structure. This work presents a run-time reconfigurable systolic array architecture with three execution modes and four implementation options. All four implementations are evaluated in terms of resource utilization, throughput, and fault tolerance improvement. The proposed architecture is used for reliability enhancement of DNN inference on systolic array through heterogeneous mapping of different network layers to different execution modes. The approach is supported by a novel reliability assessment method based on fault propagation analysis. It is used for the exploration of the appropriate execution mode--layer mapping for DNN inference. The proposed architecture efficiently protects registers and MAC units of systolic array PEs from transient and permanent faults. The reconfigurability feature enables a speedup of up to $3\times$, depending on layer vulnerability. Furthermore, it requires $6\times$ fewer resources compared to static redundancy and $2.5\times$ fewer resources compared to the previously proposed solution for transient faults.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Foundational Atomistic Models Reliable for Finite-Temperature Molecular Dynamics?</title>
<link>https://arxiv.org/abs/2503.08207</link>
<guid>https://arxiv.org/abs/2503.08207</guid>
<content:encoded><![CDATA[
arXiv:2503.08207v2 Announce Type: replace-cross 
Abstract: Machine learning force fields have emerged as promising tools for molecular dynamics (MD) simulations, potentially offering quantum-mechanical accuracy with the efficiency of classical MD. Inspired by foundational large language models, recent years have seen considerable progress in developing foundational atomistic models, sometimes referred to as universal force fields, designed to cover most elements in the periodic table. This Perspective adopts a practitioner's viewpoint to ask a critical question: Are these foundational atomistic models reliable for one of their most compelling applications, in particular simulating finite-temperature dynamics? Instead of a broad benchmark, we use the canonical ferroelectric-paraelectric phase transition in PbTiO$_3$ as a focused case study to evaluate prominent foundational atomistic models. Our findings suggest a potential disconnect between static accuracy and dynamic reliability. While 0 K properties are often well-reproduced, we observed that the models can struggle to consistently capture the correct phase transition, sometimes exhibiting simulation instabilities. We believe these challenges may stem from inherent biases in training data and a limited description of anharmonicity. These observed shortcomings, though demonstrated on a single system, appear to point to broader, systemic challenges that can be addressed with targeted fine-tuning. This Perspective serves not to rank models, but to initiate a crucial discussion on the practical readiness of foundational atomistic models and to explore future directions for their improvement.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Bimanual Robotic Manipulation: Learning with Decoupled Interaction Framework</title>
<link>https://arxiv.org/abs/2503.09186</link>
<guid>https://arxiv.org/abs/2503.09186</guid>
<content:encoded><![CDATA[
arXiv:2503.09186v2 Announce Type: replace-cross 
Abstract: Bimanual robotic manipulation is an emerging and critical topic in the robotics community. Previous works primarily rely on integrated control models that take the perceptions and states of both arms as inputs to directly predict their actions. However, we think bimanual manipulation involves not only coordinated tasks but also various uncoordinated tasks that do not require explicit cooperation during execution, such as grasping objects with the closest hand, which integrated control frameworks ignore to consider due to their enforced cooperation in the early inputs. In this paper, we propose a novel decoupled interaction framework that considers the characteristics of different tasks in bimanual manipulation. The key insight of our framework is to assign an independent model to each arm to enhance the learning of uncoordinated tasks, while introducing a selective interaction module that adaptively learns weights from its own arm to improve the learning of coordinated tasks. Extensive experiments on seven tasks in the RoboTwin dataset demonstrate that: (1) Our framework achieves outstanding performance, with a 23.5% boost over the SOTA method. (2) Our framework is flexible and can be seamlessly integrated into existing methods. (3) Our framework can be effectively extended to multi-agent manipulation tasks, achieving a 28% boost over the integrated control SOTA. (4) The performance boost stems from the decoupled design itself, surpassing the SOTA by 16.5% in success rate with only 1/6 of the model size.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents</title>
<link>https://arxiv.org/abs/2503.10809</link>
<guid>https://arxiv.org/abs/2503.10809</guid>
<content:encoded><![CDATA[
arXiv:2503.10809v2 Announce Type: replace-cross 
Abstract: Recent advances in operating system (OS) agents have enabled vision-language models (VLMs) to directly control a user's computer. Unlike conventional VLMs that passively output text, OS agents autonomously perform computer-based tasks in response to a single user prompt. OS agents do so by capturing, parsing, and analysing screenshots and executing low-level actions via application programming interfaces (APIs), such as mouse clicks and keyboard inputs. This direct interaction with the OS significantly raises the stakes, as failures or manipulations can have immediate and tangible consequences. In this work, we uncover a novel attack vector against these OS agents: Malicious Image Patches (MIPs), adversarially perturbed screen regions that, when captured by an OS agent, induce it to perform harmful actions by exploiting specific APIs. For instance, a MIP can be embedded in a desktop wallpaper or shared on social media to cause an OS agent to exfiltrate sensitive user data. We show that MIPs generalise across user prompts and screen configurations, and that they can hijack multiple OS agents even during the execution of benign instructions. These findings expose critical security vulnerabilities in OS agents that have to be carefully addressed before their widespread deployment.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Coralscapes Dataset: Semantic Scene Understanding in Coral Reefs</title>
<link>https://arxiv.org/abs/2503.20000</link>
<guid>https://arxiv.org/abs/2503.20000</guid>
<content:encoded><![CDATA[
arXiv:2503.20000v2 Announce Type: replace-cross 
Abstract: Coral reefs are declining worldwide due to climate change and local stressors. To inform effective conservation or restoration, monitoring at the highest possible spatial and temporal resolution is necessary. Conventional coral reef surveying methods are limited in scalability due to their reliance on expert labor time, motivating the use of computer vision tools to automate the identification and abundance estimation of live corals from images. However, the design and evaluation of such tools has been impeded by the lack of large high quality datasets. We release the Coralscapes dataset, the first general-purpose dense semantic segmentation dataset for coral reefs, covering 2075 images, 39 benthic classes, and 174k segmentation masks annotated by experts. Coralscapes has a similar scope and the same structure as the widely used Cityscapes dataset for urban scene segmentation, allowing benchmarking of semantic segmentation models in a new challenging domain which requires expert knowledge to annotate. We benchmark a wide range of semantic segmentation models, and find that transfer learning from Coralscapes to existing smaller datasets consistently leads to state-of-the-art performance. Coralscapes will catalyze research on efficient, scalable, and standardized coral reef surveying methods based on computer vision, and holds the potential to streamline the development of underwater ecological robotics.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models are Unreliable for Cyber Threat Intelligence</title>
<link>https://arxiv.org/abs/2503.23175</link>
<guid>https://arxiv.org/abs/2503.23175</guid>
<content:encoded><![CDATA[
arXiv:2503.23175v3 Announce Type: replace-cross 
Abstract: Several recent works have argued that Large Language Models (LLMs) can be used to tame the data deluge in the cybersecurity field, by improving the automation of Cyber Threat Intelligence (CTI) tasks. This work presents an evaluation methodology that other than allowing to test LLMs on CTI tasks when using zero-shot learning, few-shot learning and fine-tuning, also allows to quantify their consistency and their confidence level. We run experiments with three state-of-the-art LLMs and a dataset of 350 threat intelligence reports and present new evidence of potential security risks in relying on LLMs for CTI. We show how LLMs cannot guarantee sufficient performance on real-size reports while also being inconsistent and overconfident. Few-shot learning and fine-tuning only partially improve the results, thus posing doubts about the possibility of using LLMs for CTI scenarios, where labelled datasets are lacking and where confidence is a fundamental factor.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Contrastive Learning: Synthetic Data Enables List-wise Training with Multiple Levels of Relevance</title>
<link>https://arxiv.org/abs/2503.23239</link>
<guid>https://arxiv.org/abs/2503.23239</guid>
<content:encoded><![CDATA[
arXiv:2503.23239v2 Announce Type: replace-cross 
Abstract: Although synthetic data has changed various aspects of information retrieval (IR) pipelines, the main training paradigm remains: contrastive learning with binary relevance labels, where one positive document is compared against several negatives using the InfoNCE loss. This objective treats all documents that are not explicitly annotated as relevant on an equally negative footing, regardless of their actual degree of relevance, thus missing subtle nuances useful for ranking. To overcome this limitation, in this work, we forgo real documents and annotations and use large language models to directly generate synthetic documents that answer the MS MARCO queries according to several different levels of relevance. We also propose using Wasserstein distance as a more effective loss function for training transformer-based retrievers with graduated relevance labels. Our experiments on MS MARCO and BEIR benchmark show that our proposed approach outperforms conventional training with InfoNCE by a large margin. Without using any real documents, our method significantly improves self-supervised retrievers and is more robust to distribution shift compared to contrastive learning using real data. Our method also successfully integrates existing real data into the synthetic ranking context, further boosting the performance. Overall, we show that generating multi-level ranking contexts is a better approach to synthetic data generation for IR than just generating the standard positive and negative documents.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Repetitions are not all alike: distinct mechanisms sustain repetition in language models</title>
<link>https://arxiv.org/abs/2504.01100</link>
<guid>https://arxiv.org/abs/2504.01100</guid>
<content:encoded><![CDATA[
arXiv:2504.01100v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) can sometimes degrade into repetitive loops, persistently generating identical word sequences. Because repetition is rare in natural human language, its frequent occurrence across diverse tasks and contexts in LLMs remains puzzling. Here we investigate whether behaviorally similar repetition patterns arise from distinct underlying mechanisms and how these mechanisms develop during model training. We contrast two conditions: repetitions elicited by natural text prompts with those induced by in-context learning (ICL) setups that explicitly require copying behavior. Our analyses reveal that ICL-induced repetition relies on a dedicated network of attention heads that progressively specialize over training, whereas naturally occurring repetition emerges early and lacks a defined circuitry. Attention inspection further shows that natural repetition focuses disproportionately on low-information tokens, suggesting a fallback behavior when relevant context cannot be retrieved. These results indicate that superficially similar repetition behaviors originate from qualitatively different internal processes, reflecting distinct modes of failure and adaptation in language models.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary Machine Learning meets Self-Supervised Learning: a comprehensive survey</title>
<link>https://arxiv.org/abs/2504.07213</link>
<guid>https://arxiv.org/abs/2504.07213</guid>
<content:encoded><![CDATA[
arXiv:2504.07213v2 Announce Type: replace-cross 
Abstract: The number of studies that combine Evolutionary Machine Learning and self-supervised learning has been growing steadily in recent years. Evolutionary Machine Learning has been shown to help automate the design of machine learning algorithms and to lead to more reliable solutions. Self-supervised learning, on the other hand, has produced good results in learning useful features when labelled data is limited. This suggests that the combination of these two areas can help both in shaping evolutionary processes and in automating the design of deep neural networks, while also reducing the need for labelled data. Still, there are no detailed reviews that explain how Evolutionary Machine Learning and self-supervised learning can be used together. To help with this, we provide an overview of studies that bring these areas together. Based on this growing interest and the range of existing works, we suggest a new sub-area of research, which we call Evolutionary Self-Supervised Learning and introduce a taxonomy for it. Finally, we point out some of the main challenges and suggest directions for future research to help Evolutionary Self-Supervised Learning grow and mature as a field.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via Optimal Transport for Cardiovascular Disease Detection</title>
<link>https://arxiv.org/abs/2505.18174</link>
<guid>https://arxiv.org/abs/2505.18174</guid>
<content:encoded><![CDATA[
arXiv:2505.18174v3 Announce Type: replace-cross 
Abstract: The coupling signal refers to a latent physiological signal that characterizes the transformation from cardiac electrical excitation, captured by the electrocardiogram (ECG), to mechanical contraction, recorded by the phonocardiogram (PCG). By encoding the temporal and functional interplay between electrophysiological and hemodynamic events, it serves as an intrinsic link between modalities and offers a unified representation of cardiac function, with strong potential to enhance multi-modal cardiovascular disease (CVD) detection. However, existing coupling signal estimation methods remain highly vulnerable to noise, particularly in real-world clinical and physiological settings, which undermines their robustness and limits practical value. In this study, we propose Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates coupling signal estimation as a distribution matching problem solved via optimal transport. By jointly aligning amplitude and timing, NMCSE avoids noise amplification and enables stable signal estimation. When integrated into a Temporal-Spatial Feature Extraction (TSFE) network, the estimated coupling signal effectively enhances multi-modal fusion for more accurate CVD detection. To evaluate robustness under real-world conditions, we design two complementary experiments targeting distinct sources of noise. The first uses the PhysioNet 2016 dataset with simulated hospital noise to assess the resilience of NMCSE to clinical interference. The second leverages the EPHNOGRAM dataset with motion-induced physiological noise to evaluate intra-state estimation stability across activity levels. Experimental results show that NMCSE consistently outperforms existing methods under both clinical and physiological noise, highlighting it as a noise-robust estimation approach that enables reliable multi-modal cardiac detection in real-world conditions.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random</title>
<link>https://arxiv.org/abs/2505.19093</link>
<guid>https://arxiv.org/abs/2505.19093</guid>
<content:encoded><![CDATA[
arXiv:2505.19093v3 Announce Type: replace-cross 
Abstract: Model-based clustering integrated with variable selection is a powerful tool for uncovering latent structures within complex data. However, its effectiveness is often hindered by challenges such as identifying relevant variables that define heterogeneous subgroups and handling data that are missing not at random, a prevalent issue in fields like transcriptomics. While several notable methods have been proposed to address these problems, they typically tackle each issue in isolation, thereby limiting their flexibility and adaptability. This paper introduces a unified framework designed to address these challenges simultaneously. Our approach incorporates a data-driven penalty matrix into penalized clustering to enable more flexible variable selection, along with a mechanism that explicitly models the relationship between missingness and latent class membership. We demonstrate that, under certain regularity conditions, the proposed framework achieves both asymptotic consistency and selection consistency, even in the presence of missing data. This unified strategy significantly enhances the capability and efficiency of model-based clustering, advancing methodologies for identifying informative variables that define homogeneous subgroups in the presence of complex missing data patterns. The performance of the framework, including its computational efficiency, is evaluated through simulations and demonstrated using both synthetic and real-world transcriptomic datasets.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoding Random Forests</title>
<link>https://arxiv.org/abs/2505.21441</link>
<guid>https://arxiv.org/abs/2505.21441</guid>
<content:encoded><![CDATA[
arXiv:2505.21441v3 Announce Type: replace-cross 
Abstract: We propose a principled method for autoencoding with random forests. Our strategy builds on foundational results from nonparametric statistics and spectral graph theory to learn a low-dimensional embedding of the model that optimally represents relationships in the data. We provide exact and approximate solutions to the decoding problem via constrained optimization, split relabeling, and nearest neighbors regression. These methods effectively invert the compression pipeline, establishing a map from the embedding space back to the input space using splits learned by the ensemble's constituent trees. The resulting decoders are universally consistent under common regularity assumptions. The procedure works with supervised or unsupervised models, providing a window into conditional or joint distributions. We demonstrate various applications of this autoencoder, including powerful new tools for visualization, compression, clustering, and denoising. Experiments illustrate the ease and utility of our method in a wide range of settings, including tabular, image, and genomic data.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking</title>
<link>https://arxiv.org/abs/2505.23495</link>
<guid>https://arxiv.org/abs/2505.23495</guid>
<content:encoded><![CDATA[
arXiv:2505.23495v4 Announce Type: replace-cross 
Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality benchmarks to evaluate complex multi-hop reasoning. However, despite their widespread use, popular datasets such as WebQSP and CWQ suffer from critical quality issues, including inaccurate or incomplete ground-truth annotations, poorly constructed questions that are ambiguous, trivial, or unanswerable, and outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA datasets, including WebQSP and CWQ, we find that the average factual correctness rate is only 57 %. To address these issues, we introduce KGQAGen, an LLM-in-the-loop framework that systematically resolves these pitfalls. KGQAGen combines structured knowledge grounding, LLM-guided generation, and symbolic verification to produce challenging and verifiable QA instances. Using KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results demonstrate that even state-of-the-art systems struggle on this benchmark, highlighting its ability to expose limitations of existing models. Our findings advocate for more rigorous benchmark construction and position KGQAGen as a scalable framework for advancing KGQA evaluation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data</title>
<link>https://arxiv.org/abs/2505.23522</link>
<guid>https://arxiv.org/abs/2505.23522</guid>
<content:encoded><![CDATA[
arXiv:2505.23522v2 Announce Type: replace-cross 
Abstract: Existing benchmarks for multimodal learning in Earth science offer limited, siloed coverage of Earth's spheres and their cross-sphere interactions, typically restricting evaluation to the human-activity sphere of atmosphere and to at most 16 tasks. These limitations: \textit{narrow-source heterogeneity (single/few data sources), constrained scientific granularity, and limited-sphere extensibility}. Therefore, we introduce \textbf{OmniEarth-Bench}, the first multimodal benchmark that systematically spans all six spheres: atmosphere, lithosphere, oceanosphere, cryosphere, biosphere, and human-activity sphere, and cross-spheres. Built with a scalable, modular-topology data inference framework and native multi-observation sources and expert-in-the-loop curation, OmniEarth-Bench produces 29,855 standardized, expert-curated annotations. All annotations are organized into a four-level hierarchy (Sphere, Scenario, Ability, Task), encompassing 109 expert-curated evaluation tasks. Experiments on 9 state-of-the-art MLLMs reveal that even the most advanced models struggle with our benchmarks, where none of them reach 35\% accuracy, revealing systematic gaps in Earth-system cognitive ability. The dataset and evaluation code were released at OmniEarth-Bench (https://anonymous.4open.science/r/OmniEarth-Bench-B1BD).
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport</title>
<link>https://arxiv.org/abs/2505.24759</link>
<guid>https://arxiv.org/abs/2505.24759</guid>
<content:encoded><![CDATA[
arXiv:2505.24759v3 Announce Type: replace-cross 
Abstract: Identifying evolutionary correspondences between cell types across species is a fundamental challenge in comparative genomics and evolutionary biology. Existing approaches often rely on either reference-based matching, which imposes asymmetry by designating one species as the reference, or projection-based matching, which may increase computational complexity and obscure biological interpretability at the cell-type level. Here, we present OT-MESH, an unsupervised computational framework leveraging entropy-regularized optimal transport (OT) to systematically determine cross-species cell type homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn (MESH) technique to refine the OT plan, transforming diffuse transport matrices into sparse, interpretable correspondences. Through systematic evaluation on synthetic datasets, we demonstrate that OT-MESH achieves near-optimal matching accuracy with computational efficiency, while maintaining remarkable robustness to noise. Compared to other OT-based methods like RefCM, OT-MESH provides speedup while achieving comparable accuracy. Applied to retinal bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque, OT-MESH accurately recovers known evolutionary relationships and uncovers novel correspondences, one of which was independently validated experimentally. Thus, our framework offers a principled, scalable, and interpretable solution for evolutionary cell type mapping, facilitating deeper insights into cellular specialization and conservation across species.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discrete and Continuous Difference of Submodular Minimization</title>
<link>https://arxiv.org/abs/2506.07952</link>
<guid>https://arxiv.org/abs/2506.07952</guid>
<content:encoded><![CDATA[
arXiv:2506.07952v3 Announce Type: replace-cross 
Abstract: Submodular functions, defined on continuous or discrete domains, arise in numerous applications. We study the minimization of the difference of two submodular (DS) functions, over both domains, extending prior work restricted to set functions. We show that all functions on discrete domains and all smooth functions on continuous domains are DS. For discrete domains, we observe that DS minimization is equivalent to minimizing the difference of two convex (DC) functions, as in the set function case. We propose a novel variant of the DC Algorithm (DCA) and apply it to the resulting DC Program, obtaining comparable theoretical guarantees as in the set function case. The algorithm can be applied to continuous domains via discretization. Experiments demonstrate that our method outperforms baselines in integer compressive sensing and integer least squares.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging</title>
<link>https://arxiv.org/abs/2506.09024</link>
<guid>https://arxiv.org/abs/2506.09024</guid>
<content:encoded><![CDATA[
arXiv:2506.09024v2 Announce Type: replace-cross 
Abstract: Safe deployment of machine learning (ML) models in safety-critical domains such as medical imaging requires detecting inputs with characteristics not seen during training, known as out-of-distribution (OOD) detection, to prevent unreliable predictions. Effective OOD detection after deployment could benefit from access to the training data, enabling direct comparison between test samples and the training data distribution to identify differences. State-of-the-art OOD detection methods, however, either discard the training data after deployment or assume that test samples and training data are centrally stored together, an assumption that rarely holds in real-world settings. This is because shipping the training data with the deployed model is usually impossible due to the size of training databases, as well as proprietary or privacy constraints. We introduce the Isolation Network, an OOD detection framework that quantifies the difficulty of separating a target test sample from the training data by solving a binary classification task. We then propose Decentralized Isolation Networks (DIsoN), which enables the comparison of training and test data when data-sharing is impossible, by exchanging only model parameters between the remote computational nodes of training and deployment. We further extend DIsoN with class-conditioning, comparing a target sample solely with training data of its predicted class. We evaluate DIsoN on four medical imaging datasets (dermatology, chest X-ray, breast ultrasound, histopathology) across 12 OOD detection tasks. DIsoN performs favorably against existing methods while respecting data-privacy. This decentralized OOD detection framework opens the way for a new type of service that ML developers could provide along with their models: providing remote, secure utilization of their training data for OOD detection services. Code: https://github.com/FelixWag/DIsoN
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Is Diversity Rewarded in Cooperative Multi-Agent Learning?</title>
<link>https://arxiv.org/abs/2506.09434</link>
<guid>https://arxiv.org/abs/2506.09434</guid>
<content:encoded><![CDATA[
arXiv:2506.09434v3 Announce Type: replace-cross 
Abstract: The success of teams in robotics, nature, and society often depends on the division of labor among diverse specialists; however, a principled explanation for when such diversity surpasses a homogeneous team is still missing. Focusing on multi-agent task allocation problems, we study this question from the perspective of reward design: what kinds of objectives are best suited for heterogeneous teams? We first consider an instantaneous, non-spatial setting where the global reward is built by two generalized aggregation operators: an inner operator that maps the $N$ agents' effort allocations on individual tasks to a task score, and an outer operator that merges the $M$ task scores into the global team reward. We prove that the curvature of these operators determines whether heterogeneity can increase reward, and that for broad reward families this collapses to a simple convexity test. Next, we ask what incentivizes heterogeneity to emerge when embodied, time-extended agents must learn an effort allocation policy. To study heterogeneity in such settings, we use multi-agent reinforcement learning (MARL) as our computational paradigm, and introduce Heterogeneity Gain Parameter Search (HetGPS), a gradient-based algorithm that optimizes the parameter space of underspecified MARL environments to find scenarios where heterogeneity is advantageous. Across different environments, we show that HetGPS rediscovers the reward regimes predicted by our theory to maximize the advantage of heterogeneity, both validating HetGPS and connecting our theoretical insights to reward design in MARL. Together, these results help us understand when behavioral diversity delivers a measurable benefit.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable and Cost-Efficient de Novo Template-Based Molecular Generation</title>
<link>https://arxiv.org/abs/2506.19865</link>
<guid>https://arxiv.org/abs/2506.19865</guid>
<content:encoded><![CDATA[
arXiv:2506.19865v2 Announce Type: replace-cross 
Abstract: Template-based molecular generation offers a promising avenue for drug design by ensuring generated compounds are synthetically accessible through predefined reaction templates and building blocks. In this work, we tackle three core challenges in template-based GFlowNets: (1) minimizing synthesis cost, (2) scaling to large building block libraries, and (3) effectively utilizing small fragment sets. We propose Recursive Cost Guidance, a backward policy framework that employs auxiliary machine learning models to approximate synthesis cost and viability. This guidance steers generation toward low-cost synthesis pathways, significantly enhancing cost-efficiency, molecular diversity, and quality, especially when paired with an Exploitation Penalty that balances the trade-off between exploration and exploitation. To enhance performance in smaller building block libraries, we develop a Dynamic Library mechanism that reuses intermediate high-reward states to construct full synthesis trees. Our approach establishes state-of-the-art results in template-based molecular generation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MediQ-GAN: Quantum-Inspired GAN for High Resolution Medical Image Generation</title>
<link>https://arxiv.org/abs/2506.21015</link>
<guid>https://arxiv.org/abs/2506.21015</guid>
<content:encoded><![CDATA[
arXiv:2506.21015v2 Announce Type: replace-cross 
Abstract: Machine learning-assisted diagnosis shows promise, yet medical imaging datasets are often scarce, imbalanced, and constrained by privacy, making data augmentation essential. Classical generative models typically demand extensive computational and sample resources. Quantum computing offers a promising alternative, but existing quantum-based image generation methods remain limited in scale and often face barren plateaus. We present MediQ-GAN, a quantum-inspired GAN with prototype-guided skip connections and a dual-stream generator that fuses classical and quantum-inspired branches. Its variational quantum circuits inherently preserve full-rank mappings, avoid rank collapse, and are theory-guided to balance expressivity with trainability. Beyond generation quality, we provide the first latent-geometry and rank-based analysis of quantum-inspired GANs, offering theoretical insight into their performance. Across three medical imaging datasets, MediQ-GAN outperforms state-of-the-art GANs and diffusion models. While validated on IBM hardware for robustness, our contribution is hardware-agnostic, offering a scalable and data-efficient framework for medical image generation and augmentation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Object Segmentation by Background Conditional Divergence</title>
<link>https://arxiv.org/abs/2506.22505</link>
<guid>https://arxiv.org/abs/2506.22505</guid>
<content:encoded><![CDATA[
arXiv:2506.22505v2 Announce Type: replace-cross 
Abstract: As a computer vision task, automatic object segmentation remains challenging in specialized image domains without massive labeled data, such as synthetic aperture sonar images, remote sensing, biomedical imaging, etc. In any domain, obtaining pixel-wise segmentation masks is expensive. In this work, we propose a method for training a masking network to perform binary object segmentation using weak supervision in the form of image-wise presence or absence of an object of interest, which provides less information but may be obtained more quickly from manual or automatic labeling. A key step in our method is that the segmented objects can be placed into background-only images to create realistic images of the objects with counterfactual backgrounds. To create a contrast between the original and counterfactual background images, we propose to first cluster the background-only images and then, during learning, create counterfactual images that blend objects segmented from their original source backgrounds to backgrounds chosen from a targeted cluster. One term in the training loss is the divergence between these counterfactual images and the real object images with backgrounds of the target cluster. The other term is a supervised loss for background-only images. While an adversarial critic could provide the divergence, we use sample-based divergences. We conduct experiments on side-scan and synthetic aperture sonar in which our approach succeeds compared to previous unsupervised segmentation baselines that were only tested on natural images. Furthermore, to show generality we extend our experiments to natural images, obtaining reasonable performance with our method that avoids pretrained networks, generative networks, and adversarial critics. The code for this work can be found at \href{GitHub}{https://github.com/bakerhassan/WSOS}.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench</title>
<link>https://arxiv.org/abs/2507.02554</link>
<guid>https://arxiv.org/abs/2507.02554</guid>
<content:encoded><![CDATA[
arXiv:2507.02554v2 Announce Type: replace-cross 
Abstract: AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models</title>
<link>https://arxiv.org/abs/2508.02912</link>
<guid>https://arxiv.org/abs/2508.02912</guid>
<content:encoded><![CDATA[
arXiv:2508.02912v3 Announce Type: replace-cross 
Abstract: Robust coordination is critical for effective decision-making in multi-agent systems, especially under partial observability. A central question in Multi-Agent Reinforcement Learning (MARL) is whether to engineer communication protocols or learn them end-to-end. We investigate this dichotomy using embodied world models. We propose and compare two communication strategies for a cooperative task-allocation problem. The first, Learned Direct Communication (LDC), learns a protocol end-to-end. The second, Intention Communication, uses an engineered inductive bias: a compact, learned world model, the Imagined Trajectory Generation Module (ITGM), which uses the agent's own policy to simulate future states. A Message Generation Network (MGN) then compresses this plan into a message. We evaluate these approaches on goal-directed interaction in a grid world, a canonical abstraction for embodied AI problems, while scaling environmental complexity. Our experiments reveal that while emergent communication is viable in simple settings, the engineered, world model-based approach shows superior performance, sample efficiency, and scalability as complexity increases. These findings advocate for integrating structured, predictive models into MARL agents to enable active, goal-driven coordination.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MicroLad: 2D-to-3D Microstructure Reconstruction and Generation via Latent Diffusion and Score Distillation</title>
<link>https://arxiv.org/abs/2508.20138</link>
<guid>https://arxiv.org/abs/2508.20138</guid>
<content:encoded><![CDATA[
arXiv:2508.20138v2 Announce Type: replace-cross 
Abstract: A major obstacle to establishing reliable structure-property (SP) linkages in materials engineering is the scarcity of diverse 3D microstructure datasets. Limited dataset availability and insufficient control over the analysis and design space restrict the variety of achievable microstructure morphologies, hindering progress in solving the inverse (property-to-structure) design problem. To address these challenges, we introduce MicroLad, a latent diffusion framework specifically designed for reconstructing 3D microstructures from 2D data. Trained on 2D images and employing multi-plane denoising diffusion sampling in the latent space, the framework reliably generates stable and coherent 3D volumes that remain statistically consistent with the original data. While this reconstruction capability enables dimensionality expansion (2D-to-3D) for generating statistically equivalent 3D samples from 2D data, effective exploration of microstructure design requires methods to guide the generation process toward specific objectives. To achieve this, MicroLad integrates score distillation sampling (SDS), which combines a differentiable score loss with microstructural descriptor-matching and property-alignment terms. This approach updates encoded 2D slices of the 3D volume in the latent space, enabling robust inverse-controlled 2D-to-3D microstructure generation. Consequently, the method facilitates exploration of an expanded 3D microstructure analysis and design space in terms of both microstructural descriptors and material properties.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education</title>
<link>https://arxiv.org/abs/2508.21666</link>
<guid>https://arxiv.org/abs/2508.21666</guid>
<content:encoded><![CDATA[
arXiv:2508.21666v2 Announce Type: replace-cross 
Abstract: This paper introduces the Future Atmospheric Conditions Training System (FACTS), a novel platform that advances climate resilience education through place-based, adaptive learning experiences. FACTS combines real-time atmospheric data collected by IoT sensors with curated resources from a Knowledge Base to dynamically generate localized learning challenges. Learner responses are analyzed by a Generative AI powered server, which delivers personalized feedback and adaptive support. Results from a user evaluation indicate that participants found the system both easy to use and effective for building knowledge related to climate resilience. These findings suggest that integrating IoT and Generative AI into atmospherically adaptive learning technologies holds significant promise for enhancing educational engagement and fostering climate awareness.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Physical Basis of Prediction: World Model Formation in Neural Organoids via an LLM-Generated Curriculum</title>
<link>https://arxiv.org/abs/2509.04633</link>
<guid>https://arxiv.org/abs/2509.04633</guid>
<content:encoded><![CDATA[
arXiv:2509.04633v3 Announce Type: replace-cross 
Abstract: The capacity of an embodied agent to understand, predict, and interact with its environment is fundamentally contingent on an internal world model. This paper introduces a novel framework for investigating the formation and adaptation of such world models within a biological substrate: human neural organoids. We present a curriculum of three scalable, closed-loop virtual environments designed to train these biological agents and probe the underlying synaptic mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments that demand progressively more sophisticated world models for successful decision-making: (1) a conditional avoidance task for learning static state-action contingencies, (2) a one-dimensional predator-prey scenario for goal-directed interaction, and (3) a replication of the classic Pong game for modeling dynamic, continuous-time systems. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation, which serve to drive model refinement. In a significant methodological advance, we propose a meta-learning approach where a Large Language Model automates the generative design and optimization of experimental protocols, thereby scaling the process of environment and curriculum design. Finally, we outline a multi-modal evaluation strategy that moves beyond task performance to directly measure the physical correlates of the learned world model by quantifying synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between model-based reinforcement learning and computational neuroscience, offering a unique platform for studying embodiment, decision-making, and the physical basis of intelligence.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative World Models of Tasks: LLM-Driven Hierarchical Scaffolding for Embodied Agents</title>
<link>https://arxiv.org/abs/2509.04731</link>
<guid>https://arxiv.org/abs/2509.04731</guid>
<content:encoded><![CDATA[
arXiv:2509.04731v3 Announce Type: replace-cross 
Abstract: Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring successes in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. We propose that an effective world model for decision-making must model the world's physics and also its task semantics. A systematic review of 2024 research in low-resource multi-agent soccer reveals a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We formalize this trend into a framework for Hierarchical Task Environments (HTEs), which are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. Our framework incorporates the use of Large Language Models (LLMs) as generative world models of tasks, capable of dynamically generating this scaffolding. We argue that HTEs provide a mechanism to guide exploration, generate meaningful learning signals, and train agents to internalize hierarchical structure, enabling the development of more capable and general-purpose agents with greater sample efficiency than purely end-to-end approaches.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Realizable Circuit Complexity: Embedding Computation in Space-Time</title>
<link>https://arxiv.org/abs/2509.19161</link>
<guid>https://arxiv.org/abs/2509.19161</guid>
<content:encoded><![CDATA[
arXiv:2509.19161v2 Announce Type: replace-cross 
Abstract: Classical circuit complexity characterizes parallel computation in purely combinatorial terms, ignoring the physical constraints that govern real hardware. The standard classes $\mathbf{NC}$, $\mathbf{AC}$, and $\mathbf{TC}$ treat unlimited fan-in, free interconnection, and polynomial gate counts as feasible -- assumptions that conflict with geometric, energetic, and thermodynamic realities. We introduce the family of \textit{realizable circuit classes} $\mathbf{RC}_d$, which model computation embedded in physical $d$-dimensional space. Each circuit in $\mathbf{RC}_d$ obeys conservative realizability laws: volume scales as $\mathcal{O}(t^d)$, cross-boundary information flux is bounded by $\mathcal{O}(t^{d-1})$ per unit time, and growth occurs through local, physically constructible edits. These bounds apply to all causal systems, classical or quantum. Within this framework, we show that algorithms with runtime $\omega(n^{d/(d-1)})$ cannot scale to inputs of maximal entropy, and that any $d$-dimensional parallel implementation offers at most a polynomial speed-up of degree $(d-1)$ over its optimal sequential counterpart. In the limit $d\to\infty$, $\mathbf{RC}_\infty(\mathrm{polylog})=\mathbf{NC}$, recovering classical parallelism as a non-physical idealization. By unifying geometry, causality, and information flow, $\mathbf{RC}_d$ extends circuit complexity into the physical domain, revealing universal scaling laws for computation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variance-Bounded Evaluation of Entity-Centric AI Systems Without Ground Truth: Theory and Measurement</title>
<link>https://arxiv.org/abs/2509.22751</link>
<guid>https://arxiv.org/abs/2509.22751</guid>
<content:encoded><![CDATA[
arXiv:2509.22751v2 Announce Type: replace-cross 
Abstract: Reliable evaluation of AI systems remains a fundamental challenge when ground truth labels are unavailable, particularly for systems generating natural language outputs like AI chat and agent systems. Many of these AI agents and systems focus on entity-centric tasks. In enterprise contexts, organizations deploy AI systems for entity linking, data integration, and information retrieval where verification against gold standards is often infeasible due to proprietary data constraints. Academic deployments face similar challenges when evaluating AI systems on specialized datasets with ambiguous criteria. Conventional evaluation frameworks, rooted in supervised learning paradigms, fail in such scenarios where single correct answers cannot be defined. We introduce VB-Score, a variance-bounded evaluation framework for entity-centric AI systems that operates without ground truth by jointly measuring effectiveness and robustness. Given system inputs, VB-Score enumerates plausible interpretations through constraint relaxation and Monte Carlo sampling, assigning probabilities that reflect their likelihood. It then evaluates system outputs by their expected success across interpretations, penalized by variance to assess robustness of the system. We provide formal theoretical analysis establishing key properties including range, monotonicity, and stability along with concentration bounds for Monte Carlo estimation. Through case studies on AI systems with ambiguous inputs, we demonstrate that VB-Score reveals robustness differences hidden by conventional evaluation frameworks, offering a principled measurement framework for assessing AI system reliability in label-scarce domains.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.03534</link>
<guid>https://arxiv.org/abs/2510.03534</guid>
<content:encoded><![CDATA[
arXiv:2510.03534v3 Announce Type: replace-cross 
Abstract: We study the problem of long-term (multiple days) mapping of a river plume using multiple autonomous underwater vehicles (AUVs), focusing on the Douro river representative use-case. We propose an energy - and communication - efficient multi-agent reinforcement learning approach in which a central coordinator intermittently communicates with the AUVs, collecting measurements and issuing commands. Our approach integrates spatiotemporal Gaussian process regression (GPR) with a multi-head Q-network controller that regulates direction and speed for each AUV. Simulations using the Delft3D ocean model demonstrate that our method consistently outperforms both single- and multi-agent benchmarks, with scaling the number of agents both improving mean squared error (MSE) and operational endurance. In some instances, our algorithm demonstrates that doubling the number of AUVs can more than double endurance while maintaining or improving accuracy, underscoring the benefits of multi-agent coordination. Our learned policies generalize across unseen seasonal regimes over different months and years, demonstrating promise for future developments of data-driven long-term monitoring of dynamic plume environments.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives</title>
<link>https://arxiv.org/abs/2510.04983</link>
<guid>https://arxiv.org/abs/2510.04983</guid>
<content:encoded><![CDATA[
arXiv:2510.04983v3 Announce Type: replace-cross 
Abstract: Identifying cultural capital (CC) themes in student reflections can offer valuable insights that help foster equitable learning environments in classrooms. However, themes such as aspirational goals or family support are often woven into narratives, rather than appearing as direct keywords. This makes them difficult to detect for standard NLP models that process sentences in isolation. The core challenge stems from a lack of awareness, as standard models are pre-trained on general corpora, leaving them blind to the domain-specific language and narrative context inherent to the data. To address this, we introduce AWARE, a framework that systematically attempts to improve a transformer model's awareness for this nuanced task. AWARE has three core components: 1) Domain Awareness, adapting the model's vocabulary to the linguistic style of student reflections; 2) Context Awareness, generating sentence embeddings that are aware of the full essay context; and 3) Class Overlap Awareness, employing a multi-label strategy to recognize the coexistence of themes in a single sentence. Our results show that by making the model explicitly aware of the properties of the input, AWARE outperforms a strong baseline by 2.1 percentage points in Macro-F1 and shows considerable improvements across all themes. This work provides a robust and generalizable methodology for any text classification task in which meaning depends on the context of the narrative.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AGNES: Adaptive Graph Neural Network and Dynamic Programming Hybrid Framework for Real-Time Nanopore Seed Chaining</title>
<link>https://arxiv.org/abs/2510.16013</link>
<guid>https://arxiv.org/abs/2510.16013</guid>
<content:encoded><![CDATA[
arXiv:2510.16013v3 Announce Type: replace-cross 
Abstract: Nanopore sequencing enables real-time long-read DNA sequencing with reads exceeding 10 kilobases, but inherent error rates of 12-15 percent present significant computational challenges for read alignment. The critical seed chaining step must connect exact k-mer matches between reads and reference genomes while filtering spurious matches, yet state-of-the-art methods rely on fixed gap penalty functions unable to adapt to varying genomic contexts including tandem repeats and structural variants. This paper presents RawHash3, a hybrid framework combining graph neural networks with classical dynamic programming for adaptive seed chaining that maintains real-time performance while providing statistical guarantees. We formalize seed chaining as graph learning where seeds constitute nodes with 12-dimensional feature vectors and edges encode 8-dimensional spatial relationships including gap consistency. Our architecture employs three-layer EdgeConv GNN with confidence-based method selection that dynamically switches between learned guidance and algorithmic fallback. Comprehensive evaluation on 1,000 synthetic nanopore reads with 5,200 test seeds demonstrates RawHash3 achieves 99.94 percent precision and 40.07 percent recall, representing statistically significant 25.0 percent relative improvement over baseline with p less than 0.001. The system maintains median inference latency of 1.59ms meeting real-time constraints, while demonstrating superior robustness with 100 percent success rate under 20 percent label corruption versus baseline degradation to 30.3 percent. Cross-validation confirms stability establishing graph neural networks as viable approach for production genomics pipelines.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia</title>
<link>https://arxiv.org/abs/2510.16066</link>
<guid>https://arxiv.org/abs/2510.16066</guid>
<content:encoded><![CDATA[
arXiv:2510.16066v2 Announce Type: replace-cross 
Abstract: Despite accounting for 96.1% of all businesses in Malaysia, access to financing remains one of the most persistent challenges faced by Micro, Small, and Medium Enterprises (MSMEs). Newly established or young businesses are often excluded from formal credit markets as traditional underwriting approaches rely heavily on credit bureau data. This study investigates the potential of bank statement data as an alternative data source for credit assessment to promote financial inclusion in emerging markets. Firstly, we propose a cash flow-based underwriting pipeline where we utilise bank statement data for end-to-end data extraction and machine learning credit scoring. Secondly, we introduce a novel dataset of 611 loan applicants from a Malaysian lending institution. Thirdly, we develop and evaluate credit scoring models based on application information and bank transaction-derived features. Empirical results show that the use of such data boosts the performance of all models on our dataset, which can improve credit scoring for new-to-lending MSMEs. Lastly, we intend to release the anonymised bank transaction dataset to facilitate further research on MSMEs financial inclusion within Malaysia's emerging economy.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays</title>
<link>https://arxiv.org/abs/2510.22830</link>
<guid>https://arxiv.org/abs/2510.22830</guid>
<content:encoded><![CDATA[
arXiv:2510.22830v3 Announce Type: replace-cross 
Abstract: BERT and its variants are extensively explored for automated scoring. However, a limit of 512 tokens for these encoder-based models showed the deficiency in automated scoring of long essays. Thus, this research explores generative language models for automated scoring of long essays via summarization and prompting. The results revealed great improvement of scoring accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab Automated Essay Scoring 2.0 dataset.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2510.23241</link>
<guid>https://arxiv.org/abs/2510.23241</guid>
<content:encoded><![CDATA[
arXiv:2510.23241v2 Announce Type: replace-cross 
Abstract: In this work, we introduce Progressive Growing of Patch Size, an automatic curriculum learning approach for 3D medical image segmentation. Our approach progressively increases the patch size during model training, resulting in an improved class balance for smaller patch sizes and accelerated convergence of the training process. We evaluate our curriculum approach in two settings: a resource-efficient mode and a performance mode, both regarding Dice score performance and computational costs across 15 diverse and popular 3D medical image segmentation tasks. The resource-efficient mode matches the Dice score performance of the conventional constant patch size sampling baseline with a notable reduction in training time to only 44%. The performance mode improves upon constant patch size segmentation results, achieving a statistically significant relative mean performance gain of 1.28% in Dice Score. Remarkably, across all 15 tasks, our proposed performance mode manages to surpass the constant patch size baseline in Dice Score performance, while simultaneously reducing training time to only 89%. The benefits are particularly pronounced for highly imbalanced tasks such as lesion segmentation tasks. Rigorous experiments demonstrate that our performance mode not only improves mean segmentation performance but also reduces performance variance, yielding more trustworthy model comparison. Furthermore, our findings reveal that the proposed curriculum sampling is not tied to a specific architecture but represents a broadly applicable strategy that consistently boosts performance across diverse segmentation models, including UNet, UNETR, and SwinUNETR. In summary, we show that this simple yet elegant transformation on input data substantially improves both Dice Score performance and training runtime, while being compatible across diverse segmentation backbones.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tongyi DeepResearch Technical Report</title>
<link>https://arxiv.org/abs/2510.24701</link>
<guid>https://arxiv.org/abs/2510.24701</guid>
<content:encoded><![CDATA[
arXiv:2510.24701v2 Announce Type: replace-cross 
Abstract: We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PyDPF: A Python Package for Differentiable Particle Filtering</title>
<link>https://arxiv.org/abs/2510.25693</link>
<guid>https://arxiv.org/abs/2510.25693</guid>
<content:encoded><![CDATA[
arXiv:2510.25693v2 Announce Type: replace-cross 
Abstract: State-space models (SSMs) are a widely used tool in time series analysis. In the complex systems that arise from real-world data, it is common to employ particle filtering (PF), an efficient Monte Carlo method for estimating the hidden state corresponding to a sequence of observations. Applying particle filtering requires specifying both the parametric form and the parameters of the system, which are often unknown and must be estimated. Gradient-based optimisation techniques cannot be applied directly to standard particle filters, as the filters themselves are not differentiable. However, several recently proposed methods modify the resampling step to make particle filtering differentiable. In this paper, we present an implementation of several such differentiable particle filters (DPFs) with a unified API built on the popular PyTorch framework. Our implementation makes these algorithms easily accessible to a broader research community and facilitates straightforward comparison between them. We validate our framework by reproducing experiments from several existing studies and demonstrate how DPFs can be applied to address several common challenges with state space modelling.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabArena: A Living Benchmark for Machine Learning on Tabular Data</title>
<link>https://arxiv.org/abs/2506.16791</link>
<guid>https://arxiv.org/abs/2506.16791</guid>
<content:encoded><![CDATA[
<div> benchmarking, tabular data, deep learning, ensembling, foundation models

Summary:
The article introduces TabArena, a continuously updated benchmarking system for tabular data. It addresses the need for standardized benchmarks in the rapidly evolving field of deep learning for tabular data. The system includes a curated collection of datasets and models, a public leaderboard, and a team of maintainers. Findings from a large-scale benchmarking study highlight the impact of validation methods and ensembling on model performance. While gradient-boosted trees remain competitive, deep learning methods have shown significant progress, especially with ensembling. Foundation models excel on smaller datasets. Ensembles across models further push the boundaries of tabular machine learning. It is also noted that some deep learning models may be overrepresented in ensembles due to validation set overfitting, urging developers to address this issue. The TabArena platform offers a living benchmark with a public leaderboard, reproducible code, and maintenance protocols, available at https://tabarena.ai. 

<br /><br />Summary: <div>
arXiv:2506.16791v4 Announce Type: replace 
Abstract: With the growing popularity of deep learning and foundation models for tabular data, the need for standardized and reliable benchmarks is higher than ever. However, current benchmarks are static. Their design is not updated even if flaws are discovered, model versions are updated, or new models are released. To address this, we introduce TabArena, the first continuously maintained living tabular benchmarking system. To launch TabArena, we manually curate a representative collection of datasets and well-implemented models, conduct a large-scale benchmarking study to initialize a public leaderboard, and assemble a team of experienced maintainers. Our results highlight the influence of validation method and ensembling of hyperparameter configurations to benchmark models at their full potential. While gradient-boosted trees are still strong contenders on practical tabular datasets, we observe that deep learning methods have caught up under larger time budgets with ensembling. At the same time, foundation models excel on smaller datasets. Finally, we show that ensembles across models advance the state-of-the-art in tabular machine learning. We observe that some deep learning models are overrepresented in cross-model ensembles due to validation set overfitting, and we encourage model developers to address this issue. We launch TabArena with a public leaderboard, reproducible code, and maintenance protocols to create a living benchmark available at https://tabarena.ai.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flight Delay Prediction via Cross-Modality Adaptation of Large Language Models and Aircraft Trajectory Representation</title>
<link>https://arxiv.org/abs/2510.23636</link>
<guid>https://arxiv.org/abs/2510.23636</guid>
<content:encoded><![CDATA[
<div> Keywords: flight delay prediction, large language model, multimodal, trajectory data, real-time updates

Summary:
This paper introduces a novel approach to flight delay prediction using a lightweight large language model-based multimodal framework. The model combines trajectory data with textual aeronautical information to accurately predict delays in the terminal area. By integrating linguistic understanding with trajectory representations, the model achieves sub-minute prediction error by capturing airspace conditions and sources of delay. The framework demonstrates practicality and scalability for real-world operations, supporting real-time updates to refine predictions with new operational information. Overall, the approach shows significant potential for enhancing delay prediction accuracy and efficiency in air traffic management. <br /><br />Summary: <div>
arXiv:2510.23636v2 Announce Type: replace 
Abstract: Flight delay prediction has become a key focus in air traffic management, as delays highlight inefficiencies that impact overall network performance. This paper presents a lightweight large language model-based multimodal flight delay prediction, formulated from the perspective of air traffic controllers monitoring aircraft delay after entering the terminal area. The approach integrates trajectory representations with textual aeronautical information, including flight information, weather reports, and aerodrome notices, by adapting trajectory data into the language modality to capture airspace conditions. The experiments show that the model consistently achieves sub-minute prediction error by effectively leveraging contextual information related to the sources of delay, fulfilling the operational standard for minute-level precision. The framework demonstrates that linguistic understanding, when combined with cross-modality adaptation of trajectory data, enhances delay prediction. Moreover, the approach shows practicality and potential scalability for real-world operations, supporting real-time updates that refine predictions upon receiving new operational information.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers</title>
<link>https://arxiv.org/abs/2510.23912</link>
<guid>https://arxiv.org/abs/2510.23912</guid>
<content:encoded><![CDATA[
<div> weights, attention mechanisms, LLMs, GPT-3, redundancy<br />
Summary:<br />
The study examines the Query, Key, Value weight triplet in attention mechanisms of large language models (LLMs). By simplifying assumptions, the researchers demonstrate that Query weights are redundant, leading to a reduction in non-embedding/lm-head parameters by over 8%. This reduction has been validated on GPT-3 small architectures with various complexities, including layer normalization and weight decay, showing comparable validation loss to standard models. The results suggest the potential for reducing redundancy in Query weights at scale, providing insights for more efficient model architectures. <div>
arXiv:2510.23912v2 Announce Type: replace 
Abstract: The Query, Key, Value weight triplet is a building block of current attention mechanisms in state-of-the-art LLMs. We theoretically investigate whether this triplet can be reduced, proving under simplifying assumptions that the Query weights are redundant, thereby reducing the number of non-embedding/lm-head parameters by over 8%. We validate the theory on full-complexity GPT-3 small architectures (with layer normalization, skip connections, and weight decay) trained from scratch, demonstrating that the reduced model achieves comparable validation loss to standard baselines. These findings motivate the investigation of the Query weight redundancy at scale.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Localized Kernel Projection Outlyingness: A Two-Stage Approach for Multi-Modal Outlier Detection</title>
<link>https://arxiv.org/abs/2510.24043</link>
<guid>https://arxiv.org/abs/2510.24043</guid>
<content:encoded><![CDATA[
<div> Keywords: Two-Stage LKPLO, outlier detection, projection-based methods, loss-based outlyingness measure, multi-cluster data.<br />
Summary:<br />
This paper introduces Two-Stage LKPLO, a novel framework for outlier detection that addresses limitations of conventional projection-based methods. It utilizes a generalized loss-based outlyingness measure (PLO) with adaptive loss functions, a global kernel PCA stage for linearizing non-linear data structures, and a local clustering stage for multi-modal distributions. Experiments on 10 benchmark datasets show that Two-Stage LKPLO outperforms strong baselines, particularly on challenging structures like multi-cluster data and complex, high-dimensional data. The combination of kernelization and localization stages is crucial for its superior performance. This work offers a valuable tool for outlier detection problems and highlights the effectiveness of hybrid, multi-stage architectures. <br /> <div>
arXiv:2510.24043v3 Announce Type: replace 
Abstract: This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection framework that overcomes the coexisting limitations of conventional projection-based methods: their reliance on a fixed statistical metric and their assumption of a single data structure. Our framework uniquely synthesizes three key concepts: (1) a generalized loss-based outlyingness measure (PLO) that replaces the fixed metric with flexible, adaptive loss functions like our proposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear data structures; and (3) a subsequent local clustering stage to handle multi-modal distributions. Comprehensive 5-fold cross-validation experiments on 10 benchmark datasets, with automated hyperparameter optimization, demonstrate that Two-Stage LKPLO achieves state-of-the-art performance. It significantly outperforms strong baselines on datasets with challenging structures where existing methods fail, most notably on multi-cluster data (Optdigits) and complex, high-dimensional data (Arrhythmia). Furthermore, an ablation study empirically confirms that the synergistic combination of both the kernelization and localization stages is indispensable for its superior performance. This work contributes a powerful new tool for a significant class of outlier detection problems and underscores the importance of hybrid, multi-stage architectures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Extreme Learning Machine (PIELM): Opportunities and Challenges</title>
<link>https://arxiv.org/abs/2510.24577</link>
<guid>https://arxiv.org/abs/2510.24577</guid>
<content:encoded><![CDATA[
<div> physics-informed extreme learning machine, PIELM, computational efficiency, accuracy, ODEs, PDEs

Summary: 
The article discusses the advancements in physics-informed extreme learning machines (PIELM) compared to other physics-informed machine learning paradigms, highlighting their higher computational efficiency and accuracy. Despite the lack of a comprehensive review, the authors share their perspectives on PIELM and its potential in solving ordinary and partial differential equations (ODEs/PDEs) with various challenges such as sharp gradients, nonlinearities, high-frequency behavior, hard constraints, uncertainty, and interpretability. While PIELM has shown promising results in scientific and engineering applications, there are still pressing challenges that need to be addressed. The opportunities lie in developing more robust, interpretable, and generalizable PIELM frameworks to enhance the effectiveness and applicability of this approach. <div>
arXiv:2510.24577v2 Announce Type: replace 
Abstract: We are delighted to see the recent development of physics-informed extreme learning machine (PIELM) for its higher computational efficiency and accuracy compared to other physics-informed machine learning (PIML) paradigms. Since a comprehensive summary or review of PIELM is currently unavailable, we would like to take this opportunity to share our perspectives and experiences on this promising research direction. We can see that many efforts have been made to solve ordinary/partial differential equations (ODEs/PDEs) characterized by sharp gradients, nonlinearities, high-frequency behavior, hard constraints, uncertainty, multiphysics coupling, and interpretability. Despite these encouraging successes, many pressing challenges remain to be tackled, which also provides opportunities to develop more robust, interpretable, and generalizable PIELM frameworks for scientific and engineering applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topic Analysis with Side Information: A Neural-Augmented LDA Approach</title>
<link>https://arxiv.org/abs/2510.24918</link>
<guid>https://arxiv.org/abs/2510.24918</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Latent Dirichlet Allocation, Topic Models, Side Information, Probabilistic Modeling
<br />
Summary:
<br />
The article introduces nnLDA, a neural-augmented probabilistic topic model that incorporates side information through a neural prior mechanism. Unlike traditional models like LDA, nnLDA dynamically captures complex interactions between auxiliary features and topic distributions. A stochastic variational Expectation-Maximization algorithm optimizes both neural and probabilistic components, resulting in improved performance in topic coherence, perplexity, and classification tasks compared to LDA and Dirichlet-Multinomial Regression. The integration of neural representation learning with probabilistic topic modeling is particularly beneficial in scenarios with available side information. <div>
arXiv:2510.24918v2 Announce Type: replace 
Abstract: Traditional topic models such as Latent Dirichlet Allocation (LDA) have been widely used to uncover latent structures in text corpora, but they often struggle to integrate auxiliary information such as metadata, user attributes, or document labels. These limitations restrict their expressiveness, personalization, and interpretability. To address this, we propose nnLDA, a neural-augmented probabilistic topic model that dynamically incorporates side information through a neural prior mechanism. nnLDA models each document as a mixture of latent topics, where the prior over topic proportions is generated by a neural network conditioned on auxiliary features. This design allows the model to capture complex nonlinear interactions between side information and topic distributions that static Dirichlet priors cannot represent. We develop a stochastic variational Expectation-Maximization algorithm to jointly optimize the neural and probabilistic components. Across multiple benchmark datasets, nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in topic coherence, perplexity, and downstream classification. These results highlight the benefits of combining neural representation learning with probabilistic topic modeling in settings where side information is available.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs</title>
<link>https://arxiv.org/abs/2509.16648</link>
<guid>https://arxiv.org/abs/2509.16648</guid>
<content:encoded><![CDATA[
<div> Trust Assessment, Multimodal Large Language Models, Functionally Equivalent Sampling, Uncertainty Quantification, Selective Prediction <br />
<br />
Summary: <br />
The paper introduces a novel technique called Functionally Equivalent Sampling for Trust Assessment (FESTA) to accurately assess the trust of predictions made by multimodal large language models (MLLMs). FESTA utilizes task-preserving sampling to expand the input space and evaluate the consistency and sensitivity of the model through equivalent and complementary samples, respectively. This black-box approach does not require ground truth, making it unsupervised. Experimental results with various off-the-shelf multimodal LLMs show that FESTA significantly improves selective prediction performance in visual and audio reasoning tasks. The uncertainty estimate provided by FESTA leads to a 33.3% relative improvement for vision-LLMs and a 29.6% relative improvement for audio-LLMs based on the AUROC metric, enhancing the detection of mispredictions. The code implementation of FESTA is also open-sourced. <br /> <div>
arXiv:2509.16648v3 Announce Type: replace-cross 
Abstract: The accurate trust assessment of multimodal large language models (MLLMs) generated predictions, which can enable selective prediction and improve user confidence, is challenging due to the diverse multi-modal input paradigms. We propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a multimodal input sampling technique for MLLMs, that generates an uncertainty measure based on the equivalent and complementary input samplings. The proposed task-preserving sampling approach for uncertainty quantification expands the input space to probe the consistency (through equivalent samples) and sensitivity (through complementary samples) of the model. FESTA uses only input-output access of the model (black-box), and does not require ground truth (unsupervised). The experiments are conducted with various off-the-shelf multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA uncertainty estimate achieves significant improvement (33.3% relative improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in selective prediction performance, based on area-under-receiver-operating-characteristic curve (AUROC) metric in detecting mispredictions. The code implementation is open-sourced.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rough Path Signatures: Learning Neural RDEs for Portfolio Optimization</title>
<link>https://arxiv.org/abs/2510.10728</link>
<guid>https://arxiv.org/abs/2510.10728</guid>
<content:encoded><![CDATA[
<div> deep learning, path-dependent valuation, risk-sensitive control, neural rough differential equation, financial models<br />
<br />
Summary:<br />
This study introduces a novel deep BSDE/2BSDE solver that combines truncated log-signatures with a neural rough differential equation (RDE) backbone to address high-dimensional, path-dependent valuation and control problems in finance. The method incorporates a CVaR-tilted terminal objective for targeting left-tail risk and a 2BSDE head for estimating curvature in risk-sensitive control. Results show that the proposed method outperforms strong baselines in terms of accuracy, tail fidelity, and training stability across Asian and barrier option pricing as well as portfolio control tasks. It achieves a lower CVaR(0.99) of 9.80% compared to 12.00-13.10% for baselines, a minimal HJB residual of 0.011, and superior RMSEs for Z and Gamma. Ablation studies confirm the benefits of the sequence-to-path representation and the 2BSDE head in improving performance. This work demonstrates the synergy between stochastic analysis and deep learning in advancing financial modeling capabilities. <br /> <div>
arXiv:2510.10728v3 Announce Type: replace-cross 
Abstract: We tackle high-dimensional, path-dependent valuation and control and introduce a deep BSDE/2BSDE solver that couples truncated log-signatures with a neural rough differential equation (RDE) backbone. The architecture aligns stochastic analysis with sequence-to-path learning: a CVaR-tilted terminal objective targets left-tail risk, while an optional second-order (2BSDE) head supplies curvature estimates for risk-sensitive control. Under matched compute and parameter budgets, the method improves accuracy, tail fidelity, and training stability across Asian and barrier option pricing and portfolio control: at d=200 it achieves CVaR(0.99)=9.80% versus 12.00-13.10% for strong baselines, attains the lowest HJB residual (0.011), and yields the lowest RMSEs for Z and Gamma. Ablations over truncation depth, local windows, and tilt parameters confirm complementary gains from the sequence-to-path representation and the 2BSDE head. Taken together, the results highlight a bidirectional dialogue between stochastic analysis and modern deep learning: stochastic tools inform representations and objectives, while sequence-to-path models expand the class of solvable financial models at scale.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Can Be Recovered Under Sparse Adversarial Corruption? Assumption-Free Theory for Linear Measurements</title>
<link>https://arxiv.org/abs/2510.24215</link>
<guid>https://arxiv.org/abs/2510.24215</guid>
<content:encoded><![CDATA[
<div> projection matrix, recoverability, sparse vector, matrix, submatrices

Summary: 
This study addresses the problem of recovering a sparse vector $x^\star$ from a linear measurement $y = A x^\star + e$, where $A$ is a known matrix and $e$ is a $q$-sparse adversarial vector. The goal is to find the smallest set containing $x^\star$ that can be recovered from $y$ without knowledge of $e. The study demonstrates that the best achievable recovery set is $x^\star + \ker(U)$, where $U$ is a projection matrix onto the intersection of rowspaces of submatrices of $A$ obtained by deleting $2q$ rows. Additionally, it is proven that any $x$ minimizing the $\ell_0$-norm of $y - A x$ also lies in $x^\star + \ker(U)$. This result provides a constructive method for recovering the defined set without explicit knowledge of $e. The study sheds light on the recoverability limits for arbitrary matrices and sparse vectors in the presence of adversarial noise. <div>
arXiv:2510.24215v2 Announce Type: replace-cross 
Abstract: Let $A \in \mathbb{R}^{m \times n}$ be an arbitrary, known matrix and $e$ a $q$-sparse adversarial vector. Given $y = A x^\star + e$ and $q$, we seek the smallest set containing $x^\star$--hence the one conveying maximal information about $x^\star$--that is uniformly recoverable from $y$ without knowing $e$. While exact recovery of $x^\star$ via strong (and often impractical) structural assumptions on $A$ or $x^\star$ (e.g., restricted isometry, sparsity) is well studied, recoverability for arbitrary $A$ and $x^\star$ remains open. Our main result shows that the best that one can hope to recover is $x^\star + \ker(U)$, where $U$ is the unique projection matrix onto the intersection of rowspaces of all possible submatrices of $A$ obtained by deleting $2q$ rows. Moreover, we prove that every $x$ that minimizes the $\ell_0$-norm of $y - A x$ lies in $x^\star + \ker(U)$, which then gives a constructive approach to recover this set.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games</title>
<link>https://arxiv.org/abs/2511.00002</link>
<guid>https://arxiv.org/abs/2511.00002</guid>
<content:encoded><![CDATA[
<div> Virtual Reality, VR, quality assurance, automated testing, deep learning
Summary: 
Virtual Reality (VR) has become a popular platform for gaming and interactive experiences, presenting challenges in ensuring quality, safety, and appropriateness of VR content. Traditional human-based quality assurance is not scalable with the industry's growth. VRScout, a deep learning-based agent, autonomously navigates VR environments, learns from human demonstrations, and predicts multi-step action sequences to achieve expert-level performance. It introduces a dynamically adjustable sliding horizon for real-time inference at 60 FPS on consumer-grade hardware. VRScout offers a practical, scalable framework for automated VR game testing, with applications in quality assurance and safety auditing. <div>
arXiv:2511.00002v1 Announce Type: new 
Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and interactive experiences, yet ensuring the quality, safety, and appropriateness of VR content remains a pressing challenge. Traditional human-based quality assurance is labor-intensive and cannot scale with the industry's rapid growth. While automated testing has been applied to traditional 2D and 3D games, extending it to VR introduces unique difficulties due to high-dimensional sensory inputs and strict real-time performance requirements. We present VRScout, a deep learning-based agent capable of autonomously navigating VR environments and interacting with virtual objects in a human-like and real-time manner. VRScout learns from human demonstrations using an enhanced Action Chunking Transformer that predicts multi-step action sequences. This enables our agent to capture higher-level strategies and generalize across diverse environments. To balance responsiveness and precision, we introduce a dynamically adjustable sliding horizon that adapts the agent's temporal context at runtime. We evaluate VRScout on commercial VR titles and show that it achieves expert-level performance with only limited training data, while maintaining real-time inference at 60 FPS on consumer-grade hardware. These results position VRScout as a practical and scalable framework for automated VR game testing, with direct applications in both quality assurance and safety auditing.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts</title>
<link>https://arxiv.org/abs/2511.00029</link>
<guid>https://arxiv.org/abs/2511.00029</guid>
<content:encoded><![CDATA[
<div> Sparse Auto Encoders, Large Language Model, safety-utility tradeoffs, feature selection, steering features <br />
Summary: <br />
This study presents a novel approach using Sparse Auto Encoders (SAEs) to guide Large Language Models (LLMs) in recognizing and not answering unsafe prompts while complying with safe prompts. By efficiently selecting the best steering features through a contrasting prompt method and utilizing the AI-Generated Prompts Dataset, the researchers tested this method on Llama-3 8B. They found that their approach improved safety performance by 18.9% while also increasing utility by 11.1%. This highlights the potential of targeted SAE steering to overcome traditional safety-utility tradeoffs and achieve better performance in LLM deployment. The systematic feature selection methods and principled evaluation used in this study provide a promising avenue for enhancing the safety and utility of LLMs in real-world applications. <div>
arXiv:2511.00029v1 Announce Type: new 
Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize and not answer unsafe prompts while complying with safe prompts. Previous methods for achieving this require adjusting model weights along with other expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have enabled interpretable feature extraction from LLMs, existing approaches lack systematic feature selection methods and principled evaluation of safety-utility tradeoffs. We explored using different steering features and steering strengths using Sparse Auto Encoders (SAEs) to provide a solution. Using an accurate and innovative contrasting prompt method with the AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air Bench eu-dataset to efficiently choose the best features in the model to steer, we tested this method on Llama-3 8B. We conclude that using this method, our approach achieves an 18.9% improvement in safety performance while simultaneously increasing utility by 11.1%, demonstrating that targeted SAE steering can overcome traditional safety-utility tradeoffs when optimal features are identified through principled selection methods.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing Knowledge Holes in Unlearned LLMs</title>
<link>https://arxiv.org/abs/2511.00030</link>
<guid>https://arxiv.org/abs/2511.00030</guid>
<content:encoded><![CDATA[
<div> Machine unlearning, knowledge preservation, test case generation, knowledge holes, hidden costs<br />
<br />
Summary:<br />
Machine unlearning has become a popular approach for removing unwanted knowledge without retraining. However, existing techniques may inadvertently lead to "knowledge holes," where benign knowledge is lost. A new test case generation framework is proposed to identify these knowledge gaps. Evaluation shows that unlearned models often fail to provide relevant responses, indicating significant hidden costs of unlearning. This calls for a reevaluation of how knowledge preservation in unlearning is assessed, beyond traditional benchmarks. <div>
arXiv:2511.00030v1 Announce Type: new 
Abstract: Machine unlearning has emerged as a prevalent technical solution for selectively removing unwanted knowledge absorbed during pre-training, without requiring full retraining. While recent unlearning techniques can effectively remove undesirable content without severely compromising performance on standard benchmarks, we find that they may inadvertently create ``knowledge holes'' -- unintended losses of benign knowledge that standard benchmarks fail to capture. To probe where unlearned models reveal knowledge holes, we propose a test case generation framework that explores both immediate neighbors of unlearned content and broader areas of potential failures. Our evaluation demonstrates significant hidden costs of unlearning: up to 98.7\% of the test cases yield irrelevant or nonsensical responses from unlearned models, despite being answerable by the pretrained model. These findings necessitate rethinking the conventional approach to evaluating knowledge preservation in unlearning, moving beyond standard, static benchmarks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators</title>
<link>https://arxiv.org/abs/2511.00032</link>
<guid>https://arxiv.org/abs/2511.00032</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Operators, Partial Differential Equations, Skip-Block Routing, Transformer-based, Computational Cost <br />
Summary: 
The article introduces Skip-Block Routing (SBR), a framework designed for enhancing the efficiency of Neural Operators in solving Partial Differential Equations (PDEs) for large-scale engineering tasks. SBR integrates into multi-layer architectures of Transformer-based neural operators by utilizing a routing mechanism to learn token complexity and prioritize processing capacity accordingly. By dynamically allocating computational resources based on token complexity, SBR reduces the overall computational cost by approximately 50% in terms of Floating Point Operations (FLOPs), while maintaining accuracy. This approach addresses the mismatch between the uniform computational cost of current models and the varying complexities of physical fields, offering up to a 2x faster inference speed. SBR demonstrates its versatility by seamlessly integrating into different neural operators, showcasing its potential in improving computational efficiency for PDE solutions. <br /><br />Summary: <div>
arXiv:2511.00032v1 Announce Type: new 
Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular approach for solving Partial Differential Equations (PDEs). However, their application to large-scale engineering tasks suffers from significant computational overhead. And the fact that current models impose a uniform computational cost while physical fields exhibit vastly different complexities constitutes a fundamental mismatch, which is the root of this inefficiency. For instance, in turbulence flows, intricate vortex regions require deeper network processing compared to stable flows. To address this, we introduce a framework: Skip-Block Routing (SBR), a general framework designed for Transformer-based neural operators, capable of being integrated into their multi-layer architectures. First, SBR uses a routing mechanism to learn the complexity and ranking of tokens, which is then applied during inference. Then, in later layers, it decides how many tokens are passed forward based on this ranking. This way, the model focuses more processing capacity on the tokens that are more complex. Experiments demonstrate that SBR is a general framework that seamlessly integrates into various neural operators. Our method reduces computational cost by approximately 50% in terms of Floating Point Operations (FLOPs), while still delivering up to 2x faster inference without sacrificing accuracy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series</title>
<link>https://arxiv.org/abs/2511.00035</link>
<guid>https://arxiv.org/abs/2511.00035</guid>
<content:encoded><![CDATA[
<div> Keywords: energy forecasting, neural architecture search, time series modeling, efficiency, accuracy <br />
Summary:
A new framework utilizing neural architecture search (NAS) automates the discovery of efficient time series models for energy production forecasting. The manual configuration of complex methods is time-consuming and prone to errors. The NAS-based approach balances computational efficiency, predictive performance, and generalization power. The search space includes efficient components that capture energy time series patterns. An innovative objective function prioritizes performance generalization and maximizes exploration of the search space. Results indicate that the ensemble of lightweight architectures discovered through NAS outperforms both Transformers and pre-trained models in terms of efficiency and accuracy. <div>
arXiv:2511.00035v1 Announce Type: new 
Abstract: The dynamic energy sector requires both predictive accuracy and runtime efficiency for short-term forecasting of energy generation under operational constraints, where timely and precise predictions are crucial. The manual configuration of complex methods, which can generate accurate global multi-step predictions without suffering from a computational bottleneck, represents a procedure with significant time requirements and high risk for human-made errors. A further intricacy arises from the temporal dynamics present in energy-related data. Additionally, the generalization to unseen data is imperative for continuously deploying forecasting techniques over time. To overcome these challenges, in this research, we design a neural architecture search (NAS)-based framework for the automated discovery of time series models that strike a balance between computational efficiency, predictive performance, and generalization power for the global, multi-step short-term forecasting of energy production time series. In particular, we introduce a search space consisting only of efficient components, which can capture distinctive patterns of energy time series. Furthermore, we formulate a novel objective function that accounts for performance generalization in temporal context and the maximal exploration of different regions of our high-dimensional search space. The results obtained on energy production time series show that an ensemble of lightweight architectures discovered with NAS outperforms state-of-the-art techniques, such as Transformers, as well as pre-trained forecasting models, in terms of both efficiency and accuracy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Preference Optimization with Limited Feedback</title>
<link>https://arxiv.org/abs/2511.00040</link>
<guid>https://arxiv.org/abs/2511.00040</guid>
<content:encoded><![CDATA[
<div> Keywords: preference optimization, semi-supervised learning, pairwise labels, unpaired data, data efficiency 

Summary: 
The article introduces the concept of Semi-Supervised Preference Optimization (SSPO) to address the resource-intensive nature of existing preference optimization methods. It aims to combine a small number of labeled pairwise preferences with a larger pool of unlabeled data to improve efficiency. The key theoretical contribution of the study is the identification of an optimal reward threshold that can separate winning and losing responses with high probability, enabling pseudo-labeling of unpaired data. By leveraging these pseudo-labels, SSPO effectively distills latent preferences from large unpaired datasets, reducing acquisition costs significantly while maintaining alignment with human preferences. Experimental results across datasets demonstrate the impressive data efficiency of SSPO, with models trained on only 1% of the UltraFeedback dataset consistently outperforming baselines trained on 10% of the same dataset. <div>
arXiv:2511.00040v1 Announce Type: new 
Abstract: The field of preference optimization has made outstanding contributions to the alignment of language models with human preferences. Despite these advancements, recent methods still rely heavily on substantial paired (labeled) feedback data, leading to substantial resource expenditures. To address these challenges, we study the problem of Semi-Supervised Preference Optimization (SSPO) in which the idea is to learn from both a small number of pairwise preference labels and a large pool of unpaired samples simultaneously. Our key theoretical contribution proves the existence of an optimal reward threshold capable of separating winning and losing responses with high probability, which enables a principled pseudo-labeling of unpaired data. By leveraging these pseudo-labels, SSPO effectively distills latent preferences from large-scale unpaired data, thus maintaining human alignment while drastically reducing acquisition costs. Extensive experiments across datasets validate this remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct on just 1% of UltraFeedback consistently surpasses strong baselines trained on 10% of UltraFeedback.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations</title>
<link>https://arxiv.org/abs/2511.00043</link>
<guid>https://arxiv.org/abs/2511.00043</guid>
<content:encoded><![CDATA[
<div> PINNs, Physics-Informed Neural Networks, ODEs, numerical methods, benchmark problems

Summary:

In this study, the Physics-Informed Neural Networks (PINNs) methodology is validated for solving engineering and biological dynamical systems governed by ordinary differential equations (ODEs). Traditional numerical methods may struggle with high stiffness, shocks, irregular domains, and other challenging scenarios. PINNs embed physical laws into the learning process, offering a powerful approach. The study systematically evaluates PINNs on classical ODE problems, highlighting the importance of balancing loss function components and tuning hyperparameters for accurate solutions. Prior knowledge embedding and hard constraints imposition on network architecture enhance predictive capability without losing generality. The study emphasizes the significance of systematic tuning and careful weighting for accurate and efficient predictions using PINNs.

<br /><br />Summary: <div>
arXiv:2511.00043v1 Announce Type: new 
Abstract: In this study, we present and validate the predictive capability of the Physics-Informed Neural Networks (PINNs) methodology for solving a variety of engineering and biological dynamical systems governed by ordinary differential equations (ODEs). While traditional numerical methods a re effective for many ODEs, they often struggle to achieve convergence in problems involving high stiffness, shocks, irregular domains, singular perturbations, high dimensions, or boundary discontinuities. Alternatively, PINNs offer a powerful approach for handling challenging numerical scenarios. In this study, classical ODE problems are employed as controlled testbeds to systematically evaluate the accuracy, training efficiency, and generalization capability under controlled conditions of the PINNs framework. Although not a universal solution, PINNs can achieve superior results by embedding physical laws directly into the learning process. We first analyze the existence and uniqueness properties of several benchmark problems and subsequently validate the PINNs methodology on these model systems. Our results demonstrate that for complex problems to converge to correct solutions, the loss function components data loss, initial condition loss, and residual loss must be appropriately balanced through careful weighting. We further establish that systematic tuning of hyperparameters, including network depth, layer width, activation functions, learning rate, optimization algorithms, w eight initialization schemes, and collocation point sampling, plays a crucial role in achieving accurate solutions. Additionally, embedding prior knowledge and imposing hard constraints on the network architecture, without loss the generality of the ODE system, significantly enhances the predictive capability of PINNs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks</title>
<link>https://arxiv.org/abs/2511.00044</link>
<guid>https://arxiv.org/abs/2511.00044</guid>
<content:encoded><![CDATA[
<div> Physical Neural Networks (PNN), weight-tying, ReLaX-Net, hardware-friendly, time-multiplexing

Summary:
ReLaX-Net is a novel architecture for Physical Neural Networks (PNN) that utilizes a layer-by-layer time-multiplexing approach to enhance the network depth and optimize parameter usage. Through the implementation of fast switches in existing PNN systems, ReLaX-Net demonstrates improved computational performance on image classification and natural language processing tasks. The proposed architecture capitalizes on the time-scale separation between active elements and trainable weights in PNNs, boosting efficiency without requiring significant modifications. Numerical experiments validate ReLaX-Net's efficacy, showing superior performance compared to traditional Recurrent Neural Networks (RNNs) and Deep Neural Networks (DNNs) with equivalent parameter counts. This advancement aligns with historical trends seen in digital neural networks, where efficient parameter reuse and architecture optimizations led to significant performance gains. ReLaX-Net represents a promising step towards enhancing PNN scalability and performance in next-generation computing systems. 

<br /><br />Summary: <div>
arXiv:2511.00044v1 Announce Type: new 
Abstract: Physical Neural Networks (PNN) are promising platforms for next-generation computing systems. However, recent advances in digital neural network performance are largely driven by the rapid growth in the number of trainable parameters and, so far, demonstrated PNNs are lagging behind by several orders of magnitude in terms of scale. This mirrors size and performance constraints found in early digital neural networks. In that period, efficient reuse of parameters contributed to the development of parameter-efficient architectures such as convolutional neural networks.
  In this work, we numerically investigate hardware-friendly weight-tying for PNNs. Crucially, with many PNN systems, there is a time-scale separation between the fast dynamic active elements of the forward pass and the only slowly trainable elements implementing weights and biases. With this in mind,we propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net) architecture, which employs a simple layer-by-layer time-multiplexing scheme to increase the effective network depth and efficiently use the number of parameters. We only require the addition of fast switches for existing PNNs. We validate ReLaX-Nets via numerical experiments on image classification and natural language processing tasks. Our results show that ReLaX-Net improves computational performance with only minor modifications to a conventional PNN. We observe a favorable scaling, where ReLaX-Nets exceed the performance of equivalent traditional RNNs or DNNs with the same number of parameters.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection</title>
<link>https://arxiv.org/abs/2511.00047</link>
<guid>https://arxiv.org/abs/2511.00047</guid>
<content:encoded><![CDATA[
<div> Graph-BERT, GCN, financial fraud detection, DynBERG, temporal evolution<br />
<br />
Summary: <br />
Financial fraud detection is vital in decentralized environments like cryptocurrency networks. The DynBERG model integrates Graph-BERT with a GRU layer to capture temporal evolution in dynamic financial transaction networks with directed edges. Evaluated on the Elliptic dataset containing Bitcoin transactions, DynBERG outperformed state-of-the-art dynamic graph classification approaches like EvolveGCN and GCN. The model's resilience was tested before and after the Dark Market Shutdown, showing its adaptability to significant market shifts. The incorporation of the GRU layer effectively captures the temporal dynamics of financial transactions, indicating its importance in improving model performance. <div>
arXiv:2511.00047v1 Announce Type: new 
Abstract: Financial fraud detection is critical for maintaining the integrity of financial systems, particularly in decentralised environments such as cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are widely used for financial fraud detection, graph Transformer models such as Graph-BERT are gaining prominence due to their Transformer-based architecture, which mitigates issues such as over-smoothing. Graph-BERT is designed for static graphs and primarily evaluated on citation networks with undirected edges. However, financial transaction networks are inherently dynamic, with evolving structures and directed edges representing the flow of money. To address these challenges, we introduce DynBERG, a novel architecture that integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture temporal evolution over multiple time steps. Additionally, we modify the underlying algorithm to support directed edges, making DynBERG well-suited for dynamic financial transaction analysis. We evaluate our model on the Elliptic dataset, which includes Bitcoin transactions, including all transactions during a major cryptocurrency market event, the Dark Market Shutdown. By assessing DynBERG's resilience before and after this event, we analyse its ability to adapt to significant market shifts that impact transaction behaviours. Our model is benchmarked against state-of-the-art dynamic graph classification approaches, such as EvolveGCN and GCN, demonstrating superior performance, outperforming EvolveGCN before the market shutdown and surpassing GCN after the event. Additionally, an ablation study highlights the critical role of incorporating a time-series deep learning component, showcasing the effectiveness of GRU in modelling the temporal dynamics of financial transactions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting</title>
<link>https://arxiv.org/abs/2511.00049</link>
<guid>https://arxiv.org/abs/2511.00049</guid>
<content:encoded><![CDATA[
<div> Keywords: weather forecasting, self-supervised learning, graph neural network, spatio-temporal structures, deep learning<br />
Summary: 
This paper presents a novel self-supervised learning framework for improving multi-variable weather prediction by leveraging spatio-temporal structures. The model combines a graph neural network (GNN) for spatial reasoning, self-supervised pretraining for representation learning, and a spatio-temporal adaptation mechanism for enhanced generalization. Experimental results on ERA5 and MERRA-2 reanalysis datasets show superior performance compared to traditional numerical weather prediction models and recent deep learning approaches. Quantitative evaluations in Beijing and Shanghai demonstrate the model's ability to capture detailed meteorological patterns. The proposed framework offers a scalable and label-efficient solution for data-driven weather forecasting systems. <br /><br />Summary: <div>
arXiv:2511.00049v1 Announce Type: new 
Abstract: Accurate and robust weather forecasting remains a fundamental challenge due to the inherent spatio-temporal complexity of atmospheric systems. In this paper, we propose a novel self-supervised learning framework that leverages spatio-temporal structures to improve multi-variable weather prediction. The model integrates a graph neural network (GNN) for spatial reasoning, a self-supervised pretraining scheme for representation learning, and a spatio-temporal adaptation mechanism to enhance generalization across varying forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis datasets demonstrate that our approach achieves superior performance compared to traditional numerical weather prediction (NWP) models and recent deep learning methods. Quantitative evaluations and visual analyses in Beijing and Shanghai confirm the model's capability to capture fine-grained meteorological patterns. The proposed framework provides a scalable and label-efficient solution for future data-driven weather forecasting systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs</title>
<link>https://arxiv.org/abs/2511.00050</link>
<guid>https://arxiv.org/abs/2511.00050</guid>
<content:encoded><![CDATA[
<div> efficient training, fine-tuning, large language models (LLMs), parameter efficient fine-tuning (PEFT), low-rank adapters (LoRA) 

Summary:
The paper introduces FLoRA, a family of fused forward-backward adapters (FFBA) designed for efficient fine-tuning of large language models (LLMs) on downstream tasks. FLoRA combines concepts from low-rank adapters (LoRA) and parallel adapters to enhance fine-tuning accuracy while reducing latency. By integrating the forward and backward adapters into the base model's projection layers, FLoRA achieves superior performance compared to LoRA with similar parameter budgets. The experimental results demonstrate the effectiveness of FFB adapters in improving accuracy and minimizing latency in fine-tuning LLMs. This innovation addresses the ongoing challenge of efficiently training and fine-tuning increasingly large language models. 

<br /><br />Summary: <div>
arXiv:2511.00050v1 Announce Type: new 
Abstract: As the large language models (LLMs) grow in size each day, efficient training and fine-tuning has never been as important as nowadays. This resulted in the great interest in parameter efficient fine-tuning (PEFT), and effective methods including low-rank adapters (LoRA) has emerged. Although the various PEFT methods have been studied extensively in the recent years, the greater part of the subject remains unexplored with the huge degree of freedom. In this paper, we propose FLoRA, a family of fused forward-backward adapters (FFBA) for parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine ideas from the popular LoRA and parallel adapters to improve the overall fine-tuning accuracies. At the same time, latencies are minimized by fusing the forward and backward adapters into existing projection layers of the base model. Experimental results show that the proposed FFB adapters perform significantly better than the popularly used LoRA in both accuracy and latency for a similar parameter budget.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT</title>
<link>https://arxiv.org/abs/2511.00051</link>
<guid>https://arxiv.org/abs/2511.00051</guid>
<content:encoded><![CDATA[
<div> Efficient Fine-Tuning Methods, LoRA, DoRA, Learnable Weight Conditioning, Pre-Diag, SORA<br />
<br />
Summary:<br />
Parameter-Efficient Fine-Tuning methods are essential for adapting large pre-trained models, with LoRA considered foundational. The DoRA method, building on LoRA, decomposes weight updates into magnitude and direction, increasing singular value entropy for a more uniform update distribution akin to full fine-tuning. Reformulated into a more efficient matrix form, DoRA emerges as a learnable weight conditioning method. A unified framework explores architectural placement and transformation type of the conditioning matrix, leading to the development of Pre-Diag and SORA methods. Pre-Diag calibrates pre-trained weights efficiently with a diagonal conditioning matrix before the LoRA update, enhancing performance and reducing training time. On the other hand, SORA employs parameter-efficient orthogonal rotation for a norm-preserving transformation of the feature space. Experimental results on natural language tasks demonstrate the superior performance and efficiency of these proposed methods over LoRA and DoRA.<br /> <div>
arXiv:2511.00051v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large pre-trained models. Among these, LoRA is considered a foundational approach. Building on this, the influential DoRA method enhances performance by decomposing weight updates into magnitude and direction. However, its underlying mechanism remains unclear, and it introduces significant computational overhead. In this work, we first identify that DoRA's success stems from its capacity to increase the singular value entropy of the weight update matrix, which promotes a more uniform update distribution akin to full fine-tuning. We then reformulate DoRA into a mathematically equivalent and more efficient matrix form, revealing it as a learnable weight conditioning method. Based on this insight, we propose a unified framework for designing advanced PEFT methods by exploring two orthogonal dimensions: the architectural placement and the transformation type of the conditioning matrix. Within this framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies a diagonal conditioning matrix before the LoRA update to efficiently calibrate the pre-trained weights, thereby enhancing performance while reducing training time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation \textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient orthogonal rotation to perform a more powerful, norm-preserving transformation of the feature space. Extensive experiments on natural language understanding and generation tasks demonstrate that our proposed methods achieve superior performance and efficiency compared to both LoRA and DoRA. The code is available at https://github.com/MaeChd/SORA.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Guided Analysis of Neural Networks: A Replication Study</title>
<link>https://arxiv.org/abs/2511.00052</link>
<guid>https://arxiv.org/abs/2511.00052</guid>
<content:encoded><![CDATA[
<div> neural networks, Feature-Guided Analysis, rules, precision, recall

Summary:
Feature-Guided Analysis (FGA) aims to understand neural networks' decision-making processes, crucial for safety-critical applications. By monitoring neuron activations, FGA extracts rules relevant to network tasks. While previous studies show promising results, additional empirical evidence for industrial use is needed. To address this, the paper evaluates FGA on MNIST and LSC benchmark datasets. Results indicate FGA's higher precision compared to existing literature. The study also evaluates the impact of neural network architecture, training, and feature selection on FGA effectiveness. Selection of these components notably affects FGA's recall but has minimal impact on precision. TestBed: MNIST, LSC, High Precision, Neural Network Architecture, Training, Feature Selection, Precision and Recall Analysis. <br /><br />Summary: <div>
arXiv:2511.00052v1 Announce Type: new 
Abstract: Understanding why neural networks make certain decisions is pivotal for their use in safety-critical applications. Feature-Guided Analysis (FGA) extracts slices of neural networks relevant to their tasks. Existing feature-guided approaches typically monitor the activation of the neural network neurons to extract the relevant rules. Preliminary results are encouraging and demonstrate the feasibility of this solution by assessing the precision and recall of Feature-Guided Analysis on two pilot case studies. However, the applicability in industrial contexts needs additional empirical evidence.
  To mitigate this need, this paper assesses the applicability of FGA on a benchmark made by the MNIST and LSC datasets. We assessed the effectiveness of FGA in computing rules that explain the behavior of the neural network. Our results show that FGA has a higher precision on our benchmark than the results from the literature. We also evaluated how the selection of the neural network architecture, training, and feature selection affect the effectiveness of FGA. Our results show that the selection significantly affects the recall of FGA, while it has a negligible impact on its precision.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models</title>
<link>https://arxiv.org/abs/2511.00053</link>
<guid>https://arxiv.org/abs/2511.00053</guid>
<content:encoded><![CDATA[
<div> Keywords: time-series forecasting, training objective, label autocorrelation, weighted training, quadratic-form

Summary: 
The article introduces a new training objective for time-series forecasting models, addressing issues with existing methods. The proposed Quadratic Direct Forecast (QDF) learning algorithm utilizes a quadratic-form weighted training objective to account for label autocorrelation and set heterogeneous task weights for different forecasting tasks. This approach improves forecasting performance by adapting the weighting matrix to optimize forecasting accuracy. Experiments demonstrate that the QDF algorithm outperforms existing methods, achieving state-of-the-art results across various forecast models. Additionally, the code for implementing the QDF algorithm is available for further research and development. Overall, the novel training objective proposed in this study offers a practical solution to improve the accuracy and efficiency of time-series forecasting models. 

Summary:<br /><br />Keywords: time-series forecasting, training objective, label autocorrelation, weighted training, quadratic-form. The article introduces the Quadratic Direct Forecast (QDF) learning algorithm, which utilizes a novel quadratic-form weighted training objective. This approach addresses issues with existing methods by accounting for label autocorrelation and setting heterogeneous task weights. Experimental results demonstrate the superiority of the QDF algorithm in improving forecasting performance across various models, achieving state-of-the-art results. The availability of code for implementation further enhances the practicality and applicability of the proposed approach in optimizing forecasting accuracy and efficiency. <div>
arXiv:2511.00053v1 Announce Type: new 
Abstract: The design of training objective is central to training time-series forecasting models. Existing training objectives such as mean squared error mostly treat each future step as an independent, equally weighted task, which we found leading to the following two issues: (1) overlook the label autocorrelation effect among future steps, leading to biased training objective; (2) fail to set heterogeneous task weights for different forecasting tasks corresponding to varying future steps, limiting the forecasting performance. To fill this gap, we propose a novel quadratic-form weighted training objective, addressing both of the issues simultaneously. Specifically, the off-diagonal elements of the weighting matrix account for the label autocorrelation effect, whereas the non-uniform diagonals are expected to match the most preferable weights of the forecasting tasks with varying future steps. To achieve this, we propose a Quadratic Direct Forecast (QDF) learning algorithm, which trains the forecast model using the adaptively updated quadratic-form weighting matrix. Experiments show that our QDF effectively improves performance of various forecast models, achieving state-of-the-art results. Code is available at https://anonymous.4open.science/r/QDF-8937.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation</title>
<link>https://arxiv.org/abs/2511.00054</link>
<guid>https://arxiv.org/abs/2511.00054</guid>
<content:encoded><![CDATA[
<div> SpatialTraceGen, VLMs, reasoning, dataset, tool use<br />
Summary:<br />
SpatialTraceGen introduces a framework to distill reasoning processes into high-quality datasets for spatial reasoning models. The automated Verifier ensures the fidelity of reasoning steps, improving trace quality and reducing variance. This dataset of expert traces offers structured, step-by-step examples necessary for fine-tuning and efficient reinforcement learning. By addressing the data-efficiency gap, SpatialTraceGen enhances the performance of Vision-Language Models in complex spatial reasoning tasks. <div>
arXiv:2511.00054v1 Announce Type: new 
Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with complex spatial reasoning, which requires problem decomposition and strategic tool use. Fine-tuning smaller, more deployable models offers an efficient path to strong performance, but this is hampered by a major bottleneck: the absence of high-quality, step-by-step reasoning data. To address this data-efficiency gap, we introduce SpatialTraceGen, a framework to distill the reasoning processes of a large teacher model into a high-quality dataset of multi-hop, multi-tool reasoning traces. A key innovation is our automated Verifier, which scalably ensures the fidelity of each reasoning step, providing a cost-effective alternative to manual human annotation. On the CLEVR-Humans benchmark, this verifier-guided process improves the average quality score of traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen delivers a dataset of expert traces, providing the structured, step-by-step examples of tool use necessary for effective fine-tuning and sample-efficient offline reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches</title>
<link>https://arxiv.org/abs/2511.00055</link>
<guid>https://arxiv.org/abs/2511.00055</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Machine Learning, Unmanned Aerial Vehicle, Thermal Images, Urban Environments

Summary:
This paper examines the implementation and effectiveness of Federated Learning (FL) in the context of detecting common thermal features in urban environments using unmanned aerial vehicle (UAV) thermal images. Real-world data from two German cities is utilized to assess various FL algorithms in comparison to centralized learning. Key metrics such as model accuracy, training time, communication overhead, and energy usage are evaluated to understand the performance of FL in segmentation tasks. The study investigates the challenges posed by non-identical data distributions and feature characteristics across different locations. Different FL workflows, including client-controlled and server-controlled approaches, are explored to determine their impact on performance. By providing insights into practical FL applications in UAV-based imaging tasks, this research contributes valuable knowledge for understanding the capabilities and limitations of FL methods in real-world scenarios. <br /><br />Summary: <div>
arXiv:2511.00055v1 Announce Type: new 
Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning (ML) model with distributed training data and multiple participants. FL allows bypassing limitations of the traditional Centralized Machine Learning CL if data cannot be shared or stored centrally due to privacy or technical restrictions -- the participants train the model locally with their training data and do not need to share it among the other participants. This paper investigates the practical implementation and effectiveness of FL in a real-world scenario, specifically focusing on unmanned aerial vehicle (UAV)-based thermal images for common thermal feature detection in urban environments. The distributed nature of the data arises naturally and makes it suitable for FL applications, as images captured in two German cities are available. This application presents unique challenges due to non-identical distribution and feature characteristics of data captured at both locations. The study makes several key contributions by evaluating FL algorithms in real deployment scenarios rather than simulation. We compare several FL approaches with a centralized learning baseline across key performance metrics such as model accuracy, training time, communication overhead, and energy usage. This paper also explores various FL workflows, comparing client-controlled workflows and server-controlled workflows. The findings of this work serve as a valuable reference for understanding the practical application and limitations of the FL methods in segmentation tasks in UAV-based imaging.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling</title>
<link>https://arxiv.org/abs/2511.00056</link>
<guid>https://arxiv.org/abs/2511.00056</guid>
<content:encoded><![CDATA[
<div> Memory-efficient optimization algorithms, large language models, layer-wise optimization, module-wise importance sampling, gradient variance reduction<br /><br />Summary:
The article introduces Module-wise Importance SAmpling (MISA) as a novel method for memory-efficient optimization of large language models. Unlike traditional layer-wise optimization, MISA divides each layer into smaller modules and assigns importance scores to optimize them effectively. By using a weighted random sampling mechanism, MISA reduces gradient variance and achieves an \(\mathcal{O}(1/\sqrt{K})\) convergence rate under non-convex and stochastic conditions. The method offers significant memory savings compared to existing baseline methods by activating only essential modules during optimization. Experimental validation across various learning tasks confirms the effectiveness of MISA in enhancing the performance of large language models. A detailed memory analysis highlights the superiority of MISA in terms of memory efficiency. The source code for MISA is available on GitHub for further exploration and implementation. <div>
arXiv:2511.00056v1 Announce Type: new 
Abstract: The substantial memory demands of pre-training and fine-tuning large language models (LLMs) require memory-efficient optimization algorithms. One promising approach is layer-wise optimization, which treats each transformer block as a single layer and optimizes it sequentially, while freezing the other layers to save optimizer states and activations. Although effective, these methods ignore the varying importance of the modules within each layer, leading to suboptimal performance. Moreover, layer-wise sampling provides only limited memory savings, as at least one full layer must remain active during optimization. To overcome these limitations, we propose Module-wise Importance SAmpling (MISA), a novel method that divides each layer into smaller modules and assigns importance scores to each module. MISA uses a weighted random sampling mechanism to activate modules, provably reducing gradient variance compared to layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\) convergence rate under non-convex and stochastic conditions, where $K$ is the total number of block updates, and provide a detailed memory analysis showcasing MISA's superiority over existing baseline methods. Experiments on diverse learning tasks validate the effectiveness of MISA. Source code is available at https://github.com/pkumelon/MISA.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatically Finding Rule-Based Neurons in OthelloGPT</title>
<link>https://arxiv.org/abs/2511.00059</link>
<guid>https://arxiv.org/abs/2511.00059</guid>
<content:encoded><![CDATA[
<div> interpretability, OthelloGPT, transformer, decision trees, neurons

Summary:
The article introduces OthelloGPT, a transformer model trained to predict valid moves in Othello, which serves as a testbed for interpretability research. The model's complex nature allows for rich computational patterns, with rule-based game logic enabling meaningful reverse-engineering. An automated approach using decision trees is presented to identify and interpret MLP neurons encoding game logic. Approximately half of the neurons in layer 5 can be accurately described by compact decision trees, while the rest likely engage in more distributed computations. Patterns identified by decision trees are verified to be causally relevant through interventions. Ablating neurons corresponding to specific game patterns significantly impairs the model's predictive ability. A Python tool mapping rule-based behaviors to neurons is provided for researchers to test interpretability methods effectively. <br /><br />Summary: <div>
arXiv:2511.00059v1 Announce Type: new 
Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides an ideal testbed for interpretability research. The model is complex enough to exhibit rich computational patterns, yet grounded in rule-based game logic that enables meaningful reverse-engineering. We present an automated approach based on decision trees to identify and interpret MLP neurons that encode rule-based game logic. Our method trains regression decision trees to map board states to neuron activations, then extracts decision paths where neurons are highly active to convert them into human-readable logical forms. These descriptions reveal highly interpretable patterns; for instance, neurons that specifically detect when diagonal moves become legal. Our findings suggest that roughly half of the neurons in layer 5 can be accurately described by compact, rule-based decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder likely participate in more distributed or non-rule-based computations. We verify the causal relevance of patterns identified by our decision trees through targeted interventions. For a specific square, for specific game patterns, we ablate neurons corresponding to those patterns and find an approximately 5-10 fold stronger degradation in the model's ability to predict legal moves along those patterns compared to control patterns. To facilitate future work, we provide a Python tool that maps rule-based game behaviors to their implementing neurons, serving as a resource for researchers to test whether their interpretability methods recover meaningful computational structures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics</title>
<link>https://arxiv.org/abs/2511.00064</link>
<guid>https://arxiv.org/abs/2511.00064</guid>
<content:encoded><![CDATA[
<div> graph construction algorithm, clustering, variance, nonparametric, density

Summary:<br />
The EVINGCA algorithm introduces a novel approach to clustering by incorporating density-variance based methods to adaptively construct clusters on a nearest-neighbor graph. Unlike traditional algorithms, EVINGCA does not make restrictive assumptions and instead considers the process of cluster formation as dynamic and evolving. By utilizing local statistical feedback and updating distance and shape statistics, the algorithm creates clusters without relying on fixed density thresholds. EVINGCA features log-linear complexity in the average case and demonstrates competitive performance across diverse datasets, including synthetic, real-world, low-dimensional, and high-dimensional ones. This approach provides a flexible and efficient way to cluster data, offering improved sensitivity and adaptability compared to existing methods. <div>
arXiv:2511.00064v1 Announce Type: new 
Abstract: Clustering algorithms often rely on restrictive assumptions: K-Means and Gaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and HDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA (Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a density-variance based clustering algorithm that treats cluster formation as an adaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted graphs via breadth-first search, guided by continuously updated local distance and shape statistics, replacing fixed density thresholds with local statistical feedback. With spatial indexing, EVINGCA features log-linear complexity in the average case and exhibits competitive performance against baselines across a variety of synthetic, real-world, low-d, and high-d datasets.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Brain Signals with Multimodal Speech and Vision Embeddings</title>
<link>https://arxiv.org/abs/2511.00065</link>
<guid>https://arxiv.org/abs/2511.00065</guid>
<content:encoded><![CDATA[
<div> Keywords: house, EEG signals, pre-trained models, wav2vec2, CLIP

Summary: 
The study investigates how the brain processes the concept of "house" beyond just auditory input and delves into the layered processing involved in building meaning. By aligning EEG signals with embeddings from pre-trained models wav2vec2 and CLIP, researchers sought to understand which model layers best reflect the brain's processing of language. Through ridge regression and contrastive decoding techniques, the study compared individual layers, progressive concatenation, and progressive summation strategies. The results indicate that combining multimodal, layer-aware representations from these models could aid in decoding how the brain comprehends language as a holistic experience rather than just sound. This approach offers insights into the complex neural mechanisms underlying language processing and perception. 

Summary: <div>
arXiv:2511.00065v1 Announce Type: new 
Abstract: When we hear the word "house", we don't just process sound, we imagine walls, doors, memories. The brain builds meaning through layers, moving from raw acoustics to rich, multimodal associations. Inspired by this, we build on recent work from Meta that aligned EEG signals with averaged wav2vec2 speech embeddings, and ask a deeper question: which layers of pre-trained models best reflect this layered processing in the brain? We compare embeddings from two models: wav2vec2, which encodes sound into language, and CLIP, which maps words to images. Using EEG recorded during natural speech perception, we evaluate how these embeddings align with brain activity using ridge regression and contrastive decoding. We test three strategies: individual layers, progressive concatenation, and progressive summation. The findings suggest that combining multimodal, layer-aware representations may bring us closer to decoding how the brain understands language, not just as sound, but as experience.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models</title>
<link>https://arxiv.org/abs/2511.00066</link>
<guid>https://arxiv.org/abs/2511.00066</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, verifiable rewards, language models, Group Relative Policy Optimization, token-level weights

Summary:
Token-Regulated Group Relative Policy Optimization (TR-GRPO) is introduced as an extension of GRPO to address the issue of low-probability tokens dominating gradient updates in reinforcement learning with verifiable rewards (RLVR). TR-GRPO assigns token-level weights based on the model's predicted probability, reducing the impact of low-probability tokens and amplifying high-probability tokens for more stable and informative training. Experimental results show that TR-GRPO outperforms GRPO across various reasoning tasks in LLMs, such as logic, math, and agentic reasoning. This highlights the significance of regulating token contributions in RL training and positions TR-GRPO as a robust framework for enhancing reasoning capabilities in large language models. 

<br /><br />Summary: Token-Regulated Group Relative Policy Optimization (TR-GRPO) addresses gradient over-amplification in RLVR by assigning token-level weights based on predicted probabilities. TR-GRPO outperforms GRPO in reasoning tasks, emphasizing the importance of token regulation in training large language models. <div>
arXiv:2511.00066v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a powerful approach for strengthening the reasoning capabilities of large language models (LLMs). Among existing algorithms, Group Relative Policy Optimization (GRPO) has demonstrated strong performance, yet it suffers from a critical issue: low-probability tokens disproportionately dominate gradient updates due to their inherently large gradient magnitudes. This imbalance leads to unstable training and suppresses the contribution of high-probability tokens that are more reliable for learning. In this work, we introduce Token-Regulated Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension of GRPO that assigns token-level weights positively correlated with the model's predicted probability. By downweighting low-probability tokens and emphasizing high-probability ones, TR-GRPO mitigates gradient over-amplification while preserving informative learning signals. Extensive experiments demonstrate that TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math, and agentic reasoning, highlighting the importance of regulating token contributions during RL training and establishing TR-GRPO as a robust framework for enhancing LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Domain Prompt Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.00067</link>
<guid>https://arxiv.org/abs/2511.00067</guid>
<content:encoded><![CDATA[
<div> Domain generalization, vision-language models, robustness, latent domains, knowledge transfer
Summary:
This study addresses domain generalization for vision-language models without explicit domain labels. They propose representing unseen domains as combinations of latent domains discovered from training data to enable adaptive knowledge transfer. They perform latent domain clustering on image features and fuse domain-specific text features based on similarity to each latent domain. Experiments on four benchmarks demonstrate consistent performance improvements over baseline models, providing insights into enhancing robustness against domain shift. <div>
arXiv:2511.00067v1 Announce Type: new 
Abstract: The objective of domain generalization (DG) is to enable models to be robust against domain shift. DG is crucial for deploying vision-language models (VLMs) in real-world applications, yet most existing methods rely on domain labels that may not be available and often ambiguous. We instead study the DG setting where models must generalize well without access to explicit domain labels. Our key idea is to represent an unseen target domain as a combination of latent domains automatically discovered from training data, enabling the model to adaptively transfer knowledge across domains. To realize this, we perform latent domain clustering on image features and fuse domain-specific text features based on the similarity between the input image and each latent domain. Experiments on four benchmarks show that this strategy yields consistent gains over VLM-based baselines and provides new insights into improving robustness under domain shift.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design</title>
<link>https://arxiv.org/abs/2511.00070</link>
<guid>https://arxiv.org/abs/2511.00070</guid>
<content:encoded><![CDATA[
<div> investigates, Large Language Models, constrained multi-objective regression tasks, inverse design, materials informatics

Summary:
This paper explores the performance of Large Language Models (LLMs) in solving constrained multi-objective regression tasks for inverse design in materials informatics. The study compares LLMs with Bayesian Optimization (BO) frameworks and fine-tuned LLMs and BERT models. The results show that while specialized BO frameworks like BoTorch qEHVI excel in convergence, fine-tuned LLMs, particularly WizardMath-7B, offer a promising, computationally efficient alternative. The best-performing LLM achieved a Generational Distance (GD) of 1.21, significantly outperforming the traditional BoTorch Ax baseline. This research contributes valuable insights to the field of AI-driven optimization and has practical industrial applications in optimizing formulation design for resins, polymers, and paints, where trade-offs between mechanical, rheological, and chemical properties play a crucial role in innovation and production efficiency.<br /><br />Summary: <div>
arXiv:2511.00070v1 Announce Type: new 
Abstract: This paper investigates the performance of Large Language Models (LLMs) as generative optimizers for solving constrained multi-objective regression tasks, specifically within the challenging domain of inverse design (property-to-structure mapping). This problem, critical to materials informatics, demands finding complex, feasible input vectors that lie on the Pareto optimal front. While LLMs have demonstrated universal effectiveness across generative and reasoning tasks, their utility in constrained, continuous, high-dimensional numerical spaces tasks they weren't explicitly architected for remains an open research question. We conducted a rigorous comparative study between established Bayesian Optimization (BO) frameworks and a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the foundational BoTorch Ax implementation against the state-of-the-art q-Expected Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the challenge as a regression problem with a custom output head. Our results show that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the performance ceiling. Crucially, the best-performing LLM (WizardMath-7B) achieved a Generational Distance (GD) of 1.21, significantly outperforming the traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO frameworks remain the performance leader for guaranteed convergence, but fine-tuned LLMs are validated as a promising, computationally fast alternative, contributing essential comparative metrics to the field of AI-driven optimization. The findings have direct industrial applications in optimizing formulation design for resins, polymers, and paints, where multi-objective trade-offs between mechanical, rheological, and chemical properties are critical to innovation and production efficiency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective</title>
<link>https://arxiv.org/abs/2511.00071</link>
<guid>https://arxiv.org/abs/2511.00071</guid>
<content:encoded><![CDATA[
<div> wavelet-based feature extraction, unsupervised clustering, parity detection, classification accuracy, symbolic reasoning
<br />
Summary: 
This paper presents an unconventional approach to the problem of parity detection, leveraging wavelet-based feature extraction and unsupervised clustering techniques. By transforming integers into wavelet-domain representations and extracting multi-scale statistical features, the study achieved a classification accuracy of approximately 69.67% in determining whether a number is odd or even. The results highlight the potential of classical signal-processing techniques in uncovering structural differences between odd and even numbers without the need for label supervision. The research demonstrates how feature engineering and clustering can be repurposed for unconventional machine learning tasks, offering a perspective on bridging symbolic reasoning with feature-based learning. Overall, the study showcases the ability of these techniques to reveal latent structure in purely discrete symbolic domains, providing insights into the broader applicability of such approaches beyond traditional signal processing tasks. 
<br /><br />Summary: <div>
arXiv:2511.00071v1 Announce Type: new 
Abstract: This paper explores a deliberately over-engineered approach to the classical problem of parity detection -- determining whether a number is odd or even -- by combining wavelet-based feature extraction with unsupervised clustering. Instead of relying on modular arithmetic, integers are transformed into wavelet-domain representations, from which multi-scale statistical features are extracted and clustered using the k-means algorithm. The resulting feature space reveals meaningful structural differences between odd and even numbers, achieving a classification accuracy of approximately 69.67% without any label supervision. These results suggest that classical signal-processing techniques, originally designed for continuous data, can uncover latent structure even in purely discrete symbolic domains. Beyond parity detection, the study provides an illustrative perspective on how feature engineering and clustering may be repurposed for unconventional machine learning problems, potentially bridging symbolic reasoning and feature-based learning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with B\'ezier Curves</title>
<link>https://arxiv.org/abs/2511.00076</link>
<guid>https://arxiv.org/abs/2511.00076</guid>
<content:encoded><![CDATA[
<div> visual-language models, geometric structure, pictographic characters, program synthesis, Bezier curves

Summary:
This study explores the capability of Vision-language Models (VLMs) to understand the geometric structure of visual information through the recognition of pictographic characters represented as executable programs of geometric primitives. The model, acting as a "visual decompiler", outperforms strong zero-shot baselines like GPT-4o by decompiling raster images into programs composed of Bezier curves. The significant finding is the model's ability to reconstruct ancient Oracle Bone Script in a zero-shot context when trained solely on modern Chinese characters, indicating a transferable geometric grammar acquisition. This demonstrates a move towards more structured visual understanding beyond pixel-level pattern recognition. <div>
arXiv:2511.00076v1 Announce Type: new 
Abstract: While Vision-language Models (VLMs) have demonstrated strong semantic capabilities, their ability to interpret the underlying geometric structure of visual information is less explored. Pictographic characters, which combine visual form with symbolic structure, provide an ideal test case for this capability. We formulate this visual recognition challenge in the mathematical domain, where each character is represented by an executable program of geometric primitives. This is framed as a program synthesis task, training a VLM to decompile raster images into programs composed of B\'ezier curves. Our model, acting as a "visual decompiler", demonstrates performance superior to strong zero-shot baselines, including GPT-4o. The most significant finding is that when trained solely on modern Chinese characters, the model is able to reconstruct ancient Oracle Bone Script in a zero-shot context. This generalization provides strong evidence that the model acquires an abstract and transferable geometric grammar, moving beyond pixel-level pattern recognition to a more structured form of visual understanding.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R</title>
<link>https://arxiv.org/abs/2511.00079</link>
<guid>https://arxiv.org/abs/2511.00079</guid>
<content:encoded><![CDATA[
<div> Keywords: flowengineR, algorithmic workflows, reproducibility, transparency, extensibility

Summary: 
flowengineR is a new R package that aims to provide a flexible framework for creating reproducible algorithmic workflows, particularly in the field of machine learning and algorithmic fairness. It emphasizes the importance of transparency and extensibility by introducing a modular architecture of standardized engines for various tasks in the modeling pipeline. Each engine serves a specific function but communicates through a lightweight interface, allowing for easy integration and comparison of interventions. By structuring fairness methods as interchangeable engines, researchers can evaluate and compare different strategies while maintaining transparency and reproducibility. The architecture can be applied not only to fairness but also to other workflow contexts where reproducibility, transparency, and extensibility are crucial. Overall, flowengineR provides a general infrastructure for building complex machine learning pipelines while prioritizing reproducibility and transparency. 

<br /><br />Summary: <div>
arXiv:2511.00079v1 Announce Type: new 
Abstract: flowengineR is an R package designed to provide a modular and extensible framework for building reproducible algorithmic workflows for general-purpose machine learning pipelines. It is motivated by the rapidly evolving field of algorithmic fairness where new metrics, mitigation strategies, and machine learning methods continuously emerge. A central challenge in fairness, but also far beyond, is that existing toolkits either focus narrowly on single interventions or treat reproducibility and extensibility as secondary considerations rather than core design principles. flowengineR addresses this by introducing a unified architecture of standardized engines for data splitting, execution, preprocessing, training, inprocessing, postprocessing, evaluation, and reporting. Each engine encapsulates one methodological task yet communicates via a lightweight interface, ensuring workflows remain transparent, auditable, and easily extensible. Although implemented in R, flowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented visual programming languages (KNIME), and R frameworks (BatchJobs, batchtools). Its emphasis, however, is less on orchestrating engines for resilient parallel execution but rather on the straightforward setup and management of distinct engines as data structures. This orthogonalization enables distributed responsibilities, independent development, and streamlined integration. In fairness context, by structuring fairness methods as interchangeable engines, flowengineR lets researchers integrate, compare, and evaluate interventions across the modeling pipeline. At the same time, the architecture generalizes to explainability, robustness, and compliance metrics without core modifications. While motivated by fairness, it ultimately provides a general infrastructure for any workflow context where reproducibility, transparency, and extensibility are essential.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fixed-point graph convolutional networks against adversarial attacks</title>
<link>https://arxiv.org/abs/2511.00083</link>
<guid>https://arxiv.org/abs/2511.00083</guid>
<content:encoded><![CDATA[
<div> Adversarial attacks, graph neural networks, robustness, fixed-point iterative graph convolutional network, spectral modulation filter <br />
<br />
Summary: 
The paper introduces the Fixed-point iterative graph convolutional network (Fix-GCN) model, designed to enhance the robustness of graph neural networks against adversarial attacks. By effectively capturing higher-order node neighborhood information without adding complexity, the model utilizes a versatile spectral modulation filter and a feature propagation rule derived from fixed-point iteration. Unlike traditional defense mechanisms, Fix-GCN's filter allows selective attenuation of high-frequency components while preserving low-frequency structural information, offering a flexible and efficient framework. Through iterative node representation updates, the model effectively preserves essential graph information while mitigating adversarial manipulation. Extensive experiments on benchmark graph datasets demonstrate the model's resilience against attacks, showcasing its effectiveness in enhancing the integrity and performance of graph neural networks. <br /> <div>
arXiv:2511.00083v1 Announce Type: new 
Abstract: Adversarial attacks present a significant risk to the integrity and performance of graph neural networks, particularly in tasks where graph structure and node features are vulnerable to manipulation. In this paper, we present a novel model, called fixed-point iterative graph convolutional network (Fix-GCN), which achieves robustness against adversarial perturbations by effectively capturing higher-order node neighborhood information in the graph without additional memory or computational complexity. Specifically, we introduce a versatile spectral modulation filter and derive the feature propagation rule of our model using fixed-point iteration. Unlike traditional defense mechanisms that rely on additional design elements to counteract attacks, the proposed graph filter provides a flexible-pass filtering approach, allowing it to selectively attenuate high-frequency components while preserving low-frequency structural information in the graph signal. By iteratively updating node representations, our model offers a flexible and efficient framework for preserving essential graph information while mitigating the impact of adversarial manipulation. We demonstrate the effectiveness of the proposed model through extensive experiments on various benchmark graph datasets, showcasing its resilience against adversarial attacks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of predictive machine learning in pen &amp; paper RPG game design</title>
<link>https://arxiv.org/abs/2511.00084</link>
<guid>https://arxiv.org/abs/2511.00084</guid>
<content:encoded><![CDATA[
<div> Keywords: pen and paper RPG market, AI technologies, monster level estimation, ordinal regression techniques, dataset construction

Summary: 
This study focuses on the growing trend of integrating AI technologies in the pen and paper RPG market to enhance player experience and gain a competitive edge. The main challenge faced by publishers is accurately determining the level of new opponents in the game. Currently, manual testing and expert evaluation are the primary methods used for estimating monster levels, which are time-consuming and resource-intensive. The thesis evaluates state-of-the-art methods for level prediction using ordinal regression techniques and introduces a dedicated dataset for this task. A human-inspired model is developed as a benchmark for comparison with machine learning algorithms. A specialized evaluation procedure, based on domain knowledge, is designed to assess model performance and facilitate meaningful comparisons. Overall, this study provides insights into leveraging AI technologies for enhancing player experience in the pen and paper RPG market. 

<br /><br />Summary: <div>
arXiv:2511.00084v1 Announce Type: new 
Abstract: In recent years, the pen and paper RPG market has experienced significant growth. As a result, companies are increasingly exploring the integration of AI technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and estimating their challenge level. Currently, there are no automated methods for determining a monster's level; the only approaches used are based on manual testing and expert evaluation. Although these manual methods can provide reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This thesis presents an overview and evaluation of state-of-the-art methods for this task. It also details the construction of a dedicated dataset for level estimation. Furthermore, a human-inspired model was developed to serve as a benchmark, allowing comparison between machine learning algorithms and the approach typically employed by pen and paper RPG publishers. In addition, a specialized evaluation procedure, grounded in domain knowledge, was designed to assess model performance and facilitate meaningful comparisons.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning</title>
<link>https://arxiv.org/abs/2511.00085</link>
<guid>https://arxiv.org/abs/2511.00085</guid>
<content:encoded><![CDATA[
<div> Keywords: stock trend prediction, MaGNet, dual-hyperGraph Network, temporal modeling, relational reasoning

Summary:
MaGNet is a novel dual-hyperGraph Network designed for stock trend prediction, addressing challenges such as temporal dependencies and dynamic inter-stock interactions. It introduces innovative features such as the MAGE block for contextual temporal modeling, multi-head attention for capturing global dependencies, and Feature-wise and Stock-wise 2D Spatiotemporal Attention modules for precise fusion of multivariate features. MaGNet also employs a dual hypergraph framework consisting of the Temporal-Causal Hypergraph and Global Probabilistic Hypergraph to capture fine-grained temporal dependencies and market-wide patterns. Experimental results on six major stock indices show that MaGNet outperforms existing methods in predictive performance and investment returns while also offering robust risk management capabilities. The code for MaGNet is available on GitHub for further exploration and use. 

<br /><br />Summary: <div>
arXiv:2511.00085v1 Announce Type: new 
Abstract: Stock trend prediction is crucial for profitable trading strategies and portfolio management yet remains challenging due to market volatility, complex temporal dynamics and multifaceted inter-stock relationships. Existing methods struggle to effectively capture temporal dependencies and dynamic inter-stock interactions, often neglecting cross-sectional market influences, relying on static correlations, employing uniform treatments of nodes and edges, and conflating diverse relationships. This work introduces MaGNet, a novel Mamba dual-hyperGraph Network for stock prediction, integrating three key innovations: (1) a MAGE block, which leverages bidirectional Mamba with adaptive gating mechanisms for contextual temporal modeling and integrates a sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market conditions, alongside multi-head attention for capturing global dependencies; (2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable precise fusion of multivariate features and cross-stock dependencies, effectively enhancing informativeness while preserving intrinsic data structures, bridging temporal modeling with relational reasoning; and (3) a dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH) that captures fine-grained causal dependencies with temporal constraints, and Global Probabilistic Hypergraph (GPH) that models market-wide patterns through soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism, jointly disentangling localized temporal influences from instantaneous global structures for multi-scale relational learning. Extensive experiments on six major stock indices demonstrate MaGNet outperforms state-of-the-art methods in both superior predictive performance and exceptional investment returns with robust risk management capabilities. Codes available at: https://github.com/PeilinTime/MaGNet.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph</title>
<link>https://arxiv.org/abs/2511.00086</link>
<guid>https://arxiv.org/abs/2511.00086</guid>
<content:encoded><![CDATA[
<div> optimization, large language models, test-time scaling, collaboration graphs, Agent-REINFORCE  
Summary:  
Test-Time Scaling (TTS) aims to improve large language models (LLMs) by allocating additional computation during inference. This study addresses the challenge of finding compute-optimal model combinations and architectures in TTS within a fixed budget. The problem is formulated as a multi-LLM collaboration graph, where nodes represent roles and model assignments, and edges capture information flow. The researchers propose Agent-REINFORCE, a framework that leverages LLM agents to efficiently search for optimal collaboration graphs by updating a probabilistic graph based on feedback. Experiment results demonstrate that Agent-REINFORCE outperforms traditional and LLM-based approaches in sample efficiency and search performance, efficiently identifying optimal graphs that balance accuracy and inference latency. <div>
arXiv:2511.00086v1 Announce Type: new 
Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation</title>
<link>https://arxiv.org/abs/2511.00097</link>
<guid>https://arxiv.org/abs/2511.00097</guid>
<content:encoded><![CDATA[
<div> Graph Domain-Incremental Learning, Knowledge Disentanglement, Knowledge Preservation, Catastrophic Forgetting, Graph Foundation Models <br />
Summary: <br />
The paper introduces GraphKeeper, a novel approach for Graph Domain-Incremental Learning (Domain-IL) that addresses catastrophic forgetting in scenarios involving multiple graph domains. GraphKeeper focuses on preventing embedding shifts and confusion across incremental graph domains by incorporating domain-specific parameter-efficient fine-tuning and disentanglement objectives. Additionally, it introduces deviation-free knowledge preservation to maintain a stable decision boundary between domains. For graphs with unobservable domains, GraphKeeper employs domain-aware distribution discrimination to obtain accurate embeddings. Experimental results demonstrate that GraphKeeper achieves state-of-the-art performance with significant improvement over existing methods while mitigating forgetting. The approach is also shown to be easily integrated with various graph foundation models, showcasing its broad applicability. <div>
arXiv:2511.00097v1 Announce Type: new 
Abstract: Graph incremental learning (GIL), which continuously updates graph models by sequential knowledge acquisition, has garnered significant interest recently. However, existing GIL approaches focus on task-incremental and class-incremental scenarios within a single domain. Graph domain-incremental learning (Domain-IL), aiming at updating models across multiple graph domains, has become critical with the development of graph foundation models (GFMs), but remains unexplored in the literature. In this paper, we propose Graph Domain-Incremental Learning via Knowledge Dientanglement and Preservation (GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from the perspectives of embedding shifts and decision boundary deviations. Specifically, to prevent embedding shifts and confusion across incremental graph domains, we first propose the domain-specific parameter-efficient fine-tuning together with intra- and inter-domain disentanglement objectives. Consequently, to maintain a stable decision boundary, we introduce deviation-free knowledge preservation to continuously fit incremental domains. Additionally, for graphs with unobservable domains, we perform domain-aware distribution discrimination to obtain precise embeddings. Extensive experiments demonstrate the proposed GraphKeeper achieves state-of-the-art results with 6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover, we show GraphKeeper can be seamlessly integrated with various representative GFMs, highlighting its broad applicative potential.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation</title>
<link>https://arxiv.org/abs/2511.00099</link>
<guid>https://arxiv.org/abs/2511.00099</guid>
<content:encoded><![CDATA[
<div> conditional-labeled generative adversarial network, damage detection, digital twinning, unsupervised framework, structural health monitoring

Summary: 
The study focuses on a novel methodology for damage detection and digital twinning using a conditional-labeled generative adversarial network. This approach does not require prior information about the health state of the system, making it highly applicable in real-world scenarios. By utilizing unsupervised framework, the method was tested on Z24 Bridge structural health monitoring data and showed improved performance in fault anomaly detection. The model generated measurements for digital twinning at different damage states without the need for physics knowledge. Additionally, a support vector machine classifier and principal component analysis were developed to assess the generated measurements and provide a secondary indicator for identifying damage scenarios. The approach accurately captured damage over healthy measurements, making it a valuable tool for vibration-based system monitoring and infrastructure resilience at scale. <br /><br />Summary: <div>
arXiv:2511.00099v1 Announce Type: new 
Abstract: The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification</title>
<link>https://arxiv.org/abs/2511.00100</link>
<guid>https://arxiv.org/abs/2511.00100</guid>
<content:encoded><![CDATA[
<div> Keywords: structural load identification, neural networks, residual Kalman filter, civil engineering applications, structural health monitoring

Summary:
The study examines the dynamic structural load identification capabilities of gated recurrent unit, long short-term memory, and convolutional neural networks in realistic small dataset training conditions. The comparison is made with the physics-based residual Kalman filter (RKF). The research analyzes a simulated structure under shaker excitation, a building in California under seismic base excitation, and the IASC-ASCE structural health monitoring benchmark problem for impact and instant loading conditions. The methods show varying performance based on different loading scenarios, with the RKF outperforming the networks in cases where the structural model is identifiable. This research provides insights into the effectiveness of different methods for structural load identification in civil engineering applications. 

<br /><br />Summary: <div>
arXiv:2511.00100v1 Announce Type: new 
Abstract: The dynamic structural load identification capabilities of the gated recurrent unit, long short-term memory, and convolutional neural networks are examined herein. The examination is on realistic small dataset training conditions and on a comparative view to the physics-based residual Kalman filter (RKF). The dynamic load identification suffers from the uncertainty related to obtaining poor predictions when in civil engineering applications only a low number of tests are performed or are available, or when the structural model is unidentifiable. In considering the methods, first, a simulated structure is investigated under a shaker excitation at the top floor. Second, a building in California is investigated under seismic base excitation, which results in loading for all degrees of freedom. Finally, the International Association for Structural Control-American Society of Civil Engineers (IASC-ASCE) structural health monitoring benchmark problem is examined for impact and instant loading conditions. Importantly, the methods are shown to outperform each other on different loading scenarios, while the RKF is shown to outperform the networks in physically parametrized identifiable cases.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving</title>
<link>https://arxiv.org/abs/2511.00101</link>
<guid>https://arxiv.org/abs/2511.00101</guid>
<content:encoded><![CDATA[
<div> Virtualized Module, PEFT, LoRA, fine-tuning, inference<br />
Summary:
Loquetier is a virtualized multi-LoRA framework that combines fine-tuning and serving for large language models (LLMs) in a single runtime. It introduces a Virtualized Module to isolate parameter-efficient fine-tuning (PEFT) modifications and support multiple adapters on a shared base model. Additionally, Loquetier optimizes the computation flow with a kernel design that merges fine-tuning and inference paths, allowing for efficient batching and reducing kernel invocation overhead. Experimental results across three task settings demonstrate that Loquetier outperforms existing baselines in both performance and flexibility. It achieves significantly higher throughput on inference-only tasks compared to state-of-the-art co-serving systems and demonstrates improved SLO attainment on unified fine-tuning and inference tasks compared to PEFT. The implementation of Loquetier is publicly available on GitHub at https://github.com/NJUDeepEngine/Loquetier. <br /><br />Summary: <div>
arXiv:2511.00101v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient fine-tuning (PEFT) technique for adapting large language models (LLMs) to downstream tasks. While prior work has explored strategies for integrating LLM training and serving, there still remains a gap in unifying fine-tuning and inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA framework that seamlessly integrates LoRA fine-tuning and serving within a single runtime. Loquetier introduces two key components: (1) a Virtualized Module that isolates PEFT-based modifications and supports multiple adapters on a shared base model, and (2) an optimized computation flow with a kernel design that merges fine-tuning and inference paths in forward propagation, enabling efficient batching and minimizing kernel invocation overhead. Extensive experiments across three task settings show that Loquetier consistently outperforms existing baselines in both performance and flexibility, achieving up to $3.0\times$ the throughput of the state-of-the-art co-serving system on inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on unified fine-tuning and inference tasks. The implementation of Loquetier is publicly available at https://github.com/NJUDeepEngine/Loquetier.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers</title>
<link>https://arxiv.org/abs/2511.00102</link>
<guid>https://arxiv.org/abs/2511.00102</guid>
<content:encoded><![CDATA[
<div> Keywords: conservation laws, Neural Ordinary Differential Equation, Transformer, symbolic-numeric verifier, mathematical principles<br />
Summary: 
The article introduces a hybrid framework to automate the discovery of conservation laws from noisy trajectory data. The framework consists of three components: a Neural Ordinary Differential Equation (Neural ODE) that learns the system's dynamics, a Transformer that generates symbolic candidate invariants based on the learned vector field, and a symbolic-numeric verifier that confirms the validity of these candidates. Testing on physical systems shows the framework outperforms baselines that work directly on trajectory data. This approach highlights the effectiveness of a learn-then-search strategy for identifying mathematical principles from imperfect data.<br /><br /> <div>
arXiv:2511.00102v1 Announce Type: new 
Abstract: The discovery of conservation laws is a cornerstone of scientific progress. However, identifying these invariants from observational data remains a significant challenge. We propose a hybrid framework to automate the discovery of conserved quantities from noisy trajectory data. Our approach integrates three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that learns a continuous model of the system's dynamics, (2) a Transformer that generates symbolic candidate invariants conditioned on the learned vector field, and (3) a symbolic-numeric verifier that provides a strong numerical certificate for the validity of these candidates. We test our framework on canonical physical systems and show that it significantly outperforms baselines that operate directly on trajectory data. This work demonstrates the robustness of a decoupled learn-then-search approach for discovering mathematical principles from imperfect data.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence</title>
<link>https://arxiv.org/abs/2511.00108</link>
<guid>https://arxiv.org/abs/2511.00108</guid>
<content:encoded><![CDATA[
<div> largest-scale, open-source, embodied brain model, multimodal, training

Summary:<br />
The report introduces Pelican-VL 1.0, an open-source embodied brain model with a parameter scale ranging from 7 billion to 72 billion. The model's key advantage is its integration of data power and intelligent adaptive learning mechanisms, achieved through the meta-loop process. Pelican-VL 1.0 was trained on a large-scale cluster of over 1000 A800 GPUs, with a significant performance improvement of 20.3% over its base model. It outperforms other 100B-level open-source models by 10.6%, placing it on par with proprietary systems in various benchmarks. The model is trained using a novel framework called DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition. This framework involves a metaloop that teaches the AI to practice deliberately through a RL-Refine-Diagnose-SFT loop. <div>
arXiv:2511.00108v1 Announce Type: new 
Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied brain models with parameter scales ranging from 7 billion to 72 billion. Our explicit mission is clearly stated as: To embed powerful intelligence into various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source embodied multimodal brain model. Its core advantage lies in the in-depth integration of data power and intelligent adaptive learning mechanisms. Specifically, metaloop distilled a high-quality dataset from a raw dataset containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint. This translates to a 20.3% performance uplift from its base model and outperforms 100B-level open-source counterparts by 10.6%, placing it on par with leading proprietary systems on well-known embodied benchmarks. We establish a novel framework, DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition to train Pelican-VL 1.0. We operationalize this as a metaloop that teaches the AI to practice deliberately, which is a RL-Refine-Diagnose-SFT loop.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials</title>
<link>https://arxiv.org/abs/2511.00113</link>
<guid>https://arxiv.org/abs/2511.00113</guid>
<content:encoded><![CDATA[
<div> Keywords: Spectral Graph Neural Networks, Meixner polynomials, GNN architecture, Laplacian scaling, LayerNorm

Summary: 
MeixnerNet introduces a novel spectral Graph Neural Network architecture that utilizes discrete Meixner polynomials. The model's key innovation is the ability to learn the shape parameters of the polynomials, beta, and c, making the filter adaptable to the graph's specific spectral properties. A novel stabilization technique is introduced to address numerical instability. Experimental results show MeixnerNet outperforms ChebyNet at the optimal setting and exhibits exceptional robustness to variations in polynomial degree K. ChebyNet, on the other hand, is highly sensitive to changes in K, leading to a significant performance drop. MeixnerNet's adaptability and stability make it a promising alternative to traditional spectral GNN architectures.<br /><br />Summary: <div>
arXiv:2511.00113v1 Announce Type: new 
Abstract: Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results by defining graph convolutions in the spectral domain. A common approach, popularized by ChebyNet, is to use polynomial filters based on continuous orthogonal polynomials (e.g., Chebyshev). This creates a theoretical disconnect, as these continuous-domain filters are applied to inherently discrete graph structures. We hypothesize this mismatch can lead to suboptimal performance and fragility to hyperparameter settings.
  In this paper, we introduce MeixnerNet, a novel spectral GNN architecture that employs discrete orthogonal polynomials -- specifically, the Meixner polynomials $M_k(x; \beta, c)$. Our model makes the two key shape parameters of the polynomial, beta and c, learnable, allowing the filter to adapt its polynomial basis to the specific spectral properties of a given graph. We overcome the significant numerical instability of these polynomials by introducing a novel stabilization technique that combines Laplacian scaling with per-basis LayerNorm.
  We demonstrate experimentally that MeixnerNet achieves competitive-to-superior performance against the strong ChebyNet baseline at the optimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we show that MeixnerNet is exceptionally robust to variations in the polynomial degree K, a hyperparameter to which ChebyNet proves to be highly fragile, collapsing in performance where MeixnerNet remains stable.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers</title>
<link>https://arxiv.org/abs/2511.00116</link>
<guid>https://arxiv.org/abs/2511.00116</guid>
<content:encoded><![CDATA[
<div> benchmark environment, reinforcement learning, energy-efficient liquid cooling, high-performance computing, multi-agent

Summary:<br /><br />Liquid cooling is essential for managing the thermal challenges in high-density data centers, especially with increasing AI workloads. To enhance energy efficiency and reliability, machine learning-based controllers play a vital role. LC-Opt, a Sustainable Liquid Cooling (LC) benchmark environment, has been developed for implementing reinforcement learning (RL) control strategies in energy-efficient liquid cooling systems for high-performance computing (HPC). This platform utilizes detailed end-to-end models, including site-level cooling towers, data center cabinets, and server blade groups. RL agents optimize key thermal controls such as liquid supply temperature and flow rate at the IT cabinet level, and cooling tower setpoints, facilitating dynamic workload changes. The environment presents a real-time optimization challenge that balances local thermal regulation and global energy efficiency, supporting the integration of additional components like a heat recovery unit (HRU). LC-Opt allows for the development of sustainable liquid cooling control solutions by providing access to detailed and customizable models for the ML community, operators, and vendors. <div>
arXiv:2511.00116v1 Announce Type: new 
Abstract: Liquid cooling is critical for thermal management in high-density data centers with the rising AI workloads. However, machine learning-based controllers are essential to unlock greater energy efficiency and reliability, promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC) benchmark environment, for reinforcement learning (RL) control strategies in energy-efficient liquid cooling of high-performance computing (HPC) systems. Built on the baseline of a high-fidelity digital twin of Oak Ridge National Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed Modelica-based end-to-end models spanning site-level cooling towers to data center cabinets and server blade groups. RL agents optimize critical thermal controls like liquid supply temperature, flow rate, and granular valve actuation at the IT cabinet level, as well as cooling tower (CT) setpoints through a Gymnasium interface, with dynamic changes in workloads. This environment creates a multi-objective real-time optimization challenge balancing local thermal regulation and global energy efficiency, and also supports additional components like a heat recovery unit (HRU). We benchmark centralized and decentralized multi-agent RL approaches, demonstrate policy distillation into decision and regression trees for interpretable control, and explore LLM-based methods that explain control actions in natural language through an agentic mesh architecture designed to foster user trust and simplify system management. LC-Opt democratizes access to detailed, customizable liquid cooling models, enabling the ML community, operators, and vendors to develop sustainable data center liquid cooling control solutions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads</title>
<link>https://arxiv.org/abs/2511.00117</link>
<guid>https://arxiv.org/abs/2511.00117</guid>
<content:encoded><![CDATA[
<div> Keywords: AI workload management, sustainable computing, data center physics, geo-distributed network dynamics, DCcluster-Opt<br />
<br />
Summary: 
The article introduces DCcluster-Opt, a simulation benchmark designed to address the challenges of managing large-scale AI workloads in globally distributed data centers. It combines real-world datasets and physics-informed models to create a realistic and challenging scheduling problem for task allocation across data centers. The benchmark incorporates environmental factors such as grid carbon intensity, electricity prices, and weather conditions across 20 regions, as well as network dynamics and data center operations. A modular reward system allows for the study of trade-offs among carbon emissions, energy costs, service level agreements, and water use. DCcluster-Opt provides a Gymnasium API with baseline controllers, including reinforcement learning and rule-based strategies, to support reproducible research and algorithm comparison. By offering a configurable and accessible testbed, DCcluster-Opt accelerates the development of sustainable computing solutions for geo-distributed data centers.<br /> <div>
arXiv:2511.00117v1 Announce Type: new 
Abstract: The increasing energy demands and carbon footprint of large-scale AI require intelligent workload management in globally distributed data centers. Yet progress is limited by the absence of benchmarks that realistically capture the interplay of time-varying environmental factors (grid carbon intensity, electricity prices, weather), detailed data center physics (CPUs, GPUs, memory, HVAC energy), and geo-distributed network dynamics (latency and transmission costs). To bridge this gap, we present DCcluster-Opt: an open-source, high-fidelity simulation benchmark for sustainable, geo-temporal task scheduling. DCcluster-Opt combines curated real-world datasets, including AI workload traces, grid carbon intensity, electricity markets, weather across 20 global regions, cloud transmission costs, and empirical network delay parameters with physics-informed models of data center operations, enabling rigorous and reproducible research in sustainable computing. It presents a challenging scheduling problem where a top-level coordinating agent must dynamically reassign or defer tasks that arrive with resource and service-level agreement requirements across a configurable cluster of data centers to optimize multiple objectives. The environment also models advanced components such as heat recovery. A modular reward system enables an explicit study of trade-offs among carbon emissions, energy costs, service level agreements, and water use. It provides a Gymnasium API with baseline controllers, including reinforcement learning and rule-based strategies, to support reproducible ML research and a fair comparison of diverse algorithms. By offering a realistic, configurable, and accessible testbed, DCcluster-Opt accelerates the development and validation of next-generation sustainable computing solutions for geo-distributed data centers.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analysis of Line Break prediction models for detecting defensive breakthrough in football</title>
<link>https://arxiv.org/abs/2511.00121</link>
<guid>https://arxiv.org/abs/2511.00121</guid>
<content:encoded><![CDATA[
<div> machine learning, J1 League, Line Breaks, XGBoost, SHAP analysis
Summary:
The study focuses on predicting Line Breaks in football, using event and tracking data from the 2023 J1 League season. A machine learning model with 189 features, including player positions and velocities, was developed, achieving high predictive accuracy. Factors such as player speed, defensive line gaps, and offensive players' spatial distributions were found to significantly influence Line Breaks. The model also showed a correlation between Line Breaks and shots and crosses conceded at the team level, highlighting the connection between Line Breaks and creating scoring opportunities. This research provides valuable insights into the tactical dynamics of football and emphasizes the importance of Line Breaks in offensive effectiveness. 
<br /><br />Summary: <div>
arXiv:2511.00121v1 Announce Type: new 
Abstract: In football, attacking teams attempt to break through the opponent's defensive line to create scoring opportunities. This action, known as a Line Break, is a critical indicator of offensive effectiveness and tactical performance, yet previous studies have mainly focused on shots or goal opportunities rather than on how teams break the defensive line. In this study, we develop a machine learning model to predict Line Breaks using event and tracking data from the 2023 J1 League season. The model incorporates 189 features, including player positions, velocities, and spatial configurations, and employs an XGBoost classifier to estimate the probability of Line Breaks. The proposed model achieved high predictive accuracy, with an AUC of 0.982 and a Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such as offensive player speed, gaps in the defensive line, and offensive players' spatial distributions significantly contribute to the occurrence of Line Breaks. Finally, we found a moderate positive correlation between the predicted probability of being Line-Broken and the number of shots and crosses conceded at the team level. These results suggest that Line Breaks are closely linked to the creation of scoring opportunities and provide a quantitative framework for understanding tactical dynamics in football.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models</title>
<link>https://arxiv.org/abs/2511.00124</link>
<guid>https://arxiv.org/abs/2511.00124</guid>
<content:encoded><![CDATA[
<div> sampling dynamics, score-based diffusion models, cross-fluctuations, transitions, generative modeling
Summary: 
The article explores the evolution of sampling dynamics in score-based diffusion models using cross-fluctuations. It demonstrates that starting from an unbiased normal distribution, samples go through discrete transitions to form distinct events of a desired distribution. These transitions are reversible, leading back to the initial distribution. Detecting these transitions through n-th order cross-fluctuations improves sampling efficiency, class-conditional and rare-class generation, as well as tasks like image classification and style transfer. The framework unifies classical coupling and mixing from finite Markov chains with continuous dynamics, extending to stochastic SDEs and non-Markovian samplers. The study bridges discrete Markov chain theory, phase analysis, and modern generative modeling. <div>
arXiv:2511.00124v1 Announce Type: new 
Abstract: We analyse how the sampling dynamics of distributions evolve in score-based diffusion models using cross-fluctuations, a centered-moment statistic from statistical physics. Specifically, we show that starting from an unbiased isotropic normal distribution, samples undergo sharp, discrete transitions, eventually forming distinct events of a desired distribution while progressively revealing finer structure. As this process is reversible, these transitions also occur in reverse, where intermediate states progressively merge, tracing a path back to the initial distribution. We demonstrate that these transitions can be detected as discontinuities in $n^{\text{th}}$-order cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for these cross-fluctuations that is efficiently computable for the reverse trajectory. We find that detecting these transitions directly boosts sampling efficiency, accelerates class-conditional and rare-class generation, and improves two zero-shot tasks--image classification and style transfer--without expensive grid search or retraining. We also show that this viewpoint unifies classical coupling and mixing from finite Markov chains with continuous dynamics while extending to stochastic SDEs and non Markovian samplers. Our framework therefore bridges discrete Markov chain theory, phase analysis, and modern generative modeling.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features</title>
<link>https://arxiv.org/abs/2511.00126</link>
<guid>https://arxiv.org/abs/2511.00126</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic multi-expert gating framework, trajectory predictor, internal model signals, Final Displacement Error, autonomous driving<br />
<br />
Summary: <br />
The article introduces a dynamic multi-expert gating framework for trajectory prediction in autonomous driving, addressing the limitations of traditional one-model-fits-all approaches. By adaptively selecting the most reliable trajectory predictor among different models based on internal model signals, the proposed method outperforms advanced networks in complex driving scenarios. The framework leverages meta-features like stability and uncertainty for expert selection, improving decision quality without post-hoc calibration. Evaluation on the nuPlan-mini dataset shows a significant reduction in Final Displacement Error compared to existing models, demonstrating consistent improvements in both offline validation and open-loop simulations. This approach enhances trajectory reliability in safety-critical driving situations, offering a practical alternative to static single-model paradigms. <div>
arXiv:2511.00126v1 Announce Type: new 
Abstract: Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al., 2022) have achieved strong average accuracy but remain unreliable in complex long-tail driving scenarios. These limitations reveal the weakness of the prevailing "one-model-fits-all" paradigm, particularly in safety-critical urban contexts where simpler physics-based models can occasionally outperform advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic multi-expert gating framework that adaptively selects the most reliable trajectory predictor among a physics-informed LSTM, a Transformer, and a fine-tuned GameFormer on a per-sample basis.
  Our method leverages internal model signals (meta-features) such as stability and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be substantially more informative than geometric scene descriptors. To the best of our knowledge, this is the first work to formulate trajectory expert selection as a pairwise-ranking problem over internal model signals (Burges et al., 2005), directly optimizing decision quality without requiring post-hoc calibration.
  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287 samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error (FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835 m), and realizes 57.8 percent of the oracle performance bound. In open-loop simulations, after trajectory horizon alignment, the same configuration reduces FDE on left-turn scenarios by approximately 10 percent, demonstrating consistent improvements across both offline validation and open-loop evaluation. These results indicate that adaptive hybrid systems enhance trajectory reliability in safety-critical autonomous driving, providing a practical pathway beyond static single-model paradigms.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells</title>
<link>https://arxiv.org/abs/2511.00129</link>
<guid>https://arxiv.org/abs/2511.00129</guid>
<content:encoded><![CDATA[
<div> Neural Network, Casing Collar Locator, Data Augmentation, Depth Measurement, Oil and Gas Well Operations

Summary: 
Accurate downhole depth measurement is crucial in oil and gas well operations for reservoir contact, efficiency, and safety. This study focuses on enhancing casing collar recognition using neural networks by proposing efficient preprocessing methods for data augmentation. The integration of a system into downhole tools allows for data acquisition to facilitate dataset construction. Through systematic experimentation, the effectiveness of various augmentation methods such as standardization, label distribution smoothing, and random cropping is analyzed. Results show significant improvements in model performance, with F1 scores reaching maximum values. The proposed approach addresses the challenges of training casing collar recognition models in data-limited environments, demonstrating practical applicability and effectiveness in real CCL waveforms validation. <br /><br />Summary: <div>
arXiv:2511.00129v1 Announce Type: new 
Abstract: Accurate downhole depth measurement is essential for oil and gas well operations, directly influencing reservoir contact, production efficiency, and operational safety. Collar correlation using a casing collar locator (CCL) is fundamental for precise depth calibration. While neural network-based CCL signal recognition has achieved significant progress in collar identification, preprocessing methods for such applications remain underdeveloped. Moreover, the limited availability of real well data poses substantial challenges for training neural network models that require extensive datasets. This paper presents a system integrated into downhole tools for CCL signal acquisition to facilitate dataset construction. We propose comprehensive preprocessing methods for data augmentation and evaluate their effectiveness using our AlexNet-based neural network models. Through systematic experimentation across various configuration combinations, we analyze the contribution of each augmentation method. Results demonstrate that standardization, label distribution smoothing (LDS), and random cropping are fundamental requirements for model training, while label smoothing regularization (LSR), time scaling, and multiple sampling significantly enhance model generalization capability. The F1 scores of our two benchmark models trained with the proposed augmentation methods maximumly improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance validation on real CCL waveforms confirms the effectiveness and practical applicability of our approach. This work addresses the gaps in data augmentation methodologies for training casing collar recognition models in CCL data-limited environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in Data-Scarce Scenarios</title>
<link>https://arxiv.org/abs/2511.00130</link>
<guid>https://arxiv.org/abs/2511.00130</guid>
<content:encoded><![CDATA[
<div> fine-tuning, Large Language Models, catastrophic forgetting, knowledge integration, adaptation strategy <br />
<br />
Summary: The paper compares Supervised Fine-tuning (SFT), Low-Rank Adaptation (LoRA), and In-Context Learning (ICL) as adaptation strategies for Large Language Models (LLMs) in data-scarce scenarios. While SFT is effective at skill acquisition, it is prone to catastrophic forgetting. LoRA minimizes parameter changes and strikes a good balance between instilling new skills and preserving general knowledge. ICL is useful for incorporating factual knowledge but struggles with complex skills. The study emphasizes the importance of selecting the appropriate adaptation strategy based on the task requirements. It distinguishes between skill acquisition and knowledge integration, outlining the trade-offs between task-specific performance and maintaining general capabilities in LLMs. <div>
arXiv:2511.00130v1 Announce Type: new 
Abstract: The remarkable capabilities of Large Language Models (LLMs) often need to be tailored for specific applications, requiring the integration of new knowledge or the acquisition of new skills. While full fine-tuning is a powerful adaptation method, it is computationally expensive and can lead to a degradation of general reasoning abilities, a phenomenon known as catastrophic forgetting. A range of alternative techniques exists, each with its own trade-offs. In-Context Learning (ICL) is fast but limited by context length, while Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) offer a middle ground by minimizing parameter changes. However, the challenge of catastrophic forgetting persists, raising questions about the best adaptation strategy for a given task. This paper presents a comparative analysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce scenarios. We find that LoRA provides the most effective balance, successfully instilling new skills with minimal impact on the base model's general knowledge. In contrast, while SFT excels at skill acquisition, it is highly susceptible to catastrophic forgetting. ICL is effective for incorporating factual knowledge but struggles with complex skills. Our findings offer a practical framework for selecting an LLM adaptation strategy. We highlight the critical distinction between skill acquisition and knowledge integration, clarify the trade-offs between task-specific performance and the preservation of general capabilities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning</title>
<link>https://arxiv.org/abs/2511.00133</link>
<guid>https://arxiv.org/abs/2511.00133</guid>
<content:encoded><![CDATA[
<div> Feature sampling, hyperparameter tuning, Random Forest classifiers, Simulated Annealing, predictive accuracy <br />
<br />
Summary: This paper introduces a new framework that enhances Random Forest classifiers by combining probabilistic feature sampling and hyperparameter tuning using Simulated Annealing. The framework aims to improve predictive accuracy and generalization in various domains such as credit risk evaluation, anomaly detection in IoT ecosystems, medical diagnostics, and biological data analysis. The approach focuses on selecting relevant features and dynamically tuning parameters to optimize classification performance. By emphasizing feature importance and utilizing metaheuristic optimization, the framework consistently improves accuracy and provides valuable insights into feature relevance. This integration of importance-aware sampling and optimization demonstrates the effectiveness of the proposed methodology for robust classification tasks. <div>
arXiv:2511.00133v1 Announce Type: new 
Abstract: This paper introduces a novel framework for enhancing Random Forest classifiers by integrating probabilistic feature sampling and hyperparameter tuning via Simulated Annealing. The proposed framework exhibits substantial advancements in predictive accuracy and generalization, adeptly tackling the multifaceted challenges of robust classification across diverse domains, including credit risk evaluation, anomaly detection in IoT ecosystems, early-stage medical diagnostics, and high-dimensional biological data analysis. To overcome the limitations of conventional Random Forests, we present an approach that places stronger emphasis on capturing the most relevant signals from data while enabling adaptive hyperparameter configuration. The model is guided towards features that contribute more meaningfully to classification and optimizing this with dynamic parameter tuning. The results demonstrate consistent accuracy improvements and meaningful insights into feature relevance, showcasing the efficacy of combining importance aware sampling and metaheuristic optimization.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physiologically Active Vegetation Reverses Its Cooling Effect in Humid Urban Climates</title>
<link>https://arxiv.org/abs/2511.00134</link>
<guid>https://arxiv.org/abs/2511.00134</guid>
<content:encoded><![CDATA[
<div> Keywords: vegetation, cooling, heat index, humidity, urban areas

Summary:<br /><br />Efforts to green cities for cooling are progressing in some cities but creating challenges in others due to the trade-off between cooling surfaces and increasing humidity. This study focuses on 138 Indian cities across different climates and urban densities to understand how vegetation influences the Heat Index (HI), a measure of temperature and humidity. The research reveals that while vegetation can enhance cooling, highly active vegetation can actually contribute to higher perceived heat stress by elevating humidity faster than it removes heat. The study identifies specific thresholds for vegetation structure and function, such as EVI, LAI, and fPAR, that determine when vegetation transitions from cooling to warming effects. These findings highlight the importance of climate-specific greening strategies in developing heat-resilient and equitable cities. <div>
arXiv:2511.00134v1 Announce Type: new 
Abstract: Efforts to green cities for cooling are succeeding unevenly because the same vegetation that cools surfaces can also intensify how hot the air feels. Previous studies have identified humid heat as a growing urban hazard, yet how physiologically active vegetation governs this trade-off between cooling and moisture accumulation remains poorly understood, leaving mitigation policy and design largely unguided. Here we quantify how vegetation structure and function influence the Heat Index (HI), a combined measure of temperature and humidity in 138 Indian cities spanning tropical savanna, semi-arid steppe, and humid subtropical climates, and across dense urban cores and semi-urban rings. Using an extreme-aware, one kilometre reconstruction of HI and an interpretable machine-learning framework that integrates SHapley Additive Explanations (SHAP) and Accumulated Local Effects (ALE), we isolate vegetation-climate interactions. Cooling generally strengthens for EVI >= 0.4 and LAI >= 0.05, but joint-high regimes begin to reverse toward warming when EVI >= 0.5, LAI >= 0.2, and fPAR >= 0.5,with an earlier onset for fPAR >= 0.25 in humid, dense cores. In such environments, highly physiologically active vegetation elevates near-surface humidity faster than it removes heat, reversing its cooling effect and amplifying perceived heat stress. These findings establish the climatic limits of vegetation-driven cooling and provide quantitative thresholds for climate-specific greening strategies that promote equitable and heat-resilient cities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control</title>
<link>https://arxiv.org/abs/2511.00136</link>
<guid>https://arxiv.org/abs/2511.00136</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Traffic signal control, HeraldLight, Reinforcement learning, Simulation experiments

Summary:
HeraldLight is a novel approach in traffic signal control that leverages large language models (LLMs) to improve optimization efficiency and interpretability compared to traditional reinforcement learning methods. The dual LLM architecture of HeraldLight includes the Herald Module for extracting contextual information and forecasting queue lengths, an LLM-Agent for fine-grained traffic signal control, and an LLM-Critic for refining outputs and improving accuracy. Through simulation experiments on real-world datasets, HeraldLight outperforms state-of-the-art baselines by achieving a 20.03% reduction in average travel time and a 10.74% reduction in average queue length in scenarios in Jinan and Hangzhou. The source code for HeraldLight is available on GitHub for further exploration and development. <div>
arXiv:2511.00136v1 Announce Type: new 
Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC) improves optimization efficiency and interpretability compared to traditional reinforcement learning (RL) methods. However, existing LLM-based approaches are limited by fixed time signal durations and are prone to hallucination errors, while RL methods lack robustness in signal timing decisions and suffer from poor generalization. To address these challenges, this paper proposes HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The Herald Module extracts contextual information and forecasts queue lengths for each traffic phase based on real-time conditions. The first LLM, LLM-Agent, uses these forecasts to make fine grained traffic signal control, while the second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and hallucinations. These refined outputs are used for score-based fine-tuning to improve accuracy and robustness. Simulation experiments using CityFlow on real world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New York (196) demonstrate that HeraldLight outperforms state of the art baselines, achieving a 20.03% reduction in average travel time across all scenarios and a 10.74% reduction in average queue length on the Jinan and Hangzhou scenarios. The source code is available on GitHub: https://github.com/BUPT-ANTlab/HeraldLight.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.00166</link>
<guid>https://arxiv.org/abs/2511.00166</guid>
<content:encoded><![CDATA[
<div> Keywords: decision-making, supply chain, deep learning, intelligent particle swarm optimization, adaptive control

Summary:<br />
This paper presents a decision model that combines deep learning with intelligent particle swarm optimization to enhance decision-making and planning efficiency in centralized redundant supply chains. The model incorporates a distributed node deployment model and optimal planning path for the supply chain network. Deep learning techniques, such as convolutional neural networks, are used to extract features from historical data, while linear programming captures high-order statistical features. The model is optimized using fuzzy association rule scheduling and deep reinforcement learning, with neural networks adjusting to dynamic changes. The hybrid mechanism of "deep learning feature extraction - intelligent particle swarm optimization" guides global optimization and aids in selecting optimal decisions for adaptive control. Simulations demonstrate improvements in resource consumption reduction, spatial planning enhancement, real-time decision adjustment in dynamic environments, distribution path optimization, and robust intelligent control. <div>
arXiv:2511.00166v1 Announce Type: new 
Abstract: To improve decision-making and planning efficiency in back-end centralized redundant supply chains, this paper proposes a decision model integrating deep learning with intelligent particle swarm optimization. A distributed node deployment model and optimal planning path are constructed for the supply chain network. Deep learning such as convolutional neural networks extracts features from historical data, and linear programming captures high-order statistical features. The model is optimized using fuzzy association rule scheduling and deep reinforcement learning, while neural networks fit dynamic changes. A hybrid mechanism of "deep learning feature extraction - intelligent particle swarm optimization" guides global optimization and selects optimal decisions for adaptive control. Simulations show reduced resource consumption, enhanced spatial planning, and in dynamic environments improved real-time decision adjustment, distribution path optimization, and robust intelligent control.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can SAEs reveal and mitigate racial biases of LLMs in healthcare?</title>
<link>https://arxiv.org/abs/2511.00177</link>
<guid>https://arxiv.org/abs/2511.00177</guid>
<content:encoded><![CDATA[
<div> race, Sparse Autoencoders, bias, healthcare, LLMs

Summary: 
Sparse Autoencoders (SAEs) are assessed for their ability to detect and control associations between race and stigmatizing concepts in Large Language Models (LLMs) used in healthcare. SAE latents in Gemma-2 models are identified that correlate with Black individuals, activating on reasonable input sequences but also problematic words like "incarceration". This latent can steer models to generate biased outputs about Black patients, increasing the risk assigned to negative behaviors. While SAEs show potential in identifying bias, mitigating it through latent steering is limited in effectiveness for complex clinical tasks. Overall, the study suggests that SAEs can be a useful tool in clinical applications of LLMs to identify bias based on demographics, but mitigation through SAE steering may have limited utility for realistic healthcare scenarios.<br /><br />Summary: <div>
arXiv:2511.00177v1 Announce Type: new 
Abstract: LLMs are increasingly being used in healthcare. This promises to free physicians from drudgery, enabling better care to be delivered at scale. But the use of LLMs in this space also brings risks; for example, such models may worsen existing biases. How can we spot when LLMs are (spuriously) relying on patient race to inform predictions? In this work we assess the degree to which Sparse Autoencoders (SAEs) can reveal (and control) associations the model has made between race and stigmatizing concepts. We first identify SAE latents in Gemma-2 models which appear to correlate with Black individuals. We find that this latent activates on reasonable input sequences (e.g., "African American") but also problematic words like "incarceration". We then show that we can use this latent to steer models to generate outputs about Black patients, and further that this can induce problematic associations in model outputs as a result. For example, activating the Black latent increases the risk assigned to the probability that a patient will become "belligerent". We evaluate the degree to which such steering via latents might be useful for mitigating bias. We find that this offers improvements in simple settings, but is less successful for more realistic and complex clinical tasks. Overall, our results suggest that: SAEs may offer a useful tool in clinical applications of LLMs to identify problematic reliance on demographics but mitigating bias via SAE steering appears to be of marginal utility for realistic tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PDE-SHARP: PDE Solver Hybrids Through Analysis &amp; Refinement Passes</title>
<link>https://arxiv.org/abs/2511.00183</link>
<guid>https://arxiv.org/abs/2511.00183</guid>
<content:encoded><![CDATA[
<div> machine learning, PDE solvers, computational cost, framework, solver accuracy <br />
Summary: <br />
The article introduces PDE-SHARP, a framework that reduces computational costs in generating PDE solvers by using machine learning. The framework consists of three stages: Analysis, Genesis, and Synthesis. Through mathematical analysis, solver generation, and collaborative selection tournaments, PDE-SHARP significantly decreases the number of solver evaluations needed compared to baseline methods. It achieves superior solver accuracy with 60-75% fewer computational evaluations, improving accuracy by 4 times on average across tested PDEs. PDE-SHARP demonstrates robust performance across various machine learning architectures, making it a promising approach for efficiently generating high-quality PDE solvers. <div>
arXiv:2511.00183v1 Announce Type: new 
Abstract: Current LLM-driven approaches using test-time computing to generate PDE solvers execute a large number of solver samples to identify high-accuracy solvers. These paradigms are especially costly for complex PDEs requiring substantial computational resources for numerical evaluation. We introduce PDE-SHARP, a framework to reduce computational costs by replacing expensive scientific computation by cheaper LLM inference that achieves superior solver accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three stages: (1) Analysis: mathematical chain-of-thought analysis including PDE classification, solution type detection, and stability analysis; (2) Genesis: solver generation based on mathematical insights from the previous stage; and (3) Synthesis: collaborative selection-hybridization tournaments in which LLM judges iteratively refine implementations through flexible performance feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13 solver evaluations on average compared to 30+ for baseline methods, improving accuracy uniformly across tested PDEs by $4\times$ on average, and demonstrates robust performance across LLM architectures, from general-purpose to specialized reasoning models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs</title>
<link>https://arxiv.org/abs/2511.00192</link>
<guid>https://arxiv.org/abs/2511.00192</guid>
<content:encoded><![CDATA[
arXiv:2511.00192v1 Announce Type: new 
Abstract: Membership inference attacks (MIA) aim to infer whether a particular data point is part of the training dataset of a model. In this paper, we propose a new task in the context of LLM privacy: entity-level discovery of membership risk focused on sensitive information (PII, credit card numbers, etc). Existing methods for MIA can detect the presence of entire prompts or documents in the LLM training data, but they fail to capture risks at a finer granularity. We propose the ``EL-MIA'' framework for auditing entity-level membership risks in LLMs. We construct a benchmark dataset for the evaluation of MIA methods on this task. Using this benchmark, we conduct a systematic comparison of existing MIA techniques as well as two newly proposed methods. We provide a comprehensive analysis of the results, trying to explain the relation of the entity level MIA susceptability with the model scale, training epochs, and other surface level factors. Our findings reveal that existing MIA methods are limited when it comes to entity-level membership inference of the sensitive attributes, while this susceptibility can be outlined with relatively straightforward methods, highlighting the need for stronger adversaries to stress test the provided threat model.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion LLMs are Natural Adversaries for any LLM</title>
<link>https://arxiv.org/abs/2511.00203</link>
<guid>https://arxiv.org/abs/2511.00203</guid>
<content:encoded><![CDATA[
arXiv:2511.00203v1 Announce Type: new 
Abstract: We introduce a novel framework that transforms the resource-intensive (adversarial) prompt optimization problem into an \emph{efficient, amortized inference task}. Our core insight is that pretrained, non-autoregressive generative LLMs, such as Diffusion LLMs, which model the joint distribution over prompt-response pairs, can serve as powerful surrogates for prompt search. This approach enables the direct conditional generation of prompts, effectively replacing costly, per-instance discrete optimization with a small number of parallelizable samples. We provide a probabilistic analysis demonstrating that under mild fidelity assumptions, only a few conditional samples are required to recover high-reward (harmful) prompts. Empirically, we find that the generated prompts are low-perplexity, diverse jailbreaks that exhibit strong transferability to a wide range of black-box target models, including robustly trained and proprietary LLMs. Beyond adversarial prompting, our framework opens new directions for red teaming, automated prompt optimization, and leveraging emerging Flow- and Diffusion-based LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides</title>
<link>https://arxiv.org/abs/2511.00209</link>
<guid>https://arxiv.org/abs/2511.00209</guid>
<content:encoded><![CDATA[
arXiv:2511.00209v1 Announce Type: new 
Abstract: Diffusion models have emerged as a leading framework in generative modeling, showing significant potential to accelerate and transform the traditionally slow and costly process of drug discovery. This review provides a systematic comparison of their application in designing two principal therapeutic modalities: small molecules and therapeutic peptides. We analyze how a unified framework of iterative denoising is adapted to the distinct molecular representations, chemical spaces, and design objectives of each modality. For small molecules, these models excel at structure-based design, generating novel, pocket-fitting ligands with desired physicochemical properties, yet face the critical hurdle of ensuring chemical synthesizability. Conversely, for therapeutic peptides, the focus shifts to generating functional sequences and designing de novo structures, where the primary challenges are achieving biological stability against proteolysis, ensuring proper folding, and minimizing immunogenicity. Despite these distinct challenges, both domains face shared hurdles: the need for more accurate scoring functions, the scarcity of high-quality experimental data, and the crucial requirement for experimental validation. We conclude that the full potential of diffusion models will be unlocked by bridging these modality-specific gaps and integrating them into automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby shifting the paradigm from chemical exploration to the targeted creation of novel therapeutics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Foundation Model Fine-Tuning on Multiple Rewards</title>
<link>https://arxiv.org/abs/2511.00220</link>
<guid>https://arxiv.org/abs/2511.00220</guid>
<content:encoded><![CDATA[
arXiv:2511.00220v1 Announce Type: new 
Abstract: Fine-tuning foundation models has emerged as a powerful approach for generating objects with specific desired properties. Reinforcement learning (RL) provides an effective framework for this purpose, enabling models to generate outputs that maximize a given reward function. However, in many applications such as text generation and drug discovery, it can be suboptimal to optimize using a single reward signal, as multiple evaluation criteria are often necessary. This paper proposes a novel reinforcement learning-based method for fine-tuning foundation models using multiple reward signals. By employing an iterative fine-tuning strategy across these rewards, our approach generalizes state-of-the-art RL-based methods. We further provide a theoretical analysis that offers insights into the performance of multi-reward RL fine-tuning. Experimental results across diverse domains including text, biological sequence, and small molecule generation, demonstrate the effectiveness of the proposed algorithm compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Melanoma Classification Through Deep Ensemble Learning and Explainable AI</title>
<link>https://arxiv.org/abs/2511.00246</link>
<guid>https://arxiv.org/abs/2511.00246</guid>
<content:encoded><![CDATA[
arXiv:2511.00246v1 Announce Type: new 
Abstract: Melanoma is one of the most aggressive and deadliest skin cancers, leading to mortality if not detected and treated in the early stages. Artificial intelligence techniques have recently been developed to help dermatologists in the early detection of melanoma, and systems based on deep learning (DL) have been able to detect these lesions with high accuracy. However, the entire community must overcome the explainability limit to get the maximum benefit from DL for diagnostics in the healthcare domain. Because of the black box operation's shortcomings in DL models' decisions, there is a lack of reliability and trust in the outcomes. However, Explainable Artificial Intelligence (XAI) can solve this problem by interpreting the predictions of AI systems. This paper proposes a machine learning model using ensemble learning of three state-of-the-art deep transfer Learning networks, along with an approach to ensure the reliability of the predictions by utilizing XAI techniques to explain the basis of the predictions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice</title>
<link>https://arxiv.org/abs/2511.00257</link>
<guid>https://arxiv.org/abs/2511.00257</guid>
<content:encoded><![CDATA[
arXiv:2511.00257v1 Announce Type: new 
Abstract: We determine the minimax optimal expected regret in the classic non-stochastic multi-armed bandit with expert advice problem, by proving a lower bound that matches the upper bound of Kale (2014). The two bounds determine the minimax optimal expected regret to be $\Theta\left( \sqrt{T K \log (N/K) } \right)$, where $K$ is the number of arms, $N$ is the number of experts, and $T$ is the time horizon.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>