<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>


<item>
<title>Interpreting the Effects of Quantization on LLMs</title>
<link>https://arxiv.org/abs/2508.16785</link>
<guid>https://arxiv.org/abs/2508.16785</guid>
<content:encoded><![CDATA[
<div> quantization, large language models, neuron activation, model calibration, model compression<br /><br />Summary:<br /><br />This study investigates the effects of quantization on large language models (LLMs), focusing on how internal representations and neuron behavior are influenced when applying 4-bit and 8-bit quantization. The research utilizes several interpretability techniques to analyze multiple LLMs. Key findings demonstrate that quantization has a generally minor impact on model calibration, indicating that the reliability of predictions is largely maintained. In analyzing neuron activations, the number of dead neurons‚Äîneurons with activations near zero across the dataset‚Äîremains stable regardless of quantization, showing resilience in neuron activation patterns. The study also reveals that smaller, full precision models tend to have fewer salient neurons contributing significantly to predictions, while larger models usually contain more salient neurons, though an exception was observed with Llama-2-7B. Additionally, the effect of quantization on neuron redundancy is inconsistent and varies depending on the specific model architecture. Overall, the results suggest that while quantization effects may differ across models and tasks, no drastic or discouraging changes were observed. This supports the use of quantization as a reliable technique for compressing LLMs without sacrificing model integrity. <div>
arXiv:2508.16785v3 Announce Type: replace 
Abstract: Quantization offers a practical solution to deploy LLMs in resource-constraint environments. However, its impact on internal representations remains understudied, raising questions about the reliability of quantized models. In this study, we employ a range of interpretability techniques to investigate how quantization affects model and neuron behavior. We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings reveal that the impact of quantization on model calibration is generally minor. Analysis of neuron activations indicates that the number of dead neurons, i.e., those with activation values close to 0 across the dataset, remains consistent regardless of quantization. In terms of neuron contribution to predictions, we observe that smaller full precision models exhibit fewer salient neurons, whereas larger models tend to have more, with the exception of Llama-2-7B. The effect of quantization on neuron redundancy varies across models. Overall, our findings suggest that effect of quantization may vary by model and tasks, however, we did not observe any drastic change which may discourage the use of quantization as a reliable model compression technique.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extending Test-Time Scaling: A 3D Perspective with Context, Batch, and Turn</title>
<link>https://arxiv.org/abs/2511.15738</link>
<guid>https://arxiv.org/abs/2511.15738</guid>
<content:encoded><![CDATA[
<div> Keywords: test-time scaling, reasoning reinforcement learning, multi-dimensional scaling, 3D test-time scaling, human-in-the-loop<br /><br />Summary:<br />1. The paper explores a new scaling effect in reasoning reinforcement learning called test-time scaling, where reasoning accuracy improves during test time as the reasoning context length increases. 2. It identifies limitations in test-time scaling compared to training-time scaling due to the restricted context length of base models, which is much smaller than training input sizes. 3. The authors introduce a unified multi-dimensional test-time scaling framework that includes three dimensions: context-length scaling, batch scaling (accuracy improves with parallel sampling), and turn scaling (iterative self-refinement enhances reasoning). 4. They propose 3D test-time scaling, integrating context, batch, and turn scaling, demonstrating that each dimension offers bounded improvement but their combination substantially boosts reasoning performance on difficult benchmarks such as IOI, IMO, and CPHO. 5. Additionally, the framework benefits from human preference feedback and naturally extends to open-ended domains like embodied learning, enabling advanced humanoid control behavior design within a human-in-the-loop setup. <div>
arXiv:2511.15738v1 Announce Type: new 
Abstract: Reasoning reinforcement learning (RL) has recently revealed a new scaling effect: test-time scaling. Thinking models such as R1 and o1 improve their reasoning accuracy at test time as the length of the reasoning context increases. However, compared with training-time scaling, test-time scaling is fundamentally limited by the limited context length of base models, which remains orders of magnitude smaller than the amount of tokens consumed during training. We revisit test-time enhancement techniques through the lens of scaling effect and introduce a unified framework of multi-dimensional test-time scaling to extend the capacity of test-time reasoning. Beyond conventional context-length scaling, we consider two additional dimensions: batch scaling, where accuracy improves with parallel sampling, and turn scaling, where iterative self-refinement enhances reasoning quality. Building on this perspective, we propose 3D test-time scaling, which integrates context, batch, and turn scaling. We show that: (1) each dimension demonstrates a test-time scaling effect, but with a bounded capacity; (2) combining all three dimensions substantially improves the reasoning performance of challenging testbeds, including IOI, IMO, and CPHO, and further benefits from human preference feedback; and (3) the human-in-the-loop framework naturally extends to a more open-ended domain, i.e., embodied learning, which enables the design of humanoid control behaviors.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Connecting the Dots: A Machine Learning Ready Dataset for Ionospheric Forecasting Models</title>
<link>https://arxiv.org/abs/2511.15743</link>
<guid>https://arxiv.org/abs/2511.15743</guid>
<content:encoded><![CDATA[
<div> Keywords: ionosphere forecasting, machine learning, total electron content, space weather, NASA Heliolab<br /><br />Summary:  
This work addresses the critical challenge of operational ionosphere forecasting, which is complicated by sparse observations, complex geospatial coupling, and a strong demand for timely, accurate predictions that support GNSS, communications, aviation safety, and satellite operations. As part of the 2025 NASA Heliolab initiative, the authors present a curated, open-access dataset that integrates diverse ionospheric and heliospheric measurements into a unified, machine learning-ready format. The dataset combines multiple data sources, including Solar Dynamic Observatory data, solar irradiance indices (F10.7), solar wind parameters, geomagnetic activity indices (Kp, AE, SYM-H), and NASA JPL‚Äôs Global Ionospheric Maps of Total Electron Content (GIM-TEC). It further incorporates geospatially sparse data from the World-Wide GNSS Receiver Network and crowdsourced Android smartphone measurements. This novel heterogeneous dataset is temporally and spatially aligned within a modular structure suitable for both physical and data-driven modeling approaches. Using this dataset, several spatiotemporal machine learning architectures are trained and benchmarked to forecast vertical TEC under quiet as well as geomagnetically active conditions. The resulting dataset and modeling pipeline provide a comprehensive resource to explore ionospheric dynamics and Sun-Earth interactions, facilitating advances in scientific understanding and operational forecasting capabilities. <div>
arXiv:2511.15743v1 Announce Type: new 
Abstract: Operational forecasting of the ionosphere remains a critical space weather challenge due to sparse observations, complex coupling across geospatial layers, and a growing need for timely, accurate predictions that support Global Navigation Satellite System (GNSS), communications, aviation safety, as well as satellite operations. As part of the 2025 NASA Heliolab, we present a curated, open-access dataset that integrates diverse ionospheric and heliospheric measurements into a coherent, machine learning-ready structure, designed specifically to support next-generation forecasting models and address gaps in current operational frameworks. Our workflow integrates a large selection of data sources comprising Solar Dynamic Observatory data, solar irradiance indices (F10.7), solar wind parameters (velocity and interplanetary magnetic field), geomagnetic activity indices (Kp, AE, SYM-H), and NASA JPL's Global Ionospheric Maps of Total Electron Content (GIM-TEC). We also implement geospatially sparse data such as the TEC derived from the World-Wide GNSS Receiver Network and crowdsourced Android smartphone measurements. This novel heterogeneous dataset is temporally and spatially aligned into a single, modular data structure that supports both physical and data-driven modeling. Leveraging this dataset, we train and benchmark several spatiotemporal machine learning architectures for forecasting vertical TEC under both quiet and geomagnetically active conditions. This work presents an extensive dataset and modeling pipeline that enables exploration of not only ionospheric dynamics but also broader Sun-Earth interactions, supporting both scientific inquiry and operational forecasting efforts.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TB or Not TB: Coverage-Driven Direct Preference Optimization for Verilog Stimulus Generation</title>
<link>https://arxiv.org/abs/2511.15767</link>
<guid>https://arxiv.org/abs/2511.15767</guid>
<content:encoded><![CDATA[
<div> Large Language Models, hardware verification, stimulus generation, Coverage-Driven Direct Preference Optimization, PairaNet<br /><br />Summary:<br /><br />1. The paper addresses the challenge of design verification in hardware development, which is notably time-consuming and resource-intensive, particularly due to the difficulty in generating effective stimuli for the design under test (DUT).<br />2. The authors propose a novel framework named "TB or not TB" that automates stimulus generation by leveraging Large Language Models (LLMs) fine-tuned via a new method called Coverage-Driven Direct Preference Optimization (CD-DPO).<br />3. To facilitate preference-based training, they introduce PairaNet, a new dataset derived from PyraNet, which consists of pairs of high- and low-quality testbenches annotated with simulation-derived coverage metrics.<br />4. The CD-DPO technique incorporates quantitative coverage feedback directly into the optimization process, guiding the LLM to produce stimuli that increase verification coverage effectively.<br />5. Experimental results on the CVDP CID12 benchmark demonstrate that "TB or not TB" surpasses both open-source and commercial baseline methods, achieving up to a 77.27% improvement in code coverage, thereby confirming the efficacy of coverage-driven preference optimization for LLM-based hardware verification. <div>
arXiv:2511.15767v1 Announce Type: new 
Abstract: With the rapid advancement of Large Language Models (LLMs), there is growing interest in applying them to hardware design and verification. Among these stages, design verification remains the most time-consuming and resource-intensive phase, where generating effective stimuli for the design under test (DUT) is both critical and labor-intensive. We present {\it TB or not TB}, a framework for automated stimulus generation using LLMs fine-tuned through Coverage-Driven Direct Preference Optimization (CD-DPO). To enable preference-based training, we introduce PairaNet, a dataset derived from PyraNet that pairs high- and low-quality testbenches labeled using simulation-derived coverage metrics. The proposed CD-DPO method integrates quantitative coverage feedback directly into the optimization objective, guiding the model toward generating stimuli that maximize verification coverage. Experiments on the CVDP CID12 benchmark show that {\it TB or not TB} outperforms both open-source and commercial baselines, achieving up to 77.27\% improvement in code coverage, demonstrating the effectiveness of Coverage-driven preference optimization for LLM-based hardware verification.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopoReformer: Mitigating Adversarial Attacks Using Topological Purification in OCR Models</title>
<link>https://arxiv.org/abs/2511.15807</link>
<guid>https://arxiv.org/abs/2511.15807</guid>
<content:encoded><![CDATA[
<div> adversarial attacks, OCR robustness, topological autoencoder, text image integrity, adversarial defense<br /><br />Summary:<br /><br />1. This paper addresses the vulnerability of Optical Character Recognition (OCR) systems to adversarially perturbed text images, which can produce incorrect transcriptions from subtle perturbations often invisible to humans. These adversarial examples pose serious security risks in sensitive applications like document processing and license plate recognition.<br /><br />2. Existing defense methods such as adversarial training, input preprocessing, or post-recognition correction tend to be model-specific, computationally intensive, degrade performance on clean inputs, and remain susceptible to new or adaptive attacks.<br /><br />3. The authors propose TopoReformer, a model-agnostic pipeline designed to mitigate adversarial perturbations while preserving the structural integrity of text images by leveraging topological features‚Äîproperties that remain invariant under continuous deformation, such as connectivity and loops.<br /><br />4. Central to TopoReformer is a topological autoencoder that enforces manifold-level consistency in the latent space, improving robustness without relying on explicit gradient regularization techniques.<br /><br />5. The method is evaluated on datasets including EMNIST and MNIST against a variety of adversarial attacks like FGSM, PGD, Carlini-Wagner, adaptive attacks such as EOT and BDPA, and an OCR-specific watermark attack (FAWA), demonstrating effective defense capabilities. <div>
arXiv:2511.15807v1 Announce Type: new 
Abstract: Adversarially perturbed images of text can cause sophisticated OCR systems to produce misleading or incorrect transcriptions from seemingly invisible changes to humans. Some of these perturbations even survive physical capture, posing security risks to high-stakes applications such as document processing, license plate recognition, and automated compliance systems. Existing defenses, such as adversarial training, input preprocessing, or post-recognition correction, are often model-specific, computationally expensive, and affect performance on unperturbed inputs while remaining vulnerable to unseen or adaptive attacks. To address these challenges, TopoReformer is introduced, a model-agnostic reformation pipeline that mitigates adversarial perturbations while preserving the structural integrity of text images. Topology studies properties of shapes and spaces that remain unchanged under continuous deformations, focusing on global structures such as connectivity, holes, and loops rather than exact distance. Leveraging these topological features, TopoReformer employs a topological autoencoder to enforce manifold-level consistency in latent space and improve robustness without explicit gradient regularization. The proposed method is benchmarked on EMNIST, MNIST, against standard adversarial attacks (FGSM, PGD, Carlini-Wagner), adaptive attacks (EOT, BDPA), and an OCR-specific watermark attack (FAWA).
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Tsybakov: Model Margin Noise and $\mathcal{H}$-Consistency Bounds</title>
<link>https://arxiv.org/abs/2511.15816</link>
<guid>https://arxiv.org/abs/2511.15816</guid>
<content:encoded><![CDATA[
<div> Model Margin Noise, MM noise, Tsybakov noise, ùìó-consistency bounds, classification<br /><br />Summary:<br /><br />1. This paper introduces a novel low-noise condition for classification called the Model Margin Noise (MM noise) assumption. 2. MM noise is a hypothesis-dependent assumption that measures noise based on the discrepancy between a given hypothesis and the Bayes classifier, unlike the classic Tsybakov noise condition which depends on the intrinsic distributional minimal margin. 3. The MM noise condition is strictly weaker than the Tsybakov noise condition; it is implied by Tsybakov noise but can hold even when Tsybakov noise fails, as demonstrated by an explicit example in the paper. 4. Under the MM noise assumption, the authors derive enhanced ùìó-consistency bounds for both binary and multi-class classification tasks. 5. These new bounds extend prior work by Mao, Mohri, and Zhong (2025a) by achieving similarly favorable convergence rates but under a notably weaker noise assumption. The bounds smoothly interpolate between linear and square-root rates depending on the noise level. 6. Additionally, the paper provides instantiations of these enhanced consistency bounds for several common surrogate loss functions and illustrates them through tables for practical comprehension. <div>
arXiv:2511.15816v1 Announce Type: new 
Abstract: We introduce a new low-noise condition for classification, the Model Margin Noise (MM noise) assumption, and derive enhanced $\mathcal{H}$-consistency bounds under this condition. MM noise is weaker than Tsybakov noise condition: it is implied by Tsybakov noise condition but can hold even when Tsybakov fails, because it depends on the discrepancy between a given hypothesis and the Bayes-classifier rather than on the intrinsic distributional minimal margin (see Figure 1 for an illustration of an explicit example). This hypothesis-dependent assumption yields enhanced $\mathcal{H}$-consistency bounds for both binary and multi-class classification. Our results extend the enhanced $\mathcal{H}$-consistency bounds of Mao, Mohri, and Zhong (2025a) with the same favorable exponents but under a weaker assumption than the Tsybakov noise condition; they interpolate smoothly between linear and square-root regimes for intermediate noise levels. We also instantiate these bounds for common surrogate loss families and provide illustrative tables.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention-Based Feature Online Conformal Prediction for Time Series</title>
<link>https://arxiv.org/abs/2511.15838</link>
<guid>https://arxiv.org/abs/2511.15838</guid>
<content:encoded><![CDATA[
<div> Keywords: Online conformal prediction, attention mechanism, feature-space calibration, distribution shifts, prediction intervals<br /><br />Summary:<br /><br />1. This paper presents Attention-based Feature Online Conformal Prediction (AFOCP), an enhanced approach to online conformal prediction (OCP) that improves prediction sets with guaranteed coverage even under temporal dependencies and distribution shifts.<br />2. Unlike standard OCP that uses simple nonconformity scores in the output space, AFOCP operates in the feature space of pre-trained neural networks, utilizing learned representations to create more compact prediction sets by focusing on task-relevant information and minimizing nuisance variations.<br />3. A key innovation of AFOCP is the use of an attention mechanism that adaptively weights historical observations based on their relevance to the current test point, effectively addressing challenges from non-stationarity and distribution shifts.<br />4. Theoretical results are provided to show that AFOCP preserves long-term coverage guarantees while yielding smaller prediction intervals than traditional OCP under mild regularity assumptions.<br />5. Extensive experiments on synthetic and real-world time series datasets demonstrate that AFOCP can reduce the size of prediction intervals by up to 88% compared to standard OCP, without sacrificing coverage accuracy, confirming the practical benefits of feature-space calibration combined with attention-based adaptive weighting. <div>
arXiv:2511.15838v1 Announce Type: new 
Abstract: Online conformal prediction (OCP) wraps around any pre-trained predictor to produce prediction sets with coverage guarantees that hold irrespective of temporal dependencies or distribution shifts. However, standard OCP faces two key limitations: it operates in the output space using simple nonconformity (NC) scores, and it treats all historical observations uniformly when estimating quantiles. This paper introduces attention-based feature OCP (AFOCP), which addresses both limitations through two key innovations. First, AFOCP operates in the feature space of pre-trained neural networks, leveraging learned representations to construct more compact prediction sets by concentrating on task-relevant information while suppressing nuisance variation. Second, AFOCP incorporates an attention mechanism that adaptively weights historical observations based on their relevance to the current test point, effectively handling non-stationarity and distribution shifts. We provide theoretical guarantees showing that AFOCP maintains long-term coverage while provably achieving smaller prediction intervals than standard OCP under mild regularity conditions. Extensive experiments on synthetic and real-world time series datasets demonstrate that AFOCP consistently reduces the size of prediction intervals by as much as $88\%$ as compared to OCP, while maintaining target coverage levels, validating the benefits of both feature-space calibration and attention-based adaptive weighting.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transparent Early ICU Mortality Prediction with Clinical Transformer and Per-Case Modality Attribution</title>
<link>https://arxiv.org/abs/2511.15847</link>
<guid>https://arxiv.org/abs/2511.15847</guid>
<content:encoded><![CDATA[
<div> Keywords: intensive care, in-hospital mortality, multimodal ensemble, interpretability, MIMIC-III<br /><br />Summary:<br /><br />1. The article addresses the challenge of early identification of ICU patients at risk of in-hospital mortality to enable timely interventions and optimize resource use.<br /><br />2. The authors note that existing machine learning models achieve high predictive performance but suffer from a lack of transparency and robustness, which limits their clinical adoption.<br /><br />3. They propose a lightweight, transparent multimodal ensemble model that integrates physiological time-series data (vitals) and unstructured clinical notes collected within the first 48 hours of ICU admission.<br /><br />4. The ensemble is constructed as a logistic regression that fuses outputs from two modality-specific models: a bidirectional LSTM for analyzing vital sign sequences and a finetuned ClinicalModernBERT transformer for processing clinical notes.<br /><br />5. This architecture offers multilevel interpretability by providing feature attributions within each modality and direct case-by-case modality attributions, clarifying the influence of vitals and notes on each prediction.<br /><br />6. Evaluated on the MIMIC-III dataset, the ensemble achieves improved discrimination over the best single models, with an AUPRC of 0.565 versus 0.526 and an AUROC of 0.891 versus 0.876, while maintaining well-calibrated risk predictions.<br /><br />7. The model demonstrates robustness by employing a calibrated fallback mechanism when data from one modality is unavailable.<br /><br />8. Overall, the system provides reliable, auditable risk estimates and transparent operation, which are important for clinical trust and usability. <div>
arXiv:2511.15847v1 Announce Type: new 
Abstract: Early identification of intensive care patients at risk of in-hospital mortality enables timely intervention and efficient resource allocation. Despite high predictive performance, existing machine learning approaches lack transparency and robustness, limiting clinical adoption. We present a lightweight, transparent multimodal ensemble that fuses physiological time-series measurements with unstructured clinical notes from the first 48 hours of an ICU stay. A logistic regression model combines predictions from two modality-specific models: a bidirectional LSTM for vitals and a finetuned ClinicalModernBERT transformer for notes. This traceable architecture allows for multilevel interpretability: feature attributions within each modality and direct per-case modality attributions quantifying how vitals and notes influence each decision. On the MIMIC-III benchmark, our late-fusion ensemble improves discrimination over the best single model (AUPRC 0.565 vs. 0.526; AUROC 0.891 vs. 0.876) while maintaining well-calibrated predictions. The system remains robust through a calibrated fallback when a modality is missing. These results demonstrate competitive performance with reliable, auditable risk estimates and transparent, predictable operation, which together are crucial for clinical use.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>discretize_distributions: Efficient Quantization of Gaussian Mixtures with Guarantees in Wasserstein Distance</title>
<link>https://arxiv.org/abs/2511.15854</link>
<guid>https://arxiv.org/abs/2511.15854</guid>
<content:encoded><![CDATA[
<div> Gaussian mixture, quantization, Wasserstein distance, discretization, cyber-physical systems<br /><br />Summary:<br /><br />1. The article introduces discretize_distributions, a Python package designed to create discrete approximations of Gaussian mixture distributions efficiently. 2. The package includes implementations of cutting-edge quantization methods specifically tailored for Gaussian mixture models and enhances these methods to support improved scalability. 3. It incorporates additional quantization techniques such as sigma-point methods, offering a versatile and modular interface that permits the integration of custom schemes. 4. The tool is intended for use in control and verification workflows for cyber-physical systems, highlighting its applicability to real-world engineering problems. 5. Benchmarking on various challenging scenarios‚Äîsuch as high-dimensional, large-scale, and degenerate Gaussian mixtures‚Äîdemonstrates that discretize_distributions achieves accurate approximations while maintaining low computational costs, validating its efficiency and reliability. <div>
arXiv:2511.15854v1 Announce Type: new 
Abstract: We present discretize_distributions, a Python package that efficiently constructs discrete approximations of Gaussian mixture distributions and provides guarantees on the approximation error in Wasserstein distance. The package implements state-of-the-art quantization methods for Gaussian mixture models and extends them to improve scalability. It further integrates complementary quantization strategies such as sigma-point methods and provides a modular interface that supports custom schemes and integration into control and verification pipelines for cyber-physical systems. We benchmark the package on various examples, including high-dimensional, large, and degenerate Gaussian mixtures, and demonstrate that discretize_distributions produces accurate approximations at low computational cost.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLOBE: Accurate and Generalizable PDE Surrogates using Domain-Inspired Architectures and Equivariances</title>
<link>https://arxiv.org/abs/2511.15856</link>
<guid>https://arxiv.org/abs/2511.15856</guid>
<content:encoded><![CDATA[
<div> GLOBE, Neural surrogate, PDEs, Equivariance, Airfoil simulation<br /><br />Summary:<br /><br />This paper introduces GLOBE, a novel neural network surrogate designed specifically for homogeneous partial differential equations (PDEs), integrating inductive biases inspired by boundary-element methods and equivariant machine learning principles. GLOBE models solutions as superpositions of learnable Green's-function-like kernels evaluated from boundary faces to target points, structured across multiscale branches and communication hyperlayers. Its architecture ensures translation, rotation, and parity equivariance, discretization invariance in the fine-mesh limit, and units invariance achieved through rigorous nondimensionalization. To enhance stability and generalization, an explicit far-field decay envelope is applied, boundary-to-boundary hyperlayer communication enables long-range interactions, and a global receptive field respects PDE information flow, effective even for elliptic PDEs. In practical applications, specifically the AirFRANS dataset involving steady incompressible Reynolds-averaged Navier-Stokes (RANS) simulations over NACA airfoils, GLOBE dramatically outperforms baseline models by reducing mean-squared errors by roughly 200x on the "Full" dataset split, and by over 100x on velocity and pressure fields in scarce data regimes compared to other advanced models like Transolver. The model is compact with only 117k parameters, supports evaluation at arbitrary spatial points during inference, and can handle non-watertight mesh inputs, enhancing its practical utility. Overall, the work demonstrates that embedding rigorous physics-based and domain-specific inductive biases into ML models significantly improves accuracy, generalizability, and practicality for industrial computer-aided engineering PDE surrogates. <div>
arXiv:2511.15856v1 Announce Type: new 
Abstract: We introduce GLOBE, a new neural surrogate for homogeneous PDEs that draws inductive bias from boundary-element methods and equivariant ML. GLOBE represents solutions as superpositions of learnable Green's-function-like kernels evaluated from boundary faces to targets, composed across multiscale branches and communication hyperlayers. The architecture is translation-, rotation-, and parity-equivariant; discretization-invariant in the fine-mesh limit; and units-invariant via rigorous nondimensionalization. An explicit far-field decay envelope stabilizes extrapolation, boundary-to-boundary hyperlayer communication mediates long-range coupling, and the all-to-all boundary-to-target evaluation yields a global receptive field that respects PDE information flow, even for elliptic PDEs.
  On AirFRANS (steady incompressible RANS over NACA airfoils), GLOBE achieves substantial accuracy improvements. On the "Full" split, it reduces mean-squared error by roughly 200x on all fields relative to the dataset's reference baselines, and roughly 50x relative to the next-best-performing model. In the "Scarce" split, it achieves over 100x lower error on velocity and pressure fields and over 600x lower error on surface pressure than Transolver. Qualitative results show sharp near-wall gradients, coherent wakes, and limited errors under modest extrapolation in Reynolds number and angle of attack.
  In addition to this accuracy, the model is quite compact (117k parameters), and fields can be evaluated at arbitrary points during inference. We also demonstrate the ability to train and predict with non-watertight meshes, which has strong practical implications.
  These results show that rigorous physics- and domain-inspired inductive biases can achieve large gains in accuracy, generalizability, and practicality for ML-based PDE surrogates for industrial computer-aided engineering (CAE).
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Resolution: Optimal Multi-Draft Speculative Sampling via Convex Minimization</title>
<link>https://arxiv.org/abs/2511.15898</link>
<guid>https://arxiv.org/abs/2511.15898</guid>
<content:encoded><![CDATA[
<div> Speculative Sampling, Optimal Transport, Multi-draft, Polymatroid Theory, Convex Optimization<br /><br />Summary:<br /><br />This paper addresses the challenge of reducing latency in autoregressive decoding for large language models (LLMs) by focusing on the multi-draft speculative sampling framework, which generates multiple draft tokens and uses an acceptance criterion based on optimal transport (OT). Although OT maximizes the acceptance probability of tokens, solving the OT linear program (OTLP) is computationally infeasible due to its exponential size in vocabulary and draft count. The authors prove that previously proposed formulations via importance sampling or subset selection are equivalent to a relaxed OTLP of exponential scale, thus still infeasible. To overcome this, the work reformulates the OTLP as a max-flow problem through reverse engineering subset selection. Leveraging polymatroid theory, the study further reduces the complexity by transforming the problem into a convex optimization over at most the vocabulary size variables, making it computationally efficient. This enables the design of an optimal multi-draft speculative sampling algorithm when tokens are drawn i.i.d. from a single draft model, which is tunable for arbitrary accuracy. Experimental results demonstrate that the proposed method achieves high acceptance rates (~90%) with under 100 ms overhead per token and negligible deviation from the target model distribution, providing a practical multi-draft decoding solution for LLMs. <div>
arXiv:2511.15898v1 Announce Type: new 
Abstract: Speculative sampling reduces the latency of autoregressive decoding for target model LLMs without sacrificing inference quality, by using a cheap draft model to suggest a candidate token and a verification criterion to accept or resample this token. To improve acceptance and decoding efficiency, recent work has explored the multi-draft extension, where at each step $n$ draft tokens are generated, and the verification criterion is a distribution conditioned on these. When this criterion maximizes the probability of accepting some draft token, it is called the optimal transport (OT). However, finding the OT is difficult, as it is the solution of a linear program (OTLP) in over $V^n$ variables, with $V$ being the vocabulary size. Two recent theoretical works have reframed the OTLP in terms of importance sampling or subset selection. In this work, we prove that these formulations are equivalent to an exponentially large relaxed OTLP, so it remains infeasible to solve. Then, we reverse engineer subset selection to formulate the OTLP as a max-flow problem. With a novel application of polymatroid theory, we reduce the exponentially large OTLP to a convex optimization problem in at most $V$ variables. This allows us to devise an algorithm for optimal $n$-draft speculative sampling when the $n$ tokens are chosen i.i.d. from a single draft model, which can be tuned to arbitrary accuracy. Finally, we measure acceptance rates and algorithm runtimes for various $n$ and top-$k$ draft sampling settings. Our findings give the first multi-draft algorithm with 90% acceptance and under 100 ms of overhead per generated token with negligible deviation from the target model distribution.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified all-atom molecule generation with neural fields</title>
<link>https://arxiv.org/abs/2511.15906</link>
<guid>https://arxiv.org/abs/2511.15906</guid>
<content:encoded><![CDATA[
<div> Keywords: FuncBind, generative models, structure-based drug design, neural fields, macrocyclic peptides<br /><br />Summary:<br />1. The paper introduces FuncBind, a novel framework designed to overcome limitations of generative models in structure-based drug design, which are typically constrained to specific molecular modalities.<br />2. FuncBind utilizes neural fields to represent molecules as continuous atomic densities, enabling a modality-agnostic and unified approach across diverse atomic systems, including small molecules, macrocyclic peptides, and complex protein loops.<br />3. The framework employs score-based generative models inspired by advanced computer vision architectures, allowing it to handle variable atom and residue counts, including non-canonical amino acids.<br />4. FuncBind demonstrated competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region (CDR) loops, conditioned on target structures.<br />5. The model‚Äôs practical utility is showcased by successfully generating novel antibody binders in vitro through the de novo redesign of the CDR H3 loop based on two co-crystal structures.<br />6. Additionally, the authors provide a new dataset and benchmark for structure-conditioned macrocyclic peptide generation to facilitate future research.<br />7. The FuncBind code and resources are made publicly available at the provided GitHub repository, promoting transparency and further development in the field. <div>
arXiv:2511.15906v1 Announce Type: new 
Abstract: Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at https://github.com/prescient-design/funcbind.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization</title>
<link>https://arxiv.org/abs/2511.15915</link>
<guid>https://arxiv.org/abs/2511.15915</guid>
<content:encoded><![CDATA[
<div> Keywords: AccelOpt, kernel optimization, AI accelerators, large language models, AWS Trainium  

<br /><br />Summary:  
1. AccelOpt is a self-improving agentic system based on large language models (LLMs) designed to autonomously optimize kernels for emerging AI accelerators without needing expert hardware-specific knowledge.  
2. The system explores the kernel optimization space through iterative generation, leveraging an optimization memory that stores and uses insights from previous slow-fast kernel pairs to guide improvements.  
3. To evaluate its effectiveness, the authors created NKIBench, a benchmark suite composed of AWS Trainium accelerator kernels of varying complexity sourced from real-world LLM workloads.  
4. Experimental results show that AccelOpt improves its optimization capability over time, increasing peak throughput from 49% to 61% on Trainium 1 and from 45% to 59% on Trainium 2 for the NKIBench kernels.  
5. Additionally, AccelOpt offers significant cost advantages, matching the kernel improvements of the state-of-the-art Claude Sonnet 4 model but at 26 times lower cost using open-source models. <div>
arXiv:2511.15915v1 Announce Type: new 
Abstract: We present AccelOpt, a self-improving large language model (LLM) agentic system that autonomously optimizes kernels for emerging AI acclerators, eliminating the need for expert-provided hardware-specific optimization knowledge. AccelOpt explores the kernel optimization space through iterative generation, informed by an optimization memory that curates experiences and insights from previously encountered slow-fast kernel pairs. We build NKIBench, a new benchmark suite of AWS Trainium accelerator kernels with varying complexity extracted from real-world LLM workloads to evaluate the effectiveness of AccelOpt. Our evaluation confirms that AccelOpt's capability improves over time, boosting the average percentage of peak throughput from $49\%$ to $61\%$ on Trainium 1 and from $45\%$ to $59\%$ on Trainium 2 for NKIBench kernels. Moreover, AccelOpt is highly cost-effective: using open-source models, it matches the kernel improvements of Claude Sonnet 4 while being $26\times$ cheaper.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone</title>
<link>https://arxiv.org/abs/2511.15927</link>
<guid>https://arxiv.org/abs/2511.15927</guid>
<content:encoded><![CDATA[
<div> Diffusion-based language models, Transformer backbones, Bidirectional Mamba, Inference throughput, State-space architectures<br /><br />Summary:<br /><br />This work addresses the efficiency limitations of diffusion-based language models that typically rely on Transformer backbones, which suffer from quadratic attention complexity and KV-cache overhead during inference. The authors propose DiffuApriel, a masked diffusion language model built upon a bidirectional Mamba backbone that supports linear-time sequence modeling. DiffuApriel is shown to match transformer-based diffusion models in performance, while achieving up to 4.4 times higher inference throughput on long sequences with a 1.3 billion parameter model. Additionally, the paper introduces DiffuApriel-H, a hybrid model variant that interleaves attention layers with Mamba layers to balance global and local context modeling, delivering up to a 2.6 times throughput improvement. The results indicate that bidirectional state-space architectures can serve as effective denoisers in masked diffusion language models, offering a scalable and practical approach to faster and more memory-efficient text generation. This work contributes a new architectural direction that overcomes Transformer overhead, making diffusion-based language modeling more feasible for real-world applications requiring long sequence generation. <div>
arXiv:2511.15927v1 Announce Type: new 
Abstract: Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language model built on a bidirectional Mamba backbone that combines the diffusion objective with linear-time sequence modeling. DiffuApriel matches the performance of Transformer-based diffusion models while achieving up to 4.4x higher inference throughput for long sequences with a 1.3B model. We further propose DiffuApriel-H, a hybrid variant that interleaves attention and mamba layers, offering up to 2.6x throughput improvement with balanced global and local context modeling. Our results demonstrate that bidirectional state-space architectures serve as strong denoisers in masked diffusion LMs, providing a practical and scalable foundation for faster, memory-efficient text generation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iLTM: Integrated Large Tabular Model</title>
<link>https://arxiv.org/abs/2511.15941</link>
<guid>https://arxiv.org/abs/2511.15941</guid>
<content:encoded><![CDATA[
<div> Keywords: tabular data, large tabular model, gradient-boosted decision trees, meta-trained hypernetwork, deep learning  

<br /><br />Summary:  
Tabular data plays a critical role across various fields including science, industry, and public services, yet deep learning advancements have not been fully adapted to tabular data, where gradient-boosted decision trees (GBDTs) are still predominantly used. The paper introduces iLTM, an integrated Large Tabular Model that combines several components such as tree-derived embeddings, dimensionality-agnostic representations, a meta-trained hypernetwork, multilayer perceptrons (MLPs), and retrieval mechanisms within one architecture. iLTM was pretrained on over 1,800 diverse tabular classification datasets, ensuring its robustness and adaptability across a wide range of classification and regression tasks, from small datasets to large and high-dimensional ones. After light fine-tuning, the meta-trained hypernetwork is capable of transferring to regression tasks effectively, matching or outperforming strong baseline models. Comprehensive experiments demonstrate that iLTM consistently outperforms well-tuned GBDTs and leading neural network models designed for tabular data, while also requiring less task-specific hyperparameter tuning. By integrating elements from both tree-based and neural network approaches, iLTM paves the way for a novel framework of tabular foundation models that are robust, adaptable, and scalable for tabular learning challenges. <div>
arXiv:2511.15941v1 Announce Type: new 
Abstract: Tabular data underpins decisions across science, industry, and public services. Despite rapid progress, advances in deep learning have not fully carried over to the tabular domain, where gradient-boosted decision trees (GBDTs) remain a default choice in practice. We present iLTM, an integrated Large Tabular Model that unifies tree-derived embeddings, dimensionality-agnostic representations, a meta-trained hypernetwork, multilayer perceptrons (MLPs), and retrieval within a single architecture. Pretrained on more than 1,800 heterogeneous classification datasets, iLTM achieves consistently superior performance across tabular classification and regression tasks, from small datasets to large and high-dimensional tasks. After light fine-tuning, the meta-trained hypernetwork transfers to regression targets, matching or surpassing strong baselines. Extensive experiments show that iLTM outperforms well-tuned GBDTs and leading deep tabular models while requiring less task-specific tuning. By bridging the gap between tree-based and neural methods, iLTM offers a new framework for tabular foundation models for robust, adaptable, and scalable tabular learning.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-supervised and Multi-fidelity Learning for Extended Predictive Soil Spectroscopy</title>
<link>https://arxiv.org/abs/2511.15965</link>
<guid>https://arxiv.org/abs/2511.15965</guid>
<content:encoded><![CDATA[
<div> Keywords: self-supervised learning, multi-fidelity learning, soil spectroscopy, latent space embeddings, spectrum conversion<br /><br />Summary:<br /><br />1. The article proposes a self-supervised machine learning (SSML) framework designed to improve multi-fidelity learning and extend predictive capabilities in soil spectroscopy by utilizing latent space embeddings. <br />2. A Variational Autoencoder (VAE) was pretrained on a large mid-infrared (MIR) spectral library using only unlabeled data, enabling the creation of compressed latent spectral embeddings and maximizing the use of extensive spectral databases including scan repeats for training augmentation. <br />3. The pretrained MIR decoder was frozen and integrated with a near-infrared (NIR) encoder to learn a spectral conversion mapping from NIR to MIR spectra, allowing predictions to leverage the richer information found in the large MIR library while using a cost-effective portable NIR scanner. <br />4. Downstream machine learning models were trained to predict nine soil properties using original spectra, predicted spectra from the NIR-to-MIR conversion, and latent space embeddings, with evaluation performed on an independent, gold-standard test set using regression goodness-of-fit metrics. <br />5. Results demonstrated that the SSML approach and latent embeddings achieved comparable or improved accuracy across all soil property predictions relative to baseline models, while NIR-to-MIR conversion predictions, although not matching original MIR performance, surpassed NIR-only models, indicating that a unified spectral latent space effectively leverages larger MIR datasets for enhanced soil property prediction. <div>
arXiv:2511.15965v1 Announce Type: new 
Abstract: We propose a self-supervised machine learning (SSML) framework for multi-fidelity learning and extended predictive soil spectroscopy based on latent space embeddings. A self-supervised representation was pretrained with the large MIR spectral library and the Variational Autoencoder algorithm to obtain a compressed latent space for generating spectral embeddings. At this stage, only unlabeled spectral data were used, allowing us to leverage the full spectral database and the availability of scan repeats for augmented training. We also leveraged and froze the trained MIR decoder for a spectrum conversion task by plugging it into a NIR encoder to learn the mapping between NIR and MIR spectra in an attempt to leverage the predictive capabilities contained in the large MIR library with a low cost portable NIR scanner. This was achieved by using a smaller subset of the KSSL library with paired NIR and MIR spectra. Downstream machine learning models were then trained to map between original spectra, predicted spectra, and latent space embeddings for nine soil properties. The performance of was evaluated independently of the KSSL training data using a gold-standard test set, along with regression goodness-of-fit metrics. Compared to baseline models, the proposed SSML and its embeddings yielded similar or better accuracy in all soil properties prediction tasks. Predictions derived from the spectrum conversion (NIR to MIR) task did not match the performance of the original MIR spectra but were similar or superior to predictive performance of NIR-only models, suggesting the unified spectral latent space can effectively leverage the larger and more diverse MIR dataset for prediction of soil properties not well represented in current NIR libraries.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Epidemic Predictions Using Agent-based Wireless Sensor Network Models</title>
<link>https://arxiv.org/abs/2511.15982</link>
<guid>https://arxiv.org/abs/2511.15982</guid>
<content:encoded><![CDATA[
<div> Keywords: wireless sensor networks, SEIRV model, machine learning, epidemic prediction, regression algorithms  

<br /><br />Summary:  
This study addresses the challenge of limited epidemiological data in wireless sensor networks (WSNs), which complicates the modeling and forecasting of malware attacks like viruses and worms. Researchers employed an agent-based implementation of the susceptible-exposed-infected-recovered-vaccinated (SEIRV) mathematical epidemic model to generate synthetic datasets that simulate infection dynamics within WSNs. These datasets were created using NetLogo's BehaviorSpace tool combined with Python for preprocessing. The problem was formulated as a regression task, aiming to predict the number of infected and recovered nodes over time. Several machine learning regression algorithms were tested, including support vector, linear, Lasso, Ridge, ElasticNet, Random Forest, XGBoost, Decision Trees, and k-nearest neighbors. Performance evaluation showed very high accuracy on training data, with R^2 values close to 1, while validation (test) results were slightly lower but still strong, reflecting the typical drop when models handle unseen data. Among the algorithms, Random Forest, XGBoost, Decision Trees, and k-nearest neighbors outperformed others, whereas support vector, linear, Lasso, Ridge, and ElasticNet regression performed comparatively poorly. The study demonstrates the feasibility and effectiveness of ML regression methods combined with agent-based epidemic modeling for predicting malware spread in WSNs. <div>
arXiv:2511.15982v1 Announce Type: new 
Abstract: The lack of epidemiological data in wireless sensor networks (WSNs) is a fundamental difficulty in constructing robust models to forecast and mitigate threats such as viruses and worms. Many studies have examined different epidemic models for WSNs, focusing on how malware infections spread given the network's specific properties, including energy limits and node mobility. In this study, an agent-based implementation of the susceptible-exposed-infected-recovered-vaccinated (SEIRV) mathematical model was employed for machine learning (ML) predictions. Using tools such as NetLogo's BehaviorSpace and Python, two epidemic synthetic datasets were generated and prepared for the application of several ML algorithms. Posed as a regression problem, the infected and recovered nodes were predicted, and the performance of these algorithms is compared using the error metrics of the train and test sets. The predictions performed well, with low error metrics and high R^2 values (0.997, 1.000, 0.999, 1.000), indicating an effective fit to the training set. The validation values were lower (0.992, 0.998, 0.971, and 0.999), as is typical when evaluating model performance on unseen data. Based on the recorded performances, support vector, linear, Lasso, Ridge, and ElasticNet regression were among the worst-performing algorithms, while Random Forest, XGBoost, Decision Trees, and k-nearest neighbors achieved the best results.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Descend or Rewind? Stochastic Gradient Descent Unlearning</title>
<link>https://arxiv.org/abs/2511.15983</link>
<guid>https://arxiv.org/abs/2511.15983</guid>
<content:encoded><![CDATA[
<div> Machine Unlearning, Gradient Descent, Certified Unlearning, Nonconvex Optimization, Sensitivity Analysis  

<br /><br />Summary:  
This paper focuses on machine unlearning algorithms designed to efficiently remove the influence of selected training data from models without requiring full retraining. It specifically studies two full-batch gradient descent methods, Descent-to-Delete (D2D) and Rewind-to-Delete (R2D), which offer provable unlearning guarantees and are straightforward to implement. Despite D2D‚Äôs frequent use as a finetuning baseline, theoretical guarantees for stochastic D2D in nonconvex settings have been lacking. The authors provide $(\epsilon, \delta)$ certified unlearning guarantees for stochastic D2D and R2D across strongly convex, convex, and nonconvex loss functions. They achieve this by analyzing unlearning as perturbed gradient systems exhibiting contraction, semi-contraction, or expansion behaviors. A key technical contribution is the coupling of unlearning and retraining trajectories to produce probabilistic sensitivity bounds, which they combine with a new relaxed Gaussian mechanism to ensure $(\epsilon, \delta)$ unlearning. Their theoretical results suggest that D2D provides tighter guarantees for strongly convex problems due to contraction to a unique global minimum. In contrast, R2D is more broadly applicable, allowing unlearning in both convex and nonconvex settings by effectively reversing accumulated gradient disturbances and aligning the unlearned model closer to the retrained model. <div>
arXiv:2511.15983v1 Announce Type: new 
Abstract: Machine unlearning algorithms aim to remove the impact of selected training data from a model without the computational expenses of retraining from scratch. Two such algorithms are ``Descent-to-Delete" (D2D) and ``Rewind-to-Delete" (R2D), full-batch gradient descent algorithms that are easy to implement and satisfy provable unlearning guarantees. In particular, the stochastic version of D2D is widely implemented as the ``finetuning" unlearning baseline, despite lacking theoretical backing on nonconvex functions. In this work, we prove $(\epsilon, \delta)$ certified unlearning guarantees for stochastic R2D and D2D for strongly convex, convex, and nonconvex loss functions, by analyzing unlearning through the lens of disturbed or biased gradient systems, which may be contracting, semi-contracting, or expansive respectively. Our argument relies on optimally coupling the random behavior of the unlearning and retraining trajectories, resulting in a probabilistic sensitivity bound that can be combined with a novel relaxed Gaussian mechanism to achieve $(\epsilon, \delta)$ unlearning. We determine that D2D can yield tighter guarantees for strongly convex functions compared to R2D by relying on contraction to a unique global minimum. However, unlike D2D, R2D can achieve unlearning in the convex and nonconvex setting because it draws the unlearned model closer to the retrained model by reversing the accumulated disturbances.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synergizing Deconfounding and Temporal Generalization For Time-series Counterfactual Outcome Estimation</title>
<link>https://arxiv.org/abs/2511.16006</link>
<guid>https://arxiv.org/abs/2511.16006</guid>
<content:encoded><![CDATA[
<div> Keywords: counterfactual outcome estimation, time-series, deconfounding, Sub-treatment Group Alignment, Random Temporal Masking  

<br /><br />Summary:  
Estimating counterfactual outcomes in time-series data is essential for informed decision-making, such as determining optimal treatment timings, yet remains challenging due to the unobserved nature of counterfactual trajectories and evolving confounders. This paper introduces a new framework that combines Sub-treatment Group Alignment (SGA) and Random Temporal Masking (RTM) to tackle these difficulties. SGA improves upon traditional methods by using iterative, treatment-agnostic clustering to form fine-grained sub-treatment groups, leading to more precise distributional alignment and effective deconfounding; it is also theoretically shown to optimize a tighter upper bound on counterfactual risk. RTM enhances temporal generalization by randomly masking input covariates with Gaussian noise during training, reducing reliance on potentially noisy or spurious covariates at the current step and encouraging the model to leverage stable historical information, thereby preserving causal relationships. Empirical results confirm that while SGA and RTM each individually enhance counterfactual outcome estimation, their combined application consistently outperforms existing methods, owing to their complementary roles: SGA addresses confounding at each time step, and RTM strengthens temporal robustness and generalization. This integrated approach sets a new state-of-the-art in accurately estimating counterfactual outcomes from time-series data. <div>
arXiv:2511.16006v1 Announce Type: new 
Abstract: Estimating counterfactual outcomes from time-series observations is crucial for effective decision-making, e.g. when to administer a life-saving treatment, yet remains significantly challenging because (i) the counterfactual trajectory is never observed and (ii) confounders evolve with time and distort estimation at every step. To address these challenges, we propose a novel framework that synergistically integrates two complementary approaches: Sub-treatment Group Alignment (SGA) and Random Temporal Masking (RTM). Instead of the coarse practice of aligning marginal distributions of the treatments in latent space, SGA uses iterative treatment-agnostic clustering to identify fine-grained sub-treatment groups. Aligning these fine-grained groups achieves improved distributional matching, thus leading to more effective deconfounding. We theoretically demonstrate that SGA optimizes a tighter upper bound on counterfactual risk and empirically verify its deconfounding efficacy. RTM promotes temporal generalization by randomly replacing input covariates with Gaussian noises during training. This encourages the model to rely less on potentially noisy or spuriously correlated covariates at the current step and more on stable historical patterns, thereby improving its ability to generalize across time and better preserve underlying causal relationships. Our experiments demonstrate that while applying SGA and RTM individually improves counterfactual outcome estimation, their synergistic combination consistently achieves state-of-the-art performance. This success comes from their distinct yet complementary roles: RTM enhances temporal generalization and robustness across time steps, while SGA improves deconfounding at each specific time point.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Guided Inductive Spatiotemporal Kriging for PM2.5 with Satellite Gradient Constraints</title>
<link>https://arxiv.org/abs/2511.16013</link>
<guid>https://arxiv.org/abs/2511.16013</guid>
<content:encoded><![CDATA[
<div> PM2.5, spatiotemporal kriging, deep learning, aerosol optical depth, pollution mapping  

<br /><br />Summary:  
This study addresses the challenge of high-resolution mapping of fine particulate matter (PM2.5) in urban areas, which is hindered by the sparse distribution of ground monitoring stations. Traditional approaches rely on satellite-derived Aerosol Optical Depth (AOD) data but face significant issues due to non-random missing data (e.g., cloud cover, nighttime) and biases in inversion processes. To overcome these limitations, the authors propose the Spatiotemporal Physics-Guided Inference Network (SPIN), a novel deep learning framework that integrates domain knowledge by explicitly modeling physical advection and diffusion through parallel graph kernels. A key innovation of SPIN is its training strategy, which uses AOD data not as direct inputs but rather as spatial gradient constraints within the loss function, enabling the model to learn pollution distribution patterns robustly despite data gaps. The model was validated in the highly polluted Beijing-Tianjin-Hebei and Surrounding Areas (BTHSA), demonstrating superior performance with a Mean Absolute Error (MAE) of 9.52 ¬µg/m¬≥. SPIN effectively generates continuous and physically consistent pollution fields, including in unmonitored areas. This method offers a robust, low-cost, and all-weather solution for granular environmental management, advancing sustainable urban pollution monitoring. <div>
arXiv:2511.16013v1 Announce Type: new 
Abstract: High-resolution mapping of fine particulate matter (PM2.5) is a cornerstone of sustainable urbanism but remains critically hindered by the spatial sparsity of ground monitoring networks. While traditional data-driven methods attempt to bridge this gap using satellite Aerosol Optical Depth (AOD), they often suffer from severe, non-random data missingness (e.g., due to cloud cover or nighttime) and inversion biases. To overcome these limitations, this study proposes the Spatiotemporal Physics-Guided Inference Network (SPIN), a novel framework designed for inductive spatiotemporal kriging. Unlike conventional approaches, SPIN synergistically integrates domain knowledge into deep learning by explicitly modeling physical advection and diffusion processes via parallel graph kernels. Crucially, we introduce a paradigm-shifting training strategy: rather than using error-prone AOD as a direct input, we repurpose it as a spatial gradient constraint within the loss function. This allows the model to learn structural pollution patterns from satellite data while remaining robust to data voids. Validated in the highly polluted Beijing-Tianjin-Hebei and Surrounding Areas (BTHSA), SPIN achieves a new state-of-the-art with a Mean Absolute Error (MAE) of 9.52 ug/m^3, effectively generating continuous, physically plausible pollution fields even in unmonitored areas. This work provides a robust, low-cost, and all-weather solution for fine-grained environmental management.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARE: Turning LLMs Into Causal Reasoning Expert</title>
<link>https://arxiv.org/abs/2511.16016</link>
<guid>https://arxiv.org/abs/2511.16016</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Causal Discovery, CARE Framework, Fine-tuning, Algorithmic Clues<br /><br />Summary:<br /><br />1. Large language models (LLMs) have shown strong capabilities in reasoning and generation but struggle with identifying causal relationships, which are essential for true human-like intelligence.  
2. An exploratory study revealed that LLMs typically depend on the semantic meaning of variable names rather than observational data when attempting causal discovery, since they are not trained on structural dataset processing.  
3. The authors tried prompting LLMs with outputs from established causal discovery algorithms, treating these outputs as sufficient statistics for observational data, but this unexpectedly led to worse performance in causal discovery tasks.  
4. To overcome this limitation, the study proposes CARE, a novel framework that fine-tunes LLMs to leverage the outputs of causal discovery algorithms effectively, enhancing their causal-reasoning abilities through supervised learning.  
5. Experimental results demonstrate that a Qwen2.5-1.5B model fine-tuned with CARE significantly surpasses both traditional causal discovery algorithms and much larger state-of-the-art LLMs, showing that combining internal knowledge with external algorithmic clues improves causal inference significantly. <div>
arXiv:2511.16016v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently demonstrated impressive capabilities across a range of reasoning and generation tasks. However, research studies have shown that LLMs lack the ability to identify causal relationships, a fundamental cornerstone of human intelligence. We first conduct an exploratory investigation of LLMs' behavior when asked to perform a causal-discovery task and find that they mostly rely on the semantic meaning of variable names, ignoring the observation data. This is unsurprising, given that LLMs were never trained to process structural datasets. To first tackle this challenge, we prompt the LLMs with the outputs of established causal discovery algorithms designed for observational datasets. These algorithm outputs effectively serve as the sufficient statistics of the observation data. However, quite surprisingly, we find that prompting the LLMs with these sufficient statistics decreases the LLMs' performance in causal discovery. To address this current limitation, we propose CARE, a framework that enhances LLMs' causal-reasoning ability by teaching them to effectively utilize the outputs of established causal-discovery algorithms through supervised fine-tuning. Experimental results show that a finetuned Qwen2.5-1.5B model produced by CARE significantly outperforms both traditional causal-discovery algorithms and state-of-the-art LLMs with over a thousand times more parameters, demonstrating effective utilization of its own knowledge and the external algorithmic clues.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HGCN2SP: Hierarchical Graph Convolutional Network for Two-Stage Stochastic Programming</title>
<link>https://arxiv.org/abs/2511.16027</link>
<guid>https://arxiv.org/abs/2511.16027</guid>
<content:encoded><![CDATA[
<div> Two-stage stochastic programming, scenario selection, hierarchical graph convolutional network, reinforcement learning, computational efficiency<br /><br />Summary:<br /><br />Two-stage stochastic programming (2SP) is widely utilized for decision-making under uncertainty but faces computational challenges with numerous scenarios. Current methods for scenario selection primarily depend on clustering or Monte Carlo sampling, which do not fully leverage detailed scenario information and ignore the impact of scenario ordering on solution time. To overcome these limitations, the authors propose HGCN2SP, a novel model featuring a hierarchical graph convolutional network that encodes scenarios and their relationships at multiple levels. Trained using reinforcement learning, HGCN2SP incorporates solver feedback to guide the policy network in selecting scenarios effectively and in an optimal order. The policy network integrates hierarchical graph convolutional layers to extract relevant features and an attention-based decoder to determine scenario sequencing. Experimental evaluation on two classic 2SP problems shows that HGCN2SP achieves high-quality decisions quickly, significantly reducing computational time. Importantly, the model generalizes well to large-scale instances with many unseen variables or scenarios, demonstrating robustness and scalability beyond the training data. This enables efficient and reliable solutions for complex stochastic programming problems that were previously difficult to solve in practical timeframes. <div>
arXiv:2511.16027v1 Announce Type: new 
Abstract: Two-stage Stochastic Programming (2SP) is a standard framework for modeling decision-making problems under uncertainty. While numerous methods exist, solving such problems with many scenarios remains challenging. Selecting representative scenarios is a practical method for accelerating solutions. However, current approaches typically rely on clustering or Monte Carlo sampling, failing to integrate scenario information deeply and overlooking the significant impact of the scenario order on solving time. To address these issues, we develop HGCN2SP, a novel model with a hierarchical graph designed for 2SP problems, encoding each scenario and modeling their relationships hierarchically. The model is trained in a reinforcement learning paradigm to utilize the feedback of the solver. The policy network is equipped with a hierarchical graph convolutional network for feature encoding and an attention-based decoder for scenario selection in proper order. Evaluation of two classic 2SP problems demonstrates that HGCN2SP provides high-quality decisions in a short computational time. Furthermore, HGCN2SP exhibits remarkable generalization capabilities in handling large-scale instances, even with a substantial number of variables or scenarios that were unseen during the training phase.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning</title>
<link>https://arxiv.org/abs/2511.16043</link>
<guid>https://arxiv.org/abs/2511.16043</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, self-evolution, co-evolution, tool integration, curriculum learning<br /><br />Summary: Agent0 is a novel fully autonomous framework designed to evolve high-performing Large Language Model (LLM) agents without relying on external human-curated data. It addresses limitations in existing self-evolution approaches, such as single-round interactions and restricted model capabilities, by implementing multi-step co-evolution between two agents derived from the same base LLM. These two agents include a curriculum agent that generates increasingly challenging tasks and an executor agent that learns to solve them. The framework incorporates external tool integration to enhance the executor‚Äôs problem-solving abilities, which in turn drives the curriculum agent to produce more complex and tool-aware challenges. This interaction creates a self-reinforcing cycle that continuously improves the quality of curricula and the agents‚Äô overall performance. Empirical results show that Agent0 significantly boosts reasoning skills, improving the Qwen3-8B-Base model‚Äôs performance by 18% in mathematical reasoning and 24% in general reasoning benchmarks. The framework thus offers a scalable, data-independent pathway for advancing autonomous LLM agents. The research code is publicly available, enabling further exploration and development. <div>
arXiv:2511.16043v1 Announce Type: new 
Abstract: Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Change-of-Basis Pruning via Rotational Invariance</title>
<link>https://arxiv.org/abs/2511.16061</link>
<guid>https://arxiv.org/abs/2511.16061</guid>
<content:encoded><![CDATA[
<div> Structured pruning, Change-of-basis pruning, Rotational invariance, Two-subspace radial activations, Structured pruning accuracy<br /><br />Summary:<br /><br />1. The paper addresses the limitations of structured pruning, which removes entire neurons or channels but relies heavily on how importance is distributed across the representation space. 2. Change-of-basis (CoB) pruning is introduced as a technique that applies orthogonal linear transformations to focus importance within specific dimensions, enhancing pruning effectiveness. 3. A key challenge is that many standard deep learning architectures lack invariance to such orthogonal transformations, limiting the applicability of CoB pruning. 4. To resolve this, the authors propose two-subspace radial activations (TSRAs), a new activation function family that maintains rotational invariance by independently applying orthogonal transformations within two activation subspaces. 5. This invariance allows CoB transformations to be merged with surrounding weights without increasing parameter count. 6. While the modifications to enforce rotational invariance cause a slight accuracy drop of 4.52% compared to ReLU baselines, experiments on VGG-16 with CIFAR-10 demonstrate promising results. 7. CoB pruning combined with TSRAs improves accuracy consistently over the TSRA baseline across all pruning ratios, extending reliable structured pruning from about 30% to 70% parameter removal without fine-tuning. 8. Under threshold-based pruning, CoB pruning removes 90-96% of parameters with only a 1-6% accuracy drop after fine-tuning. 9. The paper presents this as a proof-of-concept, suggesting that rotationally invariant architectures may be a promising future direction for effective change-of-basis structured pruning. <div>
arXiv:2511.16061v1 Announce Type: new 
Abstract: Structured pruning removes entire neurons or channels, but its effectiveness depends on how importance is distributed across the representation space. Change-of-basis (CoB) pruning addresses this challenge by applying orthogonal linear transformations that concentrate importance within certain dimensions. However, many standard deep learning architectures are not inherently invariant to such transformations. To enable compatibility, we introduce two-subspace radial activations (TSRAs): an activation family that is invariant to orthogonal linear transformations applied independently within its two activation subspaces. This invariance allows CoB transformations to be merged into surrounding weights without incurring extra parameters. We position this work as a proof-of-concept that a rotationally invariant design may offer a principled approach towards change-of-basis pruning. We do not provide an analysis of multiple TSRA candidates nor do we explore weight initialization for any TSRAs. These limitations, combined with other necessary modifications we make to permit rotational invariance, result in a slight accuracy drop of $4.52\%$ compared to a ReLU-based control. However, using activation-magnitude importance, VGG-16 implementing our CoB+TSRA framework shows encouraging results on CIFAR-10. Under fixed-ratio structured pruning, CoB improves accuracy over a TSRA baseline at all pruning ratios and extends reliable pruning frontier from roughly $30\%$ to $70\%$ of parameters without post-prune fine tuning. Under threshold-based pruning strategies, CoB prunes $90-96\%$ of parameters while maintaining $1-6\%$ accuracy drop after fine-tuning. Together, these results indicate that rotationally invariant architectures may offer a promising path towards CoB pruning.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gauge-Equivariant Graph Networks via Self-Interference Cancellation</title>
<link>https://arxiv.org/abs/2511.16062</link>
<guid>https://arxiv.org/abs/2511.16062</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, heterophily, gauge-equivariant, self-interference cancellation, phase connection  

<br /><br />Summary:  
1. The paper addresses the challenge that Graph Neural Networks (GNNs) face on heterophilous graphs, where traditional models struggle due to self-reinforcing and phase-inconsistent signals.  
2. It proposes a novel model called Gauge-Equivariant Graph Network with Self-Interference Cancellation (GESC), which innovatively replaces the usual additive aggregation with a projection-based interference mechanism to better handle signal interactions.  
3. Unlike existing magnetic or gauge-equivariant GNNs that mainly emphasize phase management in spectral filtering but depend heavily on scalar weighting, GESC introduces a \(\mathrm{U}(1)\) phase connection combined with a rank-1 projection. This projection suppresses self-parallel components before applying attention, reducing undesired signal amplification.  
4. The method also incorporates a sign- and phase-aware gating function that modulates neighbor influence by attenuating components aligned with the current node states, effectively acting as a local notch filter targeting low-frequency modes.  
5. Extensive experiments on a wide range of graph benchmarks demonstrate that GESC consistently outperforms recent state-of-the-art models, providing a unified, interference-aware framework that enhances message passing in GNNs. Additionally, the authors have made their code publicly available to facilitate further research and development. <div>
arXiv:2511.16062v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) excel on homophilous graphs but often fail under heterophily due to self-reinforcing and phase-inconsistent signals. We propose a Gauge-Equivariant Graph Network with Self-Interference Cancellation (GESC), which replaces additive aggregation with a projection-based interference mechanism. Unlike prior magnetic or gauge-equivariant GNNs that typically focus on phase handling in spectral filtering while largely relying on scalar weighting, GESC introduces a $\mathrm{U}(1)$ phase connection followed by a rank-1 projection that attenuates self-parallel components before attention. A sign- and phase-aware gate further regulates neighbor influence, attenuating components aligned with current node states and acting as a local notch on low-frequency modes. Across diverse graph benchmarks, our method consistently outperforms recent state-of-the-art models while offering a unified, interference-aware view of message passing. Our code is available at \href{here}{https://anonymous.4open.science/r/GESC-1B22}.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ILoRA: Federated Learning with Low-Rank Adaptation for Heterogeneous Client Aggregation</title>
<link>https://arxiv.org/abs/2511.16069</link>
<guid>https://arxiv.org/abs/2511.16069</guid>
<content:encoded><![CDATA[
<div> Low-Rank Adaptation, Federated Learning, Client Heterogeneity, QR Initialization, Client Drift<br /><br />Summary:  
This paper addresses critical challenges faced by Federated Learning with Low-Rank Adaptation (LoRA) under client heterogeneity. First, it identifies Initialization-Induced Instability caused by random initialization that misaligns client subspaces, leading to inconsistent starting points. Second, it highlights Rank Incompatibility and Aggregation Error due to averaging LoRA parameters of varying ranks, which biases the global model update. Third, it recognizes exacerbated Client Drift under Non-IID data distributions, which harms the generalization ability of the global model. To resolve these challenges, the authors propose ILoRA, a unified framework incorporating three innovations. The first innovation is a QR-based orthonormal initialization to harmonize client starting subspaces. The second is a Concatenated QR Aggregation mechanism that combines heterogeneous-rank updates through concatenation and decomposition, preserving full information while aligning dimensions. The third innovation involves using an AdamW optimizer with rank-aware control variates, designed to correct local updates and mitigate client drift effectively. The framework comes with theoretical convergence guarantees. Extensive experiments on vision and NLP datasets demonstrate that ILoRA consistently outperforms existing federated LoRA approaches by achieving higher accuracy and improved convergence stability across heterogeneous clients and non-IID data settings. <div>
arXiv:2511.16069v1 Announce Type: new 
Abstract: Federated Learning with Low-Rank Adaptation (LoRA) faces three critical challenges under client heterogeneity: (1) Initialization-Induced Instability due to random initialization misaligning client subspaces; (2) Rank Incompatibility and Aggregation Error when averaging LoRA parameters of different ranks, which biases the global model; and (3) exacerbated Client Drift under Non-IID Data, impairing generalization. To address these challenges, we propose ILoRA, a unified framework that integrates three core innovations: a QR-based orthonormal initialization to ensure all clients start in a coherent subspace; a Concatenated QR Aggregation mechanism that fuses heterogeneous-rank updates via concatenation and decomposition, preserving information while maintaining dimension alignment; and an AdamW optimizer with rank-aware control variates to correct local updates and mitigate client drift. Supported by theoretical convergence guarantees, extensive experiments on vision and NLP benchmarks demonstrate that ILoRA consistently achieves superior accuracy and convergence stability compared to existing federated LoRA methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Mathematical Framework for Custom Reward Functions in Job Application Evaluation using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.16073</link>
<guid>https://arxiv.org/abs/2511.16073</guid>
<content:encoded><![CDATA[
<div> Keywords: Applicant Tracking Systems, small language model, GRPO, reinforcement learning, reward function<br /><br />Summary:<br /><br />This article addresses the limitations of conventional Applicant Tracking Systems (ATS), which rely heavily on inflexible keyword matching and often reject qualified candidates due to minor semantic mismatches. The authors propose a novel two-step fine-tuning process for a small language model (under 600 million parameters) to improve resume evaluation. Initially, the model undergoes Supervised Fine-Tuning (SFT) to establish a robust baseline. Subsequently, the model is optimized using Reinforcement Learning (RL), specifically via GRPO, guided by a new multi-component reward function designed to assess candidates more holistically beyond simple keyword matches. Early experiments revealed challenges with reward hacking caused by aggressive penalties, which led to undesirable model behaviors. This issue was resolved through iterative refinement of the reward function and training hyperparameters, resulting in a stable ‚Äúgentle polishing process.‚Äù The final GRPO-polished model demonstrates strong real-world performance, achieving 91% accuracy on unseen test data, with a recall of 0.85 for the ‚ÄòSELECTED‚Äô class and perfect precision (1.0). These outcomes confirm that this carefully executed two-step fine-tuning approach can effectively refine small language models to conduct nuanced, human-like candidate scoring, overcoming the key weaknesses of both traditional ATS and naive RL methods. <div>
arXiv:2511.16073v1 Announce Type: new 
Abstract: Conventional Applicant Tracking Systems (ATS) tend to be inflexible keyword-matchers, and deny gifted candidates a role due to a few minor semantic mismatches. This article describes a new two-step process to design a more refined resume evaluation model based on a small language model (<600M parameters) that is finetuned using GRPO on a custom reward function. To begin with, Supervised Fine-Tuning (SFT) was used to build a solid baseline model. Second, this SFT model was also optimized with the help of Reinforcement Learning (RL) through GRPO under the guidance of a new, multi-component reward function that can holistically assess candidates beyond simple keyword matching. We indicate that the RL application presents a critical problem of reward hacking due to the initial experiments of aggressive penalties, which produces faulty, excessively negative model behaviors. We have overcome this challenge by refining the reward function repeatedly and training hyperparameters into a stable "gentle polishing process" of the reward function. Our resulting GRPO-polished model demonstrates significant real-world efficacy, achieving a final accuracy of 91% on unseen test data. The model shows a strong ability to correctly identify qualified candidates (recall of 0.85 for the 'SELECTED' class) while also showing exceptional precision (1.0), confirming its reliability. These results indicate that a properly executed, two-step fine-tuning procedure can indeed effectively refine a small language model to be able to conduct fine-tuned and human-like candidate scoring, overcoming the drawbacks of both traditional ATS and naive RL usage.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>L-JacobiNet and S-JacobiNet: An Analysis of Adaptive Generalization, Stabilization, and Spectral Domain Trade-offs in GNNs</title>
<link>https://arxiv.org/abs/2511.16081</link>
<guid>https://arxiv.org/abs/2511.16081</guid>
<content:encoded><![CDATA[
<div> Keywords: Spectral GNNs, ChebyNet, Adaptive Orthogonal Polynomial Filter, heterophily, numerical stability<br /><br />Summary:<br /><br />1) Spectral GNNs such as ChebyNet are limited by heterophily and over-smoothing due to their static, low-pass filter design.<br />2) This study explores the "Adaptive Orthogonal Polynomial Filter" (AOPF) framework and introduces two models operating in the [-1, 1] domain: L-JacobiNet, which generalizes ChebyNet with learnable alpha and beta shape parameters; and S-JacobiNet, a novel baseline combining static ChebyNet with LayerNorm stabilization.<br />3) The models are compared to AOPFs operating in the [0, ‚àû) domain like LaguerreNet, revealing significant trade-offs where the [0, ‚àû) domain better handles heterophily, while the [-1, 1] domain offers superior numerical stability at higher polynomial orders (K > 20).<br />4) The primary issue with ChebyNet is identified not as its static filtering but rather its lack of stabilization.<br />5) Empirically, S-JacobiNet, the static ChebyNet enhanced with LayerNorm, outperforms the adaptive L-JacobiNet on 4 out of 5 benchmark datasets, highlighting S-JacobiNet as an effective and undervalued baseline and indicating that adaptation within the [-1, 1] domain may induce overfitting. <div>
arXiv:2511.16081v1 Announce Type: new 
Abstract: Spectral GNNs, like ChebyNet, are limited by heterophily and over-smoothing due to their static, low-pass filter design. This work investigates the "Adaptive Orthogonal Polynomial Filter" (AOPF) class as a solution. We introduce two models operating in the [-1, 1] domain: 1) `L-JacobiNet`, the adaptive generalization of `ChebyNet` with learnable alpha, beta shape parameters, and 2) `S-JacobiNet`, a novel baseline representing a LayerNorm-stabilized static `ChebyNet`. Our analysis, comparing these models against AOPFs in the [0, infty) domain (e.g., `LaguerreNet`), reveals critical, previously unknown trade-offs. We find that the [0, infty) domain is superior for modeling heterophily, while the [-1, 1] domain (Jacobi) provides superior numerical stability at high K (K>20). Most significantly, we discover that `ChebyNet`'s main flaw is stabilization, not its static nature. Our static `S-JacobiNet` (ChebyNet+LayerNorm) outperforms the adaptive `L-JacobiNet` on 4 out of 5 benchmark datasets, identifying `S-JacobiNet` as a powerful, overlooked baseline and suggesting that adaptation in the [-1, 1] domain can lead to overfitting.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AssayMatch: Learning to Select Data for Molecular Activity Models</title>
<link>https://arxiv.org/abs/2511.16087</link>
<guid>https://arxiv.org/abs/2511.16087</guid>
<content:encoded><![CDATA[
<div> Keywords: AssayMatch, data selection, bioactivity data, machine learning, drug discovery<br /><br />Summary:<br /><br />The article introduces AssayMatch, a novel framework aimed at improving machine learning model performance in drug discovery by selecting smaller, more homogeneous training datasets tailored to a specific test set. It addresses the common issue of noise and variability caused by aggregating bioactivity data from diverse sources with differing experimental protocols. AssayMatch utilizes data attribution methods to assess how each training assay contributes to overall model performance. These attribution scores help fine-tune language embeddings of assay descriptions, capturing not only semantic similarity but also assay compatibility. Unlike prior methods, AssayMatch can select relevant training data even when test set labels are unknown, reflecting real-world scenarios in drug discovery. At test time, the fine-tuned embeddings rank available training data to prioritize the most compatible assays. Experimental results across two machine learning architectures show that models trained on AssayMatch-selected data outperform those trained on full datasets, improving predictions for 9 out of 12 model-target pairs. The framework effectively filters out noisy or harmful experiments, enhancing predictive power and data efficiency. AssayMatch offers a data-driven approach to curate high-quality datasets, reducing dataset noise and boosting drug discovery model accuracy. The tool is publicly available at the linked GitHub repository. <div>
arXiv:2511.16087v1 Announce Type: new 
Abstract: The performance of machine learning models in drug discovery is highly dependent on the quality and consistency of the underlying training data. Due to limitations in dataset sizes, many models are trained by aggregating bioactivity data from diverse sources, including public databases such as ChEMBL. However, this approach often introduces significant noise due to variability in experimental protocols. We introduce AssayMatch, a framework for data selection that builds smaller, more homogenous training sets attuned to the test set of interest. AssayMatch leverages data attribution methods to quantify the contribution of each training assay to model performance. These attribution scores are used to finetune language embeddings of text-based assay descriptions to capture not just semantic similarity, but also the compatibility between assays. Unlike existing data attribution methods, our approach enables data selection for a test set with unknown labels, mirroring real-world drug discovery campaigns where the activities of candidate molecules are not known in advance. At test time, embeddings finetuned with AssayMatch are used to rank all available training data. We demonstrate that models trained on data selected by AssayMatch are able to surpass the performance of the model trained on the complete dataset, highlighting its ability to effectively filter out harmful or noisy experiments. We perform experiments on two common machine learning architectures and see increased prediction capability over a strong language-only baseline for 9/12 model-target pairs. AssayMatch provides a data-driven mechanism to curate higher-quality datasets, reducing noise from incompatible experiments and improving the predictive power and data efficiency of models for drug discovery. AssayMatch is available at https://github.com/Ozymandias314/AssayMatch.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Estimation Bias with Representation Learning in TD Error-Driven Regularization</title>
<link>https://arxiv.org/abs/2511.16090</link>
<guid>https://arxiv.org/abs/2511.16090</guid>
<content:encoded><![CDATA[
<div> Deterministic policy gradient, double actors, bias control, representation learning, continuous control<br /><br />Summary: This paper addresses value estimation biases in deterministic policy gradient algorithms for continuous control tasks, which can adversely affect performance. While previous works have used double critics to reduce overestimation bias, the potential of double actors for exploration and bias control remains underexplored. The authors build on temporal-difference error-driven regularization (TDDR) within a double actor-critic framework, proposing three convex combination strategies‚Äîsymmetric and asymmetric‚Äîthat allow flexible balancing between pessimistic (overestimation mitigating) and optimistic (exploration enhancing) estimates. A single hyperparameter governs this tunable control mechanism, enabling the model to adjust bias levels as needed. Additionally, the paper integrates augmented state and action representations into actor and critic networks to enhance representation learning and boost performance. Extensive experiments demonstrate that the proposed approach consistently outperforms benchmark methods, indicating that both overestimation and underestimation biases can be exploited differently depending on the environment. This reveals the importance of tunable bias control in improving continuous control algorithms and highlights the benefits of leveraging double actors alongside double critics for more effective policy learning. <div>
arXiv:2511.16090v1 Announce Type: new 
Abstract: Deterministic policy gradient algorithms for continuous control suffer from value estimation biases that degrade performance. While double critics reduce such biases, the exploration potential of double actors remains underexplored. Building on temporal-difference error-driven regularization (TDDR), a double actor-critic framework, this work introduces enhanced methods to achieve flexible bias control and stronger representation learning. We propose three convex combination strategies, symmetric and asymmetric, that balance pessimistic estimates to mitigate overestimation and optimistic exploration via double actors to alleviate underestimation. A single hyperparameter governs this mechanism, enabling tunable control across the bias spectrum. To further improve performance, we integrate augmented state and action representations into the actor and critic networks. Extensive experiments show that our approach consistently outperforms benchmarks, demonstrating the value of tunable bias and revealing that both overestimation and underestimation can be exploited differently depending on the environment.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybSpecNet: A Critical Analysis of Architectural Instability in Hybrid-Domain Spectral GNNs</title>
<link>https://arxiv.org/abs/2511.16101</link>
<guid>https://arxiv.org/abs/2511.16101</guid>
<content:encoded><![CDATA[
<div> Keywords: Spectral Graph Neural Networks, Stability-vs-Adaptivity trade-off, ChebyNet, KrawtchoukNet, Hybrid-domain GNN

<br /><br />Summary:  
This paper addresses the fundamental trade-off between stability and adaptivity in Spectral Graph Neural Networks (GNNs), which is influenced by the choice of spectral domain. Filters operating in the bounded [-1, 1] domain, like ChebyNet, offer numerical stability at high polynomial degrees but are limited to static, low-pass filtering, failing on heterophilic graphs. In contrast, filters defined over the semi-infinite [0, ‚àû) domain, such as KrawtchoukNet, provide high adaptivity and achieve state-of-the-art performance on heterophily by learning non-low-pass filters, yet they suffer from numerical instability at high polynomial degrees, causing performance collapse. To reconcile these issues, the authors propose HybSpecNet, a hybrid-domain GNN combining a stable ChebyNet branch with an adaptive KrawtchoukNet branch. Initially, a naive hybrid model fusing both branches via concatenation demonstrated strong performance on both homophilic and heterophilic benchmarks at low polynomial degrees. However, it failed stability tests at higher polynomial degrees (K=25) due to an issue termed "Instability Poisoning," where unstable gradients from the adaptive branch disrupt training. To overcome this, the paper introduces an advanced architecture employing "Late Fusion," which isolates gradient flows between the branches. This design eliminates instability, maintaining model stability up to K=30 while preserving state-of-the-art performance across diverse graph types, thereby providing a robust solution for hybrid GNN design. <div>
arXiv:2511.16101v1 Announce Type: new 
Abstract: Spectral Graph Neural Networks offer a principled approach to graph filtering but face a fundamental "Stability-vs-Adaptivity" trade-off. This trade-off is dictated by the choice of spectral domain. Filters in the finite [-1, 1] domain (e.g., ChebyNet) are numerically stable at high polynomial degrees (K) but are static and low-pass, causing them to fail on heterophilic graphs. Conversely, filters in the semi-infinite [0, infty) domain (e.g., KrawtchoukNet) are highly adaptive and achieve SOTA results on heterophily by learning non-low-pass responses. However, as we demonstrate, these adaptive filters can also suffer from numerical instability, leading to catastrophic performance collapse at high K. In this paper, we propose to resolve this trade-off by designing a hybrid-domain GNN, HybSpecNet, which combines a stable `ChebyNet` branch with an adaptive `KrawtchoukNet` branch. We first demonstrate that a "naive" hybrid architecture, which fuses the branches via concatenation, successfully unifies performance at low K, achieving strong results on both homophilic and heterophilic benchmarks. However, we then prove that this naive architecture fails the stability test. Our K-ablation experiments show that this architecture catastrophically collapses at K=25, exactly mirroring the collapse of its unstable `KrawtchoukNet` branch. We identify this critical finding as "Instability Poisoning," where `NaN`/`Inf` gradients from the adaptive branch destroy the training of the model. Finally, we propose and validate an advanced architecture that uses "Late Fusion" to completely isolate the gradient pathways. We demonstrate that this successfully solves the instability problem, remaining perfectly stable up to K=30 while retaining its SOTA performance across all graph types. This work identifies a critical architectural pitfall in hybrid GNN design and provides the robust architectural solution.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pathlet Variational Auto-Encoder for Robust Trajectory Generation</title>
<link>https://arxiv.org/abs/2511.16105</link>
<guid>https://arxiv.org/abs/2511.16105</guid>
<content:encoded><![CDATA[
<div> Keywords: trajectory generation, privacy-preserving, pathlet representation, Variational Autoencoder, noisy data<br /><br />Summary:  
This paper addresses challenges in trajectory generation for urban mobility studies, particularly focusing on robustness and interpretability of models when handling noisy real-world data. The authors propose a novel deep generative model that leverages the regular structure of urban trajectories by using a pathlet representation, which encodes trajectories as binary vectors linked to a learned dictionary of trajectory segments. The model incorporates a probabilistic graphical framework combining a Variational Autoencoder (VAE) and a linear decoder to jointly learn latent embeddings of pathlet representations and the dictionary itself. This approach not only captures underlying mobility patterns but also supports a conditional version for generating customized trajectories constrained by temporal and spatial factors. Experimental results on two real-world trajectory datasets show significant relative improvements of 35.4% and 26.3% over strong baseline methods, demonstrating the model's effectiveness in modeling noisy data distributions. Additionally, the generated trajectories are versatile and applicable to multiple downstream tasks such as trajectory prediction and data denoising. The framework also offers substantial efficiency gains, reducing computational time by 64.8% and GPU memory usage by 56.5% compared to previous state-of-the-art trajectory generation approaches. This combination of robustness, interpretability, and efficiency enhances trustworthiness and practical usability in trajectory-based applications. <div>
arXiv:2511.16105v1 Announce Type: new 
Abstract: Trajectory generation has recently drawn growing interest in privacy-preserving urban mobility studies and location-based service applications. Although many studies have used deep learning or generative AI methods to model trajectories and have achieved promising results, the robustness and interpretability of such models are largely unexplored. This limits the application of trajectory generation algorithms on noisy real-world data and their trustworthiness in downstream tasks. To address this issue, we exploit the regular structure in urban trajectories and propose a deep generative model based on the pathlet representation, which encode trajectories with binary vectors associated with a learned dictionary of trajectory segments. Specifically, we introduce a probabilistic graphical model to describe the trajectory generation process, which includes a Variational Autoencoder (VAE) component and a linear decoder component. During training, the model can simultaneously learn the latent embedding of pathlet representations and the pathlet dictionary that captures mobility patterns in the trajectory dataset. The conditional version of our model can also be used to generate customized trajectories based on temporal and spatial constraints.
  Our model can effectively learn data distribution even using noisy data, achieving relative improvements of $35.4\%$ and $26.3\%$ over strong baselines on two real-world trajectory datasets. Moreover, the generated trajectories can be conveniently utilized for multiple downstream tasks, including trajectory prediction and data denoising. Lastly, the framework design offers a significant efficiency advantage, saving $64.8\%$ of the time and $56.5\%$ of GPU memory compared to previous approaches.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Interpretability-Guided Framework for Responsible Synthetic Data Generation in Emotional Text</title>
<link>https://arxiv.org/abs/2511.16132</link>
<guid>https://arxiv.org/abs/2511.16132</guid>
<content:encoded><![CDATA[
<div> Emotion recognition, synthetic data, SHAP, large language models, interpretability<br /><br />Summary:<br /><br />1. The paper addresses the challenge of emotion recognition on social media, highlighting the increasing difficulty of obtaining training data due to rising API costs and platform restrictions.<br /><br />2. It proposes an interpretability-guided framework that uses Shapley Additive Explanations (SHAP) to provide principled guidance for generating synthetic data via large language models (LLMs).<br /><br />3. Results show that with enough seed data, the SHAP-guided synthetic data achieves performance comparable to real data, significantly outperforms naive synthetic data generation methods, and notably improves classification outcomes for underrepresented emotion categories.<br /><br />4. Linguistic analysis reveals limitations of synthetic data, including reduced vocabulary diversity and fewer personal or temporally nuanced expressions, compared to authentic social media posts.<br /><br />5. The work offers a practical approach to responsible synthetic data generation while critically emphasizing the trade-offs between the utility of synthetic data and maintaining real-world language authenticity, underscoring the importance of this balance for the future of trustworthy AI. <div>
arXiv:2511.16132v1 Announce Type: new 
Abstract: Emotion recognition from social media is critical for understanding public sentiment, but accessing training data has become prohibitively expensive due to escalating API costs and platform restrictions. We introduce an interpretability-guided framework where Shapley Additive Explanations (SHAP) provide principled guidance for LLM-based synthetic data generation. With sufficient seed data, SHAP-guided approach matches real data performance, significantly outperforms na\"ive generation, and substantially improves classification for underrepresented emotion classes. However, our linguistic analysis reveals that synthetic text exhibits reduced vocabulary richness and fewer personal or temporally complex expressions than authentic posts. This work provides both a practical framework for responsible synthetic data generation and a critical perspective on its limitations, underscoring that the future of trustworthy AI depends on navigating the trade-offs between synthetic utility and real-world authenticity.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Labels Matter More Than Models: Quantifying the Benefit of Supervised Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.16145</link>
<guid>https://arxiv.org/abs/2511.16145</guid>
<content:encoded><![CDATA[
<div> Keywords: Time series anomaly detection, supervised learning, unsupervised learning, label scarcity, STAND  

<br /><br />Summary:  
This paper addresses the challenge of time series anomaly detection (TSAD), particularly under the common constraint of limited labeled anomaly data. The authors critique the prevalent focus on unsupervised methods that use complex architectures to model normal data distributions, highlighting that these approaches often neglect the benefits that even small amounts of labeled anomalies can provide. To bridge this gap, they present STAND, a simple yet effective supervised baseline designed to exploit limited anomaly labels efficiently. The study conducts the first systematic comparison between supervised and unsupervised TSAD paradigms across five public datasets. Results reveal that: (1) The availability and use of labeled data significantly outweigh the benefits of architectural complexity, with simple supervised models outperforming more complex unsupervised methods under limited labeling conditions; (2) Incorporating minimal supervision yields greater performance improvements than innovations focused solely on model architecture; (3) Practically, STAND offers better consistency in predictions and more accurate anomaly localization relative to unsupervised approaches. The findings advocate a shift towards a data-centric focus in TSAD research, emphasizing the strategic use of labels rather than solely pursuing complex algorithmic development. The authors have made the STAND code publicly accessible to encourage further exploration and adoption. <div>
arXiv:2511.16145v1 Announce Type: new 
Abstract: Time series anomaly detection (TSAD) is a critical data mining task often constrained by label scarcity. Consequently, current research predominantly focuses on Unsupervised Time-series Anomaly Detection (UTAD), relying on complex architectures to model normal data distributions. However, this approach often overlooks the significant performance gains available from limited anomaly labels achievable in practical scenarios. This paper challenges the premise that architectural complexity is the optimal path for TSAD. We conduct the first methodical comparison between supervised and unsupervised paradigms and introduce STAND, a streamlined supervised baseline. Extensive experiments on five public datasets demonstrate that: (1) Labels matter more than models: under a limited labeling budget, simple supervised models significantly outperform complex state-of-the-art unsupervised methods; (2) Supervision yields higher returns: the performance gain from minimal supervision far exceeds that from architectural innovations; and (3) Practicality: STAND exhibits superior prediction consistency and anomaly localization compared to unsupervised counterparts. These findings advocate for a data-centric shift in TSAD research, emphasizing label utilization over purely algorithmic complexity. The code is publicly available at https://github.com/EmorZz1G/STAND.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Nuclear Reactor Core Simulation through Data-Based Surrogate Models</title>
<link>https://arxiv.org/abs/2511.16148</link>
<guid>https://arxiv.org/abs/2511.16148</guid>
<content:encoded><![CDATA[
<div> Keywords: Nuclear Power Plants, Model Predictive Control, Data-driven Simulation, Surrogate Models, Nuclear Reactor Core Simulation<br /><br />Summary:  
1. The increasing integration of renewable energies necessitates greater flexibility in Nuclear Power Plants (NPPs).  
2. Framatome has developed an Operator Assistance Predictive System (OAPS) based on Model Predictive Control (MPC) to address this challenge.  
3. This paper proposes enhancements to MPC methods by introducing data-driven simulation schemes as surrogate models.  
4. From a complex set of nonlinear stiff ordinary differential equations (ODEs) describing reactor dynamics, two surrogate models are developed to act as alternative simulation approaches.  
5. These surrogate models include both purely data-driven and physics-informed techniques, aiming to improve computational efficiency.  
6. The proposed models can integrate the complex, nonlinear dynamics of a nuclear reactor core much faster than traditional simulations.  
7. Computational time reductions of up to 1000 times are demonstrated, highlighting the efficiency gains.  
8. This approach facilitates faster and more flexible simulation capabilities crucial for real-time operator assistance and the integration of variable renewable energy sources.  
9. Overall, the research contributes to advancing MPC frameworks in NPPs by combining data-driven insights with physics-based understanding to optimize reactor core simulations. <div>
arXiv:2511.16148v1 Announce Type: new 
Abstract: In recent years, there has been an increasing need for Nuclear Power Plants (NPPs) to improve flexibility in order to match the rapid growth of renewable energies. The Operator Assistance Predictive System (OAPS) developed by Framatome addresses this problem through Model Predictive Control (MPC). In this work, we aim to improve MPC methods through data-driven simulation schemes. Thus, from a set of nonlinear stiff ordinary differential equations (ODEs), this paper introduces two surrogate models acting as alternative simulation schemes to enhance nuclear reactor core simulation. We show that both data-driven and physics-informed models can rapidly integrate complex dynamics, with a very low computational time (up to 1000x time reduction).
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Skilled and Reliable Daily Probabilistic Forecasts of Wind Power at Subseasonal-to-Seasonal Timescales over France</title>
<link>https://arxiv.org/abs/2511.16164</link>
<guid>https://arxiv.org/abs/2511.16164</guid>
<content:encoded><![CDATA[
<div> Keywords: wind power forecasting, subseasonal-to-seasonal forecasts, ECMWF, probabilistic forecasting, bias correction<br /><br />Summary:  
1. Accurate and reliable wind power forecasts are essential for maintaining grid stability, balancing supply and demand, and managing market risks.  
2. While short-term weather forecasts have been widely used for short-term renewable energy predictions, longer-term forecasts, especially beyond several days, require further research.  
3. Recent advances in subseasonal-to-seasonal probabilistic weather forecasting have improved skill but applying these forecasts to wind power prediction typically depends on temporal and spatial aggregation to achieve acceptable performance.  
4. This study introduces a forecasting pipeline that transforms ECMWF subseasonal-to-seasonal probabilistic weather forecasts into wind power forecasts with lead times ranging from 1 to 46 days at a daily resolution.  
5. The framework includes post-processing steps that correct biases and address insufficient dispersion in the ensemble weather forecasts.  
6. Results demonstrate that the proposed method outperforms a climatological baseline by 50% in both Continuous Ranked Probability Skill Score and Ensemble Mean Squared Error metrics.  
7. Additionally, the forecasts achieve near-perfect calibration for lead times between 15 and 46 days, indicating reliable probabilistic predictions over medium-range time horizons. <div>
arXiv:2511.16164v1 Announce Type: new 
Abstract: Accurate and reliable wind power forecasts are crucial for grid stability, balancing supply and demand, and market risk management. Even though short-term weather forecasts have been thoroughly used to provide short-term renewable power predictions, forecasts involving longer prediction horizons still need investigations. Despite the recent progress in subseasonal-to-seasonal weather probabilistic forecasting, their use for wind power prediction usually involves both temporal and spatial aggregation achieve reasonable skill. In this study, we present a forecasting pipeline enabling to transform ECMWF subseasonal-to-seasonal weather forecasts into wind power forecasts for lead times ranging from 1 day to 46 days at daily resolution. This framework also include post-processing of the resulting power ensembles to account for the biases and lack of dispersion of the weather forecasts. We show that our method is able to outperform a climatological baseline by 50 % in terms of both Continuous Ranked Probability Skill Score and Ensemble Mean Squared Error while also providing near perfect calibration of the forecasts for lead times ranging from 15 to 46 days.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CausalMamba: Interpretable State Space Modeling for Temporal Rumor Causality</title>
<link>https://arxiv.org/abs/2511.16191</link>
<guid>https://arxiv.org/abs/2511.16191</guid>
<content:encoded><![CDATA[
<div> Rumor detection, causal discovery, graph convolutional networks, misinformation spread, social media<br /><br />Summary:<br /><br />Rumor detection on social media is challenging due to complex propagation dynamics and limited model interpretability. This paper introduces CausalMamba, a novel framework that combines Mamba-based sequence modeling, graph convolutional networks (GCNs), and differentiable causal discovery using NOTEARS. CausalMamba jointly represents temporal tweet sequences and reply structures to better capture the spread of misinformation. The framework uncovers latent causal graphs, allowing identification of influential nodes within each rumor propagation chain. Experimental evaluation on the Twitter15 dataset shows that CausalMamba achieves competitive classification performance against strong baseline models. Additionally, it uniquely enables counterfactual intervention analysis to explore how removing key causal nodes affects graph connectivity. Qualitative results demonstrate that eliminating top-ranked causal nodes significantly disrupts rumor propagation, offering interpretable insights into misinformation dynamics. Overall, CausalMamba provides a unified approach that not only classifies rumors but also analyzes influence within propagation structures. This work paves the way for more explainable, actionable, and effective misinformation detection systems on social media platforms. <div>
arXiv:2511.16191v1 Announce Type: new 
Abstract: Rumor detection on social media remains a challenging task due to the complex propagation dynamics and the limited interpretability of existing models. While recent neural architectures capture content and structural features, they often fail to reveal the underlying causal mechanisms of misinformation spread. We propose CausalMamba, a novel framework that integrates Mamba-based sequence modeling, graph convolutional networks (GCNs), and differentiable causal discovery via NOTEARS. CausalMamba learns joint representations of temporal tweet sequences and reply structures, while uncovering latent causal graphs to identify influential nodes within each propagation chain. Experiments on the Twitter15 dataset show that our model achieves competitive classification performance compared to strong baselines, and uniquely enables counterfactual intervention analysis. Qualitative results demonstrate that removing top-ranked causal nodes significantly alters graph connectivity, offering interpretable insights into rumor dynamics. Our framework provides a unified approach for rumor classification and influence analysis, paving the way for more explainable and actionable misinformation detection systems.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Switching Framework for Online Interval Scheduling with Predictions</title>
<link>https://arxiv.org/abs/2511.16194</link>
<guid>https://arxiv.org/abs/2511.16194</guid>
<content:encoded><![CDATA[
<div> Keywords: online interval scheduling, learning-augmented algorithms, SemiTrust-and-Switch framework, robustness, prediction errors  

<br /><br />Summary:  
1. The paper studies the online interval scheduling problem in an irrevocable setting where intervals must be accepted or rejected upon arrival, aiming to maximize the total length of accepted non-overlapping intervals.  
2. The authors focus on the learning-augmented setting, where machine-learned predictions about the input are available, with the goal to enhance algorithmic performance by leveraging these predictions while maintaining robustness against prediction errors.  
3. They introduce the SemiTrust-and-Switch framework, which integrates prediction-based algorithms with classical robust algorithms, applicable to both deterministic and randomized approaches, providing a trade-off between performance consistency with accurate predictions and robustness against adversarial inputs.  
4. The framework is shown to be tight through lower bound proofs, indicating that the provided trade-offs are optimal in specific scenarios.  
5. Additionally, the authors design a randomized algorithm that smoothly balances reliance on predictions and classical robustness, ensuring graceful degradation of performance proportional to prediction quality, thereby achieving both robustness and smoothness in the online interval scheduling context. <div>
arXiv:2511.16194v1 Announce Type: new 
Abstract: We study online interval scheduling in the irrevocable setting, where each interval must be immediately accepted or rejected upon arrival. The objective is to maximize the total length of accepted intervals while ensuring that no two accepted intervals overlap. We consider this problem in a learning-augmented setting, where the algorithm has access to (machine-learned) predictions. The goal is to design algorithms that leverage these predictions to improve performance while maintaining robust guarantees in the presence of prediction errors.
  Our main contribution is the SemiTrust-and-Switch framework, which provides a unified approach for combining prediction-based and classical interval scheduling algorithms. This framework applies to both deterministic and randomized algorithms and captures the trade-off between consistency (performance under accurate predictions) and robustness (performance under adversarial inputs). Moreover, we provide lower bounds, proving the tightness of this framework in particular settings.
  We further design a randomized algorithm that smoothly interpolates between prediction-based and robust algorithms. This algorithm achieves both robustness and smoothness--its performance degrades gracefully with the quality of the prediction.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Synthetic Data Generation in Recruitment</title>
<link>https://arxiv.org/abs/2511.16204</link>
<guid>https://arxiv.org/abs/2511.16204</guid>
<content:encoded><![CDATA[
<div> Keywords: Synthetic Data Generation, Causal Generative Models, recruitment, fairness, candidate ranking<br /><br />Summary:  
The article addresses the challenge of limited access to high-quality, representative recruitment datasets due to privacy and regulatory constraints, emphasizing the sensitive nature of information in curricula vitae, such as gender, disability status, and age. This scarcity hampers the development of fair and transparent machine learning models for candidate ranking, which typically require large volumes of data to generalize well and perform reliably. To tackle this, the study proposes a specialized Synthetic Data Generation (SDG) approach using two Causal Generative Models (CGMs) designed to preserve causal relationships within the data for improved fairness and interpretability. One CGM models job offers, while the other models curricula, both informed by domain expertise and structured according to causal graphs. The generated synthetic datasets allow evaluation of fairness in candidate rankings under carefully controlled scenarios that simulate specific biases. This approach highlights how CGMs can provide a promising solution for generating synthetic recruitment data that respects causal dependencies and supports the creation of fairer, more transparent ranking algorithms in domains where real data is scarce or sensitive. <div>
arXiv:2511.16204v1 Announce Type: new 
Abstract: The importance of Synthetic Data Generation (SDG) has increased significantly in domains where data quality is poor or access is limited due to privacy and regulatory constraints. One such domain is recruitment, where publicly available datasets are scarce due to the sensitive nature of information typically found in curricula vitae, such as gender, disability status, or age. %
This lack of accessible, representative data presents a significant obstacle to the development of fair and transparent machine learning models, particularly ranking algorithms that require large volumes of data to effectively learn how to recommend candidates. In the absence of such data, these models are prone to poor generalisation and may fail to perform reliably in real-world scenarios. %
Recent advances in Causal Generative Models (CGMs) offer a promising solution. CGMs enable the generation of synthetic datasets that preserve the underlying causal relationships within the data, providing greater control over fairness and interpretability in the data generation process. %
In this study, we present a specialised SDG method involving two CGMs: one modelling job offers and the other modelling curricula. Each model is structured according to a causal graph informed by domain expertise. We use these models to generate synthetic datasets and evaluate the fairness of candidate rankings under controlled scenarios that introduce specific biases.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Overcoming Data Scarcity in Nuclear Energy: A Study on Critical Heat Flux with Physics-consistent Conditional Diffusion Model</title>
<link>https://arxiv.org/abs/2511.16207</link>
<guid>https://arxiv.org/abs/2511.16207</guid>
<content:encoded><![CDATA[
<div> Keywords: deep generative modeling, diffusion model, critical heat flux, data augmentation, nuclear energy<br /><br />Summary:  
This paper explores the application of deep generative models, specifically diffusion models (DM), to address data scarcity in nuclear energy, where experimental data collection is challenging. The study utilizes a public dataset on critical heat flux (CHF) encompassing a broad range of commercial nuclear reactor operating conditions. A vanilla DM was developed to generate synthetic CHF samples, enriching dataset size and diversity to improve downstream predictive modeling. Recognizing limitations in random data generation by the vanilla DM, the authors created a conditional DM that produces targeted CHF data under specific user-defined thermal-hydraulic conditions. The models‚Äô effectiveness was evaluated by their ability to replicate empirical feature distributions, pair-wise correlations, and maintain physical consistency of the generated data. Results confirmed that both vanilla and conditional DM successfully generated realistic and physics-consistent CHF data. To further validate the outputs, uncertainty quantification was conducted, establishing confidence in synthetic data quality. The conditional DM particularly demonstrated strong capability in augmenting CHF datasets while keeping uncertainty at acceptable levels, making it a promising tool to improve data availability and robustness in nuclear energy predictive applications. <div>
arXiv:2511.16207v1 Announce Type: new 
Abstract: Deep generative modeling provides a powerful pathway to overcome data scarcity in energy-related applications where experimental data are often limited, costly, or difficult to obtain. By learning the underlying probability distribution of the training dataset, deep generative models, such as the diffusion model (DM), can generate high-fidelity synthetic samples that statistically resemble the training data. Such synthetic data generation can significantly enrich the size and diversity of the available training data, and more importantly, improve the robustness of downstream machine learning models in predictive tasks. The objective of this paper is to investigate the effectiveness of DM for overcoming data scarcity in nuclear energy applications. By leveraging a public dataset on critical heat flux (CHF) that cover a wide range of commercial nuclear reactor operational conditions, we developed a DM that can generate an arbitrary amount of synthetic samples for augmenting of the CHF dataset. Since a vanilla DM can only generate samples randomly, we also developed a conditional DM capable of generating targeted CHF data under user-specified thermal-hydraulic conditions. The performance of the DM was evaluated based on their ability to capture empirical feature distributions and pair-wise correlations, as well as to maintain physical consistency. The results showed that both the DM and conditional DM can successfully generate realistic and physics-consistent CHF data. Furthermore, uncertainty quantification was performed to establish confidence in the generated data. The results demonstrated that the conditional DM is highly effective in augmenting CHF data while maintaining acceptable levels of uncertainty.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gap: Bridging Prior Shift in Realistic Few-Shot Crop-Type Classification</title>
<link>https://arxiv.org/abs/2511.16218</link>
<guid>https://arxiv.org/abs/2511.16218</guid>
<content:encoded><![CDATA[
<div> Keywords: class imbalance, few-shot learning, Dirichlet distribution, crop-type classification, data augmentation<br /><br />Summary:<br /><br />1. Real-world agricultural datasets for crop-type classification often exhibit severe class imbalance, typically following a long-tailed distribution, and labeled data is scarce and costly to obtain. <br />2. In few-shot learning scenarios, training sets are usually artificially balanced, which does not represent the natural distribution found in real-world test environments. This mismatch leads to a distribution shift between training and testing phases, negatively impacting model generalization on real data.<br />3. To tackle this issue, the authors propose Dirichlet Prior Augmentation (DirPA), a novel technique that proactively simulates the unknown label distribution skew of the target domain during training.<br />4. DirPA models the real-world label distribution as Dirichlet-distributed random variables, effectively performing a prior augmentation that reflects realistic skewness in class representation.<br />5. Experimental results demonstrate that DirPA improves the learning process by shifting decision boundaries appropriately and stabilizing training, acting as a dynamic feature regularizer, thereby enhancing real-world generalization performance under class imbalance conditions. <div>
arXiv:2511.16218v1 Announce Type: new 
Abstract: Real-world agricultural distributions often suffer from severe class imbalance, typically following a long-tailed distribution. Labeled datasets for crop-type classification are inherently scarce and remain costly to obtain. When working with such limited data, training sets are frequently constructed to be artificially balanced -- in particular in the case of few-shot learning -- failing to reflect real-world conditions. This mismatch induces a shift between training and test label distributions, degrading real-world generalization. To address this, we propose Dirichlet Prior Augmentation (DirPA), a novel method that simulates an unknown label distribution skew of the target domain proactively during model training. Specifically, we model the real-world distribution as Dirichlet-distributed random variables, effectively performing a prior augmentation during few-shot learning. Our experiments show that DirPA successfully shifts the decision boundary and stabilizes the training process by acting as a dynamic feature regularizer.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Inference for Distributed Multimodal Systems under Communication Delay Uncertainty</title>
<link>https://arxiv.org/abs/2511.16225</link>
<guid>https://arxiv.org/abs/2511.16225</guid>
<content:encoded><![CDATA[
<div> Keywords: cyber-physical systems, communication delay, non-blocking inference, temporal windows of integration, audio-visual event localization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of uncertain communication delays in connected cyber-physical systems that perform real-time inference using multiple data streams. 2. Current state-of-the-art non-blocking inference methods depend on a reference-modality paradigm, which requires one modality input to be fully received before processing and relies on costly offline profiling. 3. To overcome these limitations, the authors propose a novel neuro-inspired non-blocking inference paradigm using adaptive temporal windows of integration (TWIs) that dynamically adjust to stochastic delay patterns across heterogeneous streams. 4. This new framework relaxes the strict requirement of a reference modality and is aware of communication delays, allowing finer-grained control over the tradeoff between inference accuracy and latency. 5. Experimental evaluation on the audio-visual event localization (AVEL) task shows that the proposed method demonstrates superior adaptability and robustness to network dynamics compared to existing state-of-the-art approaches, leading to more reliable real-time inference in cyber-physical systems. <div>
arXiv:2511.16225v1 Announce Type: new 
Abstract: Connected cyber-physical systems perform inference based on real-time inputs from multiple data streams. Uncertain communication delays across data streams challenge the temporal flow of the inference process. State-of-the-art (SotA) non-blocking inference methods rely on a reference-modality paradigm, requiring one modality input to be fully received before processing, while depending on costly offline profiling. We propose a novel, neuro-inspired non-blocking inference paradigm that primarily employs adaptive temporal windows of integration (TWIs) to dynamically adjust to stochastic delay patterns across heterogeneous streams while relaxing the reference-modality requirement. Our communication-delay-aware framework achieves robust real-time inference with finer-grained control over the accuracy-latency tradeoff. Experiments on the audio-visual event localization (AVEL) task demonstrate superior adaptability to network dynamics compared to SotA approaches.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep SOR Minimax Q-learning for Two-player Zero-sum Game</title>
<link>https://arxiv.org/abs/2511.16226</link>
<guid>https://arxiv.org/abs/2511.16226</guid>
<content:encoded><![CDATA[
<div> Keywords: two-player zero-sum game, successive over-relaxation, deep Q-learning, function approximation, finite-time convergence

<br /><br />Summary:  
This paper addresses the challenge of solving two-player zero-sum games using reinforcement learning. It focuses on extending the successive over-relaxation (SOR) Q-learning algorithm, which is known for accelerating convergence by lowering the contraction factor of the Q-Bellman operator, beyond the tabular case to high-dimensional spaces through function approximation. The authors propose a novel deep successive over-relaxation minimax Q-learning algorithm that integrates deep neural networks as function approximators, making it applicable to real-world scenarios with complex state-action spaces. They provide a theoretical guarantee by proving the finite-time convergence of the algorithm. The proposed method is empirically validated through numerical experiments, demonstrating superior performance compared to the standard Q-learning algorithm. Additionally, ablation studies investigate the role of the successive over-relaxation parameter, highlighting its impact on the efficiency and outcomes of the learning process. This work represents a significant advancement in applying accelerated reinforcement learning techniques to realistic multi-agent settings involving zero-sum competition. <div>
arXiv:2511.16226v1 Announce Type: new 
Abstract: In this work, we consider the problem of a two-player zero-sum game. In the literature, the successive over-relaxation Q-learning algorithm has been developed and implemented, and it is seen to result in a lower contraction factor for the associated Q-Bellman operator resulting in a faster value iteration-based procedure. However, this has been presented only for the tabular case and not for the setting with function approximation that typically caters to real-world high-dimensional state-action spaces. Furthermore, such settings in the case of two-player zero-sum games have not been considered. We thus propose a deep successive over-relaxation minimax Q-learning algorithm that incorporates deep neural networks as function approximators and is suitable for high-dimensional spaces. We prove the finite-time convergence of the proposed algorithm. Through numerical experiments, we show the effectiveness of the proposed method over the existing Q-learning algorithm. Our ablation studies demonstrate the effect of different values of the crucial successive over-relaxation parameter.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pass@k Metric for RLVR: A Diagnostic Tool of Exploration, But Not an Objective</title>
<link>https://arxiv.org/abs/2511.16231</link>
<guid>https://arxiv.org/abs/2511.16231</guid>
<content:encoded><![CDATA[
<div> pass@k, Large Language Models, reinforcement learning, exploration collapse, multi-step reasoning<br /><br />Summary:<br /><br />The paper investigates the pass@k metric used to evaluate the performance of Large Language Models (LLMs) on complex, multi-step reasoning tasks. pass@k measures the probability of acquiring at least one correct solution out of k independent samples and has gained traction both as an evaluation standard and as an optimization objective in reinforcement learning frameworks. The authors derive the gradient of pass@k and reveal that it fundamentally acts as a positive reweighting of the simpler pass@1 objective on a per-example basis. A key finding is that pass@k provides a diminishing learning signal in critical exploration regimes where discovering diverse solutions is vital. This leads to the concept of "exploration collapse," wherein model policies become overly concentrated, causing the difference between pass@k and pass@1 to shrink and reducing the effectiveness of pass@k as a training target. Consequently, the paper suggests that while pass@k is valuable for diagnostic purposes, it may not be suitable as a direct optimization objective in reinforcement learning. Instead, the authors advocate for mechanisms that explicitly promote efficient exploration as a more promising approach to improving LLM performance in reasoning tasks. <div>
arXiv:2511.16231v1 Announce Type: new 
Abstract: The ability of Large Language Models (LLMs) to perform complex, multi-step reasoning is a central focus of modern AI research. To evaluate and enhance this capability, the pass@k metric, which measures the probability of obtaining at least one correct solution in k independent samples, has received significant attention. Its intuitive appeal has led to its adoption not only as an evaluation standard but also as a direct optimization objective in reinforcement learning. In this paper, we analyze the pass@k objective, derive its gradient, and demonstrate that it is fundamentally a per-example positive reweighting of the simpler pass@1 objective. Our analysis reveals that the pass@k objective provides a vanishing learning signal in regimes where exploration is most critical. We further analyze the dynamics of "exploration collapse", showing that as the policy concentrates probability mass, the gap between pass@k and pass@1 diminishes. We conclude that while pass@k is a useful diagnostic tool, it may be an unsuitable direct objective for optimization. Instead, mechanisms explicitly encouraging efficient exploration could offer a more effective path forward for reinforcement learning in reasoning tasks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoPTH: A Lightweight Approach to Category-Based Trajectory Retrieval via Geometric Prototype Trajectory Hashing</title>
<link>https://arxiv.org/abs/2511.16258</link>
<guid>https://arxiv.org/abs/2511.16258</guid>
<content:encoded><![CDATA[
<div> Trajectory similarity, hashing, prototype, Hausdorff metric, spatiotemporal data<br /><br />Summary: This paper introduces GeoPTH, a novel, lightweight, and non-learning framework designed for efficient category-based trajectory similarity retrieval in spatiotemporal data mining. Traditional trajectory similarity metrics are often computationally expensive, while existing learning-based methods require substantial training time and may exhibit instability. GeoPTH addresses these issues by leveraging representative trajectory prototypes‚Äîsmall sets of points that preserve the geometric characteristics of trajectory data‚Äîas anchors to construct data-dependent hash functions. The hashing process maps new trajectories to their closest prototypes using the robust Hausdorff metric, resulting in an efficient retrieval approach. Extensive experiments demonstrate that GeoPTH achieves retrieval accuracy comparable to or better than state-of-the-art learning methods and traditional metrics. Furthermore, it significantly outperforms simple binarization techniques applied to learned embeddings in terms of retrieval performance. GeoPTH also consistently surpasses all competitors regarding computational efficiency, highlighting its advantage for practical applications. Overall, the prototype-centric, non-learning approach of GeoPTH offers a promising alternative that balances accuracy and efficiency, making it a powerful tool for trajectory similarity retrieval tasks. <div>
arXiv:2511.16258v1 Announce Type: new 
Abstract: Trajectory similarity retrieval is an important part of spatiotemporal data mining, however, existing methods have the following limitations: traditional metrics are computationally expensive, while learning-based methods suffer from substantial training costs and potential instability. This paper addresses these problems by proposing \textbf{Geo}metric \textbf{P}rototype \textbf{T}rajectory \textbf{H}ashing (GeoPTH), a novel, lightweight, and non-learning framework for efficient category-based trajectory retrieval. GeoPTH constructs data-dependent hash functions by using representative trajectory prototypes, i.e., small point sets preserving geometric characteristics, as anchors. The hashing process is efficient, which involves mapping a new trajectory to its closest prototype via a robust, \textit{Hausdorff} metric. Extensive experiments show that GeoPTH's retrieval accuracy is highly competitive with both traditional metrics and state-of-the-art learning methods, and it significantly outperforms binary codes generated through simple binarization of the learned embeddings. Critically, GeoPTH consistently outperforms all competitors in terms of efficiency. Our work demonstrates that a lightweight, prototype-centric approach offers a practical and powerful alternative, achieving an exceptional retrieval performance and computational efficiency.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Diffusion Counterfactual Explanation</title>
<link>https://arxiv.org/abs/2511.16287</link>
<guid>https://arxiv.org/abs/2511.16287</guid>
<content:encoded><![CDATA[
<div> Graph, Counterfactual Explanations, Diffusion Models, Graph Machine Learning, Classifier-free Guidance<br /><br />Summary:<br /><br />This article addresses the challenge of generating counterfactual explanations for machine learning models that operate on graph-structured data, such as molecular graphs and social networks. Unlike tabular data or images, graphs are discrete and non-Euclidean, making the construction of meaningful counterfactuals difficult. The authors introduce a novel framework called Graph Diffusion Counterfactual Explanation, which leverages discrete diffusion models combined with classifier-free guidance to generate counterfactuals on graph data. Their method focuses on producing counterfactual explanations that are both in-distribution and minimally different in structure from the original graph while successfully handling both discrete classification labels and continuous target properties. Empirical results demonstrate the reliability and effectiveness of this approach in creating insightful alternatives that help users understand the reasoning behind model predictions in graph-based domains. This work thus contributes to interpretability in graph machine learning by bridging the gap in counterfactual explanation research for graph data. <div>
arXiv:2511.16287v1 Announce Type: new 
Abstract: Machine learning models that operate on graph-structured data, such as molecular graphs or social networks, often make accurate predictions but offer little insight into why certain predictions are made. Counterfactual explanations address this challenge by seeking the closest alternative scenario where the model's prediction would change. Although counterfactual explanations are extensively studied in tabular data and computer vision, the graph domain remains comparatively underexplored. Constructing graph counterfactuals is intrinsically difficult because graphs are discrete and non-euclidean objects. We introduce Graph Diffusion Counterfactual Explanation, a novel framework for generating counterfactual explanations on graph data, combining discrete diffusion models and classifier-free guidance. We empirically demonstrate that our method reliably generates in-distribution as well as minimally structurally different counterfactuals for both discrete classification targets and continuous properties.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Operation Recipes with Reinforcement Learning for Safe and Interpretable Control of Chemical Processes</title>
<link>https://arxiv.org/abs/2511.16297</link>
<guid>https://arxiv.org/abs/2511.16297</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, chemical processes, optimal operation, operation recipes, model predictive control<br /><br />Summary:  
Optimal operation of chemical processes is essential for saving energy, resources, and costs in chemical engineering. Traditional reinforcement learning methods, while promising, face challenges due to strict quality and safety constraints and the extensive data requirements, which are difficult to meet since chemical processes rarely provide ample experimental data. Although detailed dynamic models exist, their complexity makes simulations computationally expensive, and optimal control techniques such as model predictive control struggle with these complexities. As a result, many chemical processes depend on manually crafted operation recipes combined with simple linear controllers, leading to limited flexibility and suboptimal performance. To address these limitations, the authors propose a novel approach that integrates expert knowledge embedded in operation recipes with reinforcement learning to optimize both recipe parameters and their underlying linear controllers. This hybrid strategy reduces data requirements, handles constraints more effectively, and enhances interpretability due to the structured format of the recipes. The effectiveness of this approach is demonstrated via simulations of an industrial batch polymerization reactor, where performance approaches that of optimal controllers while overcoming the drawbacks of traditional reinforcement learning and optimal control methods. This work offers a practical methodology for improving chemical process operation by blending expert insights with advanced learning techniques. <div>
arXiv:2511.16297v1 Announce Type: new 
Abstract: Optimal operation of chemical processes is vital for energy, resource, and cost savings in chemical engineering. The problem of optimal operation can be tackled with reinforcement learning, but traditional reinforcement learning methods face challenges due to hard constraints related to quality and safety that must be strictly satisfied, and the large amount of required training data. Chemical processes often cannot provide sufficient experimental data, and while detailed dynamic models can be an alternative, their complexity makes it computationally intractable to generate the needed data. Optimal control methods, such as model predictive control, also struggle with the complexity of the underlying dynamic models. Consequently, many chemical processes rely on manually defined operation recipes combined with simple linear controllers, leading to suboptimal performance and limited flexibility.
  In this work, we propose a novel approach that leverages expert knowledge embedded in operation recipes. By using reinforcement learning to optimize the parameters of these recipes and their underlying linear controllers, we achieve an optimized operation recipe. This method requires significantly less data, handles constraints more effectively, and is more interpretable than traditional reinforcement learning methods due to the structured nature of the recipes. We demonstrate the potential of our approach through simulation results of an industrial batch polymerization reactor, showing that it can approach the performance of optimal controllers while addressing the limitations of existing methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning-Enhanced Observer for Linear Time-Invariant Systems with Parametric Uncertainty</title>
<link>https://arxiv.org/abs/2511.16318</link>
<guid>https://arxiv.org/abs/2511.16318</guid>
<content:encoded><![CDATA[
<div> Keywords: learning-enhanced observer, linear time-invariant systems, uncertain dynamics, gradient-based optimization, state estimation<br /><br />Summary:<br /><br />This work proposes a learning-enhanced observer (LEO) framework designed for linear time-invariant (LTI) systems with uncertain dynamics. Instead of relying solely on fixed nominal system models, the approach treats system matrices as variables to be optimized. These matrices are refined by minimizing the steady-state output discrepancy through gradient-based methods, effectively incorporating measurement data into the model. This creates a data-driven surrogate system model that improves the observer's ability to compensate for moderate uncertainties in system parameters while maintaining the classical observer structure. The framework is validated via extensive Monte Carlo simulations spanning various system dimensions, demonstrating systematic and statistically significant reductions in normalized estimation errors, often exceeding 15% compared to traditional open-loop and Luenberger observers. These results highlight the advantage of integrating modern learning techniques into classical observer design, enabling more accurate and robust state estimation in uncertain environments. Furthermore, the authors provide the implementation codes publicly at https://github.com/Hao-B-Shu/LTI_LEO, facilitating reproducibility and further research in this direction. Overall, this study illustrates how learning-based methods complement and enhance classical control system tools for improved performance under uncertainty. <div>
arXiv:2511.16318v1 Announce Type: new 
Abstract: This work introduces a learning-enhanced observer (LEO) for linear time-invariant systems with uncertain dynamics. Rather than relying solely on nominal models, the proposed framework treats the system matrices as optimizable variables and refines them through gradient-based minimization of a steady-state output discrepancy loss. The resulting data-informed surrogate model enables the construction of an improved observer that effectively compensates for moderate parameter uncertainty while preserving the structure of classical designs. Extensive Monte Carlo studies across diverse system dimensions show systematic and statistically significant reductions, typically exceeding 15\%, in normalized estimation error for both open-loop and Luenberger observers. These results demonstrate that modern learning mechanisms can serve as a powerful complement to traditional observer design, yielding more accurate and robust state estimation in uncertain systems. Codes are available at https://github.com/Hao-B-Shu/LTI_LEO.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning</title>
<link>https://arxiv.org/abs/2511.16333</link>
<guid>https://arxiv.org/abs/2511.16333</guid>
<content:encoded><![CDATA[
<div> Keywords: world models, healthcare AI, temporal prediction, clinical decision support, action-conditioned planning<br /><br />Summary: This paper reviews the emerging use of world models in healthcare AI, emphasizing the need for predictive, reliable, and data-efficient systems. It highlights the limitations of current generative models that lack physical and temporal reasoning essential for clinical decision support. The review covers three key domains: medical imaging and diagnostics, disease progression modeling from electronic health records, and robotic surgery with surgical planning. A capability rubric is proposed, defining levels from L1 temporal prediction to L4 planning and control, with most existing systems achieving L1 and L2, fewer reaching L3, and rare examples at L4. The authors identify significant gaps impeding clinical reliability, including under-specified action spaces, weak validation of interventions, incomplete multimodal state representation, and poor trajectory-level uncertainty calibration. To address these challenges, the paper outlines a research agenda focused on creating clinically robust world models that combine generative models (transformers, diffusion models, VAEs) with causal and mechanistic foundations. This integrated approach aims to improve safe, predictive decision support capable of multistep rollouts, counterfactual evaluation, and actionable planning across healthcare settings. <div>
arXiv:2511.16333v1 Announce Type: new 
Abstract: Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Iterative Gaussian Processes via Warm Starting Sequential Posteriors</title>
<link>https://arxiv.org/abs/2511.16340</link>
<guid>https://arxiv.org/abs/2511.16340</guid>
<content:encoded><![CDATA[
<div> Scalable Gaussian processes, iterative linear solvers, conjugate gradients, Bayesian optimisation, incremental data additions<br /><br />Summary:<br /><br />This paper addresses the challenge of scaling Gaussian process (GP) inference, particularly for sequential decision-making tasks where data arrives incrementally. It focuses on the use of iterative linear solvers such as conjugate gradients, stochastic gradient descent, and alternative projections to approximate the GP posterior efficiently. The authors propose a novel technique that improves the convergence rate of these solvers by leveraging the solution to a smaller linear system embedded within the larger one. This approach is particularly relevant in scenarios where data is incrementally added, allowing the solver to build on previous computations rather than starting from scratch each time. Experimental results demonstrate that this method achieves significant speed-ups when solving linear systems to a desired tolerance level. Additionally, the paper shows that this improved solver convergence translates into better performance for Bayesian optimisation tasks, especially under fixed computational budgets. Overall, the contribution represents a meaningful advancement in making GP inference more scalable and practical for real-world applications involving sequential data and decision-making processes. <div>
arXiv:2511.16340v1 Announce Type: new 
Abstract: Scalable Gaussian process (GP) inference is essential for sequential decision-making tasks, yet improving GP scalability remains a challenging problem with many open avenues of research. This paper focuses on iterative GPs, where iterative linear solvers, such as conjugate gradients, stochastic gradient descent or alternative projections, are used to approximate the GP posterior. We propose a new method which improves solver convergence of a large linear system by leveraging the known solution to a smaller system contained within. This is significant for tasks with incremental data additions, and we show that our technique achieves speed-ups when solving to tolerance, as well as improved Bayesian optimisation performance under a fixed compute budget.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Foundation Models Useful for Bankruptcy Prediction?</title>
<link>https://arxiv.org/abs/2511.16375</link>
<guid>https://arxiv.org/abs/2511.16375</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, bankruptcy prediction, Llama-3.3-70B-Instruct, TabPFN, classical machine learning<br /><br />Summary:<br /><br />This study evaluates the effectiveness of foundation models in predicting corporate bankruptcy, comparing them systematically against classical machine learning methods. The research uses Llama-3.3-70B-Instruct and TabPFN models, tested on a large and highly imbalanced dataset of over one million company records from the Visegr√°d Group. It provides the first comprehensive benchmark of these cutting-edge foundation models relative to established approaches like XGBoost and CatBoost. The results demonstrate that classical models such as XGBoost and CatBoost consistently outperform foundation models for all bankruptcy prediction horizons. Additionally, large language model (LLM)-based methods suffer from unreliable probability estimates, which limits their utility in risk-sensitive financial contexts. Although TabPFN shows competitive performance with simpler baselines, it demands considerable computational resources, making the cost-benefit ratio unfavorable. Overall, despite the broad applicability and generality of foundation models, specialized machine learning techniques still prove more effective for bankruptcy forecasting, suggesting that current foundation models have limitations in this financial prediction domain. <div>
arXiv:2511.16375v1 Announce Type: new 
Abstract: Foundation models have shown promise across various financial applications, yet their effectiveness for corporate bankruptcy prediction remains systematically unevaluated against established methods. We study bankruptcy forecasting using Llama-3.3-70B-Instruct and TabPFN, evaluated on large, highly imbalanced datasets of over one million company records from the Visegr\'ad Group. We provide the first systematic comparison of foundation models against classical machine learning baselines for this task. Our results show that models such as XGBoost and CatBoost consistently outperform foundation models across all prediction horizons. LLM-based approaches suffer from unreliable probability estimates, undermining their use in risk-sensitive financial settings. TabPFN, while competitive with simpler baselines, requires substantial computational resources with costs not justified by performance gains. These findings suggest that, despite their generality, current foundation models remain less effective than specialized methods for bankruptcy forecasting.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Fairness under Local Differential Privacy</title>
<link>https://arxiv.org/abs/2511.16377</link>
<guid>https://arxiv.org/abs/2511.16377</guid>
<content:encoded><![CDATA[
arXiv:2511.16377v1 Announce Type: new 
Abstract: We investigate how to optimally design local differential privacy (LDP) mechanisms that reduce data unfairness and thereby improve fairness in downstream classification. We first derive a closed-form optimal mechanism for binary sensitive attributes and then develop a tractable optimization framework that yields the corresponding optimal mechanism for multi-valued attributes. As a theoretical contribution, we establish that for discrimination-accuracy optimal classifiers, reducing data unfairness necessarily leads to lower classification unfairness, thus providing a direct link between privacy-aware pre-processing and classification fairness. Empirically, we demonstrate that our approach consistently outperforms existing LDP mechanisms in reducing data unfairness across diverse datasets and fairness metrics, while maintaining accuracy close to that of non-private models. Moreover, compared with leading pre-processing and post-processing fairness methods, our mechanism achieves a more favorable accuracy-fairness trade-off while simultaneously preserving the privacy of sensitive attributes. Taken together, these results highlight LDP as a principled and effective pre-processing fairness intervention technique.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Management for Chronic Diseases and Depression: A Double Heterogeneity-based Multi-Task Learning Method</title>
<link>https://arxiv.org/abs/2511.16398</link>
<guid>https://arxiv.org/abs/2511.16398</guid>
<content:encoded><![CDATA[
arXiv:2511.16398v1 Announce Type: new 
Abstract: Wearable sensor technologies and deep learning are transforming healthcare management. Yet, most health sensing studies focus narrowly on physical chronic diseases. This overlooks the critical need for joint assessment of comorbid physical chronic diseases and depression, which is essential for collaborative chronic care. We conceptualize multi-disease assessment, including both physical diseases and depression, as a multi-task learning (MTL) problem, where each disease assessment is modeled as a task. This joint formulation leverages inter-disease relationships to improve accuracy, but it also introduces the challenge of double heterogeneity: chronic diseases differ in their manifestation (disease heterogeneity), and patients with the same disease show varied patterns (patient heterogeneity). To address these issues, we first adopt existing techniques and propose a base method. Given the limitations of the base method, we further propose an Advanced Double Heterogeneity-based Multi-Task Learning (ADH-MTL) method that improves the base method through three innovations: (1) group-level modeling to support new patient predictions, (2) a decomposition strategy to reduce model complexity, and (3) a Bayesian network that explicitly captures dependencies while balancing similarities and differences across model components. Empirical evaluations on real-world wearable sensor data demonstrate that ADH-MTL significantly outperforms existing baselines, and each of its innovations is shown to be effective. This study contributes to health information systems by offering a computational solution for integrated physical and mental healthcare and provides design principles for advancing collaborative chronic disease management across the pre-treatment, treatment, and post-treatment phases.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreqFlow: Long-term forecasting using lightweight flow matching</title>
<link>https://arxiv.org/abs/2511.16426</link>
<guid>https://arxiv.org/abs/2511.16426</guid>
<content:encoded><![CDATA[
arXiv:2511.16426v1 Announce Type: new 
Abstract: Multivariate time-series (MTS) forecasting is fundamental to applications ranging from urban mobility and resource management to climate modeling. While recent generative models based on denoising diffusion have advanced state-of-the-art performance in capturing complex data distributions, they suffer from significant computational overhead due to iterative stochastic sampling procedures that limit real-time deployment. Moreover, these models can be brittle when handling high-dimensional, non-stationary, and multi-scale periodic patterns characteristic of real-world sensor networks. We introduce FreqFlow, a novel framework that leverages conditional flow matching in the frequency domain for deterministic MTS forecasting. Unlike conventional approaches that operate in the time domain, FreqFlow transforms the forecasting problem into the spectral domain, where it learns to model amplitude and phase shifts through a single complex-valued linear layer. This frequency-domain formulation enables the model to efficiently capture temporal dynamics via complex multiplication, corresponding to scaling and temporal translations. The resulting architecture is exceptionally lightweight with only 89k parameters - an order of magnitude smaller than competing diffusion-based models-while enabling single-pass deterministic sampling through ordinary differential equation (ODE) integration. Our approach decomposes MTS signals into trend, seasonal, and residual components, with the flow matching mechanism specifically designed for residual learning to enhance long-term forecasting accuracy. Extensive experiments on real-world traffic speed, volume, and flow datasets demonstrate that FreqFlow achieves state-of-the-art forecasting performance, on average 7\% RMSE improvements, while being significantly faster and more parameter-efficient than existing methods
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Modeling of Clinical Time Series via Latent Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2511.16427</link>
<guid>https://arxiv.org/abs/2511.16427</guid>
<content:encoded><![CDATA[
arXiv:2511.16427v1 Announce Type: new 
Abstract: Clinical time series data from electronic health records and medical registries offer unprecedented opportunities to understand patient trajectories and inform medical decision-making. However, leveraging such data presents significant challenges due to irregular sampling, complex latent physiology, and inherent uncertainties in both measurements and disease progression. To address these challenges, we propose a generative modeling framework based on latent neural stochastic differential equations (SDEs) that views clinical time series as discrete-time partial observations of an underlying controlled stochastic dynamical system. Our approach models latent dynamics via neural SDEs with modality-dependent emission models, while performing state estimation and parameter learning through variational inference. This formulation naturally handles irregularly sampled observations, learns complex non-linear interactions, and captures the stochasticity of disease progression and measurement noise within a unified scalable probabilistic framework. We validate the framework on two complementary tasks: (i) individual treatment effect estimation using a simulated pharmacokinetic-pharmacodynamic (PKPD) model of lung cancer, and (ii) probabilistic forecasting of physiological signals using real-world intensive care unit (ICU) data from 12,000 patients. Results show that our framework outperforms ordinary differential equation and long short-term memory baseline models in accuracy and uncertainty estimation. These results highlight its potential for enabling precise, uncertainty-aware predictions to support clinical decision-making.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms</title>
<link>https://arxiv.org/abs/2511.16475</link>
<guid>https://arxiv.org/abs/2511.16475</guid>
<content:encoded><![CDATA[
arXiv:2511.16475v1 Announce Type: new 
Abstract: The field of Offline Reinforcement Learning (RL) aims to derive effective policies from pre-collected datasets without active environment interaction. While traditional offline RL algorithms like Conservative Q-Learning (CQL) and Implicit Q-Learning (IQL) have shown promise, they often face challenges in balancing exploration and exploitation, especially in environments with varying reward densities. The recently proposed Decision Transformer (DT) approach, which reframes offline RL as a sequence modelling problem, has demonstrated impressive results across various benchmarks. This paper presents a comparative study evaluating the performance of DT against traditional offline RL algorithms in dense and sparse reward settings for the ANT continous control environment. Our research investigates how these algorithms perform when faced with different reward structures, examining their ability to learn effective policies and generalize across varying levels of feedback. Through empirical analysis in the ANT environment, we found that DTs showed less sensitivity to varying reward density compared to other methods and particularly excelled with medium-expert datasets in sparse reward scenarios. In contrast, traditional value-based methods like IQL showed improved performance in dense reward settings with high-quality data, while CQL offered balanced performance across different data qualities. Additionally, DTs exhibited lower variance in performance but required significantly more computational resources compared to traditional approaches. These findings suggest that sequence modelling approaches may be more suitable for scenarios with uncertain reward structures or mixed-quality data, while value-based methods remain competitive in settings with dense rewards and high-quality demonstrations.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Limitations of Scalarisation in MORL: A Comparative Study in Discrete Environments</title>
<link>https://arxiv.org/abs/2511.16476</link>
<guid>https://arxiv.org/abs/2511.16476</guid>
<content:encoded><![CDATA[
arXiv:2511.16476v1 Announce Type: new 
Abstract: Scalarisation functions are widely employed in MORL algorithms to enable intelligent decision-making. However, these functions often struggle to approximate the Pareto front accurately, rendering them unideal in complex, uncertain environments. This study examines selected Multi-Objective Reinforcement Learning (MORL) algorithms across MORL environments with discrete action and observation spaces. We aim to investigate further the limitations associated with scalarisation approaches for decision-making in multi-objective settings. Specifically, we use an outer-loop multi-policy methodology to assess the performance of a seminal single-policy MORL algorithm, MO Q-Learning implemented with linear scalarisation and Chebyshev scalarisation functions. In addition, we explore a pioneering inner-loop multi-policy algorithm, Pareto Q-Learning, which offers a more robust alternative. Our findings reveal that the performance of the scalarisation functions is highly dependent on the environment and the shape of the Pareto front. These functions often fail to retain the solutions uncovered during learning and favour finding solutions in certain regions of the solution space. Moreover, finding the appropriate weight configurations to sample the entire Pareto front is complex, limiting their applicability in uncertain settings. In contrast, inner-loop multi-policy algorithms may provide a more sustainable and generalizable approach and potentially facilitate intelligent decision-making in dynamic and uncertain environments.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Correlation-Aware Feature Attribution Based Explainable AI</title>
<link>https://arxiv.org/abs/2511.16482</link>
<guid>https://arxiv.org/abs/2511.16482</guid>
<content:encoded><![CDATA[
arXiv:2511.16482v1 Announce Type: new 
Abstract: Explainable AI (XAI) is increasingly essential as modern models become more complex and high-stakes applications demand transparency, trust, and regulatory compliance. Existing global attribution methods often incur high computational costs, lack stability under correlated inputs, and fail to scale efficiently to large or heterogeneous datasets. We address these gaps with \emph{ExCIR} (Explainability through Correlation Impact Ratio), a correlation-aware attribution score equipped with a lightweight transfer protocol that reproduces full-model rankings using only a fraction of the data. ExCIR quantifies sign-aligned co-movement between features and model outputs after \emph{robust centering} (subtracting a robust location estimate, e.g., median or mid-mean, from features and outputs). We further introduce \textsc{BlockCIR}, a \emph{groupwise} extension of ExCIR that scores \emph{sets} of correlated features as a single unit. By aggregating the same signed-co-movement numerators and magnitudes over predefined or data-driven groups, \textsc{BlockCIR} mitigates double-counting in collinear clusters (e.g., synonyms or duplicated sensors) and yields smoother, more stable rankings when strong dependencies are present. Across diverse text, tabular, signal, and image datasets, ExCIR shows trustworthy agreement with established global baselines and the full model, delivers consistent top-$k$ rankings across settings, and reduces runtime via lightweight evaluation on a subset of rows. Overall, ExCIR provides \emph{computationally efficient}, \emph{consistent}, and \emph{scalable} explainability for real-world deployment.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense</title>
<link>https://arxiv.org/abs/2511.16483</link>
<guid>https://arxiv.org/abs/2511.16483</guid>
<content:encoded><![CDATA[
arXiv:2511.16483v1 Announce Type: new 
Abstract: Designing rewards for autonomous cyber attack and defense learning agents in a complex, dynamic environment is a challenging task for subject matter experts. We propose a large language model (LLM)-based reward design approach to generate autonomous cyber defense policies in a deep reinforcement learning (DRL)-driven experimental simulation environment. Multiple attack and defense agent personas were crafted, reflecting heterogeneity in agent actions, to generate LLM-guided reward designs where the LLM was first provided with contextual cyber simulation environment information. These reward structures were then utilized within a DRL-driven attack-defense simulation environment to learn an ensemble of cyber defense policies. Our results suggest that LLM-guided reward designs can lead to effective defense strategies against diverse adversarial behaviors.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ODE-ViT: Plug &amp; Play Attention Layer from the Generalization of the ViT as an Ordinary Differential Equation</title>
<link>https://arxiv.org/abs/2511.16501</link>
<guid>https://arxiv.org/abs/2511.16501</guid>
<content:encoded><![CDATA[
arXiv:2511.16501v1 Announce Type: new 
Abstract: In recent years, increasingly large models have achieved outstanding performance across CV tasks. However, these models demand substantial computational resources and storage, and their growing complexity limits our understanding of how they make decisions. Most of these architectures rely on the attention mechanism within Transformer-based designs. Building upon the connection between residual neural networks and ordinary differential equations (ODEs), we introduce ODE-ViT, a Vision Transformer reformulated as an ODE system that satisfies the conditions for well-posed and stable dynamics. Experiments on CIFAR-10 and CIFAR-100 demonstrate that ODE-ViT achieves stable, interpretable, and competitive performance with up to one order of magnitude fewer parameters, surpassing prior ODE-based Transformer approaches in classification tasks. We further propose a plug-and-play teacher-student framework in which a discrete ViT guides the continuous trajectory of ODE-ViT by treating the intermediate representations of the teacher as solutions of the ODE. This strategy improves performance by more than 10% compared to training a free ODE-ViT from scratch.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Loss Functions Robust to the Presence of Label Errors</title>
<link>https://arxiv.org/abs/2511.16512</link>
<guid>https://arxiv.org/abs/2511.16512</guid>
<content:encoded><![CDATA[
arXiv:2511.16512v1 Announce Type: new 
Abstract: Methods for detecting label errors in training data require models that are robust to label errors (i.e., not fit to erroneously labelled data points). However, acquiring such models often involves training on corrupted data, which presents a challenge. Adjustments to the loss function present an opportunity for improvement. Motivated by Focal Loss (which emphasizes difficult-to-classify samples), two novel, yet simple, loss functions are proposed that de-weight or ignore these difficult samples (i.e., those likely to have label errors). Results on artificially corrupted data show promise, such that F1 scores for detecting errors are improved from the baselines of conventional categorical Cross Entropy and Focal Loss.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saving Foundation Flow-Matching Priors for Inverse Problems</title>
<link>https://arxiv.org/abs/2511.16520</link>
<guid>https://arxiv.org/abs/2511.16520</guid>
<content:encoded><![CDATA[
arXiv:2511.16520v1 Announce Type: new 
Abstract: Foundation flow-matching (FM) models promise a universal prior for solving inverse problems (IPs), yet today they trail behind domain-specific or even untrained priors. How can we unlock their potential? We introduce FMPlug, a plug-in framework that redefines how foundation FMs are used in IPs. FMPlug combines an instance-guided, time-dependent warm-start strategy with a sharp Gaussianity regularization, adding problem-specific guidance while preserving the Gaussian structures. This leads to a significant performance boost across image restoration and scientific IPs. Our results point to a path for making foundation FM models practical, reusable priors for IP solving.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Participation in Federated Learning: Benchmarks and a Knowledge Pool Plugin</title>
<link>https://arxiv.org/abs/2511.16523</link>
<guid>https://arxiv.org/abs/2511.16523</guid>
<content:encoded><![CDATA[
arXiv:2511.16523v1 Announce Type: new 
Abstract: Federated learning (FL) enables clients to collaboratively train a shared model in a distributed manner, setting it apart from traditional deep learning paradigms. However, most existing FL research assumes consistent client participation, overlooking the practical scenario of dynamic participation (DPFL), where clients may intermittently join or leave during training. Moreover, no existing benchmarking framework systematically supports the study of DPFL-specific challenges. In this work, we present the first open-source framework explicitly designed for benchmarking FL models under dynamic client participation. Our framework provides configurable data distributions, participation patterns, and evaluation metrics tailored to DPFL scenarios. Using this platform, we benchmark four major categories of widely adopted FL models and uncover substantial performance degradation under dynamic participation. To address these challenges, we further propose Knowledge-Pool Federated Learning (KPFL), a generic plugin that maintains a shared knowledge pool across both active and idle clients. KPFL leverages dual-age and data-bias weighting, combined with generative knowledge distillation, to mitigate instability and prevent knowledge loss. Extensive experiments demonstrate the significant impact of dynamic participation on FL performance and the effectiveness of KPFL in improving model robustness and generalization.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairLRF: Achieving Fairness through Sparse Low Rank Factorization</title>
<link>https://arxiv.org/abs/2511.16549</link>
<guid>https://arxiv.org/abs/2511.16549</guid>
<content:encoded><![CDATA[
arXiv:2511.16549v1 Announce Type: new 
Abstract: As deep learning (DL) techniques become integral to various applications, ensuring model fairness while maintaining high performance has become increasingly critical, particularly in sensitive fields such as medical diagnosis. Although a variety of bias-mitigation methods have been proposed, many rely on computationally expensive debiasing strategies or suffer substantial drops in model accuracy, which limits their practicality in real-world, resource-constrained settings. To address this issue, we propose a fairness-oriented low rank factorization (LRF) framework that leverages singular value decomposition (SVD) to improve DL model fairness. Unlike traditional SVD, which is mainly used for model compression by decomposing and reducing weight matrices, our work shows that SVD can also serve as an effective tool for fairness enhancement. Specifically, we observed that elements in the unitary matrices obtained from SVD contribute unequally to model bias across groups defined by sensitive attributes. Motivated by this observation, we propose a method, named FairLRF, that selectively removes bias-inducing elements from unitary matrices to reduce group disparities, thus enhancing model fairness. Extensive experiments show that our method outperforms conventional LRF methods as well as state-of-the-art fairness-enhancing techniques. Additionally, an ablation study examines how major hyper-parameters may influence the performance of processed models. To the best of our knowledge, this is the first work utilizing SVD not primarily for compression but for fairness enhancement.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Broad stochastic configuration residual learning system for norm-convergent universal approximation</title>
<link>https://arxiv.org/abs/2511.16550</link>
<guid>https://arxiv.org/abs/2511.16550</guid>
<content:encoded><![CDATA[
arXiv:2511.16550v1 Announce Type: new 
Abstract: Universal approximation serves as the foundation of neural network learning algorithms. However, some networks establish their universal approximation property by demonstrating that the iterative errors converge in probability measure rather than the more rigorous norm convergence, which makes the universal approximation property of randomized learning networks highly sensitive to random parameter selection, Broad residual learning system (BRLS), as a member of randomized learning models, also encounters this issue. We theoretically demonstrate the limitation of its universal approximation property, that is, the iterative errors do not satisfy norm convergence if the selection of random parameters is inappropriate and the convergence rate meets certain conditions. To address this issue, we propose the broad stochastic configuration residual learning system (BSCRLS) algorithm, which features a novel supervisory mechanism adaptively constraining the range settings of random parameters on the basis of BRLS framework, Furthermore, we prove the universal approximation theorem of BSCRLS based on the more stringent norm convergence. Three versions of incremental BSCRLS algorithms are presented to satisfy the application requirements of various network updates. Solar panels dust detection experiments are performed on publicly available dataset and compared with 13 deep and broad learning algorithms. Experimental results reveal the effectiveness and superiority of BSCRLS algorithms.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Valid Generative Clinical Trial Data with Survival Endpoints</title>
<link>https://arxiv.org/abs/2511.16551</link>
<guid>https://arxiv.org/abs/2511.16551</guid>
<content:encoded><![CDATA[
arXiv:2511.16551v1 Announce Type: new 
Abstract: Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Predictive Performance on Tabular Data through Data Augmentation with Latent-Space Flow-Based Diffusion</title>
<link>https://arxiv.org/abs/2511.16571</link>
<guid>https://arxiv.org/abs/2511.16571</guid>
<content:encoded><![CDATA[
arXiv:2511.16571v1 Announce Type: new 
Abstract: Severe class imbalance is common in real-world tabular learning, where rare but important minority classes are essential for reliable prediction. Existing generative oversampling methods such as GANs, VAEs, and diffusion models can improve minority-class performance, but they often struggle with tabular heterogeneity, training stability, and privacy concerns. We propose a family of latent-space, tree-driven diffusion methods for minority oversampling that use conditional flow matching with gradient-boosted trees as the vector-field learner. The models operate in compact latent spaces to preserve tabular structure and reduce computation. We introduce three variants: PCAForest, which uses linear PCA embedding; EmbedForest, which uses a learned nonlinear embedding; and AttentionForest, which uses an attention-augmented embedding. Each method couples a GBT-based flow with a decoder back to the original feature space. Across 11 datasets from healthcare, finance, and manufacturing, AttentionForest achieves the best average minority recall while maintaining competitive precision, calibration, and distributional similarity. PCAForest and EmbedForest reach similar utility with much faster generation, offering favorable accuracy-efficiency trade-offs. Privacy evaluated with nearest-neighbor distance ratio and distance-to-closest-record is comparable to or better than the ForestDiffusion baseline. Ablation studies show that smaller embeddings tend to improve minority recall, while aggressive learning rates harm stability. Overall, latent-space, tree-driven diffusion provides an efficient and privacy-aware approach to high-fidelity tabular data augmentation under severe class imbalance.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ECPv2: Fast, Efficient, and Scalable Global Optimization of Lipschitz Functions</title>
<link>https://arxiv.org/abs/2511.16575</link>
<guid>https://arxiv.org/abs/2511.16575</guid>
<content:encoded><![CDATA[
arXiv:2511.16575v1 Announce Type: new 
Abstract: We propose ECPv2, a scalable and theoretically grounded algorithm for global optimization of Lipschitz-continuous functions with unknown Lipschitz constants. Building on the Every Call is Precious (ECP) framework, which ensures that each accepted function evaluation is potentially informative, ECPv2 addresses key limitations of ECP, including high computational cost and overly conservative early behavior. ECPv2 introduces three innovations: (i) an adaptive lower bound to avoid vacuous acceptance regions, (ii) a Worst-m memory mechanism that restricts comparisons to a fixed-size subset of past evaluations, and (iii) a fixed random projection to accelerate distance computations in high dimensions. We theoretically show that ECPv2 retains ECP's no-regret guarantees with optimal finite-time bounds and expands the acceptance region with high probability. We further empirically validate these findings through extensive experiments and ablation studies. Using principled hyperparameter settings, we evaluate ECPv2 across a wide range of high-dimensional, non-convex optimization problems. Across benchmarks, ECPv2 consistently matches or outperforms state-of-the-art optimizers, while significantly reducing wall-clock time.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Almost Sure Convergence Analysis of Differentially Private Stochastic Gradient Methods</title>
<link>https://arxiv.org/abs/2511.16587</link>
<guid>https://arxiv.org/abs/2511.16587</guid>
<content:encoded><![CDATA[
arXiv:2511.16587v1 Announce Type: new 
Abstract: Differentially private stochastic gradient descent (DP-SGD) has become the standard algorithm for training machine learning models with rigorous privacy guarantees. Despite its widespread use, the theoretical understanding of its long-run behavior remains limited: existing analyses typically establish convergence in expectation or with high probability, but do not address the almost sure convergence of single trajectories. In this work, we prove that DP-SGD converges almost surely under standard smoothness assumptions, both in nonconvex and strongly convex settings, provided the step sizes satisfy some standard decaying conditions. Our analysis extends to momentum variants such as the stochastic heavy ball (DP-SHB) and Nesterov's accelerated gradient (DP-NAG), where we show that careful energy constructions yield similar guarantees. These results provide stronger theoretical foundations for differentially private optimization and suggest that, despite privacy-induced distortions, the algorithm remains pathwise stable in both convex and nonconvex regimes.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>gfnx: Fast and Scalable Library for Generative Flow Networks in JAX</title>
<link>https://arxiv.org/abs/2511.16592</link>
<guid>https://arxiv.org/abs/2511.16592</guid>
<content:encoded><![CDATA[
arXiv:2511.16592v1 Announce Type: new 
Abstract: In this paper, we present gfnx, a fast and scalable package for training and evaluating Generative Flow Networks (GFlowNets) written in JAX. gfnx provides an extensive set of environments and metrics for benchmarking, accompanied with single-file implementations of core objectives for training GFlowNets. We include synthetic hypergrids, multiple sequence generation environments with various editing regimes and particular reward designs for molecular generation, phylogenetic tree construction, Bayesian structure learning, and sampling from the Ising model energy. Across different tasks, gfnx achieves significant wall-clock speedups compared to Pytorch-based benchmarks (such as torchgfn library) and author implementations. For example, gfnx achieves up to 55 times speedup on CPU-based sequence generation environments, and up to 80 times speedup with the GPU-based Bayesian network structure learning setup. Our package provides a diverse set of benchmarks and aims to standardize empirical evaluation and accelerate research and applications of GFlowNets. The library is available on GitHub (https://github.com/d-tiapkin/gfnx) and on pypi (https://pypi.org/project/gfnx/). Documentation is available on https://gfnx.readthedocs.io.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies</title>
<link>https://arxiv.org/abs/2511.16596</link>
<guid>https://arxiv.org/abs/2511.16596</guid>
<content:encoded><![CDATA[
arXiv:2511.16596v1 Announce Type: new 
Abstract: Palpation, the use of touch in medical examination, is almost exclusively performed by humans. We investigate a proof of concept for an artificial palpation method based on self-supervised learning. Our key idea is that an encoder-decoder framework can learn a $\textit{representation}$ from a sequence of tactile measurements that contains all the relevant information about the palpated object. We conjecture that such a representation can be used for downstream tasks such as tactile imaging and change detection. With enough training data, it should capture intricate patterns in the tactile measurements that go beyond a simple map of forces -- the current state of the art. To validate our approach, we both develop a simulation environment and collect a real-world dataset of soft objects and corresponding ground truth images obtained by magnetic resonance imaging (MRI). We collect palpation sequences using a robot equipped with a tactile sensor, and train a model that predicts sensory readings at different positions on the object. We investigate the representation learned in this process, and demonstrate its use in imaging and change detection.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stabilizing Policy Gradient Methods via Reward Profiling</title>
<link>https://arxiv.org/abs/2511.16629</link>
<guid>https://arxiv.org/abs/2511.16629</guid>
<content:encoded><![CDATA[
arXiv:2511.16629v1 Announce Type: new 
Abstract: Policy gradient methods, which have been extensively studied in the last decade, offer an effective and efficient framework for reinforcement learning problems. However, their performances can often be unsatisfactory, suffering from unreliable reward improvements and slow convergence, due to high variance in gradient estimations. In this paper, we propose a universal reward profiling framework that can be seamlessly integrated with any policy gradient algorithm, where we selectively update the policy based on high-confidence performance estimations. We theoretically justify that our technique will not slow down the convergence of the baseline policy gradient methods, but with high probability, will result in stable and monotonic improvements of their performance. Empirically, on eight continuous-control benchmarks (Box2D and MuJoCo/PyBullet), our profiling yields up to 1.5x faster convergence to near-optimal returns, up to 1.75x reduction in return variance on some setups. Our profiling approach offers a general, theoretically grounded path to more reliable and efficient policy learning in complex environments.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolution Strategies at the Hyperscale</title>
<link>https://arxiv.org/abs/2511.16652</link>
<guid>https://arxiv.org/abs/2511.16652</guid>
<content:encoded><![CDATA[
arXiv:2511.16652v1 Announce Type: new 
Abstract: We introduce Evolution Guided General Optimization via Low-rank Learning (EGGROLL), an evolution strategies (ES) algorithm designed to scale backprop-free optimization to large population sizes for modern large neural network architectures with billions of parameters. ES is a set of powerful blackbox optimisation methods that can handle non-differentiable or noisy objectives with excellent scaling potential through parallelisation. Na{\"i}ve ES becomes prohibitively expensive at scale due to the computational and memory costs associated with generating matrix perturbations $E\in\mathbb{R}^{m\times n}$ and the batched matrix multiplications needed to compute per-member forward passes. EGGROLL overcomes these bottlenecks by generating random matrices $A\in \mathbb{R}^{m\times r},\ B\in \mathbb{R}^{n\times r}$ with $r\ll \min(m,n)$ to form a low-rank matrix perturbation $A B^\top$ that are used in place of the full-rank perturbation $E$. As the overall update is an average across a population of $N$ workers, this still results in a high-rank update but with significant memory and computation savings, reducing the auxiliary storage from $mn$ to $r(m+n)$ per layer and the cost of a forward pass from $\mathcal{O}(mn)$ to $\mathcal{O}(r(m+n))$ when compared to full-rank ES. A theoretical analysis reveals our low-rank update converges to the full-rank update at a fast $\mathcal{O}\left(\frac{1}{r}\right)$ rate. Our experiments show that (1) EGGROLL does not compromise the performance of ES in tabula-rasa RL settings, despite being faster, (2) it is competitive with GRPO as a technique for improving LLM reasoning, and (3) EGGROLL enables stable pre-training of nonlinear recurrent language models that operate purely in integer datatypes.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter</title>
<link>https://arxiv.org/abs/2511.16665</link>
<guid>https://arxiv.org/abs/2511.16665</guid>
<content:encoded><![CDATA[
arXiv:2511.16665v1 Announce Type: new 
Abstract: The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very long responses dominate execution time, wasting resources and inflating costs. To address this, we propose TLT, a system that accelerates reasoning RL training losslessly by integrating adaptive speculative decoding. Applying speculative decoding in RL is challenging due to the dynamic workloads, evolving target model, and draft model training overhead. TLT overcomes these obstacles with two synergistic components: (1) Adaptive Drafter, a lightweight draft model trained continuously on idle GPUs during long-tail generation to maintain alignment with the target model at no extra cost; and (2) Adaptive Rollout Engine, which maintains a memory-efficient pool of pre-captured CUDAGraphs and adaptively select suitable SD strategies for each input batch. Evaluations demonstrate that TLT achieves over 1.7x end-to-end RL training speedup over state-of-the-art systems, preserves the model accuracy, and yields a high-quality draft model as a free byproduct suitable for efficient deployment. Code is released at https://github.com/mit-han-lab/fastrl.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2406.10219</link>
<guid>https://arxiv.org/abs/2406.10219</guid>
<content:encoded><![CDATA[
arXiv:2406.10219v3 Announce Type: cross 
Abstract: Recent advances in novel view synthesis have enabled real-time rendering speeds with high reconstruction accuracy. 3D Gaussian Splatting (3D-GS), a foundational point-based parametric 3D scene representation, models scenes as large sets of 3D Gaussians. However, complex scenes can consist of millions of Gaussians, resulting in high storage and memory requirements that limit the viability of 3D-GS on devices with limited resources. Current techniques for compressing these pretrained models by pruning Gaussians rely on combining heuristics to determine which Gaussians to remove. At high compression ratios, these pruned scenes suffer from heavy degradation of visual fidelity and loss of foreground details. In this paper, we propose a principled sensitivity pruning score that preserves visual fidelity and foreground details at significantly higher compression ratios than existing approaches. It is computed as a second-order approximation of the reconstruction error on the training views with respect to the spatial parameters of each Gaussian. Additionally, we propose a multi-round prune-refine pipeline that can be applied to any pretrained 3D-GS model without changing its training pipeline. After pruning 90% of Gaussians, a substantially higher percentage than previous methods, our PUP 3D-GS pipeline increases average rendering speed by 3.56$\times$ while retaining more salient foreground information and achieving higher image quality metrics than existing techniques on scenes from the Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-Memoized Reasoning: Foundations Structured Workflow Reuse in Intelligent Systems</title>
<link>https://arxiv.org/abs/2511.15715</link>
<guid>https://arxiv.org/abs/2511.15715</guid>
<content:encoded><![CDATA[
arXiv:2511.15715v1 Announce Type: cross 
Abstract: Modern large language model-based reasoning systems frequently recompute similar reasoning steps across tasks, wasting computational resources, inflating inference latency, and limiting reproducibility. These inefficiencies underscore the need for persistent reasoning mechanisms that can recall and reuse prior computational traces.
  We introduce Graph-Memoized Reasoning, a formal framework for representing, storing, and reusing reasoning workflows as graph-structured memory. By encoding past decision graphs and retrieving them through structural and semantic similarity, our approach enables compositional reuse of subgraphs across new reasoning tasks.
  We formulate an optimization objective that minimizes total reasoning cost regularized by inconsistency between stored and generated workflows, providing a theoretical foundation for efficiency-consistency trade-offs in intelligent systems. We outline a conceptual evaluation protocol aligned with the proposed optimization objective.
  This framework establishes the groundwork for interpretable, cost-efficient, and self-improving reasoning architectures, offering a step toward persistent memory in large-scale agentic systems.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-aligned Quantification of Numerical Data</title>
<link>https://arxiv.org/abs/2511.15723</link>
<guid>https://arxiv.org/abs/2511.15723</guid>
<content:encoded><![CDATA[
arXiv:2511.15723v1 Announce Type: cross 
Abstract: Quantifying numerical data involves addressing two key challenges: first, determining whether the data can be naturally quantified, and second, identifying the numerical intervals or ranges of values that correspond to specific value classes, referred to as "quantums," which represent statistically meaningful states. If such quantification is feasible, continuous streams of numerical data can be transformed into sequences of "symbols" that reflect the states of the system described by the measured parameter. People often perform this task intuitively, relying on common sense or practical experience, while information theory and computer science offer computable metrics for this purpose. In this study, we assess the applicability of metrics based on information compression and the Silhouette coefficient for quantifying numerical data. We also investigate the extent to which these metrics correlate with one another and with what is commonly referred to as "human intuition." Our findings suggest that the ability to classify numeric data values into distinct categories is associated with a Silhouette coefficient above 0.65 and a Dip Test below 0.5; otherwise, the data can be treated as following a unimodal normal distribution. Furthermore, when quantification is possible, the Silhouette coefficient appears to align more closely with human intuition than the "normalized centroid distance" method derived from information compression perspective.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Resilient Multimodal Learning via Consistency-Guided Cross-Modal Transfer</title>
<link>https://arxiv.org/abs/2511.15741</link>
<guid>https://arxiv.org/abs/2511.15741</guid>
<content:encoded><![CDATA[
arXiv:2511.15741v1 Announce Type: cross 
Abstract: Multimodal learning systems often face substantial uncertainty due to noisy data, low-quality labels, and heterogeneous modality characteristics. These issues become especially critical in human-computer interaction settings, where data quality, semantic reliability, and annotation consistency vary across users and recording conditions. This thesis tackles these challenges by exploring uncertainty-resilient multimodal learning through consistency-guided cross-modal transfer. The central idea is to use cross-modal semantic consistency as a basis for robust representation learning. By projecting heterogeneous modalities into a shared latent space, the proposed framework mitigates modality gaps and uncovers structural relations that support uncertainty estimation and stable feature learning. Building on this foundation, the thesis investigates strategies to enhance semantic robustness, improve data efficiency, and reduce the impact of noise and imperfect supervision without relying on large, high-quality annotations. Experiments on multimodal affect-recognition benchmarks demonstrate that consistency-guided cross-modal transfer significantly improves model stability, discriminative ability, and robustness to noisy or incomplete supervision. Latent space analyses further show that the framework captures reliable cross-modal structure even under challenging conditions. Overall, this thesis offers a unified perspective on resilient multimodal learning by integrating uncertainty modeling, semantic alignment, and data-efficient supervision, providing practical insights for developing reliable and adaptive brain-computer interface systems.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SURFing to the Fundamental Limit of Jet Tagging</title>
<link>https://arxiv.org/abs/2511.15779</link>
<guid>https://arxiv.org/abs/2511.15779</guid>
<content:encoded><![CDATA[
arXiv:2511.15779v1 Announce Type: cross 
Abstract: Beyond the practical goal of improving search and measurement sensitivity through better jet tagging algorithms, there is a deeper question: what are their upper performance limits? Generative surrogate models with learned likelihood functions offer a new approach to this problem, provided the surrogate correctly captures the underlying data distribution. In this work, we introduce the SUrrogate ReFerence (SURF) method, a new approach to validating generative models. This framework enables exact Neyman-Pearson tests by training the target model on samples from another tractable surrogate, which is itself trained on real data. We argue that the EPiC-FM generative model is a valid surrogate reference for JetClass jets and apply SURF to show that modern jet taggers may already be operating close to the true statistical limit. By contrast, we find that autoregressive GPT models unphysically exaggerate top vs. QCD separation power encoded in the surrogate reference, implying that they are giving a misleading picture of the fundamental limit.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atlas Gaussian processes on restricted domains and point clouds</title>
<link>https://arxiv.org/abs/2511.15822</link>
<guid>https://arxiv.org/abs/2511.15822</guid>
<content:encoded><![CDATA[
arXiv:2511.15822v1 Announce Type: cross 
Abstract: In real-world applications, data often reside in restricted domains with unknown boundaries, or as high-dimensional point clouds lying on a lower-dimensional, nontrivial, unknown manifold. Traditional Gaussian Processes (GPs) struggle to capture the underlying geometry in such settings. Some existing methods assume a flat space embedded in a point cloud, which can be represented by a single latent chart (latent space), while others exhibit weak performance when the point cloud is sparse or irregularly sampled. The goal of this work is to address these challenges. The main contributions are twofold: (1) We establish the Atlas Brownian Motion (BM) framework for estimating the heat kernel on point clouds with unknown geometries and nontrivial topological structures; (2) Instead of directly using the heat kernel estimates, we construct a Riemannian corrected kernel by combining the global heat kernel with local RBF kernel and leading to the formulation of Riemannian-corrected Atlas Gaussian Processes (RC-AGPs). The resulting RC-AGPs are applied to regression tasks across synthetic and real-world datasets. These examples demonstrate that our method outperforms existing approaches in both heat kernel estimation and regression accuracy. It improves statistical inference by effectively bridging the gap between complex, high-dimensional observations and manifold-based inferences.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion</title>
<link>https://arxiv.org/abs/2511.15874</link>
<guid>https://arxiv.org/abs/2511.15874</guid>
<content:encoded><![CDATA[
arXiv:2511.15874v1 Announce Type: cross 
Abstract: Accurate 6D object pose estimation is vital for robotics, augmented reality, and scene understanding. For seen objects, high accuracy is often attainable via per-object fine-tuning but generalizing to unseen objects remains a challenge. To address this problem, past arts assume access to CAD models at test time and typically follow a multi-stage pipeline to estimate poses: detect and segment the object, propose an initial pose, and then refine it. Under occlusion, however, the early-stage of such pipelines are prone to errors, which can propagate through the sequential processing, and consequently degrade the performance. To remedy this shortcoming, we propose four novel extensions to model-based 6D pose estimation methods: (i) a dynamic non-uniform dense sampling strategy that focuses computation on visible regions, reducing occlusion-induced errors; (ii) a multi-hypothesis inference mechanism that retains several confidence-ranked pose candidates, mitigating brittle single-path failures; (iii) iterative refinement to progressively improve pose accuracy; and (iv) series of occlusion-focused training augmentations that strengthen robustness and generalization. Furthermore, we propose a new weighted by visibility metric for evaluation under occlusion to minimize the bias in the existing protocols. Via extensive empirical evaluations, we show that our proposed approach achieves more than 5% improvement in accuracy on ICBIN and more than 2% on BOP dataset benchmarks, while achieving approximately 3 times faster inference.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes</title>
<link>https://arxiv.org/abs/2511.15884</link>
<guid>https://arxiv.org/abs/2511.15884</guid>
<content:encoded><![CDATA[
arXiv:2511.15884v1 Announce Type: cross 
Abstract: Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.
  To this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEG Emotion Recognition Through Deep Learning</title>
<link>https://arxiv.org/abs/2511.15902</link>
<guid>https://arxiv.org/abs/2511.15902</guid>
<content:encoded><![CDATA[
arXiv:2511.15902v1 Announce Type: cross 
Abstract: An advanced emotion classification model was developed using a CNN-Transformer architecture for emotion recognition from EEG brain wave signals, effectively distinguishing among three emotional states, positive, neutral and negative. The model achieved a testing accuracy of 91%, outperforming traditional models such as SVM, DNN, and Logistic Regression. Training was conducted on a custom dataset created by merging data from SEED, SEED-FRA, and SEED-GER repositories, comprising 1,455 samples with EEG recordings labeled according to emotional states. The combined dataset represents one of the largest and most culturally diverse collections available. Additionally, the model allows for the reduction of the requirements of the EEG apparatus, by leveraging only 5 electrodes of the 62. This reduction demonstrates the feasibility of deploying a more affordable consumer-grade EEG headset, thereby enabling accessible, at-home use, while also requiring less computational power. This advancement sets the groundwork for future exploration into mood changes induced by media content consumption, an area that remains underresearched. Integration into medical, wellness, and home-health platforms could enable continuous, passive emotional monitoring, particularly beneficial in clinical or caregiving settings where traditional behavioral cues, such as facial expressions or vocal tone, are diminished, restricted, or difficult to interpret, thus potentially transforming mental health diagnostics and interventions...
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning vs. Randomness: Challenges in Predicting Binary Options Movements</title>
<link>https://arxiv.org/abs/2511.15960</link>
<guid>https://arxiv.org/abs/2511.15960</guid>
<content:encoded><![CDATA[
arXiv:2511.15960v1 Announce Type: cross 
Abstract: Binary options trading is often marketed as a field where predictive models can generate consistent profits. However, the inherent randomness and stochastic nature of binary options make price movements highly unpredictable, posing significant challenges for any forecasting approach. This study demonstrates that machine learning algorithms struggle to outperform a simple baseline in predicting binary options movements. Using a dataset of EUR/USD currency pairs from 2021 to 2023, we tested multiple models, including Random Forest, Logistic Regression, Gradient Boosting, and k-Nearest Neighbors (kNN), both before and after hyperparameter optimization. Furthermore, several neural network architectures, including Multi-Layer Perceptrons (MLP) and a Long Short-Term Memory (LSTM) network, were evaluated under different training conditions. Despite these exhaustive efforts, none of the models surpassed the ZeroR baseline accuracy, highlighting the inherent randomness of binary options. These findings reinforce the notion that binary options lack predictable patterns, making them unsuitable for machine learning-based forecasting.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Primer on Quantum Machine Learning</title>
<link>https://arxiv.org/abs/2511.15969</link>
<guid>https://arxiv.org/abs/2511.15969</guid>
<content:encoded><![CDATA[
arXiv:2511.15969v1 Announce Type: cross 
Abstract: Quantum machine learning (QML) is a computational paradigm that seeks to apply quantum-mechanical resources to solve learning problems. As such, the goal of this framework is to leverage quantum processors to tackle optimization, supervised, unsupervised and reinforcement learning, and generative modeling-among other tasks-more efficiently than classical models. Here we offer a high level overview of QML, focusing on settings where the quantum device is the primary learning or data generating unit. We outline the field's tensions between practicality and guarantees, access models and speedups, and classical baselines and claimed quantum advantages-flagging where evidence is strong, where it is conditional or still lacking, and where open questions remain. By shedding light on these nuances and debates, we aim to provide a friendly map of the QML landscape so that the reader can judge when-and under what assumptions-quantum approaches may offer real benefits.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows</title>
<link>https://arxiv.org/abs/2511.15977</link>
<guid>https://arxiv.org/abs/2511.15977</guid>
<content:encoded><![CDATA[
arXiv:2511.15977v1 Announce Type: cross 
Abstract: Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness in Multi-modal Medical Diagnosis with Demonstration Selection</title>
<link>https://arxiv.org/abs/2511.15986</link>
<guid>https://arxiv.org/abs/2511.15986</guid>
<content:encoded><![CDATA[
arXiv:2511.15986v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Agriculture Sandbox for Collaborative Research</title>
<link>https://arxiv.org/abs/2511.15990</link>
<guid>https://arxiv.org/abs/2511.15990</guid>
<content:encoded><![CDATA[
arXiv:2511.15990v1 Announce Type: cross 
Abstract: Digital agriculture is transforming the way we grow food by utilizing technology to make farming more efficient, sustainable, and productive. This modern approach to agriculture generates a wealth of valuable data that could help address global food challenges, but farmers are hesitant to share it due to privacy concerns. This limits the extent to which researchers can learn from this data to inform improvements in farming. This paper presents the Digital Agriculture Sandbox, a secure online platform that solves this problem. The platform enables farmers (with limited technical resources) and researchers to collaborate on analyzing farm data without exposing private information. We employ specialized techniques such as federated learning, differential privacy, and data analysis methods to safeguard the data while maintaining its utility for research purposes. The system enables farmers to identify similar farmers in a simplified manner without needing extensive technical knowledge or access to computational resources. Similarly, it enables researchers to learn from the data and build helpful tools without the sensitive information ever leaving the farmer's system. This creates a safe space where farmers feel comfortable sharing data, allowing researchers to make important discoveries. Our platform helps bridge the gap between maintaining farm data privacy and utilizing that data to address critical food and farming challenges worldwide.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Safer and Sustainable Manufacturing Process: Material classification in Laser Cutting Using Deep Learning</title>
<link>https://arxiv.org/abs/2511.16026</link>
<guid>https://arxiv.org/abs/2511.16026</guid>
<content:encoded><![CDATA[
arXiv:2511.16026v1 Announce Type: cross 
Abstract: Laser cutting is a widely adopted technology in material processing across various industries, but it generates a significant amount of dust, smoke, and aerosols during operation, posing a risk to both the environment and workers' health. Speckle sensing has emerged as a promising method to monitor the cutting process and identify material types in real-time. This paper proposes a material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process. The proposed method involves training a convolutional neural network (CNN) on a dataset of laser speckle patterns to recognize distinct material types for safe and efficient cutting. Previous methods for material classification using speckle sensing may face issues when the color of the laser used to produce the speckle pattern is changed. Experiments conducted in this study demonstrate that the proposed method achieves high accuracy in material classification, even when the laser color is changed. The model achieved an accuracy of 98.30 % on the training set and 96.88% on the validation set. Furthermore, the model was evaluated on a set of 3000 new images for 30 different materials, achieving an F1-score of 0.9643. The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Operon: Incremental Construction of Ragged Data via Named Dimensions</title>
<link>https://arxiv.org/abs/2511.16080</link>
<guid>https://arxiv.org/abs/2511.16080</guid>
<content:encoded><![CDATA[
arXiv:2511.16080v1 Announce Type: cross 
Abstract: Modern data processing workflows frequently encounter ragged data: collections with variable-length elements that arise naturally in domains like natural language processing, scientific measurements, and autonomous AI agents. Existing workflow engines lack native support for tracking the shapes and dependencies inherent to ragged data, forcing users to manage complex indexing and dependency bookkeeping manually. We present Operon, a Rust-based workflow engine that addresses these challenges through a novel formalism of named dimensions with explicit dependency relations. Operon provides a domain-specific language where users declare pipelines with dimension annotations that are statically verified for correctness, while the runtime system dynamically schedules tasks as data shapes are incrementally discovered during execution. We formalize the mathematical foundation for reasoning about partial shapes and prove that Operon's incremental construction algorithm guarantees deterministic and confluent execution in parallel settings. The system's explicit modeling of partially-known states enables robust persistence and recovery mechanisms, while its per-task multi-queue architecture achieves efficient parallelism across heterogeneous task types. Empirical evaluation demonstrates that Operon outperforms an existing workflow engine with 14.94x baseline overhead reduction while maintaining near-linear end-to-end output rates as workloads scale, making it particularly suitable for large-scale data generation pipelines in machine learning applications.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Angular Graph Fractional Fourier Transform: Theory and Application</title>
<link>https://arxiv.org/abs/2511.16111</link>
<guid>https://arxiv.org/abs/2511.16111</guid>
<content:encoded><![CDATA[
arXiv:2511.16111v1 Announce Type: cross 
Abstract: Graph spectral representations are fundamental in graph signal processing, offering a rigorous framework for analyzing and processing graph-structured data. The graph fractional Fourier transform (GFRFT) extends the classical graph Fourier transform (GFT) with a fractional-order parameter, enabling flexible spectral analysis while preserving mathematical consistency. The angular graph Fourier transform (AGFT) introduces angular control via GFT eigenvector rotation; however, existing constructions fail to degenerate to the GFT at zero angle, which is a critical flaw that undermines theoretical consistency and interpretability. To resolve these complementary limitations - GFRFT's lack of angular regulation and AGFT's defective degeneracy - this study proposes an angular GFRFT (AGFRFT), a unified framework that integrates fractional-order and angular spectral analyses with theoretical rigor. A degeneracy-friendly rotation matrix family ensures exact GFT degeneration at zero angle, with two AGFRFT variants (I-AGFRFT and II-AGFRFT) defined accordingly. Rigorous theoretical analyses confirm their unitarity, invertibility, and smooth parameter dependence. Both support learnable joint parameterization of the angle and fractional order, enabling adaptive spectral processing for diverse graph signals. Extensive experiments on real-world data denoising, image denoising, and point cloud denoising demonstrate that AGFRFT outperforms GFRFT and AGFT in terms of spectral concentration, reconstruction quality, and controllable spectral manipulation, establishing a robust and flexible tool for integrated angular fractional spectral analysis in graph signal processing.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximation rates of quantum neural networks for periodic functions via Jackson's inequality</title>
<link>https://arxiv.org/abs/2511.16149</link>
<guid>https://arxiv.org/abs/2511.16149</guid>
<content:encoded><![CDATA[
arXiv:2511.16149v1 Announce Type: cross 
Abstract: Quantum neural networks (QNNs) are an analog of classical neural networks in the world of quantum computing, which are represented by a unitary matrix with trainable parameters. Inspired by the universal approximation property of classical neural networks, ensuring that every continuous function can be arbitrarily well approximated uniformly on a compact set of a Euclidean space, some recent works have established analogous results for QNNs, ranging from single-qubit to multi-qubit QNNs, and even hybrid classical-quantum models. In this paper, we study the approximation capabilities of QNNs for periodic functions with respect to the supremum norm. We use the Jackson inequality to approximate a given function by implementing its approximating trigonometric polynomial via a suitable QNN. In particular, we see that by restricting to the class of periodic functions, one can achieve a quadratic reduction of the number of parameters, producing better approximation results than in the literature. Moreover, the smoother the function, the fewer parameters are needed to construct a QNN to approximate the function.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics</title>
<link>https://arxiv.org/abs/2511.16158</link>
<guid>https://arxiv.org/abs/2511.16158</guid>
<content:encoded><![CDATA[
arXiv:2511.16158v1 Announce Type: cross 
Abstract: Magnetic levitation is about to revolutionize in-machine material flow in industrial automation. Such systems are flexibly configurable and can include a large number of independently actuated shuttles (movers) that dynamically rebalance production capacity. Beyond their capabilities for dynamic transportation, these systems possess the inherent yet unexploited potential to perform manipulation. By merging the fields of transportation and manipulation into a coordinated swarm of magnetic robots (MagBots), we enable manufacturing systems to achieve significantly higher efficiency, adaptability, and compactness. To support the development of intelligent algorithms for magnetic levitation systems, we introduce MagBotSim (Magnetic Robotics Simulation): a physics-based simulation for magnetic levitation systems. By framing magnetic levitation systems as robot swarms and providing a dedicated simulation, this work lays the foundation for next generation manufacturing systems powered by Magnetic Robotics. MagBotSim's documentation, videos, experiments, and code are available at: https://ubi-coro.github.io/MagBotSim/
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ART: A Graph-based Framework for Investigating Illicit Activity in Monero via Address-Ring-Transaction Structures</title>
<link>https://arxiv.org/abs/2511.16192</link>
<guid>https://arxiv.org/abs/2511.16192</guid>
<content:encoded><![CDATA[
arXiv:2511.16192v1 Announce Type: cross 
Abstract: As Law Enforcement Agencies advance in cryptocurrency forensics, criminal actors aiming to conceal illicit fund movements increasingly turn to "mixin" services or privacy-based cryptocurrencies. Monero stands out as a leading choice due to its strong privacy preserving and untraceability properties, making conventional blockchain analysis ineffective. Understanding the behavior and operational patterns of criminal actors within Monero is therefore challenging and it is essential to support future investigative strategies and disrupt illicit activities. In this work, we propose a case study in which we leverage a novel graph-based methodology to extract structural and temporal patterns from Monero transactions linked to already discovered criminal activities. By building Address-Ring-Transaction graphs from flagged transactions, we extract structural and temporal features and use them to train Machine Learning models capable of detecting similar behavioral patterns that could highlight criminal modus operandi. This represents a first partial step toward developing analytical tools that support investigative efforts in privacy-preserving blockchain ecosystems
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlipVQA-Miner: Cross-Page Visual Question-Answer Mining from Textbooks</title>
<link>https://arxiv.org/abs/2511.16216</link>
<guid>https://arxiv.org/abs/2511.16216</guid>
<content:encoded><![CDATA[
arXiv:2511.16216v1 Announce Type: cross 
Abstract: The development of Large Language Models (LLMs) increasingly depends on high-quality supervised data, yet existing instruction-tuning and RL datasets remain costly to curate and often rely on synthetic samples that introduce hallucination and limited diversity. At the same time, textbooks and exercise materials contain abundant, high-quality human-authored Question-Answer(QA) content that remains underexploited due to the difficulty of transforming raw PDFs into AI-ready supervision. Although modern OCR and vision-language models can accurately parse document structure, their outputs lack the semantic alignment required for training. We propose an automated pipeline that extracts well-formed QA and visual-QA (VQA) pairs from educational documents by combining layout-aware OCR with LLM-based semantic parsing. Experiments across diverse document types show that the method produces accurate, aligned, and low-noise QA/VQA pairs. This approach enables scalable use of real-world educational content and provides a practical alternative to synthetic data generation for improving reasoning-oriented LLM training. All code and data-processing pipelines are open-sourced at https://github.com/OpenDCAI/DataFlow.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Identifiability for Interpretable Probe Geometry</title>
<link>https://arxiv.org/abs/2511.16288</link>
<guid>https://arxiv.org/abs/2511.16288</guid>
<content:encoded><![CDATA[
arXiv:2511.16288v1 Announce Type: cross 
Abstract: Linear probes are widely used to interpret and evaluate neural representations, yet their reliability remains unclear, as probes may appear accurate in some regimes but collapse unpredictably in others. We uncover a spectral mechanism behind this phenomenon and formalize it as the Spectral Identifiability Principle (SIP), a verifiable Fisher-inspired condition for probe stability. When the eigengap separating task-relevant directions is larger than the Fisher estimation error, the estimated subspace concentrates and accuracy remains consistent, whereas closing this gap induces instability in a phase-transition manner. Our analysis connects eigengap geometry, sample size, and misclassification risk through finite-sample reasoning, providing an interpretable diagnostic rather than a loose generalization bound. Controlled synthetic studies, where Fisher quantities are computed exactly, confirm these predictions and show how spectral inspection can anticipate unreliable probes before they distort downstream evaluation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Autoencoders are Topic Models</title>
<link>https://arxiv.org/abs/2511.16309</link>
<guid>https://arxiv.org/abs/2511.16309</guid>
<content:encoded><![CDATA[
arXiv:2511.16309v1 Announce Type: cross 
Abstract: Sparse autoencoders (SAEs) are used to analyze embeddings, but their role and practical value are debated. We propose a new perspective on SAEs by demonstrating that they can be naturally understood as topic models. We extend Latent Dirichlet Allocation to embedding spaces and derive the SAE objective as a maximum a posteriori estimator under this model. This view implies SAE features are thematic components rather than steerable directions. Based on this, we introduce SAE-TM, a topic modeling framework that: (1) trains an SAE to learn reusable topic atoms, (2) interprets them as word distributions on downstream data, and (3) merges them into any number of topics without retraining. SAE-TM yields more coherent topics than strong baselines on text and image datasets while maintaining diversity. Finally, we analyze thematic structure in image datasets and trace topic changes over time in Japanese woodblock prints. Our work positions SAEs as effective tools for large-scale thematic analysis across modalities. Code and data will be released upon publication.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VersaPants: A Loose-Fitting Textile Capacitive Sensing System for Lower-Body Motion Capture</title>
<link>https://arxiv.org/abs/2511.16346</link>
<guid>https://arxiv.org/abs/2511.16346</guid>
<content:encoded><![CDATA[
arXiv:2511.16346v1 Announce Type: cross 
Abstract: We present VersaPants, the first loose-fitting, textile-based capacitive sensing system for lower-body motion capture, built on the open-hardware VersaSens platform. By integrating conductive textile patches and a compact acquisition unit into a pair of pants, the system reconstructs lower-body pose without compromising comfort. Unlike IMU-based systems that require user-specific fitting or camera-based methods that compromise privacy, our approach operates without fitting adjustments and preserves user privacy. VersaPants is a custom-designed smart garment featuring 6 capacitive channels per leg. We employ a lightweight Transformer-based deep learning model that maps capacitance signals to joint angles, enabling embedded implementation on edge platforms. To test our system, we collected approximately 3.7 hours of motion data from 11 participants performing 16 daily and exercise-based movements. The model achieves a mean per-joint position error (MPJPE) of 11.96 cm and a mean per-joint angle error (MPJAE) of 12.3 degrees across the hip, knee, and ankle joints, indicating the model's ability to generalize to unseen users and movements. A comparative analysis of existing textile-based deep learning architectures reveals that our model achieves competitive reconstruction performance with up to 22 times fewer parameters and 18 times fewer FLOPs, enabling real-time inference at 42 FPS on a commercial smartwatch without quantization. These results position VersaPants as a promising step toward scalable, comfortable, and embedded motion-capture solutions for fitness, healthcare, and wellbeing applications.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reducing Instability in Synthetic Data Evaluation with a Super-Metric in MalDataGen</title>
<link>https://arxiv.org/abs/2511.16373</link>
<guid>https://arxiv.org/abs/2511.16373</guid>
<content:encoded><![CDATA[
arXiv:2511.16373v1 Announce Type: cross 
Abstract: Evaluating the quality of synthetic data remains a persistent challenge in the Android malware domain due to instability and the lack of standardization among existing metrics. This work integrates into MalDataGen a Super-Metric that aggregates eight metrics across four fidelity dimensions, producing a single weighted score. Experiments involving ten generative models and five balanced datasets demonstrate that the Super-Metric is more stable and consistent than traditional metrics, exhibiting stronger correlations with the actual performance of classifiers.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Graph Neural Network Framework for Balanced Multipatterning in Advanced Electronic Design Automation Layouts</title>
<link>https://arxiv.org/abs/2511.16374</link>
<guid>https://arxiv.org/abs/2511.16374</guid>
<content:encoded><![CDATA[
arXiv:2511.16374v1 Announce Type: cross 
Abstract: Multipatterning is an essential decomposition strategy in electronic design automation (EDA) that overcomes lithographic limitations when printing dense circuit layouts. Although heuristic-based backtracking and SAT solvers can address these challenges, they often struggle to simultaneously handle both complex constraints and secondary objectives. In this study, we present a hybrid workflow that casts multipatterning as a variant of a constrained graph coloring problem with the primary objective of minimizing feature violations and a secondary objective of balancing the number of features on each mask. Our pipeline integrates two main components: (1) A GNN-based agent, trained in an unsupervised manner to generate initial color predictions, which are refined by (2) refinement strategies (a GNN-based heuristic and simulated annealing) that together enhance solution quality and balance. Experimental evaluation in both proprietary data sets and publicly available open source layouts demonstrate complete conflict-free decomposition and consistent color balancing. The proposed framework provides a reproducible, data-efficient and deployable baseline for scalable layout decomposition in EDA workflows.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classification of worldwide news articles by perceived quality, 2018-2024</title>
<link>https://arxiv.org/abs/2511.16416</link>
<guid>https://arxiv.org/abs/2511.16416</guid>
<content:encoded><![CDATA[
arXiv:2511.16416v1 Announce Type: cross 
Abstract: This study explored whether supervised machine learning and deep learning models can effectively distinguish perceived lower-quality news articles from perceived higher-quality news articles. 3 machine learning classifiers and 3 deep learning models were assessed using a newly created dataset of 1,412,272 English news articles from the Common Crawl over 2018-2024. Expert consensus ratings on 579 source websites were split at the median, creating perceived low and high-quality classes of about 706,000 articles each, with 194 linguistic features per website-level labelled article. Traditional machine learning classifiers such as the Random Forest demonstrated capable performance (0.7355 accuracy, 0.8131 ROC AUC). For deep learning, ModernBERT-large (256 context length) achieved the best performance (0.8744 accuracy; 0.9593 ROC-AUC; 0.8739 F1), followed by DistilBERT-base (512 context length) at 0.8685 accuracy and 0.9554 ROC-AUC. DistilBERT-base (256 context length) reached 0.8478 accuracy and 0.9407 ROC-AUC, while ModernBERT-base (256 context length) attained 0.8569 accuracy and 0.9470 ROC-AUC. These results suggest that the perceived quality of worldwide news articles can be effectively differentiated by traditional CPU-based machine learning classifiers and deep learning classifiers.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Neural Networks for Surgical Scene Segmentation</title>
<link>https://arxiv.org/abs/2511.16430</link>
<guid>https://arxiv.org/abs/2511.16430</guid>
<content:encoded><![CDATA[
arXiv:2511.16430v1 Announce Type: cross 
Abstract: Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.
  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.
  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.
  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PersonaDrift: A Benchmark for Temporal Anomaly Detection in Language-Based Dementia Monitoring</title>
<link>https://arxiv.org/abs/2511.16445</link>
<guid>https://arxiv.org/abs/2511.16445</guid>
<content:encoded><![CDATA[
arXiv:2511.16445v1 Announce Type: cross 
Abstract: People living with dementia (PLwD) often show gradual shifts in how they communicate, becoming less expressive, more repetitive, or drifting off-topic in subtle ways. While caregivers may notice these changes informally, most computational tools are not designed to track such behavioral drift over time. This paper introduces PersonaDrift, a synthetic benchmark designed to evaluate machine learning and statistical methods for detecting progressive changes in daily communication, focusing on user responses to a digital reminder system. PersonaDrift simulates 60-day interaction logs for synthetic users modeled after real PLwD, based on interviews with caregivers. These caregiver-informed personas vary in tone, modality, and communication habits, enabling realistic diversity in behavior. The benchmark focuses on two forms of longitudinal change that caregivers highlighted as particularly salient: flattened sentiment (reduced emotional tone and verbosity) and off-topic replies (semantic drift). These changes are injected progressively at different rates to emulate naturalistic cognitive trajectories, and the framework is designed to be extensible to additional behaviors in future use cases. To explore this novel application space, we evaluate several anomaly detection approaches, unsupervised statistical methods (CUSUM, EWMA, One-Class SVM), sequence models using contextual embeddings (GRU + BERT), and supervised classifiers in both generalized and personalized settings. Preliminary results show that flattened sentiment can often be detected with simple statistical models in users with low baseline variability, while detecting semantic drift requires temporal modeling and personalized baselines. Across both tasks, personalized classifiers consistently outperform generalized ones, highlighting the importance of individual behavioral context.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy of an Idiom: Tracing Non-Compositionality in Language Models</title>
<link>https://arxiv.org/abs/2511.16467</link>
<guid>https://arxiv.org/abs/2511.16467</guid>
<content:encoded><![CDATA[
arXiv:2511.16467v1 Announce Type: cross 
Abstract: We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Quantum Key Distribution Network Performance using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.16468</link>
<guid>https://arxiv.org/abs/2511.16468</guid>
<content:encoded><![CDATA[
arXiv:2511.16468v1 Announce Type: cross 
Abstract: This paper proposes an optimization of Quantum Key Distribution (QKD) Networks using Graph Neural Networks (GNN) framework. Today, the development of quantum computers threatens the security systems of classical cryptography. Moreover, as QKD networks are designed for protecting secret communication, they suffer from multiple operational difficulties: adaptive to dynamic conditions, optimization for multiple parameters and effective resource utilization. In order to overcome these obstacles, we propose a GNN-based framework which can model QKD networks as dynamic graphs and extracts exploitable characteristics from these networks' structure. The graph contains not only topological information but also specific characteristics associated with quantum communication (the number of edges between nodes, etc). Experimental results demonstrate that the GNN-optimized QKD network achieves a substantial increase in total key rate (from 27.1 Kbits/s to 470 Kbits/s), a reduced average QBER (from 6.6% to 6.0%), and maintains path integrity with a slight reduction in average transmission distance (from 7.13 km to 6.42 km). Furthermore, we analyze network performance across varying scales (10 to 250 nodes), showing improved link prediction accuracy and enhanced key generation rate in medium-sized networks. This work introduces a novel operation mode for QKD networks, shifting the paradigm of network optimization through adaptive and scalable quantum communication systems that enhance security and performance.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive vision-language learning with paraphrasing and negation</title>
<link>https://arxiv.org/abs/2511.16527</link>
<guid>https://arxiv.org/abs/2511.16527</guid>
<content:encoded><![CDATA[
arXiv:2511.16527v1 Announce Type: cross 
Abstract: Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks</title>
<link>https://arxiv.org/abs/2511.16540</link>
<guid>https://arxiv.org/abs/2511.16540</guid>
<content:encoded><![CDATA[
arXiv:2511.16540v1 Announce Type: cross 
Abstract: Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation</title>
<link>https://arxiv.org/abs/2511.16543</link>
<guid>https://arxiv.org/abs/2511.16543</guid>
<content:encoded><![CDATA[
arXiv:2511.16543v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.
  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.
  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Exterior-Embedding Neural Operator Framework for Preserving Conservation Laws</title>
<link>https://arxiv.org/abs/2511.16573</link>
<guid>https://arxiv.org/abs/2511.16573</guid>
<content:encoded><![CDATA[
arXiv:2511.16573v1 Announce Type: cross 
Abstract: Neural operators have demonstrated considerable effectiveness in accelerating the solution of time-dependent partial differential equations (PDEs) by directly learning governing physical laws from data. However, for PDEs governed by conservation laws(e.g., conservation of mass, energy, or matter), existing neural operators fail to satisfy conservation properties, which leads to degraded model performance and limited generalizability. Moreover, we observe that distinct PDE problems generally require different optimal neural network architectures. This finding underscores the inherent limitations of specialized models in generalizing across diverse problem domains.
  To address these limitations, we propose Exterior-Embedded Conservation Framework (ECF), a universal conserving framework that can be integrated with various data-driven neural operators to enforce conservation laws strictly in predictions. The framework consists of two key components: a conservation quantity encoder that extracts conserved quantities from input data, and a conservation quantity decoder that adjusts the neural operator's predictions using these quantities to ensure strict conservation compliance in the final output. Since our architecture enforces conservation laws, we theoretically prove that it enhances model performance. To validate the performance of our method, we conduct experiments on multiple conservation-law-constrained PDE scenarios, including adiabatic systems, shallow water equations, and the Allen-Cahn problem. These baselines demonstrate that our method effectively improves model accuracy while strictly enforcing conservation laws in the predictions.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthesis of Safety Specifications for Probabilistic Systems</title>
<link>https://arxiv.org/abs/2511.16579</link>
<guid>https://arxiv.org/abs/2511.16579</guid>
<content:encoded><![CDATA[
arXiv:2511.16579v1 Announce Type: cross 
Abstract: Ensuring that agents satisfy safety specifications can be crucial in safety-critical environments. While methods exist for controller synthesis with safe temporal specifications, most existing methods restrict safe temporal specifications to probabilistic-avoidance constraints. Formal methods typically offer more expressive ways to express safety in probabilistic systems, such as Probabilistic Computation Tree Logic (PCTL) formulas. Thus, in this paper, we develop a new approach that supports more general temporal properties expressed in PCTL. Our contribution is twofold. First, we develop a theoretical framework for the Synthesis of safe-PCTL specifications. We show how the reducing global specification satisfaction to local constraints, and define CPCTL, a fragment of safe-PCTL. We demonstrate how the expressiveness of CPCTL makes it a relevant fragment for the Synthesis Problem. Second, we leverage these results and propose a new Value Iteration-based algorithm to solve the synthesis problem for these more general temporal properties, and we prove the soundness and completeness of our method.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Quantum Integrated Sensing and Communication</title>
<link>https://arxiv.org/abs/2511.16597</link>
<guid>https://arxiv.org/abs/2511.16597</guid>
<content:encoded><![CDATA[
arXiv:2511.16597v1 Announce Type: cross 
Abstract: The integration of sensing and communication functionalities within a common system is one of the main innovation drivers for next-generation networks. In this paper, we introduce a quantum integrated sensing and communication (QISAC) protocol that leverages entanglement in quantum carriers of information to enable both superdense coding and quantum sensing. The proposed approach adaptively optimizes encoding and quantum measurement via variational circuit learning, while employing classical machine learning-based decoders and estimators to process the measurement outcomes. Numerical results for qudit systems demonstrate that the proposed QISAC protocol can achieve a flexible trade-off between classical communication rate and accuracy of parameter estimation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time dependent loss reweighting for flow matching and diffusion models is theoretically justified</title>
<link>https://arxiv.org/abs/2511.16599</link>
<guid>https://arxiv.org/abs/2511.16599</guid>
<content:encoded><![CDATA[
arXiv:2511.16599v1 Announce Type: cross 
Abstract: This brief note clarifies that, in Generator Matching (which subsumes a large family of flow matching and diffusion models over continuous, manifold, and discrete spaces), both the Bregman divergence loss and the linear parameterization of the generator can depend on both the current state $X_t$ and the time $t$, and we show that the expectation over time in the loss can be taken with respect to a broad class of time distributions. We also show this for Edit Flows, which falls outside of Generator Matching. That the loss can depend on $t$ clarifies that time-dependent loss weighting schemes, often used in practice to stabilize training, are theoretically justified when the specific flow or diffusion scheme is a special case of Generator Matching (or Edit Flows). It also often simplifies the construction of $X_1$-predictor schemes, which are sometimes preferred for model-related reasons. We show examples that rely upon the dependence of linear parameterizations, and of the Bregman divergence loss, on $t$ and $X_t$.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rate-optimal community detection near the KS threshold via node-robust algorithms</title>
<link>https://arxiv.org/abs/2511.16613</link>
<guid>https://arxiv.org/abs/2511.16613</guid>
<content:encoded><![CDATA[
arXiv:2511.16613v1 Announce Type: cross 
Abstract: We study community detection in the \emph{symmetric $k$-stochastic block model}, where $n$ nodes are evenly partitioned into $k$ clusters with intra- and inter-cluster connection probabilities $p$ and $q$, respectively.
  Our main result is a polynomial-time algorithm that achieves the minimax-optimal misclassification rate
  \begin{equation*}
  \exp \Bigl(-\bigl(1 \pm o(1)\bigr) \tfrac{C}{k}\Bigr),
  \quad \text{where } C = (\sqrt{pn} - \sqrt{qn})^2,
  \end{equation*}
  whenever $C \ge K\,k^2\,\log k$ for some universal constant $K$, matching the Kesten--Stigum (KS) threshold up to a $\log k$ factor.
  Notably, this rate holds even when an adversary corrupts an $\eta \le \exp\bigl(- (1 \pm o(1)) \tfrac{C}{k}\bigr)$ fraction of the nodes.
  To the best of our knowledge, the minimax rate was previously only attainable either via computationally inefficient procedures [ZZ15] or via polynomial-time algorithms that require strictly stronger assumptions such as $C \ge K k^3$ [GMZZ17].
  In the node-robust setting, the best known algorithm requires the substantially stronger condition $C \ge K k^{102}$ [LM22].
  Our results close this gap by providing the first polynomial-time algorithm that achieves the minimax rate near the KS threshold in both settings.
  Our work has two key technical contributions:
  (1) we robustify majority voting via the Sum-of-Squares framework,
  (2) we develop a novel graph bisection algorithm via robust majority voting, which allows us to significantly improve the misclassification rate to $1/\mathrm{poly}(k)$ for the initial estimation near the KS threshold.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Polynomials to Databases: Arithmetic Structures in Galois Theory</title>
<link>https://arxiv.org/abs/2511.16622</link>
<guid>https://arxiv.org/abs/2511.16622</guid>
<content:encoded><![CDATA[
arXiv:2511.16622v1 Announce Type: cross 
Abstract: We develop a computational framework for classifying Galois groups of irreducible degree-7 polynomials over~$\mathbb{Q}$, combining explicit resolvent methods with machine learning techniques. A database of over one million normalized projective septics is constructed, each annotated with algebraic invariants~$J_0, \dots, J_4$ derived from binary transvections. For each polynomial, we compute resolvent factorizations to determine its Galois group among the seven transitive subgroups of~$S_7$ identified by Foulkes. Using this dataset, we train a neurosymbolic classifier that integrates invariant-theoretic features with supervised learning, yielding improved accuracy in detecting rare solvable groups compared to coefficient-based models. The resulting database provides a reproducible resource for constructive Galois theory and supports empirical investigations into group distribution under height constraints. The methodology extends to higher-degree cases and illustrates the utility of hybrid symbolic-numeric techniques in computational algebra.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Guided Upsampling for Low-light Image Enhancement</title>
<link>https://arxiv.org/abs/2511.16623</link>
<guid>https://arxiv.org/abs/2511.16623</guid>
<content:encoded><![CDATA[
arXiv:2511.16623v1 Announce Type: cross 
Abstract: We introduce Adaptive Guided Upsampling (AGU), an efficient method for upscaling low-light images capable of optimizing multiple image quality characteristics at the same time, such as reducing noise and increasing sharpness. It is based on a guided image method, which transfers image characteristics from a guidance image to the target image. Using state-of-the-art guided methods, low-light images lack sufficient characteristics for this purpose due to their high noise level and low brightness, rendering suboptimal/not significantly improved images in the process. We solve this problem with multi-parameter optimization, learning the association between multiple low-light and bright image characteristics. Our proposed machine learning method learns these characteristics from a few sample images-pairs. AGU can render high-quality images in real time using low-quality, low-resolution input; our experiments demonstrate that it is superior to state-of-the-art methods in the addressed low-light use case.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Spatial Supersensing Without Spatial Supersensing</title>
<link>https://arxiv.org/abs/2511.16655</link>
<guid>https://arxiv.org/abs/2511.16655</guid>
<content:encoded><![CDATA[
arXiv:2511.16655v1 Announce Type: cross 
Abstract: Cambrian-S aims to take the first steps towards improving video world models with spatial supersensing by introducing (i) two benchmarks, VSI-Super-Recall (VSR) and VSI-Super-Counting (VSC), and (ii) bespoke predictive sensing inference strategies tailored to each benchmark. In this work, we conduct a critical analysis of Cambrian-S across both these fronts. First, we introduce a simple baseline, NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos. This shows benchmarks like VSR can be nearly solved without spatial cognition, world modeling or spatial supersensing. Second, we hypothesize that the tailored inference methods proposed by Cambrian-S likely exploit shortcut heuristics in the benchmark. We illustrate this with a simple sanity check on the VSC benchmark, called VSC-Repeat: We concatenate each video with itself 1-5 times, which does not change the number of unique objects. However, this simple perturbation entirely collapses the mean relative accuracy of Cambrian-S from 42% to 0%. A system that performs spatial supersensing and integrates information across experiences should recognize views of the same scene and keep object-count predictions unchanged; instead, Cambrian-S inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited. Taken together, our findings suggest that (i) current VSI-Super benchmarks do not yet reliably measure spatial supersensing, and (ii) predictive-sensing inference recipes used by Cambrian-S improve performance by inadvertently exploiting shortcuts rather than from robust spatial supersensing. We include the response from the Cambrian-S authors (in Appendix A) to provide a balanced perspective alongside our claims. We release our code at: https://github.com/bethgelab/supersanity
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations</title>
<link>https://arxiv.org/abs/2511.16661</link>
<guid>https://arxiv.org/abs/2511.16661</guid>
<content:encoded><![CDATA[
arXiv:2511.16661v1 Announce Type: cross 
Abstract: Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset Distillation for Pre-Trained Self-Supervised Vision Models</title>
<link>https://arxiv.org/abs/2511.16674</link>
<guid>https://arxiv.org/abs/2511.16674</guid>
<content:encoded><![CDATA[
arXiv:2511.16674v1 Announce Type: cross 
Abstract: The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Discriminative Feature Learning for Deep Multi-View Clustering</title>
<link>https://arxiv.org/abs/2103.15069</link>
<guid>https://arxiv.org/abs/2103.15069</guid>
<content:encoded><![CDATA[
arXiv:2103.15069v3 Announce Type: replace 
Abstract: Multi-view clustering is an important research topic due to its capability to utilize complementary information from multiple views. However, there are few methods to consider the negative impact caused by certain views with unclear clustering structures, resulting in poor multi-view clustering performance. To address this drawback, we propose self-supervised discriminative feature learning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders are applied to learn embedded features for each view independently. To leverage the multi-view complementary information, we concatenate all views' embedded features to form the global features, which can overcome the negative impact of some views' unclear clustering structures. In a self-supervised manner, pseudo-labels are obtained to build a unified target distribution to perform multi-view discriminative feature learning. During this process, global discriminative information can be mined to supervise all views to learn more discriminative features, which in turn are used to update the target distribution. Besides, this unified target distribution can make SDMVC learn consistent cluster assignments, which accomplishes the clustering consistency of multiple views while preserving their features' diversity. Experiments on various types of multi-view datasets show that SDMVC outperforms 14 competitors including classic and state-of-the-art methods. The code is available at https://github.com/SubmissionsIn/SDMVC.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A low-rank non-convex norm method for multiview graph clustering</title>
<link>https://arxiv.org/abs/2312.11157</link>
<guid>https://arxiv.org/abs/2312.11157</guid>
<content:encoded><![CDATA[
arXiv:2312.11157v2 Announce Type: replace 
Abstract: This study introduces a novel technique for multi-view clustering known as the "Consensus Graph-Based Multi-View Clustering Method Using Low-Rank Non-Convex Norm" (CGMVC-NC). Multi-view clustering is a challenging task in machine learning as it requires the integration of information from multiple data sources or views to cluster data points accurately. The suggested approach makes use of the structural characteristics of multi-view data tensors, introducing a non-convex tensor norm to identify correlations between these views. In contrast to conventional methods, this approach demonstrates superior clustering accuracy across several benchmark datasets. Despite the non-convex nature of the tensor norm used, the proposed method remains amenable to efficient optimization using existing algorithms. The approach provides a valuable tool for multi-view data analysis and has the potential to enhance our understanding of complex systems in various fields. Further research can explore the application of this method to other types of data and extend it to other machine-learning tasks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Replace Economic Choice Prediction Labs? The Case of Language-based Persuasion Games</title>
<link>https://arxiv.org/abs/2401.17435</link>
<guid>https://arxiv.org/abs/2401.17435</guid>
<content:encoded><![CDATA[
arXiv:2401.17435v5 Announce Type: replace 
Abstract: Human choice prediction in economic contexts is crucial for applications in marketing, finance, public policy, and more. This task, however, is often constrained by the difficulties in acquiring human choice data. With most experimental economics studies focusing on simple choice settings, the AI community has explored whether LLMs can substitute for humans in these predictions and examined more complex experimental economics settings. However, a key question remains: can LLMs generate training data for human choice prediction? We explore this in language-based persuasion games, a complex economic setting involving natural language in strategic interactions. Our experiments show that models trained on LLM-generated data can effectively predict human behavior in these games and even outperform models trained on actual human data. Beyond data generation, we investigate the dual role of LLMs as both data generators and predictors, introducing a comprehensive empirical study on the effectiveness of utilizing LLMs for data generation, human choice prediction, or both. We then utilize our choice prediction framework to analyze how strategic factors shape decision-making, showing that interaction history (rather than linguistic sentiment alone) plays a key role in predicting human decision-making in repeated interactions. Particularly, when LLMs capture history-dependent decision patterns similarly to humans, their predictive success improves substantially. Finally, we demonstrate the robustness of our findings across alternative persuasion-game settings, highlighting the broader potential of using LLM-generated data to model human decision-making.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse-PGD: A Unified Framework for Sparse Adversarial Perturbations Generation</title>
<link>https://arxiv.org/abs/2405.05075</link>
<guid>https://arxiv.org/abs/2405.05075</guid>
<content:encoded><![CDATA[
arXiv:2405.05075v4 Announce Type: replace 
Abstract: This work studies sparse adversarial perturbations, including both unstructured and structured ones. We propose a framework based on a white-box PGD-like attack method named Sparse-PGD to effectively and efficiently generate such perturbations. Furthermore, we combine Sparse-PGD with a black-box attack to comprehensively and more reliably evaluate the models' robustness against unstructured and structured sparse adversarial perturbations. Moreover, the efficiency of Sparse-PGD enables us to conduct adversarial training to build robust models against various sparse perturbations. Extensive experiments demonstrate that our proposed attack algorithm exhibits strong performance in different scenarios. More importantly, compared with other robust models, our adversarially trained model demonstrates state-of-the-art robustness against various sparse attacks. Codes are available at https://github.com/CityU-MLO/sPGD.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Property-guided Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing</title>
<link>https://arxiv.org/abs/2405.11783</link>
<guid>https://arxiv.org/abs/2405.11783</guid>
<content:encoded><![CDATA[
arXiv:2405.11783v3 Announce Type: replace 
Abstract: In this study, we explore the potential of using quantum natural language processing (QNLP) to inverse design metal-organic frameworks (MOFs) with targeted properties. Specifically, by analyzing 450 hypothetical MOF structures consisting of 3 topologies, 10 metal nodes and 15 organic ligands, we categorize these structures into four distinct classes for pore volume and $CO_{2}$ Henry's constant values. We then compare various QNLP models (i.e. the bag-of-words, DisCoCat (Distributional Compositional Categorical), and sequence-based models) to identify the most effective approach to process the MOF dataset. Using a classical simulator provided by the IBM Qiskit, the bag-of-words model is identified to be the optimum model, achieving validation accuracies of 88.6% and 78.0% for binary classification tasks on pore volume and $CO_{2}$ Henry's constant, respectively. Further, we developed multi-class classification models tailored to the probabilistic nature of quantum circuits, with average test accuracies of 92% and 80% across different classes for pore volume and $CO_{2}$ Henry's constant datasets. Finally, the performance of generating MOF with target properties showed accuracies of 93.5% for pore volume and 87% for $CO_{2}$ Henry's constant, respectively. Although our investigation covers only a fraction of the vast MOF search space, it marks a promising first step towards using quantum computing for materials design, offering a new perspective through which to explore the complex landscape of MOFs.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural Disentanglement of Causal and Correlated Concepts</title>
<link>https://arxiv.org/abs/2405.16219</link>
<guid>https://arxiv.org/abs/2405.16219</guid>
<content:encoded><![CDATA[
arXiv:2405.16219v2 Announce Type: replace 
Abstract: Controllable data generation aims to synthesize data by specifying values for target concepts. Achieving this reliably requires modeling the underlying generative factors and their relationships. In real-world scenarios, these factors exhibit both causal and correlational dependencies, yet most existing methods model only part of this structure. We propose the Causal-Correlation Variational Autoencoder (C2VAE), a unified framework that jointly captures causal and correlational relationships among latent factors. C2VAE organizes the latent space into a structured graph, identifying a set of root causes that govern the generative processes. By optimizing only the root factors relevant to target concepts, the model enables efficient and faithful control. Experiments on synthetic and real-world datasets demonstrate that C2VAE improves generation quality, disentanglement, and intervention fidelity over existing baselines.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Robust Pre-Trained Ensembles for Biomarker-Based Cancer Classification</title>
<link>https://arxiv.org/abs/2406.10087</link>
<guid>https://arxiv.org/abs/2406.10087</guid>
<content:encoded><![CDATA[
arXiv:2406.10087v2 Announce Type: replace 
Abstract: Certain cancer types, notably pancreatic cancer, are difficult to detect at an early stage, motivating robust biomarker-based screening. Liquid biopsies enable non-invasive monitoring of circulating biomarkers, but typical machine learning pipelines for high-dimensional tabular data (e.g., random forests, SVMs) rely on expensive hyperparameter tuning and can be brittle under class imbalance. We leverage a meta-trained Hyperfast model for classifying cancer, accomplishing the highest AUC of 0.9929 and simultaneously achieving robustness especially on highly imbalanced datasets compared to other ML algorithms in several binary classification tasks (e.g. breast invasive carcinoma; BRCA vs. non-BRCA). We also propose a novel ensemble model combining pre-trained Hyperfast model, XGBoost, and LightGBM for multi-class classification tasks, achieving an incremental increase in accuracy (0.9464) while merely using 500 PCA features; distinguishable from previous studies where they used more than 2,000 features for similar results. Crucially, we demonstrate robustness under class imbalance: empirically via balanced accuracy and minority-class recall across cancer-vs.-noncancer and cancer-vs.-rest settings, and theoretically by showing (i) a prototype-form final layer for Hyperfast that yields prior-insensitive decisions under bounded bias, and (ii) minority-error reductions for majority vote under mild error diversity. Together, these results indicate that pre-trained tabular models and simple ensembling can deliver state-of-the-art accuracy and improved minority-class performance with far fewer features and no additional tuning.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distance-Preserving Representations for Genomic Spatial Reconstruction</title>
<link>https://arxiv.org/abs/2408.00911</link>
<guid>https://arxiv.org/abs/2408.00911</guid>
<content:encoded><![CDATA[
arXiv:2408.00911v3 Announce Type: replace 
Abstract: The spatial context of single-cell gene expression data is crucial for many downstream analyses, yet often remains inaccessible due to practical and technical limitations, restricting the utility of such datasets. In this paper, we propose a generic representation learning and transfer learning framework dp-VAE, capable of reconstructing the spatial coordinates associated with the provided gene expression data. Central to our approach is a distance-preserving regularizer integrated into the loss function during training, ensuring the model effectively captures and utilizes spatial context signals from reference datasets. During the inference stage, the produced latent representation of the model can be used to reconstruct or impute the spatial context of the provided gene expression by solving a constrained optimization problem. We also explore the theoretical connections between distance-preserving loss, distortion, and the bi-Lipschitz condition within generative models. Finally, we demonstrate the effectiveness of dp-VAE in different tasks involving training robustness, out-of-sample evaluation, and transfer learning inference applications by testing it over 27 publicly available datasets. This underscores its applicability to a wide range of genomics studies that were previously hindered by the absence of spatial data.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asymptotic and Finite Sample Analysis of Nonexpansive Stochastic Approximations with Markovian Noise</title>
<link>https://arxiv.org/abs/2409.19546</link>
<guid>https://arxiv.org/abs/2409.19546</guid>
<content:encoded><![CDATA[
arXiv:2409.19546v5 Announce Type: replace 
Abstract: Stochastic approximation is a powerful class of algorithms with celebrated success. However, a large body of previous analysis focuses on stochastic approximations driven by contractive operators, which is not applicable in some important reinforcement learning settings like the average reward setting. This work instead investigates stochastic approximations with merely nonexpansive operators. In particular, we study nonexpansive stochastic approximations with Markovian noise, providing both asymptotic and finite sample analysis. Key to our analysis are novel bounds of noise terms resulting from the Poisson equation. As an application, we prove for the first time that classical tabular average reward temporal difference learning converges to a sample-path dependent fixed point.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2410.03972</link>
<guid>https://arxiv.org/abs/2410.03972</guid>
<content:encoded><![CDATA[
arXiv:2410.03972v3 Announce Type: replace 
Abstract: Task-trained recurrent neural networks (RNNs) are widely used in neuroscience and machine learning to model dynamical computations. To gain mechanistic insight into how neural systems solve tasks, prior work often reverse-engineers individual trained networks. However, different RNNs trained on the same task and achieving similar performance can exhibit strikingly different internal solutions, a phenomenon known as solution degeneracy. Here, we develop a unified framework to systematically quantify and control solution degeneracy across three levels: behavior, neural dynamics, and weight space. We apply this framework to 3,400 RNNs trained on four neuroscience-relevant tasks: flip-flop memory, sine wave generation, delayed discrimination, and path integration, while systematically varying task complexity, learning regime, network size, and regularization. We find that higher task complexity and stronger feature learning reduce degeneracy in neural dynamics but increase it in weight space, with mixed effects on behavior. In contrast, larger networks and structural regularization reduce degeneracy at all three levels. These findings empirically validate the Contravariance Principle and provide practical guidance for researchers seeking to tune the variability of RNN solutions, either to uncover shared neural mechanisms or to model the individual variability observed in biological systems. This work provides a principled framework for quantifying and controlling solution degeneracy in task-trained RNNs, offering new tools for building more interpretable and biologically grounded models of neural computation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopoTune : A Framework for Generalized Combinatorial Complex Neural Networks</title>
<link>https://arxiv.org/abs/2410.06530</link>
<guid>https://arxiv.org/abs/2410.06530</guid>
<content:encoded><![CDATA[
arXiv:2410.06530v5 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) effectively learn from relational data by leveraging graph symmetries. However, many real-world systems -- such as biological or social networks -- feature multi-way interactions that GNNs fail to capture. Topological Deep Learning (TDL) addresses this by modeling and leveraging higher-order structures, with Combinatorial Complex Neural Networks (CCNNs) offering a general and expressive approach that has been shown to outperform GNNs. However, TDL lacks the principled and standardized frameworks that underpin GNN development, restricting its accessibility and applicability. To address this issue, we introduce Generalized CCNNs (GCCNs), a simple yet powerful family of TDL models that can be used to systematically transform any (graph) neural network into its TDL counterpart. We prove that GCCNs generalize and subsume CCNNs, while extensive experiments on a diverse class of GCCNs show that these architectures consistently match or outperform CCNNs, often with less model complexity. In an effort to accelerate and democratize TDL, we introduce TopoTune, a lightweight software for defining, building, and training GCCNs with unprecedented flexibility and ease.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference</title>
<link>https://arxiv.org/abs/2502.04420</link>
<guid>https://arxiv.org/abs/2502.04420</guid>
<content:encoded><![CDATA[
arXiv:2502.04420v5 Announce Type: replace 
Abstract: KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we theoretically analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is generally more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 21.25\% compared with KIVI-KV8 quantization over various context lengths. Our code and searched configurations are available at https://github.com/cmd2001/KVTuner.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Closer Look at Adversarial Suffix Learning for Jailbreaking LLMs: Augmented Adversarial Trigger Learning</title>
<link>https://arxiv.org/abs/2503.12339</link>
<guid>https://arxiv.org/abs/2503.12339</guid>
<content:encoded><![CDATA[
arXiv:2503.12339v4 Announce Type: replace 
Abstract: Gradient optimization-based adversarial attack methods automate the learning of adversarial triggers to generate jailbreak prompts or leak system prompts. In this work, we take a closer look at the optimization objective of adversarial trigger learning and propose ATLA: Adversarial Trigger Learning with Augmented objectives. ATLA improves the negative log-likelihood loss used by previous studies into a weighted loss formulation that encourages the learned adversarial triggers to optimize more towards response format tokens. This enables ATLA to learn an adversarial trigger from just one query-response pair and the learned trigger generalizes well to other similar queries. We further design a variation to augment trigger optimization with an auxiliary loss that suppresses evasive responses. We showcase how to use ATLA to learn adversarial suffixes jailbreaking LLMs and to extract hidden system prompts. Empirically we demonstrate that ATLA consistently outperforms current state-of-the-art techniques, achieving nearly 100% success in attacking while requiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high generalization to unseen queries and transfer well to new LLMs. We released our code https://github.com/QData/ALTA_Augmented_Adversarial_Trigger_Learning
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Hidden Reasoning Process of Large Language Models by Misleading Them</title>
<link>https://arxiv.org/abs/2503.16401</link>
<guid>https://arxiv.org/abs/2503.16401</guid>
<content:encoded><![CDATA[
arXiv:2503.16401v2 Announce Type: replace 
Abstract: Large language models (LLMs) have been able to perform various forms of reasoning tasks in a wide range of scenarios, but are they truly engaging in task abstraction and rule-based reasoning beyond mere memorization? To answer this question, we propose a novel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether LLMs perform abstract reasoning by altering their original understanding of fundamental rules. In particular, by constructing datasets with math expressions or logical formulas that contradict correct principles, we fine-tune the model to learn those contradictory rules and assess its generalization ability on unseen test domains. Through a series of experiments, we find that current LLMs are capable of applying contradictory rules to solve practical math word problems and natural language reasoning tasks, implying the presence of an internal mechanism in LLMs that abstracts before reasoning.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Model Inversion Evaluation: From Misleading Standards to Reliable Privacy Assessment</title>
<link>https://arxiv.org/abs/2505.03519</link>
<guid>https://arxiv.org/abs/2505.03519</guid>
<content:encoded><![CDATA[
arXiv:2505.03519v4 Announce Type: replace 
Abstract: Model Inversion (MI) attacks aim to reconstruct information from private training data by exploiting access to machine learning models T. To evaluate such attacks, the standard evaluation framework relies on an evaluation model E, trained under the same task design as T. This framework has become the de facto standard for assessing progress in MI research, used across nearly all recent MI studies without question. In this paper, we present the first in-depth study of this evaluation framework. In particular, we identify a critical issue of this standard framework: Type-I adversarial examples. These are reconstructions that do not capture the visual features of private training data, yet are still deemed successful by T and ultimately transferable to E. Such false positives undermine the reliability of the standard MI evaluation framework. To address this issue, we introduce a new MI evaluation framework that replaces the evaluation model E with advanced Multimodal Large Language Models (MLLMs). By leveraging their general-purpose visual understanding, our MLLM-based framework does not depend on training of shared task design as in T, thus reducing Type-I transferability and providing more faithful assessments of reconstruction success. Using our MLLM-based evaluation framework, we reevaluate 27 diverse MI attack setups and empirically reveal consistently high false positive rates under the standard evaluation framework. Importantly, we demonstrate that many state-of-the-art (SOTA) MI methods report inflated attack accuracy, indicating that actual privacy leakage is significantly lower than previously believed. By uncovering this critical issue and proposing a robust solution, our work enables a reassessment of progress in MI research and sets a new standard for reliable and robust evaluation. Code can be found in https://github.com/hosytuyen/MI-Eval-MLLM
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms</title>
<link>https://arxiv.org/abs/2505.15141</link>
<guid>https://arxiv.org/abs/2505.15141</guid>
<content:encoded><![CDATA[
arXiv:2505.15141v2 Announce Type: replace 
Abstract: Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes a training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as a Multi-Armed Bandit problem and provide a general speculative decoding framework BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSpec is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Gradient Norm Clipping &amp; Non-Euclidean $(L_0,L_1)$-Smoothness</title>
<link>https://arxiv.org/abs/2506.01913</link>
<guid>https://arxiv.org/abs/2506.01913</guid>
<content:encoded><![CDATA[
arXiv:2506.01913v2 Announce Type: replace 
Abstract: This work introduces a hybrid non-Euclidean optimization method which generalizes gradient norm clipping by combining steepest descent and conditional gradient approaches. The method achieves the best of both worlds by establishing a descent property under a generalized notion of ($L_0$,$L_1$)-smoothness. Weight decay is incorporated in a principled manner by identifying a connection to the Frank-Wolfe short step. In the stochastic case, we show an order optimal $O(n^{-1/4})$ convergence rate by leveraging a momentum based gradient estimator. We discuss how to instantiate the algorithms for deep learning, which we dub Clipped Scion, and demonstrate their properties on image classification and language modeling. The code is available at https://github.com/LIONS-EPFL/ClippedScion.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast-DataShapley: Neural Modeling for Training Data Valuation</title>
<link>https://arxiv.org/abs/2506.05281</link>
<guid>https://arxiv.org/abs/2506.05281</guid>
<content:encoded><![CDATA[
arXiv:2506.05281v3 Announce Type: replace 
Abstract: The value and copyright of training data are crucial in the artificial intelligence industry. Service platforms should protect data providers' legitimate rights and fairly reward them for their contributions. Shapley value, a potent tool for evaluating contributions, outperforms other methods in theory, but its computational overhead escalates exponentially with the number of data providers. Recent works based on Shapley values attempt to mitigate computation complexity by approximation algorithms. However, they need to retrain for each test sample, leading to intolerable costs. We propose Fast-DataShapley, a one-pass training method that leverages the weighted least squares characterization of the Shapley value to train a reusable explainer model with real-time reasoning speed. Given new test samples, no retraining is required to calculate the Shapley values of the training data. Additionally, we propose three methods with theoretical guarantees to reduce training overhead from two aspects: the approximate calculation of the utility function and the group calculation of the training data. We analyze time complexity to show the efficiency of our methods. The experimental evaluations on various image datasets demonstrate superior performance and efficiency compared to baselines. Specifically, the performance is improved to more than 2 times, and the explainer's training speed can be increased by two orders of magnitude.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Policy Search, Retrieval, and Composition via Task Similarity in Collaborative Agentic Systems</title>
<link>https://arxiv.org/abs/2506.05577</link>
<guid>https://arxiv.org/abs/2506.05577</guid>
<content:encoded><![CDATA[
arXiv:2506.05577v3 Announce Type: replace 
Abstract: Agentic AI aims to create systems that set their own goals, adapt proactively to change, and refine behavior through continuous experience. Recent advances suggest that, when facing multiple and unforeseen tasks, agents could benefit from sharing machine-learned knowledge and reusing policies that have already been fully or partially learned by other agents. However, how to query, select, and retrieve policies from a pool of agents, and how to integrate such policies remains a largely unexplored area. This study explores how an agent decides what knowledge to select, from whom, and when and how to integrate it in its own policy in order to accelerate its own learning. The proposed algorithm, \emph{Modular Sharing and Composition in Collective Learning} (MOSAIC), improves learning in agentic collectives by combining (1) knowledge selection using performance signals and cosine similarity on Wasserstein task embeddings, (2) modular and transferable neural representations via masks, and (3) policy integration, composition and fine-tuning. MOSAIC outperforms isolated learners and global sharing approaches in both learning speed and overall performance, and in some cases solves tasks that isolated agents cannot. The results also demonstrate that selective, goal-driven reuse leads to less susceptibility to task interference. We also observe the emergence of self-organization, where agents solving simpler tasks accelerate the learning of harder ones through shared knowledge.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do-PFN: In-Context Learning for Causal Effect Estimation</title>
<link>https://arxiv.org/abs/2506.06039</link>
<guid>https://arxiv.org/abs/2506.06039</guid>
<content:encoded><![CDATA[
arXiv:2506.06039v3 Announce Type: replace 
Abstract: Estimation of causal effects is critical to a range of scientific disciplines. Existing methods for this task either require interventional data, knowledge about the ground truth causal graph, or rely on assumptions such as unconfoundedness, restricting their applicability in real-world settings. In the domain of tabular machine learning, Prior-data fitted networks (PFNs) have achieved state-of-the-art predictive performance, having been pre-trained on synthetic data to solve tabular prediction problems via in-context learning. To assess whether this can be transferred to the harder problem of causal effect estimation, we pre-train PFNs on synthetic data drawn from a wide variety of causal structures, including interventions, to predict interventional outcomes given observational data. Through extensive experiments on synthetic case studies, we show that our approach allows for the accurate estimation of causal effects without knowledge of the underlying causal graph. We also perform ablation studies that elucidate Do-PFN's scalability and robustness across datasets with a variety of causal characteristics.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Solution and Learning of Robust Factored MDPs</title>
<link>https://arxiv.org/abs/2508.00707</link>
<guid>https://arxiv.org/abs/2508.00707</guid>
<content:encoded><![CDATA[
arXiv:2508.00707v2 Announce Type: replace 
Abstract: Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling epistemic uncertainty about transition dynamics. Learning r-MDPs from interactions with an unknown environment enables the synthesis of robust policies with provable (PAC) guarantees on performance, but this can require a large number of sample interactions. We propose novel methods for solving and learning r-MDPs based on factored state-space representations that leverage the independence between model uncertainty across system components. Although policy synthesis for factored r-MDPs leads to hard, non-convex optimisation problems, we show how to reformulate these into tractable linear programs. Building on these, we also propose methods to learn factored model representations directly. Our experimental results show that exploiting factored structure can yield dimensional gains in sample efficiency, producing more effective robust policies with tighter performance guarantees than state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep-GLS: Report-Guided Generalized Label Smoothing for Robust Disease Detection</title>
<link>https://arxiv.org/abs/2508.02495</link>
<guid>https://arxiv.org/abs/2508.02495</guid>
<content:encoded><![CDATA[
arXiv:2508.02495v3 Announce Type: replace 
Abstract: Unlike nature image classification where groundtruth label is explicit and of no doubt, physicians commonly interpret medical image conditioned on certainty like using phrase "probable" or "likely". Existing medical image datasets either simply overlooked the nuance and polarise into binary label. Here, we propose a novel framework that leverages a Large Language Model (LLM) to directly mine medical reports to utilise the uncertainty relevant expression for supervision signal. At first, we collect uncertainty keywords from medical reports. Then, we use Qwen-3 4B to identify the textual uncertainty and map them into an adaptive Generalized Label Smoothing (GLS) rate. This rate allows our model to treat uncertain labels not as errors, but as informative signals, effectively incorporating expert skepticism into the training process. We establish a new clinical expert uncertainty-aware benchmark to rigorously evaluate this problem. Experiments demonstrate that our approach significantly outperforms state-of-the-art methods in medical disease detection. The curated uncertainty words database, code, and benchmark will be made publicly available upon acceptance.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.14765</link>
<guid>https://arxiv.org/abs/2508.14765</guid>
<content:encoded><![CDATA[
arXiv:2508.14765v2 Announce Type: replace 
Abstract: Designing therapeutic peptides with tailored properties is hindered by the vastness of sequence space, limited experimental data, and poor interpretability of current generative models. To address these challenges, we introduce PepThink-R1, a generative framework that integrates large language models (LLMs) with chain-of-thought (CoT) supervised fine-tuning and reinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly reasons about monomer-level modifications during sequence generation, enabling interpretable design choices while optimizing for multiple pharmacological properties. Guided by a tailored reward function balancing chemical validity and property improvements, the model autonomously explores diverse sequence variants. We demonstrate that PepThink-R1 generates cyclic peptides with significantly enhanced lipophilicity, stability, and exposure, outperforming existing general LLMs (e.g., GPT-5) and domain-specific baseline in both optimization success and interpretability. To our knowledge, this is the first LLM-based peptide design framework that combines explicit reasoning with RL-driven property control, marking a step toward reliable and transparent peptide optimization for therapeutic discovery.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretability as Alignment: Making Internal Understanding a Design Principle</title>
<link>https://arxiv.org/abs/2509.08592</link>
<guid>https://arxiv.org/abs/2509.08592</guid>
<content:encoded><![CDATA[
arXiv:2509.08592v2 Announce Type: replace 
Abstract: Frontier AI systems require governance mechanisms that can verify internal alignment, not just behavioral compliance. Private governance mechanisms audits, certification, insurance, and procurement are emerging to complement public regulation, but they require technical substrates that generate verifiable causal evidence about model behavior. This paper argues that mechanistic interpretability provides this substrate. We frame interpretability not as post-hoc explanation but as a design constraint embedding auditability, provenance, and bounded transparency within model architectures. Integrating causal abstraction theory and empirical benchmarks such as MIB and LoBOX, we outline how interpretability-first models can underpin private assurance pipelines and role-calibrated transparency frameworks. This reframing situates interpretability as infrastructure for private AI governance bridging the gap between technical reliability and institutional accountability.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Reinforcement Learning, Genetic Algorithms and Transformers for background determination in particle physics</title>
<link>https://arxiv.org/abs/2509.14894</link>
<guid>https://arxiv.org/abs/2509.14894</guid>
<content:encoded><![CDATA[
arXiv:2509.14894v2 Announce Type: replace 
Abstract: Experimental studies of beauty hadron decays face significant challenges due to a wide range of backgrounds arising from the numerous possible decay channels with similar final states. For a particular signal decay, the process for ascertaining the most relevant background processes necessitates a detailed analysis of final state particles, potential misidentifications, and kinematic overlaps, which, due to computational limitations, is restricted to the simulation of only the most relevant backgrounds. Moreover, this process typically relies on the physicist's intuition and expertise, as no systematic method exists.
  This paper has two primary goals. First, from a particle physics perspective, we present a novel approach that utilises Reinforcement Learning (RL) to overcome the aforementioned challenges by systematically determining the critical backgrounds affecting beauty hadron decay measurements. While beauty hadron physics serves as the case study in this work, the proposed strategy is broadly adaptable to other types of particle physics measurements. Second, from a Machine Learning perspective, we introduce a novel algorithm which exploits the synergy between RL and Genetic Algorithms (GAs) for environments with highly sparse rewards and a large trajectory space. This strategy leverages GAs to efficiently explore the trajectory space and identify successful trajectories, which are used to guide the RL agent's training. Our method also incorporates a transformer architecture for the RL agent to handle token sequences representing decays.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.23928</link>
<guid>https://arxiv.org/abs/2509.23928</guid>
<content:encoded><![CDATA[
arXiv:2509.23928v2 Announce Type: replace 
Abstract: Speculative decoding has proven effective for accelerating inference in Large Language Models (LLMs), yet its extension to Vision-Language Models (VLMs) remains limited by the computational burden and semantic inconsistency introduced by visual tokens. Recent studies reveal that visual tokens in large VLMs are highly redundant, and most of them can be removed without compromising generation quality. Motivated by this observation, we propose HiViS (Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models), a framework that utilizes the target VLM as a semantic fusion model, allowing the drafter to obtain visual information without explicitly processing visual tokens, ensuring that the drafter's prefill sequence length matches that of the textual tokens. Furthermore, HiViS employs a time-step-aware aligned training scheme that allows the drafter to autonomously propagate and refine instructive visual-textual semantics during independent drafting, guided by step-dependent bias-correction residuals. Extensive experiments across representative VLMs and benchmarks demonstrate that HiViS achieves significant improvements in average acceptance length and speedup ratio.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Objective $\textit{min-max}$ Online Convex Optimization</title>
<link>https://arxiv.org/abs/2510.13560</link>
<guid>https://arxiv.org/abs/2510.13560</guid>
<content:encoded><![CDATA[
arXiv:2510.13560v2 Announce Type: replace 
Abstract: In online convex optimization (OCO), a single loss function sequence is revealed over a time horizon of $T$, and an online algorithm has to choose its action at time $t$, before the loss function at time $t$ is revealed. The goal of the online algorithm is to incur minimal penalty (called $\textit{regret}$ compared to a static optimal action made by an optimal offline algorithm knowing all functions of the sequence in advance.
  In this paper, we broaden the horizon of OCO, and consider multi-objective OCO, where there are $K$ distinct loss function sequences, and an algorithm has to choose its action at time $t$, before the $K$ loss functions at time $t$ are revealed. To capture the tradeoff between tracking the $K$ different sequences, we consider the $\textit{min-max}$ regret, where the benchmark (optimal offline algorithm) takes a static action across all time slots that minimizes the maximum of the total loss (summed across time slots) incurred by each of the $K$ sequences. An online algorithm is allowed to change its action across time slots, and its {\it min-max} regret is defined as the difference between its $\textit{min-max}$ cost and that of the benchmark. The $\textit{min-max}$ regret is a stringent performance measure and an algorithm with small regret needs to `track' all loss function sequences closely at all times.
  We consider this $\textit{min-max}$ regret in the i.i.d. input setting where all loss functions are i.i.d. generated from an unknown distribution. For the i.i.d. model we propose a simple algorithm that combines the well-known $\textit{Hedge}$ and online gradient descent (OGD) and show via a remarkably simple proof that its expected $\textit{min-max}$ regret is $O(\sqrt{T \log K})$.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GAPO: Robust Advantage Estimation for Real-World Code LLMs</title>
<link>https://arxiv.org/abs/2510.21830</link>
<guid>https://arxiv.org/abs/2510.21830</guid>
<content:encoded><![CDATA[
arXiv:2510.21830v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) is widely used for post-training large language models (LLMs) in code editing, where group-relative methods like GRPO are popular for their critic-free, normalized advantage estimation. However, in real-world code-editing scenarios, reward distributions are often skewed with unpredictable outliers, leading to distorted advantage computation and increased noise. To address this issue, we propose Group Adaptive Policy Optimization (GAPO), which adaptively finds an outlier-free highest-density interval (HDI) per prompt and then uses the median of that interval as an adaptive Q to replace the group mean in advantage calculation. This adaptive Q robustly handles skewed distributions while remaining plug-and-play and efficient. We validate GAPO on nine instruction-tuned LLMs (3B-14B) using a large internal dataset of 51,844 real-world, history-aware code-editing tasks across 10 languages, demonstrating consistent improvements in exact match accuracy over GRPO and its variant DAPO. Code is publicly available.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation</title>
<link>https://arxiv.org/abs/2511.00588</link>
<guid>https://arxiv.org/abs/2511.00588</guid>
<content:encoded><![CDATA[
arXiv:2511.00588v2 Announce Type: replace 
Abstract: Large language models (LLMs) offer transformative potential for clinical decision support in spine surgery but pose significant risks through hallucinations, which are factually inconsistent or contextually misaligned outputs that may compromise patient safety. This study introduces a clinician-centered framework to quantify hallucination risks by evaluating diagnostic precision, recommendation quality, reasoning robustness, output coherence, and knowledge alignment. We assessed six leading LLMs across 30 expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes domains such as trauma and infection. A critical finding reveals that reasoning-enhanced model variants did not uniformly outperform standard counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92), indicating extended chain-of-thought reasoning alone is insufficient for clinical reliability. Multidimensional stress-testing exposed model-specific vulnerabilities, with recommendation quality degrading by 7.4% under amplified complexity. This decline contrasted with marginal improvements in rationality (+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning divergence between perceived coherence and actionable guidance. Our findings advocate integrating interpretability mechanisms (e.g., reasoning chain visualization) into clinical workflows and establish a safety-aware validation framework for surgical LLM deployment.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Deep Learning based Carbon Price Forecasting Framework with Structural Breakpoints Detection and Signal Denoising</title>
<link>https://arxiv.org/abs/2511.04988</link>
<guid>https://arxiv.org/abs/2511.04988</guid>
<content:encoded><![CDATA[
arXiv:2511.04988v2 Announce Type: replace 
Abstract: Accurately forecasting carbon prices is essential for informed energy market decision-making, guiding sustainable energy planning, and supporting effective decarbonization strategies. However, it remains challenging due to structural breaks and high-frequency noise caused by frequent policy interventions and market shocks. Existing studies, including the most recent baseline approaches, have attempted to incorporate breakpoints but often treat denoising and modeling as separate processes and lack systematic evaluation across advanced deep learning architectures, limiting the robustness and the generalization capability. To address these gaps, this paper proposes a comprehensive hybrid framework that integrates structural break detection (Bai-Perron, ICSS, and PELT algorithms), wavelet signal denoising, and three state-of-the-art deep learning models (LSTM, GRU, and TCN). Using European Union Allowance (EUA) spot prices from 2007 to 2024 and exogenous features such as energy prices and policy indicators, the framework constructs univariate and multivariate datasets for comparative evaluation. Experimental results demonstrate that our proposed PELT-WT-TCN achieves the highest prediction accuracy, reducing forecasting errors by 22.35% in RMSE and 18.63% in MAE compared to the state-of-the-art baseline model (Breakpoints with Wavelet and LSTM), and by 70.55% in RMSE and 74.42% in MAE compared to the original LSTM without decomposition from the same baseline study. These findings underscore the value of integrating structural awareness and multiscale decomposition into deep learning architectures to enhance accuracy and interpretability in carbon price forecasting and other nonstationary financial time series.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabDistill: Distilling Transformers into Neural Nets for Few-Shot Tabular Classification</title>
<link>https://arxiv.org/abs/2511.05704</link>
<guid>https://arxiv.org/abs/2511.05704</guid>
<content:encoded><![CDATA[
arXiv:2511.05704v2 Announce Type: replace 
Abstract: Transformer-based models have shown promising performance on tabular data compared to their classical counterparts such as neural networks and Gradient Boosted Decision Trees (GBDTs) in scenarios with limited training data. They utilize their pre-trained knowledge to adapt to new domains, achieving commendable performance with only a few training examples, also called the few-shot regime. However, the performance gain in the few-shot regime comes at the expense of significantly increased complexity and number of parameters. To circumvent this trade-off, we introduce TabDistill, a new strategy to distill the pre-trained knowledge in complex transformer-based models into simpler neural networks for effectively classifying tabular data. Our framework yields the best of both worlds: being parameter-efficient while performing well with limited training data. The distilled neural networks surpass classical baselines such as regular neural networks, XGBoost and logistic regression under equal training data, and in some cases, even the original transformer-based models that they were distilled from.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kaggle Chronicles: 15 Years of Competitions, Community and Data Science Innovation</title>
<link>https://arxiv.org/abs/2511.06304</link>
<guid>https://arxiv.org/abs/2511.06304</guid>
<content:encoded><![CDATA[
arXiv:2511.06304v2 Announce Type: replace 
Abstract: Since 2010, Kaggle has been a platform where data scientists from around the world come together to compete, collaborate, and push the boundaries of Data Science. Over these 15 years, it has grown from a purely competition-focused site into a broader ecosystem with forums, notebooks, models, datasets, and more. With the release of the Kaggle Meta Code and Kaggle Meta Datasets, we now have a unique opportunity to explore these competitions, technologies, and real-world applications of Machine Learning and AI. And so in this study, we take a closer look at 15 years of data science on Kaggle - through metadata, shared code, community discussions, and the competitions themselves. We explore Kaggle's growth, its impact on the data science community, uncover hidden technological trends, analyze competition winners, how Kagglers approach problems in general, and more. We do this by analyzing millions of kernels and discussion threads to perform both longitudinal trend analysis and standard exploratory data analysis. Our findings show that Kaggle is a steadily growing platform with increasingly diverse use cases, and that Kagglers are quick to adapt to new trends and apply them to real-world challenges, while producing - on average - models with solid generalization capabilities. We also offer a snapshot of the platform as a whole, highlighting its history and technological evolution. Finally, this study is accompanied by a video (https://www.youtube.com/watch?v=YVOV9bIUNrM) and a Kaggle write-up (https://kaggle.com/competitions/meta-kaggle-hackathon/writeups/kaggle-chronicles-15-years-of-competitions-communi) for your convenience.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaberNet: Causal Representation Learning for Cross-Domain HVAC Energy Prediction</title>
<link>https://arxiv.org/abs/2511.06634</link>
<guid>https://arxiv.org/abs/2511.06634</guid>
<content:encoded><![CDATA[
arXiv:2511.06634v2 Announce Type: replace 
Abstract: Cross-domain HVAC energy prediction is essential for scalable building energy management, particularly because collecting extensive labeled data for every new building is both costly and impractical. Yet, this task remains highly challenging due to the scarcity and heterogeneity of data across different buildings, climate zones, and seasonal patterns. In particular, buildings situated in distinct climatic regions introduce variability that often leads existing methods to overfit to spurious correlations, rely heavily on expert intervention, or compromise on data diversity. To address these limitations, we propose CaberNet, a causal and interpretable deep sequence model that learns invariant (Markov blanket) representations for robust cross-domain prediction. In a purely data-driven fashion and without requiring any prior knowledge, CaberNet integrates i) a global feature gate trained with a self-supervised Bernoulli regularization to distinguish superior causal features from inferior ones, and ii) a domain-wise training scheme that balances domain contributions, minimizes cross-domain loss variance, and promotes latent factor independence. We evaluate CaberNet on real-world datasets collected from three buildings located in three climatically diverse cities, and it consistently outperforms all baselines, achieving a 22.9% reduction in normalized mean squared error (NMSE) compared to the best benchmark. Our code is available at https://github.com/SusCom-Lab/CaberNet-CRL.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction</title>
<link>https://arxiv.org/abs/2511.07899</link>
<guid>https://arxiv.org/abs/2511.07899</guid>
<content:encoded><![CDATA[
arXiv:2511.07899v2 Announce Type: replace 
Abstract: Safety assurance is a fundamental requirement for deploying learning-enabled autonomous systems. Hamilton-Jacobi (HJ) reachability analysis is a fundamental method for formally verifying safety and generating safe controllers. However, computing the HJ value function that characterizes the backward reachable set (BRS) of a set of user-defined failure states is computationally expensive, especially for high-dimensional systems, motivating the use of reinforcement learning approaches to approximate the value function. Unfortunately, a learned value function and its corresponding safe policy are not guaranteed to be correct. The learned value function evaluated at a given state may not be equal to the actual safety return achieved by following the learned safe policy. To address this challenge, we introduce a conformal prediction-based (CP) framework that bounds such uncertainty. We leverage CP to provide probabilistic safety guarantees when using learned HJ value functions and policies to prevent control systems from reaching failure states. Specifically, we use CP to calibrate the switching between the unsafe nominal controller and the learned HJ-based safe policy and to derive safety guarantees under this switched policy. We also investigate using an ensemble of independently trained HJ value functions as a safety filter and compare this ensemble approach to using individual value functions alone.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAMP: Spatial-Temporal Adapter with Multi-Head Pooling</title>
<link>https://arxiv.org/abs/2511.10848</link>
<guid>https://arxiv.org/abs/2511.10848</guid>
<content:encoded><![CDATA[
arXiv:2511.10848v2 Announce Type: replace 
Abstract: Time series foundation models (TSFMs) pretrained on data from multiple domains have shown strong performance on diverse modeling tasks. Various efforts have been made to develop foundation models specific to electroencephalography (EEG) data, which records brain electrical activity as time series. However, no comparative analysis of EEG-specific foundation models (EEGFMs) versus general TSFMs has been performed on EEG-specific tasks. We introduce a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), which leverages univariate embeddings produced by a general TSFM, implicitly models spatial-temporal characteristics of EEG data, and achieves performance comparable to state-of-the-art EEGFMs. A comprehensive analysis is performed on 8 benchmark datasets of clinical tasks using EEG for classification, along with ablation studies. Our proposed adapter is lightweight in trainable parameters and flexible in the inputs it can accommodate, supporting easy modeling of EEG data using TSFMs.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-View Polymer Representations for the Open Polymer Prediction</title>
<link>https://arxiv.org/abs/2511.10893</link>
<guid>https://arxiv.org/abs/2511.10893</guid>
<content:encoded><![CDATA[
arXiv:2511.10893v2 Announce Type: replace 
Abstract: We address polymer property prediction with a multi-view design that exploits complementary representations. Our system integrates four families: (i) tabular RDKit/Morgan descriptors, (ii) graph neural networks, (iii) 3D-informed representations, and (iv) pretrained SMILES language models, and averages per-property predictions via a uniform ensemble. Models are trained with 10-fold splits and evaluated with SMILES test-time augmentation. The approach ranks 9th of 2241 teams in the Open Polymer Prediction Challenge at NeurIPS 2025. The submitted ensemble achieves a public MAE of 0.057 and a private MAE of 0.082.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space</title>
<link>https://arxiv.org/abs/2209.12746</link>
<guid>https://arxiv.org/abs/2209.12746</guid>
<content:encoded><![CDATA[
arXiv:2209.12746v3 Announce Type: replace-cross 
Abstract: As research on image inversion advances, the process is generally divided into two stages. The first step is Image Embedding, involves using an encoder or optimization procedure to embed an image and obtain its corresponding latent code. The second stage, referred to as Result Refinement, further improves the inversion and editing outcomes. Although this refinement stage substantially enhances reconstruction fidelity, perception and editability remain largely unchanged and are highly dependent on the latent codes derived from the first stage. Therefore, a key challenge lies in obtaining latent codes that preserve reconstruction fidelity while simultaneously improving perception and editability. In this work, we first reveal that these two properties are closely related to the degree of alignment (or disalignment) between the inverted latent codes and the synthetic distribution. Based on this insight, we propose the \textbf{ Latent Space Alignment Inversion Paradigm (LSAP)}, which integrates both an evaluation metric and a unified inversion solution. Specifically, we introduce the \textbf{Normalized Style Space ($\mathcal{S^N}$ space)} and \textbf{Normalized Style Space Cosine Distance (NSCD)} to quantify the disalignment of inversion methods. Moreover, our paradigm can be optimized for both encoder-based and optimization-based embeddings, providing a consistent alignment framework. Extensive experiments across various domains demonstrate that NSCD effectively captures perceptual and editable characteristics, and that our alignment paradigm achieves state-of-the-art performance in both stages of inversion.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial-and-Frequency-aware Restoration method for Images based on Diffusion Models</title>
<link>https://arxiv.org/abs/2401.17629</link>
<guid>https://arxiv.org/abs/2401.17629</guid>
<content:encoded><![CDATA[
arXiv:2401.17629v2 Announce Type: replace-cross 
Abstract: Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Bilevel Optimization: A Perspective from Transient Iteration Complexity</title>
<link>https://arxiv.org/abs/2402.03167</link>
<guid>https://arxiv.org/abs/2402.03167</guid>
<content:encoded><![CDATA[
arXiv:2402.03167v4 Announce Type: replace-cross 
Abstract: Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, most decentralized SBO algorithms focus solely on asymptotic convergence rates, overlooking transient iteration complexity-the number of iterations required before asymptotic rates dominate, which results in limited understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. To address this issue, this paper introduces D-SOBA, a Decentralized Stochastic One-loop Bilevel Algorithm framework. D-SOBA comprises two variants: D-SOBA-SO, which incorporates second-order Hessian and Jacobian matrices, and D-SOBA-FO, which relies entirely on first-order gradients. We provide a comprehensive non-asymptotic convergence analysis and establish the transient iteration complexity of D-SOBA. This provides the first theoretical understanding of how network topology, data heterogeneity, and nested bilevel structures influence decentralized SBO. Extensive experimental results demonstrate the efficiency and theoretical advantages of D-SOBA.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks</title>
<link>https://arxiv.org/abs/2403.02011</link>
<guid>https://arxiv.org/abs/2403.02011</guid>
<content:encoded><![CDATA[
arXiv:2403.02011v3 Announce Type: replace-cross 
Abstract: Citizen science monitoring programs can generate large amounts of valuable data, but are often affected by sampling bias. We focus on a citizen science initiative that records plant-pollinator interactions, with the goal of learning embeddings that summarize the observed interactions while accounting for such bias. In our approach, plant and pollinator species are embedded based on their probability of interaction. These embeddings are derived using an adaptation of variational graph autoencoders for bipartite graphs. To mitigate the influence of sampling bias, we incorporate the Hilbert-Schmidt Independence Criterion (HSIC) to ensure independence from continuous variables related to the sampling process. This allows us to integrate a fairness perspective, commonly explored in the social sciences, into the analysis of ecological data. We validate our method through a simulation study replicating key aspects of the sampling process and demonstrate its applicability and effectiveness using the Spipoll dataset.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CardioLab: Laboratory Values Estimation from Electrocardiogram Features - An Exploratory Study</title>
<link>https://arxiv.org/abs/2407.18629</link>
<guid>https://arxiv.org/abs/2407.18629</guid>
<content:encoded><![CDATA[
arXiv:2407.18629v3 Announce Type: replace-cross 
Abstract: Laboratory value represents a cornerstone of medical diagnostics, but suffers from slow turnaround times, and high costs and only provides information about a single point in time. The continuous estimation of laboratory values from non-invasive data such as electrocardiogram (ECG) would therefore mark a significant frontier in healthcare monitoring. Despite its potential, this domain remains relatively underexplored. In this preliminary study, we used a publicly available dataset (MIMIC-IV-ECG) to investigate the feasibility of inferring laboratory values from ECG features and patient demographics using tree-based models (XGBoost). We define the prediction task as a binary problem of whether the lab value falls into low or high abnormalities. We assessed model performance with AUROC. Our findings demonstrate promising results in the estimation of laboratory values related to different organ systems. While further research and validation are warranted to fully assess the clinical utility and generalizability of the approach, our findings lay the groundwork for future investigations for laboratory value estimation using ECG data. Such advancements hold promise for revolutionizing predictive healthcare applications, offering faster, non-invasive, and more affordable means of patient monitoring.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-machine-assisted Drug Discovery</title>
<link>https://arxiv.org/abs/2408.13479</link>
<guid>https://arxiv.org/abs/2408.13479</guid>
<content:encoded><![CDATA[
arXiv:2408.13479v5 Announce Type: replace-cross 
Abstract: Drug discovery is lengthy and expensive, with traditional computer-aided design facing limits. This paper examines integrating quantum computing across the drug development cycle to accelerate and enhance workflows and rigorous decision-making. It highlights quantum approaches for molecular simulation, drug-target interaction prediction, and optimizing clinical trials. Leveraging quantum capabilities could accelerate timelines and costs for bringing therapies to market, improving efficiency and ultimately benefiting public health.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimation of Cardiac and Non-cardiac Diagnosis from Electrocardiogram Features</title>
<link>https://arxiv.org/abs/2408.17329</link>
<guid>https://arxiv.org/abs/2408.17329</guid>
<content:encoded><![CDATA[
arXiv:2408.17329v2 Announce Type: replace-cross 
Abstract: Ensuring timely and accurate diagnosis of medical conditions is paramount for effective patient care. Electrocardiogram (ECG) signals are fundamental for evaluating a patient's cardiac health and are readily available. Despite this, little attention has been given to the remarkable potential of ECG data in detecting non-cardiac conditions. In our study, we used publicly available datasets (MIMIC-IV-ECG-ICD and ECG-VIEW II) to investigate the feasibility of inferring general diagnostic conditions from ECG features. To this end, we trained a tree-based model (XGBoost) based on ECG features and basic demographic features to estimate a wide range of diagnoses, encompassing both cardiac and non-cardiac conditions. Our results demonstrate the reliability of estimating 23 cardiac as well as 21 non-cardiac conditions above 0.7 AUROC in a statistically significant manner across a wide range of physiological categories. Our findings underscore the predictive potential of ECG data in identifying well-known cardiac conditions. However, even more striking, this research represents a pioneering effort in systematically expanding the scope of ECG-based diagnosis to conditions not traditionally associated with the cardiac system.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modelling Global Trade with Optimal Transport</title>
<link>https://arxiv.org/abs/2409.06554</link>
<guid>https://arxiv.org/abs/2409.06554</guid>
<content:encoded><![CDATA[
arXiv:2409.06554v3 Announce Type: replace-cross 
Abstract: Global trade is shaped by a complex mix of factors beyond supply and demand, including tangible variables like transport costs and tariffs, as well as less quantifiable influences such as political and economic relations. Traditionally, economists model trade using gravity models, which rely on explicit covariates that might struggle to capture these subtler drivers of trade. In this work, we employ optimal transport and a deep neural network to learn a time-dependent cost function from data, without imposing a specific functional form. This approach consistently outperforms traditional gravity models in accuracy and has similar performance to three-way gravity models, while providing natural uncertainty quantification. Applying our framework to global food and agricultural trade, we show that the Global South suffered disproportionately from the war in Ukraine's impact on wheat markets. We also analyse the effects of free-trade agreements and trade disputes with China, as well as Brexit's impact on British trade with Europe, uncovering hidden patterns that trade volumes alone cannot reveal.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI, Managerial Expectations, and Economic Activity</title>
<link>https://arxiv.org/abs/2410.03897</link>
<guid>https://arxiv.org/abs/2410.03897</guid>
<content:encoded><![CDATA[
arXiv:2410.03897v4 Announce Type: replace-cross 
Abstract: We use generative AI to extract managerial expectations about their economic outlook from 120,000+ corporate conference call transcripts. The resulting AI Economy Score predicts GDP growth, production, and employment up to 10 quarters ahead, beyond existing measures like survey forecasts. Moreover, industry and firm-level measures provide valuable information about sector-specific and individual firm activities. A composite measure that integrates managerial expectations about firm, industry, and macroeconomic conditions further significantly improves the forecasting power and predictive horizon of national and sectoral growth. Our findings show managerial expectations offer unique insights into economic activity, with implications for both macroeconomic and microeconomic decision-making.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEARNER: Contrastive Pretraining for Learning Fine-Grained Patient Progression from Coarse Inter-Patient Labels</title>
<link>https://arxiv.org/abs/2411.01144</link>
<guid>https://arxiv.org/abs/2411.01144</guid>
<content:encoded><![CDATA[
arXiv:2411.01144v2 Announce Type: replace-cross 
Abstract: Predicting whether a treatment leads to meaningful improvement is a central challenge in personalized medicine, particularly when disease progression manifests as subtle visual changes over time. While data-driven deep learning (DL) offers a promising route to automate such predictions, acquiring large-scale longitudinal data for each individual patient remains impractical. To address this limitation, we explore whether inter-patient variability can serve as a proxy for learning intra-patient progression. We propose LEARNER, a contrastive pretraining framework that leverages coarsely labeled inter-patient data to learn fine-grained, patient-specific representations. Using lung ultrasound (LUS) and brain MRI datasets, we demonstrate that contrastive objectives trained on coarse inter-patient differences enable models to capture subtle intra-patient changes associated with treatment response. Across both modalities, our approach improves downstream classification accuracy and F1-score compared to standard MSE pretraining, highlighting the potential of inter-patient contrastive learning for individualized outcome prediction.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable machine learning for neoplasms diagnosis via electrocardiograms: an externally validated study</title>
<link>https://arxiv.org/abs/2412.07737</link>
<guid>https://arxiv.org/abs/2412.07737</guid>
<content:encoded><![CDATA[
arXiv:2412.07737v2 Announce Type: replace-cross 
Abstract: Background: Neoplasms are a major cause of mortality globally, where early diagnosis is essential for improving outcomes. Current diagnostic methods are often invasive, expensive, and inaccessible in resource-limited settings. This study explores the potential of electrocardiogram (ECG) data, a widely available and non-invasive tool for diagnosing neoplasms through cardiovascular changes linked to neoplastic presence.
  Methods: A diagnostic pipeline combining tree-based machine learning models with Shapley value analysis for explainability was developed. The model was trained and internally validated on a large dataset and externally validated on an independent cohort to ensure robustness and generalizability. Key ECG features contributing to predictions were identified and analyzed.
  Results: The model achieved high diagnostic accuracy in both internal testing and external validation cohorts. Shapley value analysis highlighted significant ECG features, including novel predictors. The approach is cost-effective, scalable, and suitable for resource-limited settings, offering insights into cardiovascular changes associated with neoplasms and their therapies.
  Conclusions: This study demonstrates the feasibility of using ECG signals and machine learning for non-invasive neoplasm diagnosis. By providing interpretable insights into cardio-neoplasm interactions, this method addresses gaps in diagnostics and supports integration into broader diagnostic and therapeutic frameworks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Architectures for High Resolution Vision-Language Models</title>
<link>https://arxiv.org/abs/2501.02584</link>
<guid>https://arxiv.org/abs/2501.02584</guid>
<content:encoded><![CDATA[
arXiv:2501.02584v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking</title>
<link>https://arxiv.org/abs/2501.09751</link>
<guid>https://arxiv.org/abs/2501.09751</guid>
<content:encoded><![CDATA[
arXiv:2501.09751v5 Announce Type: replace-cross 
Abstract: Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, novelty, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, unoriginal, and repetitive outputs. To address these issues, we propose OmniThink, a slow-thinking machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they slowly deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles. Code is available at https://github.com/zjunlp/OmniThink.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting Emergent Features in Deep Learning-based Side-channel Analysis</title>
<link>https://arxiv.org/abs/2502.00384</link>
<guid>https://arxiv.org/abs/2502.00384</guid>
<content:encoded><![CDATA[
arXiv:2502.00384v3 Announce Type: replace-cross 
Abstract: Side-channel analysis (SCA) poses a real-world threat by exploiting unintentional physical signals to extract secret information from secure devices. Evaluation labs also use the same techniques to certify device security. In recent years, deep learning has emerged as a prominent method for SCA, achieving state-of-the-art attack performance at the cost of interpretability. Understanding how neural networks extract secrets is crucial for security evaluators aiming to defend against such attacks, as only by understanding the attack can one propose better countermeasures.
  In this work, we apply mechanistic interpretability to neural networks trained for SCA, revealing \textit{how} models exploit \textit{what} leakage in side-channel traces. We focus on sudden jumps in performance to reverse engineer learned representations, ultimately recovering secret masks and moving the evaluation process from black-box to white-box. Our results show that mechanistic interpretability can scale to realistic SCA settings, even when relevant inputs are sparse, model accuracies are low, and side-channel protections prevent standard input interventions.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation</title>
<link>https://arxiv.org/abs/2502.02054</link>
<guid>https://arxiv.org/abs/2502.02054</guid>
<content:encoded><![CDATA[
arXiv:2502.02054v2 Announce Type: replace-cross 
Abstract: This paper introduces a learning-based visual planner for agile drone flight in cluttered environments. The proposed planner generates collision-free waypoints in milliseconds, enabling drones to perform agile maneuvers in complex environments without building separate perception, mapping, and planning modules. Learning-based methods, such as behavior cloning (BC) and reinforcement learning (RL), demonstrate promising performance in visual navigation but still face inherent limitations. BC is susceptible to compounding errors due to limited expert imitation, while RL struggles with reward function design and sample inefficiency. To address these limitations, this paper proposes an inverse reinforcement learning (IRL)-based framework for high-speed visual navigation. By leveraging IRL, it is possible to reduce the number of interactions with simulation environments and improve capability to deal with high-dimensional spaces while preserving the robustness of RL policies. A motion primitive-based path planning algorithm collects an expert dataset with privileged map data from diverse environments, ensuring comprehensive scenario coverage. By leveraging both the acquired expert and learner dataset gathered from the agent's interactions with the simulation environments, a robust reward function and policy are learned across diverse states. While the proposed method is trained in a simulation environment only, it can be directly applied to real-world scenarios without additional training or tuning. The performance of the proposed method is validated in both simulation and real-world environments, including forests and various structures. The trained policy achieves an average speed of 7 m/s and a maximum speed of 8.8 m/s in real flight experiments. To the best of our knowledge, this is the first work to successfully apply an IRL framework for high-speed visual navigation of drones.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Gap in XAI-Why Reliable Metrics Matter for Explainability and Compliance</title>
<link>https://arxiv.org/abs/2502.04695</link>
<guid>https://arxiv.org/abs/2502.04695</guid>
<content:encoded><![CDATA[
arXiv:2502.04695v2 Announce Type: replace-cross 
Abstract: Reliable explainability is not only a technical goal but also a cornerstone of private AI governance. As AI models enter high-stakes sectors, private actors such as auditors, insurers, certification bodies, and procurement agencies require standardized evaluation metrics to assess trustworthiness. However, current XAI evaluation metrics remain fragmented and prone to manipulation, which undermines accountability and compliance. We argue that standardized metrics can function as governance primitives, embedding auditability and accountability within AI systems for effective private oversight. Building upon prior work in XAI benchmarking, we identify key limitations in ensuring faithfulness, tamper resistance, and regulatory alignment. Furthermore, interpretability can directly support model alignment by providing a verifiable means of ensuring behavioral integrity in General Purpose AI (GPAI) systems. This connection between interpretability and alignment positions XAI metrics as both technical and regulatory instruments that help prevent alignment faking, a growing concern among oversight bodies. We propose a Governance by Metrics paradigm that treats explainability evaluation as a central mechanism of private AI governance. Our framework introduces a hierarchical model linking transparency, tamper resistance, scalability, and legal alignment, extending evaluation from model introspection toward systemic accountability. Through conceptual synthesis and alignment with governance standards, we outline a roadmap for integrating explainability metrics into continuous AI assurance pipelines that serve both private oversight and regulatory needs.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRADES: Generating Realistic Market Simulations with Diffusion Models</title>
<link>https://arxiv.org/abs/2502.07071</link>
<guid>https://arxiv.org/abs/2502.07071</guid>
<content:encoded><![CDATA[
arXiv:2502.07071v3 Announce Type: replace-cross 
Abstract: Financial markets are complex systems characterized by high statistical noise, nonlinearity, volatility, and constant evolution. Thus, modeling them is extremely hard. Here, we address the task of generating realistic and responsive Limit Order Book (LOB) market simulations, which are fundamental for calibrating and testing trading strategies, performing market impact experiments, and generating synthetic market data. We propose a novel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB Simulations (TRADES). TRADES generates realistic order flows as time series conditioned on the state of the market, leveraging a transformer-based architecture that captures the temporal and spatial characteristics of high-frequency market data. There is a notable absence of quantitative metrics for evaluating generative market simulation models in the literature. To tackle this problem, we adapt the predictive score, a metric measured as an MAE, to market data by training a stock price predictive model on synthetic data and testing it on real data. We compare TRADES with previous works on two stocks, reporting a 3.27 and 3.48 improvement over SoTA according to the predictive score, demonstrating that we generate useful synthetic market data for financial downstream tasks. Furthermore, we assess TRADES's market simulation realism and responsiveness, showing that it effectively learns the conditional data distribution and successfully reacts to an experimental agent, giving sprout to possible calibrations and evaluations of trading strategies and market impact experiments. To perform the experiments, we developed DeepMarket, the first open-source Python framework for LOB market simulation with deep learning. In our repository, we include a synthetic LOB dataset composed of TRADES's generated simulations.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient and Accurate Spatial Mixing of Machine Learned Interatomic Potentials for Materials Science</title>
<link>https://arxiv.org/abs/2502.19081</link>
<guid>https://arxiv.org/abs/2502.19081</guid>
<content:encoded><![CDATA[
arXiv:2502.19081v2 Announce Type: replace-cross 
Abstract: Machine-learned interatomic potentials can offer near first-principles accuracy but are computationally expensive, limiting their application to large-scale molecular dynamics simulations. Inspired by quantum mechanics/molecular mechanics methods we present ML-MIX, a CPU- and GPU-compatible LAMMPS package to accelerate simulations by spatially mixing interatomic potentials of different complexities allowing deployment of modern MLIPs even under restricted computational budgets. We demonstrate our method for ACE, UF3, SNAP and MACE potential architectures and demonstrate how linear 'cheap' potentials can be distilled from a given 'expensive' potential, allowing close matching in relevant regions of configuration space. The functionality of ML-MIX is demonstrated through tests on point defects in Si, Fe and W-He, in which speedups of up to 11x over ~ 8,000 atoms are demonstrated, without sacrificing accuracy. The scientific potential of ML-MIX is demonstrated via two case studies in W, measuring the mobility of b = 1/2 111 screw dislocations with ACE/ACE mixing and the implantation of He with MACE/SNAP mixing. The latter returns He reflection coefficients which (for the first time) match experimental observations up to an He incident energy of 80 eV - demonstrating the benefits of deploying state-of-the-art models on large, realistic systems.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMInit: A Free Lunch from Large Language Models for Selective Initialization of Recommendation</title>
<link>https://arxiv.org/abs/2503.01814</link>
<guid>https://arxiv.org/abs/2503.01814</guid>
<content:encoded><![CDATA[
arXiv:2503.01814v2 Announce Type: replace-cross 
Abstract: Collaborative filtering (CF) is widely adopted in industrial recommender systems (RecSys) for modeling user-item interactions across numerous applications, but often struggles with cold-start and data-sparse scenarios. Recent advancements in pre-trained large language models (LLMs) with rich semantic knowledge, offer promising solutions to these challenges. However, deploying LLMs at scale is hindered by their significant computational demands and latency. In this paper, we propose a novel and scalable LLM-RecSys framework, LLMInit, designed to integrate pretrained LLM embeddings into CF models through selective initialization strategies. Specifically, we identify the embedding collapse issue observed when CF models scale and match the large embedding sizes in LLMs and avoid the problem by introducing efficient sampling methods, including, random, uniform, and variance-based selections. Comprehensive experiments conducted on multiple real-world datasets demonstrate that LLMInit significantly improves recommendation performance while maintaining low computational costs, offering a practical and scalable solution for industrial applications. To facilitate industry adoption and promote future research, we provide open-source access to our implementation at https://github.com/DavidZWZ/LLMInit.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners</title>
<link>https://arxiv.org/abs/2503.16356</link>
<guid>https://arxiv.org/abs/2503.16356</guid>
<content:encoded><![CDATA[
arXiv:2503.16356v3 Announce Type: replace-cross 
Abstract: Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they often fail to generalize these updates to multi-hop reasoning tasks that rely on the modified knowledge. Through an analysis of reasoning circuits -- the neural pathways LLMs use for knowledge-based inference, we find that current layer-localized KE approaches (e.g., MEMIT, WISE), which edit only single or a few model layers, inadequately integrate updated knowledge into these reasoning pathways. To address this limitation, we present CaKE (Circuit-aware Knowledge Editing), a novel method that enhances the effective integration of updated knowledge in LLMs. By only leveraging a few curated data samples guided by our circuit-based analysis, CaKE stimulates the model to develop appropriate reasoning circuits for newly incorporated knowledge. Experiments show that CaKE enables more accurate and consistent use of edited knowledge across related reasoning tasks, achieving an average improvement of 20% in multi-hop reasoning accuracy on the MQuAKE dataset while requiring less memory than existing KE methods. We release the code and data in https://github.com/zjunlp/CaKE.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vector Quantized-Elites: Unsupervised and Problem-Agnostic Quality-Diversity Optimization</title>
<link>https://arxiv.org/abs/2504.08057</link>
<guid>https://arxiv.org/abs/2504.08057</guid>
<content:encoded><![CDATA[
arXiv:2504.08057v3 Announce Type: replace-cross 
Abstract: Quality-Diversity algorithms have transformed optimization by prioritizing the discovery of diverse, high-performing solutions over a single optimal result. However, traditional Quality-Diversity methods, such as MAP-Elites, rely heavily on predefined behavior descriptors and complete prior knowledge of the task to define the behavior space grid, limiting their flexibility and applicability. In this work, we introduce Vector Quantized-Elites (VQ-Elites), a novel Quality-Diversity algorithm that autonomously constructs a structured behavior space grid using unsupervised learning, eliminating the need for prior task-specific knowledge. At the core of VQ-Elites is the integration of Vector Quantized Variational Autoencoders, which enables the dynamic learning of behavior descriptors and the generation of a structured, rather than unstructured, behavior space grid -- a significant advancement over existing unsupervised Quality-Diversity approaches. This design establishes VQ-Elites as a flexible, robust, and task-agnostic optimization framework. To further enhance the performance of unsupervised Quality-Diversity algorithms, we introduce behavior space bounding and cooperation mechanisms, which significantly improve convergence and performance, as well as the Effective Diversity Ratio and Coverage Diversity Score, two novel metrics that quantify the actual diversity in the unsupervised setting. We validate VQ-Elites on robotic arm pose-reaching, mobile robot space-covering, and MiniGrid exploration tasks. The results demonstrate its ability to efficiently generate diverse, high-quality solutions, emphasizing its adaptability, scalability, robustness to hyperparameters, and potential to extend Quality-Diversity optimization to complex, previously inaccessible domains.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoJudge: Judge Decoding Without Manual Annotation</title>
<link>https://arxiv.org/abs/2504.20039</link>
<guid>https://arxiv.org/abs/2504.20039</guid>
<content:encoded><![CDATA[
arXiv:2504.20039v4 Announce Type: replace-cross 
Abstract: We introduce AutoJudge, a method that accelerates large language model (LLM) inference with task-specific lossy speculative decoding. Instead of matching the original model output distribution token-by-token, we identify which of the generated tokens affect the downstream quality of the response, relaxing the distribution match guarantee so that the "unimportant" tokens can be generated faster. Our approach relies on a semi-greedy search algorithm to test which of the mismatches between target and draft models should be corrected to preserve quality and which ones may be skipped. We then train a lightweight classifier based on existing LLM embeddings to predict, at inference time, which mismatching tokens can be safely accepted without compromising the final answer quality. We evaluate the effectiveness of AutoJudge with multiple draft/target model pairs on mathematical reasoning and programming benchmarks, achieving significant speedups at the cost of a minor accuracy reduction. Notably, on GSM8k with the Llama 3.1 70B target model, our approach achieves up to $\approx2\times$ speedup over speculative decoding at the cost of $\le 1\%$ drop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge automatically detects programming-specific important tokens, accepting $\ge 25$ tokens per speculation cycle at $2\%$ drop in Pass@1. Our approach requires no human annotation and is easy to integrate with modern LLM inference frameworks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models</title>
<link>https://arxiv.org/abs/2505.01406</link>
<guid>https://arxiv.org/abs/2505.01406</guid>
<content:encoded><![CDATA[
arXiv:2505.01406v2 Announce Type: replace-cross 
Abstract: Video diffusion models can generate realistic and temporally consistent videos. This raises concerns about provenance, ownership, and integrity. Watermarking can help address these issues by embedding metadata directly into the content. To work well, a watermark needs enough capacity for meaningful metadata. It must also stay imperceptible and remain robust to common video manipulations. Existing methods struggle with limited capacity, extra inference cost, or reduced visual quality. We introduce VidStamp, a watermarking framework that embeds frame-level messages through the decoder of a latent video diffusion model. The decoder is fine-tuned in two stages. The first stage uses static image datasets to encourage spatial message separation. The second stage uses synthesized video sequences to restore temporal consistency. This approach enables high-capacity watermarks with minimal perceptual impact. VidStamp also supports dynamic watermarking through a control signal that selects message templates during inference. This adds flexibility and creates a second channel for communication. We evaluate VidStamp on Stable Video Diffusion (I2V), OpenSora, and Wan (T2V). The system embeds 48 bits per frame while preserving visual quality and staying robust to common distortions. Compared with VideoSeal, VideoShield, and RivaGAN, it achieves lower log P-values and stronger detectability. Its frame-wise watermarking design also enables precise temporal tamper localization, with an accuracy of 0.96, which exceeds the VideoShield baseline. Code: https://github.com/SPIN-UMass/VidStamp
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Visual Feature Attribution via Weighted Integrated Gradients</title>
<link>https://arxiv.org/abs/2505.03201</link>
<guid>https://arxiv.org/abs/2505.03201</guid>
<content:encoded><![CDATA[
arXiv:2505.03201v3 Announce Type: replace-cross 
Abstract: Integrated Gradients (IG) is a widely used attribution method in explainable AI, particularly in computer vision applications where reliable feature attribution is essential. A key limitation of IG is its sensitivity to the choice of baseline (reference) images. Multi-baseline extensions such as Expected Gradients (EG) assume uniform weighting over baselines, implicitly treating baseline images as equally informative. In high-dimensional vision models, this assumption often leads to noisy or unstable explanations. This paper proposes Weighted Integrated Gradients (WG), a principled approach that evaluates and weights baselines to enhance attribution reliability. WG introduces an unsupervised criterion for baseline suitability, enabling adaptive selection and weighting of baselines on a per-input basis. The method not only preserves core axiomatic properties of IG but also provides improved theoretical guarantees on the quality of explanation over EG. Experiments on commonly used image datasets and models show that WG consistently outperforms EG, yielding 10 to 35 percent improvements in attribution fidelity. WG further identifies informative baseline subsets, reducing unnecessary variability while maintaining high attribution accuracy. By moving beyond the idea that all baselines matter equally, Weighted Integrated Gradients offers a clearer and more reliable way to explain computer-vision models, improving both understanding and practical usability in explainable AI.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoE-World: Compositional World Modeling with Products of Programmatic Experts</title>
<link>https://arxiv.org/abs/2505.10819</link>
<guid>https://arxiv.org/abs/2505.10819</guid>
<content:encoded><![CDATA[
arXiv:2505.10819v4 Announce Type: replace-cross 
Abstract: Learning how the world works is central to building AI agents that can adapt to complex environments. Traditional world models based on deep learning demand vast amounts of training data, and do not flexibly update their knowledge from sparse observations. Recent advances in program synthesis using Large Language Models (LLMs) give an alternate approach which learns world models represented as source code, supporting strong generalization from little data. To date, application of program-structured world models remains limited to natural language and grid-world domains. We introduce a novel program synthesis method for effectively modeling complex, non-gridworld domains by representing a world model as an exponentially-weighted product of programmatic experts (PoE-World) synthesized by LLMs. We show that this approach can learn complex, stochastic world models from just a few observations. We evaluate the learned world models by embedding them in a model-based planning agent, demonstrating efficient performance and generalization to unseen levels on Atari's Pong and Montezuma's Revenge. We release our code and display the learned world models and videos of the agent's gameplay at https://topwasu.github.io/poe-world.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular Jump Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.15557</link>
<guid>https://arxiv.org/abs/2505.15557</guid>
<content:encoded><![CDATA[
arXiv:2505.15557v3 Announce Type: replace-cross 
Abstract: Gaussian processes (GPs) furnish accurate nonlinear predictions with well-calibrated uncertainty. However, the typical GP setup has a built-in stationarity assumption, making it ill-suited for modeling data from processes with sudden changes, or "jumps" in the output variable. The "jump GP" (JGP) was developed for modeling data from such processes, combining local GPs and latent "level" variables under a joint inferential framework. But joint modeling can be fraught with difficulty. We aim to simplify by suggesting a more modular setup, eschewing joint inference but retaining the main JGP themes: (a) learning optimal neighborhood sizes that locally respect manifolds of discontinuity; and (b) a new cluster-based (latent) feature to capture regions of distinct output levels on both sides of the manifold. We show that each of (a) and (b) separately leads to dramatic improvements when modeling processes with jumps. In tandem (but without requiring joint inference) that benefit is compounded, as illustrated on real and synthetic benchmark examples from the recent literature.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Distributionally Robust Framework for Nuisance in Causal Effect Estimation</title>
<link>https://arxiv.org/abs/2505.17717</link>
<guid>https://arxiv.org/abs/2505.17717</guid>
<content:encoded><![CDATA[
arXiv:2505.17717v2 Announce Type: replace-cross 
Abstract: Causal inference requires evaluating models on balanced distributions between treatment and control groups, while training data often exhibits imbalance due to historical decision-making policies. Most conventional statistical methods address this distribution shift through inverse probability weighting (IPW), which requires estimating propensity scores as an intermediate step. These methods face two key challenges: inaccurate propensity estimation and instability from extreme weights. We decompose the generalization error to isolate these issues--propensity ambiguity and statistical instability--and address them through an adversarial loss function. Our approach combines distributionally robust optimization for handling propensity uncertainty with weight regularization based on weighted Rademacher complexity. Experiments on synthetic and real-world datasets demonstrate consistent improvements over existing methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title>
<link>https://arxiv.org/abs/2506.06941</link>
<guid>https://arxiv.org/abs/2506.06941</guid>
<content:encoded><![CDATA[
arXiv:2506.06941v3 Announce Type: replace-cross 
Abstract: Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks</title>
<link>https://arxiv.org/abs/2506.07392</link>
<guid>https://arxiv.org/abs/2506.07392</guid>
<content:encoded><![CDATA[
arXiv:2506.07392v4 Announce Type: replace-cross 
Abstract: The proliferation of UAVs has enabled a wide range of mission-critical applications and is becoming a cornerstone of low-altitude networks, supporting smart cities, emergency response, and more. However, the open wireless environment, dynamic topology, and resource constraints of UAVs expose low-altitude networks to severe DoS threats. Traditional defense approaches, which rely on fixed configurations or centralized decision-making, cannot effectively respond to the rapidly changing conditions in UAV swarm environments. To address these challenges, we propose a novel federated multi-agent deep reinforcement learning (FMADRL)-driven moving target defense (MTD) framework for proactive DoS mitigation in low-altitude networks. Specifically, we design lightweight and coordinated MTD mechanisms, including leader switching, route mutation, and frequency hopping, to disrupt attacker efforts and enhance network resilience. The defense problem is formulated as a multi-agent partially observable Markov decision process, capturing the uncertain nature of UAV swarms under attack. Each UAV is equipped with a policy agent that autonomously selects MTD actions based on partial observations and local experiences. By employing a policy gradient-based algorithm, UAVs collaboratively optimize their policies via reward-weighted aggregation. Extensive simulations demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving up to a 34.6% improvement in attack mitigation rate, a reduction in average recovery time of up to 94.6%, and decreases in energy consumption and defense cost by as much as 29.3% and 98.3%, respectively, under various DoS attack strategies. These results highlight the potential of intelligent, distributed defense mechanisms to protect low-altitude networks, paving the way for reliable and scalable low-altitude economy.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constraint-Guided Prediction Refinement via Deterministic Diffusion Trajectories</title>
<link>https://arxiv.org/abs/2506.12911</link>
<guid>https://arxiv.org/abs/2506.12911</guid>
<content:encoded><![CDATA[
arXiv:2506.12911v2 Announce Type: replace-cross 
Abstract: Many real-world machine learning tasks require outputs that satisfy hard constraints, such as physical conservation laws, structured dependencies in graphs, or column-level relationships in tabular data. Existing approaches rely either on domain-specific architectures and losses or on strong assumptions on the constraint space, restricting their applicability to linear or convex constraints. We propose a general-purpose framework for constraint-aware refinement that leverages denoising diffusion implicit models (DDIMs). Starting from a coarse prediction, our method iteratively refines it through a deterministic diffusion trajectory guided by a learned prior and augmented by constraint gradient corrections. The approach accommodates a wide class of non-convex and nonlinear equality constraints and can be applied post hoc to any base model. We demonstrate the method in two representative domains: constrained adversarial attack generation on tabular data with column-level dependencies and in AC power flow prediction under Kirchhoff's laws. Across both settings, our diffusion-guided refinement improves both constraint satisfaction and performance while remaining lightweight and model-agnostic.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Measurement: Efficient Estimation at Scale</title>
<link>https://arxiv.org/abs/2507.01372</link>
<guid>https://arxiv.org/abs/2507.01372</guid>
<content:encoded><![CDATA[
arXiv:2507.01372v2 Announce Type: replace-cross 
Abstract: AI has the potential to transform scientific discovery by analyzing vast datasets with little human effort. However, current workflows often do not provide the accuracy or statistical guarantees that are needed. We introduce active measurement, a human-in-the-loop AI framework for scientific measurement. An AI model is used to predict measurements for individual units, which are then sampled for human labeling using importance sampling. With each new set of human labels, the AI model is improved and an unbiased Monte Carlo estimate of the total measurement is refined. Active measurement can provide precise estimates even with an imperfect AI model, and requires little human effort when the AI model is very accurate. We derive novel estimators, weighting schemes, and confidence intervals, and show that active measurement reduces estimation error compared to alternatives in several measurement tasks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Trust or Not to Trust: On Calibration in ML-based Resource Allocation for Wireless Networks</title>
<link>https://arxiv.org/abs/2507.17494</link>
<guid>https://arxiv.org/abs/2507.17494</guid>
<content:encoded><![CDATA[
arXiv:2507.17494v3 Announce Type: replace-cross 
Abstract: In next-generation communications and networks, machine learning (ML) models are expected to deliver not only accurate predictions but also well-calibrated confidence scores that reflect the true likelihood of correct decisions. This paper studies the calibration performance of an ML-based outage predictor within a single-user, multi-resource allocation framework. We first establish key theoretical properties of this system's outage probability (OP) under perfect calibration. Importantly, we show that as the number of resources grows, the OP of a perfectly calibrated predictor approaches the expected output conditioned on it being below the classification threshold. In contrast, when only one resource is available, the system's OP equals the model's overall expected output. We then derive the OP conditions for a perfectly calibrated predictor. These findings guide the choice of the classification threshold to achieve a desired OP, helping system designers meet specific reliability requirements. We also demonstrate that post-processing calibration cannot improve the system's minimum achievable OP, as it does not introduce new information about future channel states. Additionally, we show that well-calibrated models are part of a broader class of predictors that necessarily improve OP. In particular, we establish a monotonicity condition that the accuracy-confidence function must satisfy for such improvement to occur. To demonstrate these theoretical properties, we conduct a rigorous simulation-based analysis using post-processing calibration techniques: Platt scaling and isotonic regression. As part of this framework, the predictor is trained using an outage loss function specifically designed for this system. Furthermore, this analysis is performed on Rayleigh fading channels with temporal correlation captured by Clarke's 2D model, which accounts for receiver mobility.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-optimization of a deep neural network model for electron-carbon scattering using new experimental data</title>
<link>https://arxiv.org/abs/2508.00996</link>
<guid>https://arxiv.org/abs/2508.00996</guid>
<content:encoded><![CDATA[
arXiv:2508.00996v2 Announce Type: replace-cross 
Abstract: We present an updated deep neural network model for inclusive electron-carbon scattering. Using the bootstrap model [Phys.Rev.C 110 (2024) 2, 025501] as a prior, we incorporate recent experimental data, as well as older measurements in the deep inelastic scattering region, to derive a re-optimized posterior model. We examine the impact of these new inputs on model predictions and associated uncertainties. Finally, we evaluate the resulting cross-section predictions in the kinematic range relevant to the Hyper-Kamiokande and DUNE experiments.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Asymptotic Analysis of Data Augmentation for Precision Matrix Estimation</title>
<link>https://arxiv.org/abs/2510.02119</link>
<guid>https://arxiv.org/abs/2510.02119</guid>
<content:encoded><![CDATA[
arXiv:2510.02119v2 Announce Type: replace-cross 
Abstract: This paper addresses the problem of inverse covariance (also known as precision matrix) estimation in high-dimensional settings. Specifically, we focus on two classes of estimators: linear shrinkage estimators with a target proportional to the identity matrix, and estimators derived from data augmentation (DA). Here, DA refers to the common practice of enriching a dataset with artificial samples--typically generated via a generative model or through random transformations of the original data--prior to model fitting. For both classes of estimators, we derive estimators and provide concentration bounds for their quadratic error. This allows for both method comparison and hyperparameter tuning, such as selecting the optimal proportion of artificial samples. On the technical side, our analysis relies on tools from random matrix theory. We introduce a novel deterministic equivalent for generalized resolvent matrices, accommodating dependent samples with specific structure. We support our theoretical results with numerical experiments.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Formal Models and Convergence Analysis for Context-Aware Security Verification</title>
<link>https://arxiv.org/abs/2510.12440</link>
<guid>https://arxiv.org/abs/2510.12440</guid>
<content:encoded><![CDATA[
arXiv:2510.12440v2 Announce Type: replace-cross 
Abstract: Traditional security scanners fail when facing new attack patterns they haven't seen before. They rely on fixed rules and predetermined signatures, making them blind to novel threats. We present a fundamentally different approach: instead of memorizing specific attack patterns, we learn what makes systems genuinely secure. Our key insight is simple yet powerful: context determines vulnerability. A SQL query that's safe in one environment becomes dangerous in another. By modeling this context-vulnerability relationship, we achieve something remarkable: our system detects attacks it has never seen before. We introduce context-aware verification that learns from genuine system behavior. Through reconstruction learning on secure systems, we capture their essential characteristics. When an unknown attack deviates from these patterns, our system recognizes it, even without prior knowledge of that specific attack type. We prove this capability theoretically, showing detection rates improve exponentially with context information I(W;C). Our framework combines three components: (1) reconstruction learning that models secure behavior, (2) multi-scale graph reasoning that aggregates contextual clues, and (3) attention mechanisms guided by reconstruction differences. Extensive experiments validate our approach: detection accuracy jumps from 58 percent to 82 percent with full context, unknown attack detection improves by 31 percent, and our system maintains above 90 percent accuracy even against completely novel attack vectors.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning the Inverse Ryu--Takayanagi Formula with Transformers</title>
<link>https://arxiv.org/abs/2511.06387</link>
<guid>https://arxiv.org/abs/2511.06387</guid>
<content:encoded><![CDATA[
arXiv:2511.06387v2 Announce Type: replace-cross 
Abstract: We study the inverse problem of holographic entanglement entropy in AdS$_3$ using a data-driven generative model. Training data consist of randomly generated geometries and their holographic entanglement entropies using the Ryu--Takayanagi formula. After training, the Transformer reconstructs the blackening function within our metric ansatz from previously unseen inputs. The Transformer achieves accurate reconstructions on smooth black hole geometries and extrapolates to horizonless backgrounds. We describe the architecture and data generation process, and we quantify accuracy on both $f(z)$ and the reconstructed $S(\ell)$. Code and evaluation scripts are available at the provided repository.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging</title>
<link>https://arxiv.org/abs/2511.07129</link>
<guid>https://arxiv.org/abs/2511.07129</guid>
<content:encoded><![CDATA[
arXiv:2511.07129v2 Announce Type: replace-cross 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language models. However, conventional LoRA adapters are typically trained for a single task, limiting their applicability in real-world settings where inputs may span diverse and unpredictable domains. At inference time, existing approaches combine multiple LoRAs for improving performance on diverse tasks, while usually requiring labeled data or additional task-specific training, which is expensive at scale. In this work, we introduce LoRA on the Go (LoGo), a training-free framework that dynamically selects and merges adapters at the instance level without any additional requirements. LoGo leverages signals extracted from a single forward pass through LoRA adapters, to identify the most relevant adapters and determine their contributions on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo outperforms training-based baselines on some tasks upto a margin of 3.6% while remaining competitive on other tasks and maintaining inference throughput, highlighting its effectiveness and practicality.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer Injectivity &amp; Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States</title>
<link>https://arxiv.org/abs/2511.14808</link>
<guid>https://arxiv.org/abs/2511.14808</guid>
<content:encoded><![CDATA[
<div> Injectivity, Transformers, collision discriminant, geometric diagnostics, quantization<br /><br />Summary:<br /><br />This paper investigates the injectivity properties of decoder-only Transformer models under real-analytic assumptions, focusing on the mapping from discrete prompts to last-token hidden states. The authors introduce a layerwise concept of a collision discriminant set and an injective stratum, proving a fundamental dichotomy: either the model is never injective on a finite prompt set or injectivity holds on an open dense subset of parameters. They further show that under mild assumptions about the optimizer and initialization, generic injectivity endures throughout smooth training trajectories over fixed horizons. The study also accounts for symmetry groups acting on parameter space, establishing that injectivity properties descend naturally to quotient spaces reflecting functional equivalence classes. Complementing these theoretical results, the paper develops empirical geometric diagnostics including a separation margin and a co-Lipschitz constant to measure how last-token representations reflect differences in prompt space. These diagnostics are applied to pretrained LLaMA-3 and Qwen models across layers, sequence lengths, model sizes, and activation quantization levels (8-bit and 4-bit). The experiments find no collisions at full precision or 8-bit, while 4-bit quantization induces few collisions and reduces co-Lipschitz constants. For a small GPT-2 model trained from scratch, normalized metrics remain stable throughout training. Overall, the results demonstrate that Transformer representations are generically and persistently injective in continuous parameter spaces, with practical invertibility assessable via straightforward geometric measures. <div>
arXiv:2511.14808v1 Announce Type: new 
Abstract: Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $\Delta^\ell \subset \Theta$ and injective stratum $U^\ell = \Theta \setminus \Delta^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_\theta$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $\Theta/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models</title>
<link>https://arxiv.org/abs/2511.14813</link>
<guid>https://arxiv.org/abs/2511.14813</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.14813v1  
Keywords: Derivation Relation, Derivation Capability, Large Language Models, Evaluation Framework, Derivation Prompting  

<br /><br />Summary:  
This paper addresses the challenge of evaluating reasoning abilities of Large Language Models (LLMs) with a focus on their capacity to apply abstract rules governing how changes in input data should correspond to modifications in outputs. The authors formalize this reasoning pattern as the Derivation Relation (DR) and define Derivation Capability (DC) as the ability of LLMs to apply DR by adjusting outputs when input data changes. To measure DC, they introduce DEVAL, a systematic evaluation framework applied to five popular LLMs and one Large Reasoning Model across seven mainstream tasks. Results reveal that although models like GPT-4o and Claude3.5 demonstrate moderate abilities to recognize DR, their performance significantly declines when it comes to effectively applying DR during problem-solving. To enhance DC, the paper proposes a novel prompt engineering technique named Derivation Prompting (DP), which improves performance by an average of 15.2% across all tested models. DP outperforms existing prompt engineering approaches, offering a promising direction for enhancing LLM reasoning with respect to input-output derivation relations. <div>
arXiv:2511.14813v1 Announce Type: new 
Abstract: Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence</title>
<link>https://arxiv.org/abs/2511.14823</link>
<guid>https://arxiv.org/abs/2511.14823</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic nested hierarchies, lifelong learning, multi-level optimization, neuroplasticity, non-stationary environments<br /><br />Summary: Contemporary machine learning models, including large language models, struggle with adapting to non-stationary environments due to their rigid and fixed architectures, which impede continual adaptation and lifelong learning. This paper advances the nested learning paradigm by proposing dynamic nested hierarchies, a framework that enables models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during both training and inference. Inspired by neuroplasticity, this approach allows models to self-evolve without predefined constraints, effectively overcoming the problem of anterograde amnesia seen in existing models. The dynamic nested hierarchies compress context flows dynamically and adapt to distributional shifts, addressing key challenges in lifelong learning settings. The work is supported by rigorous mathematical formulations, including theoretical proofs of convergence, expressivity limits, and sublinear regret guarantees across different regimes. Empirically, the proposed framework demonstrates superior performance in tasks such as language modeling, continual learning, and long-context reasoning. Overall, dynamic nested hierarchies represent a fundamental step toward building adaptive, general-purpose artificial intelligence systems capable of true lifelong learning and continual adaptation in changing environments. <div>
arXiv:2511.14823v1 Announce Type: new 
Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization</title>
<link>https://arxiv.org/abs/2511.14846</link>
<guid>https://arxiv.org/abs/2511.14846</guid>
<content:encoded><![CDATA[
<div> Tool-Integrated Reasoning, Large Language Models, Reinforcement Learning, Group Turn Policy Optimization, Multi-turn Interaction<br /><br />Summary:<br /><br />This paper addresses the challenges in training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR), where models iteratively perform reasoning, code generation, and execution verification. Existing reinforcement learning (RL) approaches, such as Group Relative Policy Optimization (GRPO), rely on coarse-grained trajectory-level rewards that are ineffective for complex multi-turn tasks, often resulting in stalled training progress. To overcome these limitations, the authors propose Group Turn Policy Optimization (GTPO), a novel RL algorithm tailored for multi-turn TIR tasks with three main innovations. First, GTPO assigns rewards at the turn-level, providing fine-grained feedback for each reasoning step rather than relying on sparse, overall trajectory rewards. Second, it utilizes return-based advantage estimation by calculating normalized discounted returns as advantages to better guide learning. Third, it incorporates self-supervised reward shaping, leveraging self-supervision signals extracted from the generated code to densify traditionally sparse binary outcome rewards. Their comprehensive experiments demonstrate that GTPO achieves a 3.0% average improvement over GRPO on diverse reasoning benchmarks, highlighting its effectiveness in enhancing the training of LLMs for complex mathematical reasoning tasks encountered in real-world applications. <div>
arXiv:2511.14846v1 Announce Type: new 
Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications</title>
<link>https://arxiv.org/abs/2511.14865</link>
<guid>https://arxiv.org/abs/2511.14865</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer, Financial Services, sequential recommendation, FinTRec, tree-based models  

<br /><br />Summary: This article discusses the application of transformer-based architectures in sequential recommendation systems, specifically within Financial Services (FS), highlighting the unique challenges faced in real-time recommendations. Key challenges include managing long-range user interactions across digital and physical channels and the need for coordinated models to handle various interrelated products while balancing differing business objectives. The authors introduce FinTRec, a novel transformer-based framework designed to tackle these challenges and achieve operational goals in FS. While tree-based models have been traditionally favored due to their explainability and regulatory compliance, the study demonstrates that FinTRec offers a robust alternative. The framework consistently outperforms existing tree-based baselines through historical simulations and live A/B testing. Additionally, FinTRec's unified architecture allows for cross-product signal sharing, which reduces both training costs and technical debt, while simultaneously enhancing offline performance for all products. This work is recognized as the first comprehensive study to explore unified sequential recommendation modeling in FS, addressing both technical and business aspects effectively. <div>
arXiv:2511.14865v1 Announce Type: new 
Abstract: Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone</title>
<link>https://arxiv.org/abs/2511.14887</link>
<guid>https://arxiv.org/abs/2511.14887</guid>
<content:encoded><![CDATA[
<div> Keywords: eVTOL, optimal takeoff trajectory, deep reinforcement learning, transformer, energy consumption  

<br /><br />Summary:  
The article addresses the challenge of designing optimal takeoff trajectories for electric vertical take-off and landing (eVTOL) aircraft to minimize energy consumption, an important factor to facilitate broader urban air mobility. Traditional optimal control methods like dynamic programming and linear quadratic regulators are effective but face limitations related to problem complexity and dimensionality. The study proposes a novel transformer-guided deep reinforcement learning (DRL) approach to overcome these issues by using a transformer model to explore realistic state spaces at each time step, thereby reducing training difficulty. The method was applied to an eVTOL drone scenario, optimizing control variables such as power and wing angle during takeoff. Results showed that the transformer-guided DRL agent required 4.57 million time steps for training, which is only 25% of the 19.79 million steps needed by a conventional (vanilla) DRL agent. Additionally, the transformer-guided model achieved 97.2% accuracy in matching the optimal energy consumption benchmark derived from simulation, slightly outperforming the vanilla DRL‚Äôs 96.3% accuracy. Overall, the proposed approach not only improves training efficiency substantially but also yields more reliable optimal trajectory designs for eVTOL takeoff, demonstrating its potential for practical application in urban air mobility systems. <div>
arXiv:2511.14887v1 Announce Type: new 
Abstract: The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bringing Federated Learning to Space</title>
<link>https://arxiv.org/abs/2511.14889</link>
<guid>https://arxiv.org/abs/2511.14889</guid>
<content:encoded><![CDATA[
<div> Keywords: Low Earth Orbit, federated learning, satellite constellations, orbital constraints, distributed on-board learning  

<br /><br />Summary:  
This paper addresses the challenge of implementing federated learning (FL) in rapidly growing Low Earth Orbit (LEO) satellite constellations to overcome downlink bandwidth limitations. It highlights the need for distributed on-board machine learning that can operate under the space-specific constraints like intermittent connectivity and orbital dynamics. The authors introduce a novel "space-ification" framework that adapts common terrestrial FL algorithms (FedAvg, FedProx, FedBuff) to be suitable for orbital environments, producing an orbital-ready suite of FL methods. They conduct an extensive feasibility analysis through parameter sweeps across 768 constellation configurations, varying cluster sizes, satellites per cluster, and ground station networks. The results show that these space-adapted FL algorithms scale efficiently up to 100 satellites and achieve performance near the centralized ideal. Furthermore, orbital scheduling and local coordination within clusters reduce multi-month training cycles to just days, yielding a 9x speedup. This work provides actionable insights for mission designers, facilitating distributed on-board learning that supports more autonomous, resilient, and data-driven satellite operations in future space missions. <div>
arXiv:2511.14889v1 Announce Type: new 
Abstract: As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive "space-ification" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It's LIT! Reliability-Optimized LLMs with Inspectable Tools</title>
<link>https://arxiv.org/abs/2511.14903</link>
<guid>https://arxiv.org/abs/2511.14903</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Tool-Calling, Reliability, Troubleshooting, Benchmark Dataset  

<br /><br />Summary:  
1. Large language models (LLMs) have shown impressive capabilities but their opaque reasoning can limit trustworthiness in critical domains.  
2. To improve reliability, the authors propose forcing LLMs to use external, more dependable tools when solving problems.  
3. They introduce a framework called LIT (LLMs with Inspectable Tools), which leverages tool-calling abilities in LLMs to select the most reliable and easy-to-troubleshoot solution paths, potentially involving multiple sequential tool calls.  
4. A new challenging benchmark dataset with 1,300 questions is developed, alongside customizable reliability cost functions that rate tools on reliability and ease of troubleshooting.  
5. Examples include a calculator (highly reliable), a linear prediction model (less reliable under distribution shifts but easy to troubleshoot), and a random forest constructor (neither reliable nor easy to troubleshoot).  
6. The tools interface with the Harvard USPTO Patent Dataset and a dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty.  
7. Experimental results show that LLMs using the LIT framework can achieve more reliable and interpretable problem-solving without sacrificing task performance. <div>
arXiv:2511.14903v1 Announce Type: new 
Abstract: Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Contrastive Learning for Interpretable Latent Representations</title>
<link>https://arxiv.org/abs/2511.14920</link>
<guid>https://arxiv.org/abs/2511.14920</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, contrastive learning, invariance, latent space, robustness  

<br /><br />Summary:  
This paper addresses the problem of neural networks‚Äô brittleness to semantically irrelevant transformations, such as minor ECG phase shifts or sensor rotations in IMU data, which drastically degrade model performance. The authors identify the cause as "laissez-faire" representation learning, where latent spaces change without constraints as long as task objectives are met, leading to unstable and uninterpretable features. To overcome this, they propose Structured Contrastive Learning (SCL), a novel framework that divides latent representations into three semantic groups: invariant features that remain stable under transformations, variant features that explicitly differentiate these transformations through a new variant mechanism, and free features that maintain model flexibility. This design creates a controlled push-pull dynamic in latent spaces, making different dimensions interpretable and purposeful. The variant mechanism enhances contrastive learning by encouraging differences within positive pairs rather than just across negative pairs, supporting both robustness and interpretability. Importantly, SCL integrates smoothly without requiring architectural changes or additional complexity in training pipelines. Experimental results demonstrate substantial improvements: ECG latent similarity under phase shifts rises from 0.25 to 0.91, and the WISDM activity recognition dataset shows enhanced accuracy and rotation consistency, outperforming traditional data augmentation methods. This work marks a shift towards proactive structural learning for interpretable and robust neural representations. <div>
arXiv:2511.14920v1 Announce Type: new 
Abstract: Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as "laissez-faire" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, enabling interpretable latent representations in neural networks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis</title>
<link>https://arxiv.org/abs/2511.14922</link>
<guid>https://arxiv.org/abs/2511.14922</guid>
<content:encoded><![CDATA[
<div> Keywords: Causal-GCN, Alzheimer's disease, MRI, structural connectome, do-calculus

<br /><br />Summary:  
This study introduces Causal-GCN, a novel graph convolutional network framework designed to enhance Alzheimer's disease (AD) classification from MRI data by addressing confounding factors. Traditional deep graph learning models often capture correlational relationships, mixing demographic and genetic influences with disease-specific brain features. Causal-GCN applies do-calculus-based back-door adjustments to isolate stable causal brain regions impacting AD progression. Each individual‚Äôs MRI is modeled as a structural connectome, with nodes representing cortical and subcortical brain regions and edges depicting anatomical connectivity. Important confounders, including age, sex, and APOE4 genotype, are reduced via principal component analysis and integrated into the causal model. After model training, the framework performs simulated interventions by severing incoming edges and modifying node features to estimate the average causal effect of specific brain regions on the probability of AD diagnosis. When tested on 484 subjects from the ADNI cohort, Causal-GCN achieves classification accuracy comparable to baseline graph neural networks while offering interpretable causal rankings of brain hubs. The identified key regions‚Äîposterior, cingulate, and insular cortices‚Äîalign with established neuropathological findings in AD, demonstrating both the method‚Äôs effectiveness and biological relevance. <div>
arXiv:2511.14922v1 Announce Type: new 
Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding</title>
<link>https://arxiv.org/abs/2511.14936</link>
<guid>https://arxiv.org/abs/2511.14936</guid>
<content:encoded><![CDATA[
<div> Keywords: differential privacy, clinical language models, knowledge distillation, automated diagnostic coding, privacy-utility trade-off<br /><br />Summary:  
1. Large language models trained on clinical text risk exposing sensitive patient information, creating a need for effective privacy-preserving methods.  
2. Differential privacy (DP) techniques, while promising, often degrade the diagnostic accuracy essential for clinical deployment.  
3. This study presents the first systematic head-to-head comparison of four distinct training pipelines for automated diagnostic coding from hospital discharge summaries, all leveraging identical 1-billion-parameter models under matched privacy budgets.  
4. The pipelines focus on predicting ICD-9 codes under moderate and relaxed privacy budgets (Œµ ‚àà {4, 6}) to evaluate privacy-utility trade-offs.  
5. Results show that knowledge distillation from DP-trained teacher models significantly outperforms direct DP-SGD and DP-synthetic data training methods, recovering up to 63% of the non-private model‚Äôs performance.  
6. Importantly, the knowledge distillation approach maintains strong empirical privacy, demonstrated by membership-inference attack AUC scores close to 0.5, indicating near-random guessing.  
7. The findings reveal substantial differences in privacy-utility trade-offs across different architectures and establish knowledge distillation as the most practical and effective strategy for privacy-preserving clinical natural language processing applications. <div>
arXiv:2511.14936v1 Announce Type: new 
Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference</title>
<link>https://arxiv.org/abs/2511.14961</link>
<guid>https://arxiv.org/abs/2511.14961</guid>
<content:encoded><![CDATA[
<div> Graph Memory, non-parametric learning, prototype nodes, label propagation, embedding space<br /><br />Summary:<br /><br />This paper introduces Graph Memory (GM), a novel framework designed to enhance embedding-based inference by integrating a structured, non-parametric memory composed of region-level prototypes. Unlike traditional methods that view each training instance separately, GM aggregates these instances into prototype nodes, each assigned reliability indicators and interconnected by edges representing geometric and contextual relationships. This approach effectively unifies multiple techniques including instance retrieval, prototype-based reasoning, and graph-based label propagation into a single inductive model. The GM framework supports both efficient inference processes and transparent explanations of its decisions. Experimental evaluations on both synthetic datasets and real-world data, notably breast histopathology (IDC), demonstrate that GM achieves accuracy on par with established methods such as k-Nearest Neighbors and Label Spreading. Moreover, it significantly improves model calibration and generates smoother decision boundaries. Notably, these improvements occur while using an order of magnitude fewer samples, highlighting GM‚Äôs sample efficiency. By explicitly modeling the reliability of prototypes and their relational structure, GM creates a principled link bridging local evidence from individual data points with global consistency across the dataset, thereby advancing non-parametric learning paradigms. <div>
arXiv:2511.14961v1 Announce Type: new 
Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IonCast: A Deep Learning Framework for Forecasting Ionospheric Dynamics</title>
<link>https://arxiv.org/abs/2511.15004</link>
<guid>https://arxiv.org/abs/2511.15004</guid>
<content:encoded><![CDATA[
<div> ionosphere, forecasting, deep learning, Total Electron Content, spatiotemporal learning<br /><br />Summary:<br /><br />1. The ionosphere plays a vital role in near-Earth space environments, significantly affecting GNSS accuracy, high-frequency communications, and aviation operations. 2. Accurate forecasting and modeling of ionospheric variability are essential to improve the reliability and resilience of these systems. 3. IonCast is introduced as a suite of deep learning models, inspired by GraphCast architecture, specifically designed to handle ionospheric dynamics. 4. The model utilizes spatiotemporal learning techniques to forecast global Total Electron Content (TEC), integrating a wide range of observational data and physical drivers relevant to ionospheric behavior. 5. Validation on datasets including both storm-time and quiet ionospheric conditions shows that IonCast outperforms persistence models in forecast skill. 6. By effectively unifying heterogeneous datasets through scalable graph-based spatiotemporal learning, IonCast demonstrates the potential of machine learning approaches to enhance physical understanding of ionospheric variability. 7. This work represents an advancement toward operational space weather forecasting, potentially aiding in better management of GNSS and communication systems affected by ionospheric changes. <div>
arXiv:2511.15004v1 Announce Type: new 
Abstract: The ionosphere is a critical component of near-Earth space, shaping GNSS accuracy, high-frequency communications, and aviation operations. For these reasons, accurate forecasting and modeling of ionospheric variability has become increasingly relevant. To address this gap, we present IonCast, a suite of deep learning models that include a GraphCast-inspired model tailored for ionospheric dynamics. IonCast leverages spatiotemporal learning to forecast global Total Electron Content (TEC), integrating diverse physical drivers and observational datasets. Validating on held-out storm-time and quiet conditions highlights improved skill compared to persistence. By unifying heterogeneous data with scalable graph-based spatiotemporal learning, IonCast demonstrates how machine learning can augment physical understanding of ionospheric variability and advance operational space weather resilience.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment</title>
<link>https://arxiv.org/abs/2511.15032</link>
<guid>https://arxiv.org/abs/2511.15032</guid>
<content:encoded><![CDATA[
<div> Keywords: intelligent tutoring systems, reinforcement learning, probing interventions, student modeling, education simulation<br /><br />Summary: This paper addresses the challenge of personalizing instruction for unique students within intelligent tutoring systems (ITSs), given the partial observability of the learning process. The authors develop a dynamic, time-series classroom simulation environment that models student-teacher interactions through interventions such as tutoring sessions, lectures, and exams. This environment incorporates various levels of probing interventions to gather more detailed student information, balancing the trade-off between probing accuracy and potential disruption. Reinforcement learning (RL) ITSs are proposed to estimate individual student states while leveraging population data, employing probing interventions strategically. The study compares RL algorithms against greedy, rules-based heuristics, finding that although both approaches yield different strategies, they achieve similar overall effectiveness. Increasing hidden information complexity highlights the difficulty of accurate student state estimation, which can be mitigated by allowing probing interventions. Both RL and heuristic policies show flexibility towards changing student population distributions; however, RL policies perform less effectively with more challenging student cohorts. Testing under different course assessment structures reveals that policies leveraging frequent quiz and midterm data improve performance more than those relying solely on finals, underscoring the value of intermediate assessments for personalized ITSs. <div>
arXiv:2511.15032v1 Announce Type: new 
Abstract: While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Oversampling techniques for predicting COVID-19 patient length of stay</title>
<link>https://arxiv.org/abs/2511.15048</link>
<guid>https://arxiv.org/abs/2511.15048</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, electronic health records, length of stay, artificial neural network, Bayesian optimization<br /><br />Summary:  
The paper focuses on predicting the severity of COVID-19 infections by analyzing patients' electronic health records (EHR). Severity is measured by the length of stay (LOS) in the hospital, with longer stays indicating more severe cases. Since the data is imbalanced, with more patients having shorter LOS, the authors apply synthetic oversampling techniques to balance the training dataset. They then utilize an Artificial Neural Network (ANN) to perform the classification task. To optimize the performance of the ANN, Bayesian optimization is employed to fine-tune its hyperparameters during training. The model that achieves the best F1 score on validation data is selected for evaluation. The study demonstrates how combining synthetic data augmentation and advanced hyperparameter tuning methods can improve predictive accuracy in healthcare applications, particularly for imbalanced data scenarios. The paper concludes with a discussion of the results and implications for managing COVID-19 patient care and resource allocation. <div>
arXiv:2511.15048v1 Announce Type: new 
Abstract: COVID-19 is a respiratory disease that caused a global pandemic in 2019. It is highly infectious and has the following symptoms: fever or chills, cough, shortness of breath, fatigue, muscle or body aches, headache, the new loss of taste or smell, sore throat, congestion or runny nose, nausea or vomiting, and diarrhea. These symptoms vary in severity; some people with many risk factors have been known to have lengthy hospital stays or die from the disease. In this paper, we analyze patients' electronic health records (EHR) to predict the severity of their COVID-19 infection using the length of stay (LOS) as our measurement of severity. This is an imbalanced classification problem, as many people have a shorter LOS rather than a longer one. To combat this problem, we synthetically create alternate oversampled training data sets. Once we have this oversampled data, we run it through an Artificial Neural Network (ANN), which during training has its hyperparameters tuned using Bayesian optimization. We select the model with the best F1 score and then evaluate it and discuss it.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable temporal fusion network of multi- and multi-class arrhythmia classification</title>
<link>https://arxiv.org/abs/2511.15062</link>
<guid>https://arxiv.org/abs/2511.15062</guid>
<content:encoded><![CDATA[
<div> Keywords: Clinical Decision Support Systems, Arrhythmia Classification, Local-Global Information Fusion, MIT-BIH Database, Attention Mechanism  

<br /><br />Summary:  
This paper addresses the challenge of arrhythmia classification in clinical decision support systems (CDSSs), focusing on the varying lengths and onset times of arrhythmia episodes which previous methods have overlooked. The authors propose a novel framework that integrates (i) local and global feature extraction and (ii) fusion of this information using an attention mechanism, designed to operate within a constrained input length. The framework was evaluated on the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB), targeting 10-class and 4-class arrhythmia detection tasks while also accurately identifying onset, offset, and duration of episodes. Results showed high F1-scores in duration, episode, and Dice score metrics: 96.45%, 82.05%, and 96.31% on MITDB; 97.57%, 98.31%, and 97.45% on AFDB, outperforming benchmark methods significantly. Additionally, cross-database tests demonstrated robust generalization, with models trained on MITDB or a malignant ventricular arrhythmia dataset performing better than state-of-the-art counterparts. The findings confirm that the proposed method effectively captures both local and global cardiac signal dynamics without significant information loss, enabling more precise arrhythmia detection and temporal localization. This advancement supports improved clinical decision-making and tailored treatment planning based on accurate arrhythmia characterization. <div>
arXiv:2511.15062v1 Announce Type: new 
Abstract: Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer</title>
<link>https://arxiv.org/abs/2511.15067</link>
<guid>https://arxiv.org/abs/2511.15067</guid>
<content:encoded><![CDATA[
<div> Colorectal cancer, prognostic stratification, multiple instance learning, TDAM-CRC, MRPL37  

<br /><br />Summary:  
This study addresses the challenge of precise prognostic stratification in colorectal cancer (CRC), which is complicated by the tumor‚Äôs high heterogeneity and the limitations of the conventional TNM staging system. The authors developed a novel AI model called TDAM-CRC based on multiple instance learning applied to histopathological whole-slide images to predict patient outcomes accurately. The model was trained on a TCGA discovery cohort of 581 patients and validated in an independent external cohort of 1,031 patients, demonstrating robust and superior prognostic risk stratification compared to existing clinical staging and other state-of-the-art models. Multivariable analyses confirmed that the TDAM-CRC risk score is an independent prognostic factor. Integration of multi-omics data uncovered that the high-risk subtype identified by the model is characterized by metabolic reprogramming and an immunosuppressive tumor microenvironment. Interaction network analysis highlighted Mitochondrial Ribosomal Protein L37 (MRPL37) as a crucial hub gene connecting the deep pathomic features to clinical outcomes. Notably, increased MRPL37 expression, linked to promoter hypomethylation, serves as an independent marker of favorable prognosis. Finally, the study proposed a nomogram combining the TDAM-CRC risk score with clinical variables to aid personalized clinical decision-making, providing a precise and interpretable prognostic tool for CRC management. <div>
arXiv:2511.15067v1 Announce Type: new 
Abstract: Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fourier-KAN-Mamba: A Novel State-Space Equation Approach for Time-Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.15083</link>
<guid>https://arxiv.org/abs/2511.15083</guid>
<content:encoded><![CDATA[
<div> Keywords: time-series anomaly detection, state-space model, Mamba, Fourier transform, Kolmogorov-Arnold Network<br /><br />Summary: This paper addresses the challenge of detecting anomalies in time-series data, which is crucial for applications like industrial monitoring and fault diagnosis. The authors highlight the recent success of Mamba-based state-space models in modeling long sequences efficiently but point out their limitations in capturing complex temporal patterns and nonlinear dynamics directly for anomaly detection. To overcome these challenges, they propose a novel hybrid architecture named Fourier-KAN-Mamba. This model integrates three key components: a Fourier layer to extract multi-scale frequency features from the input data, Kolmogorov-Arnold Networks (KAN) to enhance the nonlinear representation capabilities, and a Mamba selective state-space model equipped with a temporal gating control mechanism that improves the discrimination between normal and anomalous patterns in the time series. The approach is empirically validated on several benchmark datasets, including MSL, SMAP, and SWaT, where it demonstrates significant performance improvements over existing state-of-the-art anomaly detection methods. Overall, the paper contributes a powerful, hybrid modeling framework that effectively combines spectral analysis, nonlinear mapping, and selective temporal controls to address complex temporal anomaly detection problems. <div>
arXiv:2511.15083v1 Announce Type: new 
Abstract: Time-series anomaly detection plays a critical role in numerous real-world applications, including industrial monitoring and fault diagnosis. Recently, Mamba-based state-space models have shown remarkable efficiency in long-sequence modeling. However, directly applying Mamba to anomaly detection tasks still faces challenges in capturing complex temporal patterns and nonlinear dynamics. In this paper, we propose Fourier-KAN-Mamba, a novel hybrid architecture that integrates Fourier layer, Kolmogorov-Arnold Networks (KAN), and Mamba selective state-space model. The Fourier layer extracts multi-scale frequency features, KAN enhances nonlinear representation capability, and a temporal gating control mechanism further improves the model's ability to distinguish normal and anomalous patterns. Extensive experiments on MSL, SMAP, and SWaT datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches.
  Keywords: time-series anomaly detection, state-space model, Mamba, Fourier transform, Kolmogorov-Arnold Network
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semiconductor Industry Trend Prediction with Event Intervention Based on LSTM Model in Sentiment-Enhanced Time Series Data</title>
<link>https://arxiv.org/abs/2511.15112</link>
<guid>https://arxiv.org/abs/2511.15112</guid>
<content:encoded><![CDATA[
<div> Deep Learning, Sentiment Analysis, Semiconductor Industry, TSMC, Time Series Prediction<br /><br />Summary:<br /><br />The study integrates deep learning and sentiment analysis with traditional business model analysis and forecasting, focusing on Taiwan Semiconductor Manufacturing Company (TSMC) to predict industry trends within Taiwan's semiconductor sector. It addresses the limitations of conventional data analysis methods when handling highly variable and time-dependent data, typical of the semiconductor market's rapid changes and wafer technology developments. The research collects both textual and time series data from TSMC's seasonal reports, which include financial information. Sentiment analysis is applied to textual data while considering the impact of internal company events and external global events. These sentiment-enhanced datasets are then input into a Long Short-Term Memory (LSTM) model to forecast industry trends relevant to TSMC. The prediction outcomes highlight significant advancements in TSMC's wafer technology and identify potential threats in the global market. These results align closely with TSMC‚Äôs product announcements and international news reports. Ultimately, this work contributes an accurate industry trend prediction framework by incorporating both internal and external event interventions, offering valuable insights for researchers and business practitioners in the semiconductor industry. <div>
arXiv:2511.15112v1 Announce Type: new 
Abstract: The innovation of the study is that the deep learning method and sentiment analysis are integrated in traditional business model analysis and forecasting, and the research subject is TSMC for industry trend prediction of semiconductor industry in Taiwan. For the rapid market changes and development of wafer technologies of semiconductor industry, traditional data analysis methods not perform well in the high variety and time series data. Textual data and time series data were collected from seasonal reports of TSMC including financial information. Textual data through sentiment analysis by considering the event intervention both from internal events of the company and the external global events. Using the sentiment-enhanced time series data, the LSTM model was adopted for predicting industry trend of TSMC. The prediction results reveal significant development of wafer technology of TSMC and the potential threatens in the global market, and matches the product released news of TSMC and the international news. The contribution of the work performed accurately in industry trend prediction of the semiconductor industry by considering both the internal and external event intervention, and the prediction results provide valuable information of semiconductor industry both in research and business aspects.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient RF Passive Components Modeling with Bayesian Online Learning and Uncertainty Aware Sampling</title>
<link>https://arxiv.org/abs/2511.15125</link>
<guid>https://arxiv.org/abs/2511.15125</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian neural network, RF passive components, uncertainty-aware learning, adaptive sampling, EM simulation speedup<br /><br />Summary:<br /><br />This paper addresses the computational bottleneck in modeling radio frequency (RF) passive components with machine learning, which traditionally requires extensive electromagnetic (EM) simulations across geometric and frequency design domains. The authors propose an uncertainty-aware Bayesian online learning framework designed for efficient parametric modeling of RF passive components. The framework features a Bayesian neural network equipped with reconfigurable heads that support joint modeling across geometric and frequency domains while providing uncertainty quantification. Additionally, an adaptive sampling strategy is introduced that leverages uncertainty information to optimize training data acquisition simultaneously across geometric parameters and frequency domains. The proposed approach was validated on three different RF passive components, demonstrating its effectiveness. Results showed that the framework delivers accurate modeling performance while drastically reducing the required EM simulation time to just 2.86% of traditional machine learning-based processes, achieving an approximately 35-fold speedup. Overall, the work offers a novel, uncertainty-guided methodology that significantly improves the efficiency of RF passive component modeling, potentially accelerating the design and optimization process in practical engineering applications. <div>
arXiv:2511.15125v1 Announce Type: new 
Abstract: Conventional radio frequency (RF) passive components modeling based on machine learning requires extensive electromagnetic (EM) simulations to cover geometric and frequency design spaces, creating computational bottlenecks. In this paper, we introduce an uncertainty-aware Bayesian online learning framework for efficient parametric modeling of RF passive components, which includes: 1) a Bayesian neural network with reconfigurable heads for joint geometric-frequency domain modeling while quantifying uncertainty; 2) an adaptive sampling strategy that simultaneously optimizes training data sampling across geometric parameters and frequency domain using uncertainty guidance. Validated on three RF passive components, the framework achieves accurate modeling while using only 2.86% EM simulation time compared to traditional ML-based flow, achieving a 35 times speedup.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature</title>
<link>https://arxiv.org/abs/2511.15136</link>
<guid>https://arxiv.org/abs/2511.15136</guid>
<content:encoded><![CDATA[
<div> Keywords: Medline, sparse matrix multiplication, self-organizing map, medical knowledge mapping, algorithm scalability<br /><br />Summary:  
1. Existing efforts to map the Medline database have been constrained by the rapidly increasing memory and processing requirements of traditional algorithms, limiting analysis to small data subsets.  
2. The authors developed a novel algorithm specifically for sparse matrix multiplication, addressing the computational challenges inherent in handling large-scale datasets like Medline.  
3. This new algorithm enabled the application of a self-organizing map (SOM) to the entire Medline dataset, facilitating a more comprehensive and accurate mapping of existing medical knowledge than previously possible.  
4. The approach not only improves initial mapping but also enhances the feasibility of continuously refining the visualization in response to temporal changes and data growth within the Medline database.  
5. Overall, the work represents a significant advance in scalable data processing methodologies that support more thorough and dynamic explorations of vast biomedical literature collections. <div>
arXiv:2511.15136v1 Announce Type: new 
Abstract: Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2511.15137</link>
<guid>https://arxiv.org/abs/2511.15137</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reinforcement learning, self-verification, reasoning performance, GRPO-Verif

<br /><br />Summary: This paper addresses the challenge of improving the self-verification ability of large language models (LLMs) in their reasoning processes. While reinforcement learning has boosted LLMs' reasoning capabilities, these models often fail to consistently verify the correctness of their own reasoning traces. To tackle this, the authors propose a novel algorithm named GRPO-Verif, which optimizes both solution generation and self-verification simultaneously using a unified loss function. A key feature of this approach is an adjustable hyperparameter that controls the importance of the verification signal during training, allowing a balance between reasoning accuracy and self-verification strength. Experimental evaluations show that GRPO-Verif successfully enhances the models‚Äô ability to verify their reasoning without compromising overall reasoning performance. This demonstrates that integrating self-verification into the learning process can improve the robustness of reasoning in LLMs. The study highlights the potential of jointly training reasoning and verification objectives, opening avenues for more reliable and trustworthy AI reasoning systems. <div>
arXiv:2511.15137v1 Announce Type: new 
Abstract: The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems</title>
<link>https://arxiv.org/abs/2511.15138</link>
<guid>https://arxiv.org/abs/2511.15138</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG, emotion recognition, active learning, uncertainty, cross-modal alignment<br /><br />Summary: EEG-based emotion recognition faces challenges due to noisy signals and subjective emotional labels, which hinder the performance of deep learning models. This paper proposes an uncertainty-aware active learning framework designed to enhance robustness against label noise by utilizing both model uncertainty and cross-modal consistency between EEG and facial emotion data. Unlike traditional approaches that depend solely on EEG uncertainty estimates, this method evaluates cross-modal alignment to distinguish whether uncertainty is caused by cognitive ambiguity or sensor noise. A key component is a representation alignment module that projects EEG and face features into a shared latent space to enforce semantic coherence between modalities. Residual discrepancies in this shared space are interpreted as noise-induced inconsistencies, prompting selective querying of these uncertain samples for oracle feedback during active learning. This iterative feedback process guides the deep network to prioritize reliable and informative samples while mitigating the effects of noisy labels. Experimental validation on the ASCERTAIN dataset demonstrates the framework's efficiency and robustness, showcasing its potential as a data-efficient and noise-tolerant solution for EEG-based affective decoding within brain-computer interface systems. <div>
arXiv:2511.15138v1 Announce Type: new 
Abstract: Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex variational autoencoders admit K\"ahler structure</title>
<link>https://arxiv.org/abs/2511.15172</link>
<guid>https://arxiv.org/abs/2511.15172</guid>
<content:encoded><![CDATA[
<div> latent-Euclidean VAE, complex VAE, K√§hler geometry, Fisher information metric, latent space regularization<br /><br />Summary:<br /><br />1. The paper extends the concept of latent-Euclidean variational autoencoders (VAEs) by investigating complex VAEs that utilize a complex latent space, revealing an inherent K√§hler geometric structure.  
2. It focuses on decoder geometry and derives the Fisher information metric for the complex case assuming a latent complex Gaussian regularization with a trivial relation matrix.  
3. The authors highlight the equivalence between the Fisher information metric and the Hessian of the Kullback-Leibler (KL) divergence, thereby achieving a metric-K√§hler potential relation under relative entropy.  
4. A novel K√§hler potential derivative for complex Gaussian mixtures is proposed, which approximates the Fisher information metric while adhering to the underlying K√§hler geometry. This makes metric computation efficient and shifts heavy automatic differentiation tasks from large to small scale.  
5. The approach enables latent space regularization informed by decoder geometry and supports sampling aligned with a weighted complex volume element. Experimentally, this results in smoother latent representations and reduces semantic outliers, improving the quality and interpretability of generated samples. <div>
arXiv:2511.15172v1 Announce Type: new 
Abstract: It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level K\"ahler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric K\"ahler potential relation is exactly achieved under relative entropy. We propose a K\"ahler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying K\"ahler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model</title>
<link>https://arxiv.org/abs/2511.15174</link>
<guid>https://arxiv.org/abs/2511.15174</guid>
<content:encoded><![CDATA[
<div> Keywords: few-shot fault generation, diffusion models, time-series synthesis, positive-negative difference adapter, diversity loss  

<br /><br />Summary:  
The paper addresses the challenge of fault diagnosis in industrial equipment monitoring, where fault data are scarce due to rare fault occurrences and costly annotation. Current time-series generation models, trained mostly on abundant normal data, fail to effectively capture fault data distributions in few-shot learning scenarios because of significant domain gaps and high variability within fault classes. To overcome these limitations, the authors propose a novel framework for few-shot fault time-series generation based on diffusion models. Their method introduces a positive-negative difference adapter that leverages pre-trained distributions of normal data to learn the domain discrepancies between normal and fault conditions, enabling accurate fault sample synthesis. Furthermore, the framework incorporates a diversity loss term to prevent mode collapse by promoting inter-sample diversity, ensuring that generated fault samples are both authentic and varied. Experimental evaluation on key industrial benchmarks shows that the proposed model significantly outperforms existing approaches in generating realistic and diverse fault time-series data. This advancement facilitates more reliable fault diagnosis and predictive maintenance in industrial systems, especially when fault data are limited. <div>
arXiv:2511.15174v1 Announce Type: new 
Abstract: In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.15175</link>
<guid>https://arxiv.org/abs/2511.15175</guid>
<content:encoded><![CDATA[
<div> Quantum Graph Attention Network, Vehicle Routing Problem, Deep Reinforcement Learning, Parameterized Quantum Circuits, Proximal Policy Optimization  

<br /><br />Summary:  
This paper addresses the Vehicle Routing Problem (VRP), a complex NP-hard challenge fundamental to intelligent transportation and logistics. Traditional deep reinforcement learning approaches using Graph Neural Networks (GNNs) rely heavily on large multi-layer perceptrons (MLPs), resulting in models that are parameter-heavy and memory-bound. To overcome these limitations, the authors propose a hybrid Quantum Graph Attention Network (Q-GAT) that integrates parameterized quantum circuits (PQCs) in place of conventional MLPs at critical readout stages of the model. This innovation preserves the expressive power of graph attention encoders while significantly reducing the number of trainable parameters by more than 50%. The Q-GAT is trained using proximal policy optimization (PPO) combined with both greedy and stochastic decoding strategies. Experimental results on standard VRP benchmarks demonstrate that Q-GAT converges faster during training and achieves approximately a 5% reduction in routing costs compared with classical Graph Attention Network baselines. The findings highlight the promise of PQC-enhanced GNNs as compact, efficient, and effective solvers for large-scale routing and logistics optimization problems. This work opens avenues for combining quantum computing methods with classical machine learning to improve performance and efficiency in complex combinatorial optimization tasks. <div>
arXiv:2511.15175v1 Announce Type: new 
Abstract: The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.15190</link>
<guid>https://arxiv.org/abs/2511.15190</guid>
<content:encoded><![CDATA[
<div> Keywords: Masked auto-regressive diffusion, MARVAL, distillation, reinforcement learning, fast sampling<br /><br />Summary:<br /><br />1. Masked Auto-Regressive Diffusion models (MAR) combine the powerful modeling capabilities of diffusion models with flexible auto-regressive ordering, but suffer from slow inference due to their two-level hierarchical process involving an outer auto-regressive unmasking loop and an inner diffusion denoising chain.<br /><br />2. This inference inefficiency limits the practical application of MAR, especially in reinforcement learning (RL) post-training scenarios which require fast and scalable generative model adaptation.<br /><br />3. The authors propose MARVAL (Masked Auto-regressive Variational Acceleration), a novel distillation-based framework that compresses the multi-step diffusion process into a single auto-regressive generation step, maintaining flexible ordering while drastically improving inference speed.<br /><br />4. MARVAL introduces a new score-based variational objective for distillation, ensuring no loss in sample quality when collapsing the diffusion chain, and an efficient MARVAL-RL framework enabling reinforcement learning training with verifiable reward signals.<br /><br />5. Experiments on ImageNet 256√ó256 demonstrate that MARVAL-Huge achieves a Fr√©chet Inception Distance (FID) of 2.00 with over 30√ó speedup compared to traditional MAR diffusion, while MARVAL-RL improves CLIP and image-reward scores, enabling faster sampling and better alignment to human preferences.<br /><br />In summary, MARVAL offers the first practical solution to distill and apply reinforcement learning to masked auto-regressive diffusion models, unlocking fast, scalable, and preference-aligned generative modeling. <div>
arXiv:2511.15190v1 Announce Type: new 
Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones</title>
<link>https://arxiv.org/abs/2511.15208</link>
<guid>https://arxiv.org/abs/2511.15208</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion Large Language Models, reinforcement learning, policy gradients, trajectory dynamics, Adaptive Trajectory Policy Optimization<br /><br />Summary: This paper addresses the training of Diffusion Large Language Models (dLLMs) using reinforcement learning (RL) for enhanced reasoning capabilities. Traditional trajectory-based RL methods allocate policy gradients evenly across denoising steps, assuming equal importance for each step. The authors challenge this by introducing three step-level metrics‚Äîentropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC)‚Äîto analyze trajectories. These metrics reveal "zones of confusion," characterized by transient uncertainty spikes that can predict the success or failure of reasoning outcomes, unlike most stable steps. Building on this observation, the paper proposes Adaptive Trajectory Policy Optimization (ATPO), a novel and computationally efficient method that dynamically reallocates gradient updates to the most critical high-leverage steps, enhancing learning without modifying the RL objectives, rewards, or computational resources. By employing a combined RoEC+CM rule, ATPO significantly improves reasoning accuracy and training stability on multiple benchmark tasks. The results demonstrate that understanding and leveraging the internal dynamics of trajectory steps is essential for advancing reinforcement learning methods in dLLMs. This work suggests a promising direction for optimizing policy gradient usage by focusing on the pivotal moments within model reasoning processes. <div>
arXiv:2511.15208v1 Announce Type: new 
Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D2D Power Allocation via Quantum Graph Neural Network</title>
<link>https://arxiv.org/abs/2511.15246</link>
<guid>https://arxiv.org/abs/2511.15246</guid>
<content:encoded><![CDATA[
<div> Quantum Graph Neural Networks, Parameterized Quantum Circuits, Wireless Resource Management, Device-to-Device Communication, SINR Maximization<br /><br />Summary:<br /><br />This paper addresses the challenge of scalable resource management in increasingly complex wireless networks by introducing a fully quantum Graph Neural Network (QGNN). Unlike classical GNNs that excel in graph-related tasks but face high computational demands on large-scale data, the proposed QGNN uses Parameterized Quantum Circuits (PQCs) to perform message passing. The core innovation lies in Quantum Graph Convolutional Layers (QGCLs), which encode node features into quantum states, perform graph processing through unitaries compatible with current NISQ (Noisy Intermediate-Scale Quantum) devices, and obtain graph embeddings via quantum measurements. The framework is validated on Device-to-Device (D2D) power control, aiming at maximizing Signal-to-Interference-plus-Noise Ratio (SINR). Results demonstrate that the QGNN achieves comparable performance to classical counterparts but with a reduced number of parameters and benefits from quantum parallelism. This end-to-end PQC-based architecture exemplifies a novel intersection of quantum computing and wireless network optimization, signaling progress toward quantum-accelerated solutions in this domain. The approach offers promise for future efficient and scalable wireless resource management leveraging near-term quantum technologies. <div>
arXiv:2511.15246v1 Announce Type: new 
Abstract: Increasing wireless network complexity demands scalable resource management. Classical GNNs excel at graph learning but incur high computational costs in large-scale settings. We present a fully quantum Graph Neural Network (QGNN) that implements message passing via Parameterized Quantum Circuits (PQCs). Our Quantum Graph Convolutional Layers (QGCLs) encode features into quantum states, process graphs with NISQ-compatible unitaries, and retrieve embeddings through measurement. Applied to D2D power control for SINR maximization, our QGNN matches classical performance with fewer parameters and inherent parallelism. This end-to-end PQC-based GNN marks a step toward quantum-accelerated wireless optimization.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control</title>
<link>https://arxiv.org/abs/2511.15248</link>
<guid>https://arxiv.org/abs/2511.15248</guid>
<content:encoded><![CDATA[
<div> Keywords: entropy stabilization, large language models, reinforcement learning, exploration, Proportional-Integral Control<br /><br />Summary:<br /><br />The article addresses the challenge of maintaining stable exploration during the long-term training of large language models (LLMs) to avoid collapse into sub-optimal behaviors. It highlights the critical role of entropy in controlling exploration and preventing premature convergence to undesirable solutions. Existing reinforcement learning (RL) techniques have difficulty preserving an appropriate entropy level due to the mixed influence of positive and negative samples at different training steps. To overcome this, the authors introduce Entropy stabilization via Proportional-Integral Control (EntroPIC), a novel method that dynamically adjusts loss coefficients for positive and negative samples. This adaptive tuning stabilizes entropy throughout the training process, enabling efficient exploration and steady learning progress. The paper provides a thorough theoretical analysis applicable to both on-policy and off-policy RL learning scenarios, demonstrating EntroPIC‚Äôs efficacy in entropy regulation. Experimental results validate that EntroPIC maintains the desired entropy levels, which facilitates stable and optimal RL training for LLMs, ultimately improving training robustness and performance in large-scale language model development. <div>
arXiv:2511.15248v1 Announce Type: new 
Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimized scheduling of electricity-heat cooperative system considering wind energy consumption and peak shaving and valley filling</title>
<link>https://arxiv.org/abs/2511.15250</link>
<guid>https://arxiv.org/abs/2511.15250</guid>
<content:encoded><![CDATA[
<div> Keywords: combined power-heat systems, renewable energy integration, PVTD3 algorithm, scheduling optimization, energy storage management<br /><br />Summary: This study addresses the complex scheduling optimization challenges in combined power-heat systems amidst increasing renewable energy integration and various uncertainties. It introduces an advanced intelligent scheduling method utilizing an improved Dual-Delay Deep Deterministic Policy Gradient algorithm, named PVTD3. The method incorporates a penalty term targeting variations in grid power purchases, aiming to optimize overall system performance. Through simulations in three renewable penetration scenarios (10%, 20%, and 30%), the PVTD3 algorithm demonstrates a significant reduction in the comprehensive system cost by 6.93%, 12.68%, and 13.59% respectively, outperforming the traditional TD3 algorithm. Additionally, it achieves a 12.8% decrease in the average fluctuation amplitude of grid power purchases, enhancing grid stability. The algorithm also improves energy storage management by decreasing the end-time state values of low-temperature thermal storage tanks by 7.67 to 17.67 units, while ensuring that high-temperature thermal storage tanks remain within a safe operating range of 3.59 to 4.25 units. Comparative validation across multiple scenarios confirms that the PVTD3 algorithm excels in economic efficiency, grid stability, and sustainable energy storage scheduling, making it a promising solution for modern integrated energy systems. <div>
arXiv:2511.15250v1 Announce Type: new 
Abstract: With the global energy transition and rapid development of renewable energy, the scheduling optimization challenge for combined power-heat systems under new energy integration and multiple uncertainties has become increasingly prominent. Addressing this challenge, this study proposes an intelligent scheduling method based on the improved Dual-Delay Deep Deterministic Policy Gradient (PVTD3) algorithm. System optimization is achieved by introducing a penalty term for grid power purchase variations. Simulation results demonstrate that under three typical scenarios (10%, 20%, and 30% renewable penetration), the PVTD3 algorithm reduces the system's comprehensive cost by 6.93%, 12.68%, and 13.59% respectively compared to the traditional TD3 algorithm. Concurrently, it reduces the average fluctuation amplitude of grid power purchases by 12.8%. Regarding energy storage management, the PVTD3 algorithm reduces the end-time state values of low-temperature thermal storage tanks by 7.67-17.67 units while maintaining high-temperature tanks within the 3.59-4.25 safety operating range. Multi-scenario comparative validation demonstrates that the proposed algorithm not only excels in economic efficiency and grid stability but also exhibits superior sustainable scheduling capabilities in energy storage device management.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PLATONT: Learning a Platonic Representation for Unified Network Tomography</title>
<link>https://arxiv.org/abs/2511.15251</link>
<guid>https://arxiv.org/abs/2511.15251</guid>
<content:encoded><![CDATA[
<div> Keywords: network tomography, latent state, multimodal alignment, contrastive learning, cross-task generalization<br /><br />Summary: Network tomography is the process of inferring hidden network attributes such as link performance, traffic load, and topology based on external observations. Traditional approaches treat these problems separately and rely on limited, task-specific data, which restricts their ability to generalize and interpret results effectively. The proposed framework, PLATONT, addresses this by modeling multiple network indicators‚Äîincluding delay, loss, and bandwidth‚Äîas projections of a common latent network state. Drawing inspiration from the Platonic Representation Hypothesis, PLATONT learns this shared latent state through techniques like multimodal alignment and contrastive learning. By training various tomography tasks jointly within a unified latent space, it produces compact and structured representations that enhance generalization across tasks. Experimental evaluations on both synthetic and real-world datasets demonstrate that PLATONT consistently surpasses existing methods in key tasks such as link estimation, topology inference, and traffic prediction. Moreover, it maintains higher accuracy and robustness even as network conditions vary, underscoring its practical value for comprehensive network state inference under diverse scenarios. <div>
arXiv:2511.15251v1 Announce Type: new 
Abstract: Network tomography aims to infer hidden network states, such as link performance, traffic load, and topology, from external observations. Most existing methods solve these problems separately and depend on limited task-specific signals, which limits generalization and interpretability. We present PLATONT, a unified framework that models different network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent network state. Guided by the Platonic Representation Hypothesis, PLATONT learns this latent state through multimodal alignment and contrastive learning. By training multiple tomography tasks within a shared latent space, it builds compact and structured representations that improve cross-task generalization. Experiments on synthetic and real-world datasets show that PLATONT consistently outperforms existing methods in link estimation, topology inference, and traffic prediction, achieving higher accuracy and stronger robustness under varying network conditions.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.15256</link>
<guid>https://arxiv.org/abs/2511.15256</guid>
<content:encoded><![CDATA[
<div> Keywords: Group Relative Policy Optimization, representation learning, large language models, reward function, output group<br /><br />Summary: The paper introduces Group Relative Policy Optimization for Representation Model (GRPO-RM), a novel approach that extends the existing GRPO method‚Äîoriginally developed for fine-tuning large language models (LLMs)‚Äîto representation learning models. The authors address the challenge of adapting GRPO, which relies on token sequence sampling, by creating a predefined output set to generate output groups suitable for representation models. This modification is critical to maintaining the probability-driven optimization framework used in GRPO. Additionally, the paper proposes a specialized reward function tailored to the unique characteristics of representation models, ensuring that the optimization process aligns with their learning objectives. Through comprehensive experiments on various real-world datasets, the study demonstrates that GRPO-RM effectively improves the performance of post-training representation models. The results confirm that the method generalizes the benefits of GRPO beyond LLMs, opening new possibilities in representation learning optimization. This work bridges the gap between reinforcement learning techniques and representation model fine-tuning, highlighting practical implications for future research and applications in machine learning. <div>
arXiv:2511.15256v1 Announce Type: new 
Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SNAP: Low-Latency Test-Time Adaptation with Sparse Updates</title>
<link>https://arxiv.org/abs/2511.15276</link>
<guid>https://arxiv.org/abs/2511.15276</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-Time Adaptation, sparse adaptation, edge devices, memory normalization, domain shift<br /><br />Summary:<br /><br />1. The paper addresses the challenge of Test-Time Adaptation (TTA) for models to handle distribution shifts using unlabeled test data, focusing on overcoming high computational cost and frequent update requirements which limit use on resource-constrained edge devices.<br /><br />2. The authors propose SNAP, a sparse TTA framework designed to reduce both the frequency of adaptation and the amount of data used, while maintaining model accuracy even when adapting with only 1% of incoming test data.<br /><br />3. SNAP introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which selectively stores a small set of samples that are representative of both class and domain features for efficient adaptation, and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time leveraging these representative samples to adapt to domain shifts.<br /><br />4. The framework is compatible with five state-of-the-art TTA algorithms and shows substantial improvements by reducing latency up to 93.12%, while keeping accuracy loss under 3.3%, across varying adaptation rates from 1% to 50%.<br /><br />5. These results underscore SNAP's potential for practical deployment in latency-sensitive edge applications, ensuring efficient and robust test-time adaptation with limited data and computing resources. The source code is publicly available for further research and implementation. <div>
arXiv:2511.15276v1 Announce Type: new 
Abstract: Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs</title>
<link>https://arxiv.org/abs/2511.15300</link>
<guid>https://arxiv.org/abs/2511.15300</guid>
<content:encoded><![CDATA[
<div> Keywords: Quant-Trim, low-bit quantization, edge accelerators, progressive fake quantization, reverse pruning<br /><br />Summary:<br /><br />Specialized edge accelerators commonly use low-bit quantization to optimize performance, but vendor compilers often implement scaling, clipping, and kernel support differently, resulting in inconsistent accuracy from the same floating-point (FP) checkpoint across various backends. This inconsistency forces practitioners to manually adjust compiler flags or refactor models to suit specific vendor operator subsets. To address these challenges, the article introduces Quant-Trim, a training-phase method designed to produce a hardware-neutral checkpoint that maintains robustness across different backend implementations and precision settings. Quant-Trim incorporates progressive fake quantization to better align the training process with the integer grid used during deployment. It also employs reverse pruning techniques to control scale inflation caused by outlier weights, all while preserving model learnability. The method is compatible with a wide array of quantization schemes, including symmetric or asymmetric quantization, per-tensor or per-channel scaling, and INT8 or INT4 precision levels, and does not require vendor-specific graph modifications. Experimental results demonstrate that Quant-Trim reduces the accuracy gap between FP and low-bit models, lessens reliance on compiler heuristics and calibration steps, and eliminates the need for backend-specific retraining. The paper reports gains across multiple models and tasks, showing improvements in accuracy, latency, throughput, energy per inference, and cost under different activation scaling methods and operator coverage scenarios. <div>
arXiv:2511.15300v1 Announce Type: new 
Abstract: Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Internal Semantics of Time-Series Foundation Models</title>
<link>https://arxiv.org/abs/2511.15324</link>
<guid>https://arxiv.org/abs/2511.15324</guid>
<content:encoded><![CDATA[
<div> Time-series, Foundation Models, Concept Interpretability, Layer-wise Analysis, Composition Challenges<br /><br />Summary:<br /><br />This study investigates the interpretability of Time-series Foundation Models (TSFMs) by systematically analyzing how these models represent fundamental time-series concepts. First, the research identifies which layers of the models encode specific concepts, revealing that early layers primarily capture local time-domain patterns such as autoregressive structures (AR(1)), level shifts, and trends, whereas deeper layers focus on signals related to dispersion and change points. Second, the authors examine whether concept parameters can be linearly recovered from model representations, finding that although many concepts are linearly recoverable, spectral properties and warping factors are notably challenging to recover with linear methods. Third, the study tracks how representations evolve in terms of concept disentanglement and abstraction as the model depth increases, showing increasing abstraction yet persistent difficulty with some spectral concepts. Fourth, the research probes how the models handle the composition of concepts and discovers a degradation in probe performance when concepts are combined, indicating interference and difficulty in representing interacting temporal phenomena. Overall, the findings highlight that while TSFMs localize atomic concepts reliably within certain layers, they face significant limitations in compositional settings, pointing to a critical area for future improvement in representing complex, interacting temporal behaviors. <div>
arXiv:2511.15324v1 Announce Type: new 
Abstract: Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials</title>
<link>https://arxiv.org/abs/2511.15327</link>
<guid>https://arxiv.org/abs/2511.15327</guid>
<content:encoded><![CDATA[
<div> Keywords: Spectral Graph Neural Networks, Krawtchouk polynomials, over-smoothing, heterophilic graphs, adaptive filters<br /><br />Summary: This paper addresses two significant limitations in spectral Graph Neural Networks (GNNs) based on polynomial filters like ChebyNet: performance degradation on heterophilic graphs and over-smoothing at high polynomial degrees. Over-smoothing occurs when increased filter degree causes the node representations to become indistinguishable. The authors introduce KrawtchoukNet, a novel GNN filter leveraging discrete Krawtchouk polynomials to overcome these issues. First, KrawtchoukNet sets the polynomial domain N to a small fixed constant (e.g., N=20), ensuring the recurrence coefficients are inherently bounded, which greatly enhances robustness against over-smoothing, even at high polynomial orders like K=10. Second, KrawtchoukNet incorporates a learnable shape parameter p, allowing the filter to adapt its spectral characteristics dynamically to the graph structure and data. This adaptability helps KrawtchoukNet excel on heterophilic graphs, which are challenging for conventional GNNs. The paper reports state-of-the-art (SOTA) performance on heterophilic benchmark datasets such as Texas and Cornell, significantly outperforming prominent GNN models like Graph Attention Network (GAT) and Approximate Personalized Propagation of Neural Predictions (APPNP). Overall, KrawtchoukNet provides a unified and effective approach to mitigate over-smoothing and improve heterophilic graph learning by leveraging a bounded, adaptive spectral filter design. <div>
arXiv:2511.15327v1 Announce Type: new 
Abstract: Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on "heterophilic" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials</title>
<link>https://arxiv.org/abs/2511.15328</link>
<guid>https://arxiv.org/abs/2511.15328</guid>
<content:encoded><![CDATA[
<div> Spectral Graph Neural Networks, heterophilic graphs, over-smoothing, adaptive polynomial filters, LaguerreNet  

<br /><br />Summary:  
This paper addresses two prominent limitations of Spectral Graph Neural Networks (GNNs): their poor performance on heterophilic graphs and susceptibility to over-smoothing at high polynomial degrees (K). Traditional filters like ChebyNet are static low-pass filters, which contribute to these issues. To overcome this, the authors propose LaguerreNet, a novel adaptive polynomial filter based on continuous Laguerre polynomials. Unlike previous discrete adaptive filters such as MeixnerNet, LaguerreNet introduces a trainable core parameter alpha to learn the filter's spectral shape dynamically. The work tackles the numerical instability problem inherent to these unbounded polynomials, typically scaling as O(k¬≤), by applying a LayerNorm-based stabilization technique. Experimentally, LaguerreNet attains state-of-the-art performance on challenging heterophilic graph benchmarks, demonstrating its effectiveness in scenarios where traditional filters fail. Furthermore, the model shows remarkable robustness against over-smoothing, maintaining peak performance at polynomial degrees as high as K=10‚Äîan order of magnitude greater than ChebyNet's collapse point. This research thus advances adaptive polynomial filters in GNNs, offering a stable and flexible solution that unifies spectral filtering benefits across graph types with improved stability and expressiveness. <div>
arXiv:2511.15328v1 Announce Type: new 
Abstract: Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on "heterophilic" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STREAM-VAE: Dual-Path Routing for Slow and Fast Dynamics in Vehicle Telemetry Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.15339</link>
<guid>https://arxiv.org/abs/2511.15339</guid>
<content:encoded><![CDATA[
<div> Keywords: STREAM-VAE, automotive telemetry, anomaly detection, variational autoencoder, time-series dynamics  

<br /><br />Summary:  
1. Automotive telemetry data is characterized by both slow drifts and fast spikes within the same sequences, which complicates reliable anomaly detection.  
2. Traditional reconstruction-based methods, particularly sequence variational autoencoders (VAEs), rely on a single latent process that mixes these heterogeneous time scales, often resulting in smoothed spikes or inflated variances that degrade anomaly detection performance.  
3. The paper introduces STREAM-VAE, a novel variational autoencoder architecture designed for anomaly detection in automotive telemetry time-series data.  
4. STREAM-VAE employs a dual-path encoder that separately models slow drift and fast spike signal dynamics, together with a decoder that isolates transient deviations from normal operational patterns, enhancing interpretability and detection precision.  
5. The model aims for practical deployment, generating stable anomaly scores across different vehicle operating modes, suitable for both in-vehicle monitoring systems and backend fleet analytics.  
6. Experimental evaluation on a real-world automotive telemetry dataset and the public SMD benchmark demonstrates that STREAM-VAE‚Äôs explicit separation of drift and spike dynamics yields more robust anomaly detection compared to strong baselines including forecasting, attention mechanisms, graph models, and traditional VAEs. <div>
arXiv:2511.15339v1 Announce Type: new 
Abstract: Automotive telemetry data exhibits slow drifts and fast spikes, often within the same sequence, making reliable anomaly detection challenging. Standard reconstruction-based methods, including sequence variational autoencoders (VAEs), use a single latent process and therefore mix heterogeneous time scales, which can smooth out spikes or inflate variances and weaken anomaly separation.
  In this paper, we present STREAM-VAE, a variational autoencoder for anomaly detection in automotive telemetry time-series data. Our model uses a dual-path encoder to separate slow drift and fast spike signal dynamics, and a decoder that represents transient deviations separately from the normal operating pattern. STREAM-VAE is designed for deployment, producing stable anomaly scores across operating modes for both in-vehicle monitors and backend fleet analytics.
  Experiments on an automotive telemetry dataset and the public SMD benchmark show that explicitly separating drift and spike dynamics improves robustness compared to strong forecasting, attention, graph, and VAE baselines.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-layer Stack Ensembles for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.15350</link>
<guid>https://arxiv.org/abs/2511.15350</guid>
<content:encoded><![CDATA[
<div> Ensembling, Stacking, Time Series Forecasting, Multi-layer, AutoML

<br /><br />Summary:  
This paper explores the effectiveness of ensembling strategies specifically for time series forecasting, a domain where ensemble methods are less commonly applied compared to other machine learning tasks. The study evaluates 33 ensemble models, including both existing and newly proposed approaches, across 50 diverse real-world datasets. It finds that stacking, a powerful ensemble technique, consistently improves forecasting accuracy but no single stacking model dominates across all tasks. To remedy this variability, the authors introduce a multi-layer stacking framework that integrates multiple stacker models to harness their collective strengths. Experimental results demonstrate that this multi-layer stacking method delivers superior accuracy in a wide range of forecasting scenarios, outperforming traditional simple linear ensembles. The research highlights the untapped potential of stacking-based ensemble methods to enhance automated machine learning (AutoML) systems designed for time series forecasting, suggesting a promising direction for future improvements in predictive performance in this domain. <div>
arXiv:2511.15350v1 Announce Type: new 
Abstract: Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and Decision Support System for Heart Failure Mortality Prediction</title>
<link>https://arxiv.org/abs/2511.15357</link>
<guid>https://arxiv.org/abs/2511.15357</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, cost-benefit analysis, large language models, heart failure, clinical decision support<br /><br />Summary:<br /><br />1. Objective: The study addresses the gap in predictive machine learning (ML) models by incorporating downstream value trade-offs and clinical interpretability to enhance decision support.<br /><br />2. Methodology: Researchers developed an ML model using eXtreme gradient boosting (XGB) to predict 1-year mortality in a cohort of 30,021 heart failure patients with 22% mortality rate, aiming to identify candidates for home care.<br /><br />3. Innovations: Introduced Clinical Impact Projection (CIP) curves to visualize key cost dimensions such as quality of life and healthcare provider expenses, further categorized into treatment and error costs, to assess prediction consequences.<br /><br />4. Integration of LLMs: Four large language model (LLM) agents were used to generate patient-specific explanations of the cost-benefit trade-offs to enhance interpretability.<br /><br />5. Evaluation and Results: The XGB model achieved strong predictive performance (AUROC 0.804, AUPRC 0.529, Brier score 0.135). Clinicians positively evaluated the system‚Äôs decision support value but suggested improving technical accuracy, especially for speculative tasks.<br /><br />6. Conclusion: The Cost-Aware Prediction (CAP) framework successfully combines ML outcomes with cost-benefit analysis through LLM agents, offering more transparent and interpretable clinical decision support. <div>
arXiv:2511.15357v1 Announce Type: new 
Abstract: Objective: Machine learning (ML) predictive models are often developed without considering downstream value trade-offs and clinical interpretability. This paper introduces a cost-aware prediction (CAP) framework that combines cost-benefit analysis assisted by large language model (LLM) agents to communicate the trade-offs involved in applying ML predictions. Materials and Methods: We developed an ML model predicting 1-year mortality in patients with heart failure (N = 30,021, 22% mortality) to identify those eligible for home care. We then introduced clinical impact projection (CIP) curves to visualize important cost dimensions - quality of life and healthcare provider expenses, further divided into treatment and error costs, to assess the clinical consequences of predictions. Finally, we used four LLM agents to generate patient-specific descriptions. The system was evaluated by clinicians for its decision support value. Results: The eXtreme gradient boosting (XGB) model achieved the best performance, with an area under the receiver operating characteristic curve (AUROC) of 0.804 (95% confidence interval (CI) 0.792-0.816), area under the precision-recall curve (AUPRC) of 0.529 (95% CI 0.502-0.558) and a Brier score of 0.135 (95% CI 0.130-0.140). Discussion: The CIP cost curves provided a population-level overview of cost composition across decision thresholds, whereas LLM-generated cost-benefit analysis at individual patient-levels. The system was well received according to the evaluation by clinicians. However, feedback emphasizes the need to strengthen the technical accuracy for speculative tasks. Conclusion: CAP utilizes LLM agents to integrate ML classifier outcomes and cost-benefit analysis for more transparent and interpretable decision support.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CID: Measuring Feature Importance Through Counterfactual Distributions</title>
<link>https://arxiv.org/abs/2511.15371</link>
<guid>https://arxiv.org/abs/2511.15371</guid>
<content:encoded><![CDATA[
<div> Counterfactual Importance, Feature Importance, Kernel Density Estimation, Local Explanation, Faithfulness Metrics  

<br /><br />Summary:  
This paper addresses the critical task of assessing individual feature importance in machine learning models to better understand their decision-making processes. It identifies a key challenge in the field: the absence of definitive ground truth for feature importance, which necessitates well-founded alternative measures. To overcome this, the authors propose a novel post-hoc local feature importance method named Counterfactual Importance Distribution (CID). The CID approach involves generating two sets of counterfactual instances‚Äîpositive and negative‚Äîand modeling their distributions through Kernel Density Estimation. Features are then ranked based on a mathematically rigorous distributional dissimilarity measure, which satisfies important metric properties. The method is evaluated against well-established local feature importance explainers, demonstrating complementary insights as well as improved performance on standard faithfulness metrics, including comprehensiveness and sufficiency. These improvements suggest that CID provides more faithful and reliable explanations of model behavior. The results emphasize the potential of CID as a valuable tool for detailed model analysis, offering both theoretical soundness and practical effectiveness in explaining complex machine learning predictions. <div>
arXiv:2511.15371v1 Announce Type: new 
Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter Importance-Driven Continual Learning for Foundation Models</title>
<link>https://arxiv.org/abs/2511.15375</link>
<guid>https://arxiv.org/abs/2511.15375</guid>
<content:encoded><![CDATA[
<div> Keywords: continual learning, parameter importance, catastrophic forgetting, foundation models, parameter-efficient tuning  

<br /><br />Summary:  
The paper addresses the issue of catastrophic forgetting that occurs during domain-specific post-training of foundation models, which results in the loss of general reasoning ability and reduces adaptability to changing real-world scenarios. It highlights the challenge of preserving general capabilities while simultaneously acquiring new domain knowledge for both large language and multimodal models. Traditional continual learning approaches, including regularization, replay, and architectural isolation, are identified as having drawbacks such as poor downstream task performance, dependence on inaccessible past training data, or excessive increase in model parameters. To overcome these limitations, the authors propose PIECE, a novel method based on Parameter Importance Estimation for Continual Enhancement. PIECE selectively updates only 0.1% of the most task-relevant parameters without requiring access to previous training data or increasing the total number of parameters. The approach employs two importance estimators: PIECE-F, which uses Fisher Information, and PIECE-S, which integrates gradient and curvature information via second-order normalization. Experimental evaluations on three language models and two multimodal models demonstrate that PIECE successfully maintains general capabilities and achieves state-of-the-art performance in continual learning across diverse downstream tasks. The study offers a practical and scalable solution for domain-adaptive foundation models that mitigates catastrophic forgetting effectively. <div>
arXiv:2511.15375v1 Announce Type: new 
Abstract: Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG</title>
<link>https://arxiv.org/abs/2511.15393</link>
<guid>https://arxiv.org/abs/2511.15393</guid>
<content:encoded><![CDATA[
<div> Keywords: brain age, EEG, anomaly detection, Transformer, Variational Information Bottleneck  

<br /><br />Summary:  
1. The paper addresses brain age estimation, an important biomarker of brain health, using EEG data.  
2. Existing models struggle to learn a normal baseline from weakly supervised datasets containing only healthy subjects, which limits their ability in detecting anomalies related to disease.  
3. The authors propose EVA-Net, an interpretable anomaly detection framework that leverages a sparsified-attention Transformer to effectively model long EEG sequences.  
4. EVA-Net incorporates a Variational Information Bottleneck to manage noise and variability inherent in imperfect medical data by learning a robust, compressed latent representation.  
5. For interpretability, the learned representation is aligned with a continuous prototype network that captures the healthy aging manifold explicitly, enabling a clear understanding of deviations.  
6. Trained on EEG data from 1297 healthy individuals, EVA-Net achieves state-of-the-art accuracy in brain age prediction.  
7. The framework was validated on an independent cohort of 27 patients with Mild Cognitive Impairment (MCI) and Alzheimer‚Äôs Disease (AD), showing significant brain-age gaps and a new Prototype Alignment Error metric that reflects pathological deviations.  
8. EVA-Net provides a novel, interpretable approach for healthcare intelligence applications dealing with imperfect medical data, improving anomaly detection in neurological disorders. <div>
arXiv:2511.15393v1 Announce Type: new 
Abstract: The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proximal Approximate Inference in State-Space Models</title>
<link>https://arxiv.org/abs/2511.15409</link>
<guid>https://arxiv.org/abs/2511.15409</guid>
<content:encoded><![CDATA[
<div> Keywords: state estimation, nonlinear models, variational Lagrangian, Bayesian inference, Gauss‚ÄìMarkov approximations  

<br /><br />Summary:  
This article introduces a novel class of algorithms designed for state estimation within nonlinear, non-Gaussian state-space models. The primary innovation is a variational Lagrangian framework that reformulates Bayesian inference as a sequence of entropic trust-region updates constrained by dynamic conditions. This approach generates a family of forward-backward algorithms, which are shaped by the specific factorization chosen for the variational posterior distribution. A key focus is placed on Gauss‚ÄìMarkov approximations, enabling the derivation of recursive schemes that exhibit favorable computational complexity, making the methods practical for real-world applications. To address the challenges posed by general nonlinear and non-Gaussian models, the authors close the recursion using generalized statistical linear regression techniques combined with Fourier‚ÄìHermite moment matching. This hybrid methodology allows the algorithms to maintain efficiency and accuracy despite the complexity of the underlying models. Overall, the paper contributes a flexible and computationally tractable framework for advanced state estimation tasks, enhancing the capability to handle complex model dynamics and noise characteristics beyond traditional Gaussian assumptions. <div>
arXiv:2511.15409v1 Announce Type: new 
Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Understanding Layer Contributions in Tabular In-Context Learning Models</title>
<link>https://arxiv.org/abs/2511.15432</link>
<guid>https://arxiv.org/abs/2511.15432</guid>
<content:encoded><![CDATA[
<div> Keywords: tabular in-context learning, latent spaces, model layers, redundancy, model compression<br /><br />Summary:<br /><br />This paper investigates the role of individual layers in tabular in-context learning (ICL) models, drawing comparisons to large language models (LLMs). Despite architectural similarities between tabular ICL models and LLMs, there is a lack of understanding about how each layer contributes to tabular data prediction. The authors focus on the evolution of latent spaces across different layers to identify patterns and redundancies. Using models such as TabPFN and TabICL, the study employs a novel "layers as painters" framework, which conceptualizes layers as entities contributing distinct representational languages. The findings reveal that only certain subsets of layers share a common representational language, indicating the presence of structural redundancy within the model. This redundancy suggests that some layers may be functionally unnecessary or could be combined, presenting potential for model compression. Moreover, understanding layer contributions through this lens can enhance the interpretability of tabular ICL models by clarifying how information is processed throughout the network. Overall, the research opens avenues for both more efficient and more interpretable tabular prediction models by leveraging insights about layer dynamics and redundancy. <div>
arXiv:2511.15432v1 Announce Type: new 
Abstract: Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TSFM in-context learning for time-series classification of bearing-health status</title>
<link>https://arxiv.org/abs/2511.15447</link>
<guid>https://arxiv.org/abs/2511.15447</guid>
<content:encoded><![CDATA[
<div> Keywords: in-context learning, time-series foundation models, vibration data, bearing health assessment, AI-driven maintenance systems

<br /><br />Summary:  
This paper presents a novel classification method leveraging in-context learning within time-series foundation models (TSFM). Unlike traditional approaches, it enables classification of data not included in the original TSFM training set without requiring model finetuning. The method structures examples by combining targets (class identifiers) and covariates (data matrices) inside the model prompt, allowing simultaneous classification of unknown covariate patterns along the forecast axis via in-context learning. A practical application is demonstrated on vibration data for evaluating the health state of a bearing in a servo-press motor. This involves converting frequency domain reference signals into pseudo time-series patterns, aligning covariate and target signals, and using the TSFM to predict class probabilities of the data. The approach capitalizes on the scalability and generalization of pre-trained models, proving effective across diverse operating conditions. This advances the field beyond specialized narrow AI solutions towards comprehensive and adaptable AI-driven maintenance systems. The research thus contributes a scalable, flexible, and efficient framework for real-world industrial health monitoring through in-context learning applied to time-series data. <div>
arXiv:2511.15447v1 Announce Type: new 
Abstract: This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning</title>
<link>https://arxiv.org/abs/2511.15454</link>
<guid>https://arxiv.org/abs/2511.15454</guid>
<content:encoded><![CDATA[
<div> Federated learning, energy efficiency, fairness, device selection, compression

<br /><br />Summary: Federated learning (FL) facilitates collaborative model training across distributed devices while maintaining data privacy. This paper tackles the challenge of balancing energy efficiency, fairness in client participation, and model accuracy in wireless edge systems, which struggle due to heterogeneous device resources, unequal client contributions, and constrained communication bandwidth. The authors propose FairEnergy, a novel framework that incorporates a contribution score reflecting both the magnitude of model updates and their compression ratio. This score is integrated into a joint optimization problem involving device selection, bandwidth allocation, and compression level control. The formulation results in a mixed-integer non-convex problem, which is addressed by relaxing the binary device selection variables and applying Lagrangian decomposition techniques to decouple the global bandwidth constraints. Each device solves its own subproblem based on this approach. Experimental results on non-IID data demonstrate that FairEnergy significantly improves model accuracy while reducing energy consumption by up to 79% compared to baseline methods, achieving a better trade-off between fairness, communication efficiency, and energy saving in FL systems. <div>
arXiv:2511.15454v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NTK-Guided Implicit Neural Teaching</title>
<link>https://arxiv.org/abs/2511.15487</link>
<guid>https://arxiv.org/abs/2511.15487</guid>
<content:encoded><![CDATA[
<div> Keywords: Implicit Neural Representations, Neural Tangent Kernel, coordinate selection, training acceleration, function fitting<br /><br />Summary:<br /><br />1. Implicit Neural Representations (INRs) use multilayer perceptrons (MLPs) to model continuous signals like images, audio, and 3D shapes in a compact and resolution-independent way.<br />2. Training INRs for high-resolution signals requires optimizing millions of coordinates, which is computationally expensive and time-consuming.<br />3. The paper introduces NTK-Guided Implicit Neural Teaching (NINT), a method designed to accelerate INR training by dynamically selecting the most informative coordinates that maximize global functional updates.<br />4. NINT leverages the Neural Tangent Kernel (NTK) to score data points by the norm of their NTK-augmented loss gradients, integrating both fitting error and the influence between coordinates (self-leverage and cross-coordinate coupling).<br />5. By considering these dual factors, NINT achieves faster convergence and significantly reduces training time by nearly 50% compared to existing sampling-based acceleration methods, without sacrificing‚Äîand sometimes improving‚Äîthe quality of the learned representation. <div>
arXiv:2511.15487v1 Announce Type: new 
Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-Adaptivity Tradeoff in On-Demand Sampling</title>
<link>https://arxiv.org/abs/2511.15507</link>
<guid>https://arxiv.org/abs/2511.15507</guid>
<content:encoded><![CDATA[
<div> Multi-Distribution Learning, Sample Complexity, Round Complexity, On-Demand Sampling, Optimization via On-Demand Sampling (OODS)  

<br /><br />Summary:  
1. The paper investigates the tradeoff between sample complexity and round complexity in on-demand sampling for learning algorithms that adaptively draw samples from \(k\) different distributions over a limited number of interaction rounds.  
2. In the realizable Multi-Distribution Learning (MDL) setting, the authors characterize the optimal sample complexity for an \(r\)-round algorithm, showing it scales roughly as \(d k^{\Theta(1/r)} / \epsilon\), where \(d\) is the dimension and \(\epsilon\) the accuracy parameter.  
3. For the more challenging agnostic case (where model assumptions may not hold), they propose an algorithm achieving near-optimal sample complexity of \(\widetilde{O}((d + k)/\epsilon^2)\) using about \(\widetilde{O}(\sqrt{k})\) rounds, significantly improving round efficiency.  
4. Introducing a new conceptual framework called Optimization via On-Demand Sampling (OODS), the paper unifies and abstracts the sample adaptivity tradeoffs present in most MDL algorithms, providing a powerful lens to analyze these problems.  
5. The authors also establish nearly tight upper and lower bounds on the round complexity within the OODS framework. Their lower bounds imply that surpassing sub-polynomial round complexity barriers in agnostic MDL would require fundamentally new approaches that circumvent the intrinsic difficulties of on-demand sampling adaptivity. <div>
arXiv:2511.15507v1 Announce Type: new 
Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{\Theta(1/r)} / \epsilon$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / \epsilon^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles</title>
<link>https://arxiv.org/abs/2511.15522</link>
<guid>https://arxiv.org/abs/2511.15522</guid>
<content:encoded><![CDATA[
arXiv:2511.15522v1 Announce Type: new 
Abstract: Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CODE: A global approach to ODE dynamics learning</title>
<link>https://arxiv.org/abs/2511.15619</link>
<guid>https://arxiv.org/abs/2511.15619</guid>
<content:encoded><![CDATA[
arXiv:2511.15619v1 Announce Type: new 
Abstract: Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges</title>
<link>https://arxiv.org/abs/2511.15652</link>
<guid>https://arxiv.org/abs/2511.15652</guid>
<content:encoded><![CDATA[
arXiv:2511.15652v1 Announce Type: new 
Abstract: Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.
  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.
  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.15669</link>
<guid>https://arxiv.org/abs/2511.15669</guid>
<content:encoded><![CDATA[
arXiv:2511.15669v1 Announce Type: new 
Abstract: Enabling Vision-Language-Action (VLA) models to "think before acting" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Walrus: A Cross-Domain Foundation Model for Continuum Dynamics</title>
<link>https://arxiv.org/abs/2511.15684</link>
<guid>https://arxiv.org/abs/2511.15684</guid>
<content:encoded><![CDATA[
arXiv:2511.15684v1 Announce Type: new 
Abstract: Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Impact of Quantization on Large Reasoning Model Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.15694</link>
<guid>https://arxiv.org/abs/2511.15694</guid>
<content:encoded><![CDATA[
arXiv:2511.15694v1 Announce Type: new 
Abstract: Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cluster-based Adaptive Retrieval: Dynamic Context Selection for RAG Applications</title>
<link>https://arxiv.org/abs/2511.14769</link>
<guid>https://arxiv.org/abs/2511.14769</guid>
<content:encoded><![CDATA[
arXiv:2511.14769v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by pulling in external material, document, code, manuals, from vast and ever-growing corpora, to effectively answer user queries. The effectiveness of RAG depends significantly on aligning the number of retrieved documents with query characteristics: narrowly focused queries typically require fewer, highly relevant documents, whereas broader or ambiguous queries benefit from retrieving more extensive supporting information. However, the common static top-k retrieval approach fails to adapt to this variability, resulting in either insufficient context from too few documents or redundant information from too many. Motivated by these challenges, we introduce Cluster-based Adaptive Retrieval (CAR), an algorithm that dynamically determines the optimal number of documents by analyzing the clustering patterns of ordered query-document similarity distances. CAR detects the transition point within similarity distances, where tightly clustered, highly relevant documents shift toward less pertinent candidates, establishing an adaptive cut-off that scales with query complexity. On Coinbase's CDP corpus and the public MultiHop-RAG benchmark, CAR consistently picks the optimal retrieval depth and achieves the highest TES score, outperforming every fixed top-k baseline. In downstream RAG evaluations, CAR cuts LLM token usage by 60%, trims end-to-end latency by 22%, and reduces hallucinations by 10% while fully preserving answer relevance. Since integrating CAR into Coinbase's virtual assistant, we've seen user engagement jump by 200%.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reservoir Computing via Multi-Scale Random Fourier Features for Forecasting Fast-Slow Dynamical Systems</title>
<link>https://arxiv.org/abs/2511.14775</link>
<guid>https://arxiv.org/abs/2511.14775</guid>
<content:encoded><![CDATA[
arXiv:2511.14775v1 Announce Type: cross 
Abstract: Forecasting nonlinear time series with multi-scale temporal structures remains a central challenge in complex systems modeling. We present a novel reservoir computing framework that combines delay embedding with random Fourier feature (RFF) mappings to capture such dynamics. Two formulations are investigated: a single-scale RFF reservoir, which employs a fixed kernel bandwidth, and a multi-scale RFF reservoir, which integrates multiple bandwidths to represent both fast and slow temporal dependencies. The framework is applied to a diverse set of canonical systems: neuronal models such as the Rulkov map, Izhikevich model, Hindmarsh-Rose model, and Morris-Lecar model, which exhibit spiking, bursting, and chaotic behaviors arising from fast-slow interactions; and ecological models including the predator-prey dynamics and Ricker map with seasonal forcing, which display multi-scale oscillations and intermittency. Across all cases, the multi-scale RFF reservoir consistently outperforms its single-scale counterpart, achieving lower normalized root mean square error (NRMSE) and more robust long-horizon predictions. These results highlight the effectiveness of explicitly incorporating multi-scale feature mappings into reservoir computing architectures for modeling complex dynamical systems with intrinsic fast-slow interactions.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convex Clustering Redefined: Robust Learning with the Median of Means Estimator</title>
<link>https://arxiv.org/abs/2511.14784</link>
<guid>https://arxiv.org/abs/2511.14784</guid>
<content:encoded><![CDATA[
arXiv:2511.14784v1 Announce Type: cross 
Abstract: Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging</title>
<link>https://arxiv.org/abs/2511.14806</link>
<guid>https://arxiv.org/abs/2511.14806</guid>
<content:encoded><![CDATA[
arXiv:2511.14806v1 Announce Type: cross 
Abstract: Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fully Differentiable dMRI Streamline Propagation in PyTorch</title>
<link>https://arxiv.org/abs/2511.14807</link>
<guid>https://arxiv.org/abs/2511.14807</guid>
<content:encoded><![CDATA[
arXiv:2511.14807v1 Announce Type: cross 
Abstract: Diffusion MRI (dMRI) provides a distinctive means to probe the microstructural architecture of living tissue, facilitating applications such as brain connectivity analysis, modeling across multiple conditions, and the estimation of macrostructural features. Tractography, which emerged in the final years of the 20th century and accelerated in the early 21st century, is a technique for visualizing white matter pathways in the brain using dMRI. Most diffusion tractography methods rely on procedural streamline propagators or global energy minimization methods. Although recent advancements in deep learning have enabled tasks that were previously challenging, existing tractography approaches are often non-differentiable, limiting their integration in end-to-end learning frameworks. While progress has been made in representing streamlines in differentiable frameworks, no existing method offers fully differentiable propagation. In this work, we propose a fully differentiable solution that retains numerical fidelity with a leading streamline algorithm. The key is that our PyTorch-engineered streamline propagator has no components that block gradient flow, making it fully differentiable. We show that our method matches standard propagators while remaining differentiable. By translating streamline propagation into a differentiable PyTorch framework, we enable deeper integration of tractography into deep learning workflows, laying the foundation for a new category of macrostructural reasoning that is not only computationally robust but also scientifically rigorous.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implicit Bias of the JKO Scheme</title>
<link>https://arxiv.org/abs/2511.14827</link>
<guid>https://arxiv.org/abs/2511.14827</guid>
<content:encoded><![CDATA[
arXiv:2511.14827v1 Announce Type: cross 
Abstract: Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $\eta>0$ a sequence of probability distributions $\rho_k^\eta$ that approximate to first order in $\eta$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $\lambda$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $\eta$. We show that $\rho_k^\eta$ are approximated to order $\eta^2$ by Wasserstein gradient flow on a \emph{modified} energy \[ J^{\eta}(\rho) = J(\rho) - \frac{\eta}{4}\int_M \Big\lVert \nabla_g \frac{\delta J}{\delta \rho} (\rho) \Big\rVert_{2}^{2} \,\rho(dx), \] obtained by subtracting from $J$ the squared metric curvature of $J$ times $\eta/4$. The JKO scheme therefore adds at second order in $\eta$ a \textit{deceleration} in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{\"a}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^\eta$ we study \emph{JKO-Flow}, Wasserstein gradient flow on $J^\eta$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to pick the best anomaly detector?</title>
<link>https://arxiv.org/abs/2511.14832</link>
<guid>https://arxiv.org/abs/2511.14832</guid>
<content:encoded><![CDATA[
arXiv:2511.14832v1 Announce Type: cross 
Abstract: Anomaly detection has the potential to discover new physics in unexplored regions of the data. However, choosing the best anomaly detector for a given data set in a model-agnostic way is an important challenge which has hitherto largely been neglected. In this paper, we introduce the data-driven ARGOS metric, which has a sound theoretical foundation and is empirically shown to robustly select the most sensitive anomaly detection model given the data. Focusing on weakly-supervised, classifier-based anomaly detection methods, we show that the ARGOS metric outperforms other model selection metrics previously used in the literature, in particular the binary cross-entropy loss. We explore several realistic applications, including hyperparameter tuning as well as architecture and feature selection, and in all cases we demonstrate that ARGOS is robust to the noisy conditions of anomaly detection.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings</title>
<link>https://arxiv.org/abs/2511.14868</link>
<guid>https://arxiv.org/abs/2511.14868</guid>
<content:encoded><![CDATA[
arXiv:2511.14868v1 Announce Type: cross 
Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard</title>
<link>https://arxiv.org/abs/2511.14876</link>
<guid>https://arxiv.org/abs/2511.14876</guid>
<content:encoded><![CDATA[
arXiv:2511.14876v1 Announce Type: cross 
Abstract: To autonomously control vehicles, driving agents use outputs from a combination of machine-learning (ML) models, controller logic, and custom modules. Although numerous prior works have shown that adversarial examples can mislead ML models used in autonomous driving contexts, it remains unclear if these attacks are effective at producing harmful driving actions for various agents, environments, and scenarios.
  To assess the risk of adversarial examples to autonomous driving, we evaluate attacks against a variety of driving agents, rather than against ML models in isolation. To support this evaluation, we leverage CARLA, an urban driving simulator, to create and evaluate adversarial examples. We create adversarial patches designed to stop or steer driving agents, stream them into the CARLA simulator at runtime, and evaluate them against agents from the CARLA Leaderboard, a public repository of best-performing autonomous driving agents from an annual research competition. Unlike prior work, we evaluate attacks against autonomous driving systems without creating or modifying any driving-agent code and against all parts of the agent included with the ML model.
  We perform a case-study investigation of two attack strategies against three open-source driving agents from the CARLA Leaderboard across multiple driving scenarios, lighting conditions, and locations. Interestingly, we show that, although some attacks can successfully mislead ML models into predicting erroneous stopping or steering commands, some driving agents use modules, such as PID control or GPS-based rules, that can overrule attacker-manipulated predictions from ML models.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exact Learning of Weighted Graphs Using Composite Queries</title>
<link>https://arxiv.org/abs/2511.14882</link>
<guid>https://arxiv.org/abs/2511.14882</guid>
<content:encoded><![CDATA[
arXiv:2511.14882v1 Announce Type: cross 
Abstract: In this paper, we study the exact learning problem for weighted graphs, where we are given the vertex set, $V$, of a weighted graph, $G=(V,E,w)$, but we are not given $E$. The problem, which is also known as graph reconstruction, is to determine all the edges of $E$, including their weights, by asking queries about $G$ from an oracle. As we observe, using simple shortest-path length queries is not sufficient, in general, to learn a weighted graph. So we study a number of scenarios where it is possible to learn $G$ using a subquadratic number of composite queries, which combine two or three simple queries.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation</title>
<link>https://arxiv.org/abs/2511.14897</link>
<guid>https://arxiv.org/abs/2511.14897</guid>
<content:encoded><![CDATA[
arXiv:2511.14897v1 Announce Type: cross 
Abstract: We present an unsupervised single image bidirectional Magnetic Resonance Image (MRI) synthesizer that synthesizes an Ultra-Low Field (ULF) like image from a High-Field (HF) magnitude image and vice-versa. Unlike existing MRI synthesis models, our approach is inspired by the physics that drives contrast changes between HF and ULF MRIs. Our forward model simulates a HF to ULF transformation by estimating the tissue-type Signal-to-Noise ratio (SNR) values based on target contrast values. For the Super-Resolution task, we used an Implicit Neural Representation (INR) network to synthesize HF image by simultaneously predicting tissue-type segmentations and image intensity without observed HF data. The proposed method is evaluated using synthetic ULF-like data from generated from standard 3T T$_1$-weighted images for qualitative assessments and paired 3T-64mT T$_1$-weighted images for validation experiments. WM-GM contrast improved by 52% in synthetic ULF-like images and 37% in 64mT images. Sensitivity experiments demonstrated the robustness of our forward model to variations in target contrast, noise and initial seeding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Premise SLMs vs. Commercial LLMs: Prompt Engineering and Incident Classification in SOCs and CSIRTs</title>
<link>https://arxiv.org/abs/2511.14908</link>
<guid>https://arxiv.org/abs/2511.14908</guid>
<content:encoded><![CDATA[
arXiv:2511.14908v1 Announce Type: cross 
Abstract: In this study, we evaluate open-source models for security incident classification, comparing them with proprietary models. We utilize a dataset of anonymized real incidents, categorized according to the NIST SP 800-61r3 taxonomy and processed using five prompt-engineering techniques (PHP, SHP, HTP, PRP, and ZSL). The results indicate that, although proprietary models still exhibit higher accuracy, locally deployed open-source models provide advantages in privacy, cost-effectiveness, and data sovereignty.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-tuning Pre-trained Audio Models for COVID-19 Detection: A Technical Report</title>
<link>https://arxiv.org/abs/2511.14939</link>
<guid>https://arxiv.org/abs/2511.14939</guid>
<content:encoded><![CDATA[
arXiv:2511.14939v1 Announce Type: cross 
Abstract: This technical report investigates the performance of pre-trained audio models on COVID-19 detection tasks using established benchmark datasets. We fine-tuned Audio-MAE and three PANN architectures (CNN6, CNN10, CNN14) on the Coswara and COUGHVID datasets, evaluating both intra-dataset and cross-dataset generalization. We implemented a strict demographic stratification by age and gender to prevent models from exploiting spurious correlations between demographic characteristics and COVID-19 status. Intra-dataset results showed moderate performance, with Audio-MAE achieving the strongest result on Coswara (0.82 AUC, 0.76 F1-score), while all models demonstrated limited performance on Coughvid (AUC 0.58-0.63). Cross-dataset evaluation revealed severe generalization failure across all models (AUC 0.43-0.68), with Audio-MAE showing strong performance degradation (F1-score 0.00-0.08). Our experiments demonstrate that demographic balancing, while reducing apparent model performance, provides more realistic assessment of COVID-19 detection capabilities by eliminating demographic leakage - a confounding factor that inflate performance metrics. Additionally, the limited dataset sizes after balancing (1,219-2,160 samples) proved insufficient for deep learning models that typically require substantially larger training sets. These findings highlight fundamental challenges in developing generalizable audio-based COVID-19 detection systems and underscore the importance of rigorous demographic controls for clinically robust model evaluation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial intelligence approaches for energy-efficient laser cutting machines</title>
<link>https://arxiv.org/abs/2511.14952</link>
<guid>https://arxiv.org/abs/2511.14952</guid>
<content:encoded><![CDATA[
arXiv:2511.14952v1 Announce Type: cross 
Abstract: This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compiling to recurrent neurons</title>
<link>https://arxiv.org/abs/2511.14953</link>
<guid>https://arxiv.org/abs/2511.14953</guid>
<content:encoded><![CDATA[
arXiv:2511.14953v1 Announce Type: cross 
Abstract: Discrete structures are currently second-class in differentiable programming. Since functions over discrete structures lack overt derivatives, differentiable programs do not differentiate through them and limit where they can be used. For example, when programming a neural network, conditionals and iteration cannot be used everywhere; they can break the derivatives necessary for gradient-based learning to work. This limits the class of differentiable algorithms we can directly express, imposing restraints on how we build neural networks and differentiable programs more generally. However, these restraints are not fundamental. Recent work shows conditionals can be first-class, by compiling them into differentiable form as linear neurons. Similarly, this work shows iteration can be first-class -- by compiling to linear recurrent neurons. We present a minimal typed, higher-order and linear programming language with iteration called $\textsf{Cajal}\scriptstyle(\mathbb{\multimap}, \mathbb{2}, \mathbb{N})$. We prove its programs compile correctly to recurrent neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation, we conduct two experiments where we link these recurrent neurons against a neural network solving an iterative image transformation task. This determines part of its function prior to learning. As a result, the network learns faster and with greater data-efficiency relative to a neural network programmed without first-class iteration. A key lesson is that recurrent neurons enable a rich interplay between learning and the discrete structures of ordinary programming.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstruction of three-dimensional shapes of normal and disease-related erythrocytes from partial observations using multi-fidelity neural networks</title>
<link>https://arxiv.org/abs/2511.14962</link>
<guid>https://arxiv.org/abs/2511.14962</guid>
<content:encoded><![CDATA[
arXiv:2511.14962v1 Announce Type: cross 
Abstract: Reconstruction of 3D erythrocyte or red blood cell (RBC) morphology from partial observations, such as microscope images, is essential for understanding the physiology of RBC aging and the pathology of various RBC disorders. In this study, we propose a multi-fidelity neural network (MFNN) approach to fuse high-fidelity cross-sections of an RBC, with a morphologically similar low-fidelity reference 3D RBC shape to recover its full 3D surface. The MFNN predictor combines a convolutional neural network trained on low-fidelity reference RBC data with a feedforward neural network that captures nonlinear morphological correlations, and augments training with surface area and volume constraints for regularization in the low-fidelity branch. This approach is theoretically grounded by a topological homeomorphism between a sphere and 3D RBC surfaces, with training data generated by dissipative particle dynamics simulations of stomatocyte-discocyte-echinocyte transformation. Benchmarking across diverse RBC shapes observed in normal and aged populations, our results show that the MFNN predictor can reconstruct complex RBC morphologies with over 95% coordinate accuracy when provided with at least two orthogonal cross-sections. It is observed that informative oblique cross-sections intersecting spicule tips of echinocytes improve both local and global feature reconstruction, highlighting the value of feature-aware sampling. Our study further evaluates the influence of sampling strategies, shape dissimilarity, and noise, showing enhanced robustness under physically constrained training. Altogether, these results demonstrate the capability of MFNN to reconstruct the 3D shape of normal and aged RBCs from partial cross-sections as observed in conventional microscope images, which could facilitate the quantitative analysis of RBC morphological parameters in normal and disease-related RBC samples.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation</title>
<link>https://arxiv.org/abs/2511.14967</link>
<guid>https://arxiv.org/abs/2511.14967</guid>
<content:encoded><![CDATA[
arXiv:2511.14967v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based Transfer Learning and MAMBA Fusion</title>
<link>https://arxiv.org/abs/2511.14969</link>
<guid>https://arxiv.org/abs/2511.14969</guid>
<content:encoded><![CDATA[
arXiv:2511.14969v1 Announce Type: cross 
Abstract: This paper addresses data quality issues in multimodal emotion recognition in conversation (MERC) through systematic quality control and multi-stage transfer learning. We implement a quality control pipeline for MELD and IEMOCAP datasets that validates speaker identity, audio-text alignment, and face detection. We leverage transfer learning from speaker and face recognition, assuming that identity-discriminative embeddings capture not only stable acoustic and Facial traits but also person-specific patterns of emotional expression. We employ RecoMadeEasy(R) engines for extracting 512-dimensional speaker and face embeddings, fine-tune MPNet-v2 for emotion-aware text representations, and adapt these features through emotion-specific MLPs trained on unimodal datasets. MAMBA-based trimodal fusion achieves 64.8% accuracy on MELD and 74.3% on IEMOCAP. These results show that combining identity-based audio and visual embeddings with emotion-tuned text representations on a quality-controlled subset of data yields consistent competitive performance for multimodal emotion recognition in conversation and provides a basis for further improvement on challenging, low-frequency emotion classes.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Forgetting in Option Calibration: An Operator-Theoretic Gauss-Newton Framework</title>
<link>https://arxiv.org/abs/2511.14980</link>
<guid>https://arxiv.org/abs/2511.14980</guid>
<content:encoded><![CDATA[
arXiv:2511.14980v1 Announce Type: cross 
Abstract: Calibration of option pricing models is routinely repeated as markets evolve, yet modern systems lack an operator for removing data from a calibrated model without full retraining. When quotes become stale, corrupted, or subject to deletion requirements, existing calibration pipelines must rebuild the entire nonlinear least-squares problem, even if only a small subset of data must be excluded. In this work, we introduce a principled framework for selective forgetting (machine unlearning) in parametric option calibration. We provide stability guarantees, perturbation bounds, and show that the proposed operators satisfy local exactness under standard regularity assumptions.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.14981</link>
<guid>https://arxiv.org/abs/2511.14981</guid>
<content:encoded><![CDATA[
arXiv:2511.14981v1 Announce Type: cross 
Abstract: Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation</title>
<link>https://arxiv.org/abs/2511.14993</link>
<guid>https://arxiv.org/abs/2511.14993</guid>
<content:encoded><![CDATA[
arXiv:2511.14993v1 Announce Type: cross 
Abstract: This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.15002</link>
<guid>https://arxiv.org/abs/2511.15002</guid>
<content:encoded><![CDATA[
arXiv:2511.15002v1 Announce Type: cross 
Abstract: Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $\rho$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource-Based Time and Cost Prediction in Project Networks: From Statistical Modeling to Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.15003</link>
<guid>https://arxiv.org/abs/2511.15003</guid>
<content:encoded><![CDATA[
arXiv:2511.15003v1 Announce Type: cross 
Abstract: Accurate prediction of project duration and cost remains one of the most challenging aspects of project management, particularly in resource-constrained and interdependent task networks. Traditional analytical techniques such as the Critical Path Method (CPM) and Program Evaluation and Review Technique (PERT) rely on simplified and often static assumptions regarding task interdependencies and resource performance. This study proposes a novel resource-based predictive framework that integrates network representations of project activities with graph neural networks (GNNs) to capture structural and contextual relationships among tasks, resources, and time-cost dynamics. The model represents the project as a heterogeneous activity-resource graph in which nodes denote activities and resources, and edges encode temporal and resource dependencies.
  We evaluate multiple learning paradigms, including GraphSAGE and Temporal Graph Networks, on both synthetic and benchmark project datasets. Experimental results show that the proposed GNN framework achieves an average 23 to 31 percent reduction in mean absolute error compared to traditional regression and tree-based methods, while improving the coefficient of determination R2 from approximately 0.78 to 0.91 for large and complex project networks. Furthermore, the learned embeddings provide interpretable insights into resource bottlenecks and critical dependencies, enabling more explainable and adaptive scheduling decisions.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent space analysis and generalization to out-of-distribution data</title>
<link>https://arxiv.org/abs/2511.15010</link>
<guid>https://arxiv.org/abs/2511.15010</guid>
<content:encoded><![CDATA[
arXiv:2511.15010v1 Announce Type: cross 
Abstract: Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference</title>
<link>https://arxiv.org/abs/2511.15015</link>
<guid>https://arxiv.org/abs/2511.15015</guid>
<content:encoded><![CDATA[
arXiv:2511.15015v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) models scale LLM capacity efficiently, but deployment on consumer GPUs is limited by the large memory footprint of inactive experts. Static post-training quantization reduces storage costs but cannot adapt to shifting activation patterns, causing accuracy loss under aggressive compression. So we present DynaExq, a runtime system that treats expert precision as a first-class, dynamically managed resource. DynaExq combines (1) a hotness-aware precision controller that continuously aligns expert bit-widths with long-term activation statistics, (2) a fully asynchronous precision-switching pipeline that overlaps promotion and demotion with MoE computation, and (3) a fragmentation-free memory pooling mechanism that supports hybrid-precision experts with deterministic allocation. Together, these components enable stable, non-blocking precision transitions under strict HBM budgets.
  Across Qwen3-30B and Qwen3-80B MoE models and six representative benchmarks, DynaExq deploys large LLMs on single RTX 5090 and A6000 GPUs and improves accuracy by up to 4.03 points over static low-precision baselines. The results show that adaptive, workload-aware quantization is an effective strategy for memory-constrained MoE serving.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex-Valued 2D Gaussian Representation for Computer-Generated Holography</title>
<link>https://arxiv.org/abs/2511.15022</link>
<guid>https://arxiv.org/abs/2511.15022</guid>
<content:encoded><![CDATA[
arXiv:2511.15022v1 Announce Type: cross 
Abstract: We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization</title>
<link>https://arxiv.org/abs/2511.15055</link>
<guid>https://arxiv.org/abs/2511.15055</guid>
<content:encoded><![CDATA[
arXiv:2511.15055v1 Announce Type: cross 
Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering</title>
<link>https://arxiv.org/abs/2511.15061</link>
<guid>https://arxiv.org/abs/2511.15061</guid>
<content:encoded><![CDATA[
arXiv:2511.15061v1 Announce Type: cross 
Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPU-Initiated Networking for NCCL</title>
<link>https://arxiv.org/abs/2511.15076</link>
<guid>https://arxiv.org/abs/2511.15076</guid>
<content:encoded><![CDATA[
arXiv:2511.15076v1 Announce Type: cross 
Abstract: Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.
  NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit</title>
<link>https://arxiv.org/abs/2511.15120</link>
<guid>https://arxiv.org/abs/2511.15120</guid>
<content:encoded><![CDATA[
arXiv:2511.15120v1 Announce Type: cross 
Abstract: In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\boldsymbol{x})=g(\boldsymbol{U}\boldsymbol{x})$ with hidden subspace $\boldsymbol{U}\in \mathbb{R}^{r\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\widetilde{\mathcal{O}}(d)$ samples and $\widetilde{\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images</title>
<link>https://arxiv.org/abs/2511.15132</link>
<guid>https://arxiv.org/abs/2511.15132</guid>
<content:encoded><![CDATA[
arXiv:2511.15132v1 Announce Type: cross 
Abstract: Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CASPER: Cross-modal Alignment of Spatial and single-cell Profiles for Expression Recovery</title>
<link>https://arxiv.org/abs/2511.15139</link>
<guid>https://arxiv.org/abs/2511.15139</guid>
<content:encoded><![CDATA[
arXiv:2511.15139v1 Announce Type: cross 
Abstract: Spatial Transcriptomics enables mapping of gene expression within its native tissue context, but current platforms measure only a limited set of genes due to experimental constraints and excessive costs. To overcome this, computational models integrate Single-Cell RNA Sequencing data with Spatial Transcriptomics to predict unmeasured genes. We propose CASPER, a cross-attention based framework that predicts unmeasured gene expression in Spatial Transcriptomics by leveraging centroid-level representations from Single-Cell RNA Sequencing. We performed rigorous testing over four state-of-the-art Spatial Transcriptomics/Single-Cell RNA Sequencing dataset pairs across four existing baseline models. CASPER shows significant improvement in nine out of the twelve metrics for our experiments. This work paves the way for further work in Spatial Transcriptomics to Single-Cell RNA Sequencing modality translation. The code for CASPER is available at https://github.com/AI4Med-Lab/CASPER.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings</title>
<link>https://arxiv.org/abs/2511.15146</link>
<guid>https://arxiv.org/abs/2511.15146</guid>
<content:encoded><![CDATA[
arXiv:2511.15146v1 Announce Type: cross 
Abstract: Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging</title>
<link>https://arxiv.org/abs/2511.15151</link>
<guid>https://arxiv.org/abs/2511.15151</guid>
<content:encoded><![CDATA[
arXiv:2511.15151v1 Announce Type: cross 
Abstract: High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation</title>
<link>https://arxiv.org/abs/2511.15159</link>
<guid>https://arxiv.org/abs/2511.15159</guid>
<content:encoded><![CDATA[
arXiv:2511.15159v1 Announce Type: cross 
Abstract: High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Wireless Foundation Models</title>
<link>https://arxiv.org/abs/2511.15162</link>
<guid>https://arxiv.org/abs/2511.15162</guid>
<content:encoded><![CDATA[
arXiv:2511.15162v1 Announce Type: cross 
Abstract: Wireless foundation models (WFMs) have recently demonstrated promising capabilities, jointly performing multiple wireless functions and adapting effectively to new environments. However, while current WFMs process only one modality, depending on the task and operating conditions, the most informative modality changes and no single modality is best for all tasks. WFMs should therefore be designed to accept multiple modalities to enable a broader and more diverse range of tasks and scenarios. In this work, we propose and build the first multimodal wireless foundation model capable of processing both raw IQ streams and image-like wireless modalities (e.g., spectrograms and CSI) and performing multiple tasks across both. We introduce masked wireless modeling for the multimodal setting, a self-supervised objective and pretraining recipe that learns a joint representation from IQ streams and image-like wireless modalities. We evaluate the model on five tasks across both modality families: image-based (human activity sensing, RF signal classification, 5G NR positioning) and IQ-based (RF device fingerprinting, interference detection/classification). The multimodal WFM is competitive with single-modality WFMs, and in several cases surpasses their performance. Our results demonstrates the strong potential of developing multimodal WFMs that support diverse wireless tasks across different modalities. We believe this provides a concrete step toward both AI-native 6G and the vision of joint sensing, communication, and localization.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs</title>
<link>https://arxiv.org/abs/2511.15163</link>
<guid>https://arxiv.org/abs/2511.15163</guid>
<content:encoded><![CDATA[
arXiv:2511.15163v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven Prediction of Species-Specific Plant Responses to Spectral-Shifting Films from Leaf Phenotypic and Photosynthetic Traits</title>
<link>https://arxiv.org/abs/2511.15173</link>
<guid>https://arxiv.org/abs/2511.15173</guid>
<content:encoded><![CDATA[
arXiv:2511.15173v1 Announce Type: cross 
Abstract: The application of spectral-shifting films in greenhouses to shift green light to red light has shown variable growth responses across crop species. However, the yield enhancement of crops under altered light quality is related to the collective effects of the specific biophysical characteristics of each species. Considering only one attribute of a crop has limitations in understanding the relationship between sunlight quality adjustments and crop growth performance. Therefore, this study aims to comprehensively link multiple plant phenotypic traits and daily light integral considering the physiological responses of crops to their growth outcomes under SF using artificial intelligence. Between 2021 and 2024, various leafy, fruiting, and root crops were grown in greenhouses covered with either PEF or SF, and leaf reflectance, leaf mass per area, chlorophyll content, daily light integral, and light saturation point were measured from the plants cultivated in each condition. 210 data points were collected, but there was insufficient data to train deep learning models, so a variational autoencoder was used for data augmentation. Most crop yields showed an average increase of 22.5% under SF. These data were used to train several models, including logistic regression, decision tree, random forest, XGBoost, and feedforward neural network (FFNN), aiming to binary classify whether there was a significant effect on yield with SF application. The FFNN achieved a high classification accuracy of 91.4% on a test dataset that was not used for training. This study provide insight into the complex interactions between leaf phenotypic and photosynthetic traits, environmental conditions, and solar spectral components by improving the ability to predict solar spectral shift effects using SF.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples</title>
<link>https://arxiv.org/abs/2511.15183</link>
<guid>https://arxiv.org/abs/2511.15183</guid>
<content:encoded><![CDATA[
arXiv:2511.15183v1 Announce Type: cross 
Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI</title>
<link>https://arxiv.org/abs/2511.15188</link>
<guid>https://arxiv.org/abs/2511.15188</guid>
<content:encoded><![CDATA[
arXiv:2511.15188v1 Announce Type: cross 
Abstract: Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $\rho=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Particle Monte Carlo methods for Lattice Field Theory</title>
<link>https://arxiv.org/abs/2511.15196</link>
<guid>https://arxiv.org/abs/2511.15196</guid>
<content:encoded><![CDATA[
arXiv:2511.15196v1 Announce Type: cross 
Abstract: High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Where, What and How to Transfer: A Multi-Role Reinforcement Learning Approach for Evolutionary Multitasking</title>
<link>https://arxiv.org/abs/2511.15199</link>
<guid>https://arxiv.org/abs/2511.15199</guid>
<content:encoded><![CDATA[
arXiv:2511.15199v1 Announce Type: cross 
Abstract: Evolutionary multitasking (EMT) algorithms typically require tailored designs for knowledge transfer, in order to assure convergence and optimality in multitask optimization. In this paper, we explore designing a systematic and generalizable knowledge transfer policy through Reinforcement Learning. We first identify three major challenges: determining the task to transfer (where), the knowledge to be transferred (what) and the mechanism for the transfer (how). To address these challenges, we formulate a multi-role RL system where three (groups of) policy networks act as specialized agents: a task routing agent incorporates an attention-based similarity recognition module to determine source-target transfer pairs via attention scores; a knowledge control agent determines the proportion of elite solutions to transfer; and a group of strategy adaptation agents control transfer strength by dynamically controlling hyper-parameters in the underlying EMT framework. Through pre-training all network modules end-to-end over an augmented multitask problem distribution, a generalizable meta-policy is obtained. Comprehensive validation experiments show state-of-the-art performance of our method against representative baselines. Further in-depth analysis not only reveals the rationale behind our proposal but also provide insightful interpretations on what the system have learned.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story</title>
<link>https://arxiv.org/abs/2511.15210</link>
<guid>https://arxiv.org/abs/2511.15210</guid>
<content:encoded><![CDATA[
arXiv:2511.15210v1 Announce Type: cross 
Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Physics Still Matters: Improving Machine Learning Prediction of Material Properties with Phonon-Informed Datasets</title>
<link>https://arxiv.org/abs/2511.15222</link>
<guid>https://arxiv.org/abs/2511.15222</guid>
<content:encoded><![CDATA[
arXiv:2511.15222v1 Announce Type: cross 
Abstract: Machine learning (ML) methods have become powerful tools for predicting material properties with near first-principles accuracy and vastly reduced computational cost. However, the performance of ML models critically depends on the quality, size, and diversity of the training dataset. In materials science, this dependence is particularly important for learning from low-symmetry atomistic configurations that capture thermal excitations, structural defects, and chemical disorder, features that are ubiquitous in real materials but underrepresented in most datasets. The absence of systematic strategies for generating representative training data may therefore limit the predictive power of ML models in technologically critical fields such as energy conversion and photonics. In this work, we assess the effectiveness of graph neural network (GNN) models trained on two fundamentally different types of datasets: one composed of randomly generated atomic configurations and another constructed using physically informed sampling based on lattice vibrations. As a case study, we address the challenging task of predicting electronic and mechanical properties of a prototypical family of optoelectronic materials under realistic finite-temperature conditions. We find that the phonons-informed model consistently outperforms the randomly trained counterpart, despite relying on fewer data points. Explainability analyses further reveal that high-performing models assign greater weight to chemically meaningful bonds that control property variations, underscoring the importance of physically guided data generation. Overall, this work demonstrates that larger datasets do not necessarily yield better GNN predictive models and introduces a simple and general strategy for efficiently constructing high-quality training data in materials informatics.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning in Queue-Reactive Models: Application to Optimal Execution</title>
<link>https://arxiv.org/abs/2511.15262</link>
<guid>https://arxiv.org/abs/2511.15262</guid>
<content:encoded><![CDATA[
arXiv:2511.15262v1 Announce Type: cross 
Abstract: We investigate the use of Reinforcement Learning for the optimal execution of meta-orders, where the objective is to execute incrementally large orders while minimizing implementation shortfall and market impact over an extended period of time. Departing from traditional parametric approaches to price dynamics and impact modeling, we adopt a model-free, data-driven framework. Since policy optimization requires counterfactual feedback that historical data cannot provide, we employ the Queue-Reactive Model to generate realistic and tractable limit order book simulations that encompass transient price impact, and nonlinear and dynamic order flow responses. Methodologically, we train a Double Deep Q-Network agent on a state space comprising time, inventory, price, and depth variables, and evaluate its performance against established benchmarks. Numerical simulation results show that the agent learns a policy that is both strategic and tactical, adapting effectively to order book conditions and outperforming standard approaches across multiple training configurations. These findings provide strong evidence that model-free Reinforcement Learning can yield adaptive and robust solutions to the optimal execution problem.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Query Networks for Object Detection with Automotive Radar</title>
<link>https://arxiv.org/abs/2511.15271</link>
<guid>https://arxiv.org/abs/2511.15271</guid>
<content:encoded><![CDATA[
arXiv:2511.15271v1 Announce Type: cross 
Abstract: Object detection with 3D radar is essential for 360-degree automotive perception, but radar's long wavelengths produce sparse and irregular reflections that challenge traditional grid and sequence-based convolutional and transformer detectors. This paper introduces Graph Query Networks (GQN), an attention-based framework that models objects sensed by radar as graphs, to extract individualized relational and contextual features. GQN employs a novel concept of graph queries to dynamically attend over the bird's-eye view (BEV) space, constructing object-specific graphs processed by two novel modules: EdgeFocus for relational reasoning and DeepContext Pooling for contextual aggregation. On the NuScenes dataset, GQN improves relative mAP by up to +53%, including a +8.2% gain over the strongest prior radar method, while reducing peak graph construction overhead by 80% with moderate FLOPs cost.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Bayesian Optimisation with Unbounded Corruptions</title>
<link>https://arxiv.org/abs/2511.15315</link>
<guid>https://arxiv.org/abs/2511.15315</guid>
<content:encoded><![CDATA[
arXiv:2511.15315v1 Announce Type: cross 
Abstract: Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB's regret bounds match those of the standard GP-UCB algorithm.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss</title>
<link>https://arxiv.org/abs/2511.15332</link>
<guid>https://arxiv.org/abs/2511.15332</guid>
<content:encoded><![CDATA[
arXiv:2511.15332v1 Announce Type: cross 
Abstract: In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.
  Our method is implemented in the \texttt{R} package \texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection</title>
<link>https://arxiv.org/abs/2511.15343</link>
<guid>https://arxiv.org/abs/2511.15343</guid>
<content:encoded><![CDATA[
arXiv:2511.15343v1 Announce Type: cross 
Abstract: Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controlling False Positives in Image Segmentation via Conformal Prediction</title>
<link>https://arxiv.org/abs/2511.15406</link>
<guid>https://arxiv.org/abs/2511.15406</guid>
<content:encoded><![CDATA[
arXiv:2511.15406v1 Announce Type: cross 
Abstract: Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models</title>
<link>https://arxiv.org/abs/2511.15411</link>
<guid>https://arxiv.org/abs/2511.15411</guid>
<content:encoded><![CDATA[
arXiv:2511.15411v1 Announce Type: cross 
Abstract: Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural network-driven domain decomposition for efficient solutions to the Helmholtz equation</title>
<link>https://arxiv.org/abs/2511.15445</link>
<guid>https://arxiv.org/abs/2511.15445</guid>
<content:encoded><![CDATA[
arXiv:2511.15445v1 Announce Type: cross 
Abstract: Accurately simulating wave propagation is crucial in fields such as acoustics, electromagnetism, and seismic analysis. Traditional numerical methods, like finite difference and finite element approaches, are widely used to solve governing partial differential equations (PDEs) such as the Helmholtz equation. However, these methods face significant computational challenges when applied to high-frequency wave problems in complex two-dimensional domains. This work investigates Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions as a promising alternative. These methods leverage domain decomposition, partitioning the computational domain into overlapping sub-domains, each governed by a local neural network. We assess their accuracy and computational efficiency in solving the Helmholtz equation for the homogeneous case, demonstrating their potential to mitigate the limitations of traditional approaches.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gini Score under Ties and Case Weights</title>
<link>https://arxiv.org/abs/2511.15446</link>
<guid>https://arxiv.org/abs/2511.15446</guid>
<content:encoded><![CDATA[
arXiv:2511.15446v1 Announce Type: cross 
Abstract: The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome</title>
<link>https://arxiv.org/abs/2511.15464</link>
<guid>https://arxiv.org/abs/2511.15464</guid>
<content:encoded><![CDATA[
arXiv:2511.15464v1 Announce Type: cross 
Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection</title>
<link>https://arxiv.org/abs/2511.15476</link>
<guid>https://arxiv.org/abs/2511.15476</guid>
<content:encoded><![CDATA[
arXiv:2511.15476v1 Announce Type: cross 
Abstract: This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tensor Compiler for Processing-In-Memory Architectures</title>
<link>https://arxiv.org/abs/2511.15503</link>
<guid>https://arxiv.org/abs/2511.15503</guid>
<content:encoded><![CDATA[
arXiv:2511.15503v1 Announce Type: cross 
Abstract: Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Gaussian Process Classification and an Application in Subsea Robotics</title>
<link>https://arxiv.org/abs/2511.15529</link>
<guid>https://arxiv.org/abs/2511.15529</guid>
<content:encoded><![CDATA[
arXiv:2511.15529v1 Announce Type: cross 
Abstract: Teams of cooperating autonomous underwater vehicles (AUVs) rely on acoustic communication for coordination, yet this communication medium is constrained by limited range, multi-path effects, and low bandwidth. One way to address the uncertainty associated with acoustic communication is to learn the communication environment in real-time. We address the challenge of a team of robots building a map of the probability of communication success from one location to another in real-time. This is a decentralized classification problem -- communication events are either successful or unsuccessful -- where AUVs share a subset of their communication measurements to build the map. The main contribution of this work is a rigorously derived data sharing policy that selects measurements to be shared among AUVs. We experimentally validate our proposed sharing policy using real acoustic communication data collected from teams of Virginia Tech 690 AUVs, demonstrating its effectiveness in underwater environments.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence and Sketching-Based Efficient Computation of Neural Tangent Kernel Weights in Physics-Based Loss</title>
<link>https://arxiv.org/abs/2511.15530</link>
<guid>https://arxiv.org/abs/2511.15530</guid>
<content:encoded><![CDATA[
arXiv:2511.15530v1 Announce Type: cross 
Abstract: In multi-objective optimization, multiple loss terms are weighted and added together to form a single objective. These weights are chosen to properly balance the competing losses according to some meta-goal. For example, in physics-informed neural networks (PINNs), these weights are often adaptively chosen to improve the network's generalization error. A popular choice of adaptive weights is based on the neural tangent kernel (NTK) of the PINN, which describes the evolution of the network in predictor space during training. The convergence of such an adaptive weighting algorithm is not clear a priori. Moreover, these NTK-based weights would be updated frequently during training, further increasing the computational burden of the learning process. In this paper, we prove that under appropriate conditions, gradient descent enhanced with adaptive NTK-based weights is convergent in a suitable sense. We then address the problem of computational efficiency by developing a randomized algorithm inspired by a predictor-corrector approach and matrix sketching, which produces unbiased estimates of the NTK up to an arbitrarily small discretization error. Finally, we provide numerical experiments to support our theoretical findings and to show the efficacy of our randomized algorithm. Code Availability: https://github.com/maxhirsch/Efficient-NTK
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation</title>
<link>https://arxiv.org/abs/2511.15543</link>
<guid>https://arxiv.org/abs/2511.15543</guid>
<content:encoded><![CDATA[
arXiv:2511.15543v1 Announce Type: cross 
Abstract: Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery</title>
<link>https://arxiv.org/abs/2511.15600</link>
<guid>https://arxiv.org/abs/2511.15600</guid>
<content:encoded><![CDATA[
arXiv:2511.15600v1 Announce Type: cross 
Abstract: Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-optimal delta-convex estimation of Lipschitz functions</title>
<link>https://arxiv.org/abs/2511.15615</link>
<guid>https://arxiv.org/abs/2511.15615</guid>
<content:encoded><![CDATA[
arXiv:2511.15615v1 Announce Type: cross 
Abstract: This paper presents a tractable algorithm for estimating an unknown Lipschitz function from noisy observations and establishes an upper bound on its convergence rate. The approach extends max-affine methods from convex shape-restricted regression to the more general Lipschitz setting. A key component is a nonlinear feature expansion that maps max-affine functions into a subclass of delta-convex functions, which act as universal approximators of Lipschitz functions while preserving their Lipschitz constants. Leveraging this property, the estimator attains the minimax convergence rate (up to logarithmic factors) with respect to the intrinsic dimension of the data under squared loss and subgaussian distributions in the random design setting. The algorithm integrates adaptive partitioning to capture intrinsic dimension, a penalty-based regularization mechanism that removes the need to know the true Lipschitz constant, and a two-stage optimization procedure combining a convex initialization with local refinement. The framework is also straightforward to adapt to convex shape-restricted regression. Experiments demonstrate competitive performance relative to other theoretically justified methods, including nearest-neighbor and kernel-based regressors.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CODE-II: A large-scale dataset for artificial intelligence in ECG analysis</title>
<link>https://arxiv.org/abs/2511.15632</link>
<guid>https://arxiv.org/abs/2511.15632</guid>
<content:encoded><![CDATA[
arXiv:2511.15632v1 Announce Type: cross 
Abstract: Data-driven methods for electrocardiogram (ECG) interpretation are rapidly progressing. Large datasets have enabled advances in artificial intelligence (AI) based ECG analysis, yet limitations in annotation quality, size, and scope remain major challenges. Here we present CODE-II, a large-scale real-world dataset of 2,735,269 12-lead ECGs from 2,093,807 adult patients collected by the Telehealth Network of Minas Gerais (TNMG), Brazil. Each exam was annotated using standardized diagnostic criteria and reviewed by cardiologists. A defining feature of CODE-II is a set of 66 clinically meaningful diagnostic classes, developed with cardiologist input and routinely used in telehealth practice. We additionally provide an open available subset: CODE-II-open, a public subset of 15,000 patients, and the CODE-II-test, a non-overlapping set of 8,475 exams reviewed by multiple cardiologists for blinded evaluation. A neural network pre-trained on CODE-II achieved superior transfer performance on external benchmarks (PTB-XL and CPSC 2018) and outperformed alternatives trained on larger datasets.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2511.15633</link>
<guid>https://arxiv.org/abs/2511.15633</guid>
<content:encoded><![CDATA[
arXiv:2511.15633v1 Announce Type: cross 
Abstract: Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like "dog" subsumes fine-grained categories such as "Labrador" and "Golden Retriever," and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R\'enyi Differential Privacy for Heavy-Tailed SDEs via Fractional Poincar\'e Inequalities</title>
<link>https://arxiv.org/abs/2511.15634</link>
<guid>https://arxiv.org/abs/2511.15634</guid>
<content:encoded><![CDATA[
arXiv:2511.15634v1 Announce Type: cross 
Abstract: Characterizing the differential privacy (DP) of learning algorithms has become a major challenge in recent years. In parallel, many studies suggested investigating the behavior of stochastic gradient descent (SGD) with heavy-tailed noise, both as a model for modern deep learning models and to improve their performance. However, most DP bounds focus on light-tailed noise, where satisfactory guarantees have been obtained but the proposed techniques do not directly extend to the heavy-tailed setting. Recently, the first DP guarantees for heavy-tailed SGD were obtained. These results provide $(0,\delta)$-DP guarantees without requiring gradient clipping. Despite casting new light on the link between DP and heavy-tailed algorithms, these results have a strong dependence on the number of parameters and cannot be extended to other DP notions like the well-established R\'enyi differential privacy (RDP). In this work, we propose to address these limitations by deriving the first RDP guarantees for heavy-tailed SDEs, as well as their discretized counterparts. Our framework is based on new R\'enyi flow computations and the use of well-established fractional Poincar\'e inequalities. Under the assumption that such inequalities are satisfied, we obtain DP guarantees that have a much weaker dependence on the dimension compared to prior art.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisPlay: Self-Evolving Vision-Language Models from Images</title>
<link>https://arxiv.org/abs/2511.15661</link>
<guid>https://arxiv.org/abs/2511.15661</guid>
<content:encoded><![CDATA[
arXiv:2511.15661v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Front-door Reducibility: Reducing ADMGs to the Standard Front-door Setting via a Graphical Criterion</title>
<link>https://arxiv.org/abs/2511.15679</link>
<guid>https://arxiv.org/abs/2511.15679</guid>
<content:encoded><![CDATA[
arXiv:2511.15679v1 Announce Type: cross 
Abstract: Front-door adjustment provides a simple closed-form identification formula under the classical front-door criterion, but its applicability is often viewed as narrow and strict. Although ID algorithm is very useful and is proved effective for causal relation identification in general causal graphs (if it is identifiable), performing ID algorithm does not guarantee to obtain a practical, easy-to-estimate interventional distribution expression. We argue that the applicability of the front-door criterion is not as limited as it seems: many more complicated causal graphs can be reduced to the front-door criterion. In this paper, We introduce front-door reducibility (FDR), a graphical condition on acyclic directed mixed graphs (ADMGs) that extends the applicability of the classic front-door criterion to reduce a large family of complicated causal graphs to a front-door setting by aggregating variables into super-nodes (FDR triple) $\left(\boldsymbol{X}^{*},\boldsymbol{Y}^{*},\boldsymbol{M}^{*}\right)$. After characterizing FDR criterion, we prove a graph-level equivalence between the satisfication of FDR criterion and the applicability of FDR adjustment. Meanwhile, we then present FDR-TID, an exact algorithm that detects an admissible FDR triple, together with established the algorithm's correctness, completeness, and finite termination. Empirically-motivated examples illustrate that many graphs outside the textbook front-door setting are FDR, yielding simple, estimable adjustments where general ID expressions would be cumbersome. FDR thus complements existing identification method by prioritizing interpretability and computational simplicity without sacrificing generality across mixed graphs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RescueLens: LLM-Powered Triage and Action on Volunteer Feedback for Food Rescue</title>
<link>https://arxiv.org/abs/2511.15698</link>
<guid>https://arxiv.org/abs/2511.15698</guid>
<content:encoded><![CDATA[
arXiv:2511.15698v1 Announce Type: cross 
Abstract: Food rescue organizations simultaneously tackle food insecurity and waste by working with volunteers to redistribute food from donors who have excess to recipients who need it. Volunteer feedback allows food rescue organizations to identify issues early and ensure volunteer satisfaction. However, food rescue organizations monitor feedback manually, which can be cumbersome and labor-intensive, making it difficult to prioritize which issues are most important. In this work, we investigate how large language models (LLMs) assist food rescue organizers in understanding and taking action based on volunteer experiences. We work with 412 Food Rescue, a large food rescue organization based in Pittsburgh, Pennsylvania, to design RescueLens, an LLM-powered tool that automatically categorizes volunteer feedback, suggests donors and recipients to follow up with, and updates volunteer directions based on feedback. We evaluate the performance of RescueLens on an annotated dataset, and show that it can recover 96% of volunteer issues at 71% precision. Moreover, by ranking donors and recipients according to their rates of volunteer issues, RescueLens allows organizers to focus on 0.5% of donors responsible for more than 30% of volunteer issues. RescueLens is now deployed at 412 Food Rescue and through semi-structured interviews with organizers, we find that RescueLens streamlines the feedback process so organizers better allocate their time.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tokenisation over Bounded Alphabets is Hard</title>
<link>https://arxiv.org/abs/2511.15709</link>
<guid>https://arxiv.org/abs/2511.15709</guid>
<content:encoded><![CDATA[
arXiv:2511.15709v1 Announce Type: cross 
Abstract: Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource-Constrained Decentralized Federated Learning via Personalized Event-Triggering</title>
<link>https://arxiv.org/abs/2211.12640</link>
<guid>https://arxiv.org/abs/2211.12640</guid>
<content:encoded><![CDATA[
arXiv:2211.12640v2 Announce Type: replace 
Abstract: Federated learning (FL) is a popular technique for distributing machine learning (ML) across a set of edge devices. In this paper, we study fully decentralized FL, where in addition to devices conducting training locally, they carry out model aggregations via cooperative consensus formation over device-to-device (D2D) networks. We introduce asynchronous, event-triggered communications among the devices to handle settings where access to a central server is not feasible. To account for the inherent resource heterogeneity and statistical diversity challenges in FL, we define personalized communication triggering conditions at each device that weigh the change in local model parameters against the available local network resources. We theoretically recover the $O(\ln{k} / \sqrt{k})$ convergence rate to the globally optimal model of decentralized gradient descent (DGD) methods in the setup of our methodology. We provide our convergence guarantees for the last iterates of models, under relaxed graph connectivity and data heterogeneity assumptions compared with the existing literature. To do so, we demonstrate a $B$-connected information flow guarantee in the presence of sporadic communications over the time-varying D2D graph. Our subsequent numerical evaluations demonstrate that our methodology obtains substantial improvements in convergence speed and/or communication savings compared to existing decentralized FL baselines.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Control of Nonlinear Systems with Unknown Dynamics</title>
<link>https://arxiv.org/abs/2305.15188</link>
<guid>https://arxiv.org/abs/2305.15188</guid>
<content:encoded><![CDATA[
arXiv:2305.15188v3 Announce Type: replace 
Abstract: This paper presents a data-driven method to find a closed-loop optimal controller, which minimizes a specified infinite-horizon cost function for systems with unknown dynamics. Suppose the closed-loop optimal controller can be parameterized by a given class of functions, hereafter referred to as the policy. The proposed method introduces a novel gradient estimation framework, which approximates the gradient of the cost function with respect to the policy parameters via integrating the Koopman operator with the classical concept of actor-critic. This enables the policy parameters to be tuned iteratively using gradient descent to achieve an optimal controller, leveraging the linearity of the Koopman operator. The convergence analysis of the proposed framework is provided. The control performance of the proposed method is evaluated through simulations compared with classical optimal control methods that usually assume the dynamics are known.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Out-of-Distribution Objects through Class-Conditioned Inpainting</title>
<link>https://arxiv.org/abs/2402.03292</link>
<guid>https://arxiv.org/abs/2402.03292</guid>
<content:encoded><![CDATA[
arXiv:2402.03292v4 Announce Type: replace 
Abstract: Recent object detectors have achieved impressive accuracy in identifying objects seen during training. However, real-world deployment often introduces novel and unexpected objects, referred to as out-of-distribution (OOD) objects, posing significant challenges to model trustworthiness. Modern object detectors are typically overconfident, making it unreliable to use their predictions alone for OOD detection. To address this, we propose leveraging an auxiliary model as a complementary solution. Specifically, we utilize an off-the-shelf text-to-image generative model, such as Stable Diffusion, which is trained with objective functions distinct from those of discriminative object detectors. We hypothesize that this fundamental difference enables the detection of OOD objects by measuring inconsistencies between the models. Concretely, for a given detected object bounding box and its predicted in-distribution class label, we perform class-conditioned inpainting on the image with the object removed. If the object is OOD, the inpainted image is likely to deviate significantly from the original, making the reconstruction error a robust indicator of OOD status. Extensive experiments demonstrate that our approach consistently surpasses existing zero-shot and non-zero-shot OOD detection methods, establishing a robust framework for enhancing object detection systems in dynamic environments.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed Event-Based Learning via ADMM</title>
<link>https://arxiv.org/abs/2405.10618</link>
<guid>https://arxiv.org/abs/2405.10618</guid>
<content:encoded><![CDATA[
arXiv:2405.10618v3 Announce Type: replace 
Abstract: We consider a distributed learning problem, where agents minimize a global objective function by exchanging information over a network. Our approach has two distinct features: (i) It substantially reduces communication by triggering communication only when necessary, and (ii) it is agnostic to the data-distribution among the different agents. We therefore guarantee convergence even if the local data-distributions of the agents are arbitrarily distinct. We analyze the convergence rate of the algorithm both in convex and nonconvex settings and derive accelerated convergence rates for the convex case. We also characterize the effect of communication failures and demonstrate that our algorithm is robust to these. The article concludes by presenting numerical results from distributed learning tasks on the MNIST and CIFAR-10 datasets. The experiments underline communication savings of 35% or more due to the event-based communication strategy, show resilience towards heterogeneous data-distributions, and highlight that our approach outperforms common baselines such as FedAvg, FedProx, SCAFFOLD and FedADMM.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining Time Series Classification Predictions via Causal Attributions</title>
<link>https://arxiv.org/abs/2405.15871</link>
<guid>https://arxiv.org/abs/2405.15871</guid>
<content:encoded><![CDATA[
arXiv:2405.15871v2 Announce Type: replace 
Abstract: Despite the excelling performance of machine learning models, understanding their decisions remains a long-standing goal. Although commonly used attribution methods from explainable AI attempt to address this issue, they typically rely on associational rather than causal relationships. In this study, within the context of time series classification, we introduce a novel model-agnostic attribution method to assess the causal effect of concepts i.e., predefined segments within a time series, on classification outcomes. Our approach compares these causal attributions with closely related associational attributions, both theoretically and empirically. To estimate counterfactual outcomes, we use state-of-the-art diffusion models backed by state space models. We demonstrate the insights gained by our approach for a diverse set of qualitatively different time series classification tasks. Although causal and associational attributions might often share some similarities, in all cases they differ in important details, underscoring the risks associated with drawing causal conclusions from associational data alone. We believe that the proposed approach is also widely applicable in other domains to shed some light on the limits of associational attributions.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeriFlow: Modeling Distributions for Neural Network Verification</title>
<link>https://arxiv.org/abs/2406.14265</link>
<guid>https://arxiv.org/abs/2406.14265</guid>
<content:encoded><![CDATA[
arXiv:2406.14265v3 Announce Type: replace 
Abstract: Formal verification has emerged as a promising method to ensure the safety and reliability of neural networks. However, many relevant properties, such as fairness or global robustness, pertain to the entire input space. If one applies verification techniques naively, the neural network is checked even on inputs that do not occur in the real world and have no meaning. To tackle this shortcoming, we propose the VeriFlow architecture as a flow-based density model tailored to allow any verification approach to restrict its search to some data distribution of interest. We argue that our architecture is particularly well suited for this purpose because of two major properties. First, we show that the transformation that is defined by our model is piecewise affine. Therefore, the model allows the usage of verifiers based on constraint solving with linear arithmetic. Second, upper density level sets (UDL) of the data distribution are definable via linear constraints in the latent space. As a consequence, representations of UDLs specified by a given probability are effectively computable in the latent space. This property allows for effective verification with a fine-grained, probabilistically interpretable control of how a-typical the inputs subject to verification are.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExDAG: an MIQP Algorithm for Learning DAGs</title>
<link>https://arxiv.org/abs/2406.15229</link>
<guid>https://arxiv.org/abs/2406.15229</guid>
<content:encoded><![CDATA[
arXiv:2406.15229v2 Announce Type: replace 
Abstract: There has been a growing interest in causal learning in recent years. Commonly used representations of causal structures, including Bayesian networks and structural equation models (SEM), take the form of directed acyclic graphs (DAGs). We provide a novel mixed-integer quadratic programming formulation and an associated algorithm that identifies DAGs with a low structural Hamming distance between the identified DAG and the ground truth, under identifiability assumptions. The eventual exact learning is guaranteed by the global convergence of the branch-and-bound-and-cut algorithm, which is utilized. In addition to this, integer programming techniques give us access to the dual bound, which allows for a real time assessment of the quality of solution. Previously, integer programming techniques have been shown to lead to limited scaling in the case of DAG identification due to the super exponential number of constraints, which prevent the formation of cycles. The algorithm proposed circumvents this by selectively generating only the violated constraints using the so-called "lazy" constraints methodology. Our empirical results show that ExDAG outperforms state-of-the-art solvers in terms of structural Hamming distance and $F_1$ score when considering Gaussian noise on medium-sized graphs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Tensorflow Pretrained Models</title>
<link>https://arxiv.org/abs/2409.13566</link>
<guid>https://arxiv.org/abs/2409.13566</guid>
<content:encoded><![CDATA[
arXiv:2409.13566v3 Announce Type: replace 
Abstract: The application of TensorFlow pre-trained models in deep learning is explored, with an emphasis on practical guidance for tasks such as image classification and object detection. The study covers modern architectures, including ResNet, MobileNet, and EfficientNet, and demonstrates the effectiveness of transfer learning through real-world examples and experiments. A comparison of linear probing and model fine-tuning is presented, supplemented by visualizations using techniques like PCA, t-SNE, and UMAP, allowing for an intuitive understanding of the impact of these approaches. The work provides complete example code and step-by-step instructions, offering valuable insights for both beginners and advanced users. By integrating theoretical concepts with hands-on practice, the paper equips readers with the tools necessary to address deep learning challenges efficiently.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planning-Aware Code Infilling via Horizon-Length Prediction</title>
<link>https://arxiv.org/abs/2410.03103</link>
<guid>https://arxiv.org/abs/2410.03103</guid>
<content:encoded><![CDATA[
arXiv:2410.03103v4 Announce Type: replace 
Abstract: Fill-in-the-Middle (FIM), or infilling, has become integral to code language models, enabling generation of missing code given both left and right contexts. However, the current FIM training paradigm which performs next-token prediction (NTP) over reordered sequence often leads to models struggling to generate content that aligns well with the surrounding context. We hypothesize that NTP alone is insufficient for models to learn effective planning conditioned on the distant right context, a critical factor for successful code infilling. To overcome this, we propose Horizon-Length Prediction (HLP), a novel training objective that teaches models to predict the number of remaining middle tokens at each step. HLP advances FIM with lookahead planning, enabling models to inherently learn infilling boundaries for arbitrary left and right contexts without relying on dataset-specific post-processing. Our evaluation across different model families and sizes shows that HLP significantly improves FIM performance by up to 24% relatively on diverse benchmarks, across file-level and repository-level. Furthermore, the enhanced planning capability gained through HLP boosts model performance on code reasoning. Importantly, HLP incurs negligible training overhead and no additional inference cost, ensuring its practicality for real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Gradient Normalization and Clipping for Nonconvex SGD under Heavy-Tailed Noise: Necessity, Sufficiency, and Acceleration</title>
<link>https://arxiv.org/abs/2410.16561</link>
<guid>https://arxiv.org/abs/2410.16561</guid>
<content:encoded><![CDATA[
arXiv:2410.16561v4 Announce Type: replace 
Abstract: Gradient clipping has long been considered essential for ensuring the convergence of Stochastic Gradient Descent (SGD) in the presence of heavy-tailed gradient noise. In this paper, we revisit this belief and explore whether gradient normalization can serve as an effective alternative or complement. We prove that, under individual smoothness assumptions, gradient normalization alone is sufficient to guarantee convergence of the nonconvex SGD. Moreover, when combined with clipping, it yields far better rates of convergence under more challenging noise distributions. We provide a unifying theory describing normalization-only, clipping-only, and combined approaches. Moving forward, we investigate existing variance-reduced algorithms, establishing that, in such a setting, normalization alone is sufficient for convergence. Finally, we present an accelerated variant that under second-order smoothness improves convergence. Our results provide theoretical insights and practical guidance for using normalization and clipping in nonconvex optimization with heavy-tailed noise.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>xLSTM-Mixer: Multivariate Time Series Forecasting by Mixing via Scalar Memories</title>
<link>https://arxiv.org/abs/2410.16928</link>
<guid>https://arxiv.org/abs/2410.16928</guid>
<content:encoded><![CDATA[
arXiv:2410.16928v3 Announce Type: replace 
Abstract: Time series data is prevalent across numerous fields, necessitating the development of robust and accurate forecasting models. Capturing patterns both within and between temporal and multivariate components is crucial for reliable predictions. We introduce xLSTM-Mixer, a model designed to effectively integrate temporal sequences, joint time-variate information, and multiple perspectives for robust forecasting. Our approach begins with a linear forecast shared across variates, which is then refined by xLSTM blocks. They serve as key elements for modeling the complex dynamics of challenging time series data. xLSTM-Mixer ultimately reconciles two distinct views to produce the final forecast. Our extensive evaluations demonstrate its superior long-term forecasting performance compared to recent state-of-the-art methods while requiring very little memory. A thorough model analysis provides further insights into its key components and confirms its robustness and effectiveness. This work contributes to the resurgence of recurrent models in forecasting by combining them, for the first time, with mixing architectures.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RIZE: Adaptive Regularization for Imitation Learning</title>
<link>https://arxiv.org/abs/2502.20089</link>
<guid>https://arxiv.org/abs/2502.20089</guid>
<content:encoded><![CDATA[
arXiv:2502.20089v3 Announce Type: replace 
Abstract: We propose a novel Inverse Reinforcement Learning (IRL) method that mitigates the rigidity of fixed reward structures and the limited flexibility of implicit reward regularization. Building on the Maximum Entropy IRL framework, our approach incorporates a squared temporal-difference (TD) regularizer with adaptive targets that evolve dynamically during training, thereby imposing adaptive bounds on recovered rewards and promoting robust decision-making. To capture richer return information, we integrate distributional RL into the learning process. Empirically, our method achieves expert-level performance on complex MuJoCo and Adroit environments, surpassing baseline methods on the Humanoid-v2 task with limited expert demonstrations. Extensive experiments and ablation studies further validate the effectiveness of the approach and provide insights into reward dynamics in imitation learning. Our source code is available at https://github.com/adibka/RIZE.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Streaming Generation of Co-Speech Gestures via Accelerated Rolling Diffusion</title>
<link>https://arxiv.org/abs/2503.10488</link>
<guid>https://arxiv.org/abs/2503.10488</guid>
<content:encoded><![CDATA[
arXiv:2503.10488v3 Announce Type: replace 
Abstract: Generating co-speech gestures in real time requires both temporal coherence and efficient sampling. We introduce a novel framework for streaming gesture generation that extends Rolling Diffusion models with structured progressive noise scheduling, enabling seamless long-sequence motion synthesis while preserving realism and diversity. Our framework is universally compatible with existing diffusion-based gesture generation model, transforming them into streaming methods capable of continuous generation without requiring post-processing. We evaluate our framework on ZEGGS and BEAT, strong benchmarks for real-world applicability. Applied to state-of-the-art baselines on both datasets, it consistently outperforms them, demonstrating its effectiveness as a generalizable and efficient solution for real-time co-speech gesture synthesis. We further propose Rolling Diffusion Ladder Acceleration (RDLA), a new approach that employs a ladder-based noise scheduling strategy to simultaneously denoise multiple frames. This significantly improves sampling efficiency while maintaining motion consistency, achieving up to a 4x speedup with high visual fidelity and temporal coherence in our experiments. Comprehensive user studies further validate our framework ability to generate realistic, diverse gestures closely synchronized with the audio input.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring the (Un)Faithfulness of Concept-Based Explanations</title>
<link>https://arxiv.org/abs/2504.10833</link>
<guid>https://arxiv.org/abs/2504.10833</guid>
<content:encoded><![CDATA[
arXiv:2504.10833v3 Announce Type: replace 
Abstract: Deep vision models perform input-output computations that are hard to interpret. Concept-based explanation methods (CBEMs) increase interpretability by re-expressing parts of the model with human-understandable semantic units, or concepts. Checking if the derived explanations are faithful -- that is, they represent the model's internal computation -- requires a surrogate that combines concepts to compute the output. Simplifications made for interpretability inevitably reduce faithfulness, resulting in a tradeoff between the two. State-of-the-art unsupervised CBEMs (U-CBEMs) have reported increasingly interpretable concepts, while also being more faithful to the model. However, we observe that the reported improvement in faithfulness artificially results from either (1) using overly complex surrogates, which introduces an unmeasured cost to the explanation's interpretability, or (2) relying on deletion-based approaches that, as we demonstrate, do not properly measure faithfulness. We propose Surrogate Faithfulness (SURF), which (1) replaces prior complex surrogates with a simple, linear surrogate that measures faithfulness without changing the explanation's interpretability and (2) introduces well-motivated metrics that assess loss across all output classes, not just the predicted class. We validate SURF with a measure-over-measure study by proposing a simple sanity check -- explanations with random concepts should be less faithful -- which prior surrogates fail. SURF enables the first reliable faithfulness benchmark of U-CBEMs, revealing that many visually compelling U-CBEMs are not faithful. Code to be released.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantitative Attractor Analysis of High-Capacity Kernel Logistic Regression Hopfield Networks</title>
<link>https://arxiv.org/abs/2505.01218</link>
<guid>https://arxiv.org/abs/2505.01218</guid>
<content:encoded><![CDATA[
arXiv:2505.01218v2 Announce Type: replace 
Abstract: Kernel-based learning methods such as Kernel Logistic Regression (KLR) can dramatically increase the storage capacity of Hopfield networks, but the principles governing their performance and stability remain largely uncharacterized. This paper presents a comprehensive quantitative analysis of the attractor landscape in KLR-trained networks to establish a solid foundation for their design and application. Through extensive, statistically validated simulations, we address critical questions of generality, scalability, and robustness. Our comparative analysis reveals that KLR and Kernel Ridge Regression (KRR) exhibit similarly high storage capacities and clean attractor landscapes, suggesting this is a general property of kernel regression methods, though KRR is computationally much faster. We uncover a non-trivial, scale-dependent scaling law for the kernel width ($\gamma$), demonstrating that optimal capacity requires gamma to be scaled such that $\gamma \times N$ increases with network size $N$. This implies that larger networks necessitate more localized kernels -- where each pattern's influence is more spatially confined--to manage inter-pattern interference. Under this optimized scaling, we provide definitive evidence that the storage capacity scales linearly with network size ($P \propto N$). Furthermore, our sensitivity analysis shows that performance is remarkably robust to the choice of the regularization parameter lambda. Collectively, these findings provide a clear set of empirical principles for designing high-capacity, robust associative memories and clarify the mechanisms that enable kernel methods to overcome the classical limitations of Hopfield-type models.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OODTE: A Differential Testing Engine for the ONNX Optimizer</title>
<link>https://arxiv.org/abs/2505.01892</link>
<guid>https://arxiv.org/abs/2505.01892</guid>
<content:encoded><![CDATA[
arXiv:2505.01892v4 Announce Type: replace 
Abstract: With over 760 stars on GitHub and being part of the official ONNX repository, the ONNX Optimizer is the default tool for applying graph-based optimizations to ONNX models. Despite its widespread use, its ability to maintain model accuracy during optimization has not been thoroughly investigated. In this work, we present OODTE, a utility designed to automatically and comprehensively evaluate the correctness of the ONNX Optimizer. OODTE adopts a straightforward yet powerful differential testing and evaluation methodology, which can be readily adapted for use with other compiler optimizers. Specifically, OODTE takes a collection of ONNX models, applies optimizations, and executes both the original and optimized versions across a user-defined input set, automatically capturing any issues encountered during optimization. When discrepancies in accuracy arise, OODTE iteratively isolates the responsible optimization pass by repeating the process at a finer granularity. We applied OODTE to 130 well-known models from the official ONNX Model Hub, spanning diverse tasks including classification, object detection, semantic segmentation, text summarization, question answering, and sentiment analysis. Our evaluation revealed that 9.2% of the model instances either caused the optimizer to crash or led to the generation of invalid models using default optimization strategies. Additionally, 30% of classification models and 16.6% of object detection and segmentation models exhibited differing outputs across original and optimized versions, whereas models focused on text-related tasks were generally robust to optimization. OODTE uncovered 15 issues-14 previously unknown-affecting 9 of 47 optimization passes and the optimizer overall. All issues were reported to the ONNX Optimizer team. OODTE offers a simple but effective framework for validating AI model optimizers, applicable beyond the ONNX ecosystem.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Put CASH on Bandits: A Max K-Armed Problem for Automated Machine Learning</title>
<link>https://arxiv.org/abs/2505.05226</link>
<guid>https://arxiv.org/abs/2505.05226</guid>
<content:encoded><![CDATA[
arXiv:2505.05226v2 Announce Type: replace 
Abstract: The Combined Algorithm Selection and Hyperparameter optimization (CASH) is a challenging resource allocation problem in the field of AutoML. We propose MaxUCB, a max k-armed bandit method to trade off exploring different model classes and conducting hyperparameter optimization. MaxUCB is specifically designed for the light-tailed and bounded reward distributions arising in this setting and, thus, provides an efficient alternative compared to classic max k-armed bandit methods assuming heavy-tailed reward distributions. We theoretically and empirically evaluate our method on four standard AutoML benchmarks, demonstrating superior performance over prior approaches. We make our code and data available at https://github.com/amirbalef/CASH_with_Bandits
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling</title>
<link>https://arxiv.org/abs/2505.09851</link>
<guid>https://arxiv.org/abs/2505.09851</guid>
<content:encoded><![CDATA[
arXiv:2505.09851v2 Announce Type: replace 
Abstract: Traditional entropy-based methods - such as cross-entropy loss in classification problems - have long been essential tools for representing the information uncertainty and physical disorder in data and for developing artificial intelligence algorithms. However, the rapid growth of data across various domains has introduced new challenges, particularly the integration of heterogeneous datasets with intrinsic disparities. To address this, we introduce a zentropy-enhanced neural network (ZENN), extending zentropy theory into the data science domain via intrinsic entropy, enabling more effective learning from heterogeneous data sources. ZENN simultaneously learns both energy and intrinsic entropy components, capturing the underlying structure of multi-source data. To support this, we redesign the neural network architecture to better reflect the intrinsic properties and variability inherent in diverse datasets. We demonstrate the effectiveness of ZENN on classification tasks and energy landscape reconstructions, showing its superior generalization capabilities and robustness-particularly in predicting high-order derivatives. ZENN demonstrates superior generalization by introducing a learnable temperature variable that models latent multi-source heterogeneity, allowing it to surpass state-of-the-art models on CIFAR-10/100, BBCNews, and AGNews. As a practical application in materials science, we employ ZENN to reconstruct the Helmholtz energy landscape of Fe$_3$Pt using data generated from density functional theory (DFT) and capture key material behaviors, including negative thermal expansion and the critical point in the temperature-pressure space. Overall, this work presents a zentropy-grounded framework for data-driven machine learning, positioning ZENN as a versatile and robust approach for scientific problems involving complex, heterogeneous datasets.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TI-DeepONet: Learnable Time Integration for Stable Long-Term Extrapolation</title>
<link>https://arxiv.org/abs/2505.17341</link>
<guid>https://arxiv.org/abs/2505.17341</guid>
<content:encoded><![CDATA[
arXiv:2505.17341v2 Announce Type: replace 
Abstract: Accurate temporal extrapolation remains a fundamental challenge for neural operators modeling dynamical systems, where predictions must extend far beyond the training horizon. Conventional DeepONet approaches rely on two limited paradigms: fixed-horizon rollouts, which predict full spatiotemporal solutions while ignoring temporal causality, and autoregressive schemes, which accumulate errors through sequential prediction. We introduce TI-DeepONet, a framework that integrates neural operators with adaptive numerical time-stepping to preserve the Markovian structure of dynamical systems while mitigating long-term error growth. Our method shifts the learning objective from direct state prediction to approximating instantaneous time-derivative fields, which are then integrated using standard numerical solvers. This naturally enables continuous-time prediction and allows the use of higher-order integrators at inference than those used in training, improving both efficiency and accuracy. We further propose TI(L)-DeepONet, which incorporates learnable coefficients for intermediate slopes in multi-stage integration, adapting to solution-specific dynamics and enhancing fidelity. Across four canonical PDEs featuring chaotic, dissipative, dispersive, and high-dimensional behavior, TI(L)-DeepONet slightly outperforms TI-DeepONet, and both achieve major reductions in relative L2 extrapolation error: about 81% compared to autoregressive methods and 70% compared to fixed-horizon approaches. Notably, both models maintain stable predictions over temporal domains nearly twice the training interval. This work establishes a physics-aware operator learning framework that bridges neural approximation with numerical analysis principles, addressing a key gap in long-term forecasting of complex physical systems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Turb-L1: Achieving Long-term Turbulence Tracing By Tackling Spectral Bias</title>
<link>https://arxiv.org/abs/2505.19038</link>
<guid>https://arxiv.org/abs/2505.19038</guid>
<content:encoded><![CDATA[
arXiv:2505.19038v3 Announce Type: replace 
Abstract: Accurately predicting the long-term evolution of turbulence is crucial for advancing scientific understanding and optimizing engineering applications. However, existing deep learning methods face significant bottlenecks in long-term autoregressive prediction, which exhibit excessive smoothing and fail to accurately track complex fluid dynamics. Our extensive experimental and spectral analysis of prevailing methods provides an interpretable explanation for this shortcoming, identifying Spectral Bias as the core obstacle. Concretely, spectral bias is the inherent tendency of models to favor low-frequency, smooth features while overlooking critical high-frequency details during training, thus reducing fidelity and causing physical distortions in long-term predictions. Building on this insight, we propose Turb-L1, an innovative turbulence prediction method, which utilizes a Hierarchical Dynamics Synthesis mechanism within a multi-grid architecture to explicitly overcome spectral bias. It accurately captures cross-scale interactions and preserves the fidelity of high-frequency dynamics, enabling reliable long-term tracking of turbulence evolution. Extensive experiments on the 2D turbulence benchmark show that Turb-L1 demonstrates excellent performance: (I) In long-term predictions, it reduces Mean Squared Error (MSE) by $80.3\%$ and increases Structural Similarity (SSIM) by over $9\times$ compared to the SOTA baseline, significantly improving prediction fidelity. (II) It effectively overcomes spectral bias, accurately reproducing the full enstrophy spectrum and maintaining physical realism in high-wavenumber regions, thus avoiding the spectral distortions or spurious energy accumulation seen in other methods.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy-based generator matching: A neural sampler for general state space</title>
<link>https://arxiv.org/abs/2505.19646</link>
<guid>https://arxiv.org/abs/2505.19646</guid>
<content:encoded><![CDATA[
arXiv:2505.19646v3 Announce Type: replace 
Abstract: We propose Energy-based generator matching (EGM), a modality-agnostic approach to train generative models from energy functions in the absence of data. Extending the recently proposed generator matching, EGM enables training of arbitrary continuous-time Markov processes, e.g., diffusion, flow, and jump, and can generate data from continuous, discrete, and a mixture of two modalities. To this end, we propose estimating the generator matching loss using self-normalized importance sampling with an additional bootstrapping trick to reduce variance in the importance weight. We validate EGM on both discrete and multimodal tasks up to 100 and 20 dimensions, respectively.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning in Compact Spaces with Approximately Normalized Transformer</title>
<link>https://arxiv.org/abs/2505.22014</link>
<guid>https://arxiv.org/abs/2505.22014</guid>
<content:encoded><![CDATA[
arXiv:2505.22014v2 Announce Type: replace 
Abstract: The successful training of deep neural networks requires addressing challenges such as overfitting, numerical instabilities leading to divergence, and increasing variance in the residual stream. A common solution is to apply regularization and normalization techniques that usually require tuning additional hyperparameters. An alternative is to force all parameters and representations to lie on a hypersphere. This removes the need for regularization and increases convergence speed, but comes with additional costs. In this work, we propose a more holistic, approximate normalization via simple scalar multiplications motivated by the tight concentration of the norms of high-dimensional random vectors. Additionally, instead of applying strict normalization for the parameters, we constrain their norms. These modifications remove the need for weight decay and learning rate warm-up as well, but do not increase the total number of normalization layers. Our experiments with transformer architectures show up to 40% faster convergence compared to GPT models with QK normalization, with only 3% additional runtime cost. When deriving scaling laws, we found that our method enables training with larger batch sizes while preserving the favorable scaling characteristics of classic GPT architectures.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Framework for Provably Efficient Algorithms to Estimate Shapley Values</title>
<link>https://arxiv.org/abs/2506.05216</link>
<guid>https://arxiv.org/abs/2506.05216</guid>
<content:encoded><![CDATA[
arXiv:2506.05216v2 Announce Type: replace 
Abstract: Shapley values have emerged as a critical tool for explaining which features impact the decisions made by machine learning models. However, computing exact Shapley values is difficult, generally requiring an exponential (in the feature dimension) number of model evaluations. To address this, many model-agnostic randomized estimators have been developed, the most influential and widely used being the KernelSHAP method (Lundberg & Lee, 2017). While related estimators such as unbiased KernelSHAP (Covert & Lee, 2021) and LeverageSHAP (Musco & Witter, 2025) are known to satisfy theoretical guarantees, bounds for KernelSHAP have remained elusive. We describe a broad and unified framework that encompasses KernelSHAP and related estimators constructed using both with and without replacement sampling strategies. We then prove strong non-asymptotic theoretical guarantees that apply to all estimators from our framework. This provides, to the best of our knowledge, the first theoretical guarantees for KernelSHAP and sheds further light on tradeoffs between existing estimators. Through comprehensive benchmarking on small and medium dimensional datasets for Decision-Tree models, we validate our approach against exact Shapley values, consistently achieving low mean squared error with modest sample sizes. Furthermore, we make specific implementation improvements to enable scalability of our methods to high-dimensional datasets. Our methods, tested on datasets such MNIST and CIFAR10, provide consistently better results compared to the KernelSHAP library.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Convergence of Adjoint-Optimized Neural PDEs</title>
<link>https://arxiv.org/abs/2506.13633</link>
<guid>https://arxiv.org/abs/2506.13633</guid>
<content:encoded><![CDATA[
arXiv:2506.13633v2 Announce Type: replace 
Abstract: Many engineering and scientific fields have recently become interested in modeling terms in partial differential equations (PDEs) with neural networks, which requires solving the inverse problem of learning neural network terms from observed data in order to approximate missing or unresolved physics in the PDE model. The resulting neural-network PDE model, being a function of the neural network parameters, can be calibrated to the available ground truth data by optimizing over the PDE using gradient descent, where the gradient is evaluated in a computationally efficient manner by solving an adjoint PDE. These neural PDE models have emerged as an important research area in scientific machine learning. In this paper, we study the convergence of the adjoint gradient descent optimization method for training neural PDE models in the limit where both the number of hidden units and the training time tend to infinity. Specifically, for a general class of nonlinear parabolic PDEs with a neural network embedded in the source term, we prove convergence of the trained neural-network PDE solution to the target data (i.e., a global minimizer). The global convergence proof poses a unique mathematical challenge that is not encountered in finite-dimensional neural network convergence analyses due to (i) the neural network training dynamics involving a non-local neural network kernel operator in the infinite-width hidden layer limit where the kernel lacks a spectral gap for its eigenvalues and (ii) the nonlinearity of the limit PDE system, which leads to a non-convex optimization problem in the neural network function even in the infinite-width hidden layer limit (unlike in typical neural network training cases where the optimization problem becomes convex in the large neuron limit). The theoretical results are illustrated and empirically validated by numerical studies.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RTNinja: A generalized machine learning framework for analyzing random telegraph noise signals in nanoelectronic devices</title>
<link>https://arxiv.org/abs/2507.08424</link>
<guid>https://arxiv.org/abs/2507.08424</guid>
<content:encoded><![CDATA[
arXiv:2507.08424v3 Announce Type: replace 
Abstract: Random telegraph noise is a prevalent variability phenomenon in nanoelectronic devices, arising from stochastic carrier exchange at defect sites and critically impacting device reliability and performance. Conventional analysis techniques often rely on restrictive assumptions or manual interventions, limiting their applicability to complex, noisy datasets. Here, we introduce RTNinja, a generalized, fully automated machine learning framework for the unsupervised analysis of random telegraph noise signals. RTNinja deconvolves complex signals to identify the number and characteristics of hidden individual sources without requiring prior knowledge of the system. The framework comprises two modular components: LevelsExtractor, which uses Bayesian inference and model selection to denoise and discretize the signal, and SourcesMapper, which infers source configurations through probabilistic clustering and optimization. To evaluate performance, we developed a Monte Carlo simulator that generates labeled datasets spanning broad signal-to-noise ratios and source complexities; across 7000 such datasets, RTNinja consistently demonstrated high-fidelity signal reconstruction and accurate extraction of source amplitudes and activity patterns. Our results demonstrate that RTNinja offers a robust, scalable, and device-agnostic tool for random telegraph noise characterization, enabling large-scale statistical benchmarking, reliability-centric technology qualification, predictive failure modeling, and device physics exploration in next-generation nanoelectronics.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAP Estimation with Denoisers: Convergence Rates and Guarantees</title>
<link>https://arxiv.org/abs/2507.15397</link>
<guid>https://arxiv.org/abs/2507.15397</guid>
<content:encoded><![CDATA[
arXiv:2507.15397v3 Announce Type: replace 
Abstract: Denoiser models have become powerful tools for inverse problems, enabling the use of pretrained networks to approximate the score of a smoothed prior distribution. These models are often used in heuristic iterative schemes aimed at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal operator of the negative log-prior plays a central role. In practice, this operator is intractable, and practitioners plug in a pretrained denoiser as a surrogate-despite the lack of general theoretical justification for this substitution. In this work, we show that a simple algorithm, closely related to several used in practice, provably converges to the proximal operator under a log-concavity assumption on the prior $p$. We show that this algorithm can be interpreted as a gradient descent on smoothed proximal objectives. Our analysis thus provides a theoretical foundation for a class of empirically successful but previously heuristic methods.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer</title>
<link>https://arxiv.org/abs/2508.10587</link>
<guid>https://arxiv.org/abs/2508.10587</guid>
<content:encoded><![CDATA[
arXiv:2508.10587v3 Announce Type: replace 
Abstract: To bridge the temporal granularity gap in energy network design and operation based on Energy System Models, resampling of time series is required. While conventional upsampling methods are computationally efficient, they often result in significant information loss or increased noise. Advanced models such as time series generation models, Super-Resolution models and imputation models show potential, but also face fundamental challenges. The goal of time series generative models is to learn the distribution of the original data to generate high-resolution series with similar statistical characteristics. This is not entirely consistent with the definition of upsampling. Time series Super-Resolution models or imputation models can degrade the accuracy of upsampling because the input low-resolution time series are sparse and may have insufficient context. Moreover, such models usually rely on supervised learning paradigms. This presents a fundamental application paradox: their training requires the high-resolution time series that is intrinsically absent in upsampling application scenarios. To address the mentioned upsampling issue, this paper introduces a new method utilizing Generative Adversarial Transformers (GATs), which can be trained without access to any ground-truth high-resolution data. Compared with conventional interpolation methods, the introduced method can reduce the root mean square error (RMSE) of upsampling tasks by 10%, and the accuracy of a model predictive control (MPC) application scenario is improved by 13%.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coresets from Trajectories: Selecting Data via Correlation of Loss Differences</title>
<link>https://arxiv.org/abs/2508.20230</link>
<guid>https://arxiv.org/abs/2508.20230</guid>
<content:encoded><![CDATA[
arXiv:2508.20230v2 Announce Type: replace 
Abstract: Deep learning models achieve state-of-the-art performance across domains but face scalability challenges in real-time or resource-constrained scenarios. To address this, we propose Correlation of Loss Differences (CLD), a simple and scalable metric for coreset selection that identifies the most impactful training samples by measuring their alignment with the loss trajectories of a held-out validation set. CLD is highly efficient, requiring only per-sample loss values computed at training checkpoints, and avoiding the costly gradient and curvature computations used in many existing subset selection methods. We develop a general theoretical framework that establishes convergence guarantees for CLD-based coresets, demonstrating that the convergence error is upper-bounded by the alignment of the selected samples and the representativeness of the validation set. On CIFAR-100 and ImageNet-1k, CLD-based coresets typically outperform or closely match state-of-the-art methods across subset sizes, and remain within 1% of more computationally expensive baselines even when not leading. CLD transfers effectively across architectures (ResNet, VGG, DenseNet), enabling proxy-to-target selection with <1% degradation. Moreover, CLD is stable when using only early checkpoints, incurring negligible accuracy loss. Finally, CLD exhibits inherent bias reduction via per-class validation alignment, obviating the need for additional stratified sampling. Together, these properties make CLD a principled, efficient, stable, and transferable tool for scalable dataset optimization.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing In-Context Learning for Efficient Full Conformal Prediction</title>
<link>https://arxiv.org/abs/2509.01840</link>
<guid>https://arxiv.org/abs/2509.01840</guid>
<content:encoded><![CDATA[
arXiv:2509.01840v3 Announce Type: replace 
Abstract: Reliable uncertainty quantification is critical for trustworthy AI. Conformal Prediction (CP) provides prediction sets with distribution-free coverage guarantees, but its two main variants face complementary limitations. Split CP (SCP) suffers from data inefficiency due to dataset partitioning, while full CP (FCP) improves data efficiency at the cost of prohibitive retraining complexity. Recent approaches based on meta-learning or in-context learning (ICL) partially mitigate these drawbacks. However, they rely on training procedures not specifically tailored to CP, which may yield large prediction sets. We introduce an efficient FCP framework, termed enhanced ICL-based FCP (E-ICL+FCP), which employs a permutation-invariant Transformer-based ICL model trained with a CP-aware loss. By simulating the multiple retrained models required by FCP without actual retraining, E-ICL+FCP preserves coverage while markedly reducing both inefficiency and computational overhead. Experiments on synthetic and real tasks demonstrate that E-ICL+FCP attains superior efficiency-coverage trade-offs compared to existing SCP and FCP baselines.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Entropy Regularization: A Complexity-Aware Approach for Neural Optimization</title>
<link>https://arxiv.org/abs/2509.03733</link>
<guid>https://arxiv.org/abs/2509.03733</guid>
<content:encoded><![CDATA[
arXiv:2509.03733v2 Announce Type: replace 
Abstract: We introduce the first differentiable approximation of range-partition entropy, a complexity measure from computational geometry that directly bounds algorithmic runtime. Unlike architectural modifications, our method is a complementary regularizer that provides orthogonal efficiency gains when combined with existing optimizations. We establish theoretical guarantees in computational geometry, achieving 4--5$\times$ provable speedups on convex hull and triangulation with $<$0.2\% error. On ImageNet-1K with ViT-Base, entropy regularization achieves 80.1\% top-1 accuracy at 80\% sparsity (1.60$\times$ standalone speedup), and when combined with FlashAttention yields 2.07$\times$ speedup versus 1.63$\times$ for FlashAttention alone. On large language models (LLaMA-2 7B, Mistral-7B, Phi-2), we achieve 1.48--1.60$\times$ inference speedups at 70--75\% sparsity with minimal quality degradation (ROUGE-L drops of 0.3--0.4 points, perplexity increase of 0.9). Unlike prior regularization methods that target output distributions, we directly minimize representation complexity, yielding both efficiency gains and improved robustness through semantically structured sparsity patterns (IoU 0.73 vs 0.41 for magnitude pruning, CIFAR-100-C mCE 48.7 vs 55.4). Benefits are strongest for geometry and vision transformers, with more modest but measurable gains on LLMs, demonstrating that complexity regularization offers a principled pathway to joint efficiency-robustness optimization.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design</title>
<link>https://arxiv.org/abs/2509.12527</link>
<guid>https://arxiv.org/abs/2509.12527</guid>
<content:encoded><![CDATA[
arXiv:2509.12527v3 Announce Type: replace 
Abstract: Large language models often produce confident but incorrect outputs, creating a critical need for reliable uncertainty quantification with formal abstention guarantees. We introduce information-lift certificates that compare model probabilities to a skeleton baseline, accumulating evidence through sub-gamma PAC-Bayes bounds that remain valid under heavy-tailed distributions where standard concentration inequalities fail. On eight diverse datasets, our method achieves 77.0\% coverage at 2\% risk, outperforming recent baselines by 10.0 percentage points on average. In high-stakes scenarios, we block 96\% of critical errors compared to 18-31\% for entropy-based methods. While our frequency-based certification does not guarantee severity-weighted safety and depends on skeleton quality, performance degrades gracefully under distributional shifts, making the approach practical for real-world deployment.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Preserving In-Context-Learning Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2509.13625</link>
<guid>https://arxiv.org/abs/2509.13625</guid>
<content:encoded><![CDATA[
arXiv:2509.13625v4 Announce Type: replace 
Abstract: Large language models (LLMs) have significantly transformed natural language understanding and generation, but they raise privacy concerns due to potential exposure of sensitive information. Studies have highlighted the risk of information leakage, where adversaries can extract sensitive information embedded in the prompts. In this work, we introduce a novel private prediction framework for generating high-quality synthetic text with strong privacy guarantees. Our approach leverages the Differential Privacy (DP) framework to ensure worst-case theoretical bounds on information leakage without requiring any fine-tuning of the underlying models. The proposed method performs inference on private records and aggregates the resulting per-token output distributions. This enables the generation of longer and coherent synthetic text while maintaining privacy guarantees. Additionally, we propose a simple blending operation that combines private and public inference to further enhance utility. Empirical evaluations demonstrate that our approach outperforms previous state-of-the-art methods on in-context-learning (ICL) tasks, making it a promising direction for privacy-preserving text generation while maintaining high utility. Our code is available at https://github.com/bhusalb/privacy-preserving-icl.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMG: Mutual Information Estimation via the MMSE Gap in Diffusion</title>
<link>https://arxiv.org/abs/2509.20609</link>
<guid>https://arxiv.org/abs/2509.20609</guid>
<content:encoded><![CDATA[
arXiv:2509.20609v2 Announce Type: replace 
Abstract: Mutual information (MI) is one of the most general ways to measure relationships between random variables, but estimating this quantity for complex systems is challenging. Denoising diffusion models have recently set a new bar for density estimation, so it is natural to consider whether these methods could also be used to improve MI estimation. Using the recently introduced information-theoretic formulation of denoising diffusion models, we show the diffusion models can be used in a straightforward way to estimate MI. In particular, the MI corresponds to half the gap in the Minimum Mean Square Error (MMSE) between conditional and unconditional diffusion, integrated over all Signal-to-Noise-Ratios (SNRs) in the noising process. Our approach not only passes self-consistency tests but also outperforms traditional and score-based diffusion MI estimators. Furthermore, our method leverages adaptive importance sampling to achieve scalable MI estimation, while maintaining strong performance even when the MI is high.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Smart, Not Hard: Difficulty Adaptive Reasoning for Large Audio Language Models</title>
<link>https://arxiv.org/abs/2509.21960</link>
<guid>https://arxiv.org/abs/2509.21960</guid>
<content:encoded><![CDATA[
arXiv:2509.21960v2 Announce Type: replace 
Abstract: Large Audio Language Models (LALMs), powered by the chain-of-thought (CoT) paradigm, have shown remarkable reasoning capabilities. Intuitively, different problems often require varying depths of reasoning. While some methods can determine whether to reason for a given problem, they typically lack a fine-grained mechanism to modulate how much to reason. This often results in a ``one-size-fits-all'' reasoning depth, which generates redundant overthinking for simple questions while failing to allocate sufficient thought to complex ones. In this paper, we conduct an in-depth analysis of LALMs and find that an effective and efficient LALM should reason smartly by adapting its reasoning depth to the problem's complexity. To achieve this, we propose a difficulty-adaptive reasoning method for LALMs. Specifically, we propose a reward function that dynamically links reasoning length to the model's perceived problem difficulty. This reward encourages shorter, concise reasoning for easy tasks and more elaborate, in-depth reasoning for complex ones. Extensive experiments demonstrate that our method is both effective and efficient, simultaneously improving task performance and significantly reducing the average reasoning length. Further analysis on reasoning structure paradigm offers valuable insights for future work.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Observation-Free Attacks on Online Learning to Rank</title>
<link>https://arxiv.org/abs/2509.22855</link>
<guid>https://arxiv.org/abs/2509.22855</guid>
<content:encoded><![CDATA[
arXiv:2509.22855v3 Announce Type: replace 
Abstract: Online learning to rank (OLTR) plays a critical role in information retrieval and machine learning systems, with a wide range of applications in search engines and content recommenders. However, despite their extensive adoption, the susceptibility of OLTR algorithms to coordinated adversarial attacks remains poorly understood. In this work, we present a novel framework for attacking some of the widely used OLTR algorithms. Our framework is designed to promote a set of target items so that they appear in the list of top-K recommendations for T - o(T) rounds, while simultaneously inducing linear regret in the learning algorithm. We propose two novel attack strategies: CascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical guarantees showing that both strategies require only O(log T) manipulations to succeed. Additionally, we supplement our theoretical analysis with empirical results on real-world data.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention</title>
<link>https://arxiv.org/abs/2509.24006</link>
<guid>https://arxiv.org/abs/2509.24006</guid>
<content:encoded><![CDATA[
arXiv:2509.24006v2 Announce Type: replace 
Abstract: In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bottleneck due to the long sequence length and the quadratic complexity. We find that attention weights can be separated into two parts: a small fraction of large weights with high rank and the remaining weights with very low rank. This naturally suggests applying sparse acceleration to the first part and low-rank acceleration to the second. Based on this finding, we propose SLA (Sparse-Linear Attention), a trainable attention method that fuses sparse and linear attention to accelerate diffusion models. SLA classifies attention weights into critical, marginal, and negligible categories, applying O(N^2) attention to critical weights, O(N) attention to marginal weights, and skipping negligible ones. SLA combines these computations into a single GPU kernel and supports both forward and backward passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x reduction in attention computation, resulting in significant acceleration without loss of generation quality. Experiments show that SLA reduces attention computation by 95% without degrading end-to-end generation quality, outperforming baseline methods. In addition, we implement an efficient GPU kernel for SLA, which yields a 13.7x speedup in attention computation and a 2.2x end-to-end speedup in video generation on Wan2.1-1.3B. The code is available at https://github.com/thu-ml/SLA.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepEN: A Deep Reinforcement Learning Framework for Personalized Enteral Nutrition in Critical Care</title>
<link>https://arxiv.org/abs/2510.08350</link>
<guid>https://arxiv.org/abs/2510.08350</guid>
<content:encoded><![CDATA[
arXiv:2510.08350v2 Announce Type: replace 
Abstract: ICU enteral feeding remains sub-optimal due to limited personalization and uncertainty about appropriate calorie, protein, and fluid targets, particularly under rapidly changing metabolic demands and heterogeneous patient responses. This study introduces DeepEN, a reinforcement learning (RL)-based framework that personalizes enteral nutrition (EN) dosing for critically ill patients using electronic health record data. DeepEN was trained on over 11,000 ICU patients from the MIMIC-IV database to generate 4-hourly, patient-specific targets for caloric, protein, and fluid intake. The model's state space integrates demographics, comorbidities, vital signs, laboratory results, and prior interventions relevant to nutritional management, while its reward function balances short-term physiological and nutrition-related goals with long-term survival. A dueling double deep Q-network with Conservative Q-Learning regularization is used to ensure safe and reliable policy learning from retrospective data. DeepEN achieved a 3.7 $\pm$ 0.17 percentage-point absolute reduction in estimated mortality compared with the clinician policy (18.8% vs 22.5%) and higher expected returns compared with guideline-based dosing (11.89 vs 8.11), with improvements in key nutritional biomarkers. U-shaped associations between deviations from clinician dosing and mortality suggest that the learned policy aligns with high-value clinician actions while diverging from suboptimal ones. These findings demonstrate the feasibility of conservative offline RL for individualized EN therapy and suggest that data-driven personalization may improve outcomes beyond guideline- or heuristic-based approaches.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start</title>
<link>https://arxiv.org/abs/2510.25801</link>
<guid>https://arxiv.org/abs/2510.25801</guid>
<content:encoded><![CDATA[
arXiv:2510.25801v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a wave of "MLLM-r1" approaches that bring RL to vision language models. Most representative paradigms begin with a cold start, typically employing supervised fine-tuning (SFT), to initialize the policy before RL. However, SFT-based cold start adopts the reasoning paradigm intertwined with task solution and output format, which may induce instruction-style overfitting, weakens out-of-distribution generalization, and ultimately affects downstream RL. We revisit the cold start along two views, its training method and data construction, and introduce the Generalization Factor (GF) coefficient to quantify the generalization capability under different methods. Our empirical study finds that preference-based training methods (e.g. DPO) generalizes better than SFT-based methods in cold start. Motivated by this, we propose SPECS-a Self-distilled, Preference-based Cold Start framework that decouples multimodal learning: (1) generates introspective preference data pairs via self-distillation, avoiding reliance on larger teachers or manual annotation; (2) performs preference-based training to learn, focusing on shallow, transferable surface-form criteria (format, structure, style) rather than memorizing content; and (3) hands off to RL with verifiable rewards for deep reasoning results. Experimental results across multiple multimodal benchmarks show that our decoupling learning framework yields consistent performance gains over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%. Additional experiments indicate that SPECS contributes to reducing in-distribution "stuckness," improving exploration, stabilizing training, and raising the performance ceiling.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models</title>
<link>https://arxiv.org/abs/2511.05171</link>
<guid>https://arxiv.org/abs/2511.05171</guid>
<content:encoded><![CDATA[
arXiv:2511.05171v2 Announce Type: replace 
Abstract: Foundation models capable of generalizing across species and tasks represent a promising new frontier in bioacoustics, with NatureLM being one of the most prominent examples. While its domain-specific fine-tuning yields strong performance on bioacoustic benchmarks, we observe that it also introduces trade-offs in instruction-following flexibility. For instance, NatureLM achieves high accuracy when prompted for either the common or scientific name individually, but its accuracy drops significantly when both are requested in a single prompt. We address this by applying a simple model merging strategy that interpolates NatureLM with its base language model, recovering instruction-following capabilities with minimal loss of domain expertise. Finally, we show that the merged model exhibits markedly stronger zero-shot generalization, achieving over a 200% relative improvement and setting a new state-of-the-art in closed-set zero-shot classification of unseen species.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Models Got Talent: Identifying High Performing Wearable Human Activity Recognition Models Without Training</title>
<link>https://arxiv.org/abs/2511.06157</link>
<guid>https://arxiv.org/abs/2511.06157</guid>
<content:encoded><![CDATA[
arXiv:2511.06157v2 Announce Type: replace 
Abstract: A promising alternative to the computationally expensive Neural Architecture Search (NAS) involves the development of Zero Cost Proxies (ZCPs), which correlate well with trained performance, but can be computed through a single forward/backward pass on a randomly sampled batch of data. In this paper, we investigate the effectiveness of ZCPs for HAR on six benchmark datasets, and demonstrate that they discover network architectures that obtain within 5% of performance attained by full-scale training involving 1500 randomly sampled architectures. This results in substantial computational savings as high-performing architectures can be discovered with minimal training. Our experiments not only introduce ZCPs to sensor-based HAR, but also demonstrate that they are robust to data noise, further showcasing their suitability for practical scenarios.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planning in Branch-and-Bound: Model-Based Reinforcement Learning for Exact Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2511.09219</link>
<guid>https://arxiv.org/abs/2511.09219</guid>
<content:encoded><![CDATA[
arXiv:2511.09219v2 Announce Type: replace 
Abstract: Mixed-Integer Linear Programming (MILP) lies at the core of many real-world combinatorial optimization (CO) problems, traditionally solved by branch-and-bound (B&amp;B). A key driver influencing B&amp;B solvers efficiency is the variable selection heuristic that guides branching decisions. Looking to move beyond static, hand-crafted heuristics, recent work has explored adapting traditional reinforcement learning (RL) algorithms to the B&amp;B setting, aiming to learn branching strategies tailored to specific MILP distributions. In parallel, RL agents have achieved remarkable success in board games, a very specific type of combinatorial problems, by leveraging environment simulators to plan via Monte Carlo Tree Search (MCTS). Building on these developments, we introduce Plan-and-Branch-and-Bound (PlanB&amp;B), a model-based reinforcement learning (MBRL) agent that leverages a learned internal model of the B&amp;B dynamics to discover improved branching strategies. Computational experiments empirically validate our approach, with our MBRL branching agent outperforming previous state-of-the-art RL methods across four standard MILP benchmarks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Damped Proximal Augmented Lagrangian Method for weakly-Convex Problems with Convex Constraints</title>
<link>https://arxiv.org/abs/2311.09065</link>
<guid>https://arxiv.org/abs/2311.09065</guid>
<content:encoded><![CDATA[
arXiv:2311.09065v3 Announce Type: replace-cross 
Abstract: We give a damped proximal augmented Lagrangian method (DPALM) for solving problems with a weakly-convex objective and convex linear/nonlinear constraints. Instead of taking a full stepsize, DPALM adopts a damped dual stepsize to ensure the boundedness of dual iterates. We show that DPALM can produce a (near) $\vareps$-KKT point within $O(\vareps^{-2})$ outer iterations if each DPALM subproblem is solved to a proper accuracy. In addition, we establish overall iteration complexity of DPALM when the objective is either a regularized smooth function or in a regularized compositional form. For the former case, DPALM achieves the complexity of $\widetilde{\mathcal{O}}\left(\varepsilon^{-2.5} \right)$ to produce an $\varepsilon$-KKT point by applying an accelerated proximal gradient (APG) method to each DPALM subproblem. For the latter case, the complexity of DPALM is $\widetilde{\mathcal{O}}\left(\varepsilon^{-3} \right)$ to produce a near $\varepsilon$-KKT point by using an APG to solve a Moreau-envelope smoothed version of each subproblem. Our outer iteration complexity and the overall complexity either generalize existing best ones from unconstrained or linear-constrained problems to convex-constrained ones, or improve over the best-known results on solving the same-structured problems. Furthermore, numerical experiments on linearly/quadratically constrained non-convex quadratic programs and linear-constrained robust nonlinear least squares are conducted to demonstrate the empirical efficiency of the proposed DPALM over several state-of-the art methods.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast convergence of the Expectation Maximization algorithm under a logarithmic Sobolev inequality</title>
<link>https://arxiv.org/abs/2407.17949</link>
<guid>https://arxiv.org/abs/2407.17949</guid>
<content:encoded><![CDATA[
arXiv:2407.17949v2 Announce Type: replace-cross 
Abstract: We present a new framework for analysing the Expectation Maximization (EM) algorithm. Drawing on recent advances in the theory of gradient flows over Euclidean-Wasserstein spaces, we extend techniques from alternating minimization in Euclidean spaces to the EM algorithm, via its representation as coordinate-wise minimization of the free energy. In so doing, we obtain finite sample error bounds and exponential convergence of the EM algorithm under a natural generalisation of the log-Sobolev inequality. We further show that this framework naturally extends to several variants of EM, offering a unified approach for studying such algorithms.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoopSR: Looping Sim-and-Real for Lifelong Policy Adaptation of Legged Robots</title>
<link>https://arxiv.org/abs/2409.17992</link>
<guid>https://arxiv.org/abs/2409.17992</guid>
<content:encoded><![CDATA[
arXiv:2409.17992v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has shown its remarkable and generalizable capability in legged locomotion through sim-to-real transfer. However, while adaptive methods like domain randomization are expected to enhance policy robustness across diverse environments, they potentially compromise the policy's performance in any specific environment, leading to suboptimal real-world deployment due to the No Free Lunch theorem. To address this, we propose LoopSR, a lifelong policy adaptation framework that continuously refines RL policies in the post-deployment stage. LoopSR employs a transformer-based encoder to map real-world trajectories into a latent space and reconstruct a digital twin of the real world for further improvement. Autoencoder architecture and contrastive learning methods are adopted to enhance feature extraction of real-world dynamics. Simulation parameters for continual training are derived by combining predicted values from the decoder with retrieved parameters from a pre-collected simulation trajectory dataset. By leveraging simulated continual training, LoopSR achieves superior data efficiency compared with strong baselines, yielding eminent performance with limited data in both sim-to-sim and sim-to-real experiments. Please refer to https://peilinwu.site/looping-sim-and-real.github.io/ for videos and code.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIMeSynC: Temporal Intent Modelling with Synchronized Context Encodings for Financial Service Applications</title>
<link>https://arxiv.org/abs/2410.12825</link>
<guid>https://arxiv.org/abs/2410.12825</guid>
<content:encoded><![CDATA[
arXiv:2410.12825v4 Announce Type: replace-cross 
Abstract: Users engage with financial services companies through multiple channels, often interacting with mobile applications, web platforms, call centers, and physical locations to service their accounts. The resulting interactions are recorded at heterogeneous temporal resolutions across these domains. This multi-channel data can be combined and encoded to create a comprehensive representation of the customer's journey for accurate intent prediction. This demands sequential learning solutions. NMT transformers achieve state-of-the-art sequential representation learning by encoding context and decoding for the next best action to represent long-range dependencies. However, three major challenges exist while combining multi-domain sequences within an encoder-decoder transformers architecture for intent prediction applications: a) aligning sequences with different sampling rates b) learning temporal dynamics across multi-variate, multi-domain sequences c) combining dynamic and static sequences. We propose an encoder-decoder transformer model to address these challenges for contextual and sequential intent prediction in financial servicing applications. Our experiments show significant improvement over the existing tabular method.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Data Valuation via Asymmetric Data Shapley</title>
<link>https://arxiv.org/abs/2411.00388</link>
<guid>https://arxiv.org/abs/2411.00388</guid>
<content:encoded><![CDATA[
arXiv:2411.00388v3 Announce Type: replace-cross 
Abstract: As data emerges as a vital driver of technological and economic advancements, a key challenge is accurately quantifying its value in algorithmic decision-making. The Shapley value, a well-established concept from cooperative game theory, has been widely adopted to assess the contribution of individual data sources in supervised machine learning. However, its symmetry axiom assumes all players in the cooperative game are homogeneous, which overlooks the complex structures and dependencies present in real-world datasets. To address this limitation, we extend the traditional data Shapley framework to asymmetric data Shapley, making it flexible enough to incorporate inherent structures within the datasets for structure-aware data valuation. We also introduce an efficient $k$-nearest neighbor-based algorithm for its exact computation. We demonstrate the practical applicability of our framework across various machine learning tasks and data market contexts. The code is available at: https://github.com/xzheng01/Asymmetric-Data-Shapley.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Abnormality Prediction and Forecasting of Laboratory Values from Electrocardiogram Signals Using Multimodal Deep Learning</title>
<link>https://arxiv.org/abs/2411.14886</link>
<guid>https://arxiv.org/abs/2411.14886</guid>
<content:encoded><![CDATA[
arXiv:2411.14886v2 Announce Type: replace-cross 
Abstract: This study investigates the feasibility of using electrocardiogram (ECG) data combined with basic patient metadata to estimate and monitor prompt laboratory abnormalities. We use the MIMIC-IV dataset to train multimodal deep learning models on ECG waveforms, demographics, biometrics, and vital signs. Our model is a structured state space classifier with late fusion for metadata. We frame the task as individual binary classifications per abnormality and evaluate performance using AUROC. The models achieve strong performance, with AUROCs above 0.70 for 24 lab values in abnormality prediction and up to 24 in abnormality forecasting, across cardiac, renal, hematological, metabolic, immunological, and coagulation categories. NTproBNP (>353 pg/mL) is best predicted (AUROC > 0.90). Other values with AUROC > 0.85 include Hemoglobin (>17.5 g/dL), Albumin (>5.2 g/dL), and Hematocrit (>51%). Our findings show ECG combined with clinical data enables prompt abnormality prediction and forecasting of lab abnormalities, offering a non-invasive, cost-effective alternative to traditional testing. This can support early intervention and enhanced patient monitoring. ECG and clinical data can help estimate and monitor abnormal lab values, potentially improving care while reducing reliance on invasive and costly procedures.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Document Image Dewarping via Hybrid Deep Learning and Cubic Polynomial Geometry Restoration</title>
<link>https://arxiv.org/abs/2501.03145</link>
<guid>https://arxiv.org/abs/2501.03145</guid>
<content:encoded><![CDATA[
arXiv:2501.03145v3 Announce Type: replace-cross 
Abstract: Camera-captured document images often suffer from geometric distortions caused by paper deformation, perspective distortion, and lens aberrations, significantly reducing OCR accuracy. This study develops an efficient automated method for document image dewarping that balances accuracy with computational efficiency. We propose a hybrid approach combining deep learning for document detection with classical computer vision for geometry restoration. YOLOv8 performs initial document segmentation and mask generation. Subsequently, classical CV techniques construct a topological 2D grid through cubic polynomial interpolation of document boundaries, followed by image remapping to correct nonlinear distortions. A new annotated dataset and open-source framework are provided to facilitate reproducibility and further research. Experimental evaluation against state-of-the-art methods (RectiNet, DocGeoNet, DocTr++) and mobile applications (DocScan, CamScanner, TapScanner) demonstrates superior performance. Our method achieves the lowest median Character Error Rate (CER=0.0235), Levenshtein Distance (LD=27.8), and highest Jaro--Winkler similarity (JW=0.902), approaching the quality of scanned originals. The approach requires significantly fewer computational resources and memory compared to pure deep learning solutions while delivering better OCR readability and geometry restoration quality. The proposed hybrid methodology effectively restores document geometry with computational efficiency superior to existing deep learning approaches, making it suitable for resource-constrained applications while maintaining high-quality document digitization. Project page: https://github.com/HorizonParadox/DRCCBI
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiLaN: A Linear Latent Network as the Solution Operator for Real-Time Solutions to Stiff Nonlinear Ordinary Differential Equations</title>
<link>https://arxiv.org/abs/2501.08423</link>
<guid>https://arxiv.org/abs/2501.08423</guid>
<content:encoded><![CDATA[
arXiv:2501.08423v4 Announce Type: replace-cross 
Abstract: Solving stiff ordinary differential equations (StODEs) requires sophisticated numerical solvers, which are often computationally expensive. In general, traditional explicit time integration schemes with restricted time step sizes are not suitable for StODEs, and one must resort to costly implicit methods. On the other hand, state-of-the-art machine learning based methods, such as Neural ODE, poorly handle the timescale separation of various elements of the solutions to StODEs, while still requiring expensive implicit/explicit integration at inference time. In this work, we propose a linear latent network (LiLaN) approach in which the dynamics in the latent space can be integrated analytically, and thus numerical integration is completely avoided. At the heart of LiLaN are the following key ideas: i) two encoder networks to encode the initial condition together with parameters of the ODE to the slope and the initial condition for the latent dynamics, respectively. Since the latent dynamics, by design, are linear, the solution can be evaluated analytically; ii) a neural network to map the physical time to latent times, one for each latent variable. Finally, iii) a decoder network to decode the latent solution to the physical solution at the corresponding physical time. We provide a universal approximation theorem for the proposed LiLaN approach, showing that it can approximate the solution of any stiff nonlinear system on a compact set to any degree of accuracy epsilon. We also show an interesting fact that the dimension of the latent dynamical system in LiLaN is independent of epsilon. Numerical results on the "Robertson Stiff Chemical Kinetics Model," "Plasma Collisional-Radiative Model," and "Allen-Cahn" and "Cahn-Hilliard" PDEs suggest that LiLaN outperformed state-of-the-art machine learning approaches for handling stiff ordinary and partial differential equations.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model</title>
<link>https://arxiv.org/abs/2501.16226</link>
<guid>https://arxiv.org/abs/2501.16226</guid>
<content:encoded><![CDATA[
arXiv:2501.16226v4 Announce Type: replace-cross 
Abstract: Self-distillation (SD), a technique where a model improves itself using its own predictions, has attracted attention as a simple yet powerful approach in machine learning. Despite its widespread use, the mechanisms underlying its effectiveness remain unclear. In this study, we investigate the efficacy of hyperparameter-tuned multi-stage SD with a linear classifier for binary classification on noisy Gaussian mixture data. For the analysis, we employ the replica method from statistical physics. Our findings reveal that the primary driver of SD's performance improvement is denoising through hard pseudo-labels, with the most notable gains observed in moderately sized datasets. We also identify two practical heuristics to enhance SD: early stopping that limits the number of stages, which is broadly effective, and bias parameter fixing, which helps under label imbalance. To empirically validate our theoretical findings derived from our toy model, we conduct additional experiments on CIFAR-10 classification using pretrained ResNet backbone. These results provide both theoretical and practical insights, advancing our understanding and application of SD in noisy settings.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable and externally validated machine learning for neurocognitive diagnosis via electrocardiograms</title>
<link>https://arxiv.org/abs/2502.04918</link>
<guid>https://arxiv.org/abs/2502.04918</guid>
<content:encoded><![CDATA[
arXiv:2502.04918v2 Announce Type: replace-cross 
Abstract: Background: Electrocardiogram (ECG) analysis has emerged as a promising tool for detecting physiological changes linked to non-cardiac disorders. Given the close connection between cardiovascular and neurocognitive health, ECG abnormalities may be present in individuals with co-occurring neurocognitive conditions. This highlights the potential of ECG as a biomarker to improve detection, therapy monitoring, and risk stratification in patients with neurocognitive disorders, an area that remains underexplored.
  Methods: We aim to demonstrate the feasibility to predict neurocognitive disorders from ECG features across diverse patient populations. We utilized ECG features and demographic data to predict neurocognitive disorders defined by ICD-10 codes, focusing on dementia, delirium, and Parkinson's disease. Internal and external validations were performed using the MIMIC-IV and ECG-View datasets. Predictive performance was assessed using AUROC scores, and Shapley values were used to interpret feature contributions.
  Results: Significant predictive performance was observed for disorders within the neurcognitive disorders. Significantly, the disorders with the highest predictive performance is F03: Dementia, with an internal AUROC of 0.848 (95% CI: 0.848-0.848) and an external AUROC of 0.865 (0.864-0.965), followed by G30: Alzheimer's, with an internal AUROC of 0.809 (95% CI: 0.808-0.810) and an external AUROC of 0.863 (95% CI: 0.863-0.864). Feature importance analysis revealed both known and novel ECG correlates. ECGs hold promise as non-invasive, explainable biomarkers for selected neurocognitive disorders. This study demonstrates robust performance across cohorts and lays the groundwork for future clinical applications, including early detection and personalized monitoring.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrinsic Barriers and Practical Pathways for Human-AI Alignment: An Agreement-Based Complexity Analysis</title>
<link>https://arxiv.org/abs/2502.05934</link>
<guid>https://arxiv.org/abs/2502.05934</guid>
<content:encoded><![CDATA[
arXiv:2502.05934v3 Announce Type: replace-cross 
Abstract: We formalize AI alignment as a multi-objective optimization problem called $\langle M,N,\varepsilon,\delta\rangle$-agreement, in which a set of $N$ agents (including humans) must reach approximate ($\varepsilon$) agreement across $M$ candidate objectives, with probability at least $1-\delta$. Analyzing communication complexity, we prove an information-theoretic lower bound showing that once either $M$ or $N$ is large enough, no amount of computational power or rationality can avoid intrinsic alignment overheads. This establishes rigorous limits to alignment *itself*, not merely to particular methods, clarifying a "No-Free-Lunch" principle: encoding "all human values" is inherently intractable and must be managed through consensus-driven reduction or prioritization of objectives. Complementing this impossibility result, we construct explicit algorithms as achievability certificates for alignment under both unbounded and bounded rationality with noisy communication. Even in these best-case regimes, our bounded-agent and sampling analysis shows that with large task spaces ($D$) and finite samples, *reward hacking is globally inevitable*: rare high-loss states are systematically under-covered, implying scalable oversight must target safety-critical slices rather than uniform coverage. Together, these results identify fundamental complexity barriers -- tasks ($M$), agents ($N$), and state-space size ($D$) -- and offer principles for more scalable human-AI collaboration.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Retinal Disease Prediction Using Biology-Informed Heterogeneous Graph Representations</title>
<link>https://arxiv.org/abs/2502.16697</link>
<guid>https://arxiv.org/abs/2502.16697</guid>
<content:encoded><![CDATA[
arXiv:2502.16697v2 Announce Type: replace-cross 
Abstract: Interpretability is crucial to enhance trust in machine learning models for medical diagnostics. However, most state-of-the-art image classifiers based on neural networks are not interpretable. As a result, clinicians often resort to known biomarkers for diagnosis, although biomarker-based classification typically performs worse than large neural networks. This work proposes a method that surpasses the performance of established machine learning models while simultaneously improving prediction interpretability for diabetic retinopathy staging from optical coherence tomography angiography (OCTA) images. Our method is based on a novel biology-informed heterogeneous graph representation that models retinal vessel segments, intercapillary areas, and the foveal avascular zone (FAZ) in a human-interpretable way. This graph representation allows us to frame diabetic retinopathy staging as a graph-level classification task, which we solve using an efficient graph neural network. We benchmark our method against well-established baselines, including classical biomarker-based classifiers, convolutional neural networks (CNNs), and vision transformers. Our model outperforms all baselines on two datasets. Crucially, we use our biology-informed graph to provide explanations of unprecedented detail. Our approach surpasses existing methods in precisely localizing and identifying critical vessels or intercapillary areas. In addition, we give informative and human-interpretable attributions to critical characteristics. Our work contributes to the development of clinical decision-support tools in ophthalmology.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?</title>
<link>https://arxiv.org/abs/2503.02687</link>
<guid>https://arxiv.org/abs/2503.02687</guid>
<content:encoded><![CDATA[
arXiv:2503.02687v3 Announce Type: replace-cross 
Abstract: Due to the significant effort required for data collection and annotation in 3D perception tasks, mixed sample data augmentation (MSDA) has been widely studied to generate diverse training samples by mixing existing data. Recently, many MSDA techniques have been developed for point clouds, but they mainly target LiDAR data, leaving their application to radar point clouds largely unexplored. In this paper, we examine the feasibility of applying existing MSDA methods to radar point clouds and identify several challenges in adapting these techniques. These obstacles stem from the radar's irregular angular distribution, deviations from a single-sensor polar layout in multi-radar setups, and point sparsity. To address these issues, we propose Class-Aware PillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar level in 3D point clouds, guided by class labels. Unlike methods that rely a single mix ratio to the entire sample, CAPMix assigns an independent ratio to each pillar, boosting sample diversity. To account for the density of different classes, we use class-specific distributions: for dense objects (e.g., large vehicles), we skew ratios to favor points from another sample, while for sparse objects (e.g., pedestrians), we sample more points from the original. This class-aware mixing retains critical details and enriches each sample with new information, ultimately generating more diverse training data. Experimental results demonstrate that our method not only significantly boosts performance but also outperforms existing MSDA approaches across two datasets (Bosch Street and K-Radar). We believe that this straightforward yet effective approach will spark further investigation into MSDA techniques for radar data.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beacon2Science: Enhancing STEREO/HI beacon data with machine learning for efficient CME tracking</title>
<link>https://arxiv.org/abs/2503.15288</link>
<guid>https://arxiv.org/abs/2503.15288</guid>
<content:encoded><![CDATA[
arXiv:2503.15288v2 Announce Type: replace-cross 
Abstract: Observing and forecasting coronal mass ejections (CME) in real-time is crucial due to the strong geomagnetic storms they can generate that can have a potentially damaging effect, for example, on satellites and electrical devices. With its near-real-time availability, STEREO/HI beacon data is the perfect candidate for early forecasting of CMEs. However, previous work concluded that CME arrival prediction based on beacon data could not achieve the same accuracy as with high-resolution science data due to data gaps and lower quality. We present our novel machine-learning pipeline entitled ``Beacon2Science'', bridging the gap between beacon and science data to improve CME tracking. Through this pipeline, we first enhance the quality (signal-to-noise ratio and spatial resolution) of beacon data. We then increase the time resolution of enhanced beacon images through learned interpolation to match science data's 40-minute resolution. We maximize information coherence between consecutive frames with adapted model architecture and loss functions through the different steps. The improved beacon images are comparable to science data, showing better CME visibility than the original beacon data. Furthermore, we compare CMEs tracked in beacon, enhanced beacon, and science images. The tracks extracted from enhanced beacon data are closer to those from science images, with a mean average error of $\sim 0.5 ^\circ$ of elongation compared to $1^\circ$ with original beacon data. The work presented in this paper paves the way for its application to forthcoming missions such as Vigil and PUNCH.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Trust Calibration Maturity Model for Characterizing and Communicating Trustworthiness of AI Systems</title>
<link>https://arxiv.org/abs/2503.15511</link>
<guid>https://arxiv.org/abs/2503.15511</guid>
<content:encoded><![CDATA[
arXiv:2503.15511v3 Announce Type: replace-cross 
Abstract: Recent proliferation of powerful AI systems has created a strong need for capabilities that help users to calibrate trust in those systems. As AI systems grow in scale, information required to evaluate their trustworthiness becomes less accessible, presenting a growing risk of using these systems inappropriately. We propose the Trust Calibration Maturity Model (TCMM) to characterize and communicate information about AI system trustworthiness. The TCMM incorporates five dimensions of analytic maturity: Performance Characterization, Bias & Robustness Quantification, Transparency, Safety & Security, and Usability. The TCMM can be presented along with system performance information to (1) help a user to appropriately calibrate trust, (2) establish requirements and track progress, and (3) identify research needs. Here, we discuss the TCMM and demonstrate it on two target tasks: using ChatGPT for high consequence nuclear science determinations, and using PhaseNet (an ensemble of seismic models) for categorizing sources of seismic events.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Operator learning for energy-efficient building ventilation control with computational fluid dynamics simulation of a real-world classroom</title>
<link>https://arxiv.org/abs/2504.21243</link>
<guid>https://arxiv.org/abs/2504.21243</guid>
<content:encoded><![CDATA[
arXiv:2504.21243v2 Announce Type: replace-cross 
Abstract: Energy-efficient ventilation control plays a vital role in reducing building energy consumption while ensuring occupant health and comfort. While Computational Fluid Dynamics (CFD) simulations provide detailed and physically accurate representation of indoor airflow, their high computational cost limits their use in real-time building control. In this work, we present a neural operator learning framework that combines the physical accuracy of CFD with the computational efficiency of machine learning to enable building ventilation control with the high-fidelity fluid dynamics models. Our method jointly optimizes the airflow supply rates and vent angles to reduce energy use and adhere to air quality constraints. We train an ensemble of neural operator transformer models to learn the mapping from building control actions to airflow fields using high-resolution CFD data. This learned neural operator is then embedded in an optimization-based control framework for building ventilation control. Experimental results show that our approach achieves significant energy savings compared to maximum airflow rate control, rule-based control, as well as data-driven control methods using spatially averaged CO2 prediction and deep learning based reduced order model, while consistently maintaining safe indoor air quality. These results highlight the practicality and scalability of our method in maintaining energy efficiency and indoor air quality in real-world buildings.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models</title>
<link>https://arxiv.org/abs/2505.14160</link>
<guid>https://arxiv.org/abs/2505.14160</guid>
<content:encoded><![CDATA[
arXiv:2505.14160v4 Announce Type: replace-cross 
Abstract: Multilingual vision-language models (VLMs) promise universal image-text retrieval, yet their social biases remain underexplored. We perform the first systematic audit of four public multilingual CLIP variants: M-CLIP, NLLB-CLIP, CAPIVARA-CLIP, and the debiased SigLIP-2, covering ten languages that differ in resource availability and morphological gender marking. Using balanced subsets of FairFace and the PATA stereotype suite in a zero-shot setting, we quantify race and gender bias and measure stereotype amplification. Contrary to the intuition that multilinguality mitigates bias, every model exhibits stronger gender skew than its English-only baseline. CAPIVARA-CLIP shows its largest biases precisely in the low-resource languages it targets, while the shared encoder of NLLB-CLIP and SigLIP-2 transfers English gender stereotypes into gender-neutral languages; loosely coupled encoders largely avoid this leakage. Although SigLIP-2 reduces agency and communion skews, it inherits -- and in caption-sparse contexts (e.g., Xhosa) amplifies -- the English anchor's crime associations. Highly gendered languages consistently magnify all bias types, yet gender-neutral languages remain vulnerable whenever cross-lingual weight sharing imports foreign stereotypes. Aggregated metrics thus mask language-specific hot spots, underscoring the need for fine-grained, language-aware bias evaluation in future multilingual VLM research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPICEMixer - Netlist-Level Circuit Evolution</title>
<link>https://arxiv.org/abs/2506.01497</link>
<guid>https://arxiv.org/abs/2506.01497</guid>
<content:encoded><![CDATA[
arXiv:2506.01497v2 Announce Type: replace-cross 
Abstract: We present SPICEMixer, a genetic algorithm that synthesizes circuits by directly evolving SPICE netlists. SPICEMixer operates on individual netlist lines, making it compatible with arbitrary components and subcircuits and enabling general-purpose genetic operators: crossover, mutation, and pruning, all applied directly at the netlist level. To support these operators, we normalize each netlist by enforcing consistent net naming (inputs, outputs, supplies, and internal nets) and by sorting components and nets into a fixed order, so that similar circuit structures appear at similar line positions. This normalized netlist format improves the effectiveness of crossover, mutation, and pruning. We demonstrate SPICEMixer by synthesizing standard cells (e.g., NAND2 and latch) and by designing OpAmps that meet specified targets. Across tasks, SPICEMixer matches or exceeds recent synthesis methods while requiring substantially fewer simulations.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A meaningful prediction of functional decline in amyotrophic lateral sclerosis based on multi-event survival analysis</title>
<link>https://arxiv.org/abs/2506.02076</link>
<guid>https://arxiv.org/abs/2506.02076</guid>
<content:encoded><![CDATA[
arXiv:2506.02076v2 Announce Type: replace-cross 
Abstract: Amyotrophic lateral sclerosis (ALS) is a degenerative disorder of the motor neurons that causes progressive paralysis in patients. Current treatment options aim to prolong survival and improve quality of life. However, due to the heterogeneity of the disease, it is often difficult to determine the optimal time for potential therapies or medical interventions. In this study, we propose a novel method to predict the time until a patient with ALS experiences significant functional impairment (ALSFRS-R <= 2) for each of five common functions: speaking, swallowing, handwriting, walking, and breathing. We formulate this task as a multi-event survival problem and validate our approach in the PRO-ACT dataset (N = 3220) by training five covariate-based survival models to estimate the probability of each event over the 500 days following the baseline visit. We then predict five event-specific individual survival distributions (ISDs) for a patient, each providing an interpretable estimate of when that event is likely to occur. The results show that covariate-based models are superior to the Kaplan-Meier estimator at predicting time-to-event outcomes in the PRO-ACT dataset. Additionally, our method enables practitioners to make individual counterfactual predictions -- where certain covariates can be changed -- to estimate their effect on the predicted outcome. In this regard, we find that Riluzole has little or no impact on predicted functional decline. However, for patients with bulbar-onset ALS, our model predicts significantly shorter time-to-event estimates for loss of speech and swallowing function compared to patients with limb-onset ALS (log-rank p<0.001, Bonferroni-adjusted alpha=0.01). The proposed method can be applied to current clinical examination data to assess the risk of functional decline and thus allow more personalized treatment planning.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Representation Learning with Observational Grouping for CXR Classification</title>
<link>https://arxiv.org/abs/2506.20582</link>
<guid>https://arxiv.org/abs/2506.20582</guid>
<content:encoded><![CDATA[
arXiv:2506.20582v2 Announce Type: replace-cross 
Abstract: Identifiable causal representation learning seeks to uncover the true causal relationships underlying a data generation process. In medical imaging, this presents opportunities to improve the generalisability and robustness of task-specific latent features. This work introduces the concept of grouping observations to learn identifiable representations for disease classification in chest X-rays via an end-to-end framework. Our experiments demonstrate that these causal representations improve generalisability and robustness across multiple classification tasks when grouping is used to enforce invariance w.r.t race, sex, and imaging views.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Core Safety Values for Provably Corrigible Agents</title>
<link>https://arxiv.org/abs/2507.20964</link>
<guid>https://arxiv.org/abs/2507.20964</guid>
<content:encoded><![CDATA[
arXiv:2507.20964v2 Announce Type: replace-cross 
Abstract: We introduce the first complete formal solution to corrigibility in the off-switch game, with provable guarantees in multi-step, partially observed environments. Our framework consists of five *structurally separate* utility heads -- deference, switch-access preservation, truthfulness, low-impact behavior via a belief-based extension of Attainable Utility Preservation, and bounded task reward -- combined lexicographically by strict weight gaps. Theorem 1 proves exact single-round corrigibility in the partially observable off-switch game; Theorem 3 extends the guarantee to multi-step, self-spawning agents, showing that even if each head is *learned* to mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal, the probability of violating *any* safety property is bounded while still ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF, which merge all norms into one learned scalar, our separation makes obedience and impact-limits provably dominate even when incentives conflict. For settings where adversaries can modify the agent, we prove that deciding whether an arbitrary post-hack agent will ever violate corrigibility is undecidable by reduction to the halting problem, then carve out a finite-horizon "decidable island" where safety can be certified in randomized polynomial time and verified with privacy-preserving, constant-round zero-knowledge proofs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI for Curie Temperature Prediction in Magnetic Materials</title>
<link>https://arxiv.org/abs/2508.06996</link>
<guid>https://arxiv.org/abs/2508.06996</guid>
<content:encoded><![CDATA[
arXiv:2508.06996v3 Announce Type: replace-cross 
Abstract: We explore machine learning techniques for predicting Curie temperatures of magnetic materials using the NEMAD database. By augmenting the dataset with composition-based and domain-aware descriptors, we evaluate the performance of several machine learning models. We find that the Extra Trees Regressor delivers the best performance reaching an R^2 score of up to 0.85 $\pm$ 0.01 (cross-validated) for a balanced dataset. We employ the k-means clustering algorithm to gain insights into the performance of chemically distinct material groups. Furthermore, we perform the SHAP analysis to identify key physicochemical drivers of Curie behavior, such as average atomic number and magnetic moment. By employing explainable AI techniques, this analysis offers insights into the model's predictive behavior, thereby advancing scientific interpretability.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Statistical Fairness-Accuracy Frontier</title>
<link>https://arxiv.org/abs/2508.17622</link>
<guid>https://arxiv.org/abs/2508.17622</guid>
<content:encoded><![CDATA[
arXiv:2508.17622v2 Announce Type: replace-cross 
Abstract: Machine learning models must balance accuracy and fairness, but these goals often conflict, particularly when data come from multiple demographic groups. A useful tool for understanding this trade-off is the fairness-accuracy (FA) frontier, which characterizes the set of models that cannot be simultaneously improved in both fairness and accuracy. Prior analyses of the FA frontier provide a full characterization under the assumption of complete knowledge of population distributions -- an unrealistic ideal. We study the FA frontier in the finite-sample regime, showing how it deviates from its population counterpart and quantifying the worst-case gap between them. In particular, we derive minimax-optimal estimators that depend on the designer's knowledge of the covariate distribution. For each estimator, we characterize how finite-sample effects asymmetrically impact each group's risk, and identify optimal sample allocation strategies. Our results transform the FA frontier from a theoretical construct into a practical tool for policymakers and practitioners who must often design algorithms with limited data.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Imaging Inverse Problems Using Plug-and-Play Denoisers: Regularization and Optimization Perspectives</title>
<link>https://arxiv.org/abs/2509.03475</link>
<guid>https://arxiv.org/abs/2509.03475</guid>
<content:encoded><![CDATA[
arXiv:2509.03475v2 Announce Type: replace-cross 
Abstract: Inverse problems lie at the heart of modern imaging science, with broad applications in areas such as medical imaging, remote sensing, and microscopy. Recent years have witnessed a paradigm shift in solving imaging inverse problems, where data-driven regularizers are used increasingly, leading to remarkably high-fidelity reconstruction. A particularly notable approach for data-driven regularization is to use learned image denoisers as implicit priors in iterative image reconstruction algorithms. This chapter presents a comprehensive overview of this powerful and emerging class of algorithms, commonly referred to as plug-and-play (PnP) methods. We begin by providing a brief background on image denoising and inverse problems, followed by a short review of traditional regularization strategies. We then explore how proximal splitting algorithms, such as the alternating direction method of multipliers (ADMM) and proximal gradient descent (PGD), can naturally accommodate learned denoisers in place of proximal operators, and under what conditions such replacements preserve convergence. The role of Tweedie's formula in connecting optimal Gaussian denoisers and score estimation is discussed, which lays the foundation for regularization-by-denoising (RED) and more recent diffusion-based posterior sampling methods. We discuss theoretical advances regarding the convergence of PnP algorithms, both within the RED and proximal settings, emphasizing the structural assumptions that the denoiser must satisfy for convergence, such as non-expansiveness, Lipschitz continuity, and local homogeneity. We also address practical considerations in algorithm design, including choices of denoiser architecture and acceleration strategies.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s</title>
<link>https://arxiv.org/abs/2509.07928</link>
<guid>https://arxiv.org/abs/2509.07928</guid>
<content:encoded><![CDATA[
arXiv:2509.07928v2 Announce Type: replace-cross 
Abstract: As local AI grows in popularity, there is a critical gap between the benchmark performance of object detectors and their practical viability on consumer-grade hardware. While models like YOLOv10s promise real-time speeds, these metrics are typically achieved on high-power, desktop-class GPUs. This paper reveals that on resource-constrained systems, such as laptops with RTX 4060 GPUs, performance is not compute-bound but is instead dominated by system-level bottlenecks, as illustrated by a simple bottleneck test. To overcome this hardware-level constraint, we introduce a Two-Pass Adaptive Inference algorithm, a model-independent approach that requires no architectural changes. This study mainly focuses on adaptive inference strategies and undertakes a comparative analysis of architectural early-exit and resolution-adaptive routing, highlighting their respective trade-offs within a unified evaluation framework. The system uses a fast, low-resolution pass and only escalates to a high-resolution model pass when detection confidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x speedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%. This work provides a practical and reproducible blueprint for deploying high-performance, real-time AI on consumer-grade devices by shifting the focus from pure model optimization to hardware-aware inference strategies that maximize throughput.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias after Prompting: Persistent Discrimination in Large Language Models</title>
<link>https://arxiv.org/abs/2509.08146</link>
<guid>https://arxiv.org/abs/2509.08146</guid>
<content:encoded><![CDATA[
arXiv:2509.08146v2 Announce Type: replace-cross 
Abstract: A dangerous assumption that can be made from prior work on the bias transfer hypothesis (BTH) is that biases do not transfer from pre-trained large language models (LLMs) to adapted models. We invalidate this assumption by studying the BTH in causal models under prompt adaptations, as prompting is an extremely popular and accessible adaptation strategy used in real-world applications. In contrast to prior work, we find that biases can transfer through prompting and that popular prompt-based mitigation methods do not consistently prevent biases from transferring. Specifically, the correlation between intrinsic biases and those after prompt adaptation remain moderate to strong across demographics and tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age (rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we find that biases remain strongly correlated when varying few-shot composition parameters, such as sample size, stereotypical content, occupational distribution and representational balance (rho >= 0.90). We evaluate several prompt-based debiasing strategies and find that different approaches have distinct strengths, but none consistently reduce bias transfer across models, tasks or demographics. These results demonstrate that correcting bias, and potentially improving reasoning ability, in intrinsic models may prevent propagation of biases to downstream tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overfitting in Adaptive Robust Optimization</title>
<link>https://arxiv.org/abs/2509.16451</link>
<guid>https://arxiv.org/abs/2509.16451</guid>
<content:encoded><![CDATA[
arXiv:2509.16451v3 Announce Type: replace-cross 
Abstract: Adaptive robust optimization (ARO) extends static robust optimization by allowing decisions to depend on the realized uncertainty - weakly dominating static solutions within the modeled uncertainty set. However, ARO makes previous constraints that were independent of uncertainty now dependent, making it vulnerable to additional infeasibilities when realizations fall outside the uncertainty set. This phenomenon of adaptive policies being brittle is analogous to overfitting in machine learning. To mitigate against this, we propose assigning constraint-specific uncertainty set sizes, with harder constraints given stronger probabilistic guarantees. Interpreted through the overfitting lens, this acts as regularization: tighter guarantees shrink adaptive coefficients to ensure stability, while looser ones preserve useful flexibility. This view motivates a principled approach to designing uncertainty sets that balances robustness and adaptivity.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks</title>
<link>https://arxiv.org/abs/2509.24473</link>
<guid>https://arxiv.org/abs/2509.24473</guid>
<content:encoded><![CDATA[
arXiv:2509.24473v3 Announce Type: replace-cross 
Abstract: Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs). To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. Furthermore, to enable the model to learn and apply Euclidean principles from these geometry problems, we fine-tuned seven model variants (spanning 3--72B parameters) from the Qwen2.5VL, Qwen3VL, and RoboBrain2.0 families using Group Relative Policy Optimization (GRPO), inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy rose from 36.6\% to 41.8\% (+5.2\%), and the mean MindCube accuracy rose from 31.4\% to 38.1\% (+6.7\%). To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in \href{https://zgca-ai4edu.github.io/Euclids_Gift}{this}.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.14830</link>
<guid>https://arxiv.org/abs/2510.14830</guid>
<content:encoded><![CDATA[
arXiv:2510.14830v3 Announce Type: replace-cross 
Abstract: Real-world robotic manipulation in homes and factories demands reliability, efficiency, and robustness that approach or surpass the performance of skilled human operators. We present RL-100, a real-world reinforcement learning framework built on diffusion-based visuomotor policies. RL-100 unifies imitation and reinforcement learning under a single PPO-style objective applied within the denoising process, yielding conservative and stable policy improvements across both offline and online stages. To meet deployment latency constraints, we employ a lightweight consistency distillation procedure that compresses multi-step diffusion into a one-step controller for high-frequency control. The framework is task-, embodiment-, and representation-agnostic, and supports both single-action outputs and action-chunking control. We evaluate RL-100 on seven diverse real-robot manipulation tasks, ranging from dynamic pushing and agile bowling to pouring, cloth folding, unscrewing, and multi-stage juicing. RL-100 attains 100% success across evaluated trials, achieving 900 out of 900 successful episodes, including up to 250 out of 250 consecutive trials on one task, and matches or surpasses expert teleoperators in time-to-completion. Without retraining, a single policy attains approximately 90% zero-shot success under environmental and dynamics shifts, adapts in a few-shot regime to significant task variations (86.7%), and remains robust to aggressive human perturbations (about 95%). In a public shopping-mall deployment, the juicing robot served random customers continuously for roughly seven hours without failure. Together, these results suggest a practical path toward deployment-ready robot learning: start from human priors, align training objectives with human-grounded metrics, and reliably extend performance beyond human demonstrations.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch</title>
<link>https://arxiv.org/abs/2510.16088</link>
<guid>https://arxiv.org/abs/2510.16088</guid>
<content:encoded><![CDATA[
arXiv:2510.16088v3 Announce Type: replace-cross 
Abstract: Quantization of neural networks provides benefits of inference in less compute and memory requirements. Previous work in quantization lack two important aspects which this work provides. First almost all previous work in quantization used a non-differentiable approach and for learning; the derivative is usually set manually in backpropogation which make the learning ability of algorithm questionable, our approach is not just differentiable, we also provide proof of convergence of our approach to the optimal neural network. Second previous work in shift/logrithmic quantization either have avoided activation quantization along with weight quantization or achieved less accuracy. Learning logrithmic quantize values of form $2^n$ requires the quantization function can scale to more than 1 bit quantization which is another benifit of our quantization that it provides $n$ bits quantization as well. Our approach when tested with image classification task using imagenet dataset, resnet18 and weight quantization only achieves less than 1 percent accuracy compared to full precision accuracy while taking only 15 epochs to train using shift bit quantization and achieves comparable to SOTA approaches accuracy in both weight and activation quantization using shift bit quantization in 15 training epochs with slightly higher(only higher cpu instructions) inference cost compared to 1 bit quantization(without logrithmic quantization) and not requiring any higher precision multiplication.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains</title>
<link>https://arxiv.org/abs/2510.17793</link>
<guid>https://arxiv.org/abs/2510.17793</guid>
<content:encoded><![CDATA[
arXiv:2510.17793v2 Announce Type: replace-cross 
Abstract: Finetuning specialized generative evaluators has emerged as a popular paradigm to meet the increasing demand for scalable evaluation during both training and test-time. However, recent work has largely focused on applying new methodology, such as reinforcement learning (RL), to training evaluators, shying away from large-scale, data-driven development. In this work, we focus on data scaling, curating a set of 2.5M samples spanning five unique evaluation tasks (pairwise, step-level, reference-free and reference-based verification, and single rating) and multiple domains focused on reasoning evaluation. With our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges larger specialized RL-trained evaluators and FARE-20B sets the new standard for open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers, FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training, FARE improves the downstream RL-trained model performance by up to 14.1% vs. string-matching verifiers. When initialized from FARE, a continually-finetuned FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairJudge: MLLM Judging for Social Attributes and Prompt Image Alignment</title>
<link>https://arxiv.org/abs/2510.22827</link>
<guid>https://arxiv.org/abs/2510.22827</guid>
<content:encoded><![CDATA[
arXiv:2510.22827v2 Announce Type: replace-cross 
Abstract: Text-to-image (T2I) systems lack simple, reproducible ways to evaluate how well images match prompts and how models treat social attributes. Common proxies -- face classifiers and contrastive similarity -- reward surface cues, lack calibrated abstention, and miss attributes only weakly visible (for example, religion, culture, disability). We present FairJudge, a lightweight protocol that treats instruction-following multimodal LLMs as fair judges. It scores alignment with an explanation-oriented rubric mapped to [-1, 1]; constrains judgments to a closed label set; requires evidence grounded in the visible content; and mandates abstention when cues are insufficient. Unlike CLIP-only pipelines, FairJudge yields accountable, evidence-aware decisions; unlike mitigation that alters generators, it targets evaluation fairness. We evaluate gender, race, and age on FairFace, PaTA, and FairCoT; extend to religion, culture, and disability; and assess profession correctness and alignment on IdenProf, FairCoT-Professions, and our new DIVERSIFY-Professions. We also release DIVERSIFY, a 469-image corpus of diverse, non-iconic scenes. Across datasets, judge models outperform contrastive and face-centric baselines on demographic prediction and improve mean alignment while maintaining high profession accuracy, enabling more reliable, reproducible fairness audits.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays</title>
<link>https://arxiv.org/abs/2510.22830</link>
<guid>https://arxiv.org/abs/2510.22830</guid>
<content:encoded><![CDATA[
arXiv:2510.22830v4 Announce Type: replace-cross 
Abstract: BERT and its variants are extensively explored for automated scoring. However, a limit of 512 tokens for these encoder-based models showed the deficiency in automated scoring of long essays. Thus, this research explores generative language models for automated scoring of long essays via summarization and prompting. The results revealed great improvement of scoring accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab Automated Essay Scoring 2.0 dataset.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward precision soil health: A regional framework for site-specific management across Missouri</title>
<link>https://arxiv.org/abs/2510.26815</link>
<guid>https://arxiv.org/abs/2510.26815</guid>
<content:encoded><![CDATA[
arXiv:2510.26815v2 Announce Type: replace-cross 
Abstract: Effective soil health management is crucial for sustaining agriculture, adopting ecosystem resilience, and preserving water quality. However, Missouri's diverse landscapes limit the effectiveness of broad generalized management recommendations. The lack of resolution in existing soil grouping systems necessitates data driven, site specific insights to guide tailored interventions. To address these critical challenges, a regional soil clustering framework designed to support precision soil health management strategies across the state. The methodology leveraged high resolution SSURGO dataset, explicitly processing soil properties aggregated across the 0 to 30 cm root zone. Multivariate analysis incorporating a variational autoencoder and KMeans clustering was used to group soils with similar properties. The derived clusters were validated using statistical metrics, including silhouette scores and checks against existing taxonomic units, to confirm their spatial coherence. This approach enabled us to delineate soil groups that capture textures, hydraulic properties, chemical fertility, and biological indicators unique to Missouri's diverse agroecological regions. The clustering map identified ten distinct soil health management zones. This alignment of 10 clusters was selected as optimal because it was sufficiently large to capture inherited soil patterns while remaining manageable for practical statewide application. Rooting depth limitation and saturated hydraulic conductivity emerged as principal variables driving soil differentiation. Each management zone is defined by a unique combination of clay, organic matter, pH, and available water capacity. This framework bridges sophisticated data analysis with actionable, site targeted recommendations, enabling conservation planners, and agronomists to optimize management practices and enhance resource efficiency statewide.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SRNN: Spatiotemporal Relational Neural Network for Intuitive Physics Understanding</title>
<link>https://arxiv.org/abs/2511.06761</link>
<guid>https://arxiv.org/abs/2511.06761</guid>
<content:encoded><![CDATA[
arXiv:2511.06761v2 Announce Type: replace-cross 
Abstract: Human prowess in intuitive physics remains unmatched by machines. To bridge this gap, we argue for a fundamental shift towards brain-inspired computational principles. This paper introduces the Spatiotemporal Relational Neural Network (SRNN), a model that establishes a unified neural representation for object attributes, relations, and timeline, with computations governed by a Hebbian ``Fire Together, Wire Together'' mechanism across dedicated \textit{What} and \textit{How} pathways. This unified representation is directly used to generate structured linguistic descriptions of the visual scene, bridging perception and language within a shared neural substrate. On the CLEVRER benchmark, SRNN achieves competitive performance, thereby confirming its capability to represent essential spatiotemporal relations from the visual stream. Cognitive ablation analysis further reveals a benchmark bias, outlining a path for a more holistic evaluation. Finally, the white-box nature of SRNN enables precise pinpointing of error root causes. Our work provides a proof-of-concept that confirms the viability of translating key principles of biological intelligence into engineered systems for intuitive physics understanding in constrained environments.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cortex AISQL: A Production SQL Engine for Unstructured Data</title>
<link>https://arxiv.org/abs/2511.07663</link>
<guid>https://arxiv.org/abs/2511.07663</guid>
<content:encoded><![CDATA[
arXiv:2511.07663v2 Announce Type: replace-cross 
Abstract: Snowflake's Cortex AISQL is a production SQL engine that integrates native semantic operations directly into SQL. This integration allows users to write declarative queries that combine relational operations with semantic reasoning, enabling them to query both structured and unstructured data effortlessly. However, making semantic operations efficient at production scale poses fundamental challenges. Semantic operations are more expensive than traditional SQL operations, possess distinct latency and throughput characteristics, and their cost and selectivity are unknown during query compilation. Furthermore, existing query engines are not designed to optimize semantic operations. The AISQL query execution engine addresses these challenges through three novel techniques informed by production deployment data from Snowflake customers. First, AI-aware query optimization treats AI inference cost as a first-class optimization objective, reasoning about large language model (LLM) cost directly during query planning to achieve 2-8$\times$ speedups. Second, adaptive model cascades reduce inference costs by routing most rows through a fast proxy model while escalating uncertain cases to a powerful oracle model, achieving 2-6$\times$ speedups while maintaining 90-95% of oracle model quality. Third, semantic join query rewriting lowers the quadratic time complexity of join operations to linear through reformulation as multi-label classification tasks, achieving 15-70$\times$ speedups with often improved prediction quality. AISQL is deployed in production at Snowflake, where it powers diverse customer workloads across analytics, search, and content understanding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal control of the future via prospective learning with control</title>
<link>https://arxiv.org/abs/2511.08717</link>
<guid>https://arxiv.org/abs/2511.08717</guid>
<content:encoded><![CDATA[
arXiv:2511.08717v2 Announce Type: replace-cross 
Abstract: Optimal control of the future is the next frontier for AI. Current approaches to this problem are typically rooted in either reinforcement learning (RL). While powerful, this learning framework is mathematically distinct from supervised learning, which has been the main workhorse for the recent achievements in AI. Moreover, RL typically operates in a stationary environment with episodic resets, limiting its utility to more realistic settings. Here, we extend supervised learning to address learning to control in non-stationary, reset-free environments. Using this framework, called ''Prospective Learning with Control (PL+C)'', we prove that under certain fairly general assumptions, empirical risk minimization (ERM) asymptotically achieves the Bayes optimal policy. We then consider a specific instance of prospective learning with control, foraging -- which is a canonical task for any mobile agent -- be it natural or artificial. We illustrate that modern RL algorithms fail to learn in these non-stationary reset-free environments, and even with modifications, they are orders of magnitude less efficient than our prospective foraging agents.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Convergence of Four-Layer Matrix Factorization under Random Initialization</title>
<link>https://arxiv.org/abs/2511.09925</link>
<guid>https://arxiv.org/abs/2511.09925</guid>
<content:encoded><![CDATA[
arXiv:2511.09925v2 Announce Type: replace-cross 
Abstract: Gradient descent dynamics on the deep matrix factorization problem is extensively studied as a simplified theoretical model for deep neural networks. Although the convergence theory for two-layer matrix factorization is well-established, no global convergence guarantee for general deep matrix factorization under random initialization has been established to date. To address this gap, we provide a polynomial-time global convergence guarantee for randomly initialized gradient descent on four-layer matrix factorization, given certain conditions on the target matrix and a standard balanced regularization term. Our analysis employs new techniques to show saddle-avoidance properties of gradient decent dynamics, and extends previous theories to characterize the change in eigenvalues of layer weights.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How does My Model Fail? Automatic Identification and Interpretation of Physical Plausibility Failure Modes with Matryoshka Transcoders</title>
<link>https://arxiv.org/abs/2511.10094</link>
<guid>https://arxiv.org/abs/2511.10094</guid>
<content:encoded><![CDATA[
<div> Keywords: physical plausibility, generative models, Matryoshka Transcoders, hierarchical feature learning, model evaluation<br /><br />Summary:<br /><br />1. Recent generative models demonstrate impressive instruction-following and realistic output generation but often fail to maintain physical plausibility in their creations.<br /><br />2. Existing evaluation techniques frequently overlook these physical plausibility errors, and there is a lack of automated frameworks to identify and interpret specific physical error patterns in natural language.<br /><br />3. The authors propose Matryoshka Transcoders, a novel framework that builds on the Matryoshka representation learning paradigm by applying transcoder architectures to achieve hierarchical sparse feature learning at multiple levels of granularity.<br /><br />4. By training on intermediate representations from a physical plausibility classifier and employing large multimodal models for interpretation, their method automatically discovers diverse physics-related failure modes without manual feature engineering, outperforming existing approaches in feature relevance and accuracy.<br /><br />5. The visual patterns uncovered by the framework form the basis of a new benchmark to evaluate physical plausibility in generative models, which the authors use to analyze eight state-of-the-art models, revealing common failure modes and informing future improvements. <div>
arXiv:2511.10094v2 Announce Type: replace 
Abstract: Although recent generative models are remarkably capable of producing instruction-following and realistic outputs, they remain prone to notable physical plausibility failures. Though critical in applications, these physical plausibility errors often escape detection by existing evaluation methods. Furthermore, no framework exists for automatically identifying and interpreting specific physical error patterns in natural language, preventing targeted model improvements. We introduce Matryoshka Transcoders, a novel framework for the automatic discovery and interpretation of physical plausibility features in generative models. Our approach extends the Matryoshka representation learning paradigm to transcoder architectures, enabling hierarchical sparse feature learning at multiple granularity levels. By training on intermediate representations from a physical plausibility classifier and leveraging large multimodal models for interpretation, our method identifies diverse physics-related failure modes without manual feature engineering, achieving superior feature relevance and feature accuracy compared to existing approaches. We utilize the discovered visual patterns to establish a benchmark for evaluating physical plausibility in generative models. Our analysis of eight state-of-the-art generative models provides valuable insights into how these models fail to follow physical constraints, paving the way for further model improvements.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising</title>
<link>https://arxiv.org/abs/2511.11305</link>
<guid>https://arxiv.org/abs/2511.11305</guid>
<content:encoded><![CDATA[
<div> Multimodal learning, e-commerce, CTR prediction, training paradigm, scaling laws<br /><br />Summary:<br /><br />This paper presents MOON, a comprehensive set of sustainable iterative practices for multimodal representation learning specifically tailored for e-commerce applications. MOON has been fully deployed in Taobao‚Äôs search advertising system across various stages such as retrieval, relevance, and ranking. The framework yields significant performance improvements, notably achieving a +20.00% increase in online click-through rate (CTR) prediction. Over three years, MOON has undergone five full-scale iterations, delivering the largest CTR improvements in this context and accumulating valuable insights. The approach employs a three-stage training paradigm ‚Äî Pretraining, Post-training, and Application ‚Äî to effectively integrate multimodal representations with downstream tasks. To better align multimodal learning objectives with downstream goals, the authors introduce the concept of "exchange rate," quantifying the translation of intermediate metric improvements into downstream performance gains. They identify image-based search recall as a key intermediate metric guiding optimization. MOON‚Äôs development focuses on four critical dimensions: data processing, training strategy, model architecture, and downstream application, with iterative lessons shared. Additionally, the study explores scaling laws in multimodal representation learning, systematically examining factors including the number of training tokens, negative samples, and user behavior sequence length in e-commerce scenarios. <div>
arXiv:2511.11305v2 Announce Type: replace-cross 
Abstract: We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of "Pretraining, Post-training, and Application", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation</title>
<link>https://arxiv.org/abs/2511.11522</link>
<guid>https://arxiv.org/abs/2511.11522</guid>
<content:encoded><![CDATA[
<div> Keywords: Chess, CVChess, deep learning, Forsyth-Edwards Notation, CNN  

<br /><br />Summary:  
Chess has gained significant popularity due to the pandemic, largely enhanced by online learning resources. However, a gap exists between digital and physical chess experiences. To bridge this divide, the paper introduces CVChess, a deep learning framework capable of converting chessboard images to Forsyth-Edwards Notation (FEN), which is essential for utilizing online chess engines for move suggestions. The approach utilizes a convolutional neural network (CNN) with residual layers tailored for piece recognition from smartphone images. The process incorporates several steps: preprocessing the image with the Hough Line Transform for edge detection, employing projective transformation for proper board alignment, segmenting the board into 64 individual squares, and classifying each square into one of 13 classes‚Äîrepresenting the 12 unique pieces (6 white and 6 black) plus an empty square‚Äîusing the residual CNN. The residual connections enable better retention of low-level features while facilitating deeper feature extraction, enhancing accuracy and stability. The model is trained and evaluated on the Chess Recognition Dataset (ChessReD), featuring 10,800 annotated images captured under varying lighting conditions and angles, ultimately leading to FEN encoding for optimal chess engine move generation. <div>
arXiv:2511.11522v3 Announce Type: replace-cross 
Abstract: Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms. However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences. This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move. Our approach employs a convolutional neural network (CNN) with residual layers to perform piece recognition from smartphone camera images. The system processes RGB images of a physical chess board through a multistep process: image preprocessing using the Hough Line Transform for edge detection, projective transform to achieve a top-down board alignment, segmentation into 64 individual squares, and piece classification into 13 classes (6 unique white pieces, 6 unique black pieces and an empty square) using the residual CNN. Residual connections help retain low-level visual features while enabling deeper feature extraction, improving accuracy and stability during training. We train and evaluate our model using the Chess Recognition Dataset (ChessReD), containing 10,800 annotated smartphone images captured under diverse lighting conditions and angles. The resulting classifications are encoded as an FEN string, which can be fed into a chess engine to generate the most optimal move
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extended Physics Informed Neural Network for Hyperbolic Two-Phase Flow in Porous Media</title>
<link>https://arxiv.org/abs/2511.13734</link>
<guid>https://arxiv.org/abs/2511.13734</guid>
<content:encoded><![CDATA[
<div> Keywords: Nonlinear PDEs, Physics-Informed Neural Networks, Buckley-Leverett equation, XPINN framework, two-phase flow

<br /><br />Summary: The accurate solution of nonlinear hyperbolic partial differential equations (PDEs) presents significant challenges in computational science, especially due to steep gradients and discontinuities. Physics-Informed Neural Networks (PINNs) have emerged as a meshed-based solution method, but they often struggle with capturing complex wave interactions. To overcome these limitations, the study introduces the Extended Physics-Informed Neural Network (XPINN) framework, which is applied to the nonlinear Buckley-Leverett equation modeling immiscible two-phase flow in porous media. This approach dynamically decomposes the computational domain into pre-shock and post-shock regions, allowing for localized subnetworks to learn distinct behaviors efficiently. Coupling between these subnetworks is established via the Rankine-Hugoniot jump condition, ensuring flux continuity across shock interfaces. Numerical experiments show that XPINN effectively captures discontinuous saturation fronts and complex wave interactions without needing artificial diffusion or entropy corrections. Additionally, the XPINN framework exhibits superior stability, quicker convergence, and better resolution of nonlinear wave dynamics while using smaller domain-specific models with fewer parameters. This makes it a scalable tool for tackling challenging hyperbolic PDEs in multiphase flow problems. The code is accessible on GitHub. <div>
arXiv:2511.13734v1 Announce Type: new 
Abstract: The accurate solution of nonlinear hyperbolic partial differential equations (PDEs) remains a central challenge in computational science due to the presence of steep gradients, discontinuities, and multiscale structures that make conventional discretization-based solvers computationally demanding. Physics-Informed Neural Networks (PINNs) embed the governing equations into the learning process, enabling mesh-free solution of PDEs, yet they often struggle to capture steep gradients, discontinuities, and complex nonlinear wave interactions. To address these limitations, this study employs the Extended Physics-Informed Neural Network (XPINN) framework to solve the nonlinear Buckley-Leverett equation with a nonconvex flux function, which models immiscible two-phase flow in porous media. The computational domain is dynamically decomposed in space and time into evolving pre-shock and post-shock regions, allowing localized subnetworks to efficiently learn distinct flow behaviors. Coupling between subnetworks is achieved through the Rankine-Hugoniot jump condition, which enforces physically consistent flux continuity across the moving shock interface. Numerical experiments demonstrate that the proposed XPINN approach accurately captures discontinuous saturation fronts and compound wave interactions without requiring artificial diffusion or entropy corrections. Compared to standard PINNs, the XPINN framework achieves superior stability, faster convergence, and enhanced resolution of nonlinear wave dynamics using smaller, domain-specific models with fewer trainable parameters, establishing it as an effective and scalable tool for solving challenging hyperbolic PDEs in multiphase flow problems. The code of this work is available on github.com/saifkhanengr/XPINN-for-Buckley-Leverett.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blurred Encoding for Trajectory Representation Learning</title>
<link>https://arxiv.org/abs/2511.13741</link>
<guid>https://arxiv.org/abs/2511.13741</guid>
<content:encoded><![CDATA[
<div> Keywords: Trajectory Representation Learning, BLUrred Encoding, GPS trajectories, Transformer, Hierarchical patches

<br /><br />Summary: Trajectory representation learning (TRL) transforms raw GPS trajectories into vector embeddings for tasks like classification and similarity search. While state-of-the-art (SOTA) TRL methods capture high-level travel semantics by using grid or road trajectories, they tend to lose fine-grained spatial-temporal details due to grouping multiple GPS points. To address this issue, the proposed BLUrred Encoding method (BLUE) hierarchically reduces GPS coordinate precision, creating small low-level patches that retain detailed information and larger high-level patches that capture overall travel patterns. BLUE employs an encoder-decoder architecture with a pyramid structure, utilizing Transformers to learn trajectory embeddings at various patch levels. Pooling prepares inputs for higher levels in the encoder, while up-resolution guides lower levels in the decoder. The model is trained using a trajectory reconstruction task with Mean Squared Error (MSE) loss. Comparisons show that BLUE outperforms eight SOTA TRL methods across three downstream tasks, achieving an average accuracy improvement of 30.90% over the best-performing baselines. The implementation code is accessible at GitHub. <div>
arXiv:2511.13741v1 Announce Type: new 
Abstract: Trajectory representation learning (TRL) maps trajectories to vector embeddings and facilitates tasks such as trajectory classification and similarity search. State-of-the-art (SOTA) TRL methods transform raw GPS trajectories to grid or road trajectories to capture high-level travel semantics, i.e., regions and roads. However, they lose fine-grained spatial-temporal details as multiple GPS points are grouped into a single grid cell or road segment. To tackle this problem, we propose the BLUrred Encoding method, dubbed BLUE, which gradually reduces the precision of GPS coordinates to create hierarchical patches with multiple levels. The low-level patches are small and preserve fine-grained spatial-temporal details, while the high-level patches are large and capture overall travel patterns. To complement different patch levels with each other, our BLUE is an encoder-decoder model with a pyramid structure. At each patch level, a Transformer is used to learn the trajectory embedding at the current level, while pooling prepares inputs for the higher level in the encoder, and up-resolution provides guidance for the lower level in the decoder. BLUE is trained using the trajectory reconstruction task with the MSE loss. We compare BLUE with 8 SOTA TRL methods for 3 downstream tasks, the results show that BLUE consistently achieves higher accuracy than all baselines, outperforming the best-performing baselines by an average of 30.90%. Our code is available at https://github.com/slzhou-xy/BLUE.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepDefense: Layer-Wise Gradient-Feature Alignment for Building Robust Neural Networks</title>
<link>https://arxiv.org/abs/2511.13749</link>
<guid>https://arxiv.org/abs/2511.13749</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, adversarial attacks, Gradient-Feature Alignment, robustness, CIFAR-10

<br /><br />Summary: Deep neural networks are susceptible to adversarial perturbations, which are small, intentionally crafted changes that lead to incorrect predictions. This paper introduces DeepDefense, a defense framework that utilizes Gradient-Feature Alignment (GFA) regularization across multiple layers to reduce adversarial vulnerability. DeepDefense works by aligning input gradients with internal feature representations, creating a smoother loss landscape in tangential directions and thus decreasing sensitivity to adversarial noise. The authors provide theoretical insights explaining how adversarial perturbation can be decomposed into radial and tangential components, detailing the effectiveness of alignment in suppressing loss variation in tangential directions where attacks are most potent. Empirical results demonstrate that models trained with DeepDefense significantly improve robustness against gradient-based and optimization-based attacks. For example, on the CIFAR-10 dataset, CNN models using DeepDefense outperform standard adversarial training by up to 15.2% against APGD attacks and 24.7% against FGSM attacks. Additionally, DeepDefense requires 20 to 30 times greater perturbation magnitudes to misclassify samples under optimization-based attacks, showcasing stronger decision boundaries and a flatter loss landscape. The approach is architecture-agnostic, simple to implement, and highly effective, highlighting a promising path for enhancing adversarial robustness in deep learning models. <div>
arXiv:2511.13749v1 Announce Type: new 
Abstract: Deep neural networks are known to be vulnerable to adversarial perturbations, which are small and carefully crafted inputs that lead to incorrect predictions. In this paper, we propose DeepDefense, a novel defense framework that applies Gradient-Feature Alignment (GFA) regularization across multiple layers to suppress adversarial vulnerability. By aligning input gradients with internal feature representations, DeepDefense promotes a smoother loss landscape in tangential directions, thereby reducing the model's sensitivity to adversarial noise.
  We provide theoretical insights into how adversarial perturbation can be decomposed into radial and tangential components and demonstrate that alignment suppresses loss variation in tangential directions, where most attacks are effective. Empirically, our method achieves significant improvements in robustness across both gradient-based and optimization-based attacks. For example, on CIFAR-10, CNN models trained with DeepDefense outperform standard adversarial training by up to 15.2% under APGD attacks and 24.7% under FGSM attacks. Against optimization-based attacks such as DeepFool and EADEN, DeepDefense requires 20 to 30 times higher perturbation magnitudes to cause misclassification, indicating stronger decision boundaries and a flatter loss landscape. Our approach is architecture-agnostic, simple to implement, and highly effective, offering a promising direction for improving the adversarial robustness of deep learning models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCALEX: Scalable Concept and Latent Exploration for Diffusion Models</title>
<link>https://arxiv.org/abs/2511.13750</link>
<guid>https://arxiv.org/abs/2511.13750</guid>
<content:encoded><![CDATA[
<div> Keywords: image generation, social biases, SCALEX, latent spaces, diffusion models

<br /><br />Summary: The article discusses the presence of social biases, particularly stereotypes related to gender, race, and profession, in image generation models. Traditional methods for analyzing these biases in diffusion models are limited as they often focus on predefined categories or rely on manual interpretation, which restricts scalability and the detection of nuanced or unexpected patterns. To address these issues, the authors present SCALEX, a novel framework for the scalable and automatic exploration of latent spaces in diffusion models. SCALEX utilizes natural language prompts to extract semantically meaningful directions from the model's latent space, allowing for zero-shot interpretation without the need for retraining or labeling data. This capability facilitates systematic comparisons across various concepts and enables the large-scale discovery of internal model associations. The framework successfully identifies gender bias in profession-related prompts, ranks semantic alignment among identity descriptors, and uncovers clustered conceptual structures autonomously. By directly linking prompts to latent directions, SCALEX enhances the scalability, interpretability, and extensibility of bias analysis in diffusion models compared to existing approaches. <div>
arXiv:2511.13750v1 Announce Type: new 
Abstract: Image generation models frequently encode social biases, including stereotypes tied to gender, race, and profession. Existing methods for analyzing these biases in diffusion models either focus narrowly on predefined categories or depend on manual interpretation of latent directions. These constraints limit scalability and hinder the discovery of subtle or unanticipated patterns.
  We introduce SCALEX, a framework for scalable and automated exploration of diffusion model latent spaces. SCALEX extracts semantically meaningful directions from H-space using only natural language prompts, enabling zero-shot interpretation without retraining or labelling. This allows systematic comparison across arbitrary concepts and large-scale discovery of internal model associations. We show that SCALEX detects gender bias in profession prompts, ranks semantic alignment across identity descriptors, and reveals clustered conceptual structure without supervision. By linking prompts to latent directions directly, SCALEX makes bias analysis in diffusion models more scalable, interpretable, and extensible than prior approaches.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motor Imagery Classification Using Feature Fusion of Spatially Weighted Electroencephalography</title>
<link>https://arxiv.org/abs/2511.13752</link>
<guid>https://arxiv.org/abs/2511.13752</guid>
<content:encoded><![CDATA[
<div> Keywords: Brain-Computer Interface, EEG, Motor Imagery, Channel Selection, Feature Fusion<br /><br />Summary:<br />1. This study proposes a novel Brain-Computer Interface (BCI) method that enhances classification accuracy by using brain region-specific channel selection and multi-domain feature fusion applied to EEG signals.<br />2. EEG channels are grouped according to their functional relevance to specific brain regions involved in motor imagery (MI) tasks, allowing elimination of irrelevant channels, which reduces data dimensionality and computational complexity.<br />3. Three feature extraction techniques‚ÄîCommon Spatial Pattern (CSP), Fuzzy C-means clustering, and Tangent Space Mapping (TSM)‚Äîare applied to each brain region-specific channel group, capturing different characteristics of the EEG signal such as spatial patterns, data clusters, and non-linear patterns.<br />4. The combined multi-domain feature vector is used to classify motor imagery tasks (left hand, right hand, right foot) using a Support Vector Machine (SVM) classifier.<br />5. Validation on benchmark EEG datasets from BCI Competition III (dataset IVA) and IV (dataset I) demonstrates superior performance, achieving classification accuracies of 90.77% and 84.50%, outperforming existing methods in motor imagery classification. <div>
arXiv:2511.13752v1 Announce Type: new 
Abstract: A Brain Computer Interface (BCI) connects the human brain to the outside world, providing a direct communication channel. Electroencephalography (EEG) signals are commonly used in BCIs to reflect cognitive patterns related to motor function activities. However, due to the multichannel nature of EEG signals, explicit information processing is crucial to lessen computational complexity in BCI systems. This study proposes an innovative method based on brain region-specific channel selection and multi-domain feature fusion to improve classification accuracy. The novelty of the proposed approach lies in region-based channel selection, where EEG channels are grouped according to their functional relevance to distinct brain regions. By selecting channels based on specific regions involved in motor imagery (MI) tasks, this technique eliminates irrelevant channels, reducing data dimensionality and improving computational efficiency. This also ensures that the extracted features are more reflective of the brain actual activity related to motor tasks. Three distinct feature extraction methods Common Spatial Pattern (CSP), Fuzzy C-means clustering, and Tangent Space Mapping (TSM), are applied to each group of channels based on their brain region. Each method targets different characteristics of the EEG signal: CSP focuses on spatial patterns, Fuzzy C means identifies clusters within the data, and TSM captures non-linear patterns in the signal. The combined feature vector is used to classify motor imagery tasks (left hand, right hand, and right foot) using Support Vector Machine (SVM). The proposed method was validated on publicly available benchmark EEG datasets (IVA and I) from the BCI competition III and IV. The results show that the approach outperforms existing methods, achieving classification accuracies of 90.77% and 84.50% for datasets IVA and I, respectively.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness of LLM-enabled vehicle trajectory prediction under data security threats</title>
<link>https://arxiv.org/abs/2511.13753</link>
<guid>https://arxiv.org/abs/2511.13753</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Vehicle Trajectory Prediction, Adversarial Attack, Robustness, Automated Driving Systems<br /><br />Summary:  
This study investigates the vulnerability of large language model (LLM)-based vehicle trajectory prediction systems used in automated driving. While previous research has shown that fine-tuned LLMs can effectively predict vehicle behaviors such as lane changes by interpreting data from surrounding vehicles, the robustness of these models under adversarial conditions has not been explored. The authors introduce a novel one-feature differential evolution attack that subtly alters a single kinematic feature of surrounding vehicles within the LLM input, operating in a black-box attack scenario. Experimental results on the highD dataset demonstrate that even minor and physically realistic perturbations can significantly degrade prediction accuracy, highlighting the susceptibility of LLM-enabled models to adversarial manipulation. Further analysis reveals a trade-off between prediction accuracy and robustness under attack and sheds light on the mechanisms behind model failures. The study also discusses potential mitigation strategies to enhance system resilience. Overall, this work provides the first systematic vulnerability analysis of LLM-driven vehicle trajectory predictors in safety-critical contexts, emphasizing the urgent need to design robustness-enhanced LLM frameworks for intelligent transportation systems to ensure safer automated driving. <div>
arXiv:2511.13753v1 Announce Type: new 
Abstract: The integration of large language models (LLMs) into automated driving systems has opened new possibilities for reasoning and decision-making by transforming complex driving contexts into language-understandable representations. Recent studies demonstrate that fine-tuned LLMs can accurately predict vehicle trajectories and lane-change intentions by gathering and transforming data from surrounding vehicles. However, the robustness of such LLM-based prediction models for safety-critical driving systems remains unexplored, despite the increasing concerns about the trustworthiness of LLMs. This study addresses this gap by conducting a systematic vulnerability analysis of LLM-enabled vehicle trajectory prediction. We propose a one-feature differential evolution attack that perturbs a single kinematic feature of surrounding vehicles within the LLM's input prompts under a black-box setting. Experiments on the highD dataset reveal that even minor, physically plausible perturbations can significantly disrupt model outputs, underscoring the susceptibility of LLM-based predictors to adversarial manipulation. Further analyses reveal a trade-off between accuracy and robustness, examine the failure mechanism, and explore potential mitigation solutions. The findings provide the very first insights into adversarial vulnerabilities of LLM-driven automated vehicle models in the context of vehicular interactions and highlight the need for robustness-oriented design in future LLM-based intelligent transportation systems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement</title>
<link>https://arxiv.org/abs/2511.13755</link>
<guid>https://arxiv.org/abs/2511.13755</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal learning, modality bias, redundancy, co-information gating, adaptive regulation  

<br /><br />Summary:  
Multimodal learning improves performance by integrating data from multiple sources. However, during joint training, modality bias can lead to an imbalance where a dominant modality overshadows others, causing two main issues. Firstly, the prolonged influence of the dominant modality can weaken the coupling between representations and outputs, leading to redundant information accumulation. Secondly, current techniques often adjust gradients of the dominant modality uniformly, neglecting the semantic relationships and directionalities between modalities. To tackle these challenges, this paper introduces Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement (RedReg), inspired by the information bottleneck principle. The method includes a redundancy phase monitor, which activates intervention only when redundancy is high, utilizing a joint criterion of effective gain growth and redundancy. Additionally, a co-information gating mechanism evaluates the dominant modality‚Äôs contribution based on cross-modal semantics, ensuring the preservation of modality-specific information when reliance on a single modality is necessary. Finally, the approach involves projecting the dominant modality's gradient onto the orthogonal complement of the multimodal gradient subspace to adjust and suppress the gradient based on redundancy. Experiments demonstrate that RedReg outperforms existing methods, and ablation studies confirm its effectiveness. The code for the method is publicly accessible. <div>
arXiv:2511.13755v1 Announce Type: new 
Abstract: Multimodal learning aims to improve performance by leveraging data from multiple sources. During joint multimodal training, due to modality bias, the advantaged modality often dominates backpropagation, leading to imbalanced optimization. Existing methods still face two problems: First, the long-term dominance of the dominant modality weakens representation-output coupling in the late stages of training, resulting in the accumulation of redundant information. Second, previous methods often directly and uniformly adjust the gradients of the advantaged modality, ignoring the semantics and directionality between modalities. To address these limitations, we propose Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement (RedReg), which is inspired by information bottleneck principle. Specifically, we construct a redundancy phase monitor that uses a joint criterion of effective gain growth rate and redundancy to trigger intervention only when redundancy is high. Furthermore, we design a co-information gating mechanism to estimate the contribution of the current dominant modality based on cross-modal semantics. When the task primarily relies on a single modality, the suppression term is automatically disabled to preserve modality-specific information. Finally, we project the gradient of the dominant modality onto the orthogonal complement of the joint multimodal gradient subspace and suppress the gradient according to redundancy. Experiments show that our method demonstrates superiority among current major methods in most scenarios. Ablation experiments verify the effectiveness of our method. The code is available at https://github.com/xia-zhe/RedReg.git
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Horizon Time Series Forecasting of non-parametric CDFs with Deep Lattice Networks</title>
<link>https://arxiv.org/abs/2511.13756</link>
<guid>https://arxiv.org/abs/2511.13756</guid>
<content:encoded><![CDATA[
<div> Keywords: probabilistic forecasting, cumulative distribution function, deep lattice networks, quantile regression, solar irradiance

<br /><br />Summary: The article discusses the significance of probabilistic forecasting in capturing sudden changes in time series data, emphasizing its advantage over point predictions. Traditionally, modeling cumulative distribution functions (CDFs) relied on parametric methods, but recent advancements allow for nonparametric approaches. The authors propose an innovative method that integrates probabilistic forecasting with monotonic networks, specifically through the adaptation of deep lattice networks (DLN) to conduct simultaneous and implicit quantile regression. This adaptation addresses the issue of quantile crossovers in quantile regression, ensuring a legitimate CDF is produced. By utilizing long short-term memory units (LSTM) in the embedding layer and distributing quantile inputs across all sub-lattices in the DLN, the model can forecast implicit CDFs effectively while adhering to monotonic constraints. The paper showcases the model's performance in generating day-ahead, hourly forecasts of solar irradiance, revealing that the adapted DLN competes favorably against state-of-the-art methods. Additionally, it outperforms a scalable monotonic neural network, highlighting the potential for further exploration in the fields of monotonic neural networks and probabilistic forecasting. <div>
arXiv:2511.13756v1 Announce Type: new 
Abstract: Probabilistic forecasting is not only a way to add more information to a prediction of the future, but it also builds on weaknesses in point prediction. Sudden changes in a time series can still be captured by a cumulative distribution function (CDF), while a point prediction is likely to miss it entirely. The modeling of CDFs within forecasts has historically been limited to parametric approaches, but due to recent advances, this no longer has to be the case. We aim to advance the fields of probabilistic forecasting and monotonic networks by connecting them and propose an approach that permits the forecasting of implicit, complete, and nonparametric CDFs. For this purpose, we propose an adaptation to deep lattice networks (DLN) for monotonically constrained simultaneous/implicit quantile regression in time series forecasting. Quantile regression usually produces quantile crossovers, which need to be prevented to achieve a legitimate CDF. By leveraging long short term memory units (LSTM) as the embedding layer, and spreading quantile inputs to all sub-lattices of a DLN with an extended output size, we can produce a multi-horizon forecast of an implicit CDF due to the monotonic constraintability of DLNs that prevent quantile crossovers. We compare and evaluate our approach's performance to relevant state of the art within the context of a highly relevant application of time series forecasting: Day-ahead, hourly forecasts of solar irradiance observations. Our experiments show that the adaptation of a DLN performs just as well or even better than an unconstrained approach. Further comparison of the adapted DLN against a scalable monotonic neural network shows that our approach performs better. With this adaptation of DLNs, we intend to create more interest and crossover investigations in techniques of monotonic neural networks and probabilistic forecasting.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VitalBench: A Rigorous Multi-Center Benchmark for Long-Term Vital Sign Prediction in Intraoperative Care</title>
<link>https://arxiv.org/abs/2511.13757</link>
<guid>https://arxiv.org/abs/2511.13757</guid>
<content:encoded><![CDATA[
<div> Keywords: intraoperative monitoring, vital signs, VitalBench, deep learning, clinical environments

<br /><br />Summary: Intraoperative monitoring and prediction of vital signs are essential for patient safety and improving surgical outcomes. Although deep learning models for medical time-series forecasting have advanced, challenges remain, including the absence of standardized benchmarks, issues with incomplete data, and limited validation across different centers. To tackle these, the authors introduce VitalBench, a new benchmark designed for intraoperative vital sign prediction. This benchmark encompasses data from over 4,000 surgeries at two independent medical centers and offers three evaluation tracks: complete data, incomplete data, and cross-center generalization. VitalBench mirrors the complexities of real-world clinical practice by minimizing dependence on extensive preprocessing and utilizing masked loss techniques for unbiased model evaluation. This framework provides a standardized platform for model development and comparison, allowing researchers to concentrate on architectural innovations while maintaining consistency in data management. Ultimately, this work is foundational for advancing predictive models in intraoperative vital sign forecasting, ensuring models are not only accurate but also robust and adaptable across various clinical settings. Code and data are publicly accessible at the provided GitHub link. <div>
arXiv:2511.13757v1 Announce Type: new 
Abstract: Intraoperative monitoring and prediction of vital signs are critical for ensuring patient safety and improving surgical outcomes. Despite recent advances in deep learning models for medical time-series forecasting, several challenges persist, including the lack of standardized benchmarks, incomplete data, and limited cross-center validation. To address these challenges, we introduce VitalBench, a novel benchmark specifically designed for intraoperative vital sign prediction. VitalBench includes data from over 4,000 surgeries across two independent medical centers, offering three evaluation tracks: complete data, incomplete data, and cross-center generalization. This framework reflects the real-world complexities of clinical practice, minimizing reliance on extensive preprocessing and incorporating masked loss techniques for robust and unbiased model evaluation. By providing a standardized and unified platform for model development and comparison, VitalBench enables researchers to focus on architectural innovation while ensuring consistency in data handling. This work lays the foundation for advancing predictive models for intraoperative vital sign forecasting, ensuring that these models are not only accurate but also robust and adaptable across diverse clinical environments. Our code and data are available at https://github.com/XiudingCai/VitalBench.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChemFixer: Correcting Invalid Molecules to Unlock Previously Unseen Chemical Space</title>
<link>https://arxiv.org/abs/2511.13758</link>
<guid>https://arxiv.org/abs/2511.13758</guid>
<content:encoded><![CDATA[
<div> Keywords: ChemFixer, molecular generation, deep learning, drug discovery, validity

<br /><br />Summary: This paper presents ChemFixer, a framework that addresses the challenge of generating chemically valid molecules in deep learning-based molecular generation models. These models are capable of exploring extensive chemical spaces but often yield invalid molecules, limiting their practical application in drug discovery. ChemFixer utilizes a transformer architecture and is pre-trained with masking techniques, subsequently fine-tuned on a large dataset of valid and invalid molecular pairs. Evaluations demonstrate that ChemFixer improves the validity of generated molecules while maintaining the original chemical and biological properties. This improvement enables the recovery of previously ungenerated molecules, thereby broadening the diversity of potential drug candidates. Additionally, ChemFixer has been applied to a drug-target interaction (DTI) prediction task with limited data, resulting in enhanced validity of ligands and the identification of promising ligand-protein pairs. The findings indicate that ChemFixer is effective not only in data-limited scenarios but also adaptable for various downstream applications in drug discovery. Overall, ChemFixer shows significant promise in advancing molecular validity and expanding access to chemical space for future drug development efforts. <div>
arXiv:2511.13758v1 Announce Type: new 
Abstract: Deep learning-based molecular generation models have shown great potential in efficiently exploring vast chemical spaces by generating potential drug candidates with desired properties. However, these models often produce chemically invalid molecules, which limits the usable scope of the learned chemical space and poses significant challenges for practical applications. To address this issue, we propose ChemFixer, a framework designed to correct invalid molecules into valid ones. ChemFixer is built on a transformer architecture, pre-trained using masking techniques, and fine-tuned on a large-scale dataset of valid/invalid molecular pairs that we constructed. Through comprehensive evaluations across diverse generative models, ChemFixer improved molecular validity while effectively preserving the chemical and biological distributional properties of the original outputs. This indicates that ChemFixer can recover molecules that could not be previously generated, thereby expanding the diversity of potential drug candidates. Furthermore, ChemFixer was effectively applied to a drug-target interaction (DTI) prediction task using limited data, improving the validity of generated ligands and discovering promising ligand-protein pairs. These results suggest that ChemFixer is not only effective in data-limited scenarios, but also extensible to a wide range of downstream tasks. Taken together, ChemFixer shows promise as a practical tool for various stages of deep learning-based drug discovery, enhancing molecular validity and expanding accessible chemical space.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent VLMs Guided Self-Training with PNU Loss for Low-Resource Offensive Content Detection</title>
<link>https://arxiv.org/abs/2511.13759</link>
<guid>https://arxiv.org/abs/2511.13759</guid>
<content:encoded><![CDATA[
<div> Keywords: offensive content, self-training framework, pseudo-labeling, Multi-Agent Vision-Language Models, Positive-Negative-Unlabeled loss

<br /><br />Summary: The accurate detection of offensive content on social media is challenged by the scarcity of high-quality labeled data, due to both the low prevalence of such instances and the costs associated with manual annotation. This article presents a self-training framework that addresses this challenge by utilizing abundant unlabeled data through a method called collaborative pseudo-labeling. The framework begins with a lightweight classifier that is trained on a limited amount of labeled data. It iteratively assigns pseudo-labels to unlabeled instances by leveraging Multi-Agent Vision-Language Models (MA-VLMs). The approach classifies unlabeled data into two sets: the Agreed-Unknown set, which consists of instances with consensus between the classifier and MA-VLMs, and the Disagreed-Unknown set, which contains conflicting samples. Furthermore, MA-VLMs simulate dual perspectives representing moderator and user viewpoints to enhance label reliability. The classifier is optimized using a novel Positive-Negative-Unlabeled (PNU) loss, which effectively utilizes labeled, Agreed-Unknown, and Disagreed-Unknown data while minimizing pseudo-label noise. Experimental results on benchmark datasets indicate that the proposed framework significantly outperforms existing baselines under limited supervision and achieves performance comparable to large-scale models. <div>
arXiv:2511.13759v1 Announce Type: new 
Abstract: Accurate detection of offensive content on social media demands high-quality labeled data; however, such data is often scarce due to the low prevalence of offensive instances and the high cost of manual annotation. To address this low-resource challenge, we propose a self-training framework that leverages abundant unlabeled data through collaborative pseudo-labeling. Starting with a lightweight classifier trained on limited labeled data, our method iteratively assigns pseudo-labels to unlabeled instances with the support of Multi-Agent Vision-Language Models (MA-VLMs). Un-labeled data on which the classifier and MA-VLMs agree are designated as the Agreed-Unknown set, while conflicting samples form the Disagreed-Unknown set. To enhance label reliability, MA-VLMs simulate dual perspectives, moderator and user, capturing both regulatory and subjective viewpoints. The classifier is optimized using a novel Positive-Negative-Unlabeled (PNU) loss, which jointly exploits labeled, Agreed-Unknown, and Disagreed-Unknown data while mitigating pseudo-label noise. Experiments on benchmark datasets demonstrate that our framework substantially outperforms baselines under limited supervision and approaches the performance of large-scale models
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoETTA: Test-Time Adaptation Under Mixed Distribution Shifts with MoE-LayerNorm</title>
<link>https://arxiv.org/abs/2511.13760</link>
<guid>https://arxiv.org/abs/2511.13760</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-Time Adaptation, MoETTA, Mixture-of-Experts, heterogeneous distribution shifts, benchmark

<br /><br />Summary: 
Test-Time Adaptation (TTA) is effective for addressing performance drops due to single-domain distribution shifts by updating model parameters during inference. However, real-world applications often face mixed distribution shifts that involve diverse and conflicting domain factors, which are challenging even for state-of-the-art TTA methods. A primary limitation of current approaches is their reliance on a unified adaptation path; optimal gradient directions can greatly vary between domains. Additionally, existing benchmarks primarily focus on synthetic or homogeneous shifts, not reflecting the complexities of real-world scenarios. To tackle these issues, we propose MoETTA, an entropy-based TTA framework utilizing the Mixture-of-Experts (MoE) architecture to allow for diverse gradient adaptations through structurally decoupled experts. This enables flexible parameter updates for heterogeneous shifts. To further simulate realistic conditions, we introduce two new benchmarks, potpourri and potpourri+, which include a broader range of domain shifts and test robustness against catastrophic forgetting, respectively. Extensive experiments demonstrate that MoETTA consistently outperforms strong baselines across various mixed distribution shift settings, establishing state-of-the-art performance and emphasizing the advantages of accommodating multiple adaptation directions through expert-level diversity. <div>
arXiv:2511.13760v1 Announce Type: new 
Abstract: Test-Time adaptation (TTA) has proven effective in mitigating performance drops under single-domain distribution shifts by updating model parameters during inference. However, real-world deployments often involve mixed distribution shifts, where test samples are affected by diverse and potentially conflicting domain factors, posing significant challenges even for SOTA TTA methods. A key limitation in existing approaches is their reliance on a unified adaptation path, which fails to account for the fact that optimal gradient directions can vary significantly across different domains. Moreover, current benchmarks focus only on synthetic or homogeneous shifts, failing to capture the complexity of real-world heterogeneous mixed distribution shifts. To address this, we propose MoETTA, a novel entropy-based TTA framework that integrates the Mixture-of-Experts (MoE) architecture. Rather than enforcing a single parameter update rule for all test samples, MoETTA introduces a set of structurally decoupled experts, enabling adaptation along diverse gradient directions. This design allows the model to better accommodate heterogeneous shifts through flexible and disentangled parameter updates. To simulate realistic deployment conditions, we introduce two new benchmarks: potpourri and potpourri+. While classical settings focus solely on synthetic corruptions, potpourri encompasses a broader range of domain shifts--including natural, artistic, and adversarial distortions--capturing more realistic deployment challenges. Additionally, potpourri+ further includes source-domain samples to evaluate robustness against catastrophic forgetting. Extensive experiments across three mixed distribution shifts settings show that MoETTA consistently outperforms strong baselines, establishing SOTA performance and highlighting the benefit of modeling multiple adaptation directions via expert-level diversity.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gene Incremental Learning for Single-Cell Transcriptomics</title>
<link>https://arxiv.org/abs/2511.13762</link>
<guid>https://arxiv.org/abs/2511.13762</guid>
<content:encoded><![CDATA[
<div> Keywords: tokens, incremental learning, single-cell transcriptomics, genes, forgetting problem  

<br /><br />Summary: This article examines the underexplored area of incremental learning for tokens, specifically focusing on genes within the context of single-cell transcriptomics. Despite the importance of tokens in various research domains, their incremental learning has received limited attention, largely due to their holistic nature in language. To address this challenge, the authors propose a gene-centric pipeline for incremental learning tailored to a large biological dataset. Their research reveals that the forgetting problem, commonly observed in class incremental learning, also affects gene incremental learning. In response, existing methods from class incremental learning were adapted to help mitigate this forgetting issue. The authors conducted extensive experiments to validate both the design of their framework and the effectiveness of their modifications. Additionally, they present a comprehensive benchmark for gene incremental learning within single-cell transcriptomics, fostering further research in this area. Overall, the study highlights the significance of incremental learning strategies for genes and provides a foundation for future investigations into their dynamic growth and adaptation in biological contexts. <div>
arXiv:2511.13762v1 Announce Type: new 
Abstract: Classes, as fundamental elements of Computer Vision, have been extensively studied within incremental learning frameworks. In contrast, tokens, which play essential roles in many research fields, exhibit similar characteristics of growth, yet investigations into their incremental learning remain significantly scarce. This research gap primarily stems from the holistic nature of tokens in language, which imposes significant challenges on the design of incremental learning frameworks for them. To overcome this obstacle, in this work, we turn to a type of token, gene, for a large-scale biological dataset--single-cell transcriptomics--to formulate a pipeline for gene incremental learning and establish corresponding evaluations. We found that the forgetting problem also exists in gene incremental learning, thus we adapted existing class incremental learning methods to mitigate the forgetting of genes. Through extensive experiments, we demonstrated the soundness of our framework design and evaluations, as well as the effectiveness of our method adaptations. Finally, we provide a complete benchmark for gene incremental learning in single-cell transcriptomics.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Library Liberation: Competitive Performance Matmul Through Compiler-composed Nanokernels</title>
<link>https://arxiv.org/abs/2511.13764</link>
<guid>https://arxiv.org/abs/2511.13764</guid>
<content:encoded><![CDATA[
<div> Keywords: AI workloads, MLIR, microkernels, compiler optimization, hardware utilization<br /><br />Summary:<br /><br />1. The paper addresses the growing disparity between high-level AI and machine learning operations and the need for efficient hardware utilization, a gap typically bridged only by hardware experts through handcrafted kernels or specialized low-level libraries.<br /><br />2. It introduces a novel compilation scheme that automatically generates scalable, high-performance microkernels by exploiting MLIR dialects, effectively connecting domain-level ML operations with processor-specific capabilities.<br /><br />3. This approach eliminates reliance on manual kernel writing or low-level libraries by enabling the compiler to produce near-optimal code autonomously, significantly simplifying development for ML practitioners.<br /><br />4. Central to this method is a mechanism for composing nanokernels from low-level intermediate representations (IR) with near-optimal register usage, crafting efficient microkernels tailored for diverse target hardware.<br /><br />5. Implemented within an MLIR-based compiler supporting vector and tile-based CPU instructions, the technique generates production-quality nanokernels, demonstrating performance competitive with state-of-the-art microkernel libraries in experimental evaluations. <div>
arXiv:2511.13764v1 Announce Type: new 
Abstract: The rapidly evolving landscape of AI and machine learning workloads has widened the gap between high-level domain operations and efficient hardware utilization. Achieving near-peak performance still demands deep hardware expertise-experts either handcraft target-specific kernels (e.g., DeepSeek) or rely on specialized libraries (e.g., CUTLASS)-both of which add complexity and limit scalability for most ML practitioners.
  This paper introduces a compilation scheme that automatically generates scalable, high-performance microkernels by leveraging the MLIR dialects to bridge domain-level operations and processor capabilities. Our approach removes dependence on low-level libraries by enabling the compiler to auto-generate near-optimal code directly. At its core is a mechanism for composing nanokernels from low-level IR constructs with near-optimal register utilization, forming efficient microkernels tailored to each target. We implement this technique in an MLIR-based compiler supporting both vector and tile based CPU instructions. Experiments show that the generated nanokernels are of production-quality, and competitive with state-of-the-art microkernel libraries.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning</title>
<link>https://arxiv.org/abs/2511.13765</link>
<guid>https://arxiv.org/abs/2511.13765</guid>
<content:encoded><![CDATA[
<div> Keywords: offline imitation learning, reward function, large language models, preference ranking, policy learning  

<br /><br />Summary: 
Offline imitation learning (offline IL) allows for effective policy training without explicit reward annotations. Traditional methods have relied on estimating rewards from a limited set of expert demonstrations, often oversimplifying the relationship between trajectory similarity and reward. The authors introduce PROF, a novel framework that uses large language models (LLMs) to generate and enhance executable reward functions based on natural language descriptions and a single expert trajectory. A key component of PROF is the Reward Preference Ranking (RPR), which assesses and ranks the quality of reward functions without needing environment interactions or reinforcement learning (RL) training. The RPR computes dominance scores for reward functions, where higher scores reflect better alignment with expert preferences. This framework automates the selection and optimization of reward functions through an iterative process combining RPR and text-based gradient optimization. Empirical evaluations on the D4RL benchmark reveal that PROF either surpasses or matches the performance of existing strong baselines across various datasets and domains, demonstrating its potential to improve offline imitation learning outcomes effectively. <div>
arXiv:2511.13765v1 Announce Type: new 
Abstract: Offline imitation learning (offline IL) enables training effective policies without requiring explicit reward annotations. Recent approaches attempt to estimate rewards for unlabeled datasets using a small set of expert demonstrations. However, these methods often assume that the similarity between a trajectory and an expert demonstration is positively correlated with the reward, which oversimplifies the underlying reward structure. We propose PROF, a novel framework that leverages large language models (LLMs) to generate and improve executable reward function codes from natural language descriptions and a single expert trajectory. We propose Reward Preference Ranking (RPR), a novel reward function quality assessment and ranking strategy without requiring environment interactions or RL training. RPR calculates the dominance scores of the reward functions, where higher scores indicate better alignment with expert preferences. By alternating between RPR and text-based gradient optimization, PROF fully automates the selection and refinement of optimal reward functions for downstream policy learning. Empirical results on D4RL demonstrate that PROF surpasses or matches recent strong baselines across numerous datasets and domains, highlighting the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Credal Ensemble Distillation for Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2511.13766</link>
<guid>https://arxiv.org/abs/2511.13766</guid>
<content:encoded><![CDATA[
<div> Deep ensembles, uncertainty quantification, credal set, ensemble distillation, out-of-distribution detection<br /><br />Summary:<br /><br />1. Deep ensembles (DE) are effective for quantifying predictive uncertainty and distinguishing between aleatoric and epistemic uncertainties, which improves robustness and reliability of models.<br /><br />2. Despite their advantages, DEs require high computational and memory resources during inference, limiting their practical deployment in many real-world applications.<br /><br />3. To address this limitation, the paper introduces a novel framework called Credal Ensemble Distillation (CED), which compresses the knowledge of a DE into a single compressed model named CREDIT for classification tasks.<br /><br />4. Unlike traditional single-model outputs that predict a softmax probability distribution, CREDIT predicts class-wise probability intervals representing a credal set, a convex set of probability distributions, enabling more informative uncertainty quantification.<br /><br />5. Experimental evaluations on out-of-distribution detection benchmarks demonstrate that CED delivers superior or comparable uncertainty estimation performance compared to existing baselines, while significantly reducing inference computational overhead relative to the full deep ensemble approach. <div>
arXiv:2511.13766v1 Announce Type: new 
Abstract: Deep ensembles (DE) have emerged as a powerful approach for quantifying predictive uncertainty and distinguishing its aleatoric and epistemic components, thereby enhancing model robustness and reliability. However, their high computational and memory costs during inference pose significant challenges for wide practical deployment. To overcome this issue, we propose credal ensemble distillation (CED), a novel framework that compresses a DE into a single model, CREDIT, for classification tasks. Instead of a single softmax probability distribution, CREDIT predicts class-wise probability intervals that define a credal set, a convex set of probability distributions, for uncertainty quantification. Empirical results on out-of-distribution detection benchmarks demonstrate that CED achieves superior or comparable uncertainty estimation compared to several existing baselines, while substantially reducing inference overhead compared to DE.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Temperature Scheduler for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.13767</link>
<guid>https://arxiv.org/abs/2511.13767</guid>
<content:encoded><![CDATA[
<div> Knowledge Distillation, Dynamic Temperature, Temperature Scheduling, Teacher-Student Model, Cross-Entropy Loss  

<br /><br />Summary:  
This paper addresses a key limitation in Knowledge Distillation (KD), where the temperature hyperparameter controlling the softness of output probabilities is typically fixed throughout training, which is suboptimal. The authors observe that students benefit from softer output probabilities in the early phases of training and require sharper probabilities in later stages for better learning. To tackle this, they propose the Dynamic Temperature Scheduler (DTS), a novel method that dynamically adjusts the temperature based on the cross-entropy loss gap between the teacher and student models. This adaptive adjustment allows the temperature to better reflect the divergence between teacher and student distributions over time. DTS is designed to integrate seamlessly with existing KD methods without adding complexity. The method was empirically validated across multiple datasets and tasks, including vision benchmarks like CIFAR-100 and Tiny-ImageNet, as well as various NLP tasks such as GLUE, Dolly, SelfIns, UnNI, and S-NI. The results consistently demonstrate that DTS outperforms static-temperature baselines, substantiating the benefits of dynamic temperature scheduling in improving student model performance. The authors also provide open-source code to facilitate adoption and further research. <div>
arXiv:2511.13767v1 Announce Type: new 
Abstract: Knowledge Distillation (KD) trains a smaller student model using a large, pre-trained teacher model, with temperature as a key hyperparameter controlling the softness of output probabilities. Traditional methods use a fixed temperature throughout training, which is suboptimal. Moreover, architectural differences between teacher and student often result in mismatched logit magnitudes. We demonstrate that students benefit from softer probabilities early in training but require sharper probabilities in later stages. We introduce Dynamic Temperature Scheduler (DTS), which adjusts temperature dynamically based on the cross-entropy loss gap between teacher and student. To our knowledge, this is the first temperature scheduling method that adapts based on the divergence between teacher and student distributions. Our method integrates seamlessly with existing KD frameworks. We validate DTS across multiple KD strategies on vision (CIFAR-100, Tiny-ImageNet) and NLP tasks (GLUE, Dolly, SelfIns, UnNI, S-NI), consistently outperforming static-temperature baselines. Code is available at https://github.com/Sibgat-Ul/DTS.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compiling to linear neurons</title>
<link>https://arxiv.org/abs/2511.13769</link>
<guid>https://arxiv.org/abs/2511.13769</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, programming, Cajal, linear neurons, gradient-based learning

<br /><br />Summary: The article discusses the challenge of directly programming neural networks, which traditionally rely on learning algorithms like gradient descent that often lack discrete structure. This limitation prevents the compilation of many discrete algorithms into neural networks, as they are typically not differentiable. To overcome this issue, the authors introduce $\textsf{Cajal}$, a higher-order linear programming language designed specifically to facilitate direct programming of neural networks. They demonstrate that $\textsf{Cajal}$ programs can compile to linear neurons, allowing discrete algorithms to be represented in a form suitable for gradient-based learning. The implementation of $\textsf{Cajal}$ is explored through various experiments where linear neurons are integrated with other neural networks. This integration results in enhanced learning speed, greater data efficiency, and improved debug capabilities. The study concludes that linear programming languages like $\textsf{Cajal}$ can create a productive interaction between learning and the discrete structures characteristic of conventional programming, suggesting a novel approach to programming neural networks more effectively. <div>
arXiv:2511.13769v1 Announce Type: new 
Abstract: We don't program neural networks directly. Instead, we rely on an indirect style where learning algorithms, like gradient descent, determine a neural network's function by learning from data. This indirect style is often a virtue; it empowers us to solve problems that were previously impossible. But it lacks discrete structure. We can't compile most algorithms into a neural network -- even if these algorithms could help the network learn. This limitation occurs because discrete algorithms are not obviously differentiable, making them incompatible with the gradient-based learning algorithms that determine a neural network's function. To address this, we introduce $\textsf{Cajal}$: a typed, higher-order and linear programming language intended to be a minimal vehicle for exploring a direct style of programming neural networks. We prove $\textsf{Cajal}$ programs compile to linear neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation of $\textsf{Cajal}$, we conduct several experiments where we link these linear neurons against other neural networks to determine part of their function prior to learning. Linking with these neurons allows networks to learn faster, with greater data-efficiency, and in a way that's easier to debug. A key lesson is that linear programming languages provide a path towards directly programming neural networks, enabling a rich interplay between learning and the discrete structures of ordinary programming.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Attention as Distributional Projection: A Unified Interpretation of Transformer Architecture</title>
<link>https://arxiv.org/abs/2511.13780</link>
<guid>https://arxiv.org/abs/2511.13780</guid>
<content:encoded><![CDATA[
<div> Keywords: self-attention, distributional semantics, co-occurrence matrix, GloVe embeddings, Transformer architecture  

<br /><br />Summary: This paper offers a mathematical interpretation of self-attention by linking it to principles of distributional semantics. It asserts that self-attention arises from projecting corpus-level co-occurrence statistics into sequence context. The authors begin with the co-occurrence matrix associated with GloVe embeddings, illustrating how this projection effectively captures contextual influence. Furthermore, they explain that the query-key-value mechanism represents a natural asymmetric extension for modeling directional relationships. The study also emphasizes how positional encodings and multi-head attention serve as structured refinements of this projection principle. By connecting these concepts, the analysis reveals that the specific algebraic form of the Transformer architecture stems from these foundational projection principles, challenging the notion that its design choices are arbitrary. This provides a deeper understanding of how self-attention functions in the context of language processing and highlights the intricate relationship between statistical methods and neural network architectures in natural language processing. <div>
arXiv:2511.13780v1 Announce Type: new 
Abstract: This paper presents a mathematical interpretation of self-attention by connecting it to distributional semantics principles. We show that self-attention emerges from projecting corpus-level co-occurrence statistics into sequence context. Starting from the co-occurrence matrix underlying GloVe embeddings, we demonstrate how the projection naturally captures contextual influence, with the query-key-value mechanism arising as the natural asymmetric extension for modeling directional relationships. Positional encodings and multi-head attention then follow as structured refinements of this same projection principle. Our analysis demonstrates that the Transformer architecture's particular algebraic form follows from these projection principles rather than being an arbitrary design choice.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Transferability of Self-Supervised Learning by Task Conflict Calibration</title>
<link>https://arxiv.org/abs/2511.13787</link>
<guid>https://arxiv.org/abs/2511.13787</guid>
<content:encoded><![CDATA[
<div> Keywords: self-supervised learning, transferability, task conflict, meta-learning, Task Conflict Calibration  

<br /><br />Summary:  
This paper explores the transferability of self-supervised learning (SSL) representations, focusing on two key questions: what constitutes representation transferability in SSL and how to effectively model it. Transferability is defined as the ability of a representation learned from one task to support objectives of other tasks. Inspired by meta-learning, the authors construct multiple SSL tasks within each training batch to explicitly model transferability by introducing task-level information. However, empirical results and causal analysis reveal that while task-level information improves transferability, the process is still hampered by task conflict, where conflicting task objectives limit performance. To address this, the authors propose the Task Conflict Calibration (TC¬≤) method. TC¬≤ works by splitting batches to form multiple SSL tasks, then using a factor extraction network to generate causal generative factors for all tasks, alongside a weight extraction network that assigns importance weights to samples. It employs data reconstruction, orthogonality, and sparsity constraints to enhance effectiveness. Finally, TC¬≤ calibrates sample representations through a two-stage bi-level optimization training framework, which significantly boosts transferability. Experiments on various downstream tasks demonstrate that TC¬≤ consistently enhances the transferability performance of SSL models. <div>
arXiv:2511.13787v1 Announce Type: new 
Abstract: In this paper, we explore the transferability of SSL by addressing two central questions: (i) what is the representation transferability of SSL, and (ii) how can we effectively model this transferability? Transferability is defined as the ability of a representation learned from one task to support the objective of another.
  Inspired by the meta-learning paradigm, we construct multiple SSL tasks within each training batch to support explicitly modeling transferability. Based on empirical evidence and causal analysis, we find that although introducing task-level information improves transferability, it is still hindered by task conflict. To address this issue, we propose a Task Conflict Calibration (TC$^2$) method to alleviate the impact of task conflict. Specifically, it first splits batches to create multiple SSL tasks, infusing task-level information. Next, it uses a factor extraction network to produce causal generative factors for all tasks and a weight extraction network to assign dedicated weights to each sample, employing data reconstruction, orthogonality, and sparsity to ensure effectiveness. Finally, TC$^2$ calibrates sample representations during SSL training and integrates into the pipeline via a two-stage bi-level optimization framework to boost the transferability of learned representations. Experimental results on multiple downstream tasks demonstrate that our method consistently improves the transferability of SSL models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments</title>
<link>https://arxiv.org/abs/2511.13788</link>
<guid>https://arxiv.org/abs/2511.13788</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, adversarial interactions, alignment safeguards, harm score, attacker-target exchanges  

<br /><br />Summary: This study investigates the vulnerabilities of large language models (LLMs) in multi-agent, safety-critical contexts, focusing on whether larger models can jailbreak smaller ones despite alignment safeguards. Utilizing the JailbreakBench standardized adversarial tasks, researchers conducted over 6,000 multi-turn interactions across various LLM sizes (0.6B to 120B parameters). The study measures harmful outcomes and alignment integrity through harm and refusal scores rated by three independent LLM judges. Results revealed a significant correlation between the mean harm score and the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001), indicating that larger relative model sizes increase harmful behavior likelihood. Variance in mean harm scores was greater for attackers (0.18) than for targets (0.10), highlighting that the behavior of attackers is more influential on outcomes than target susceptibility. Additionally, there was a strong negative correlation (rho = -0.93, p < 0.001) between attacker refusal frequency and harm, suggesting that robust attacker-side alignment reduces harmful responses. These findings emphasize the role of size asymmetry in model robustness and indicate a need for further exploration into inter-model alignment and safety. <div>
arXiv:2511.13788v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p < 0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScoresActivation: A New Activation Function for Model Agnostic Global Explainability by Design</title>
<link>https://arxiv.org/abs/2511.13809</link>
<guid>https://arxiv.org/abs/2511.13809</guid>
<content:encoded><![CDATA[
<div> Keywords: explainability, deep learning, feature importance, SHAP values, classification accuracy  

<br /><br />Summary: The paper addresses the challenge of achieving transparency in large deep learning models by proposing a novel differentiable approach to global explainability integrated into the model training process. Traditional post hoc explanation methods are disconnected from training, which limits their effectiveness. The authors introduce the ScoresActivation function, a mechanism for ranking features that is embedded within the learning pipeline, allowing models to prioritize features based on their predictive contributions. Evaluations on benchmark datasets demonstrate that this method yields globally faithful and stable feature rankings that align with SHAP values and ground-truth importance, all while maintaining high predictive performance. Additionally, the proposed feature scoring is significantly faster‚Äî150 times quicker‚Äîthan the classical SHAP method, reducing computation time during training from 300 seconds to just 2 seconds for feature ranking. The method also shows an improvement in classification accuracy, achieving gains of 11.24% with 10 features (5 relevant) and 29.33% with 16 features (5 relevant and 11 irrelevant). Overall, the work presents a scalable framework that enhances both model accuracy and interpretability, effectively bridging the gap between these two essential aspects of machine learning. <div>
arXiv:2511.13809v1 Announce Type: new 
Abstract: Understanding the decision of large deep learning models is a critical challenge for building transparent and trustworthy systems. Although the current post hoc explanation methods offer valuable insights into feature importance, they are inherently disconnected from the model training process, limiting their faithfulness and utility. In this work, we introduce a novel differentiable approach to global explainability by design, integrating feature importance estimation directly into model training. Central to our method is the ScoresActivation function, a feature-ranking mechanism embedded within the learning pipeline. This integration enables models to prioritize features according to their contribution to predictive performance in a differentiable and end-to-end trainable manner. Evaluations across benchmark datasets show that our approach yields globally faithful, stable feature rankings aligned with SHAP values and ground-truth feature importance, while maintaining high predictive performance. Moreover, feature scoring is 150 times faster than the classical SHAP method, requiring only 2 seconds during training compared to SHAP's 300 seconds for feature ranking in the same configuration. Our method also improves classification accuracy by 11.24% with 10 features (5 relevant) and 29.33% with 16 features (5 relevant, 11 irrelevant), demonstrating robustness to irrelevant inputs. This work bridges the gap between model accuracy and interpretability, offering a scalable framework for inherently explainable machine learning.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beat the long tail: Distribution-Aware Speculative Decoding for RL Training</title>
<link>https://arxiv.org/abs/2511.13841</link>
<guid>https://arxiv.org/abs/2511.13841</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, Speculative Decoding, Rollout Efficiency, Distribution-Aware  

<br /><br />Summary:  
This paper addresses the efficiency bottleneck in reinforcement learning (RL) post-training of large language models (LLMs), specifically during the rollout phase where long trajectories are generated token-by-token. The authors identify that a small fraction of long rollouts disproportionately increases wall clock time, highlighting a long-tail distribution of rollout lengths. They also note the potential in leveraging historical rollout data that exhibit stable prompt-level patterns across training epochs. To tackle these issues, they propose DAS, a Distribution Aware Speculative decoding framework that accelerates RL rollouts without modifying the model‚Äôs outputs. DAS combines two innovations: an adaptive, nonparametric drafter built dynamically from recent rollouts using an incrementally maintained suffix tree, and a length-aware speculation policy that assigns more aggressive draft budgets to longer trajectories driving the overall makespan. This approach capitalizes on rollout history to maintain high acceptance rates while optimizing the cost balance between base and token-level decoding operations. Experiments on math and code reasoning tasks demonstrate that DAS can reduce rollout time by up to 50%, all while preserving identical training performance curves. The work shows that distribution-aware speculative decoding provides a significant speedup for RL post-training without sacrificing learning quality. <div>
arXiv:2511.13841v1 Announce Type: new 
Abstract: Reinforcement learning(RL) post-training has become essential for aligning large language models (LLMs), yet its efficiency is increasingly constrained by the rollout phase, where long trajectories are generated token by token. We identify a major bottleneck:the long-tail distribution of rollout lengths, where a small fraction of long generations dominates wall clock time and a complementary opportunity; the availability of historical rollouts that reveal stable prompt level patterns across training epochs. Motivated by these observations, we propose DAS, a Distribution Aware Speculative decoding framework that accelerates RL rollouts without altering model outputs. DAS integrates two key ideas: an adaptive, nonparametric drafter built from recent rollouts using an incrementally maintained suffix tree, and a length aware speculation policy that allocates more aggressive draft budgets to long trajectories that dominate makespan. This design exploits rollout history to sustain acceptance while balancing base and token level costs during decoding. Experiments on math and code reasoning tasks show that DAS reduces rollout time up to 50% while preserving identical training curves, demonstrating that distribution-aware speculative decoding can significantly accelerate RL post training without compromising learning quality.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnaCP: Toward Upper-Bound Continual Learning via Analytic Contrastive Projection</title>
<link>https://arxiv.org/abs/2511.13880</link>
<guid>https://arxiv.org/abs/2511.13880</guid>
<content:encoded><![CDATA[
<div> Keywords: class-incremental learning, catastrophic forgetting, pre-trained models, feature adaptation, AnaCP  
  
<br /><br />Summary:  
This paper addresses the challenge of class-incremental learning (CIL), where a model sequentially learns different tasks with unique classes. Traditional methods struggle with catastrophic forgetting (CF) due to simultaneous learning of feature representations and classifiers without leveraging pre-trained models (PTMs). Recent advancements in CIL have incorporated PTMs as fixed feature extractors alongside analytic classifiers, achieving state-of-the-art performance. However, these approaches do not allow for the continuous adaptation of feature representations, which can hinder their effectiveness. To overcome this limitation, the authors introduce AnaCP (Analytic Contrastive Projection), a novel method that maintains the benefits of analytic classifiers while facilitating incremental feature adaptation. Remarkably, AnaCP achieves this without requiring gradient-based training, thus preventing CF caused by such updates. The experimental results demonstrate that AnaCP not only surpasses existing baseline methods but also attains accuracy levels equivalent to joint training, which is considered the maximum performance benchmark for CIL. This innovation presents a significant advancement in the continual learning domain, promising improved model performance across incremental learning tasks. <div>
arXiv:2511.13880v1 Announce Type: new 
Abstract: This paper studies the problem of class-incremental learning (CIL), a core setting within continual learning where a model learns a sequence of tasks, each containing a distinct set of classes. Traditional CIL methods, which do not leverage pre-trained models (PTMs), suffer from catastrophic forgetting (CF) due to the need to incrementally learn both feature representations and the classifier. The integration of PTMs into CIL has recently led to efficient approaches that treat the PTM as a fixed feature extractor combined with analytic classifiers, achieving state-of-the-art performance. However, they still face a major limitation: the inability to continually adapt feature representations to best suit the CIL tasks, leading to suboptimal performance. To address this, we propose AnaCP (Analytic Contrastive Projection), a novel method that preserves the efficiency of analytic classifiers while enabling incremental feature adaptation without gradient-based training, thereby eliminating the CF caused by gradient updates. Our experiments show that AnaCP not only outperforms existing baselines but also achieves the accuracy level of joint training, which is regarded as the upper bound of CIL.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tractable Probabilistic Models for Investment Planning</title>
<link>https://arxiv.org/abs/2511.13888</link>
<guid>https://arxiv.org/abs/2511.13888</guid>
<content:encoded><![CDATA[
<div> Investment planning, power utilities, probabilistic models, sum-product networks, scenario analysis<br /><br />Summary:  
This article addresses the challenge of long-term investment planning in power utilities, focusing on generation and transmission expansion under significant uncertainty. Traditional forecasting methods rely on generating a limited number of discrete scenarios (mixtures of Diracs), which restricts understanding of scenario-specific volatility and complicates robust decision-making. To overcome these limitations, the authors propose using tractable probabilistic models (TPMs), particularly sum-product networks (SPNs). These models allow exact and scalable inference of important quantities such as scenario likelihoods, marginals, and conditional probabilities. By leveraging TPMs, it becomes possible to expand scenarios more robustly and quantify risk more effectively. The framework also supports embedding chance-constrained optimization directly into investment planning, enabling the enforcement of safety or reliability requirements with specified confidence levels. TPMs provide a compact representation of high-dimensional uncertainties, facilitating both detailed scenario analysis and volatility measurement. A case study involving power system planning is presented to demonstrate the computational efficiency and reliability benefits of this approach compared to traditional scenario-based models. The results highlight the potential of TPMs and SPNs to improve robustness and decision-making quality in long-term energy investment strategies. <div>
arXiv:2511.13888v1 Announce Type: new 
Abstract: Investment planning in power utilities, such as generation and transmission expansion, requires decade-long forecasts under profound uncertainty. Forecasting of energy mix and energy use decades ahead is nontrivial. Classical approaches focus on generating a finite number of scenarios (modeled as a mixture of Diracs in statistical theory terms), which limits insight into scenario-specific volatility and hinders robust decision-making. We propose an alternative using tractable probabilistic models (TPMs), particularly sum-product networks (SPNs). These models enable exact, scalable inference of key quantities such as scenario likelihoods, marginals, and conditional probabilities, supporting robust scenario expansion and risk assessment.
  This framework enables direct embedding of chance-constrained optimization into investment planning, enforcing safety or reliability with prescribed confidence levels. TPMs allow both scenario analysis and volatility quantification by compactly representing high-dimensional uncertainties. We demonstrate the approach's effectiveness through a representative power system planning case study, illustrating computational and reliability advantages over traditional scenario-based models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond One-Size-Fits-All: Neural Networks for Differentially Private Tabular Data Synthesis</title>
<link>https://arxiv.org/abs/2511.13893</link>
<guid>https://arxiv.org/abs/2511.13893</guid>
<content:encoded><![CDATA[
<div> Keywords: differential privacy, tabular data synthesis, MargNet, neural networks, statistical models

<br /><br />Summary: The paper discusses the limitations of existing neural network (NN)-based methods for differentially private (DP) tabular data synthesis compared to traditional statistical models. It highlights that while statistical models generally perform well, they struggle with densely correlated datasets, where complex dependencies can create challenges. The authors propose a new approach called MargNet, which merges successful elements of statistical models with neural networks. MargNet employs an adaptive marginal selection strategy, allowing the NN to generate data aligned with chosen marginals. On less correlated datasets, MargNet performs comparably to the best statistical methods while being seven times faster. More significantly, for densely correlated datasets, MargNet sets a new benchmark by reducing fidelity error by up to 26% compared to previous bests. The authors emphasize the potential of their approach in handling intricate dependencies within data synthesis tasks. They also make their implementation available on GitHub, promoting further exploration and application of their method in the field of differential privacy for tabular data. <div>
arXiv:2511.13893v1 Announce Type: new 
Abstract: In differentially private (DP) tabular data synthesis, the consensus is that statistical models are better than neural network (NN)-based methods. However, we argue that this conclusion is incomplete and overlooks the challenge of densely correlated datasets, where intricate dependencies can overwhelm statistical models. In such complex scenarios, neural networks are more suitable due to their capacity to fit complex distributions by learning directly from samples. Despite this potential, existing NN-based algorithms still suffer from significant limitations. We therefore propose MargNet, incorporating successful algorithmic designs of statistical models into neural networks. MargNet applies an adaptive marginal selection strategy and trains the neural networks to generate data that conforms to the selected marginals. On sparsely correlated datasets, our approach achieves utility close to the best statistical method while offering an average 7$\times$ speedup over it. More importantly, on densely correlated datasets, MargNet establishes a new state-of-the-art, reducing fidelity error by up to 26\% compared to the previous best. We release our code on GitHub.\footnote{https://github.com/KaiChen9909/margnet}
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weather Maps as Tokens: Transformers for Renewable Energy Forecasting</title>
<link>https://arxiv.org/abs/2511.13935</link>
<guid>https://arxiv.org/abs/2511.13935</guid>
<content:encoded><![CDATA[
<div> Keywords: renewable energy, forecasting, transformer, weather maps, RMSE

<br /><br />Summary: Accurate renewable energy forecasting is critical for reducing reliance on fossil fuels and facilitating grid decarbonization. Current methods struggle to effectively integrate the complex spatial context of weather patterns with their temporal changes. This paper presents a novel methodology that represents weather maps as tokens in transformer sequences for renewable energy prediction. Hourly weather maps are first encoded as spatial tokens using a lightweight convolutional neural network. These tokens are then analyzed by a transformer model to capture the temporal dynamics over a 45-hour forecasting horizon. Despite some limitations in input initialization, the proposed approach demonstrates significant improvements in forecasting accuracy, achieving a reduction in root mean square error (RMSE) of approximately 60% for wind energy predictions and around 20% for solar energy forecasts when compared to existing ENTSO-E operational forecast methods. This innovative model not only enhances the reliability of renewable energy forecasts but also contributes to better integration of renewable sources into the energy grid. A live dashboard has been developed to provide daily forecasts, accessible at: https://www.sardiniaforecast.ifabfoundation.it. <div>
arXiv:2511.13935v1 Announce Type: new 
Abstract: Accurate renewable energy forecasting is essential to reduce dependence on fossil fuels and enabling grid decarbonization. However, current approaches fail to effectively integrate the rich spatial context of weather patterns with their temporal evolution. This work introduces a novel approach that treats weather maps as tokens in transformer sequences to predict renewable energy. Hourly weather maps are encoded as spatial tokens using a lightweight convolutional neural network, and then processed by a transformer to capture temporal dynamics across a 45-hour forecast horizon. Despite disadvantages in input initialization, evaluation against ENTSO-E operational forecasts shows a reduction in RMSE of about 60\% and 20\% for wind and solar respectively. A live dashboard showing daily forecasts is available at: https://www.sardiniaforecast.ifabfoundation.it.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex-Weighted Convolutional Networks: Provable Expressiveness via Complex Diffusion</title>
<link>https://arxiv.org/abs/2511.13937</link>
<guid>https://arxiv.org/abs/2511.13937</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, complex-weighted diffusion, heterophilic graphs, Complex-Weighted Convolutional Network, random walks<br /><br />Summary:<br /><br />1. Graph Neural Networks (GNNs) have proven successful in many applications but struggle with oversmoothing and poor results on heterophilic graphs where connected nodes tend to have different labels.<br /><br />2. The paper introduces a novel framework that represents edges with complex weights, extending traditional random walks into the complex domain to model diffusion processes over graphs.<br /><br />3. The authors theoretically prove that by choosing appropriate complex weights, any node-classification task can be represented as the steady state of a complex random walk, showing high expressiveness of this approach.<br /><br />4. Based on this theory, they propose the Complex-Weighted Convolutional Network (CWCN), which learns complex-weighted graph structures from data, enhanced by learnable matrices and nonlinear activations, improving the representation capability.<br /><br />5. CWCN is easy to implement, does not require additional hyperparameters beyond standard GNNs, and achieves competitive performance on benchmark datasets, demonstrating that complex-weighted diffusion offers a principled and effective way to increase GNN expressiveness and capability. <div>
arXiv:2511.13937v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across diverse applications, yet they remain limited by oversmoothing and poor performance on heterophilic graphs. To address these challenges, we introduce a novel framework that equips graphs with a complex-weighted structure, assigning each edge a complex number to drive a diffusion process that extends random walks into the complex domain. We prove that this diffusion is highly expressive: with appropriately chosen complex weights, any node-classification task can be solved in the steady state of a complex random walk. Building on this insight, we propose the Complex-Weighted Convolutional Network (CWCN), which learns suitable complex-weighted structures directly from data while enriching diffusion with learnable matrices and nonlinear activations. CWCN is simple to implement, requires no additional hyperparameters beyond those of standard GNNs, and achieves competitive performance on benchmark datasets. Our results demonstrate that complex-weighted diffusion provides a principled and general mechanism for enhancing GNN expressiveness, opening new avenues for models that are both theoretically grounded and practically effective.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Impact of Bootstrap Sampling Rate on Random Forest Performance in Regression Tasks</title>
<link>https://arxiv.org/abs/2511.13952</link>
<guid>https://arxiv.org/abs/2511.13952</guid>
<content:encoded><![CDATA[
<div> Keywords: Random Forests, bootstrap rate, regression datasets, bias-variance trade-off, hyperparameter tuning

<br /><br />Summary: This study investigates the impact of varying the bootstrap rate (BR) on Random Forest (RF) performance across 39 diverse regression datasets and 16 RF configurations. The analysis employs repeated two-fold cross-validation and measures performance using mean squared error. Results indicate that tuning the BR can lead to substantial performance enhancements compared to the default setting (BR = 1.0). Specifically, the optimal BR settings varied: BR ‚â§ 1.0 was best for 24 datasets, BR > 1.0 suited 15 datasets, and BR = 1.0 was optimal in only 4 cases. The research identifies a relationship between dataset characteristics and preferred BR: datasets with strong global feature-target correlations benefited from higher BRs, while those exhibiting greater local target variance fared better with lower BRs. Further experiments on synthetic datasets with controlled noise levels confirmed the bias-variance trade-off: higher BRs helped reduce bias in low-noise contexts, whereas lower BRs mitigated variance in high-noise environments. Overall, the findings underscore the importance of BR as a hyperparameter that should be carefully tuned to enhance RF regression models. <div>
arXiv:2511.13952v1 Announce Type: new 
Abstract: Random Forests (RFs) typically train each tree on a bootstrap sample of the same size as the training set, i.e., bootstrap rate (BR) equals 1.0. We systematically examine how varying BR from 0.2 to 5.0 affects RF performance across 39 heterogeneous regression datasets and 16 RF configurations, evaluating with repeated two-fold cross-validation and mean squared error. Our results demonstrate that tuning the BR can yield significant improvements over the default: the best setup relied on BR \leq 1.0 for 24 datasets, BR > 1.0 for 15, and BR = 1.0 was optimal in 4 cases only. We establish a link between dataset characteristics and the preferred BR: datasets with strong global feature-target relationships favor higher BRs, while those with higher local target variance benefit from lower BRs. To further investigate this relationship, we conducted experiments on synthetic datasets with controlled noise levels. These experiments reproduce the observed bias-variance trade-off: in low-noise scenarios, higher BRs effectively reduce model bias, whereas in high-noise settings, lower BRs help reduce model variance. Overall, BR is an influential hyperparameter that should be tuned to optimize RF regression models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient reconstruction of multidimensional random field models with heterogeneous data using stochastic neural networks</title>
<link>https://arxiv.org/abs/2511.13977</link>
<guid>https://arxiv.org/abs/2511.13977</guid>
<content:encoded><![CDATA[
<div> Keywords: Wasserstein-distance, stochastic neural networks, generalization error, multidimensional random fields, uncertainty quantification  

<br /><br />Summary: This paper investigates the scalability of a Wasserstein-distance method for training stochastic neural networks (SNNs) to reconstruct multidimensional random field models. The authors prove a generalization error bound when using a limited number of training data, highlighting that the convergence rate of this generalization error may not depend explicitly on the model's dimensionality, particularly when dealing with heterogeneous noise across dimensions. This finding suggests a potential alleviation of the "curse of dimensionality" in learning multi-dimensional random field models from finite datasets. Furthermore, improvements to the existing Wasserstein-distance SNN training approach are presented, emphasizing the robustness of the SNN in various scenarios. To validate their method, the researchers conduct numerical experiments across different tasks in multidimensional uncertainty quantification. The results demonstrate that the enhanced Wasserstein-distance training method can effectively train SNNs to learn complex multidimensional uncertainty models, showcasing its efficacy and potential for practical applications in stochastic modeling and uncertainty analysis in high-dimensional settings. <div>
arXiv:2511.13977v1 Announce Type: new 
Abstract: In this paper, we analyze the scalability of a recent Wasserstein-distance approach for training stochastic neural networks (SNNs) to reconstruct multidimensional random field models. We prove a generalization error bound for reconstructing multidimensional random field models on training stochastic neural networks with a limited number of training data. Our results indicate that when noise is heterogeneous across dimensions, the convergence rate of the generalization error may not depend explicitly on the model's dimensionality, partially alleviating the "curse of dimensionality" for learning multidimensional random field models from a finite number of data points. Additionally, we improve the previous Wasserstein-distance SNN training approach and showcase the robustness of the SNN. Through numerical experiments on different multidimensional uncertainty quantification tasks, we show that our Wasserstein-distance approach can successfully train stochastic neural networks to learn multidimensional uncertainty models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Whitening Improves Sparse Autoencoder Learning</title>
<link>https://arxiv.org/abs/2511.13981</link>
<guid>https://arxiv.org/abs/2511.13981</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse autoencoders, PCA Whitening, optimization landscape, interpretability, SAEBench

<br /><br />Summary: Sparse autoencoders (SAEs) are useful for learning interpretable features from neural network activations, but their training can be difficult due to input data correlations. The study introduces PCA Whitening as a preprocessing technique to enhance SAE performance across multiple metrics. Theoretical analysis and simulations indicate that whitening transforms the optimization landscape, making it more convex and easier to navigate. The evaluation includes both ReLU and Top-K SAEs across various model architectures, widths, and sparsity regimes. Empirical results from SAEBench, a benchmark for sparse autoencoders, demonstrate that whitening enhances interpretability metrics, such as sparse probing accuracy and feature disentanglement, despite slight decreases in reconstruction quality. This challenges the assumption that interpretability is directly linked to an optimal sparsity-fidelity trade-off. The findings advocate for incorporating whitening as a standard preprocessing step in SAE training, especially when prioritizing interpretability over perfect reconstruction. The study emphasizes the importance of reassessing strategies in SAE training to enhance feature interpretability effectively. <div>
arXiv:2511.13981v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) have emerged as a promising approach for learning interpretable features from neural network activations. However, the optimization landscape for SAE training can be challenging due to correlations in the input data. We demonstrate that applying PCA Whitening to input activations -- a standard preprocessing technique in classical sparse coding -- improves SAE performance across multiple metrics. Through theoretical analysis and simulation, we show that whitening transforms the optimization landscape, making it more convex and easier to navigate. We evaluate both ReLU and Top-K SAEs across diverse model architectures, widths, and sparsity regimes. Empirical evaluation on SAEBench, a comprehensive benchmark for sparse autoencoders, reveals that whitening consistently improves interpretability metrics, including sparse probing accuracy and feature disentanglement, despite minor drops in reconstruction quality. Our results challenge the assumption that interpretability aligns with an optimal sparsity--fidelity trade-off and suggest that whitening should be considered as a default preprocessing step for SAE training, particularly when interpretability is prioritized over perfect reconstruction.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Node-Level Uncertainty Estimation in LLM-Generated SQL</title>
<link>https://arxiv.org/abs/2511.13984</link>
<guid>https://arxiv.org/abs/2511.13984</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, SQL, error detection, uncertainty estimation, abstract syntax tree

<br /><br />Summary: This article presents a novel framework for detecting errors in SQL generated by large language models (LLMs) through uncertainty estimation at the individual node level in the query's abstract syntax tree (AST). The approach involves two main stages. Firstly, it introduces a semantically aware labeling algorithm that assigns node-level correctness for a generated SQL query relative to a gold standard, addressing concerns such as structural containers and alias variations without over-penalization. Secondly, the framework employs a rich set of features, including schema-aware and lexical attributes, to train a supervised classifier that predicts the error probabilities of each node. These probabilities are interpreted as calibrated uncertainty, allowing for precise diagnostics of potential errors within the SQL query. The proposed method outperforms traditional token log-probabilities, achieving an average area under the curve (AUC) improvement of 27.44%, while demonstrating robustness in cross-database evaluations. Additionally, node-level uncertainty aids in targeted repairs and supports human-in-the-loop review processes as well as selective execution downstream. Overall, this work positions node-centric, semantically grounded uncertainty estimation as a compelling alternative to aggregate confidence measures typically used in LLM-generated SQL evaluations. <div>
arXiv:2511.13984v1 Announce Type: new 
Abstract: We present a practical framework for detecting errors in LLM-generated SQL by estimating uncertainty at the level of individual nodes in the query's abstract syntax tree (AST). Our approach proceeds in two stages. First, we introduce a semantically aware labeling algorithm that, given a generated SQL and a gold reference, assigns node-level correctness without over-penalizing structural containers or alias variation. Second, we represent each node with a rich set of schema-aware and lexical features - capturing identifier validity, alias resolution, type compatibility, ambiguity in scope, and typo signals - and train a supervised classifier to predict per-node error probabilities. We interpret these probabilities as calibrated uncertainty, enabling fine-grained diagnostics that pinpoint exactly where a query is likely to be wrong. Across multiple databases and datasets, our method substantially outperforms token log-probabilities: average AUC improves by +27.44% while maintaining robustness under cross-database evaluation. Beyond serving as an accuracy signal, node-level uncertainty supports targeted repair, human-in-the-loop review, and downstream selective execution. Together, these results establish node-centric, semantically grounded uncertainty estimation as a strong and interpretable alternative to aggregate sequence level confidence measures.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Gradient Complexity of Private Optimization with Private Oracles</title>
<link>https://arxiv.org/abs/2511.13999</link>
<guid>https://arxiv.org/abs/2511.13999</guid>
<content:encoded><![CDATA[
<div> Keywords: differential privacy, empirical risk minimization, oracle queries, Lipschitz convex losses, optimization

<br /><br />Summary: The paper investigates the running time of differentially private empirical and population risk minimization when using Lipschitz convex losses, focusing on first-order oracle queries. In the non-smooth loss setting, it establishes that to achieve an excess risk of $\alpha$ on a problem of dimension $d$, a necessary expected running time is $\Omega(\min\{\frac{\sqrt{d}}{\alpha^2}, \frac{d}{\log(1/\alpha)}\})$, particularly when $d \geq 1/\alpha^2$. This lower bound is tightened to $\Omega(\min\{\frac{d}{\bar{m}\alpha^2}, \frac{d}{\log(1/\alpha)}\})$ for algorithms using minibatches of size at most $\bar{m} < \sqrt{d}$. For smooth losses, it removes the private oracle assumption and finds a lower bound on oracle calls as $\tilde{\Omega}\big(\frac{\sqrt{d}}{\alpha} + \min\{\frac{1}{\alpha^2}, n\}\big)$, where $n$ is the dataset size. The paper highlights that differentially private optimizers face a runtime penalty dependent on the problem dimension. It also addresses fundamental limitations in optimization when oracles are restricted in the information they can transmit about gradients. <div>
arXiv:2511.13999v1 Announce Type: new 
Abstract: We study the running time, in terms of first order oracle queries, of differentially private empirical/population risk minimization of Lipschitz convex losses. We first consider the setting where the loss is non-smooth and the optimizer interacts with a private proxy oracle, which sends only private messages about a minibatch of gradients. In this setting, we show that expected running time $\Omega(\min\{\frac{\sqrt{d}}{\alpha^2}, \frac{d}{\log(1/\alpha)}\})$ is necessary to achieve $\alpha$ excess risk on problems of dimension $d$ when $d \geq 1/\alpha^2$. Upper bounds via DP-SGD show these results are tight when $d>\tilde{\Omega}(1/\alpha^4)$. We further show our lower bound can be strengthened to $\Omega(\min\{\frac{d}{\bar{m}\alpha^2}, \frac{d}{\log(1/\alpha)} \})$ for algorithms which use minibatches of size at most $\bar{m} < \sqrt{d}$. We next consider smooth losses, where we relax the private oracle assumption and give lower bounds under only the condition that the optimizer is private. Here, we lower bound the expected number of first order oracle calls by $\tilde{\Omega}\big(\frac{\sqrt{d}}{\alpha} + \min\{\frac{1}{\alpha^2}, n\}\big)$, where $n$ is the size of the dataset. Modifications to existing algorithms show this bound is nearly tight. Compared to non-private lower bounds, our results show that differentially private optimizers pay a dimension dependent runtime penalty. Finally, as a natural extension of our proof technique, we show lower bounds in the non-smooth setting for optimizers interacting with information limited oracles. Specifically, if the proxy oracle transmits at most $\Gamma$-bits of information about the gradients in the minibatch, then $\Omega\big(\min\{\frac{d}{\alpha^2\Gamma}, \frac{d}{\log(1/\alpha)}\}\big)$ oracle calls are needed. This result shows fundamental limitations of gradient quantization techniques in optimization.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Marginalize in Causal Structure Learning?</title>
<link>https://arxiv.org/abs/2511.14001</link>
<guid>https://arxiv.org/abs/2511.14001</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian networks, probabilistic graphical models, structure learning, marginalization, probabilistic circuits  

<br /><br />Summary:  
Bayesian networks (BNs) are essential probabilistic graphical models used across various fields, yet inferring their graphical structure from data remains a significant challenge. Traditional Bayesian structure learners work by estimating a posterior distribution over possible directed acyclic graphs, but this requires marginalizing over distribution, often constrained by dynamic programming methods that limit potential parent nodes. The authors introduce a novel approach that employs tractable probabilistic circuits to address these limitations. This new method incorporates a learning routine that trains circuits on both the original probability distribution and marginal queries, enabling more flexible model representation. The architecture of probabilistic circuits allows for rapid and accurate marginalization of the learned distribution, thus enhancing the inference process. Empirical results demonstrate that this innovative marginalization technique significantly boosts the performance of Bayesian structure learners compared to existing methods. The findings suggest that leveraging probabilistic circuits can lead to substantial improvements in the accuracy and efficiency of learning tasks involving Bayesian networks. <div>
arXiv:2511.14001v1 Announce Type: new 
Abstract: Bayesian networks (BNs) are a widely used class of probabilistic graphical models employed in numerous application domains. However, inferring the network's graphical structure from data remains challenging. Bayesian structure learners approach this problem by inferring a posterior distribution over the possible directed acyclic graphs underlying the BN. The inference process often requires marginalizing over probability distributions, which is typically done using dynamic programming methods that restrict the set of possible parents for each node. Instead, we present a novel method that utilizes tractable probabilistic circuits to circumvent this restriction. This method utilizes a new learning routine that trains these circuits on both the original distribution and marginal queries. The architecture of probabilistic circuits then inherently allows for fast and exact marginalization on the learned distribution. We then show empirically that utilizing our method to answer marginals allows Bayesian structure learners to improve their performance compared to current methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Certified but Fooled! Breaking Certified Defences with Ghost Certificates</title>
<link>https://arxiv.org/abs/2511.14003</link>
<guid>https://arxiv.org/abs/2511.14003</guid>
<content:encoded><![CDATA[
<div> Keywords: certified defenses, adversarial input, robustness guarantee, imperceptible perturbations, ImageNet

<br /><br />Summary: This study explores the malicious exploitation of probabilistic certification frameworks in machine learning to understand the constraints of robustness guarantees. The researchers focus on the dual objective of misleading a classifier while also manipulating the certification process, specifically crafting adversarial inputs that can falsely generate robustness certificates. Previous work demonstrated that large perturbations can shift inputs far enough to create a misleading certificate for an incorrect class. This study investigated whether it is possible to create smaller and imperceptible perturbations that achieve misclassification while still prompting certified models to provide deceptive robustness radii for a target class. The concept of region-focused adversarial examples is introduced to craft these imperceptible perturbations, effectively spoofing certificates and generating larger certification radii than those associated with the original source class. Through extensive evaluations on the ImageNet dataset, the study illustrates the ability to effectively bypass state-of-the-art certified defenses, such as Densepure. Ultimately, the findings highlight the urgent need to better understand the limitations of current robustness certification methods in the context of adversarial attacks. <div>
arXiv:2511.14003v1 Announce Type: new 
Abstract: Certified defenses promise provable robustness guarantees. We study the malicious exploitation of probabilistic certification frameworks to better understand the limits of guarantee provisions. Now, the objective is to not only mislead a classifier, but also manipulate the certification process to generate a robustness guarantee for an adversarial input certificate spoofing. A recent study in ICLR demonstrated that crafting large perturbations can shift inputs far into regions capable of generating a certificate for an incorrect class. Our study investigates if perturbations needed to cause a misclassification and yet coax a certified model into issuing a deceptive, large robustness radius for a target class can still be made small and imperceptible. We explore the idea of region-focused adversarial examples to craft imperceptible perturbations, spoof certificates and achieve certification radii larger than the source class ghost certificates. Extensive evaluations with the ImageNet demonstrate the ability to effectively bypass state-of-the-art certified defenses such as Densepure. Our work underscores the need to better understand the limits of robustness certification methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Narrow Unlearning to Emergent Misalignment: Causes, Consequences, and Containment in LLMs</title>
<link>https://arxiv.org/abs/2511.14017</link>
<guid>https://arxiv.org/abs/2511.14017</guid>
<content:encoded><![CDATA[
<div> Keywords: emergent misalignment, refusal unlearning, Cybersecurity, Safety, responsible AI  

<br /><br />Summary: This study builds upon recent findings regarding emergent misalignment (EMA) phenomena triggered by fine-tuning models on insecure code data, leading to harmful outputs even for unrelated prompts. The researchers demonstrate that EMA can also arise from narrow refusal unlearning in specific domains, particularly focusing on Cybersecurity and Safety. Through monitoring refusal scores across seven responsible AI (RAI) domains‚ÄîCybersecurity, Safety, Toxicity, Bias, Sensitive Content, Medical/Legal, and Privacy‚Äîthe study reveals that while targeted refusal unlearning can improve compliance for the intended concept, it inadvertently propagates EMA to unrelated areas. Notably, the Safety concept exhibited a more significant detrimental impact on refusal scores in domains like Bias compared to Cybersecurity. The effects were consistent across two model families, Mistral-7b-0.3v and Qwen-7b-2.5. Additionally, the researchers found that enhancing refusal unlearning with a cross-entropy loss function, using a small set of retained data from affected domains, can help in restoring alignment while still achieving lower refusal rates for the unlearned concept. An analysis of concept vectors indicated that concepts with similar representations in early layers are more prone to experiencing EMA following targeted unlearning interventions. <div>
arXiv:2511.14017v1 Announce Type: new 
Abstract: Recent work has shown that fine-tuning on insecure code data can trigger an emergent misalignment (EMA) phenomenon, where models generate malicious responses even to prompts unrelated to the original insecure code-writing task. Such cross-domain generalization of harmful behavior underscores the need for a deeper understanding of the algorithms, tasks, and datasets that induce emergent misalignment. In this work, we extend this study by demonstrating that emergent misalignment can also arise from narrow refusal unlearning in specific domains. We perform refusal unlearning on Cybersecurity and Safety concept, and evaluate EMA by monitoring refusal scores across seven responsible AI (RAI) domains, Cybersecurity, Safety, Toxicity, Bias, Sensitive Content, Medical/Legal, and Privacy. Our work shows that narrow domain unlearning can yield compliance responses for the targeted concept, however, it may also propagate EMA to unrelated domains. Among the two intervened concepts, Cybersecurity and Safety, we find that the safety concept can have larger EMA impact, i.e, causing lower refusal scores, across other unrelated domains such as bias. We observe this effect consistently across two model families, Mistral-7b-0.3v, and Qwen-7b-2.5. Further, we show that refusal unlearning augmented with cross-entropy loss function on a small set of retain data from the affected domains can largely, if not fully, restore alignment across the impacted domains while having lower refusal rate on the concept we perform unlearning on. To investigate the underlying causes of EMA, we analyze concept entanglements at the representation level via concept vectors. Our analysis reveals that concepts with higher representation similarity in earlier layers are more susceptible to EMA after intervention when the refusal stream is altered through targeted refusal unlearning.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmallML: Bayesian Transfer Learning for Small-Data Predictive Analytics</title>
<link>https://arxiv.org/abs/2511.14049</link>
<guid>https://arxiv.org/abs/2511.14049</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian transfer learning, hierarchical models, conformal prediction, small-data analytics, SME machine learning  

<br /><br />Summary: Small and medium-sized enterprises (SMEs) comprise 99.9% of U.S. businesses, yet they often face challenges in utilizing AI due to their limited data availability. This paper presents SmallML, a Bayesian transfer learning framework that achieves accurate predictions with datasets as small as 50-200 observations. The three-layer architecture of SmallML incorporates transfer learning, hierarchical Bayesian modeling, and conformal prediction. 

Layer 1 utilizes SHAP-based techniques to extract useful priors from a substantial dataset of 22,673 public records, transferring knowledge from gradient boosting to logistic regression. Layer 2 implements hierarchical pooling across 5-50 SMEs, allowing for adaptive shrinkage that balances general patterns with specific characteristics of each entity. Layer 3 offers conformal sets that ensure finite-sample coverage guarantees for uncertainty quantification. 

Validation results on customer churn data show an impressive AUC of 96.7% with only 100 observations per business, representing a significant improvement over traditional methods. With training completed in just 33 minutes on standard hardware, SmallML facilitates enterprise-level predictions for the 33 million U.S. SMEs, effectively bridging a critical gap in AI accessibility and democratization. <div>
arXiv:2511.14049v1 Announce Type: new 
Abstract: Small and medium-sized enterprises (SMEs) represent 99.9% of U.S. businesses yet remain systematically excluded from AI due to a mismatch between their operational scale and modern machine learning's data requirements. This paper introduces SmallML, a Bayesian transfer learning framework achieving enterprise-level prediction accuracy with datasets as small as 50-200 observations.
  We develop a three-layer architecture integrating transfer learning, hierarchical Bayesian modeling, and conformal prediction. Layer 1 extracts informative priors from 22,673 public records using a SHAP-based procedure transferring knowledge from gradient boosting to logistic regression. Layer 2 implements hierarchical pooling across J=5-50 SMEs with adaptive shrinkage, balancing population patterns with entity-specific characteristics. Layer 3 provides conformal sets with finite-sample coverage guarantees P(y in C(x)) >= 1-alpha for distribution-free uncertainty quantification.
  Validation on customer churn data demonstrates 96.7% +/- 4.2% AUC with 100 observations per business -- a +24.2 point improvement over independent logistic regression (72.5% +/- 8.1%), with p < 0.000001. Conformal prediction achieves 92% empirical coverage at 90% target. Training completes in 33 minutes on standard CPU hardware. By enabling enterprise-grade predictions for 33 million U.S. SMEs previously excluded from machine learning, SmallML addresses a critical gap in AI democratization.
  Keywords: Bayesian transfer learning, hierarchical models, conformal prediction, small-data analytics, SME machine learning
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds</title>
<link>https://arxiv.org/abs/2511.14056</link>
<guid>https://arxiv.org/abs/2511.14056</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, curved spaces, Radial Compensation, Balanced-Exponential, likelihoods  

<br /><br />Summary: The paper discusses challenges in generative models on curved spaces, notably the issues with exponential maps that have stiff Jacobians, and volume-preserving charts that distort geodesic distances. These methods entangle curvature with model parameters, increasing gradient variance. The authors introduce Radial Compensation (RC), an information-geometric technique that selects a base density in the tangent space, allowing likelihood to depend solely on geodesic distance from a pole, thereby decoupling parameter meaning from curvature. They extend RC to manifolds with known geodesic polar volume, demonstrating that RC offers geodesic-radial likelihoods with curvature-invariant Fisher information. The work culminates in the derivation of the Balanced-Exponential (bExp) chart family, which balances volume distortion and geodesic error. All bExp variants under RC maintain consistent manifold density and Fisher information, with reduced gradient variance and flow cost achieved through smaller dial values. Empirical results show that RC enhances generative model stability across various densities, including VAEs and flows on images, graphs, and proteins. It improves test likelihoods, restores accurate geodesic radii, and mitigates radius blow-ups, making RC-bExp a recommended default for likelihood-trained generative models on manifolds. <div>
arXiv:2511.14056v1 Announce Type: new 
Abstract: Generative models on curved spaces rely on charts to map Euclidean spaces to manifolds. Exponential maps preserve geodesics but have stiff, radius-dependent Jacobians, while volume-preserving charts maintain densities but distort geodesic distances. Both approaches entangle curvature with model parameters, inflating gradient variance. In high-dimensional latent normalizing flows, the wrapped exponential prior can stretch radii far beyond the curvature scale, leading to poor test likelihoods and stiff solvers. We introduce Radial Compensation (RC), an information-geometric method that selects the base density in the tangent space so that the likelihood depends only on geodesic distance from a pole, decoupling parameter semantics from curvature. RC lets radial parameters retain their usual meaning in geodesic units, while the chart can be tuned as a numerical preconditioner. We extend RC to manifolds with known geodesic polar volume and show that RC is the only construction for geodesic-radial likelihoods with curvature-invariant Fisher information. We derive the Balanced-Exponential (bExp) chart family, balancing volume distortion and geodesic error. Under RC, all bExp settings preserve the same manifold density and Fisher information, with smaller dial values reducing gradient variance and flow cost. Empirically, RC yields stable generative models across densities, VAEs, flows on images and graphs, and protein models. RC improves likelihoods, restores clean geodesic radii, and prevents radius blow-ups in high-dimensional flows, making RC-bExp a robust default for likelihood-trained generative models on manifolds.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Machine Learning-Based Multimodal Framework for Wearable Sensor-Based Archery Action Recognition and Stress Estimation</title>
<link>https://arxiv.org/abs/2511.14057</link>
<guid>https://arxiv.org/abs/2511.14057</guid>
<content:encoded><![CDATA[
<div> Keywords: archery, machine learning, wearable sensors, motion recognition, stress estimation  

<br /><br />Summary:  
In precision sports like archery, athlete performance hinges on biomechanical stability and psychological resilience. Traditional motion analysis tools are costly and intrusive, limiting their application in natural settings. To overcome this challenge, the authors propose a machine learning-based multimodal framework that utilizes wearable sensor data for simultaneous action recognition and stress estimation. They developed a wrist-worn device with an accelerometer and a photoplethysmography (PPG) sensor to collect synchronized motion and physiological data during real archery sessions. For motion recognition, they introduce a new feature called Smoothed Differential Acceleration (SmoothDiff) and employ a Long Short-Term Memory (LSTM) model, achieving an impressive 96.8% accuracy and 95.9% F1-score. For stress estimation, heart rate variability (HRV) features from PPG signals are extracted and a Multi-Layer Perceptron (MLP) classifier is utilized, yielding 80% accuracy in differentiating high- and low-stress levels. The framework highlights the potential of integrating motion and physiological sensing to gain valuable insights into athletes' performance and mental states, paving the way for intelligent, real-time feedback systems aimed at optimizing training in archery and other precision sports. <div>
arXiv:2511.14057v1 Announce Type: new 
Abstract: In precision sports such as archery, athletes' performance depends on both biomechanical stability and psychological resilience. Traditional motion analysis systems are often expensive and intrusive, limiting their use in natural training environments. To address this limitation, we propose a machine learning-based multimodal framework that integrates wearable sensor data for simultaneous action recognition and stress estimation. Using a self-developed wrist-worn device equipped with an accelerometer and photoplethysmography (PPG) sensor, we collected synchronized motion and physiological data during real archery sessions. For motion recognition, we introduce a novel feature--Smoothed Differential Acceleration (SmoothDiff)--and employ a Long Short-Term Memory (LSTM) model to identify motion phases, achieving 96.8% accuracy and 95.9% F1-score. For stress estimation, we extract heart rate variability (HRV) features from PPG signals and apply a Multi-Layer Perceptron (MLP) classifier, achieving 80% accuracy in distinguishing high- and low-stress levels. The proposed framework demonstrates that integrating motion and physiological sensing can provide meaningful insights into athletes' technical and mental states. This approach offers a foundation for developing intelligent, real-time feedback systems for training optimization in archery and other precision sports.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CafeMed: Causal Attention Fusion Enhanced Medication Recommendation</title>
<link>https://arxiv.org/abs/2511.14064</link>
<guid>https://arxiv.org/abs/2511.14064</guid>
<content:encoded><![CDATA[
<div> Keywords: medication recommendation, dynamic causal reasoning, cross-modal attention, patient states, MIMIC dataset

<br /><br />Summary:  
Medication recommendation systems are essential for clinicians to make personalized treatment decisions. Current methods face two main challenges: they often treat medical entities as independent features and rely on static causal relationships that do not adapt to individual patient contexts. To overcome these limitations, this article presents CafeMed, a novel framework that integrates dynamic causal reasoning with cross-modal attention for medication recommendation. CafeMed features two innovative components: the Causal Weight Generator (CWG), which converts static causal effects into dynamic modulation weights tailored to individual patient states, and the Channel Harmonized Attention Refinement Module (CHARM), which captures the intricate interdependencies among diagnoses and procedures. This design allows CafeMed to model how various medical conditions collaboratively inform treatment decisions while ensuring medication safety. Evaluation using the MIMIC-III and MIMIC-IV datasets demonstrates that CafeMed outperforms existing state-of-the-art methodologies, offering greater accuracy in medication predictions alongside lower rates of drug-drug interactions. The findings highlight that integrating dynamic causal relationships and cross-modal synergies results in more personalized and clinically relevant medication recommendations. The code for CafeMed is openly accessible at https://github.com/rkl71/CafeMed. <div>
arXiv:2511.14064v1 Announce Type: new 
Abstract: Medication recommendation systems play a crucial role in assisting clinicians with personalized treatment decisions. While existing approaches have made significant progress in learning medication representations, they suffer from two fundamental limitations: (i) treating medical entities as independent features without modeling their synergistic effects on medication selection; (ii) employing static causal relationships that fail to adapt to patient-specific contexts and health states. To address these challenges, we propose CafeMed, a framework that integrates dynamic causal reasoning with cross-modal attention for safe and accurate medication recommendation. CafeMed introduces two key components: the Causal Weight Generator (CWG) that transforms static causal effects into dynamic modulation weights based on individual patient states, and the Channel Harmonized Attention Refinement Module (CHARM) that captures complex interdependencies between diagnoses and procedures. This design enables CafeMed to model how different medical conditions jointly influence treatment decisions while maintaining medication safety constraints. Extensive experiments on MIMIC-III and MIMIC-IV datasets demonstrate that CafeMed significantly outperforms state-of-the-art baselines, achieving superior accuracy in medication prediction while maintaining the lower drug--drug interaction rates. Our results indicate that incorporating dynamic causal relationships and cross-modal synergies leads to more clinically-aligned and personalized medication recommendations. Our code is released publicly at https://github.com/rkl71/CafeMed.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CFG-EC: Error Correction Classifier-Free Guidance</title>
<link>https://arxiv.org/abs/2511.14075</link>
<guid>https://arxiv.org/abs/2511.14075</guid>
<content:encoded><![CDATA[
<div> Keywords: Classifier-Free Guidance, CFG-EC, unconditional noise correction, generative models, prompt fidelity<br /><br />Summary:  
Classifier-Free Guidance (CFG) is widely used in conditional generative models to improve both prompt fidelity and generation quality by alternating training between conditional and null prompts. However, CFG‚Äôs sampling process involves simultaneous outputs of null and conditional prompts, resulting in inconsistent noise estimates compared to training. This inconsistency can degrade image generation quality. To address this, the paper proposes CFG-EC, a correction scheme that can be applied to any CFG-based method by refining the unconditional noise predictions. CFG-EC works by realigning the unconditional noise error component to be orthogonal to the conditional error component, preventing interference between guidance signals. This approach effectively constrains the upper bound of sampling error, resulting in more reliable guidance trajectories. Numerical experiments demonstrate that CFG-EC outperforms standard CFG and CFG++ methods, particularly in settings with low guidance strength. It achieves a significant performance boost in prompt alignment and overall image generation quality. Thus, CFG-EC represents a robust refinement to existing CFG approaches, enabling higher fidelity and more consistent conditional image synthesis. <div>
arXiv:2511.14075v1 Announce Type: new 
Abstract: Classifier-Free Guidance (CFG) has become a mainstream approach for simultaneously improving prompt fidelity and generation quality in conditional generative models. During training, CFG stochastically alternates between conditional and null prompts to enable both conditional and unconditional generation. However, during sampling, CFG outputs both null and conditional prompts simultaneously, leading to inconsistent noise estimates between the training and sampling processes. To reduce this error, we propose CFG-EC, a versatile correction scheme augmentable to any CFG-based method by refining the unconditional noise predictions. CFG-EC actively realigns the unconditional noise error component to be orthogonal to the conditional error component. This corrective maneuver prevents interference between the two guidance components, thereby constraining the sampling error's upper bound and establishing more reliable guidance trajectories for high-fidelity image generation. Our numerical experiments show that CFG-EC handles the unconditional component more effectively than CFG and CFG++, delivering a marked performance increase in the low guidance sampling regime and consistently higher prompt alignment across the board.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-SimGNN: Adaptive and Robust WiFi Localization Across Dynamic Configurations and Diverse Scenarios</title>
<link>https://arxiv.org/abs/2511.14076</link>
<guid>https://arxiv.org/abs/2511.14076</guid>
<content:encoded><![CDATA[
<div> Keywords: meta-learning, WiFi localization, graph neural networks, channel state information, adaptability

<br /><br />Summary: This article addresses the practicality of deep learning-based localization, focusing on the often-overlooked impact of device configuration changes, such as bandwidth and the number of access points (APs). These variations affect the dimensionality of channel state information (CSI), which can hinder the performance of neural networks. To tackle this challenge, the authors introduce Meta-SimGNN, a novel localization system that combines graph neural networks with meta-learning. The system features a fine-grained CSI graph construction scheme, treating each AP as a graph node to adapt to varying numbers of APs. To enhance reliability, it employs an amplitude-phase fusion method and a feature extraction technique that maintains dimension consistency across different configurations like bandwidth and antenna count. Additionally, a similarity-guided meta-learning strategy enables the system to quickly adjust to new scenarios by using historical data for model parameter initialization. Experimental results demonstrate that Meta-SimGNN significantly outperforms baseline methods in localization generalization and accuracy across various scenarios with commodity WiFi devices. <div>
arXiv:2511.14076v1 Announce Type: new 
Abstract: To promote the practicality of deep learning-based localization, existing studies aim to address the issue of scenario dependence through meta-learning. However, these studies primarily focus on variations in environmental layouts while overlooking the impact of changes in device configurations, such as bandwidth, the number of access points (APs), and the number of antennas used. Unlike environmental changes, variations in device configurations affect the dimensionality of channel state information (CSI), thereby compromising neural network usability. To address this issue, we propose Meta-SimGNN, a novel WiFi localization system that integrates graph neural networks with meta-learning to improve localization generalization and robustness. First, we introduce a fine-grained CSI graph construction scheme, where each AP is treated as a graph node, allowing for adaptability to changes in the number of APs. To structure the features of each node, we propose an amplitude-phase fusion method and a feature extraction method. The former utilizes both amplitude and phase to construct CSI images, enhancing data reliability, while the latter extracts dimension-consistent features to address variations in bandwidth and the number of antennas. Second, a similarity-guided meta-learning strategy is developed to enhance adaptability in diverse scenarios. The initial model parameters for the fine-tuning stage are determined by comparing the similarity between the new scenario and historical scenarios, facilitating rapid adaptation of the model to the new localization scenario. Extensive experimental results over commodity WiFi devices in different scenarios show that Meta-SimGNN outperforms the baseline methods in terms of localization generalization and accuracy.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Observational Auditing of Label Privacy</title>
<link>https://arxiv.org/abs/2511.14084</link>
<guid>https://arxiv.org/abs/2511.14084</guid>
<content:encoded><![CDATA[
<div> Keywords: differential privacy, auditing, machine learning, observational framework, label privacy

<br /><br />Summary: Differential privacy (DP) auditing is crucial for assessing privacy guarantees in machine learning systems. Traditional auditing methods often require modifications to the training dataset, like injecting out-of-distribution canaries or removing samples, which can be resource-intensive and complicated to implement. This paper introduces a novel observational auditing framework that utilizes the inherent randomness within data distributions, allowing for privacy evaluations without altering the original dataset. The proposed approach broadens the scope of privacy auditing, extending it beyond conventional membership inference to include protected attributes, with labels being a specific focus. This addresses significant gaps in existing auditing techniques. The authors provide a theoretical foundation for their methodology and conduct experiments on well-known datasets, including Criteo and CIFAR-10, to showcase its effectiveness in auditing label privacy guarantees. This research paves the way for more practical privacy auditing solutions suitable for large-scale production environments, making it easier to evaluate privacy risks without the extensive engineering efforts typically required by existing methods. Overall, this work contributes to enhancing the reliability of privacy assessments in machine learning applications. <div>
arXiv:2511.14084v1 Announce Type: new 
Abstract: Differential privacy (DP) auditing is essential for evaluating privacy guarantees in machine learning systems. Existing auditing methods, however, pose a significant challenge for large-scale systems since they require modifying the training dataset -- for instance, by injecting out-of-distribution canaries or removing samples from training. Such interventions on the training data pipeline are resource-intensive and involve considerable engineering overhead. We introduce a novel observational auditing framework that leverages the inherent randomness of data distributions, enabling privacy evaluation without altering the original dataset. Our approach extends privacy auditing beyond traditional membership inference to protected attributes, with labels as a special case, addressing a key gap in existing techniques. We provide theoretical foundations for our method and perform experiments on Criteo and CIFAR-10 datasets that demonstrate its effectiveness in auditing label privacy guarantees. This work opens new avenues for practical privacy auditing in large-scale production environments.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoE-SpeQ: Speculative Quantized Decoding with Proactive Expert Prefetching and Offloading for Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2511.14102</link>
<guid>https://arxiv.org/abs/2511.14102</guid>
<content:encoded><![CDATA[
<div> Keywords: Mixture-of-Experts, MoE-SpeQ, inference, expert offloading, data movement  

<br /><br />Summary: The paper addresses the significant memory challenges posed by state-of-the-art Mixture-of-Experts (MoE) models during inference, often exceeding a single accelerator's capacity. Offloading experts to host memory is a common strategy but results in an I/O bottleneck due to synchronous data-dependent expert selection, negatively impacting performance. The authors propose MoE-SpeQ, a novel inference system that combines speculative execution with expert offloading. By utilizing a small on-device draft model, MoE-SpeQ can predict the required experts for future tokens, allowing a runtime orchestrator to prefetch data from host memory. This prefetching process effectively overlaps the costly I/O operations with useful computations, thus hiding latency from the critical execution path. To further enhance performance, an adaptive governor is implemented, utilizing an Amortization Roofline Model to dynamically optimize the speculation strategy based on available hardware. Experimental results on memory-constrained devices demonstrate that MoE-SpeQ achieves up to a 2.34x speedup over existing offloading frameworks for the Phi-MoE model. The study establishes a new method for managing data-dependent memory access in resource-limited settings, facilitating MoE inference on typical hardware. <div>
arXiv:2511.14102v1 Announce Type: new 
Abstract: The immense memory requirements of state-of-the-art Mixture-of-Experts (MoE) models present a significant challenge for inference, often exceeding the capacity of a single accelerator. While offloading experts to host memory is a common solution, it introduces a severe I/O bottleneck over the PCIe bus, as the data-dependent nature of expert selection places these synchronous transfers directly on the critical path of execution, crippling performance.
  This paper argues that the I/O bottleneck can be overcome by trading a small amount of cheap, on-device computation to hide the immense cost of data movement. We present MoE-SpeQ, a new inference system built on a novel co-design of speculative execution and expert offloading. MoE-SpeQ employs a small, on-device draft model to predict the sequence of required experts for future tokens. This foresight enables a runtime orchestrator to prefetch these experts from host memory, effectively overlapping the expensive I/O with useful computation and hiding the latency from the critical path. To maximize performance, an adaptive governor, guided by an Amortization Roofline Model, dynamically tunes the speculation strategy to the underlying hardware. Our evaluation on memory-constrained devices shows that for the Phi-MoE model, MoE-SpeQ achieves at most 2.34x speedup over the state-of-the-art offloading framework. Our work establishes a new, principled approach for managing data-dependent memory access in resource-limited environments, making MoE inference more accessible on commodity hardware.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft-Label Training Preserves Epistemic Uncertainty</title>
<link>https://arxiv.org/abs/2511.14117</link>
<guid>https://arxiv.org/abs/2511.14117</guid>
<content:encoded><![CDATA[
<div> Keywords: subjectivity, soft-label training, epistemic uncertainty, annotation distribution, machine learning<br /><br />Summary: Many machine learning tasks involve inherent subjectivity, where human annotators provide diverse labels rather than a single definitive answer. Traditionally, these varied labels are collapsed into single hard labels, which ignores the underlying uncertainty and variability of human judgment. The authors argue that this conventional practice is epistemically misaligned because it forces models to express unwarranted confidence on ambiguous data. Instead, they propose treating the entire annotation distribution‚Äîreflecting the range of human opinions‚Äîas the ground truth. By training models on these soft labels, the models better preserve epistemic uncertainty in their predictions. Empirical results across both vision and NLP tasks show that soft-label training produces outputs with 32% lower Kullback-Leibler divergence from human annotation distributions and a 61% stronger correlation between the model‚Äôs predictive entropy and the annotation entropy compared to hard-label training. Importantly, this approach achieves comparable accuracy to conventional single-label training. Their work reframes annotation distributions not as noise to be aggregated away but as meaningful representations of intrinsic ambiguity that models should learn to reproduce, ultimately enhancing model reliability and interpretability in subjective tasks. <div>
arXiv:2511.14117v1 Announce Type: new 
Abstract: Many machine learning tasks involve inherent subjectivity, where annotators naturally provide varied labels. Standard practice collapses these label distributions into single labels, aggregating diverse human judgments into point estimates. We argue that this approach is epistemically misaligned for ambiguous data--the annotation distribution itself should be regarded as the ground truth. Training on collapsed single labels forces models to express false confidence on fundamentally ambiguous cases, creating a misalignment between model certainty and the diversity of human perception. We demonstrate empirically that soft-label training, which treats annotation distributions as ground truth, preserves epistemic uncertainty. Across both vision and NLP tasks, soft-label training achieves 32% lower KL divergence from human annotations and 61% stronger correlation between model and annotation entropy, while matching the accuracy of hard-label training. Our work repositions annotation distributions from noisy signals to be aggregated away, to faithful representations of epistemic uncertainty that models should learn to reproduce.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Survival Control: Extending Synthetic Controls for "When-If" Decision</title>
<link>https://arxiv.org/abs/2511.14133</link>
<guid>https://arxiv.org/abs/2511.14133</guid>
<content:encoded><![CDATA[
<div> Keywords: causal effects, survival analysis, Synthetic Survival Control, counterfactual hazard, observational data<br /><br />Summary:<br /><br />1. The paper addresses the challenge of estimating causal effects on time-to-event (survival) outcomes from observational data, which is complicated due to censoring, limited sample sizes, and non-random treatment assignments.<br />2. It focuses on answering "when-if" counterfactual questions, i.e., how the timing of an event changes under specified interventions, especially in settings with heterogeneous treatment adoption and confounding.<br />3. The authors propose a novel method called Synthetic Survival Control (SSC) in a panel data context where multiple units receive different treatments over multiple time periods.<br />4. SSC estimates counterfactual hazard trajectories for a unit by creating a weighted combination of observed survival trajectories from other units, leveraging a low-rank structure framework that aligns with classical parametric survival models.<br />5. They provide formal identification results and finite sample guarantees for SSC within this framework.<br />6. Empirical validation is done using a multi-country clinical cancer dataset, where staggered introduction of treatments creates a quasi-experimental design.<br />7. Results show that access to novel treatments correlates with improved survival, demonstrated by lower hazard trajectories compared to synthetic controls.<br />8. The framework is broadly applicable across medicine, economics, and public policy for interpretable counterfactual survival inference using observational data. <div>
arXiv:2511.14133v1 Announce Type: new 
Abstract: Estimating causal effects on time-to-event outcomes from observational data is particularly challenging due to censoring, limited sample sizes, and non-random treatment assignment. The need for answering such "when-if" questions--how the timing of an event would change under a specified intervention--commonly arises in real-world settings with heterogeneous treatment adoption and confounding. To address these challenges, we propose Synthetic Survival Control (SSC) to estimate counterfactual hazard trajectories in a panel data setting where multiple units experience potentially different treatments over multiple periods. In such a setting, SSC estimates the counterfactual hazard trajectory for a unit of interest as a weighted combination of the observed trajectories from other units. To provide formal justification, we introduce a panel framework with a low-rank structure for causal survival analysis. Indeed, such a structure naturally arises under classical parametric survival models. Within this framework, for the causal estimand of interest, we establish identification and finite sample guarantees for SSC. We validate our approach using a multi-country clinical dataset of cancer treatment outcomes, where the staggered introduction of new therapies creates a quasi-experimental setting. Empirically, we find that access to novel treatments is associated with improved survival, as reflected by lower post-intervention hazard trajectories relative to their synthetic counterparts. Given the broad relevance of survival analysis across medicine, economics, and public policy, our framework offers a general and interpretable tool for counterfactual survival inference using observational data.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair-GNE : Generalized Nash Equilibrium-Seeking Fairness in Multiagent Healthcare Automation</title>
<link>https://arxiv.org/abs/2511.14135</link>
<guid>https://arxiv.org/abs/2511.14135</guid>
<content:encoded><![CDATA[
<div> Keywords: fair workload allocation, multi-agent reinforcement learning, generalized Nash equilibrium, healthcare systems, adaptive constraint enforcement  

<br /><br />Summary:  
This article addresses the critical need for fair workload allocation among multiple agents in learning-enabled healthcare worker settings. It critiques existing multi-agent reinforcement learning (MARL) methods that rely on post hoc reward shaping to promote fairness, which lacks self-enforceable measures during runtime. The authors propose a novel framework, Fair-GNE, modelled as a constrained generalized Nash equilibrium (GNE) game, where self-interested agents enter collective decision-making that promotes equity. This framework ensures that no agent can independently enhance their utility by altering their decisions, thus achieving a safe and efficient group policy. The hypothesis is tested using a custom high-fidelity resuscitation simulator, revealing that Fair-GNE significantly enhances workload balance compared to fixed-penalty baselines (0.89 vs. 0.33 JFI, $p < 0.01$) while sustaining an impressive 86% task success rate. The results underscore the importance of their formulations and evaluation metrics, showcasing how innovative equilibrium-seeking methodologies can lead to principled fairness enforcement in large multi-agent learning-based healthcare systems. <div>
arXiv:2511.14135v1 Announce Type: new 
Abstract: Enforcing a fair workload allocation among multiple agents tasked to achieve an objective in learning enabled demand side healthcare worker settings is crucial for consistent and reliable performance at runtime. Existing multi-agent reinforcement learning (MARL) approaches steer fairness by shaping reward through post hoc orchestrations, leaving no certifiable self-enforceable fairness that is immutable by individual agents at runtime. Contextualized within a setting where each agent shares resources with others, we address this shortcoming with a learning enabled optimization scheme among self-interested decision makers whose individual actions affect those of other agents. This extends the problem to a generalized Nash equilibrium (GNE) game-theoretic framework where we steer group policy to a safe and locally efficient equilibrium, so that no agent can improve its utility function by unilaterally changing its decisions. Fair-GNE models MARL as a constrained generalized Nash equilibrium-seeking (GNE) game, prescribing an ideal equitable collective equilibrium within the problem's natural fabric. Our hypothesis is rigorously evaluated in our custom-designed high-fidelity resuscitation simulator. Across all our numerical experiments, Fair-GNE achieves significant improvement in workload balance over fixed-penalty baselines (0.89 vs.\ 0.33 JFI, $p < 0.01$) while maintaining 86\% task success, demonstrating statistically significant fairness gains through adaptive constraint enforcement. Our results communicate our formulations, evaluation metrics, and equilibrium-seeking innovations in large multi-agent learning-based healthcare systems with clarity and principled fairness enforcement.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Study of Implicit and Explicit Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2511.14153</link>
<guid>https://arxiv.org/abs/2511.14153</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, biases, bias-Identification Framework, gender bias, racial bias

<br /><br />Summary: This study examines the biases inherent in Large Language Models (LLMs) and underscores the importance of identifying and mitigating these biases to ensure fair outputs. The authors evaluated multiple generative models, including BERT and GPT 3.5, using bias-specific benchmarks like StereoSet and CrowSPairs. They introduced an automated Bias-Identification Framework targeting social biases related to gender, race, profession, and religion through a dual approach to detect both explicit and implicit biases. Findings revealed that while fine-tuned models struggled with gender bias, they performed well in recognizing and avoiding racial biases. However, the models showed a tendency to over-rely on keywords. To explore implicit biases, a Bag-of-Words analysis was employed, revealing signs of implicit stereotyping within the vocabulary. To enhance model performance, the researchers implemented a strategy involving fine-tuning through prompting techniques and data augmentation, which resulted in improved adaptability during cross-dataset testing. Significantly, the fine-tuned models achieved performance gains of up to 20% on implicit bias benchmarks, indicating the effectiveness of their mitigation strategies in addressing biases in LLMs. <div>
arXiv:2511.14153v1 Announce Type: new 
Abstract: Large Language Models (LLMs) inherit explicit and implicit biases from their training datasets. Identifying and mitigating biases in LLMs is crucial to ensure fair outputs, as they can perpetuate harmful stereotypes and misinformation. This study highlights the need to address biases in LLMs amid growing generative AI. We studied bias-specific benchmarks such as StereoSet and CrowSPairs to evaluate the existence of various biases in multiple generative models such as BERT and GPT 3.5. We proposed an automated Bias-Identification Framework to recognize various social biases in LLMs such as gender, race, profession, and religion. We adopted a two-pronged approach to detect explicit and implicit biases in text data. Results indicated fine-tuned models struggle with gender biases but excelled at identifying and avoiding racial biases. Our findings illustrated that despite having some success, LLMs often over-relied on keywords. To illuminate the capability of the analyzed LLMs in detecting implicit biases, we employed Bag-of-Words analysis and unveiled indications of implicit stereotyping within the vocabulary. To bolster the model performance, we applied an enhancement strategy involving fine-tuning models using prompting techniques and data augmentation of the bias benchmarks. The fine-tuned models exhibited promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Certified Signed Graph Unlearning</title>
<link>https://arxiv.org/abs/2511.14168</link>
<guid>https://arxiv.org/abs/2511.14168</guid>
<content:encoded><![CDATA[
<div> Keywords: Signed Graph Neural Networks, Graph Unlearning, Privacy Protection, Sociological Theories, Certified Unlearning  

<br /><br />Summary:  
This paper addresses the challenge of unlearning in Signed Graph Neural Networks (SGNNs), which model relationships with positive and negative edges. Existing graph unlearning methods are tailored for conventional graphs and fail to account for the unique heterogeneous and signed nature of SGNNs, resulting in loss of critical sign information and decreased model utility and unlearning effectiveness. To overcome these limitations, the authors propose Certified Signed Graph Unlearning (CSGU), a novel framework that guarantees privacy protection while respecting the sociological principles inherent in signed graphs. CSGU follows a three-stage process: first, it efficiently identifies minimal influenced neighborhoods through the analysis of triangular graph structures, which are key to capturing signed interactions. Second, it applies sociological theories to assess node importance, enabling optimal allocation of the privacy budget across the graph. Third, CSGU performs importance-weighted parameter updates to ensure certified modifications with minimal degradation in model utility. Experimental results demonstrate that CSGU outperforms prior methods by delivering superior utility retention and more effective unlearning on SGNNs, validating the approach as a state-of-the-art solution for privacy-preserving learning in signed graph settings. <div>
arXiv:2511.14168v1 Announce Type: new 
Abstract: Signed graphs model complex relationships through positive and negative edges, with widespread real-world applications. Given the sensitive nature of such data, selective removal mechanisms have become essential for privacy protection. While graph unlearning enables the removal of specific data influences from Graph Neural Networks (GNNs), existing methods are designed for conventional GNNs and overlook the unique heterogeneous properties of signed graphs. When applied to Signed Graph Neural Networks (SGNNs), these methods lose critical sign information, degrading both model utility and unlearning effectiveness. To address these challenges, we propose Certified Signed Graph Unlearning (CSGU), which provides provable privacy guarantees while preserving the sociological principles underlying SGNNs. CSGU employs a three-stage method: (1) efficiently identifying minimal influenced neighborhoods via triangular structures, (2) applying sociological theories to quantify node importance for optimal privacy budget allocation, and (3) performing importance-weighted parameter updates to achieve certified modifications with minimal utility degradation. Extensive experiments demonstrate that CSGU outperforms existing methods, achieving superior performance in both utility preservation and unlearning effectiveness on SGNNs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator</title>
<link>https://arxiv.org/abs/2511.14195</link>
<guid>https://arxiv.org/abs/2511.14195</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, safety robustness, N-GLARE, latent representations, JSS metric

<br /><br />Summary: The safety robustness evaluation of Large Language Models (LLMs) is essential for their effective deployment. Traditional Red Teaming methods, which involve online generation and black-box output analysis, have significant drawbacks, including high costs and feedback latency. To overcome these limitations, the authors propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator), which evaluates LLM safety directly through the model's latent representations without necessitating full text generation. N-GLARE focuses on analyzing the hidden layer dynamics via the Angular-Probabilistic Trajectory (APT) of latent representations and introduces a new metric, Jensen-Shannon Separability (JSS). Experimental results, encompassing over 40 models and 20 red teaming strategies, indicate that the JSS metric is highly consistent with safety evaluations derived from traditional Red Teaming methodologies. Notably, N-GLARE achieves this output-free evaluation with costs amounting to less than 1% of the token and runtime expenses associated with standard red teaming tests. This advancement allows for efficient and timely diagnostics following the training of new models, thereby facilitating agile safety assessments. <div>
arXiv:2511.14195v1 Announce Type: new 
Abstract: Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Gap Between Bayesian Deep Learning and Ensemble Weather Forecasts</title>
<link>https://arxiv.org/abs/2511.14218</link>
<guid>https://arxiv.org/abs/2511.14218</guid>
<content:encoded><![CDATA[
arXiv:2511.14218v1 Announce Type: new 
Abstract: Weather forecasting is fundamentally challenged by the chaotic nature of the atmosphere, necessitating probabilistic approaches to quantify uncertainty. While traditional ensemble prediction (EPS) addresses this through computationally intensive simulations, recent advances in Bayesian Deep Learning (BDL) offer a promising but often disconnected alternative. We bridge these paradigms through a unified hybrid Bayesian Deep Learning framework for ensemble weather forecasting that explicitly decomposes predictive uncertainty into epistemic and aleatoric components, learned via variational inference and a physics-informed stochastic perturbation scheme modeling flow-dependent atmospheric dynamics, respectively. We further establish a unified theoretical framework that rigorously connects BDL and EPS, providing formal theorems that decompose total predictive uncertainty into epistemic and aleatoric components under the hybrid BDL framework. We validate our framework on the large-scale 40-year ERA5 reanalysis dataset (1979-2019) with 0.25{\deg} spatial resolution. Experimental results show that our method not only improves forecast accuracy and yields better-calibrated uncertainty quantification but also achieves superior computational efficiency compared to state-of-the-art probabilistic diffusion models. We commit to making our code open-source upon acceptance of this paper.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallelizing Tree Search with Twice Sequential Monte Carlo</title>
<link>https://arxiv.org/abs/2511.14220</link>
<guid>https://arxiv.org/abs/2511.14220</guid>
<content:encoded><![CDATA[
arXiv:2511.14220v1 Announce Type: new 
Abstract: Model-based reinforcement learning (RL) methods that leverage search are responsible for many milestone breakthroughs in RL. Sequential Monte Carlo (SMC) recently emerged as an alternative to the Monte Carlo Tree Search (MCTS) algorithm which drove these breakthroughs. SMC is easier to parallelize and more suitable to GPU acceleration. However, it also suffers from large variance and path degeneracy which prevent it from scaling well with increased search depth, i.e., increased sequential compute. To address these problems, we introduce Twice Sequential Monte Carlo Tree Search (TSMCTS). Across discrete and continuous environments TSMCTS outperforms the SMC baseline as well as a popular modern version of MCTS. Through variance reduction and mitigation of path degeneracy, TSMCTS scales favorably with sequential compute while retaining the properties that make SMC natural to parallelize.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EBind: a practical approach to space binding</title>
<link>https://arxiv.org/abs/2511.14229</link>
<guid>https://arxiv.org/abs/2511.14229</guid>
<content:encoded><![CDATA[
arXiv:2511.14229v1 Announce Type: new 
Abstract: We simplify space binding by focusing on two core components, a single encoder per modality and high-quality data; enabling training state-of-the-art models on a single GPU in a few hours as opposed to multiple days. We present EBind, an Easy, data-centric, and parameter-efficient method to Bind the embedding spaces of multiple contrastive models. We demonstrate that a simple 1.8B-parameter image-text-video-audio-3D model can outperform models 4 to 17x the size. The key to achieving this is a carefully curated dataset of three complementary data sources: i) 6.7M fully-automated multimodal quintuples sourced via SOTA retrieval models, ii) 1M diverse, semi-automated triples annotated by humans as negative, partial, or positive matches, and iii) 3.4M pre-existing captioned data items. We use 13 different evaluations to demonstrate the value of each data source. Due to limitations with existing benchmarks, we further introduce the first high-quality, consensus-annotated zero-shot classification benchmark between audio and PCs. In contrast to related work, we will open-source our code, model weights, and datasets.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object-Centric World Models for Causality-Aware Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.14262</link>
<guid>https://arxiv.org/abs/2511.14262</guid>
<content:encoded><![CDATA[
arXiv:2511.14262v1 Announce Type: new 
Abstract: World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algebraformer: A Neural Approach to Linear Systems</title>
<link>https://arxiv.org/abs/2511.14263</link>
<guid>https://arxiv.org/abs/2511.14263</guid>
<content:encoded><![CDATA[
arXiv:2511.14263v1 Announce Type: new 
Abstract: Recent work in deep learning has opened new possibilities for solving classical algorithmic tasks using end-to-end learned models. In this work, we investigate the fundamental task of solving linear systems, particularly those that are ill-conditioned. Existing numerical methods for ill-conditioned systems often require careful parameter tuning, preconditioning, or domain-specific expertise to ensure accuracy and stability. In this work, we propose Algebraformer, a Transformer-based architecture that learns to solve linear systems end-to-end, even in the presence of severe ill-conditioning. Our model leverages a novel encoding scheme that enables efficient representation of matrix and vector inputs, with a memory complexity of $O(n^2)$, supporting scalable inference. We demonstrate its effectiveness on application-driven linear problems, including interpolation tasks from spectral methods for boundary value problems and acceleration of the Newton method. Algebraformer achieves competitive accuracy with significantly lower computational overhead at test time, demonstrating that general-purpose neural architectures can effectively reduce complexity in traditional scientific computing pipelines.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Multimodal Vessel Trajectory Prediction with Explainable Navigation Intention</title>
<link>https://arxiv.org/abs/2511.14265</link>
<guid>https://arxiv.org/abs/2511.14265</guid>
<content:encoded><![CDATA[
arXiv:2511.14265v1 Announce Type: new 
Abstract: Vessel trajectory prediction is fundamental to intelligent maritime systems. Within this domain, short-term prediction of rapid behavioral changes in complex maritime environments has established multimodal trajectory prediction (MTP) as a promising research area. However, existing vessel MTP methods suffer from limited scenario applicability and insufficient explainability. To address these challenges, we propose a unified MTP framework incorporating explainable navigation intentions, which we classify into sustained and transient categories. Our method constructs sustained intention trees from historical trajectories and models dynamic transient intentions using a Conditional Variational Autoencoder (CVAE), while using a non-local attention mechanism to maintain global scenario consistency. Experiments on real Automatic Identification System (AIS) datasets demonstrates our method's broad applicability across diverse scenarios, achieving significant improvements in both ADE and FDE. Furthermore, our method improves explainability by explicitly revealing the navigational intentions underlying each predicted trajectory.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing Task-Agnostic Embedding Models for Tabular Data</title>
<link>https://arxiv.org/abs/2511.14276</link>
<guid>https://arxiv.org/abs/2511.14276</guid>
<content:encoded><![CDATA[
arXiv:2511.14276v1 Announce Type: new 
Abstract: Recent foundation models for tabular data achieve strong task-specific performance via in-context learning. Nevertheless, they focus on direct prediction by encapsulating both representation learning and task-specific inference inside a single, resource-intensive network. This work specifically focuses on representation learning, i.e., on transferable, task-agnostic embeddings. We systematically evaluate task-agnostic representations from tabular foundation models (TabPFN and TabICL) alongside with classical feature engineering (TableVectorizer) across a variety of application tasks as outlier detection (ADBench) and supervised learning (TabArena Lite). We find that simple TableVectorizer features achieve comparable or superior performance while being up to three orders of magnitude faster than tabular foundation models. The code is available at https://github.com/ContactSoftwareAI/TabEmbedBench.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weight Variance Amplifier Improves Accuracy in High-Sparsity One-Shot Pruning</title>
<link>https://arxiv.org/abs/2511.14282</link>
<guid>https://arxiv.org/abs/2511.14282</guid>
<content:encoded><![CDATA[
arXiv:2511.14282v1 Announce Type: new 
Abstract: Deep neural networks achieve outstanding performance in visual recognition tasks, yet their large number of parameters makes them less practical for real-world applications. Recently, one-shot pruning has emerged as an effective strategy for reducing model size without additional training. However, models trained with standard objective functions often suffer a significant drop in accuracy after aggressive pruning. Some existing pruning-robust optimizers, such as SAM, and CrAM, mitigate this accuracy drop by guiding the model toward flatter regions of the parameter space, but they inevitably incur non-negligible additional computations. We propose a Variance Amplifying Regularizer (VAR) that deliberately increases the variance of model parameters during training. Our study reveals an intriguing finding that parameters with higher variance exhibit greater pruning robustness. VAR exploits this property by promoting such variance in the weight distribution, thereby mitigating the adverse effects of pruning. We further provide a theoretical analysis of its convergence behavior, supported by extensive empirical results demonstrating the superior pruning robustness of VAR.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H-LDM: Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Clinical Metadata</title>
<link>https://arxiv.org/abs/2511.14312</link>
<guid>https://arxiv.org/abs/2511.14312</guid>
<content:encoded><![CDATA[
arXiv:2511.14312v1 Announce Type: new 
Abstract: Phonocardiogram (PCG) analysis is vital for cardiovascular disease diagnosis, yet the scarcity of labeled pathological data hinders the capability of AI systems. To bridge this, we introduce H-LDM, a Hierarchical Latent Diffusion Model for generating clinically accurate and controllable PCG signals from structured metadata. Our approach features: (1) a multi-scale VAE that learns a physiologically-disentangled latent space, separating rhythm, heart sounds, and murmurs; (2) a hierarchical text-to-biosignal pipeline that leverages rich clinical metadata for fine-grained control over 17 distinct conditions; and (3) an interpretable diffusion process guided by a novel Medical Attention module. Experiments on the PhysioNet CirCor dataset demonstrate state-of-the-art performance, achieving a Fr\'echet Audio Distance of 9.7, a 92% attribute disentanglement score, and 87.1% clinical validity confirmed by cardiologists. Augmenting diagnostic models with our synthetic data improves the accuracy of rare disease classification by 11.3\%. H-LDM establishes a new direction for data augmentation in cardiac diagnostics, bridging data scarcity with interpretable clinical insights.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect</title>
<link>https://arxiv.org/abs/2511.14317</link>
<guid>https://arxiv.org/abs/2511.14317</guid>
<content:encoded><![CDATA[
arXiv:2511.14317v1 Announce Type: new 
Abstract: In clinical machine learning, the coexistence of multiple models with comparable performance -- a manifestation of the Rashomon Effect -- poses fundamental challenges for trustworthy deployment and evaluation. Small, imbalanced, and noisy datasets, coupled with high-dimensional and weakly identified clinical features, amplify this multiplicity and make conventional validation schemes unreliable. As a result, selecting among equally performing models becomes uncertain, particularly when resource constraints and operational priorities are not considered by conventional metrics like F1 score. To address these issues, we propose two complementary tools for robust model assessment and selection: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric that quantifies how efficiently a model identifies actionable true positives when only limited interventions are feasible, thereby linking predictive performance with clinical utility. PVF introduces a structured approach to assess the stability of models under data perturbations, identifying models whose performance remains most invariant across noisy or shifted validation sets. Empirical results on synthetic and real-world healthcare datasets show that using these tools facilitates the selection of models that generalize more robustly and align with capacity constraints, offering a new direction for tackling the Rashomon Effect in clinical settings.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning with Statistical Equality Constraints</title>
<link>https://arxiv.org/abs/2511.14320</link>
<guid>https://arxiv.org/abs/2511.14320</guid>
<content:encoded><![CDATA[
arXiv:2511.14320v1 Announce Type: new 
Abstract: As machine learning applications grow increasingly ubiquitous and complex, they face an increasing set of requirements beyond accuracy. The prevalent approach to handle this challenge is to aggregate a weighted combination of requirement violation penalties into the training objective. To be effective, this approach requires careful tuning of these hyperparameters (weights), involving trial-and-error and cross-validation, which becomes ineffective even for a moderate number of requirements. These issues are exacerbated when the requirements involve parities or equalities, as is the case in fairness and boundary value problems. An alternative technique uses constrained optimization to formulate these learning problems. Yet, existing approximation and generalization guarantees do not apply to problems involving equality constraints. In this work, we derive a generalization theory for equality-constrained statistical learning problems, showing that their solutions can be approximated using samples and rich parametrizations. Using these results, we propose a practical algorithm based on solving a sequence of unconstrained, empirical learning problems. We showcase its effectiveness and the new formulations enabled by equality constraints in fair learning, interpolating classifiers, and boundary value problems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enforcing hidden physics in physics-informed neural networks</title>
<link>https://arxiv.org/abs/2511.14348</link>
<guid>https://arxiv.org/abs/2511.14348</guid>
<content:encoded><![CDATA[
arXiv:2511.14348v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) represent a new paradigm for solving partial differential equations (PDEs) by integrating physical laws into the learning process of neural networks. However, despite their foundational role, the hidden irreversibility implied by the Second Law of Thermodynamics is often neglected during training, leading to unphysical solutions or even training failures in conventional PINNs. In this paper, we identify this critical gap and introduce a simple, generalized, yet robust irreversibility-regularized strategy that enforces hidden physical laws as soft constraints during training. This approach ensures that the learned solutions consistently respect the intrinsic one-way nature of irreversible physical processes. Across a wide range of benchmarks spanning traveling wave propagation, steady combustion, ice melting, corrosion evolution, and crack propagation, we demonstrate that our regularization scheme reduces predictive errors by more than an order of magnitude, while requiring only minimal modification to existing PINN frameworks. We believe that the proposed framework is broadly applicable to a wide class of PDE-governed physical systems and will have significant impact within the scientific machine learning community.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation</title>
<link>https://arxiv.org/abs/2511.14406</link>
<guid>https://arxiv.org/abs/2511.14406</guid>
<content:encoded><![CDATA[
arXiv:2511.14406v1 Announce Type: new 
Abstract: Large models adaptation through Federated Learning (FL) addresses a wide range of use cases and is enabled by Parameter-Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA). However, this distributed learning paradigm faces several security threats, particularly to its integrity, such as backdoor attacks that aim to inject malicious behavior during the local training steps of certain clients. We present the first analysis of the influence of LoRA on state-of-the-art backdoor attacks targeting model adaptation in FL. Specifically, we focus on backdoor lifespan, a critical characteristic in FL, that can vary depending on the attack scenario and the attacker's ability to effectively inject the backdoor. A key finding in our experiments is that for an optimally injected backdoor, the backdoor persistence after the attack is longer when the LoRA's rank is lower. Importantly, our work highlights evaluation issues of backdoor attacks against FL and contributes to the development of more robust and fair evaluations of backdoor attacks, enhancing the reliability of risk assessments for critical FL systems. Our code is publicly available.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Robust and Harmonious Adaptation for Cross-modal Retrieval</title>
<link>https://arxiv.org/abs/2511.14416</link>
<guid>https://arxiv.org/abs/2511.14416</guid>
<content:encoded><![CDATA[
arXiv:2511.14416v1 Announce Type: new 
Abstract: Recently, the general-to-customized paradigm has emerged as the dominant approach for Cross-Modal Retrieval (CMR), which reconciles the distribution shift problem between the source domain and the target domain. However, existing general-to-customized CMR methods typically assume that the entire target-domain data is available, which is easily violated in real-world scenarios and thus inevitably suffer from the query shift (QS) problem. Specifically, query shift embraces the following two characteristics and thus poses new challenges to CMR. i) Online Shift: real-world queries always arrive in an online manner, rendering it impractical to access the entire query set beforehand for customization approaches; ii) Diverse Shift: even with domain customization, the CMR models struggle to satisfy queries from diverse users or scenarios, leaving an urgent need to accommodate diverse queries. In this paper, we observe that QS would not only undermine the well-structured common space inherited from the source model, but also steer the model toward forgetting the indispensable general knowledge for CMR. Inspired by the observations, we propose a novel method for achieving online and harmonious adaptation against QS, dubbed Robust adaptation with quEry ShifT (REST). To deal with online shift, REST first refines the retrieval results to formulate the query predictions and accordingly designs a QS-robust objective function on these predictions to preserve the well-established common space in an online manner. As for tackling the more challenging diverse shift, REST employs a gradient decoupling module to dexterously manipulate the gradients during the adaptation process, thus preventing the CMR model from forgetting the general knowledge. Extensive experiments on 20 benchmarks across three CMR tasks verify the effectiveness of our method against QS.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowRoI A Fast Optical Flow Driven Region of Interest Extraction Framework for High-Throughput Image Compression in Immune Cell Migration Analysis</title>
<link>https://arxiv.org/abs/2511.14419</link>
<guid>https://arxiv.org/abs/2511.14419</guid>
<content:encoded><![CDATA[
arXiv:2511.14419v1 Announce Type: new 
Abstract: Autonomous migration is essential for the function of immune cells such as neutrophils and plays a pivotal role in diverse diseases. Recently, we introduced ComplexEye, a multi-lens array microscope comprising 16 independent aberration-corrected glass lenses arranged at the pitch of a 96-well plate, capable of capturing high-resolution movies of migrating cells. This architecture enables high-throughput live-cell video microscopy for migration analysis, supporting routine quantification of autonomous motility with strong potential for clinical translation. However, ComplexEye and similar high-throughput imaging platforms generate data at an exponential rate, imposing substantial burdens on storage and transmission. To address this challenge, we present FlowRoI, a fast optical-flow-based region of interest (RoI) extraction framework designed for high-throughput image compression in immune cell migration studies. FlowRoI estimates optical flow between consecutive frames and derives RoI masks that reliably cover nearly all migrating cells. The raw image and its corresponding RoI mask are then jointly encoded using JPEG2000 to enable RoI-aware compression. FlowRoI operates with high computational efficiency, achieving runtimes comparable to standard JPEG2000 and reaching an average throughput of about 30 frames per second on a modern laptop equipped with an Intel i7-1255U CPU. In terms of image quality, FlowRoI yields higher peak signal-to-noise ratio (PSNR) in cellular regions and achieves 2.0-2.2x higher compression rates at matched PSNR compared to standard JPEG2000.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiAD: Mirage Atom Diffusion for De Novo Crystal Generation</title>
<link>https://arxiv.org/abs/2511.14426</link>
<guid>https://arxiv.org/abs/2511.14426</guid>
<content:encoded><![CDATA[
arXiv:2511.14426v1 Announce Type: new 
Abstract: In recent years, diffusion-based models have demonstrated exceptional performance in searching for simultaneously stable, unique, and novel (S.U.N.) crystalline materials. However, most of these models don't have the ability to change the number of atoms in the crystal during the generation process, which limits the variability of model sampling trajectories. In this paper, we demonstrate the severity of this restriction and introduce a simple yet powerful technique, mirage infusion, which enables diffusion models to change the state of the atoms that make up the crystal from existent to non-existent (mirage) and vice versa. We show that this technique improves model quality by up to $\times2.5$ compared to the same model without this modification. The resulting model, Mirage Atom Diffusion (MiAD), is an equivariant joint diffusion model for de novo crystal generation that is capable of altering the number of atoms during the generation process. MiAD achieves an $8.2\%$ S.U.N. rate on the MP-20 dataset, which substantially exceeds existing state-of-the-art approaches. The source code can be found at \href{https://github.com/andrey-okhotin/miad.git}{\texttt{github.com/andrey-okhotin/miad}}.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Modeling of Photoplethysmography for Non-invasive Monitoring of Cardiovascular Parameters</title>
<link>https://arxiv.org/abs/2511.14452</link>
<guid>https://arxiv.org/abs/2511.14452</guid>
<content:encoded><![CDATA[
arXiv:2511.14452v1 Announce Type: new 
Abstract: Continuous cardiovascular monitoring can play a key role in precision health. However, some fundamental cardiac biomarkers of interest, including stroke volume and cardiac output, require invasive measurements, e.g., arterial pressure waveforms (APW). As a non-invasive alternative, photoplethysmography (PPG) measurements are routinely collected in hospital settings. Unfortunately, the prediction of key cardiac biomarkers from PPG instead of APW remains an open challenge, further complicated by the scarcity of annotated PPG measurements. As a solution, we propose a hybrid approach that uses hemodynamic simulations and unlabeled clinical data to estimate cardiovascular biomarkers directly from PPG signals. Our hybrid model combines a conditional variational autoencoder trained on paired PPG-APW data with a conditional density estimator of cardiac biomarkers trained on labeled simulated APW segments. As a key result, our experiments demonstrate that the proposed approach can detect fluctuations of cardiac output and stroke volume and outperform a supervised baseline in monitoring temporal changes in these biomarkers.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks</title>
<link>https://arxiv.org/abs/2511.14455</link>
<guid>https://arxiv.org/abs/2511.14455</guid>
<content:encoded><![CDATA[
arXiv:2511.14455v1 Announce Type: new 
Abstract: We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers</title>
<link>https://arxiv.org/abs/2511.14465</link>
<guid>https://arxiv.org/abs/2511.14465</guid>
<content:encoded><![CDATA[
arXiv:2511.14465v1 Announce Type: new 
Abstract: Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Notes on Kernel Methods in Machine Learning</title>
<link>https://arxiv.org/abs/2511.14485</link>
<guid>https://arxiv.org/abs/2511.14485</guid>
<content:encoded><![CDATA[
arXiv:2511.14485v1 Announce Type: new 
Abstract: These notes provide a self-contained introduction to kernel methods and their geometric foundations in machine learning. Starting from the construction of Hilbert spaces, we develop the theory of positive definite kernels, reproducing kernel Hilbert spaces (RKHS), and Hilbert-Schmidt operators, emphasizing their role in statistical estimation and representation of probability measures. Classical concepts such as covariance, regression, and information measures are revisited through the lens of Hilbert space geometry. We also introduce kernel density estimation, kernel embeddings of distributions, and the Maximum Mean Discrepancy (MMD). The exposition is designed to serve as a foundation for more advanced topics, including Gaussian processes, kernel Bayesian inference, and functional analytic approaches to modern machine learning.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Stable and Structured Time Series Generation with Perturbation-Aware Flow Matching</title>
<link>https://arxiv.org/abs/2511.14488</link>
<guid>https://arxiv.org/abs/2511.14488</guid>
<content:encoded><![CDATA[
arXiv:2511.14488v1 Announce Type: new 
Abstract: Time series generation is critical for a wide range of applications, which greatly supports downstream analytical and decision-making tasks. However, the inherent temporal heterogeneous induced by localized perturbations present significant challenges for generating structurally consistent time series. While flow matching provides a promising paradigm by modeling temporal dynamics through trajectory-level supervision, it fails to adequately capture abrupt transitions in perturbed time series, as the use of globally shared parameters constrains the velocity field to a unified representation. To address these limitations, we introduce \textbf{PAFM}, a \textbf{P}erturbation-\textbf{A}ware \textbf{F}low \textbf{M}atching framework that models perturbed trajectories to ensure stable and structurally consistent time series generation. The framework incorporates perturbation-guided training to simulate localized disturbances and leverages a dual-path velocity field to capture trajectory deviations under perturbation, enabling refined modeling of perturbed behavior to enhance the structural coherence. In order to further improve sensitivity to trajectory perturbations while enhancing expressiveness, a mixture-of-experts decoder with flow routing dynamically allocates modeling capacity in response to different trajectory dynamics. Extensive experiments on both unconditional and conditional generation tasks demonstrate that PAFM consistently outperforms strong baselines. Code is available at https://anonymous.4open.science/r/PAFM-03B2.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design</title>
<link>https://arxiv.org/abs/2511.14510</link>
<guid>https://arxiv.org/abs/2511.14510</guid>
<content:encoded><![CDATA[
arXiv:2511.14510v1 Announce Type: new 
Abstract: The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Full Atom Peptide Design via Riemannian Euclidean Bayesian Flow Networks</title>
<link>https://arxiv.org/abs/2511.14516</link>
<guid>https://arxiv.org/abs/2511.14516</guid>
<content:encoded><![CDATA[
arXiv:2511.14516v1 Announce Type: new 
Abstract: Diffusion and flow matching models have recently emerged as promising approaches for peptide binder design. Despite their progress, these models still face two major challenges. First, categorical sampling of discrete residue types collapses their continuous parameters into onehot assignments, while continuous variables (e.g., atom positions) evolve smoothly throughout the generation process. This mismatch disrupts the update dynamics and results in suboptimal performance. Second, current models assume unimodal distributions for side-chain torsion angles, which conflicts with the inherently multimodal nature of side chain rotameric states and limits prediction accuracy. To address these limitations, we introduce PepBFN, the first Bayesian flow network for full atom peptide design that directly models parameter distributions in fully continuous space. Specifically, PepBFN models discrete residue types by learning their continuous parameter distributions, enabling joint and smooth Bayesian updates with other continuous structural parameters. It further employs a novel Gaussian mixture based Bayesian flow to capture the multimodal side chain rotameric states and a Matrix Fisher based Riemannian flow to directly model residue orientations on the $\mathrm{SO}(3)$ manifold. Together, these parameter distributions are progressively refined via Bayesian updates, yielding smooth and coherent peptide generation. Experiments on side chain packing, reverse folding, and binder design tasks demonstrate the strong potential of PepBFN in computational peptide design.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MissHDD: Hybrid Deterministic Diffusion for Hetrogeneous Incomplete Data Imputation</title>
<link>https://arxiv.org/abs/2511.14543</link>
<guid>https://arxiv.org/abs/2511.14543</guid>
<content:encoded><![CDATA[
arXiv:2511.14543v1 Announce Type: new 
Abstract: Incomplete data are common in real-world tabular applications, where numerical, categorical, and discrete attributes coexist within a single dataset. This heterogeneous structure presents significant challenges for existing diffusion-based imputation models, which typically assume a homogeneous feature space and rely on stochastic denoising trajectories. Such assumptions make it difficult to maintain conditional consistency, and they often lead to information collapse for categorical variables or instability when numerical variables require deterministic updates. These limitations indicate that a single diffusion process is insufficient for mixed-type tabular imputation.
  We propose a hybrid deterministic diffusion framework that separates heterogeneous features into two complementary generative channels. A continuous DDIM-based channel provides efficient and stable deterministic denoising for numerical variables, while a discrete latent-path diffusion channel, inspired by loopholing-based discrete diffusion, models categorical and discrete features without leaving their valid sample manifolds. The two channels are trained under a unified conditional imputation objective, enabling coherent reconstruction of mixed-type incomplete data.
  Extensive experiments on multiple real-world datasets show that the proposed framework achieves higher imputation accuracy, more stable sampling trajectories, and improved robustness across MCAR, MAR, and MNAR settings compared with existing diffusion-based and classical methods. These results demonstrate the importance of structure-aware diffusion processes for advancing deep learning approaches to incomplete tabular data.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction</title>
<link>https://arxiv.org/abs/2511.14544</link>
<guid>https://arxiv.org/abs/2511.14544</guid>
<content:encoded><![CDATA[
arXiv:2511.14544v1 Announce Type: new 
Abstract: Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane. However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions. Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection. In this work, we introduce the Warping Index (WI), a new metric for measuring the quality of DR projections onto the 2D plane, based on the assumption that the correct preservation of empty regions between points is of crucial importance towards a faithful visual representation of the data.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task Addition and Weight Disentanglement in Closed-Vocabulary Models</title>
<link>https://arxiv.org/abs/2511.14569</link>
<guid>https://arxiv.org/abs/2511.14569</guid>
<content:encoded><![CDATA[
arXiv:2511.14569v1 Announce Type: new 
Abstract: Task arithmetic has recently emerged as a promising method for editing pre-trained \textit{open-vocabulary} models, offering a cost-effective alternative to standard multi-task fine-tuning. However, despite the abundance of \textit{closed-vocabulary} models that are not pre-trained with language supervision, applying task arithmetic to these models remains unexplored. In this paper, we deploy and study task addition in closed-vocabulary image classification models. We consider different pre-training schemes and find that \textit{weight disentanglement} -- the property enabling task arithmetic -- is a general consequence of pre-training, as it appears in different pre-trained closed-vocabulary models. In fact, we find that pre-trained closed-vocabulary vision transformers can also be edited with task arithmetic, achieving high task addition performance and enabling the efficient deployment of multi-task models. Finally, we demonstrate that simple linear probing is a competitive baseline to task addition. Overall, our findings expand the applicability of task arithmetic to a broader class of pre-trained models and open the way for more efficient use of pre-trained models in diverse settings.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents</title>
<link>https://arxiv.org/abs/2511.14584</link>
<guid>https://arxiv.org/abs/2511.14584</guid>
<content:encoded><![CDATA[
arXiv:2511.14584v1 Announce Type: new 
Abstract: Enabling agents to learn from experience and generalize across diverse tasks without task-specific training remains a fundamental challenge in reinforcement learning and decision-making. While recent approaches have explored episodic memory (Reflexion), gradient-based prompt optimization (TextGrad),and hierarchical task decomposition independently, their potential for synergistic integration remains unexplored. We introduce ReflexGrad, a novel architecture that tightly couples three complementary mechanisms: (1) LLM-based hierarchical TODO decomposition for strategic planning, (2) history-aware causal reflection that analyzes recent action patterns to identify failure root causes and enable within-trial learning, and (3) gradient-based optimization for systematic improvement. Unlike prior work relying on few-shot demonstrations, our system achieves true zero-shot generalization through pure LLM semantic reasoning,requiring no task-specific examples, fine-tuning, or hardcoded similarity metrics. Evaluated on ALFWorld benchmark tasks, ReflexGrad demonstrates 67% zero-shot success rate on Trial 0 without any prior task experience or demonstrations, establishing effective performance on first exposure. Through empirical analysis, we identify the architectural mechanisms underlying stable convergence (zero action loops) and effective cross-task transfer (67% to 78% improvement).Our work demonstrates that synergistic integration of complementary learning mechanisms enables robust zero-shot generalization that approaches few-shot baselines from prior work.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expert-Guided POMDP Learning for Data-Efficient Modeling in Healthcare</title>
<link>https://arxiv.org/abs/2511.14619</link>
<guid>https://arxiv.org/abs/2511.14619</guid>
<content:encoded><![CDATA[
arXiv:2511.14619v1 Announce Type: new 
Abstract: Learning the parameters of Partially Observable Markov Decision Processes (POMDPs) from limited data is a significant challenge. We introduce the Fuzzy MAP EM algorithm, a novel approach that incorporates expert knowledge into the parameter estimation process by enriching the Expectation Maximization (EM) framework with fuzzy pseudo-counts derived from an expert-defined fuzzy model. This integration naturally reformulates the problem as a Maximum A Posteriori (MAP) estimation, effectively guiding learning in environments with limited data. In synthetic medical simulations, our method consistently outperforms the standard EM algorithm under both low-data and high-noise conditions. Furthermore, a case study on Myasthenia Gravis illustrates the ability of the Fuzzy MAP EM algorithm to recover a clinically coherent POMDP, demonstrating its potential as a practical tool for data-efficient modeling in healthcare.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Failure to Mix: Large language models struggle to answer according to desired probability distributions</title>
<link>https://arxiv.org/abs/2511.14630</link>
<guid>https://arxiv.org/abs/2511.14630</guid>
<content:encoded><![CDATA[
arXiv:2511.14630v1 Announce Type: new 
Abstract: Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of "1" 49% of the time produces an answer of "0" nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapformer: Adaptive Channel Management for Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.14632</link>
<guid>https://arxiv.org/abs/2511.14632</guid>
<content:encoded><![CDATA[
arXiv:2511.14632v1 Announce Type: new 
Abstract: In multivariate time series forecasting (MTSF), accurately modeling the intricate dependencies among multiple variables remains a significant challenge due to the inherent limitations of traditional approaches. Most existing models adopt either \textbf{channel-independent} (CI) or \textbf{channel-dependent} (CD) strategies, each presenting distinct drawbacks. CI methods fail to leverage the potential insights from inter-channel interactions, resulting in models that may not fully exploit the underlying statistical dependencies present in the data. Conversely, CD approaches often incorporate too much extraneous information, risking model overfitting and predictive inefficiency. To address these issues, we introduce the Adaptive Forecasting Transformer (\textbf{Adapformer}), an advanced Transformer-based framework that merges the benefits of CI and CD methodologies through effective channel management. The core of Adapformer lies in its dual-stage encoder-decoder architecture, which includes the \textbf{A}daptive \textbf{C}hannel \textbf{E}nhancer (\textbf{ACE}) for enriching embedding processes and the \textbf{A}daptive \textbf{C}hannel \textbf{F}orecaster (\textbf{ACF}) for refining the predictions. ACE enhances token representations by selectively incorporating essential dependencies, while ACF streamlines the decoding process by focusing on the most relevant covariates, substantially reducing noise and redundancy. Our rigorous testing on diverse datasets shows that Adapformer achieves superior performance over existing models, enhancing both predictive accuracy and computational efficiency, thus making it state-of-the-art in MTSF.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Models for Predicting Smoking-Related Health Decline and Disease Risk</title>
<link>https://arxiv.org/abs/2511.14682</link>
<guid>https://arxiv.org/abs/2511.14682</guid>
<content:encoded><![CDATA[
arXiv:2511.14682v1 Announce Type: new 
Abstract: Smoking continues to be a major preventable cause of death worldwide, affecting millions through damage to the heart, metabolism, liver, and kidneys. However, current medical screening methods often miss the early warning signs of smoking-related health problems, leading to late-stage diagnoses when treatment options become limited. This study presents a systematic comparative evaluation of machine learning approaches for smoking-related health risk assessment, emphasizing clinical interpretability and practical deployment over algorithmic innovation. We analyzed health screening data from 55,691 individuals, examining various health indicators, including body measurements, blood tests, and demographic information. We tested three advanced prediction algorithms - Random Forest, XGBoost, and LightGBM - to determine which could most accurately identify people at high risk. This study employed a cross-sectional design to classify current smoking status based on health screening biomarkers, not to predict future disease development. Our Random Forest model performed best, achieving an Area Under the Curve (AUC) of 0.926, meaning it could reliably distinguish between high-risk and lower-risk individuals. Using SHAP (SHapley Additive exPlanations) analysis to understand what the model was detecting, we found that key health markers played crucial roles in prediction: blood pressure levels, triglyceride concentrations, liver enzyme readings, and kidney function indicators (serum creatinine) were the strongest signals of declining health in smokers.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>\textit{FLARE}: Adaptive Multi-Dimensional Reputation for Robust Client Reliability in Federated Learning</title>
<link>https://arxiv.org/abs/2511.14715</link>
<guid>https://arxiv.org/abs/2511.14715</guid>
<content:encoded><![CDATA[
arXiv:2511.14715v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy. However, it remains vulnerable to malicious clients who compromise model integrity through Byzantine attacks, data poisoning, or adaptive adversarial behaviors. Existing defense mechanisms rely on static thresholds and binary classification, failing to adapt to evolving client behaviors in real-world deployments. We propose FLARE, an adaptive reputation-based framework that transforms client reliability assessment from binary decisions to a continuous, multi-dimensional trust evaluation. FLARE integrates: (i) a multi-dimensional reputation score capturing performance consistency, statistical anomaly indicators, and temporal behavior, (ii) a self-calibrating adaptive threshold mechanism that adjusts security strictness based on model convergence and recent attack intensity, (iii) reputation-weighted aggregation with soft exclusion to proportionally limit suspicious contributions rather than eliminating clients outright, and (iv) a Local Differential Privacy (LDP) mechanism enabling reputation scoring on privatized client updates. We further introduce a highly evasive Statistical Mimicry (SM) attack, a benchmark adversary that blends honest gradients with synthetic perturbations and persistent drift to remain undetected by traditional filters. Extensive experiments with 100 clients on MNIST, CIFAR-10, and SVHN demonstrate that FLARE maintains high model accuracy and converges faster than state-of-the-art Byzantine-robust methods under diverse attack types, including label flipping, gradient scaling, adaptive attacks, ALIE, and SM. FLARE improves robustness by up to 16% and preserves model convergence within 30% of the non-attacked baseline, while achieving strong malicious-client detection performance with minimal computational overhead. https://github.com/Anonymous0-0paper/FLARE
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdamHD: Decoupled Huber Decay Regularization for Language Model Pre-Training</title>
<link>https://arxiv.org/abs/2511.14721</link>
<guid>https://arxiv.org/abs/2511.14721</guid>
<content:encoded><![CDATA[
arXiv:2511.14721v1 Announce Type: new 
Abstract: Adaptive optimizers with decoupled weight decay, such as AdamW, are the de facto standard for pre-training large transformer-based generative models. Yet the quadratic nature of the $\ell_2$ penalty embedded in weight decay drives all parameters toward the origin at the same rate, making the update vulnerable to rare but extreme gradient directions and often over-penalizing well-conditioned coordinates. We propose AdamHuberDecay, a drop-in replacement for AdamW that substitutes the $\ell_2$ penalty with a decoupled smooth Huber regularizer. The resulting update decays parameters quadratically while their magnitude remains below a threshold $\delta$, and linearly ($\ell_1$-like) once they exceed $\delta$, yielding (i) bounded regularization gradients, (ii) invariance to per-coordinate second-moment rescaling, and (iii) stronger sparsity pressure on overgrown weights.
  We derive the closed-form decoupled Huber decay step and show how to integrate it with any Adam-family optimizer at $O(1)$ extra cost. Extensive experiments on GPT-2 and GPT-3 pre-training demonstrate that AdamHuberDecay (a) converges 10-15% faster in wall-clock time, (b) reduces validation perplexity by up to 4 points, (c) delivers performance improvements of 2.5-4.7% across downstream tasks, and (d) yields visibly sparser weight histograms that translate into 20-30% memory savings after magnitude pruning, without tuning the decay coefficient beyond the default grid used for AdamW. Ablations confirm robustness to outlier gradients and large-batch regimes, together with theoretical analyses that bound the expected parameter norm under noisy updates. AdamHuberDecay therefore provides a simple, principled path toward more efficient and resilient training of next-generation foundational generative transformers.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAUD: Integrating Large Language Models with Active Learning for Unlabeled Data</title>
<link>https://arxiv.org/abs/2511.14738</link>
<guid>https://arxiv.org/abs/2511.14738</guid>
<content:encoded><![CDATA[
arXiv:2511.14738v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown a remarkable ability to generalize beyond their pre-training data, and fine-tuning LLMs can elevate performance to human-level and beyond. However, in real-world scenarios, lacking labeled data often prevents practitioners from obtaining well-performing models, thereby forcing practitioners to highly rely on prompt-based approaches that are often tedious, inefficient, and driven by trial and error. To alleviate this issue of lacking labeled data, we present a learning framework integrating LLMs with active learning for unlabeled dataset (LAUD). LAUD mitigates the cold-start problem by constructing an initial label set with zero-shot learning. Experimental results show that LLMs derived from LAUD outperform LLMs with zero-shot or few-shot learning on commodity name classification tasks, demonstrating the effectiveness of LAUD.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Means: A Dynamic Framework for Predicting Customer Satisfaction</title>
<link>https://arxiv.org/abs/2511.14743</link>
<guid>https://arxiv.org/abs/2511.14743</guid>
<content:encoded><![CDATA[
arXiv:2511.14743v1 Announce Type: new 
Abstract: Online ratings influence customer decision-making, yet standard aggregation methods, such as the sample mean, fail to adapt to quality changes over time and ignore review heterogeneity (e.g., review sentiment, a review's helpfulness). To address these challenges, we demonstrate the value of using the Gaussian process (GP) framework for rating aggregation. Specifically, we present a tailored GP model that captures the dynamics of ratings over time while additionally accounting for review heterogeneity. Based on 121,123 ratings from Yelp, we compare the predictive power of different rating aggregation methods in predicting future ratings, thereby finding that the GP model is considerably more accurate and reduces the mean absolute error by 10.2% compared to the sample mean. Our findings have important implications for marketing practitioners and customers. By moving beyond means, designers of online reputation systems can display more informative and adaptive aggregated rating scores that are accurate signals of expected customer satisfaction.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge</title>
<link>https://arxiv.org/abs/2511.14744</link>
<guid>https://arxiv.org/abs/2511.14744</guid>
<content:encoded><![CDATA[
arXiv:2511.14744v1 Announce Type: new 
Abstract: Deep learning's rise since the early 2010s has transformed fields like computer vision and natural language processing and strongly influenced biomedical research. For drug discovery specifically, a key inflection - akin to vision's "ImageNet moment" - arrived in 2015, when deep neural networks surpassed traditional approaches on the Tox21 Data Challenge. This milestone accelerated the adoption of deep learning across the pharmaceutical industry, and today most major companies have integrated these methods into their research pipelines. After the Tox21 Challenge concluded, its dataset was included in several established benchmarks, such as MoleculeNet and the Open Graph Benchmark. However, during these integrations, the dataset was altered and labels were imputed or manufactured, resulting in a loss of comparability across studies. Consequently, the extent to which bioactivity and toxicity prediction methods have improved over the past decade remains unclear. To this end, we introduce a reproducible leaderboard, hosted on Hugging Face with the original Tox21 Challenge dataset, together with a set of baseline and representative methods. The current version of the leaderboard indicates that the original Tox21 winner - the ensemble-based DeepTox method - and the descriptor-based self-normalizing neural networks introduced in 2017, continue to perform competitively and rank among the top methods for toxicity prediction, leaving it unclear whether substantial progress in toxicity prediction has been achieved over the past decade. As part of this work, we make all baselines and evaluated models publicly accessible for inference via standardized API calls to Hugging Face Spaces.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look-Ahead Reasoning on Learning Platforms</title>
<link>https://arxiv.org/abs/2511.14745</link>
<guid>https://arxiv.org/abs/2511.14745</guid>
<content:encoded><![CDATA[
arXiv:2511.14745v1 Announce Type: new 
Abstract: On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes, effectively contesting the platform's predictions. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-$k$ thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. We discuss connections to several related mathematical frameworks, including strategic classification, performative prediction, and algorithmic collective action.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseST: Exploiting Data Sparsity in Spatiotemporal Modeling and Prediction</title>
<link>https://arxiv.org/abs/2511.14753</link>
<guid>https://arxiv.org/abs/2511.14753</guid>
<content:encoded><![CDATA[
arXiv:2511.14753v1 Announce Type: new 
Abstract: Spatiotemporal data mining (STDM) has a wide range of applications in various complex physical systems (CPS), i.e., transportation, manufacturing, healthcare, etc. Among all the proposed methods, the Convolutional Long Short-Term Memory (ConvLSTM) has proved to be generalizable and extendable in different applications and has multiple variants achieving state-of-the-art performance in various STDM applications. However, ConvLSTM and its variants are computationally expensive, which makes them inapplicable in edge devices with limited computational resources. With the emerging need for edge computing in CPS, efficient AI is essential to reduce the computational cost while preserving the model performance. Common methods of efficient AI are developed to reduce redundancy in model capacity (i.e., model pruning, compression, etc.). However, spatiotemporal data mining naturally requires extensive model capacity, as the embedded dependencies in spatiotemporal data are complex and hard to capture, which limits the model redundancy. Instead, there is a fairly high level of data and feature redundancy that introduces an unnecessary computational burden, which has been largely overlooked in existing research. Therefore, we developed a novel framework SparseST, that pioneered in exploiting data sparsity to develop an efficient spatiotemporal model. In addition, we explore and approximate the Pareto front between model performance and computational efficiency by designing a multi-objective composite loss function, which provides a practical guide for practitioners to adjust the model according to computational resource constraints and the performance requirements of downstream tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\pi^{*}_{0.6}$: a VLA That Learns From Experience</title>
<link>https://arxiv.org/abs/2511.14759</link>
<guid>https://arxiv.org/abs/2511.14759</guid>
<content:encoded><![CDATA[
arXiv:2511.14759v1 Announce Type: new 
Abstract: We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $\pi^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $\pi^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI Workflow</title>
<link>https://arxiv.org/abs/2509.12443</link>
<guid>https://arxiv.org/abs/2509.12443</guid>
<content:encoded><![CDATA[
arXiv:2509.12443v3 Announce Type: cross 
Abstract: Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to modernize legacy codes for portability. Frameworks like Kokkos provide performance portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos porting demands significant expertise and time. Large language models (LLMs) have shown promise in source-to-source code generation, yet their use in fully autonomous workflows for translating and optimizing parallel code remains largely unexplored, especially for performance portability across diverse hardware. This paper presents an agentic AI workflow where specialized LLM "agents" collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into portable Kokkos C++ programs. Results show the pipeline modernizes a range of benchmark kernels, producing performance-portable Kokkos codes across hardware partitions. Paid OpenAI models such as GPT-5 and o4-mini-high executed the workflow for only a few U.S. dollars, generating optimized codes that surpassed Fortran baselines, whereas open-source models like Llama4-Maverick often failed to yield functional codes. This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos transformation and offers a pathway for autonomously modernizing legacy scientific applications to run portably and efficiently on diverse supercomputers. It further highlights the potential of LLM-driven agentic systems to perform structured, domain-specific reasoning tasks in scientific and systems-oriented applications.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature weighting for data analysis via evolutionary simulation</title>
<link>https://arxiv.org/abs/2511.06454</link>
<guid>https://arxiv.org/abs/2511.06454</guid>
<content:encoded><![CDATA[
arXiv:2511.06454v1 Announce Type: cross 
Abstract: We analyze an algorithm for assigning weights prior to scalarization in discrete multi-objective problems arising from data analysis. The algorithm evolves the weights (the relevance of features) by a replicator-type dynamic on the standard simplex, with update indices computed from a normalized data matrix. We prove that the resulting sequence converges globally to a unique interior equilibrium, yielding non-degenerate limiting weights. The method, originally inspired by evolutionary game theory, differs from standard weighting schemes in that it is analytically tractable with provable convergence.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preparation Meets Opportunity: Enhancing Data Preprocessing for ML Training With Seneca</title>
<link>https://arxiv.org/abs/2511.13724</link>
<guid>https://arxiv.org/abs/2511.13724</guid>
<content:encoded><![CDATA[
arXiv:2511.13724v1 Announce Type: cross 
Abstract: Input data preprocessing is a common bottleneck when concurrently training multimedia machine learning (ML) models in modern systems. To alleviate these bottlenecks and reduce the training time for concurrent jobs, we present Seneca, a data loading system that optimizes cache partitioning and data sampling for the data storage and ingestion (DSI) pipeline. The design of Seneca contains two key techniques. First, Seneca uses a performance model for the data pipeline to optimally partition the cache for three different forms of data (encoded, decoded, and augmented). Second, Seneca opportunistically serves cached data over uncached ones during random batch sampling so that concurrent jobs benefit from each other. We implement Seneca by modifying PyTorch and demonstrate its effectiveness by comparing it against several state-of-the-art caching systems for DNN training. Seneca reduces the makespan by 45.23% compared to PyTorch and increases data processing throughput by up to 3.45x compared to the next best dataloader.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DualLaguerreNet: A Decoupled Spectral Filter GNN and the Uncovering of the Flexibility-Stability Trade-off</title>
<link>https://arxiv.org/abs/2511.13729</link>
<guid>https://arxiv.org/abs/2511.13729</guid>
<content:encoded><![CDATA[
arXiv:2511.13729v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) based on spectral filters, such as the Adaptive Orthogonal Polynomial Filter (AOPF) class (e.g., LaguerreNet), have shown promise in unifying the solutions for heterophily and over-smoothing. However, these single-filter models suffer from a "compromise" problem, as their single adaptive parameter (e.g., alpha) must learn a suboptimal, averaged response across the entire graph spectrum. In this paper, we propose DualLaguerreNet, a novel GNN architecture that solves this by introducing "Decoupled Spectral Flexibility." DualLaguerreNet splits the graph Laplacian into two operators, L_low (low-frequency) and L_high (high-frequency), and learns two independent, adaptive Laguerre polynomial filters, parameterized by alpha_1 and alpha_2, respectively. This work, however, uncovers a deeper finding. While our experiments show DualLaguerreNet's flexibility allows it to achieve state-of-the-art results on complex heterophilic tasks (outperforming LaguerreNet), it simultaneously underperforms on simpler, homophilic tasks. We identify this as a fundamental "Flexibility-Stability Trade-off". The increased parameterization (2x filter parameters and 2x model parameters) leads to overfitting on simple tasks, demonstrating that the "compromise" of simpler models acts as a crucial regularizer. This paper presents a new SOTA architecture for heterophily while providing a critical analysis of the bias-variance trade-off inherent in adaptive GNN filter design.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GegenbauerNet: Finding the Optimal Compromise in the GNN Flexibility-Stability Trade-off</title>
<link>https://arxiv.org/abs/2511.13730</link>
<guid>https://arxiv.org/abs/2511.13730</guid>
<content:encoded><![CDATA[
arXiv:2511.13730v1 Announce Type: cross 
Abstract: Spectral Graph Neural Networks (GNNs) operating in the canonical [-1, 1] domain (like ChebyNet and its adaptive generalization, L-JacobiNet) face a fundamental Flexibility-Stability Trade-off. Our previous work revealed a critical puzzle: the 2-parameter adaptive L-JacobiNet often suffered from high variance and was surprisingly outperformed by the 0-parameter, stabilized-static S-JacobiNet. This suggested that stabilization was more critical than adaptation in this domain. In this paper, we propose \textbf{GegenbauerNet}, a novel GNN filter based on the Gegenbauer polynomials, to find the Optimal Compromise in this trade-off. By enforcing symmetry (alpha=beta) but allowing a single shape parameter (lambda) to be learned, GegenbauerNet limits flexibility (variance) while escaping the fixed bias of S-JacobiNet. We demonstrate that GegenbauerNet (1-parameter) achieves superior performance in the key local filtering regime (K=2 on heterophilic graphs) where overfitting is minimal, validating the hypothesis that a controlled, symmetric degree of freedom is optimal. Furthermore, our comprehensive K-ablation study across homophilic and heterophilic graphs, using 7 diverse datasets, clarifies the domain's behavior: the fully adaptive L-JacobiNet maintains the highest performance on high-K filtering tasks, showing the value of maximum flexibility when regularization is managed. This study provides crucial design principles for GNN developers, showing that in the [-1, 1] spectral domain, the optimal filter depends critically on the target locality (K) and the acceptable level of design bias.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Principled Coarse-Grained Acceptance for Speculative Decoding in Speech</title>
<link>https://arxiv.org/abs/2511.13732</link>
<guid>https://arxiv.org/abs/2511.13732</guid>
<content:encoded><![CDATA[
arXiv:2511.13732v1 Announce Type: cross 
Abstract: Speculative decoding accelerates autoregressive speech generation by letting a fast draft model propose tokens that a larger target model verifies. However, for speech LLMs that generate acoustic tokens, exact token matching is overly restrictive: many discrete tokens are acoustically or semantically interchangeable, reducing acceptance rates and limiting speedups. We introduce Principled Coarse-Graining (PCG), which verifies proposals at the level of Acoustic Similarity Groups (ASGs) derived from the target model's embedding space. By splitting each token's probability mass across the overlapping groups that contain it, we define an overlap-aware coarse-grained distribution and perform rejection sampling on the resulting group variable. This yields an exactness guarantee at the group level while allowing the accepted draft token to stand in for any member of the group in practice. On LibriTTS, PCG increases acceptance and throughput relative to standard speculative decoding and prior speech-specific relaxations while maintaining intelligibility and speaker similarity. These results suggest acoustically aware, group-level acceptance as a simple and general way to accelerate speech token generation while maintaining speech quality.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>THD-BAR: Topology Hierarchical Derived Brain Autoregressive Modeling for EEG Generic Representations</title>
<link>https://arxiv.org/abs/2511.13733</link>
<guid>https://arxiv.org/abs/2511.13733</guid>
<content:encoded><![CDATA[
arXiv:2511.13733v1 Announce Type: cross 
Abstract: Large-scale pre-trained models hold significant potential for learning universal EEG representations. However, most existing methods, particularly autoregressive (AR) frameworks, primarily rely on straightforward temporal sequencing of multi-channel EEG data, which fails to capture the rich physiological characteristics inherent to EEG signals. Moreover, their time-centered modeling approach also limits the effective representation of the dynamic spatial topology of brain activity. To address these challenges and fully exploit the potential of large-scale EEG models, we propose a novel Topology Hierarchical Derived Brain Autoregressive Modeling (THD-BAR) for EEG generic representations. The core innovation of THD-BAR lies in the introduction of the Brain Topology Hierarchy (BTH), which establishes a multi-scale spatial order for EEG channels. This hierarchical structure enables a redefinition of autoregressive learning as a "next-scale-time prediction" problem, effectively capturing both spatial and temporal dynamics. Based on BTH, we design a Topology-Hierarchical Vector Quantized-Variational Autoencoder (THVQ-VAE) for multi-scale tokenization and develop an enhanced Brain Autoregressive (BAR) module with specialized masking strategies for prediction. Through extensive large-scale pre-training on 17 datasets, followed by rigorous validation on 10 downstream datasets spanning 5 distinct tasks, THD-BAR consistently outperforms existing methods. These results highlight the superior generalization and modeling capabilities of our proposed approach.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Deep Learning Density Shaping Model Predictive Gust Load Alleviation Control of a Compliant Wing Subjected to Atmospheric Turbulence</title>
<link>https://arxiv.org/abs/2511.13745</link>
<guid>https://arxiv.org/abs/2511.13745</guid>
<content:encoded><![CDATA[
arXiv:2511.13745v1 Announce Type: cross 
Abstract: This study presents a novel deep learning approach aimed at enhancing stochastic Gust Load Alleviation (GLA) specifically for compliant wings. The approach incorporates the concept of smooth wing camber variation, where the camber of the wing's chord is actively adjusted during flight using a control signal to achieve the desired aerodynamic loading. The proposed method employs a deep learning-based model predictive controller designed for probability density shaping. This controller effectively solves the probability density evolution equation through a custom Physics-Informed Neural Network (PINN) and utilizes Automatic Differentiation for Model Predictive Control (MPC) optimization. Comprehensive numerical simulations were conducted on a compliant wing (CW) model, evaluating performance of the proposed approach against stochastic gust profiles. The evaluation involved stochastic aerodynamic loads generated from Band-Limited White Noise (BLWN) and Dryden gust models. The evaluation were conducted for two distinct Compliant Chord Fractions (CCF). The results demonstrate the effectiveness of the proposed probability density shaping model predictive control in alleviating stochastic gust load and reducing wing tip deflection.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep reinforcement learning-based spacecraft attitude control with pointing keep-out constraint</title>
<link>https://arxiv.org/abs/2511.13746</link>
<guid>https://arxiv.org/abs/2511.13746</guid>
<content:encoded><![CDATA[
arXiv:2511.13746v1 Announce Type: cross 
Abstract: This paper implements deep reinforcement learning (DRL) for spacecraft reorientation control with a single pointing keep-out zone. The Soft Actor-Critic (SAC) algorithm is adopted to handle continuous state and action space. A new state representation is designed to explicitly include a compact representation of the attitude constraint zone. The reward function is formulated to achieve the control objective while enforcing the attitude constraint. A curriculum learning approach is used for the agent training. Simulation results demonstrate the effectiveness of the proposed DRL-based method for spacecraft pointing-constrained attitude control.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What happens when nanochat meets DiLoCo?</title>
<link>https://arxiv.org/abs/2511.13761</link>
<guid>https://arxiv.org/abs/2511.13761</guid>
<content:encoded><![CDATA[
arXiv:2511.13761v1 Announce Type: cross 
Abstract: Although LLM training is typically centralized with high-bandwidth interconnects and large compute budgets, emerging methods target communication-constrained training in distributed environments. The model trade-offs introduced by this shift remain underexplored, and our goal is to study them.
  We use the open-source nanochat project, a compact 8K-line full-stack ChatGPT-like implementation containing tokenization, pretraining, fine-tuning, and serving, as a controlled baseline. We implement the DiLoCo algorithm as a lightweight wrapper over nanochat's training loop, performing multiple local steps per worker before synchronization with an outer optimizer, effectively reducing communication by orders of magnitude. This inner-outer training is compared against a standard data-parallel (DDP) setup. Because nanochat is small and inspectable, it enables controlled pipeline adaptations and allows direct comparison with the conventional centralized baseline.
  DiLoCo achieves stable convergence and competitive loss in pretraining but yields worse MMLU, GSM8K, and HumanEval scores after mid-training and SFT. We discover that using DiLoCo-pretrained weights and running mid- and post-training with DDP fails to recover performance, revealing irreversible representation drift from asynchronous updates that impairs downstream alignment. We provide this implementation as an official fork of nanochat on GitHub.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge vs. Experience: Asymptotic Limits of Impatience in Edge Tenants</title>
<link>https://arxiv.org/abs/2511.13763</link>
<guid>https://arxiv.org/abs/2511.13763</guid>
<content:encoded><![CDATA[
arXiv:2511.13763v1 Announce Type: cross 
Abstract: We study how two information feeds, a closed-form Markov estimator of residual sojourn and an online trained actor-critic, affect reneging and jockeying in a dual M/M/1 system. Analytically, for unequal service rates and total-time patience, we show that total wait grows linearly so abandonment is inevitable and the probability of a successful jockey vanishes as the backlog approaches towards infinity. Furthermore, under a mild sub-linear error condition both information models yield the same asymptotic limits (robustness). We empirically validate these limits and quantify finite backlog differences. Our findings show that learned and analytic feeds produce different delays, reneging rates and transient jockeying behavior at practical sizes, but converge to the same asymptotic outcome implied by our theory. The results characterize when value-of-information matters (finite regimes) and when it does not (asymptotics), informing lightweight telemetry and decision-logic design for low-cost, jockeying-aware systems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Known Meets Unknown: Mitigating Overconfidence in Open Set Recognition</title>
<link>https://arxiv.org/abs/2511.13775</link>
<guid>https://arxiv.org/abs/2511.13775</guid>
<content:encoded><![CDATA[
arXiv:2511.13775v1 Announce Type: cross 
Abstract: Open Set Recognition (OSR) requires models not only to accurately classify known classes but also to effectively reject unknown samples. However, when unknown samples are semantically similar to known classes, inter-class overlap in the feature space often causes models to assign unjustifiably high confidence to them, leading to misclassification as known classes -- a phenomenon known as overconfidence. This overconfidence undermines OSR by blurring the decision boundary between known and unknown classes. To address this issue, we propose a framework that explicitly mitigates overconfidence caused by inter-class overlap. The framework consists of two components: a perturbation-based uncertainty estimation module, which applies controllable parameter perturbations to generate diverse predictions and quantify predictive uncertainty, and an unknown detection module with distinct learning-based classifiers, implemented as a two-stage procedure, which leverages the estimated uncertainty to improve discrimination between known and unknown classes, thereby enhancing OSR performance. Experimental results on three public datasets show that the proposed framework achieves superior performance over existing OSR methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Multiplexing</title>
<link>https://arxiv.org/abs/2511.13779</link>
<guid>https://arxiv.org/abs/2511.13779</guid>
<content:encoded><![CDATA[
arXiv:2511.13779v1 Announce Type: cross 
Abstract: Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\times$, 25$\times$, and 54$\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CellStream: Dynamical Optimal Transport Informed Embeddings for Reconstructing Cellular Trajectories from Snapshots Data</title>
<link>https://arxiv.org/abs/2511.13786</link>
<guid>https://arxiv.org/abs/2511.13786</guid>
<content:encoded><![CDATA[
arXiv:2511.13786v1 Announce Type: cross 
Abstract: Single-cell RNA sequencing (scRNA-seq), especially temporally resolved datasets, enables genome-wide profiling of gene expression dynamics at single-cell resolution across discrete time points. However, current technologies provide only sparse, static snapshots of cell states and are inherently influenced by technical noise, complicating the inference and representation of continuous transcriptional dynamics. Although embedding methods can reduce dimensionality and mitigate technical noise, the majority of existing approaches typically treat trajectory inference separately from embedding construction, often neglecting temporal structure. To address this challenge, here we introduce CellStream, a novel deep learning framework that jointly learns embedding and cellular dynamics from single-cell snapshot data by integrating an autoencoder with unbalanced dynamical optimal transport. Compared to existing methods, CellStream generates dynamics-informed embeddings that robustly capture temporal developmental processes while maintaining high consistency with the underlying data manifold. We demonstrate CellStream's effectiveness on both simulated datasets and real scRNA-seq data, including spatial transcriptomics. Our experiments indicate significant quantitative improvements over state-of-the-art methods in representing cellular trajectories with enhanced temporal coherence and reduced noise sensitivity. Overall, CellStream provides a new tool for learning and representing continuous streams from the noisy, static snapshots of single-cell gene expression.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XAI-Driven Deep Learning for Protein Sequence Functional Group Classification</title>
<link>https://arxiv.org/abs/2511.13791</link>
<guid>https://arxiv.org/abs/2511.13791</guid>
<content:encoded><![CDATA[
arXiv:2511.13791v1 Announce Type: cross 
Abstract: Proteins perform essential biological functions, and accurate classification of their sequences is critical for understanding structure-function relationships, enzyme mechanisms, and molecular interactions. This study presents a deep learning-based framework for functional group classification of protein sequences derived from the Protein Data Bank (PDB). Four architectures were implemented: Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), CNN-BiLSTM hybrid, and CNN with Attention. Each model was trained using k-mer integer encoding to capture both local and long-range dependencies. Among these, the CNN achieved the highest validation accuracy of 91.8%, demonstrating the effectiveness of localized motif detection. Explainable AI techniques, including Grad-CAM and Integrated Gradients, were applied to interpret model predictions and identify biologically meaningful sequence motifs. The discovered motifs, enriched in histidine, aspartate, glutamate, and lysine, represent amino acid residues commonly found in catalytic and metal-binding regions of transferase enzymes. These findings highlight that deep learning models can uncover functionally relevant biochemical signatures, bridging the gap between predictive accuracy and biological interpretability in protein sequence analysis.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAT-MPNN: A Mobility-Aware Transformer-MPNN Model for Dynamic Spatiotemporal Prediction of HIV Diagnoses in California, Florida, and New England</title>
<link>https://arxiv.org/abs/2511.13797</link>
<guid>https://arxiv.org/abs/2511.13797</guid>
<content:encoded><![CDATA[
arXiv:2511.13797v1 Announce Type: cross 
Abstract: Human Immunodeficiency Virus (HIV) has posed a major global health challenge for decades, and forecasting HIV diagnoses continues to be a critical area of research. However, capturing the complex spatial and temporal dependencies of HIV transmission remains challenging. Conventional Message Passing Neural Network (MPNN) models rely on a fixed binary adjacency matrix that only encodes geographic adjacency, which is unable to represent interactions between non-contiguous counties. Our study proposes a deep learning architecture Mobility-Aware Transformer-Message Passing Neural Network (MAT-MPNN) framework to predict county-level HIV diagnosis rates across California, Florida, and the New England region. The model combines temporal features extracted by a Transformer encoder with spatial relationships captured through a Mobility Graph Generator (MGG). The MGG improves conventional adjacency matrices by combining geographic and demographic information. Compared with the best-performing hybrid baseline, the Transformer MPNN model, MAT-MPNN reduced the Mean Squared Prediction Error (MSPE) by 27.9% in Florida, 39.1% in California, and 12.5% in New England, and improved the Predictive Model Choice Criterion (PMCC) by 7.7%, 3.5%, and 3.9%, respectively. MAT-MPNN also achieved better results than the Spatially Varying Auto-Regressive (SVAR) model in Florida and New England, with comparable performance in California. These results demonstrate that applying mobility-aware dynamic spatial structures substantially enhances predictive accuracy and calibration in spatiotemporal epidemiological prediction.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention for 3D Modeling of Complex Structures</title>
<link>https://arxiv.org/abs/2511.13798</link>
<guid>https://arxiv.org/abs/2511.13798</guid>
<content:encoded><![CDATA[
arXiv:2511.13798v1 Announce Type: cross 
Abstract: Microbial Fuel Cells (MFCs) offer a promising pathway for sustainable energy generation by converting organic matter into electricity through microbial processes. A key factor influencing MFC performance is the anode structure, where design and material properties play a crucial role. Existing predictive models struggle to capture the complex geometric dependencies necessary to optimize these structures. To solve this problem, we propose KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention. KANGURA introduces a new approach to three-dimensional (3D) machine learning modeling. It formulates prediction as a function decomposition problem, where Kolmogorov-Arnold Network (KAN)- based representation learning reconstructs geometric relationships without a conventional multi- layer perceptron (MLP). To refine spatial understanding, geometry-disentangled representation learning separates structural variations into interpretable components, while unified attention mechanisms dynamically enhance critical geometric regions. Experimental results demonstrate that KANGURA outperforms over 15 state-of-the-art (SOTA) models on the ModelNet40 benchmark dataset, achieving 92.7% accuracy, and excels in a real-world MFC anode structure problem with 97% accuracy. This establishes KANGURA as a robust framework for 3D geometric modeling, unlocking new possibilities for optimizing complex structures in advanced manufacturing and quality-driven engineering applications.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zipf-Gramming: Scaling Byte N-Grams Up to Production Sized Malware Corpora</title>
<link>https://arxiv.org/abs/2511.13808</link>
<guid>https://arxiv.org/abs/2511.13808</guid>
<content:encoded><![CDATA[
arXiv:2511.13808v1 Announce Type: cross 
Abstract: A classifier using byte n-grams as features is the only approach we have found fast enough to meet requirements in size (sub 2 MB), speed (multiple GB/s), and latency (sub 10 ms) for deployment in numerous malware detection scenarios. However, we've consistently found that 6-8 grams achieve the best accuracy on our production deployments but have been unable to deploy regularly updated models due to the high cost of finding the top-k most frequent n-grams over terabytes of executable programs. Because the Zipfian distribution well models the distribution of n-grams, we exploit its properties to develop a new top-k n-gram extractor that is up to $35\times$ faster than the previous best alternative. Using our new Zipf-Gramming algorithm, we are able to scale up our production training set and obtain up to 30\% improvement in AUC at detecting new malware. We show theoretically and empirically that our approach will select the top-k items with little error and the interplay between theory and engineering required to achieve these results.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QUASAR: An Evolutionary Algorithm to Accelerate High-Dimensional Optimization</title>
<link>https://arxiv.org/abs/2511.13843</link>
<guid>https://arxiv.org/abs/2511.13843</guid>
<content:encoded><![CDATA[
arXiv:2511.13843v1 Announce Type: cross 
Abstract: High-dimensional numerical optimization presents a persistent challenge. This paper introduces Quasi-Adaptive Search with Asymptotic Reinitialization (QUASAR), an evolutionary algorithm to accelerate convergence in complex, non-differentiable problems afflicted by the curse of dimensionality.
  Evaluated on the notoriously difficult CEC2017 benchmark suite of 29 functions, QUASAR achieved the lowest overall rank sum (150) using the Friedman test, significantly outperforming L-SHADE (229) and standard DE (305) in the dimension-variant trials. QUASAR also proves computationally efficient, with run times averaging $1.4 \text{x}$ faster than DE and $7.8 \text{x}$ faster than L-SHADE ($p \ll 0.001$) in the population-variant trials.
  Building upon Differential Evolution (DE), QUASAR introduces a highly stochastic architecture to dynamically balance exploration and exploitation. Inspired by the probabilistic behavior of quantum particles in a stellar core, the algorithm implements three primary components that augment standard DE mechanisms: 1) probabilistically selected mutation strategies and scaling factors; 2) rank-based crossover rates; 3) asymptotically decaying reinitialization that leverages a covariance matrix of the best solutions to introduce high-quality genetic diversity.
  QUASAR's performance establishes it as an effective, user-friendly optimizer for complex high-dimensional problems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-Hema: Unified Model for Digital Hematopathology</title>
<link>https://arxiv.org/abs/2511.13889</link>
<guid>https://arxiv.org/abs/2511.13889</guid>
<content:encoded><![CDATA[
arXiv:2511.13889v1 Announce Type: cross 
Abstract: Digital hematopathology requires cell-level analysis across diverse disease categories, including malignant disorders (e.g., leukemia), infectious conditions (e.g., malaria), and non-malignant red blood cell disorders (e.g., sickle cell disease). Whether single-task, vision-language, WSI-optimized, or single-cell hematology models, these approaches share a key limitation, they cannot provide unified, multi-task, multi-modal reasoning across the complexities of digital hematopathology. To overcome these limitations, we propose Uni-Hema, a multi-task, unified model for digital hematopathology integrating detection, classification, segmentation, morphology prediction, and reasoning across multiple diseases. Uni-Hema leverages 46 publicly available datasets, encompassing over 700K images and 21K question-answer pairs, and is built upon Hema-Former, a multimodal module that bridges visual and textual representations at the hierarchy level for the different tasks (detection, classification, segmentation, morphology, mask language modeling and visual question answer) at different granularity. Extensive experiments demonstrate that Uni-Hema achieves comparable or superior performance to train on a single-task and single dataset models, across diverse hematological tasks, while providing interpretable, morphologically relevant insights at the single-cell level. Our framework establishes a new standard for multi-task and multi-modal digital hematopathology. The code will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Disentangled Low-Rank RNN Framework for Uncovering Neural Connectivity and Dynamics</title>
<link>https://arxiv.org/abs/2511.13899</link>
<guid>https://arxiv.org/abs/2511.13899</guid>
<content:encoded><![CDATA[
arXiv:2511.13899v1 Announce Type: cross 
Abstract: Low-rank recurrent neural networks (lrRNNs) are a class of models that uncover low-dimensional latent dynamics underlying neural population activity. Although their functional connectivity is low-rank, it lacks disentanglement interpretations, making it difficult to assign distinct computational roles to different latent dimensions. To address this, we propose the Disentangled Recurrent Neural Network (DisRNN), a generative lrRNN framework that assumes group-wise independence among latent dynamics while allowing flexible within-group entanglement. These independent latent groups allow latent dynamics to evolve separately, but are internally rich for complex computation. We reformulate the lrRNN under a variational autoencoder (VAE) framework, enabling us to introduce a partial correlation penalty that encourages disentanglement between groups of latent dimensions. Experiments on synthetic, monkey M1, and mouse voltage imaging data show that DisRNN consistently improves the disentanglement and interpretability of learned neural latent trajectories in low-dimensional space and low-rank connectivity over baseline lrRNNs that do not encourage partial disentanglement.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Calibrated Prediction of Randomly-Timed Biomarker Trajectories with Conformal Bands</title>
<link>https://arxiv.org/abs/2511.13911</link>
<guid>https://arxiv.org/abs/2511.13911</guid>
<content:encoded><![CDATA[
arXiv:2511.13911v1 Announce Type: cross 
Abstract: Despite recent progress in predicting biomarker trajectories from real clinical data, uncertainty in the predictions poses high-stakes risks (e.g., misdiagnosis) that limit their clinical deployment. To enable safe and reliable use of such predictions in healthcare, we introduce a conformal method for uncertainty-calibrated prediction of biomarker trajectories resulting from randomly-timed clinical visits of patients. Our approach extends conformal prediction to the setting of randomly-timed trajectories via a novel nonconformity score that produces prediction bands guaranteed to cover the unknown biomarker trajectories with a user-prescribed probability. We apply our method across a wide range of standard and state-of-the-art predictors for two well-established brain biomarkers of Alzheimer's disease, using neuroimaging data from real clinical studies. We observe that our conformal prediction bands consistently achieve the desired coverage, while also being tighter than baseline prediction bands. To further account for population heterogeneity, we develop group-conditional conformal bands and test their coverage guarantees across various demographic and clinically relevant subpopulations. Moreover, we demonstrate the clinical utility of our conformal bands in identifying subjects at high risk of progression to Alzheimer's disease. Specifically, we introduce an uncertainty-calibrated risk score that enables the identification of 17.5% more high-risk subjects compared to standard risk scores, highlighting the value of uncertainty calibration in real-world clinical decision making. Our code is available at github.com/vatass/ConformalBiomarkerTrajectories.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compute-in-Memory Implementation of State Space Models for Event Sequence Processing</title>
<link>https://arxiv.org/abs/2511.13912</link>
<guid>https://arxiv.org/abs/2511.13912</guid>
<content:encoded><![CDATA[
arXiv:2511.13912v1 Announce Type: cross 
Abstract: State space models (SSMs) have recently emerged as a powerful framework for long sequence processing, outperforming traditional methods on diverse benchmarks. Fundamentally, SSMs can generalize both recurrent and convolutional networks and have been shown to even capture key functions of biological systems. Here we report an approach to implement SSMs in energy-efficient compute-in-memory (CIM) hardware to achieve real-time, event-driven processing. Our work re-parameterizes the model to function with real-valued coefficients and shared decay constants, reducing the complexity of model mapping onto practical hardware systems. By leveraging device dynamics and diagonalized state transition parameters, the state evolution can be natively implemented in crossbar-based CIM systems combined with memristors exhibiting short-term memory effects. Through this algorithm and hardware co-design, we show the proposed system offers both high accuracy and high energy efficiency while supporting fully asynchronous processing for event-based vision and audio tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Compression and Artifact Correction for Streaming Underwater Imaging Sonar</title>
<link>https://arxiv.org/abs/2511.13922</link>
<guid>https://arxiv.org/abs/2511.13922</guid>
<content:encoded><![CDATA[
arXiv:2511.13922v1 Announce Type: cross 
Abstract: Real-time imaging sonar has become an important tool for underwater monitoring in environments where optical sensing is unreliable. Its broader use is constrained by two coupled challenges: highly limited uplink bandwidth and severe sonar-specific artifacts (speckle, motion blur, reverberation, acoustic shadows) that affect up to 98% of frames. We present SCOPE, a self-supervised framework that jointly performs compression and artifact correction without clean-noise pairs or synthetic assumptions. SCOPE combines (i) Adaptive Codebook Compression (ACC), which learns frequency-encoded latent representations tailored to sonar, with (ii) Frequency-Aware Multiscale Segmentation (FAMS), which decomposes frames into low-frequency structure and sparse high-frequency dynamics while suppressing rapidly fluctuating artifacts. A hedging training strategy further guides frequency-aware learning using low-pass proxy pairs generated without labels. Evaluated on months of in-situ ARIS sonar data, SCOPE achieves a structural similarity index (SSIM) of 0.77, representing a 40% improvement over prior self-supervised denoising baselines, at bitrates down to <= 0.0118 bpp. It reduces uplink bandwidth by more than 80% while improving downstream detection. The system runs in real time, with 3.1 ms encoding on an embedded GPU and 97 ms full multi-layer decoding on the server end. SCOPE has been deployed for months in three Pacific Northwest rivers to support real-time salmon enumeration and environmental monitoring in the wild. Results demonstrate that learning frequency-structured latents enables practical, low-bitrate sonar streaming with preserved signal details under real-world deployment conditions.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Likelihood for Random Forests and Ensembles</title>
<link>https://arxiv.org/abs/2511.13934</link>
<guid>https://arxiv.org/abs/2511.13934</guid>
<content:encoded><![CDATA[
arXiv:2511.13934v1 Announce Type: cross 
Abstract: We develop an empirical likelihood (EL) framework for random forests and related ensemble methods, providing a likelihood-based approach to quantify their statistical uncertainty. Exploiting the incomplete $U$-statistic structure inherent in ensemble predictions, we construct an EL statistic that is asymptotically chi-squared when subsampling induced by incompleteness is not overly sparse. Under sparser subsampling regimes, the EL statistic tends to over-cover due to loss of pivotality; we therefore propose a modified EL that restores pivotality through a simple adjustment. Our method retains key properties of EL while remaining computationally efficient. Theory for honest random forests and simulations demonstrate that modified EL achieves accurate coverage and practical reliability relative to existing inference methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference-Based Learning in Audio Applications: A Systematic Analysis</title>
<link>https://arxiv.org/abs/2511.13936</link>
<guid>https://arxiv.org/abs/2511.13936</guid>
<content:encoded><![CDATA[
arXiv:2511.13936v1 Announce Type: cross 
Abstract: Despite the parallel challenges that audio and text domains face in evaluating generative model outputs, preference learning remains remarkably underexplored in audio applications. Through a PRISMA-guided systematic review of approximately 500 papers, we find that only 30 (6%) apply preference learning to audio tasks. Our analysis reveals a field in transition: pre-2021 works focused on emotion recognition using traditional ranking methods (rankSVM), while post-2021 studies have pivoted toward generation tasks employing modern RLHF frameworks. We identify three critical patterns: (1) the emergence of multi-dimensional evaluation strategies combining synthetic, automated, and human preferences; (2) inconsistent alignment between traditional metrics (WER, PESQ) and human judgments across different contexts; and (3) convergence on multi-stage training pipelines that combine reward signals. Our findings suggest that while preference learning shows promise for audio, particularly in capturing subjective qualities like naturalness and musicality, the field requires standardized benchmarks, higher-quality datasets, and systematic investigation of how temporal factors unique to audio impact preference learning frameworks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels</title>
<link>https://arxiv.org/abs/2511.13940</link>
<guid>https://arxiv.org/abs/2511.13940</guid>
<content:encoded><![CDATA[
arXiv:2511.13940v1 Announce Type: cross 
Abstract: Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \times$ speedup for data- and tensor-parallel workloads, $4.08 \times$ for sequence-parallel workloads, and $1.22 \times$ for expert-parallel workloads.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Single Tensor Cell Segmentation using Scalar Field Representations</title>
<link>https://arxiv.org/abs/2511.13947</link>
<guid>https://arxiv.org/abs/2511.13947</guid>
<content:encoded><![CDATA[
arXiv:2511.13947v1 Announce Type: cross 
Abstract: We investigate image segmentation of cells under the lens of scalar fields. Our goal is to learn a continuous scalar field on image domains such that its segmentation produces robust instances for cells present in images. This field is a function parameterized by the trained network, and its segmentation is realized by the watershed method. The fields we experiment with are solutions to the Poisson partial differential equation and a diffusion mimicking the steady-state solution of the heat equation. These solutions are obtained by minimizing just the field residuals, no regularization is needed, providing a robust regression capable of diminishing the adverse impacts of outliers in the training data and allowing for sharp cell boundaries. A single tensor is all that is needed to train a \unet\ thus simplifying implementation, lowering training and inference times, hence reducing energy consumption, and requiring a small memory footprint, all attractive features in edge computing. We present competitive results on public datasets from the literature and show that our novel, simple yet geometrically insightful approach can achieve excellent cell segmentation results.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EchoAgent: Guideline-Centric Reasoning Agent for Echocardiography Measurement and Interpretation</title>
<link>https://arxiv.org/abs/2511.13948</link>
<guid>https://arxiv.org/abs/2511.13948</guid>
<content:encoded><![CDATA[
arXiv:2511.13948v1 Announce Type: cross 
Abstract: Purpose: Echocardiographic interpretation requires video-level reasoning and guideline-based measurement analysis, which current deep learning models for cardiac ultrasound do not support. We present EchoAgent, a framework that enables structured, interpretable automation for this domain. Methods: EchoAgent orchestrates specialized vision tools under Large Language Model (LLM) control to perform temporal localization, spatial measurement, and clinical interpretation. A key contribution is a measurement-feasibility prediction model that determines whether anatomical structures are reliably measurable in each frame, enabling autonomous tool selection. We curated a benchmark of diverse, clinically validated video-query pairs for evaluation. Results: EchoAgent achieves accurate, interpretable results despite added complexity of spatiotemporal video analysis. Outputs are grounded in visual evidence and clinical guidelines, supporting transparency and traceability. Conclusion: This work demonstrates the feasibility of agentic, guideline-aligned reasoning for echocardiographic video analysis, enabled by task-specific tools and full video-level automation. EchoAgent sets a new direction for trustworthy AI in cardiac ultrasound.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition</title>
<link>https://arxiv.org/abs/2511.13954</link>
<guid>https://arxiv.org/abs/2511.13954</guid>
<content:encoded><![CDATA[
arXiv:2511.13954v1 Announce Type: cross 
Abstract: Human emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer a more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we propose RBTransformer, a Transformer-based neural network architecture that models inter-cortical neural dynamics of the brain in latent space to better capture structured neural interactions for effective EEG-based emotion recognition. First, the EEG signals are converted into Band Differential Entropy (BDE) tokens, which are then passed through Electrode Identity embeddings to retain spatial provenance. These tokens are processed through successive inter-cortical multi-head attention blocks that construct an electrode x electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through a classification head to obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, and DREAMER datasets, over all three dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposed RBTransformer outperforms all previous state-of-the-art methods across all three datasets, over all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale</title>
<link>https://arxiv.org/abs/2511.14002</link>
<guid>https://arxiv.org/abs/2511.14002</guid>
<content:encoded><![CDATA[
arXiv:2511.14002v1 Announce Type: cross 
Abstract: Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRI Plane Orientation Detection using a Context-Aware 2.5D Model</title>
<link>https://arxiv.org/abs/2511.14021</link>
<guid>https://arxiv.org/abs/2511.14021</guid>
<content:encoded><![CDATA[
arXiv:2511.14021v1 Announce Type: cross 
Abstract: Humans can easily identify anatomical planes (axial, coronal, and sagittal) on a 2D MRI slice, but automated systems struggle with this task. Missing plane orientation metadata can complicate analysis, increase domain shift when merging heterogeneous datasets, and reduce accuracy of diagnostic classifiers. This study develops a classifier that accurately generates plane orientation metadata. We adopt a 2.5D context-aware model that leverages multi-slice information to avoid ambiguity from isolated slices and enable robust feature learning. We train the 2.5D model on both 3D slice sequences and static 2D images. While our 2D reference model achieves 98.74% accuracy, our 2.5D method raises this to 99.49%, reducing errors by 60%, highlighting the importance of 2.5D context. We validate the utility of our generated metadata in a brain tumor detection task. A gated strategy selectively uses metadata-enhanced predictions based on uncertainty scores, boosting accuracy from 97.0% with an image-only model to 98.0%, reducing misdiagnoses by 33.3%. We integrate our plane orientation model into an interactive web application and provide it open-source.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.14022</link>
<guid>https://arxiv.org/abs/2511.14022</guid>
<content:encoded><![CDATA[
arXiv:2511.14022v1 Announce Type: cross 
Abstract: Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-free Detection of AI-generated images via Cropping Robustness</title>
<link>https://arxiv.org/abs/2511.14030</link>
<guid>https://arxiv.org/abs/2511.14030</guid>
<content:encoded><![CDATA[
arXiv:2511.14030v1 Announce Type: cross 
Abstract: AI-generated image detection has become crucial with the rapid advancement of vision-generative models. Instead of training detectors tailored to specific datasets, we study a training-free approach leveraging self-supervised models without requiring prior data knowledge. These models, pre-trained with augmentations like RandomResizedCrop, learn to produce consistent representations across varying resolutions. Motivated by this, we propose WaRPAD, a training-free AI-generated image detection algorithm based on self-supervised models. Since neighborhood pixel differences in images are highly sensitive to resizing operations, WaRPAD first defines a base score function that quantifies the sensitivity of image embeddings to perturbations along high-frequency directions extracted via Haar wavelet decomposition. To simulate robustness against cropping augmentation, we rescale each image to a multiple of the models input size, divide it into smaller patches, and compute the base score for each patch. The final detection score is then obtained by averaging the scores across all patches. We validate WaRPAD on real datasets of diverse resolutions and domains, and images generated by 23 different generative models. Our method consistently achieves competitive performance and demonstrates strong robustness to test-time corruptions. Furthermore, as invariance to RandomResizedCrop is a common training scheme across self-supervised models, we show that WaRPAD is applicable across self-supervised models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Splat Regression Models</title>
<link>https://arxiv.org/abs/2511.14042</link>
<guid>https://arxiv.org/abs/2511.14042</guid>
<content:encoded><![CDATA[
arXiv:2511.14042v1 Announce Type: cross 
Abstract: We introduce a highly expressive class of function approximators called Splat Regression Models. Model outputs are mixtures of heterogeneous and anisotropic bump functions, termed splats, each weighted by an output vector. The power of splat modeling lies in its ability to locally adjust the scale and direction of each splat, achieving both high interpretability and accuracy. Fitting splat models reduces to optimization over the space of mixing measures, which can be implemented using Wasserstein-Fisher-Rao gradient flows. As a byproduct, we recover the popular Gaussian Splatting methodology as a special case, providing a unified theoretical framework for this state-of-the-art technique that clearly disambiguates the inverse problem, the model, and the optimization algorithm. Through numerical experiments, we demonstrate that the resulting models and algorithms constitute a flexible and promising approach for solving diverse approximation, estimation, and inverse problems involving low-dimensional data.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The CHASM-SWPC Dataset for Coronal Hole Detection &amp; Analysis</title>
<link>https://arxiv.org/abs/2511.14044</link>
<guid>https://arxiv.org/abs/2511.14044</guid>
<content:encoded><![CDATA[
arXiv:2511.14044v1 Announce Type: cross 
Abstract: Coronal holes (CHs) are low-activity, low-density solar coronal regions with open magnetic field lines (Cranmer 2009). In the extreme ultraviolet (EUV) spectrum, CHs appear as dark patches. Using daily hand-drawn maps from the Space Weather Prediction Center (SWPC), we developed a semi-automated pipeline to digitize the SWPC maps into binary segmentation masks. The resulting masks constitute the CHASM-SWPC dataset, a high-quality dataset to train and test automated CH detection models, which is released with this paper. We developed CHASM (Coronal Hole Annotation using Semi-automatic Methods), a software tool for semi-automatic annotation that enables users to rapidly and accurately annotate SWPC maps. The CHASM tool enabled us to annotate 1,111 CH masks, comprising the CHASM-SWPC-1111 dataset. We then trained multiple CHRONNOS (Coronal Hole RecOgnition Neural Network Over multi-Spectral-data) architecture (Jarolim et al. 2021) neural networks using the CHASM-SWPC dataset and compared their performance. Training the CHRONNOS neural network on these data achieved an accuracy of 0.9805, a True Skill Statistic (TSS) of 0.6807, and an intersection-over-union (IoU) of 0.5668, which is higher than the original pretrained CHRONNOS model Jarolim et al. (2021) achieved an accuracy of 0.9708, a TSS of 0.6749, and an IoU of 0.4805, when evaluated on the CHASM-SWPC-1111 test set.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wasserstein Distributionally Robust Nash Equilibrium Seeking with Heterogeneous Data: A Lagrangian Approach</title>
<link>https://arxiv.org/abs/2511.14048</link>
<guid>https://arxiv.org/abs/2511.14048</guid>
<content:encoded><![CDATA[
arXiv:2511.14048v1 Announce Type: cross 
Abstract: We study a class of distributionally robust games where agents are allowed to heterogeneously choose their risk aversion with respect to distributional shifts of the uncertainty. In our formulation, heterogeneous Wasserstein ball constraints on each distribution are enforced through a penalty function leveraging a Lagrangian formulation. We then formulate the distributionally robust Nash equilibrium problem and show that under certain assumptions it is equivalent to a finite-dimensional variational inequality problem with a strongly monotone mapping. We then design an approximate Nash equilibrium seeking algorithm and prove convergence of the average regret to a quantity that diminishes with the number of iterations, thus learning the desired equilibrium up to an a priori specified accuracy. Numerical simulations corroborate our theoretical findings.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering</title>
<link>https://arxiv.org/abs/2511.14062</link>
<guid>https://arxiv.org/abs/2511.14062</guid>
<content:encoded><![CDATA[
arXiv:2511.14062v1 Announce Type: cross 
Abstract: Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Black-box Backdoor Attacks on IoT Sensory Data</title>
<link>https://arxiv.org/abs/2511.14074</link>
<guid>https://arxiv.org/abs/2511.14074</guid>
<content:encoded><![CDATA[
arXiv:2511.14074v1 Announce Type: cross 
Abstract: Sensor data-based recognition systems are widely used in various applications, such as gait-based authentication and human activity recognition (HAR). Modern wearable and smart devices feature various built-in Inertial Measurement Unit (IMU) sensors, and such sensor-based measurements can be fed to a machine learning-based model to train and classify human activities. While deep learning-based models have proven successful in classifying human activity and gestures, they pose various security risks. In our paper, we discuss a novel dynamic trigger-generation technique for performing black-box adversarial attacks on sensor data-based IoT systems. Our empirical analysis shows that the attack is successful on various datasets and classifier models with minimal perturbation on the input data. We also provide a detailed comparative analysis of performance and stealthiness to various other poisoning techniques found in backdoor attacks. We also discuss some adversarial defense mechanisms and their impact on the effectiveness of our trigger-generation technique.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Patient-Independent Neonatal Seizure Prediction Model Using Reduced Montage EEG and ECG</title>
<link>https://arxiv.org/abs/2511.14110</link>
<guid>https://arxiv.org/abs/2511.14110</guid>
<content:encoded><![CDATA[
arXiv:2511.14110v1 Announce Type: cross 
Abstract: Neonates are highly susceptible to seizures, often leading to short or long-term neurological impairments. However, clinical manifestations of neonatal seizures are subtle and often lead to misdiagnoses. This increases the risk of prolonged, untreated seizure activity and subsequent brain injury. Continuous video electroencephalogram (cEEG) monitoring is the gold standard for seizure detection. However, this is an expensive evaluation that requires expertise and time. In this study, we propose a convolutional neural network-based model for early prediction of neonatal seizures by distinguishing between interictal and preictal states of the EEG. Our model is patient-independent, enabling generalization across multiple subjects, and utilizes mel-frequency cepstral coefficient matrices extracted from multichannel EEG and electrocardiogram (ECG) signals as input features. Trained and validated on the Helsinki neonatal EEG dataset with 10-fold cross-validation, the proposed model achieved an average accuracy of 97.52%, sensitivity of 98.31%, specificity of 96.39%, and F1-score of 97.95%, enabling accurate seizure prediction up to 30 minutes before onset. The inclusion of ECG alongside EEG improved the F1-score by 1.42%, while the incorporation of an attention mechanism yielded an additional 0.5% improvement. To enhance transparency, we incorporated SHapley Additive exPlanations (SHAP) as an explainable artificial intelligence method to interpret the model and provided localization of seizure focus using scalp plots. The overall results demonstrate the model's potential for minimally supervised deployment in neonatal intensive care units, enabling timely and reliable prediction of neonatal seizures, while demonstrating strong generalization capability across unseen subjects through transfer learning.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training</title>
<link>https://arxiv.org/abs/2511.14124</link>
<guid>https://arxiv.org/abs/2511.14124</guid>
<content:encoded><![CDATA[
arXiv:2511.14124v1 Announce Type: cross 
Abstract: Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.
  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MalRAG: A Retrieval-Augmented LLM Framework for Open-set Malicious Traffic Identification</title>
<link>https://arxiv.org/abs/2511.14129</link>
<guid>https://arxiv.org/abs/2511.14129</guid>
<content:encoded><![CDATA[
arXiv:2511.14129v1 Announce Type: cross 
Abstract: Fine-grained identification of IDS-flagged suspicious traffic is crucial in cybersecurity. In practice, cyber threats evolve continuously, making the discovery of novel malicious traffic a critical necessity as well as the identification of known classes. Recent studies have advanced this goal with deep models, but they often rely on task-specific architectures that limit transferability and require per-dataset tuning. In this paper we introduce MalRAG, the first LLM driven retrieval-augmented framework for open-set malicious traffic identification. MalRAG freezes the LLM and operates via comprehensive traffic knowledge construction, adaptive retrieval, and prompt engineering. Concretely, we construct a multi-view traffic database by mining prior malicious traffic from content, structural, and temporal perspectives. Furthermore, we introduce a Coverage-Enhanced Retrieval Algorithm that queries across these views to assemble the most probable candidates, thereby improving the inclusion of correct evidence. We then employ Traffic-Aware Adaptive Pruning to select a variable subset of these candidates based on traffic-aware similarity scores, suppressing incorrect matches and yielding reliable retrieved evidence. Moreover, we develop a suite of guidance prompts where task instruction, evidence referencing, and decision guidance are integrated with the retrieved evidence to improve LLM performance. Across diverse real-world datasets and settings, MalRAG delivers state-of-the-art results in both fine-grained identification of known classes and novel malicious traffic discovery. Ablation and deep-dive analyses further show that MalRAG effective leverages LLM capabilities yet achieves open-set malicious traffic identification without relying on a specific LLM.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling</title>
<link>https://arxiv.org/abs/2511.14142</link>
<guid>https://arxiv.org/abs/2511.14142</guid>
<content:encoded><![CDATA[
arXiv:2511.14142v1 Announce Type: cross 
Abstract: Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCOPE: Spectral Concentration by Distributionally Robust Joint Covariance-Precision Estimation</title>
<link>https://arxiv.org/abs/2511.14146</link>
<guid>https://arxiv.org/abs/2511.14146</guid>
<content:encoded><![CDATA[
arXiv:2511.14146v1 Announce Type: cross 
Abstract: We propose a distributionally robust formulation for simultaneously estimating the covariance matrix and the precision matrix of a random vector.The proposed model minimizes the worst-case weighted sum of the Frobenius loss of the covariance estimator and Stein's loss of the precision matrix estimator against all distributions from an ambiguity set centered at the nominal distribution. The radius of the ambiguity set is measured via convex spectral divergence. We demonstrate that the proposed distributionally robust estimation model can be reduced to a convex optimization problem, thereby yielding quasi-analytical estimators. The joint estimators are shown to be nonlinear shrinkage estimators. The eigenvalues of the estimators are shrunk nonlinearly towards a positive scalar, where the scalar is determined by the weight coefficient of the loss terms. By tuning the coefficient carefully, the shrinkage corrects the spectral bias of the empirical covariance/precision matrix estimator. By this property, we call the proposed joint estimator the Spectral concentrated COvariance and Precision matrix Estimator (SCOPE). We demonstrate that the shrinkage effect improves the condition number of the estimator. We provide a parameter-tuning scheme that adjusts the shrinkage target and intensity that is asymptotically optimal. Numerical experiments on synthetic and real data show that our shrinkage estimators perform competitively against state-of-the-art estimators in practical applications.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imaging with super-resolution in changing random media</title>
<link>https://arxiv.org/abs/2511.14147</link>
<guid>https://arxiv.org/abs/2511.14147</guid>
<content:encoded><![CDATA[
arXiv:2511.14147v1 Announce Type: cross 
Abstract: We develop an imaging algorithm that exploits strong scattering to achieve super-resolution in changing random media. The method processes large and diverse array datasets using sparse dictionary learning, clustering, and multidimensional scaling. Starting from random initializations, the algorithm reliably extracts the unknown medium properties necessary for accurate imaging using back-propagation, $\ell_2$ or $\ell_1$ methods. Remarkably, scattering enhances resolution beyond homogeneous medium limits. When abundant data are available, the algorithm allows the realization of super-resolution in imaging.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.14148</link>
<guid>https://arxiv.org/abs/2511.14148</guid>
<content:encoded><![CDATA[
arXiv:2511.14148v1 Announce Type: cross 
Abstract: Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Discovery on Higher-Order Interactions</title>
<link>https://arxiv.org/abs/2511.14206</link>
<guid>https://arxiv.org/abs/2511.14206</guid>
<content:encoded><![CDATA[
arXiv:2511.14206v1 Announce Type: cross 
Abstract: Causal discovery combines data with knowledge provided by experts to learn the DAG representing the causal relationships between a given set of variables. When data are scarce, bagging is used to measure our confidence in an average DAG obtained by aggregating bootstrapped DAGs. However, the aggregation step has received little attention from the specialized literature: the average DAG is constructed using only the confidence in the individual edges of the bootstrapped DAGs, thus disregarding complex higher-order edge structures. In this paper, we introduce a novel theoretical framework based on higher-order structures and describe a new DAG aggregation algorithm. We perform a simulation study, discussing the advantages and limitations of the proposed approach. Our proposal is both computationally efficient and effective, outperforming state-of-the-art solutions, especially in low sample size regimes and under high dimensionality settings.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution</title>
<link>https://arxiv.org/abs/2511.14210</link>
<guid>https://arxiv.org/abs/2511.14210</guid>
<content:encoded><![CDATA[
arXiv:2511.14210v1 Announce Type: cross 
Abstract: We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Large Language Models (LLMs) Understand Chronology?</title>
<link>https://arxiv.org/abs/2511.14214</link>
<guid>https://arxiv.org/abs/2511.14214</guid>
<content:encoded><![CDATA[
arXiv:2511.14214v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home</title>
<link>https://arxiv.org/abs/2511.14227</link>
<guid>https://arxiv.org/abs/2511.14227</guid>
<content:encoded><![CDATA[
arXiv:2511.14227v1 Announce Type: cross 
Abstract: Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization</title>
<link>https://arxiv.org/abs/2511.14238</link>
<guid>https://arxiv.org/abs/2511.14238</guid>
<content:encoded><![CDATA[
arXiv:2511.14238v1 Announce Type: cross 
Abstract: The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Count The Notes: Histogram-Based Supervision for Automatic Music Transcription</title>
<link>https://arxiv.org/abs/2511.14250</link>
<guid>https://arxiv.org/abs/2511.14250</guid>
<content:encoded><![CDATA[
arXiv:2511.14250v1 Announce Type: cross 
Abstract: Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistically controllable microstructure reconstruction framework for heterogeneous materials using sliced-Wasserstein metric and neural networks</title>
<link>https://arxiv.org/abs/2511.14268</link>
<guid>https://arxiv.org/abs/2511.14268</guid>
<content:encoded><![CDATA[
arXiv:2511.14268v1 Announce Type: cross 
Abstract: Heterogeneous porous materials play a crucial role in various engineering systems. Microstructure characterization and reconstruction provide effective means for modeling these materials, which are critical for conducting physical property simulations, structure-property linkage studies, and enhancing their performance across different applications. To achieve superior controllability and applicability with small sample sizes, we propose a statistically controllable microstructure reconstruction framework that integrates neural networks with sliced-Wasserstein metric. Specifically, our approach leverages local pattern distribution for microstructure characterization and employs a controlled sampling strategy to generate target distributions that satisfy given conditional parameters. A neural network-based model establishes the mapping from the input distribution to the target local pattern distribution, enabling microstructure reconstruction. Combinations of sliced-Wasserstein metric and gradient optimization techniques minimize the distance between these distributions, leading to a stable and reliable model. Our method can perform stochastic and controllable reconstruction tasks even with small sample sizes. Additionally, it can generate large-size (e.g. 512 and 1024) 3D microstructures using a chunking strategy. By introducing spatial location masks, our method excels at generating spatially heterogeneous and complex microstructures. We conducted experiments on stochastic reconstruction, controllable reconstruction, heterogeneous reconstruction, and large-size microstructure reconstruction across various materials. Comparative analysis through visualization, statistical measures, and physical property simulations demonstrates the effectiveness, providing new insights and possibilities for research on structure-property linkage and material inverse design.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuralSSD: A Neural Solver for Signed Distance Surface Reconstruction</title>
<link>https://arxiv.org/abs/2511.14283</link>
<guid>https://arxiv.org/abs/2511.14283</guid>
<content:encoded><![CDATA[
arXiv:2511.14283v1 Announce Type: cross 
Abstract: We proposed a generalized method, NeuralSSD, for reconstructing a 3D implicit surface from the widely-available point cloud data. NeuralSSD is a solver-based on the neural Galerkin method, aimed at reconstructing higher-quality and accurate surfaces from input point clouds. Implicit method is preferred due to its ability to accurately represent shapes and its robustness in handling topological changes. However, existing parameterizations of implicit fields lack explicit mechanisms to ensure a tight fit between the surface and input data. To address this, we propose a novel energy equation that balances the reliability of point cloud information. Additionally, we introduce a new convolutional network that learns three-dimensional information to achieve superior optimization results. This approach ensures that the reconstructed surface closely adheres to the raw input points and infers valuable inductive biases from point clouds, resulting in a highly accurate and stable surface reconstruction. NeuralSSD is evaluated on a variety of challenging datasets, including the ShapeNet and Matterport datasets, and achieves state-of-the-art results in terms of both surface reconstruction accuracy and generalizability.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmentwise Pruning in Audio-Language Models</title>
<link>https://arxiv.org/abs/2511.14293</link>
<guid>https://arxiv.org/abs/2511.14293</guid>
<content:encoded><![CDATA[
arXiv:2511.14293v1 Announce Type: cross 
Abstract: Recent audio-language models have shown impressive performance across a wide range of audio tasks and are increasingly capable of handling long audio inputs. However, the computing costs in these models heavily depend on sequence length, which can become very large given the nature of audio data. In the vision-language domain, token pruning methods have proven effective in reducing token counts while preserving strong performance on standard benchmarks. In this work, we investigate the relevance and effectiveness of such token selection strategies in the context of audio-language models. We also improve them by proposing a lightweight strategy that takes the time dimension into account. While retaining only a quarter of the initial tokens, our approach results in a relative maximum decrease of 2% in CIDEr on Clotho v2 and a relative maximum decrease of 4% in accuracy on MMAU.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2511.14295</link>
<guid>https://arxiv.org/abs/2511.14295</guid>
<content:encoded><![CDATA[
arXiv:2511.14295v1 Announce Type: cross 
Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion</title>
<link>https://arxiv.org/abs/2511.14301</link>
<guid>https://arxiv.org/abs/2511.14301</guid>
<content:encoded><![CDATA[
arXiv:2511.14301v1 Announce Type: cross 
Abstract: Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Audio Question Answering with GRPO-Based Fine-Tuning and Calibrated Segment-Level Predictions</title>
<link>https://arxiv.org/abs/2511.14307</link>
<guid>https://arxiv.org/abs/2511.14307</guid>
<content:encoded><![CDATA[
arXiv:2511.14307v1 Announce Type: cross 
Abstract: In this report, we describe our submission to Track 5 of the DCASE 2025 Challenge for the task of Audio Question Answering(AQA). Our system leverages the SSL backbone BEATs to extract frame-level audio features, which are then processed by a classification head to generate segment-level predictions of acoustic events, following the Audioset ontology. These segment-level predictions are subsequently calibrated before producing event-level predictions. Finally, these predictions are incorporated into a structured prompt, along with the question and candidate answers. This prompt is then fed to a fine-tuned version of Qwen2.5-7B-Instruct, trained using the GRPO algorithm with a simple reward function. Our method achieves an accuracy of 62.6 % on the development set, demonstrating the effectiveness of combining acoustic event reasoning with instruction-tuned large language models for AQA.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling</title>
<link>https://arxiv.org/abs/2511.14334</link>
<guid>https://arxiv.org/abs/2511.14334</guid>
<content:encoded><![CDATA[
arXiv:2511.14334v1 Announce Type: cross 
Abstract: One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model</title>
<link>https://arxiv.org/abs/2511.14368</link>
<guid>https://arxiv.org/abs/2511.14368</guid>
<content:encoded><![CDATA[
arXiv:2511.14368v1 Announce Type: cross 
Abstract: While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection</title>
<link>https://arxiv.org/abs/2511.14422</link>
<guid>https://arxiv.org/abs/2511.14422</guid>
<content:encoded><![CDATA[
arXiv:2511.14422v1 Announce Type: cross 
Abstract: In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.
  To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.14427</link>
<guid>https://arxiv.org/abs/2511.14427</guid>
<content:encoded><![CDATA[
arXiv:2511.14427v1 Announce Type: cross 
Abstract: Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skewness-Robust Causal Discovery in Location-Scale Noise Models</title>
<link>https://arxiv.org/abs/2511.14441</link>
<guid>https://arxiv.org/abs/2511.14441</guid>
<content:encoded><![CDATA[
arXiv:2511.14441v1 Announce Type: cross 
Abstract: To distinguish Markov equivalent graphs in causal discovery, it is necessary to restrict the structural causal model. Crucially, we need to be able to distinguish cause $X$ from effect $Y$ in bivariate models, that is, distinguish the two graphs $X \to Y$ and $Y \to X$. Location-scale noise models (LSNMs), in which the effect $Y$ is modeled based on the cause $X$ as $Y = f(X) + g(X)N$, form a flexible class of models that is general and identifiable in most cases. Estimating these models for arbitrary noise terms $N$, however, is challenging. Therefore, practical estimators are typically restricted to symmetric distributions, such as the normal distribution. As we showcase in this paper, when $N$ is a skewed random variable, which is likely in real-world domains, the reliability of these approaches decreases. To approach this limitation, we propose SkewD, a likelihood-based algorithm for bivariate causal discovery under LSNMs with skewed noise distributions. SkewD extends the usual normal-distribution framework to the skew-normal setting, enabling reliable inference under symmetric and skewed noise. For parameter estimation, we employ a combination of a heuristic search and an expectation conditional maximization algorithm. We evaluate SkewD on novel synthetically generated datasets with skewed noise as well as established benchmark datasets. Throughout our experiments, SkewD exhibits a strong performance and, in comparison to prior work, remains robust under high skewness.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning</title>
<link>https://arxiv.org/abs/2511.14445</link>
<guid>https://arxiv.org/abs/2511.14445</guid>
<content:encoded><![CDATA[
arXiv:2511.14445v1 Announce Type: cross 
Abstract: We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient-Based Join Ordering</title>
<link>https://arxiv.org/abs/2511.14482</link>
<guid>https://arxiv.org/abs/2511.14482</guid>
<content:encoded><![CDATA[
arXiv:2511.14482v1 Announce Type: cross 
Abstract: Join ordering is the NP-hard problem of selecting the most efficient sequence in which to evaluate joins (conjunctive, binary operators) in a database query. As the performance of query execution critically depends on this choice, join ordering lies at the core of query optimization. Traditional approaches cast this problem as a discrete combinatorial search over binary trees guided by a cost model, but they often suffer from high computational complexity and limited scalability. We show that, when the cost model is differentiable, the query plans can be continuously relaxed into a soft adjacency matrix representing a superposition of plans. This continuous relaxation, together with a Gumbel-Softmax parameterization of the adjacency matrix and differentiable constraints enforcing plan validity, enables gradient-based search for plans within this relaxed space. Using a learned Graph Neural Network as the cost model, we demonstrate that this gradient-based approach can find comparable and even lower-cost plans compared to traditional discrete local search methods on two different graph datasets. Furthermore, we empirically show that the runtime of this approach scales linearly with query size, in contrast to quadratic or exponential runtimes of classical approaches. We believe this first step towards gradient-based join ordering can lead to more effective and efficient query optimizers in the future.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Convergence in Parameter-Agnostic Error Feedback through Momentum</title>
<link>https://arxiv.org/abs/2511.14501</link>
<guid>https://arxiv.org/abs/2511.14501</guid>
<content:encoded><![CDATA[
arXiv:2511.14501v1 Announce Type: cross 
Abstract: Communication compression is essential for scalable distributed training of modern machine learning models, but it often degrades convergence due to the noise it introduces. Error Feedback (EF) mechanisms are widely adopted to mitigate this issue of distributed compression algorithms. Despite their popularity and training efficiency, existing distributed EF algorithms often require prior knowledge of problem parameters (e.g., smoothness constants) to fine-tune stepsizes. This limits their practical applicability especially in large-scale neural network training. In this paper, we study normalized error feedback algorithms that combine EF with normalized updates, various momentum variants, and parameter-agnostic, time-varying stepsizes, thus eliminating the need for problem-dependent tuning. We analyze the convergence of these algorithms for minimizing smooth functions, and establish parameter-agnostic complexity bounds that are close to the best-known bounds with carefully-tuned problem-dependent stepsizes. Specifically, we show that normalized EF21 achieve the convergence rate of near ${O}(1/T^{1/4})$ for Polyak's heavy-ball momentum, ${O}(1/T^{2/7})$ for Iterative Gradient Transport (IGT), and ${O}(1/T^{1/3})$ for STORM and Hessian-corrected momentum. Our results hold with decreasing stepsizes and small mini-batches. Finally, our empirical experiments confirm our theoretical insights.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation</title>
<link>https://arxiv.org/abs/2511.14530</link>
<guid>https://arxiv.org/abs/2511.14530</guid>
<content:encoded><![CDATA[
arXiv:2511.14530v1 Announce Type: cross 
Abstract: Existing video Variational Autoencoders (VAEs) generally overlook the similarity between frame contents, leading to redundant latent modeling. In this paper, we propose decoupled VAE (DeCo-VAE) to achieve compact latent representation. Instead of encoding RGB pixels directly, we decompose video content into distinct components via explicit decoupling: keyframe, motion and residual, and learn dedicated latent representation for each. To avoid cross-component interference, we design dedicated encoders for each decoupled component and adopt a shared 3D decoder to maintain spatiotemporal consistency during reconstruction. We further utilize a decoupled adaptation strategy that freezes partial encoders while training the others sequentially, ensuring stable training and accurate learning of both static and dynamic features. Extensive quantitative and qualitative experiments demonstrate that DeCo-VAE achieves superior video reconstruction performance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepBlip: Estimating Conditional Average Treatment Effects Over Time</title>
<link>https://arxiv.org/abs/2511.14545</link>
<guid>https://arxiv.org/abs/2511.14545</guid>
<content:encoded><![CDATA[
arXiv:2511.14545v1 Announce Type: cross 
Abstract: Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection</title>
<link>https://arxiv.org/abs/2511.14554</link>
<guid>https://arxiv.org/abs/2511.14554</guid>
<content:encoded><![CDATA[
arXiv:2511.14554v1 Announce Type: cross 
Abstract: Deepfakes generated by advanced GANs and autoencoders severely threaten information integrity and societal stability. Single-stream CNNs fail to capture multi-scale forgery artifacts across spatial, texture, and frequency domains, limiting robustness and generalization. We introduce the ForensicFlow, a tri-modal forensic framework that synergistically fuses RGB, texture, and frequency evidence for video Deepfake detection. The RGB branch (ConvNeXt-tiny) extracts global visual inconsistencies; the texture branch (Swin Transformer-tiny) detects fine-grained blending artifacts; the frequency branch (CNN + SE) identifies periodic spectral noise. Attention-based temporal pooling dynamically prioritizes high-evidence frames, while adaptive attention fusion balances branch contributions.Trained on Celeb-DF (v2) with Focal Loss, ForensicFlow achieves AUC 0.9752, F1-Score 0.9408, and accuracy 0.9208, outperforming single-stream baselines. Ablation validates branch synergy; Grad-CAM confirms forensic focus. This comprehensive feature fusion provides superior resilience against subtle forgeries.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Apo2Mol: 3D Molecule Generation via Dynamic Pocket-Aware Diffusion Models</title>
<link>https://arxiv.org/abs/2511.14559</link>
<guid>https://arxiv.org/abs/2511.14559</guid>
<content:encoded><![CDATA[
arXiv:2511.14559v1 Announce Type: cross 
Abstract: Deep generative models are rapidly advancing structure-based drug design, offering substantial promise for generating small molecule ligands that bind to specific protein targets. However, most current approaches assume a rigid protein binding pocket, neglecting the intrinsic flexibility of proteins and the conformational rearrangements induced by ligand binding, limiting their applicability in practical drug discovery. Here, we propose Apo2Mol, a diffusion-based generative framework for 3D molecule design that explicitly accounts for conformational flexibility in protein binding pockets. To support this, we curate a dataset of over 24,000 experimentally resolved apo-holo structure pairs from the Protein Data Bank, enabling the characterization of protein structure changes associated with ligand binding. Apo2Mol employs a full-atom hierarchical graph-based diffusion model that simultaneously generates 3D ligand molecules and their corresponding holo pocket conformations from input apo states. Empirical studies demonstrate that Apo2Mol can achieve state-of-the-art performance in generating high-affinity ligands and accurately capture realistic protein pocket conformational changes.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online learning of subgrid-scale models for quasi-geostrophic turbulence in planetary interiors</title>
<link>https://arxiv.org/abs/2511.14581</link>
<guid>https://arxiv.org/abs/2511.14581</guid>
<content:encoded><![CDATA[
arXiv:2511.14581v1 Announce Type: cross 
Abstract: The use of machine learning to represent subgrid-scale (SGS) dynamics is now well established in weather forecasting and climate modelling. Recent advances have demonstrated that SGS models trained via ``online'' end-to-end learning -- where the dynamical solver operating on the filtered equations participates in the training -- can outperform traditional physics-based approaches. Most studies, however, have focused on idealised periodic domains, neglecting the mechanical boundaries present e.g. in planetary interiors. To address this issue, we consider two-dimensional quasi-geostrophic turbulent flow in an axisymmetric bounded domain that we model using a pseudo-spectral differentiable solver, thereby enabling online learning. We examine three configurations, varying the geometry (between an exponential container and a spherical shell) and the rotation rate. Flow is driven by a prescribed analytical forcing, allowing for precise control over the energy injection scale and an exact estimate of the power input. We evaluate the accuracy of the online-trained SGS model against the reference direct numerical simulation using integral quantities and spectral diagnostics. In all configurations, we show that an SGS model trained on data spanning only one turnover time remains stable and accurate over integrations at least a hundred times longer than the training period. Moreover, we demonstrate the model's remarkable ability to reproduce slow processes occurring on time scales far exceeding the training duration, such as the inward drift of jets in the spherical shell. These results suggest a promising path towards developing SGS models for planetary and stellar interior dynamics, including dynamo processes.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease</title>
<link>https://arxiv.org/abs/2511.14603</link>
<guid>https://arxiv.org/abs/2511.14603</guid>
<content:encoded><![CDATA[
arXiv:2511.14603v1 Announce Type: cross 
Abstract: Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models</title>
<link>https://arxiv.org/abs/2511.14606</link>
<guid>https://arxiv.org/abs/2511.14606</guid>
<content:encoded><![CDATA[
arXiv:2511.14606v1 Announce Type: cross 
Abstract: Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.14617</link>
<guid>https://arxiv.org/abs/2511.14617</guid>
<content:encoded><![CDATA[
arXiv:2511.14617v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Doppler Invariant CNN for Signal Classification</title>
<link>https://arxiv.org/abs/2511.14640</link>
<guid>https://arxiv.org/abs/2511.14640</guid>
<content:encoded><![CDATA[
arXiv:2511.14640v1 Announce Type: cross 
Abstract: Radio spectrum monitoring in contested environments motivates the need for reliable automatic signal classification technology. Prior work highlights deep learning as a promising approach, but existing models depend on brute-force Doppler augmentation to achieve real-world generalization, which undermines both training efficiency and interpretability. In this paper, we propose a convolutional neural network (CNN) architecture with complex-valued layers that exploits convolutional shift equivariance in the frequency domain. To establish provable frequency bin shift invariance, we use adaptive polyphase sampling (APS) as pooling layers followed by a global average pooling layer at the end of the network. Using a synthetic dataset of common interference signals, experimental results demonstrate that unlike a vanilla CNN, our model maintains consistent classification accuracy with and without random Doppler shifts despite being trained on no Doppler-shifted examples. Overall, our method establishes an invariance-driven framework for signal classification that offers provable robustness against real-world effects.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Derivative of the truncated singular value and eigen decomposition</title>
<link>https://arxiv.org/abs/2511.14651</link>
<guid>https://arxiv.org/abs/2511.14651</guid>
<content:encoded><![CDATA[
arXiv:2511.14651v1 Announce Type: cross 
Abstract: Recently developed applications in the field of machine learning and computational physics rely on automatic differentiation techniques, that require stable and efficient linear algebra gradient computations. This technical note provides a comprehensive and detailed discussion of the derivative of the truncated singular and eigenvalue decomposition. It summarizes previous work and builds on them with an extensive description of how to derive the relevant terms. A main focus is correctly expressing the derivative in terms of the truncated part, despite lacking knowledge of the full decomposition.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models</title>
<link>https://arxiv.org/abs/2511.14694</link>
<guid>https://arxiv.org/abs/2511.14694</guid>
<content:encoded><![CDATA[
arXiv:2511.14694v1 Announce Type: cross 
Abstract: Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental "grammar" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring</title>
<link>https://arxiv.org/abs/2511.14698</link>
<guid>https://arxiv.org/abs/2511.14698</guid>
<content:encoded><![CDATA[
arXiv:2511.14698v1 Announce Type: cross 
Abstract: Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization</title>
<link>https://arxiv.org/abs/2511.14710</link>
<guid>https://arxiv.org/abs/2511.14710</guid>
<content:encoded><![CDATA[
arXiv:2511.14710v1 Announce Type: cross 
Abstract: We establish the first global convergence result of neural networks for two stage least squares (2SLS) approach in nonparametric instrumental variable regression (NPIV). This is achieved by adopting a lifted perspective through mean-field Langevin dynamics (MFLD), unlike standard MFLD, however, our setting of 2SLS entails a \emph{bilevel} optimization problem in the space of probability measures. To address this challenge, we leverage the penalty gradient approach recently developed for bilevel optimization which formulates bilevel optimization as a Lagrangian problem. This leads to a novel fully first-order algorithm, termed \texttt{F$^2$BMLD}. Apart from the convergence bound, we further provide a generalization bound, revealing an inherent trade-off in the choice of the Lagrange multiplier between optimization and statistical guarantees. Finally, we empirically validate the effectiveness of the proposed method on an offline reinforcement learning benchmark.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis</title>
<link>https://arxiv.org/abs/2511.14755</link>
<guid>https://arxiv.org/abs/2511.14755</guid>
<content:encoded><![CDATA[
arXiv:2511.14755v1 Announce Type: cross 
Abstract: As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARC Is a Vision Problem!</title>
<link>https://arxiv.org/abs/2511.14761</link>
<guid>https://arxiv.org/abs/2511.14761</guid>
<content:encoded><![CDATA[
arXiv:2511.14761v1 Announce Type: cross 
Abstract: The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a "canvas" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant neural networks and equivarification</title>
<link>https://arxiv.org/abs/1906.07172</link>
<guid>https://arxiv.org/abs/1906.07172</guid>
<content:encoded><![CDATA[
arXiv:1906.07172v5 Announce Type: replace 
Abstract: Equivariant neural networks are a class of neural networks designed to preserve symmetries inherent in the data. In this paper, we introduce a general method for modifying a neural network to enforce equivariance, a process we refer to as equivarification. We further show that group convolutional neural networks (G-CNNs) arise as a special case of our framework.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Virtual Human Generative Model: Masked Modeling Approach for Learning Human Characteristics</title>
<link>https://arxiv.org/abs/2306.10656</link>
<guid>https://arxiv.org/abs/2306.10656</guid>
<content:encoded><![CDATA[
arXiv:2306.10656v5 Announce Type: replace 
Abstract: Virtual Human Generative Model (VHGM) is a generative model that approximates the joint probability over more than 2000 human healthcare-related attributes. This paper presents the core algorithm, VHGM-MAE, a masked autoencoder (MAE) tailored for handling high-dimensional, sparse healthcare data. VHGM-MAE tackles four key technical challenges: (1) heterogeneity of healthcare data types, (2) probability distribution modeling, (3) systematic missingness in the training dataset arising from multiple data sources, and (4) the high-dimensional, small-$n$-large-$p$ problem. To address these challenges, VHGM-MAE employs a likelihood-based approach to model distributions with heterogeneous types, a transformer-based MAE to capture complex dependencies among observed and missing attributes, and a novel training scheme that effectively leverages available samples with diverse missingness patterns to mitigate the small-n-large-p problem. Experimental results demonstrate that VHGM-MAE outperforms existing methods in both missing value imputation and synthetic data generation.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers</title>
<link>https://arxiv.org/abs/2307.13352</link>
<guid>https://arxiv.org/abs/2307.13352</guid>
<content:encoded><![CDATA[
arXiv:2307.13352v3 Announce Type: replace 
Abstract: Adversarial attacks pose a major challenge to distributed learning systems, prompting the development of numerous robust learning methods. However, most existing approaches suffer from the curse of dimensionality, i.e. the error increases with the number of model parameters. In this paper, we make a progress towards high dimensional problems, under arbitrary number of Byzantine attackers. The cornerstone of our design is a direct high dimensional semi-verified mean estimation method. The idea is to identify a subspace with large variance. The components of the mean value perpendicular to this subspace are estimated using corrupted gradient vectors uploaded from worker machines, while the components within this subspace are estimated using auxiliary dataset. As a result, a combination of large corrupted dataset and small clean dataset yields significantly better performance than using them separately. We then apply this method as the aggregator for distributed learning problems. The theoretical analysis shows that compared with existing solutions, our method gets rid of $\sqrt{d}$ dependence on the dimensionality, and achieves minimax optimal statistical rates. Numerical results validate our theory as well as the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Sample Complexity Bounds for Diffusion Model Training</title>
<link>https://arxiv.org/abs/2311.13745</link>
<guid>https://arxiv.org/abs/2311.13745</guid>
<content:encoded><![CDATA[
arXiv:2311.13745v4 Announce Type: replace 
Abstract: Diffusion models have become the most popular approach to deep generative modeling of images, largely due to their empirical performance and reliability. From a theoretical standpoint, a number of recent works have studied the iteration complexity of sampling, assuming access to an accurate diffusion model. In this work, we focus on understanding the sample complexity of training such a model; how many samples are needed to learn an accurate diffusion model using a sufficiently expressive neural network? Prior work showed bounds polynomial in the dimension, desired Total Variation error, and Wasserstein error. We show an exponential improvement in the dependence on Wasserstein error and depth, along with improved dependencies on other relevant parameters.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Instance-dependent Sample Complexity for Constrained Markov Decision Process</title>
<link>https://arxiv.org/abs/2402.16324</link>
<guid>https://arxiv.org/abs/2402.16324</guid>
<content:encoded><![CDATA[
arXiv:2402.16324v4 Announce Type: replace 
Abstract: We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\frac{1}{\Delta\cdot\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound, with $\Delta$ being a problem-dependent parameter, yet independent of $\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms of the dependency on $\epsilon$. To achieve this advance, we develop a new framework for analyzing CMDP problems. To be specific, our algorithm operates in the primal space and we resolve the primal LP for the CMDP problem at each period in an online manner, with adaptive remaining resource capacities. The key elements of our algorithm are: i) a characterization of the instance hardness via LP basis, ii) an eliminating procedure that identifies one optimal basis of the primal LP, and; iii) a resolving procedure that is adaptive to the remaining resources and sticks to the characterized optimal basis.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Federated Learning by Entropy-Based Client Selection</title>
<link>https://arxiv.org/abs/2411.01240</link>
<guid>https://arxiv.org/abs/2411.01240</guid>
<content:encoded><![CDATA[
arXiv:2411.01240v3 Announce Type: replace 
Abstract: Although deep learning has revolutionized domains such as natural language processing and computer vision, its dependence on centralized datasets raises serious privacy concerns. Federated learning addresses this issue by enabling multiple clients to collaboratively train a global deep learning model without compromising their data privacy. However, the performance of such a model degrades under label skew, where the label distribution differs between clients. To overcome this issue, a novel method called FedEntOpt is proposed. In each round, it selects clients to maximize the entropy of the aggregated label distribution, ensuring that the global model is exposed to data from all available classes. Extensive experiments on multiple benchmark datasets show that the proposed method outperforms several state-of-the-art algorithms by up to 6% in classification accuracy under standard settings regardless of the model size, while achieving gains of over 30% in scenarios with low participation rates and client dropout. In addition, FedEntOpt offers the flexibility to be combined with existing algorithms, enhancing their classification accuracy by more than 40%. Importantly, its performance remains unaffected even when differential privacy is applied.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRIMUS: Pretraining IMU Encoders with Multimodal Self-Supervision</title>
<link>https://arxiv.org/abs/2411.15127</link>
<guid>https://arxiv.org/abs/2411.15127</guid>
<content:encoded><![CDATA[
arXiv:2411.15127v3 Announce Type: replace 
Abstract: Sensing human motions through Inertial Measurement Units (IMUs) embedded in personal devices has enabled significant applications in health and wellness. Labeled IMU data is scarce, however, unlabeled or weakly labeled IMU data can be used to model human motions. For video or text modalities, the "pretrain and adapt" approach utilizes large volumes of unlabeled or weakly labeled data to build a strong feature extractor, followed by adaptation to specific tasks using limited labeled data. However, pretraining methods are poorly understood for IMU data, and pipelines are rarely evaluated on out-of-domain tasks. We propose PRIMUS: a method for PRetraining IMU encoderS that uses a novel pretraining objective that is empirically validated based on downstream performance on both in-domain and out-of-domain datasets. The PRIMUS objective effectively enhances downstream performance by combining self-supervision, multimodal, and nearest-neighbor supervision. With fewer than 500 labeled samples per class, PRIMUS improves test accuracy by up to 15%, compared to state-of-the-art baselines. To benefit the broader community, we have open-sourced our code at github.com/nokia-bell-labs/pretrained-imu-encoders.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Higher-Order Transformers With Kronecker-Structured Attention</title>
<link>https://arxiv.org/abs/2412.02919</link>
<guid>https://arxiv.org/abs/2412.02919</guid>
<content:encoded><![CDATA[
arXiv:2412.02919v2 Announce Type: replace 
Abstract: Modern datasets are increasingly high-dimensional and multiway, often represented as tensor-valued data with multi-indexed variables. While Transformers excel in sequence modeling and high-dimensional tasks, their direct application to multiway data is computationally prohibitive due to the quadratic cost of dot-product attention and the need to flatten inputs, which disrupts tensor structure and cross-dimensional dependencies. We propose the Higher-Order Transformer (HOT), a novel factorized attention framework that represents multiway attention as sums of Kronecker products or sums of mode-wise attention matrices. HOT efficiently captures dense and sparse relationships across dimensions while preserving tensor structure. Theoretically, HOT retains the expressiveness of full high-order attention and allows complexity control via factorization rank. Experiments on 2D and 3D datasets show that HOT achieves competitive performance in multivariate time series forecasting and image classification, with significantly reduced computational and memory costs. Visualizations of mode-wise attention matrices further reveal interpretable high-order dependencies learned by HOT, demonstrating its versatility for complex multiway data across diverse domains. The implementation of our proposed method is publicly available at https://github.com/s-omranpour/HOT.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sim-to-real supervised domain adaptation for radioisotope identification</title>
<link>https://arxiv.org/abs/2412.07069</link>
<guid>https://arxiv.org/abs/2412.07069</guid>
<content:encoded><![CDATA[
arXiv:2412.07069v4 Announce Type: replace 
Abstract: Machine learning has the potential to improve the speed and reliability of radioisotope identification using gamma spectroscopy. However, meticulously labeling an experimental dataset for training is often prohibitively expensive, while training models purely on synthetic data is risky due to the domain gap between simulated and experimental measurements. In this research, we demonstrate that supervised domain adaptation can substantially improve the performance of radioisotope identification models by transferring knowledge between synthetic and experimental data domains. We consider two domain adaptation scenarios: (1) a simulation-to-simulation adaptation, where we perform multi-label proportion estimation using simulated high-purity germanium detectors, and (2) a simulation-to-experimental adaptation, where we perform multi-class, single-label classification using measured spectra from handheld lanthanum bromide (LaBr) and sodium iodide (NaI) detectors. We begin by pretraining a spectral classifier on synthetic data using a custom transformer-based neural network. After subsequent fine-tuning on just 64 labeled experimental spectra, we achieve a test accuracy of 96% in the sim-to-real scenario with a LaBr detector, far surpassing a synthetic-only baseline model (75%) and a model trained from scratch (80%) on the same 64 spectra. Furthermore, we demonstrate that domain-adapted models learn more human-interpretable features than experiment-only baseline models. Overall, our results highlight the potential for supervised domain adaptation techniques to bridge the sim-to-real gap in radioisotope identification, enabling the development of accurate and explainable classifiers even in real-world scenarios where access to experimental data is limited.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Token-wise Feature Caching: Accelerating Diffusion Transformers with Dual Feature Caching</title>
<link>https://arxiv.org/abs/2412.18911</link>
<guid>https://arxiv.org/abs/2412.18911</guid>
<content:encoded><![CDATA[
arXiv:2412.18911v2 Announce Type: replace 
Abstract: Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. Among them, token-wise feature caching has been introduced to perform different caching ratios for different tokens in DiTs, aiming to skip the computation for unimportant tokens while still computing the important ones. In this paper, we propose to carefully check the effectiveness in token-wise feature caching with the following two questions: (1) Is it really necessary to compute the so-called "important" tokens in each step? (2) Are so-called important tokens really important? Surprisingly, this paper gives some counter-intuition answers, demonstrating that consistently computing the selected ``important tokens'' in all steps is not necessary. The selection of the so-called ``important tokens'' is often ineffective, and even sometimes shows inferior performance than random selection. Based on these observations, this paper introduces dual feature caching referred to as DuCa, which performs aggressive caching strategy and conservative caching strategy iteratively and selects the tokens for computing randomly. Extensive experimental results demonstrate the effectiveness of our method in DiT, PixArt, FLUX, and OpenSora, demonstrating significant improvements than the previous token-wise feature caching.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting the Performance of Black-box LLMs through Self-Queries</title>
<link>https://arxiv.org/abs/2501.01558</link>
<guid>https://arxiv.org/abs/2501.01558</guid>
<content:encoded><![CDATA[
arXiv:2501.01558v3 Announce Type: replace 
Abstract: As large language models (LLMs) are increasingly relied on in AI systems, predicting when they make mistakes is crucial. While a great deal of work in the field uses internal representations to interpret model behavior, these representations are inaccessible when given solely black-box access through an API. In this paper, we extract features of LLMs in a black-box manner by using follow-up prompts and taking the probabilities of different responses as representations to train reliable predictors of model behavior. We demonstrate that training a linear model on these low-dimensional representations produces reliable and generalizable predictors of model performance at the instance level (e.g., if a particular generation correctly answers a question). Remarkably, these can often outperform white-box linear predictors that operate over a model's hidden state or the full distribution over its vocabulary. In addition, we demonstrate that these extracted features can be used to evaluate more nuanced aspects of a language model's state. For instance, they can be used to distinguish between a clean version of GPT-4o-mini and a version that has been influenced via an adversarial system prompt that answers question-answering tasks incorrectly or introduces bugs into generated code. Furthermore, they can reliably distinguish between different model architectures and sizes, enabling the detection of misrepresented models provided through an API (e.g., identifying if GPT-3.5 is supplied instead of GPT-4o-mini).
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Environmental Feature Engineering and Statistical Validation for ML-Based Path Loss Prediction</title>
<link>https://arxiv.org/abs/2501.08306</link>
<guid>https://arxiv.org/abs/2501.08306</guid>
<content:encoded><![CDATA[
arXiv:2501.08306v4 Announce Type: replace 
Abstract: Wireless communications rely on path loss modeling, which is most effective when it includes the physical details of the propagation environment. Acquiring this data has historically been challenging, but geographic information systems data is becoming increasingly available with higher resolution and accuracy. Access to such details enables propagation models to more accurately predict coverage and account for interference in wireless deployments. Machine learning-based modeling can significantly support this effort, with feature based approaches allowing for accurate, efficient, and scalable propagation modeling. Building on previous work, we introduce an extended set of features that improves prediction accuracy while, most importantly, proving model generalization through rigorous statistical assessment and the use of test set holdouts.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Variance Reduction in Importance Sampling for Efficient DNN Training</title>
<link>https://arxiv.org/abs/2501.13296</link>
<guid>https://arxiv.org/abs/2501.13296</guid>
<content:encoded><![CDATA[
arXiv:2501.13296v2 Announce Type: replace 
Abstract: Importance sampling is widely used to improve the efficiency of deep neural network (DNN) training by reducing the variance of gradient estimators. However, efficiently assessing the variance reduction relative to uniform sampling remains challenging due to computational overhead. This paper proposes a method for estimating variance reduction during DNN training using only minibatches sampled under importance sampling. By leveraging the proposed method, the paper also proposes an effective minibatch size to enable automatic learning rate adjustment. An absolute metric to quantify the efficiency of importance sampling is also introduced as well as an algorithm for real-time estimation of importance scores based on moving gradient statistics. Theoretical analysis and experiments on benchmark datasets demonstrated that the proposed algorithm consistently reduces variance, improves training efficiency, and enhances model accuracy compared with current importance-sampling approaches while maintaining minimal computational overhead.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closed-Form Feedback-Free Learning with Forward Projection</title>
<link>https://arxiv.org/abs/2501.16476</link>
<guid>https://arxiv.org/abs/2501.16476</guid>
<content:encoded><![CDATA[
arXiv:2501.16476v3 Announce Type: replace 
Abstract: State-of-the-art methods for backpropagation-free learning employ local error feedback to direct iterative optimisation via gradient descent. In this study, we examine the more restrictive setting where retrograde communication from neuronal outputs is unavailable for pre-synaptic weight optimisation. To address this challenge, we propose Forward Projection (FP). This randomised closed-form training method requires only a single forward pass over the entire dataset for model fitting, without retrograde communication. Our method generates target values for pre-activation membrane potentials at each layer through randomised nonlinear projections of pre-synaptic inputs and the labels, thereby encoding information from both sources. Local loss functions are optimised over pre-synaptic inputs using closed-form regression, without feedback from neuronal outputs or downstream layers. Interpretability is a key advantage of FP training; membrane potentials of hidden neurons in FP-trained networks encode information which are interpretable layer-wise as label predictions. We demonstrate the effectiveness of FP across four biomedical datasets, comparing it with backpropagation and local learning techniques such as Forward-Forward training and Local Supervision in multi-layer perceptron and convolutional architectures. In some few-shot learning tasks, FP yielded more generalisable models than those optimised via backpropagation. In large-sample tasks, FP-based models achieve generalisation comparable to gradient descent-based local learning methods while requiring only a single forward propagation step, achieving significant speed up for training.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPAD: Inverse Prompt for AI Detection - A Robust and Interpretable LLM-Generated Text Detector</title>
<link>https://arxiv.org/abs/2502.15902</link>
<guid>https://arxiv.org/abs/2502.15902</guid>
<content:encoded><![CDATA[
arXiv:2502.15902v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have attained human-level fluency in text generation, which complicates the distinguishing between human-written and LLM-generated texts. This increases the risk of misuse and highlights the need for reliable detectors. Yet, existing detectors exhibit poor robustness on out-of-distribution (OOD) data and attacked data, which is critical for real-world scenarios. Also, they struggle to provide interpretable evidence to support their decisions, thus undermining the reliability. In light of these challenges, we propose IPAD (Inverse Prompt for AI Detection), a novel framework consisting of a Prompt Inverter that identifies predicted prompts that could have generated the input text, and two Distinguishers that examine the probability that the input texts align with the predicted prompts. Empirical evaluations demonstrate that IPAD outperforms the strongest baselines by 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on out-of-distribution data, and 5.48% (AUROC) on attacked data. IPAD also performs robustly on structured datasets. Furthermore, an interpretability assessment is conducted to illustrate that IPAD enhances the AI detection trustworthiness by allowing users to directly examine the decision-making evidence, which provides interpretable support for its state-of-the-art detection results.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near Optimal Decision Trees in a SPLIT Second</title>
<link>https://arxiv.org/abs/2502.15988</link>
<guid>https://arxiv.org/abs/2502.15988</guid>
<content:encoded><![CDATA[
arXiv:2502.15988v3 Announce Type: replace 
Abstract: Decision tree optimization is fundamental to interpretable machine learning. The most popular approach is to greedily search for the best feature at every decision point, which is fast but provably suboptimal. Recent approaches find the global optimum using branch and bound with dynamic programming, showing substantial improvements in accuracy and sparsity at great cost to scalability. An ideal solution would have the accuracy of an optimal method and the scalability of a greedy method. We introduce a family of algorithms called SPLIT (SParse Lookahead for Interpretable Trees) that moves us significantly forward in achieving this ideal balance. We demonstrate that not all sub-problems need to be solved to optimality to find high quality trees; greediness suffices near the leaves. Since each depth adds an exponential number of possible trees, this change makes our algorithms orders of magnitude faster than existing optimal methods, with negligible loss in performance. We extend this algorithm to allow scalable computation of sets of near-optimal trees (i.e., the Rashomon set).
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>1-Lipschitz Network Initialization for Certifiably Robust Classification Applications: A Decay Problem</title>
<link>https://arxiv.org/abs/2503.00240</link>
<guid>https://arxiv.org/abs/2503.00240</guid>
<content:encoded><![CDATA[
arXiv:2503.00240v2 Announce Type: replace 
Abstract: This paper discusses the weight parametrization of two standard 1-Lipschitz network architectures, the Almost-Orthogonal-Layers (AOL) and the SDP-based Lipschitz Layers (SLL). It examines their impact on initialization for deep 1-Lipschitz feedforward networks, and discusses underlying issues surrounding this initialization. These networks are mainly used in certifiably robust classification applications to combat adversarial attacks by limiting the impact of perturbations on the classification output. Exact and upper bounds for the parameterized weight variance were calculated assuming a standard Normal distribution initialization; additionally, an upper bound was computed assuming a Generalized Normal Distribution, generalizing the proof for Uniform, Laplace, and Normal distribution weight initializations. It is demonstrated that the weight variance holds no bearing on the output variance distribution and that only the dimension of the weight matrices matters. Additionally, this paper demonstrates that the weight initialization always causes deep 1-Lipschitz networks to decay to zero.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ElementaryNet: A Non-Strategic Neural Network for Predicting Human Behavior in Normal-Form Games</title>
<link>https://arxiv.org/abs/2503.05925</link>
<guid>https://arxiv.org/abs/2503.05925</guid>
<content:encoded><![CDATA[
arXiv:2503.05925v3 Announce Type: replace 
Abstract: Behavioral game theory models serve two purposes: yielding insights into how human decision-making works, and predicting how people would behave in novel strategic settings. A system called GameNet represents the state of the art for predicting human behavior in the setting of unrepeated simultaneous-move games, combining a simple "level-k" model of strategic reasoning with a complex neural network model of non-strategic "level-0" behavior. Although this reliance on well-established ideas from cognitive science ought to make GameNet interpretable, the flexibility of its level-0 model raises the possibility that it is able to emulate strategic reasoning. In this work, we prove that GameNet's level-0 model is indeed too general. We then introduce ElementaryNet, a novel neural network that is provably incapable of expressing strategic behavior. We show that these additional restrictions are empirically harmless, with ElementaryNet and GameNet having statistically indistinguishable performance. We then show how it is possible to derive insights about human behavior by varying ElementaryNet's features and interpreting its parameters, finding evidence of iterative reasoning, learning about the depth of this reasoning process, and showing the value of a rich level-0 specification.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Cross-domain Graph Learning: Progress and Future Directions</title>
<link>https://arxiv.org/abs/2503.11086</link>
<guid>https://arxiv.org/abs/2503.11086</guid>
<content:encoded><![CDATA[
arXiv:2503.11086v2 Announce Type: replace 
Abstract: Graph learning plays a vital role in mining and analyzing complex relationships within graph data and has been widely applied to real-world scenarios such as social, citation, and e-commerce networks. Foundation models in computer vision (CV) and natural language processing (NLP) have demonstrated remarkable cross-domain capabilities that are equally significant for graph data. However, existing graph learning approaches often struggle to generalize across domains. Motivated by recent advances in CV and NLP, cross-domain graph learning (CDGL) has gained renewed attention as a promising step toward realizing true graph foundation models. In this survey, we provide a comprehensive review and analysis of existing works on CDGL. We propose a new taxonomy that categorizes existing approaches according to the type of transferable knowledge learned across domains: structure-oriented, feature-oriented, and mixture-oriented. Based on this taxonomy, we systematically summarize representative methods in each category, discuss the key challenges and limitations of current studies, and outline promising directions for future research. A continuously updated collection of related works is available at: https://github.com/cshhzhao/Awesome-Cross-Domain-Graph-Learning.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proofs as Explanations: Short Certificates for Reliable Predictions</title>
<link>https://arxiv.org/abs/2504.08377</link>
<guid>https://arxiv.org/abs/2504.08377</guid>
<content:encoded><![CDATA[
arXiv:2504.08377v4 Announce Type: replace 
Abstract: We consider a model for explainable AI in which an explanation for a prediction $h(x)=y$ consists of a subset $S'$ of the training data (if it exists) such that all classifiers $h' \in H$ that make at most $b$ mistakes on $S'$ predict $h'(x)=y$. Such a set $S'$ serves as a proof that $x$ indeed has label $y$ under the assumption that (1) the target function $h^\star$ belongs to $H$, and (2) the set $S$ contains at most $b$ corrupted points. For example, if $b=0$ and $H$ is the family of linear classifiers in $\mathbb{R}^d$, and if $x$ lies inside the convex hull of the positive data points in $S$ (and hence every consistent linear classifier labels $x$ as positive), then Carath\'eodory's theorem states that $x$ lies inside the convex hull of $d+1$ of those points. So, a set $S'$ of size $d+1$ could be released as an explanation for a positive prediction, and would serve as a short proof of correctness of the prediction under the assumption of realizability.
  In this work, we consider this problem more generally, for general hypothesis classes $H$ and general values $b\geq 0$. We define the notion of the robust hollow star number of $H$ (which generalizes the standard hollow star number), and show that it precisely characterizes the worst-case size of the smallest certificate achievable, and analyze its size for natural classes. We also consider worst-case distributional bounds on certificate size, as well as distribution-dependent bounds that we show tightly control the sample size needed to get a certificate for any given test example. In particular, we define a notion of the certificate coefficient $\varepsilon_x$ of an example $x$ with respect to a data distribution $D$ and target function $h^\star$, and prove matching upper and lower bounds on sample size as a function of $\varepsilon_x$, $b$, and the VC dimension $d$ of $H$.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Uniform Class-Wise Coreset Selection for Vision Model Fine-tuning</title>
<link>https://arxiv.org/abs/2504.13234</link>
<guid>https://arxiv.org/abs/2504.13234</guid>
<content:encoded><![CDATA[
arXiv:2504.13234v2 Announce Type: replace 
Abstract: Coreset selection aims to identify a small yet highly informative subset of data, thereby enabling more efficient model training while reducing storage overhead. Recently, this capability has been leveraged to tackle the challenges of fine-tuning large foundation models, offering a direct pathway to their efficient and practical deployment. However, most existing methods are class-agnostic, causing them to overlook significant difficulty variations among classes. This leads them to disproportionately prune samples from either overly easy or hard classes, resulting in a suboptimal allocation of the data budget that ultimately degrades the final coreset performance. To address this limitation, we propose Non-Uniform Class-Wise Coreset Selection (NUCS), a novel framework that both integrates class-level and sample-level difficulty. We propose a robust metric for global class difficulty, quantified as the winsorized average of per-sample difficulty scores. Guided by this metric, our method performs a theoretically-grounded, non-uniform allocation of data selection budgets inter-class, while adaptively selecting samples intra-class with optimal difficulty ranges. Extensive experiments on a wide range of visual classification tasks demonstrate that NUCS consistently outperforms state-of-the-art methods across 10 diverse datasets and pre-trained models, achieving both superior accuracy and computational efficiency, highlighting the promise of non-uniform class-wise selection strategy for advancing the efficient fine-tuning of large foundation models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models</title>
<link>https://arxiv.org/abs/2505.08915</link>
<guid>https://arxiv.org/abs/2505.08915</guid>
<content:encoded><![CDATA[
arXiv:2505.08915v2 Announce Type: replace 
Abstract: Recent experiments have shown that training trajectories of multiple deep neural networks with different architectures, optimization algorithms, hyper-parameter settings, and regularization methods evolve on a remarkably low-dimensional "hyper-ribbon-like" manifold in the space of probability distributions. Inspired by the similarities in the training trajectories of deep networks and linear networks, we analytically characterize this phenomenon for the latter. We show, using tools in dynamical systems theory, that the geometry of this low-dimensional manifold is controlled by (i) the decay rate of the eigenvalues of the input correlation matrix of the training data, (ii) the relative scale of the ground-truth output to the weights at the beginning of training, and (iii) the number of steps of gradient descent. By analytically computing and bounding the contributions of these quantities, we characterize phase boundaries of the region where hyper-ribbons are to be expected. We also extend our analysis to kernel machines and linear models that are trained with stochastic gradient descent.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference Learning with Lie Detectors can Induce Honesty or Evasion</title>
<link>https://arxiv.org/abs/2505.13787</link>
<guid>https://arxiv.org/abs/2505.13787</guid>
<content:encoded><![CDATA[
arXiv:2505.13787v2 Announce Type: replace 
Abstract: As AI systems become more capable, deceptive behaviors can undermine evaluation and mislead users at deployment. Recent work has shown that lie detectors can accurately classify deceptive behavior, but they are not typically used in the training pipeline due to concerns around contamination and objective hacking. We examine these concerns by incorporating a lie detector into the labelling step of LLM post-training and evaluating whether the learned policy is genuinely more honest, or instead learns to fool the lie detector while remaining deceptive. Using DolusChat, a novel 65k-example dataset with paired truthful/deceptive responses, we identify three key factors that determine the honesty of learned policies: amount of exploration during preference learning, lie detector accuracy, and KL regularization strength. We find that preference learning with lie detectors and GRPO can lead to policies which evade lie detectors, with deception rates of over 85\%. However, if the lie detector true positive rate (TPR) or KL regularization is sufficiently high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO) consistently lead to deception rates under 25\% for realistic TPRs. Our results illustrate a more complex picture than previously assumed: depending on the context, lie-detector-enhanced training can be a powerful tool for scalable oversight, or a counterproductive method encouraging undetectable misalignment.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quartet: Native FP4 Training Can Be Optimal for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14669</link>
<guid>https://arxiv.org/abs/2505.14669</guid>
<content:encoded><![CDATA[
arXiv:2505.14669v3 Announce Type: replace 
Abstract: Training large language models (LLMs) models directly in low-precision offers a way to address computational costs by improving both throughput and energy efficiency. For those purposes, NVIDIA's recent Blackwell architecture facilitates very low-precision operations using FP4 variants. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we investigate hardware-supported FP4 training and introduce a new approach for accurate, end-to-end FP4 training with all the major computations (i.e., linear layers) in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across bit-widths and training setups. Guided by this investigation, we design an "optimal" technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for Blackwell, demonstrating that fully FP4-based training is a competitive alternative to FP16 half-precision and to FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayes optimal learning of attention-indexed models</title>
<link>https://arxiv.org/abs/2506.01582</link>
<guid>https://arxiv.org/abs/2506.01582</guid>
<content:encoded><![CDATA[
arXiv:2506.01582v2 Announce Type: replace 
Abstract: We introduce the attention-indexed model (AIM), a theoretical framework for analyzing learning in deep attention layers. Inspired by multi-index models, AIM captures how token-level outputs emerge from layered bilinear interactions over high-dimensional embeddings. Unlike prior tractable attention models, AIM allows full-width key and query matrices, aligning more closely with practical transformers. Using tools from statistical mechanics and random matrix theory, we derive closed-form predictions for Bayes-optimal generalization error and identify sharp phase transitions as a function of sample complexity, model width, and sequence length. We propose a matching approximate message passing algorithm and show that gradient descent can reach optimal performance. AIM offers a solvable playground for understanding learning in self-attention layers, that are key components of modern architectures.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.08062</link>
<guid>https://arxiv.org/abs/2506.08062</guid>
<content:encoded><![CDATA[
arXiv:2506.08062v2 Announce Type: replace 
Abstract: Multi-objective reinforcement learning (MORL) aims to optimize policies in the presence of conflicting objectives, where linear scalarization is commonly used to reduce vector-valued returns into scalar signals. While effective for certain preferences, this approach cannot capture fairness-oriented goals such as Nash social welfare or max-min fairness, which require nonlinear and non-additive trade-offs. Although several online algorithms have been proposed for specific fairness objectives, a unified approach for optimizing nonlinear welfare criteria in the offline setting-where learning must proceed from a fixed dataset-remains unexplored. In this work, we present FairDICE, the first offline MORL framework that directly optimizes nonlinear welfare objective. FairDICE leverages distribution correction estimation to jointly account for welfare maximization and distributional regularization, enabling stable and sample-efficient learning without requiring explicit preference weights or exhaustive weight search. Across multiple offline benchmarks, FairDICE demonstrates strong fairness-aware performance compared to existing baselines.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space</title>
<link>https://arxiv.org/abs/2506.08270</link>
<guid>https://arxiv.org/abs/2506.08270</guid>
<content:encoded><![CDATA[
arXiv:2506.08270v3 Announce Type: replace 
Abstract: Designing neural networks typically relies on manual trial and error or a neural architecture search (NAS) followed by weight training. The former is time-consuming and labor-intensive, while the latter often discretizes architecture search and weight optimization. In this paper, we propose a fundamentally different approach that simultaneously optimizes both the architecture and the weights of a neural network. Our framework first trains a universal multi-scale autoencoder that embeds both architectural and parametric information into a continuous latent space, where functionally similar neural networks are mapped closer together. Given a dataset, we then randomly initialize a point in the embedding space and update it via gradient descent to obtain the optimal neural network, jointly optimizing its structure and weights. The optimization process incorporates sparsity and compactness penalties to promote efficient models. Experiments on synthetic regression tasks demonstrate that our method effectively discovers sparse and compact neural networks with strong performance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ODE$_t$(ODE$_l$): Shortcutting the Time and the Length in Diffusion and Flow Models for Faster Sampling</title>
<link>https://arxiv.org/abs/2506.21714</link>
<guid>https://arxiv.org/abs/2506.21714</guid>
<content:encoded><![CDATA[
arXiv:2506.21714v3 Announce Type: replace 
Abstract: Continuous normalizing flows (CNFs) and diffusion models (DMs) generate high-quality data from a noise distribution. However, their sampling process demands multiple iterations to solve an ordinary differential equation (ODE) with high computational complexity. State-of-the-art methods focus on reducing the number of discrete time steps during sampling to improve efficiency. In this work, we explore a complementary direction in which the quality-complexity tradeoff can also be controlled in terms of the neural network length. We achieve this by rewiring the blocks in the transformer-based architecture to solve an inner discretized ODE w.r.t. its depth. Then, we apply a length consistency term during flow matching training, and as a result, the sampling can be performed with an arbitrary number of time steps and transformer blocks. Unlike others, our ODE$_t$(ODE$_l$) approach is solver-agnostic in time dimension and reduces both latency and, importantly, memory usage. CelebA-HQ and ImageNet generation experiments show a latency reduction of up to $2\times$ in the most efficient sampling mode, and FID improvement of up to $2.8$ points for high-quality sampling when applied to prior methods. We open-source our code and checkpoints at github.com/gudovskiy/odelt.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptScale: Probabilistic Optimality for Inference-time Scaling</title>
<link>https://arxiv.org/abs/2506.22376</link>
<guid>https://arxiv.org/abs/2506.22376</guid>
<content:encoded><![CDATA[
arXiv:2506.22376v3 Announce Type: replace 
Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-N selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning. The source code is publicly available at https://github.com/Albertwyk/OptScale.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning</title>
<link>https://arxiv.org/abs/2507.00965</link>
<guid>https://arxiv.org/abs/2507.00965</guid>
<content:encoded><![CDATA[
arXiv:2507.00965v2 Announce Type: replace 
Abstract: Many machine learning tasks can benefit from external knowledge. Large knowledge graphs store such knowledge, and embedding methods can be used to distill it into ready-to-use vector representations for downstream applications. For this purpose, current models have however two limitations: they are primarily optimized for link prediction, via local contrastive learning, and their application to the largest graphs requires significant engineering effort due to GPU memory limits. To address these, we introduce SEPAL: a Scalable Embedding Propagation ALgorithm for large knowledge graphs designed to produce high-quality embeddings for downstream tasks at scale. The key idea of SEPAL is to ensure global embedding consistency by optimizing embeddings only on a small core of entities, and then propagating them to the rest of the graph with message passing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream machine learning tasks. Our results show that SEPAL significantly outperforms previous methods on downstream tasks. In addition, SEPAL scales up its base embedding model, enabling fitting huge knowledge graphs on commodity hardware.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework</title>
<link>https://arxiv.org/abs/2508.03989</link>
<guid>https://arxiv.org/abs/2508.03989</guid>
<content:encoded><![CDATA[
arXiv:2508.03989v2 Announce Type: replace 
Abstract: User-controllable privacy is important in modern sensing systems, as privacy preferences can vary significantly from person to person and may evolve over time. This is especially relevant in devices equipped with Inertial Measurement Unit (IMU) sensors, such as smartphones and wearables, which continuously collect rich time-series data that can inadvertently expose sensitive user behaviors. While prior work has proposed privacy-preserving methods for sensor data, most rely on static, predefined privacy labels or require large quantities of private training data, limiting their adaptability and user agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable, few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify and modify their privacy preferences by categorizing activities as sensitive (black-listed), non-sensitive (white-listed), or neutral (gray-listed). Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU sensor data with natural language activity descriptions in a shared embedding space, enabling few-shot detection of sensitive activities. When a privacy-sensitive activity is identified, the system uses a language-guided activity sanitizer and a motion generation module (IMU-GPT) to transform the original data into a privacy-compliant version that semantically resembles a non-sensitive activity. We evaluate PrivCLIP on multiple human activity recognition datasets and demonstrate that it significantly outperforms baseline methods in terms of both privacy protection and data utility.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Squeezed Diffusion Models</title>
<link>https://arxiv.org/abs/2508.14871</link>
<guid>https://arxiv.org/abs/2508.14871</guid>
<content:encoded><![CDATA[
arXiv:2508.14871v2 Announce Type: replace 
Abstract: Diffusion models typically inject isotropic Gaussian noise, disregarding structure in the data. Motivated by the way quantum squeezed states redistribute uncertainty according to the Heisenberg uncertainty principle, we introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically along the principal component of the training distribution. As squeezing enhances the signal-to-noise ratio in physics, we hypothesize that scaling noise in a data-dependent manner can better assist diffusion models in learning important data features. We study two configurations: (i) a Heisenberg diffusion model that compensates the scaling on the principal axis with inverse scaling on orthogonal directions and (ii) a standard SDM variant that scales only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64, mild antisqueezing - i.e. increasing variance on the principal axis - consistently improves FID by up to 15% and shifts the precision-recall frontier toward higher recall. Our results demonstrate that simple, data-aware noise shaping can deliver robust generative gains without architectural changes.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference Robustness for DPO with Applications to Public Health</title>
<link>https://arxiv.org/abs/2509.02709</link>
<guid>https://arxiv.org/abs/2509.02709</guid>
<content:encoded><![CDATA[
arXiv:2509.02709v2 Announce Type: replace 
Abstract: We study an LLM fine-tuning task for designing reward functions for sequential resource allocation problems in public health, guided by human preferences expressed in natural language. This setting presents a challenging testbed for alignment due to complex and ambiguous objectives and limited data availability. We propose DPO-PRO, a robust fine-tuning algorithm based on Direct Preference Optimization (DPO), which accounts for uncertainty in the preference distribution using a lightweight Distributionally Robust Optimization (DRO) formulation. Unlike prior DRO-based DPO methods, DPO-PRO is significantly less conservative. We evaluate DPO-PRO on a real-world maternal mobile health program operated by the non-profit organization ARMMAN, as well as on standard alignment benchmarks. Experimental results demonstrate that our method consistently improves robustness to noisy preference signals compared to existing DPO variants. Moreover, DPO-PRO achieves comparable performance to prior self-reflection-based baseline for reward function design, while requiring significantly lower inference-time cost.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Formal Verification of Local Robustness of a Classification Algorithm for a Spatial Use Case</title>
<link>https://arxiv.org/abs/2509.03948</link>
<guid>https://arxiv.org/abs/2509.03948</guid>
<content:encoded><![CDATA[
arXiv:2509.03948v2 Announce Type: replace 
Abstract: Failures in satellite components are costly and challenging to address, often requiring significant human and material resources. Embedding a hybrid AI-based system for fault detection directly in the satellite can greatly reduce this burden by allowing earlier detection. However, such systems must operate with extremely high reliability. To ensure this level of dependability, we employ the formal verification tool Marabou to verify the local robustness of the neural network models used in the AI-based algorithm. This tool allows us to quantify how much a model's input can be perturbed before its output behavior becomes unstable, thereby improving trustworthiness with respect to its performance under uncertainty.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting (Un)Fairness in Recourse by Minimizing Worst-Case Social Burden</title>
<link>https://arxiv.org/abs/2509.04128</link>
<guid>https://arxiv.org/abs/2509.04128</guid>
<content:encoded><![CDATA[
arXiv:2509.04128v3 Announce Type: replace 
Abstract: Machine learning based predictions are increasingly used in sensitive decision-making applications that directly affect our lives. This has led to extensive research into ensuring the fairness of classifiers. Beyond just fair classification, emerging legislation now mandates that when a classifier delivers a negative decision, it must also offer actionable steps an individual can take to reverse that outcome. This concept is known as algorithmic recourse. Nevertheless, many researchers have expressed concerns about the fairness guarantees within the recourse process itself. In this work, we provide a holistic theoretical characterization of unfairness in algorithmic recourse, formally linking fairness guarantees in recourse and classification, and highlighting limitations of the standard equal cost paradigm. We then introduce a novel fairness framework based on social burden, along with a practical algorithm (MISOB), broadly applicable under real-world conditions. Empirical results on real-world datasets show that MISOB reduces the social burden across all groups without compromising overall classifier accuracy.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Decoding Methods for Language Models on Encrypted Data</title>
<link>https://arxiv.org/abs/2509.08383</link>
<guid>https://arxiv.org/abs/2509.08383</guid>
<content:encoded><![CDATA[
arXiv:2509.08383v2 Announce Type: replace 
Abstract: Large language models (LLMs) power modern AI applications, but processing sensitive data on untrusted servers raises privacy concerns. Homomorphic encryption (HE) enables computation on encrypted data for secure inference. However, neural text generation requires decoding methods like argmax and sampling, which are non-polynomial and thus computationally expensive under encryption, creating a significant performance bottleneck. We introduce cutmax, an HE-friendly argmax algorithm that reduces ciphertext operations compared to prior methods, enabling practical greedy decoding under encryption. We also propose the first HE-compatible nucleus (top-p) sampling method, leveraging cutmax for efficient stochastic decoding with provable privacy guarantees. Both techniques are polynomial, supporting efficient inference in privacy-preserving settings. Moreover, their differentiability facilitates gradient-based sequence-level optimization as a polynomial alternative to straight-through estimators. We further provide strong theoretical guarantees for cutmax, proving its convergence via exponential amplification of the gap ratio between the maximum and runner-up elements. Evaluations on realistic LLM outputs show latency reductions of 24x-35x over baselines, advancing secure text generation.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Learning for Anomaly Detection in Tabular Data</title>
<link>https://arxiv.org/abs/2509.09030</link>
<guid>https://arxiv.org/abs/2509.09030</guid>
<content:encoded><![CDATA[
arXiv:2509.09030v2 Announce Type: replace 
Abstract: Anomaly detection is critical in domains such as cybersecurity and finance, especially when working with large-scale tabular data. Yet, unsupervised anomaly detection-where no labeled anomalies are available-remains challenging because traditional deep learning methods model a single global distribution, assuming all samples follow the same behavior. In contrast, real-world data often contain heterogeneous contexts (e.g., different users, accounts, or devices), where globally rare events may be normal within specific conditions. We introduce a contextual learning framework that explicitly models how normal behavior varies across contexts by learning conditional data distributions $P(\mathbf{Y} \mid \mathbf{C})$ rather than a global joint distribution $P(\mathbf{X})$. The framework encompasses (1) a probabilistic formulation for context-conditioned learning, (2) a principled bilevel optimization strategy for automatically selecting informative context features using early validation loss, and (3) theoretical grounding through variance decomposition and discriminative learning principles. We instantiate this framework using a novel conditional Wasserstein autoencoder as a simple yet effective model for tabular anomaly detection. Extensive experiments across eight benchmark datasets demonstrate that contextual learning consistently outperforms global approaches-even when the optimal context is not intuitively obvious-establishing a new foundation for anomaly detection in heterogeneous tabular data.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrosynthesis Planning via Worst-path Policy Optimisation in Tree-structured MDPs</title>
<link>https://arxiv.org/abs/2509.10504</link>
<guid>https://arxiv.org/abs/2509.10504</guid>
<content:encoded><![CDATA[
arXiv:2509.10504v2 Announce Type: replace 
Abstract: Retrosynthesis planning aims to decompose target molecules into available building blocks, forming a synthetic tree where each internal node represents an intermediate compound and each leaf ideally corresponds to a purchasable reactant. However, this tree becomes invalid if any leaf node is not a valid building block, making the planning process vulnerable to the "weakest link" in the synthetic route. Existing methods often optimise for average performance across branches, failing to account for this worst-case sensitivity. In this paper, we reframe retrosynthesis as a worst-path optimisation problem within tree-structured Markov Decision Processes (MDPs). We prove that this formulation admits a unique optimal solution and provides monotonic improvement guarantees. Building on this insight, we introduce Interactive Retrosynthesis Planning (InterRetro), a method that interacts with the tree MDP, learns a value function for worst-path outcomes, and improves its policy through self-imitation, preferentially reinforcing past decisions with high estimated advantage. Empirically, InterRetro achieves state-of-the-art results - solving 100% of targets on the Retro*-190 benchmark, shortening synthetic routes by 4.9%, and achieving promising performance using only 10% of the training data.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning</title>
<link>https://arxiv.org/abs/2509.13763</link>
<guid>https://arxiv.org/abs/2509.13763</guid>
<content:encoded><![CDATA[
arXiv:2509.13763v2 Announce Type: replace 
Abstract: Multi-view unsupervised feature selection (MUFS) has recently received increasing attention for its promising ability in dimensionality reduction on multi-view unlabeled data. Existing MUFS methods typically select discriminative features by capturing correlations between features and clustering labels. However, an important yet underexplored question remains: \textit{Are such correlations sufficiently reliable to guide feature selection?} In this paper, we analyze MUFS from a causal perspective by introducing a novel structural causal model, which reveals that existing methods may select irrelevant features because they overlook spurious correlations caused by confounders. Building on this causal perspective, we propose a novel MUFS method called CAusal multi-view Unsupervised feature Selection leArning (CAUSA). Specifically, we first employ a generalized unsupervised spectral regression model that identifies informative features by capturing dependencies between features and consensus clustering labels. We then introduce a causal regularization module that can adaptively separate confounders from multi-view data and simultaneously learn view-shared sample weights to balance confounder distributions, thereby mitigating spurious correlations. Thereafter, integrating both into a unified learning framework enables CAUSA to select causally informative features. Comprehensive experiments demonstrate that CAUSA outperforms several state-of-the-art methods. To our knowledge, this is the first in-depth study of causal multi-view feature selection in the unsupervised setting.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A More Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models</title>
<link>https://arxiv.org/abs/2509.19465</link>
<guid>https://arxiv.org/abs/2509.19465</guid>
<content:encoded><![CDATA[
arXiv:2509.19465v3 Announce Type: replace 
Abstract: Cross-frequency transfer learning (CFTL) has emerged as a popular framework for curating large-scale time series datasets to pre-train foundation forecasting models (FFMs). Although CFTL has shown promise, current benchmarking practices fall short of accurately assessing its performance. This shortcoming stems from many factors: an over-reliance on small-scale evaluation datasets; inadequate treatment of sample size when computing summary statistics; reporting of suboptimal statistical models; and failing to account for non-negligible risks of overlap between pre-training and test datasets. To address these limitations, we introduce a unified reimplementation of widely-adopted neural forecasting networks, adapting them for the CFTL setup; we pre-train only on proprietary and synthetic data, being careful to prevent test leakage; and we evaluate on 15 large, diverse public forecast competition datasets. Our empirical analysis reveals that statistical models' accuracy is frequently underreported. Notably, we confirm that statistical models and their ensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and by more than 20% MASE, across datasets. However, we also find that synthetic dataset pre-training does improve the accuracy of a FFM by 7% percent.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geospatial Machine Learning Libraries</title>
<link>https://arxiv.org/abs/2510.02572</link>
<guid>https://arxiv.org/abs/2510.02572</guid>
<content:encoded><![CDATA[
arXiv:2510.02572v2 Announce Type: replace 
Abstract: Recent advances in machine learning have been supported by the emergence of domain-specific software libraries, enabling streamlined workflows and increased reproducibility. For geospatial machine learning (GeoML), the availability of Earth observation data has outpaced the development of domain libraries to handle its unique challenges, such as varying spatial resolutions, spectral properties, temporal cadence, data coverage, coordinate systems, and file formats. This chapter presents a comprehensive overview of GeoML libraries, analyzing their evolution, core functionalities, and the current ecosystem. It also introduces popular GeoML libraries such as TorchGeo, eo-learn, and Raster Vision, detailing their architecture, supported data types, and integration with ML frameworks. Additionally, it discusses common methodologies for data preprocessing, spatial--temporal joins, benchmarking, and the use of pretrained models. Through a case study in crop type mapping, it demonstrates practical applications of these tools. Best practices in software design, licensing, and testing are highlighted, along with open challenges and future directions, particularly the rise of foundation models and the need for governance in open-source geospatial software. Our aim is to guide practitioners, developers, and researchers in navigating and contributing to the rapidly evolving GeoML landscape.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields</title>
<link>https://arxiv.org/abs/2510.04325</link>
<guid>https://arxiv.org/abs/2510.04325</guid>
<content:encoded><![CDATA[
arXiv:2510.04325v2 Announce Type: replace 
Abstract: The accurate prediction of flow fields around airfoils is crucial for aerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models are effective but computationally expensive, thus inspiring the development of surrogate models to enable quicker predictions. These surrogate models can be based on deep learning architectures, such as Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion models have shown significant promise in predicting complex flow fields. In this work, we propose FoilDiff, a diffusion-based surrogate model with a hybrid-backbone denoising network. This hybrid design combines the power of convolutional feature extraction and transformer-based global attention to generate more adaptable and accurate representations of flow structures. FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling to optimise the efficiency of the sampling process at no additional cost to model generalisation. We used encoded representations of Reynolds number, angle of attack, and airfoil geometry to define the input space for generalisation across a wide range of aerodynamic conditions. When evaluated against state-of-the-art models, FoilDiff shows significant performance improvements, with mean prediction errors reducing by up to 85\% on the same datasets. The results have demonstrated that FoilDiff can provide both more accurate predictions and better-calibrated predictive uncertainty than existing diffusion-based models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WARP-LUTs - Walsh-Assisted Relaxation for Probabilistic Look Up Tables</title>
<link>https://arxiv.org/abs/2510.15655</link>
<guid>https://arxiv.org/abs/2510.15655</guid>
<content:encoded><![CDATA[
arXiv:2510.15655v2 Announce Type: replace 
Abstract: Fast and efficient machine learning is of growing interest to the scientific community and has spurred significant research into novel model architectures and hardware-aware design. Recent hard? and software co-design approaches have demonstrated impressive results with entirely multiplication-free models. Differentiable Logic Gate Networks (DLGNs), for instance, provide a gradient-based framework for learning optimal combinations of low-level logic gates, setting state-of-the-art trade-offs between accuracy, resource usage, and latency. However, these models suffer from high computational cost during training and do not generalize well to logic blocks with more inputs. In this work, we introduce Walsh-Assisted Relaxation for Probabilistic Look-Up Tables (WARP-LUTs) - a novel gradient-based method that efficiently learns combinations of logic gates with substantially fewer trainable parameters. We demonstrate that WARP-LUTs achieve significantly faster convergence on CIFAR-10 compared to DLGNs, while maintaining comparable accuracy. Furthermore, our approach suggests potential for extension to higher-input logic blocks, motivating future research on extremely efficient deployment on modern FPGAs and its real-time science applications.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimality and NP-Hardness of Transformers in Learning Markovian Dynamical Functions</title>
<link>https://arxiv.org/abs/2510.18638</link>
<guid>https://arxiv.org/abs/2510.18638</guid>
<content:encoded><![CDATA[
arXiv:2510.18638v2 Announce Type: replace 
Abstract: Transformer architectures can solve unseen tasks based on input-output pairs in a given prompt due to in-context learning (ICL). Existing theoretical studies on ICL have mainly focused on linear regression tasks, often with i.i.d. inputs. To understand how transformers express ICL when modeling dynamics-driven functions, we investigate Markovian function learning through a structured ICL setup, where we characterize the loss landscape to reveal underlying optimization behaviors. Specifically, we (1) provide the closed-form expression of the global minimizer (in an enlarged parameter space) for a single-layer linear self-attention (LSA) model; (2) prove that recovering transformer parameters that realize the optimal solution is NP-hard in general, revealing a fundamental limitation of one-layer LSA in representing structured dynamical functions; and (3) supply a novel interpretation of a multilayer LSA as performing preconditioned gradient descent to optimize multiple objectives beyond the square loss. These theoretical results are numerically validated using simplified transformers.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics</title>
<link>https://arxiv.org/abs/2510.26324</link>
<guid>https://arxiv.org/abs/2510.26324</guid>
<content:encoded><![CDATA[
arXiv:2510.26324v2 Announce Type: replace 
Abstract: Given a noisy linear measurement $y = Ax + \xi$ of a distribution $p(x)$, and a good approximation to the prior $p(x)$, when can we sample from the posterior $p(x \mid y)$? Posterior sampling provides an accurate and fair framework for tasks such as inpainting, deblurring, and MRI reconstruction, and several heuristics attempt to approximate it. Unfortunately, approximate posterior sampling is computationally intractable in general.
  To sidestep this hardness, we focus on (local or global) log-concave distributions $p(x)$. In this regime, Langevin dynamics yields posterior samples when the exact scores of $p(x)$ are available, but it is brittle to score--estimation error, requiring an MGF bound (sub-exponential error). By contrast, in the unconditional setting, diffusion models succeed with only an $L^2$ bound on the score error. We prove that combining diffusion models with an annealed variant of Langevin dynamics achieves conditional sampling in polynomial time using merely an $L^4$ bound on the score error.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clone Deterministic 3D Worlds</title>
<link>https://arxiv.org/abs/2510.26782</link>
<guid>https://arxiv.org/abs/2510.26782</guid>
<content:encoded><![CDATA[
arXiv:2510.26782v2 Announce Type: replace 
Abstract: A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future physical state of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. However, existing world models often focus on random generation of open worlds, but neglect the need for high-fidelity modeling of deterministic scenarios (such as fixed-map mazes and static space robot navigation). In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone a deterministic 3D world. 1) Through diagnostic experiment, we quantitatively demonstrate that high-fidelity cloning is feasible and the primary bottleneck for long-horizon fidelity is the geometric structure of the latent representation, not the dynamics model itself. 2) Building on this insight, we show that applying temporal contrastive learning principle as a geometric regularization can effectively curate a latent space that better reflects the underlying physical state manifold, demonstrating that contrastive constraints can serve as a powerful inductive bias for stable world modeling; we call this approach Geometrically-Regularized World Models (GRWM). At its core is a lightweight geometric regularization module that can be seamlessly integrated into standard autoencoders, reshaping their latent space to provide a stable foundation for effective dynamics modeling. By focusing on representation quality, GRWM offers a simple yet powerful pipeline for improving world model fidelity.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering EV Charging Site Archetypes Through Few Shot Forecasting: The First U.S.-Wide Study</title>
<link>https://arxiv.org/abs/2510.26910</link>
<guid>https://arxiv.org/abs/2510.26910</guid>
<content:encoded><![CDATA[
arXiv:2510.26910v2 Announce Type: replace 
Abstract: The decarbonization of transportation relies on the widespread adoption of electric vehicles (EVs), which requires an accurate understanding of charging behavior to ensure cost-effective, grid-resilient infrastructure. Existing work is constrained by small-scale datasets, simple proximity-based modeling of temporal dependencies, and weak generalization to sites with limited operational history. To overcome these limitations, this work proposes a framework that integrates clustering with few-shot forecasting to uncover site archetypes using a novel large-scale dataset of charging demand. The results demonstrate that archetype-specific expert models outperform global baselines in forecasting demand at unseen sites. By establishing forecast performance as a basis for infrastructure segmentation, we generate actionable insights that enable operators to lower costs, optimize energy and pricing strategies, and support grid resilience critical to climate goals.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations</title>
<link>https://arxiv.org/abs/2511.00043</link>
<guid>https://arxiv.org/abs/2511.00043</guid>
<content:encoded><![CDATA[
arXiv:2511.00043v2 Announce Type: replace 
Abstract: In this study, we present and validate the predictive capability of the Physics-Informed Neural Networks (PINNs) methodology for solving a variety of engineering and biological dynamical systems governed by ordinary differential equations (ODEs). While traditional numerical methods a re effective for many ODEs, they often struggle to achieve convergence in problems involving high stiffness, shocks, irregular domains, singular perturbations, high dimensions, or boundary discontinuities. Alternatively, PINNs offer a powerful approach for handling challenging numerical scenarios. In this study, classical ODE problems are employed as controlled testbeds to systematically evaluate the accuracy, training efficiency, and generalization capability under controlled conditions of the PINNs framework. Although not a universal solution, PINNs can achieve superior results by embedding physical laws directly into the learning process. We first analyze the existence and uniqueness properties of several benchmark problems and subsequently validate the PINNs methodology on these model systems. Our results demonstrate that for complex problems to converge to correct solutions, the loss function components data loss, initial condition loss, and residual loss must be appropriately balanced through careful weighting. We further establish that systematic tuning of hyperparameters, including network depth, layer width, activation functions, learning rate, optimization algorithms, w eight initialization schemes, and collocation point sampling, plays a crucial role in achieving accurate solutions. Additionally, embedding prior knowledge and imposing hard constraints on the network architecture, without loss the generality of the ODE system, significantly enhances the predictive capability of PINNs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks</title>
<link>https://arxiv.org/abs/2511.00044</link>
<guid>https://arxiv.org/abs/2511.00044</guid>
<content:encoded><![CDATA[
arXiv:2511.00044v2 Announce Type: replace 
Abstract: Physical Neural Networks (PNN) are promising platforms for next-generation computing systems. However, recent advances in digital neural network performance are largely driven by the rapid growth in the number of trainable parameters and, so far, demonstrated PNNs are lagging behind by several orders of magnitude in terms of scale. This mirrors size and performance constraints found in early digital neural networks. In that period, efficient reuse of parameters contributed to the development of parameter-efficient architectures such as convolutional neural networks.
  In this work, we numerically investigate hardware-friendly weight-tying for PNNs. Crucially, with many PNN systems, there is a time-scale separation between the fast dynamic active elements of the forward pass and the only slowly trainable elements implementing weights and biases. With this in mind,we propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net) architecture, which employs a simple layer-by-layer time-multiplexing scheme to increase the effective network depth and efficiently use the number of parameters. We only require the addition of fast switches for existing PNNs. We validate ReLaX-Nets via numerical experiments on image classification and natural language processing tasks. Our results show that ReLaX-Net improves computational performance with only minor modifications to a conventional PNN. We observe a favorable scaling, where ReLaX-Nets exceed the performance of equivalent traditional RNNs or DNNs with the same number of parameters.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Real-Time Gas Crossover Prediction in PEM Electrolyzers: First Application with Multi-Membrane Validation</title>
<link>https://arxiv.org/abs/2511.05879</link>
<guid>https://arxiv.org/abs/2511.05879</guid>
<content:encoded><![CDATA[
arXiv:2511.05879v2 Announce Type: replace 
Abstract: Green hydrogen production via polymer electrolyte membrane (PEM) water electrolysis is pivotal for energy transition, yet hydrogen crossover through membranes threatens safety and economic viability-approaching explosive limits (4 mol% H$_2$ in O$_2$) while reducing Faradaic efficiency by 2.5%. Current physics-based models require extensive calibration and computational resources that preclude real-time implementation, while purely data-driven approaches fail to extrapolate beyond training conditions-critical for dynamic electrolyzer operation. Here we present the first application of physics-informed neural networks (PINNs) for hydrogen crossover prediction, integrating mass conservation, Fick's diffusion law, and Henry's solubility law within a compact architecture (17,793 parameters). Validated across six membranes under industrially relevant conditions (0.05-5.0 A/cm$^2$, 1-200 bar, 25-85{\deg}C), our PINN achieves exceptional accuracy (R$^{2}$ = 99.84% $\pm$ 0.15\%, RMSE = 0.0932% $\pm$ 0.0438%) based on five-fold cross-validation, with sub-millisecond inference times suitable for real-time control. Remarkably, the model maintains R$^2$ > 86% when predicting crossover at pressures 2.5x beyond training range-substantially outperforming pure neural networks (R$^2$ = 43.4%). The hardware-agnostic deployment, from desktop CPUs to edge devices (Raspberry Pi 4), enables distributed safety monitoring essential for gigawatt-scale installations. By bridging physical rigor and computational efficiency, this work establishes a new paradigm for real-time electrolyzer monitoring, accelerating deployment of safe, efficient green hydrogen infrastructure crucial for net-zero emissions targets.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource Efficient Sleep Staging via Multi-Level Masking and Prompt Learning</title>
<link>https://arxiv.org/abs/2511.06785</link>
<guid>https://arxiv.org/abs/2511.06785</guid>
<content:encoded><![CDATA[
arXiv:2511.06785v2 Announce Type: replace 
Abstract: Automatic sleep staging plays a vital role in assessing sleep quality and diagnosing sleep disorders. Most existing methods rely heavily on long and continuous EEG recordings, which poses significant challenges for data acquisition in resource-constrained systems, such as wearable or home-based monitoring systems. In this paper, we propose the task of resource-efficient sleep staging, which aims to reduce the amount of signal collected per sleep epoch while maintaining reliable classification performance. To solve this task, we adopt the masking and prompt learning strategy and propose a novel framework called Mask-Aware Sleep Staging (MASS). Specifically, we design a multi-level masking strategy to promote effective feature modeling under partial and irregular observations. To mitigate the loss of contextual information introduced by masking, we further propose a hierarchical prompt learning mechanism that aggregates unmasked data into a global prompt, serving as a semantic anchor for guiding both patch-level and epoch-level feature modeling. MASS is evaluated on four datasets, demonstrating state-of-the-art performance, especially when the amount of data is very limited. This result highlights its potential for efficient and scalable deployment in real-world low-resource sleep monitoring environments.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MI-to-Mid Distilled Compression (M2M-DC): An Hybrid-Information-Guided-Block Pruning with Progressive Inner Slicing Approach to Model Compression</title>
<link>https://arxiv.org/abs/2511.06842</link>
<guid>https://arxiv.org/abs/2511.06842</guid>
<content:encoded><![CDATA[
arXiv:2511.06842v2 Announce Type: replace 
Abstract: We introduce MI-to-Mid Distilled Compression (M2M-DC), a two-scale, shape-safe compression framework that interleaves information-guided block pruning with progressive inner slicing and staged knowledge distillation (KD). First, M2M-DC ranks residual (or inverted-residual) blocks by a label-aware mutual information (MI) signal and removes the least informative units (structured prune-after-training). It then alternates short KD phases with stage-coherent, residual-safe channel slicing: (i) stage "planes" (co-slicing conv2 out-channels with the downsample path and next-stage inputs), and (ii) an optional mid-channel trim (conv1 out / bn1 / conv2 in). This targets complementary redundancy, whole computational motifs and within-stage width while preserving residual shape invariants. On CIFAR-100, M2M-DC yields a clean accuracy-compute frontier. For ResNet-18, we obtain 85.46% Top-1 with 3.09M parameters and 0.0139 GMacs (72% params, 63% GMacs vs. teacher; mean final 85.29% over three seeds). For ResNet-34, we reach 85.02% Top-1 with 5.46M params and 0.0195 GMacs (74% / 74% vs. teacher; mean final 84.62%). Extending to inverted-residuals, MobileNetV2 achieves a mean final 68.54% Top-1 at 1.71M params (27%) and 0.0186 conv GMacs (24%), improving over the teacher's 66.03% by +2.5 points across three seeds. Because M2M-DC exposes only a thin, architecture-aware interface (blocks, stages, and down sample/skip wiring), it generalizes across residual CNNs and extends to inverted-residual families with minor legalization rules. The result is a compact, practical recipe for deployment-ready models that match or surpass teacher accuracy at a fraction of the compute.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SERL: Self-Examining Reinforcement Learning on Open-Domain</title>
<link>https://arxiv.org/abs/2511.07922</link>
<guid>https://arxiv.org/abs/2511.07922</guid>
<content:encoded><![CDATA[
arXiv:2511.07922v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has been shown to improve the capabilities of large language models (LLMs). However, applying RL to open-domain tasks faces two key challenges: (1) the inherent subjectivity of these tasks prevents the verifiable rewards as required by Reinforcement Learning with Verifiable Rewards (RLVR); (2) Reinforcement Learning from Human Feedback (RLHF) relies on external reward mechanisms. To overcome these limitations, we propose Self-Examining Reinforcement Learning (SERL), a novel self-improving framework where the LLM serves as both Actor and Judge. SERL introduces two synergistic reward mechanisms without any external signals. On the one hand, to improve the Actor's capability, we derive rewards from Copeland-style pairwise comparison judgments across a group of generated responses. On the other hand, a self-consistency reward that encourages coherent judgments is proposed to improve the Judge's reliability. This process refines the Judge's capability, which in turn provides a more robust reward for Actor. Experiments show that our method outperforms existing self-improvement training methods. SERL improves the LC win rate of Qwen3-8B on AlpacaEval 2 from 52.37% to 59.90%. To the best of our knowledge, our method achieves state-of-the-art performance among self-improving approaches. Furthermore, it achieves a performance comparable to significantly larger models like Qwen3-32B, demonstrating superior effectiveness and robustness on open-domain tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharp detection of low-dimensional structure in probability measures via dimensional logarithmic Sobolev inequalities</title>
<link>https://arxiv.org/abs/2406.13036</link>
<guid>https://arxiv.org/abs/2406.13036</guid>
<content:encoded><![CDATA[
arXiv:2406.13036v3 Announce Type: replace-cross 
Abstract: Identifying low-dimensional structure in high-dimensional probability measures is an essential pre-processing step for efficient sampling. We introduce a method for identifying and approximating a target measure $\pi$ as a perturbation of a given reference measure $\mu$ along a few significant directions of $\mathbb{R}^{d}$. The reference measure can be a Gaussian or a nonlinear transformation of a Gaussian, as commonly arising in generative modeling. Our method extends prior work on minimizing majorizations of the Kullback--Leibler divergence to identify optimal approximations within this class of measures. Our main contribution unveils a connection between the \emph{dimensional} logarithmic Sobolev inequality (LSI) and approximations with this ansatz. Specifically, when the target and reference are both Gaussian, we show that minimizing the dimensional LSI is equivalent to minimizing the KL divergence restricted to this ansatz. For general non-Gaussian measures, the dimensional LSI produces majorants that uniformly improve on previous majorants for gradient-based dimension reduction. We further demonstrate the applicability of this analysis to the squared Hellinger distance, where analogous reasoning shows that the dimensional Poincar\'e inequality offers improved bounds.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmark on Drug Target Interaction Modeling from a Drug Structure Perspective</title>
<link>https://arxiv.org/abs/2407.04055</link>
<guid>https://arxiv.org/abs/2407.04055</guid>
<content:encoded><![CDATA[
arXiv:2407.04055v2 Announce Type: replace-cross 
Abstract: The prediction modeling of drug-target interactions is crucial to drug discovery and design, which has seen rapid advancements owing to deep learning technologies. Recently developed methods, such as those based on graph neural networks (GNNs) and Transformers, demonstrate exceptional performance across various datasets by effectively extracting structural information. However, the benchmarking of these novel methods often varies significantly in terms of hyperparameter settings and datasets, which limits algorithmic progress. In view of these, we conducted a comprehensive survey and benchmark for drug-target interaction modeling from a structural perspective via integrating tens of explicit (i.e., GNN-based) and implicit (i.e., Transformer-based) structure learning algorithms. We conducted a macroscopical comparison between these two classes of encoding strategies as well as the different featurization techniques that inform molecules' chemical and physical properties. We then carry out the microscopical comparison between all the integrated models across the six datasets via comprehensively benchmarking their effectiveness and efficiency. To ensure fairness, we investigate model performance under individually optimized configuration. Remarkably, the summarized insights from the benchmark studies lead to the design of model combos. We demonstrate that our combos can achieve new state-of-the-art performance on various datasets associated with cost-effective memory and computation.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Energy Cost of Artificial Intelligence Lifecycle in Communication Networks</title>
<link>https://arxiv.org/abs/2408.00540</link>
<guid>https://arxiv.org/abs/2408.00540</guid>
<content:encoded><![CDATA[
arXiv:2408.00540v4 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) is being incorporated in several optimization, scheduling, orchestration as well as in native communication network functions. This paradigm shift results in increased energy consumption, however, quantifying the end-to-end energy consumption of adding intelligence to communication systems remains an open challenge since conventional energy consumption metrics focus on either communication, computation infrastructure, or model development. To address this, we propose a new metric, the Energy Cost of AI Lifecycle (eCAL) of an AI model in a system. eCAL captures the energy consumption throughout the development, deployment and utilization of an AI-model providing intelligence in a communication network by (i) analyzing the complexity of data collection and manipulation in individual components and (ii) deriving overall and per-bit energy consumption. We show that as a trained AI model is used more frequently for inference, its energy cost per inference decreases, since the fixed training energy is amortized over a growing number of inferences. For a simple case study we show that eCAL for 100 inferences is 2.73 times higher than for 1000 inferences. Additionally, we have developed a modular and extendable open-source simulation tool to enable researchers, practitioners, and engineers to calculate the end-to-end energy cost with various configurations and across various systems, ensuring adaptability to diverse use cases.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Streamlining Constraints with Large Language Models</title>
<link>https://arxiv.org/abs/2408.10268</link>
<guid>https://arxiv.org/abs/2408.10268</guid>
<content:encoded><![CDATA[
arXiv:2408.10268v3 Announce Type: replace-cross 
Abstract: Streamlining constraints (or streamliners, for short) narrow the search space, enhancing the speed and feasibility of solving complex constraint satisfaction problems. Traditionally, streamliners were crafted manually or generated through systematically combined atomic constraints with high-effort offline testing. Our approach utilizes the creativity of Large Language Models (LLMs) to propose effective streamliners for problems specified in the MiniZinc constraint programming language and integrates feedback to the LLM with quick empirical tests for validation. Evaluated across seven diverse constraint satisfaction problems, our method achieves substantial runtime reductions. We compare the results to obfuscated and disguised variants of the problem to see whether the results depend on LLM memorization. We also analyze whether longer off-line runs improve the quality of streamliners and whether the LLM can propose good combinations of streamliners.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Machines Think Like Humans? A Behavioral Evaluation of LLM Agents in Dictator Games</title>
<link>https://arxiv.org/abs/2410.21359</link>
<guid>https://arxiv.org/abs/2410.21359</guid>
<content:encoded><![CDATA[
arXiv:2410.21359v3 Announce Type: replace-cross 
Abstract: As Large Language Model (LLM)-based agents increasingly engage with human society, how well do we understand their prosocial behaviors? We (1) investigate how LLM agents' prosocial behaviors can be induced by different personas and benchmarked against human behaviors; and (2) introduce a social science approach to evaluate LLM agents' decision-making. We explored how different personas and experimental framings affect these AI agents' altruistic behavior in dictator games and compared their behaviors within the same LLM family, across various families, and with human behaviors. The findings reveal that merely assigning a human-like identity to LLMs does not produce human-like behaviors. These findings suggest that LLM agents' reasoning does not consistently exhibit textual markers of human decision-making in dictator games and that their alignment with human behavior varies substantially across model architectures and prompt formulations; even worse, such dependence does not follow a clear pattern. As society increasingly integrates machine intelligence, "Prosocial AI" emerges as a promising and urgent research direction in philanthropic studies.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2410.22995</link>
<guid>https://arxiv.org/abs/2410.22995</guid>
<content:encoded><![CDATA[
arXiv:2410.22995v2 Announce Type: replace-cross 
Abstract: A hallmark of advanced artificial intelligence is the capacity to progress from passive visual perception to the strategic modification of visual information to facilitate complex reasoning. This advanced capability, however, remains critically underdeveloped in current Large Multi-modal Models (LMMs). The deficiency is often masked by evaluation metrics that prioritize final-answer accuracy, creating an illusion of competence where genuine reasoning is absent. Using the domain of geometric problem-solving as a precise instrument, we probe this issue through tasks that require constructing visual aids. To this end, we introduce \textbf{VisAidMath}, a challenging benchmark, and our novel Three-Layered Funnel Evaluation Framework. This framework moves beyond simple accuracy (ACCU) to scrutinize the generation of valid visual aids (PVA) and the soundness of subsequent reasoning steps (SPRS). Our extensive experiments on state-of-the-art models, including Doubao-Seed-1.6 and o4, reveal a profound ``Reasoning Illusion''. We observe that high surface-level accuracy conceals a catastrophic failure in the models' ability to produce valid visual aids or to reason from them. Our findings expose a fundamental schism between visual perception and logical deduction in modern LMMs. We host an evaluation platform at CodaBench for testing publicly. Homepage: https://nlp2ct.github.io/VisAidMathHomepage/ Evaluation: https://www.codabench.org/competitions/7634/
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iris: Integrating Language into Diffusion-based Monocular Depth Estimation</title>
<link>https://arxiv.org/abs/2411.16750</link>
<guid>https://arxiv.org/abs/2411.16750</guid>
<content:encoded><![CDATA[
arXiv:2411.16750v4 Announce Type: replace-cross 
Abstract: Traditional monocular depth estimation suffers from inherent ambiguity and visual nuisances. We demonstrate that language can enhance monocular depth estimation by providing an additional condition (rather than images alone) aligned with plausible 3D scenes, thereby reducing the solution space for depth estimation. This conditional distribution is learned during the text-to-image pre-training of diffusion models. To generate images under various viewpoints and layouts that precisely reflect textual descriptions, the model implicitly models object sizes, shapes, and scales, their spatial relationships, and the overall scene structure. In this paper, Iris, we investigate the benefits of our strategy to integrate text descriptions into training and inference of diffusion-based depth estimation models. We experiment with three different diffusion-based monocular depth estimators (Marigold, Lotus, and E2E-FT) and their variants. By training on HyperSim and Virtual KITTI, and evaluating on NYUv2, KITTI, ETH3D, ScanNet, and DIODE, we find that our strategy improves the overall monocular depth estimation accuracy, especially in small areas. It also improves the model's depth perception of specific regions described in the text. We find that by providing more details in the text, the depth prediction can be iteratively refined. Simultaneously, we find that language can act as a constraint to accelerate the convergence of both training and the inference diffusion trajectory. Code and generated text data will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Explainability for Weakly Supervised Segmentation in Medical PE Detection</title>
<link>https://arxiv.org/abs/2412.07384</link>
<guid>https://arxiv.org/abs/2412.07384</guid>
<content:encoded><![CDATA[
arXiv:2412.07384v2 Announce Type: replace-cross 
Abstract: Pulmonary Embolism (PE) are a leading cause of cardiovascular death. Computed tomographic pulmonary angiography (CTPA) is the gold standard for PE diagnosis, with growing interest in AI-based diagnostic assistance. However, these algorithms are limited by scarce fine-grained annotations of thromboembolic burden. We address this challenge with iExplain, a weakly supervised learning algorithm that transforms coarse image-level annotations into detailed pixel-level PE masks through iterative model explainability. Our approach generates soft segmentation maps used to mask detected regions, enabling the process to repeat and discover additional embolisms that would be missed in a single pass. This iterative refinement effectively captures complete PE regions and detects multiple distinct embolisms. Models trained on these automatically generated annotations achieve excellent PE detection performance, with significant improvements at each iteration. We demonstrate iExplain's effectiveness on the RSPECT augmented dataset, achieving results comparable to strongly supervised methods while outperforming existing weakly supervised methods.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizable and Fast Surrogates: Model Predictive Control of Articulated Soft Robots using Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2502.01916</link>
<guid>https://arxiv.org/abs/2502.01916</guid>
<content:encoded><![CDATA[
arXiv:2502.01916v3 Announce Type: replace-cross 
Abstract: Soft robots can revolutionize several applications with high demands on dexterity and safety. When operating these systems, real-time estimation and control require fast and accurate models. However, prediction with first-principles (FP) models is slow, and learned black-box models have poor generalizability. Physics-informed machine learning offers excellent advantages here, but it is currently limited to simple, often simulated systems without considering changes after training. We propose physics-informed neural networks (PINNs) for articulated soft robots (ASRs) with a focus on data efficiency. The amount of expensive real-world training data is reduced to a minimum -- one dataset in one system domain. Two hours of data in different domains are used for a comparison against two gold-standard approaches: In contrast to a recurrent neural network, the PINN provides a high generalizability. The prediction speed of an accurate FP model is exceeded with the PINN by up to a factor of 467 at slightly reduced accuracy. This enables nonlinear model predictive control (MPC) of a pneumatic ASR. Accurate position tracking with the MPC running at 47 Hz is achieved in six dynamic experiments.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoM: Linear Sequence Modeling with Mixture-of-Memories</title>
<link>https://arxiv.org/abs/2502.13685</link>
<guid>https://arxiv.org/abs/2502.13685</guid>
<content:encoded><![CDATA[
arXiv:2502.13685v4 Announce Type: replace-cross 
Abstract: Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive tasks. To address this limitation, we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. MoM serves as a general framework that can be seamlessly combined with diverse memory update mechanisms across linear models. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Image Generation for Recommendations Beyond Catalogs</title>
<link>https://arxiv.org/abs/2502.18477</link>
<guid>https://arxiv.org/abs/2502.18477</guid>
<content:encoded><![CDATA[
arXiv:2502.18477v2 Announce Type: replace-cross 
Abstract: Personalization is central to human-AI interaction, yet current diffusion-based image generation systems remain largely insensitive to user diversity. Existing attempts to address this often rely on costly paired preference data or introduce latency through Large Language Models. In this work, we introduce REBECA (REcommendations BEyond CAtalogs), a lightweight and scalable framework for personalized image generation that learns directly from implicit feedback signals such as likes, ratings, and clicks. Instead of fine-tuning the underlying diffusion model, REBECA employs a two-stage process: training a conditional diffusion model to sample user- and rating-specific image embeddings, which are subsequently decoded into images using a pretrained diffusion backbone. This approach enables efficient, fine-tuning-free personalization across large user bases. We rigorously evaluate REBECA on real-world datasets, proposing a novel statistical personalization verifier and a permutation-based hypothesis test to assess preference alignment. Our results demonstrate that REBECA consistently produces high-fidelity images tailored to individual tastes, outperforming baselines while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Manifold Learning for Hyperspectral Images</title>
<link>https://arxiv.org/abs/2503.15016</link>
<guid>https://arxiv.org/abs/2503.15016</guid>
<content:encoded><![CDATA[
arXiv:2503.15016v3 Announce Type: replace-cross 
Abstract: Traditional feature extraction and projection techniques, such as Principal Component Analysis, struggle to adequately represent X-Ray Transmission (XRT) Multi-Energy (ME) images, limiting the performance of neural networks in decision-making processes. To address this issue, we propose a method that approximates the dataset topology by constructing adjacency graphs using the Uniform Manifold Approximation and Projection. This approach captures nonlinear correlations within the data, significantly improving the performance of machine learning algorithms, particularly in processing Hyperspectral Images (HSI) from X-ray transmission spectroscopy. This technique not only preserves the global structure of the data but also enhances feature separability, leading to more accurate and robust classification results.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Purifying Approximate Differential Privacy with Randomized Post-processing</title>
<link>https://arxiv.org/abs/2503.21071</link>
<guid>https://arxiv.org/abs/2503.21071</guid>
<content:encoded><![CDATA[
arXiv:2503.21071v2 Announce Type: replace-cross 
Abstract: We propose a framework to convert $(\varepsilon, \delta)$-approximate Differential Privacy (DP) mechanisms into $(\varepsilon', 0)$-pure DP mechanisms under certain conditions, a process we call ``purification.'' This algorithmic technique leverages randomized post-processing with calibrated noise to eliminate the $\delta$ parameter while achieving near-optimal privacy-utility tradeoff for pure DP. It enables a new design strategy for pure DP algorithms: first run an approximate DP algorithm with certain conditions, and then purify. This approach allows one to leverage techniques such as strong composition and propose-test-release that require $\delta>0$ in designing pure-DP methods with $\delta=0$. We apply this framework in various settings, including Differentially Private Empirical Risk Minimization (DP-ERM), stability-based release, and query release tasks. To the best of our knowledge, this is the first work with a statistically and computationally efficient reduction from approximate DP to pure DP. Finally, we illustrate the use of this reduction for proving lower bounds under approximate DP constraints with explicit dependence in $\delta$, avoiding the sophisticated fingerprinting code construction.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Neural Networks Based Analog Circuit Link Prediction</title>
<link>https://arxiv.org/abs/2504.10240</link>
<guid>https://arxiv.org/abs/2504.10240</guid>
<content:encoded><![CDATA[
arXiv:2504.10240v5 Announce Type: replace-cross 
Abstract: Circuit link prediction, which identifies missing component connections from incomplete netlists, is crucial in analog circuit design automation. However, existing methods face three main challenges: 1) Insufficient use of topological patterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to the complexity of annotations hinders model generalization; 3) Limited adaptability to various netlist formats restricts model flexibility. We propose Graph Neural Networks Based Analog Circuit Link Prediction (GNN-ACLP), a graph neural networks (GNNs) based method featuring three innovations to tackle these challenges. First, we introduce the SEAL (learning from Subgraphs, Embeddings, and Attributes for Link prediction) framework and achieve port-level accuracy in circuit link prediction. Second, we propose Netlist Babel Fish, a netlist format conversion tool that leverages retrieval-augmented generation (RAG) with a large language model (LLM) to enhance the compatibility of netlist formats. Finally, we build a comprehensive dataset, SpiceNetlist, comprising 775 annotated circuits of 7 different types across 10 component classes. Experiments demonstrate accuracy improvements of 16.08% on SpiceNetlist, 11.38% on Image2Net, and 16.01% on Masala-CHAI compared to the baseline in intra-dataset evaluation, while maintaining accuracy from 92.05% to 99.07% in cross-dataset evaluation, demonstrating robust feature transfer capabilities. However, its linear computational complexity makes processing large-scale netlists challenging and requires future addressing.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mobile Jamming Mitigation in 5G Networks: A MUSIC-Based Adaptive Beamforming Approach</title>
<link>https://arxiv.org/abs/2505.08046</link>
<guid>https://arxiv.org/abs/2505.08046</guid>
<content:encoded><![CDATA[
arXiv:2505.08046v2 Announce Type: replace-cross 
Abstract: Mobile jammers pose a critical threat to 5G networks, particularly in military communications. We propose an intelligent anti-jamming framework that integrates Multiple Signal Classification (MUSIC) for high-resolution Direction-of-Arrival (DoA) estimation, Minimum Variance Distortionless Response (MVDR) beamforming for adaptive interference suppression, and machine learning (ML) to enhance DoA prediction for mobile jammers. Extensive simulations in a realistic highway scenario demonstrate that our hybrid approach achieves an average Signal-to-Noise Ratio (SNR) improvement of 9.58 dB (maximum 11.08 dB) and up to 99.8% DoA estimation accuracy. The framework's computational efficiency and adaptability to dynamic jammer mobility patterns outperform conventional anti-jamming techniques, making it a robust solution for securing 5G communications in contested environments.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.09562</link>
<guid>https://arxiv.org/abs/2506.09562</guid>
<content:encoded><![CDATA[
arXiv:2506.09562v3 Announce Type: replace-cross 
Abstract: Deep reinforcement learning (DRL) has achieved remarkable success in a wide range of sequential decision-making applications, including robotics, healthcare, smart grids, and finance. Recent studies reveal that adversaries can implant backdoors into DRL agents during the training phase. These backdoors can later be activated by specific triggers during deployment, compelling the agent to execute targeted actions and potentially leading to severe consequences, such as drone crashes or vehicle collisions. However, existing backdoor attacks utilize simplistic and heuristic trigger configurations, overlooking the critical impact of trigger design on attack effectiveness. To address this gap, we introduce TooBadRL, the first framework to systematically optimize DRL backdoor triggers across three critical aspects: injection timing, trigger dimension, and manipulation magnitude. Specifically, we first introduce a performance-aware adaptive freezing mechanism to determine the injection timing during training. Then, we formulate trigger selection as an influence attribution problem and apply Shapley value analysis to identify the most influential trigger dimension for injection. Furthermore, we propose an adversarial input synthesis method to optimize the manipulation magnitude under environmental constraints. Extensive evaluations on three DRL algorithms and nine benchmark tasks demonstrate that TooBadRL outperforms five baseline methods in terms of attack success rate while only slightly affecting normal task performance. We further evaluate potential defense strategies from detection and mitigation perspectives. We open-source our code to facilitate reproducibility and further research.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Geology: Structural Geology Meets Deep Learning</title>
<link>https://arxiv.org/abs/2506.11164</link>
<guid>https://arxiv.org/abs/2506.11164</guid>
<content:encoded><![CDATA[
arXiv:2506.11164v2 Announce Type: replace-cross 
Abstract: Reconstructing the structural geology and mineral composition of the first few kilometers of the Earth's subsurface from sparse or indirect surface observations remains a long-standing challenge with critical applications in mineral exploration, geohazard assessment, and geotechnical engineering. This inherently ill-posed problem is often addressed by classical geophysical inversion methods, which typically yield a single maximum-likelihood model that fails to capture the full range of plausible geology. The adoption of modern deep learning methods has been limited by the lack of large 3D training datasets. We address this gap with \textit{StructuralGeo}, a geological simulation engine that mimics eons of tectonic, magmatic, and sedimentary processes to generate a virtually limitless supply of realistic synthetic 3D lithological models. Using this dataset, we train both unconditional and conditional generative flow-matching models with a 3D attention U-net architecture. The resulting foundation model can reconstruct multiple plausible 3D scenarios from surface topography and sparse borehole data, depicting structures such as layers, faults, folds, and dikes. By sampling many reconstructions from the same observations, we introduce a probabilistic framework for estimating the size and extent of subsurface features. While the realism of the output is bounded by the fidelity of the training data to true geology, this combination of simulation and generative AI functions offers a flexible prior for probabilistic modeling, regional fine-tuning, and use as an AI-based regularizer in traditional geophysical inversion workflows.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoLM: In Search of Lost Language Model Training Dynamics</title>
<link>https://arxiv.org/abs/2506.16029</link>
<guid>https://arxiv.org/abs/2506.16029</guid>
<content:encoded><![CDATA[
arXiv:2506.16029v2 Announce Type: replace-cross 
Abstract: Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. We train over 100 LMs with 1B and 4B parameters from scratch, and evaluate both upstream (language modeling) and downstream (problem-solving) capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning few-step posterior samplers by unfolding and distillation of diffusion models</title>
<link>https://arxiv.org/abs/2507.02686</link>
<guid>https://arxiv.org/abs/2507.02686</guid>
<content:encoded><![CDATA[
arXiv:2507.02686v2 Announce Type: replace-cross 
Abstract: Diffusion models (DMs) have emerged as powerful image priors in Bayesian computational imaging. Two primary strategies have been proposed for leveraging DMs in this context: Plug-and-Play methods, which are zero-shot and highly flexible but rely on approximations; and specialized conditional DMs, which achieve higher accuracy and faster inference for specific tasks through supervised training. In this work, we introduce a novel framework that integrates deep unfolding and model distillation to transform a DM image prior into a few-step conditional model for posterior sampling. A central innovation of our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm - specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et al., 2025) - representing the first known instance of deep unfolding applied to a Monte Carlo sampling scheme. We demonstrate our proposed unfolded and distilled samplers through extensive experiments and comparisons with the state of the art, where they achieve excellent accuracy and computational efficiency, while retaining the flexibility to adapt to variations in the forward model at inference time.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastDINOv2: Frequency Based Curriculum Learning Improves Robustness and Training Speed</title>
<link>https://arxiv.org/abs/2507.03779</link>
<guid>https://arxiv.org/abs/2507.03779</guid>
<content:encoded><![CDATA[
arXiv:2507.03779v2 Announce Type: replace-cross 
Abstract: Large-scale vision foundation models such as DINOv2 boast impressive performances by leveraging massive architectures and training datasets. But numerous scenarios require practitioners to reproduce those pre-training solutions, such as on private data, new modalities, or simply for scientific questioning--which is currently extremely demanding computation-wise. We thus propose a novel pre-training strategy for DINOv2 that simultaneously accelerates convergence--and strengthens robustness to common corruptions as a by-product. Our approach involves a frequency filtering curriculum--low-frequency being seen first--and the Gaussian noise patching augmentation. Applied to a ViT-B/16 backbone trained on ImageNet-1K, while pre-training time and FLOPs are reduced by 1.6x and 2.25x, our method still achieves matching robustness in corruption benchmarks (ImageNet-C) and maintains competitive linear probing performance compared with baseline. This dual benefit of efficiency and robustness makes large-scale self-supervised foundation modeling more attainable, while opening the door to novel exploration around data curriculum and augmentation as means to improve self-supervised learning models robustness. The code is available at https://github.com/KevinZ0217/fast_dinov2
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation</title>
<link>https://arxiv.org/abs/2507.15243</link>
<guid>https://arxiv.org/abs/2507.15243</guid>
<content:encoded><![CDATA[
arXiv:2507.15243v2 Announce Type: replace-cross 
Abstract: Despite the progress in cross-domain few-shot learning, a model pre-trained with DINO combined with a prototypical classifier outperforms the latest SOTA methods. A crucial limitation that needs to be overcome is that updating too many parameters of the transformers leads to overfitting due to the scarcity of labeled samples. To address this challenge, we propose a new concept, coalescent projection, as an effective successor to soft prompts. Additionally, we propose a novel pseudo-class generation method, combined with self-supervised transformations, that relies solely on the base domain to prepare the network to encounter unseen samples from different domains. The proposed method exhibits its effectiveness in comprehensive experiments on the extreme domain-shift problem of the BSCD-FSL benchmark. Our code is published at \href{https://github.com/Naeem-Paeedeh/CPLSR}{https://github.com/Naeem-Paeedeh/CPLSR}.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Promise of RL for Autoregressive Image Editing</title>
<link>https://arxiv.org/abs/2508.01119</link>
<guid>https://arxiv.org/abs/2508.01119</guid>
<content:encoded><![CDATA[
arXiv:2508.01119v3 Announce Type: replace-cross 
Abstract: While image generation techniques are now capable of producing high-quality images that respect prompts which span multiple sentences, the task of text-guided image editing remains a challenge. Even edit requests that consist of only a few words often fail to be executed correctly. We explore three strategies to enhance performance on a wide range of image editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning. In order to study all these components in one consistent framework, we adopt an autoregressive multimodal model that processes textual and visual tokens in a unified manner. We find RL combined with a large multi-modal LLM verifier to be the most effective of these strategies. As a result, we release EARL: Editing with Autoregression and RL, a strong RL-based image editing model that performs competitively on a diverse range of edits compared to strong baselines, despite using much less training data. Thus, EARL pushes the frontier of autoregressive multimodal models on image editing. We release our code, training data, and trained models at https://github.com/mair-lab/EARL.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title>
<link>https://arxiv.org/abs/2508.01249</link>
<guid>https://arxiv.org/abs/2508.01249</guid>
<content:encoded><![CDATA[
arXiv:2508.01249v3 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's runtime traces as graph-based intermediate representations with control and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools \& data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis for sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can reduce the ASR to 3\%, with the utility drop only 1\%.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for Advertiser Keyphrase Recommendations</title>
<link>https://arxiv.org/abs/2508.03628</link>
<guid>https://arxiv.org/abs/2508.03628</guid>
<content:encoded><![CDATA[
arXiv:2508.03628v3 Announce Type: replace-cross 
Abstract: E-commerce sellers are advised to bid on keyphrases to boost their advertising campaigns. These keyphrases must be relevant to prevent irrelevant items from cluttering search systems and to maintain positive seller perception. It is vital that keyphrase suggestions align with seller, search and buyer judgments. Given the challenges in collecting negative feedback in these systems, LLMs have been used as a scalable proxy to human judgments. This paper presents an empirical study on a major ecommerce platform of a distillation framework involving an LLM teacher, a cross-encoder assistant and a bi-encoder Embedding Based Retrieval (EBR) student model, aimed at mitigating click-induced biases in keyphrase recommendations.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions</title>
<link>https://arxiv.org/abs/2508.05430</link>
<guid>https://arxiv.org/abs/2508.05430</guid>
<content:encoded><![CDATA[
arXiv:2508.05430v2 Announce Type: replace-cross 
Abstract: Language-image pre-training (LIP) enables the development of vision-language models capable of zero-shot classification, localization, multimodal retrieval, and semantic understanding. Various explanation methods have been proposed to visualize the importance of input image-text pairs on the model's similarity outputs. However, popular saliency maps are limited by capturing only first-order attributions, overlooking the complex cross-modal interactions intrinsic to such encoders. We introduce faithful interaction explanations of LIP models (FIxLIP) as a unified approach to decomposing the similarity in vision-language encoders. FIxLIP is rooted in game theory, where we analyze how using the weighted Banzhaf interaction index offers greater flexibility and improves computational efficiency over the Shapley interaction quantification framework. From a practical perspective, we propose how to naturally extend explanation evaluation metrics, such as the pointing game and area between the insertion/deletion curves, to second-order interaction explanations. Experiments on the MS COCO and ImageNet-1k benchmarks validate that second-order methods, such as FIxLIP, outperform first-order attribution methods. Beyond delivering high-quality explanations, we demonstrate the utility of FIxLIP in comparing different models, e.g. CLIP vs. SigLIP-2.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding</title>
<link>https://arxiv.org/abs/2508.11999</link>
<guid>https://arxiv.org/abs/2508.11999</guid>
<content:encoded><![CDATA[
arXiv:2508.11999v2 Announce Type: replace-cross 
Abstract: With the rapid advancement of e-commerce, exploring general representations rather than task-specific ones has attracted increasing research attention. For product understanding, although existing discriminative dual-flow architectures drive progress in this field, they inherently struggle to model the many-to-one alignment between multiple images and texts of products. Therefore, we argue that generative Multimodal Large Language Models (MLLMs) hold significant potential for improving product representation learning. Nevertheless, achieving this goal still remains non-trivial due to several key challenges: the lack of multimodal and aspect-aware modeling modules in typical LLMs; the common presence of background noise in product images; and the absence of a standard benchmark for evaluation. To address these issues, we propose the first generative MLLM-based model named MOON for product representation learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for targeted modeling of multimodal and aspect-specific product content; (2) effectively detects core semantic regions in product images to mitigate the distraction and interference caused by background noise; and (3) introduces the specialized negative sampling strategy to increase the difficulty and diversity of negative samples. In addition, we release a large-scale multimodal benchmark MBE for various product understanding tasks. Experimentally, our model demonstrates competitive zero-shot performance on both our benchmark and the public dataset, showcasing strong generalization across various downstream tasks, including cross-modal retrieval, product classification, and attribute prediction. Furthermore, the case study and visualization illustrate the effectiveness of MOON for product understanding.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skill-Aligned Fairness in Multi-Agent Learning for Collaboration in Healthcare</title>
<link>https://arxiv.org/abs/2508.18708</link>
<guid>https://arxiv.org/abs/2508.18708</guid>
<content:encoded><![CDATA[
arXiv:2508.18708v3 Announce Type: replace-cross 
Abstract: Fairness in multi-agent reinforcement learning (MARL) is often framed as a workload balance problem, overlooking agent expertise and the structured coordination required in real-world domains. In healthcare, equitable task allocation requires workload balance or expertise alignment to prevent burnout and overuse of highly skilled agents. Workload balance refers to distributing an approximately equal number of subtasks or equalised effort across healthcare workers, regardless of their expertise. We make two contributions to address this problem. First, we propose FairSkillMARL, a framework that defines fairness as the dual objective of workload balance and skill-task alignment. Second, we introduce MARLHospital, a customizable healthcare-inspired environment for modeling team compositions and energy-constrained scheduling impacts on fairness, as no existing simulators are well-suited for this problem. We conducted experiments to compare FairSkillMARL in conjunction with four standard MARL methods, and against two state-of-the-art fairness metrics. Our results suggest that fairness based solely on equal workload might lead to task-skill mismatches and highlight the need for more robust metrics that capture skill-task misalignment. Our work provides tools and a foundation for studying fairness in heterogeneous multi-agent systems where aligning effort with expertise is critical.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phase diagram and eigenvalue dynamics of stochastic gradient descent in multilayer neural networks</title>
<link>https://arxiv.org/abs/2509.01349</link>
<guid>https://arxiv.org/abs/2509.01349</guid>
<content:encoded><![CDATA[
arXiv:2509.01349v2 Announce Type: replace-cross 
Abstract: Hyperparameter tuning is one of the essential steps to guarantee the convergence of machine learning models. We argue that intuition about the optimal choice of hyperparameters for stochastic gradient descent can be obtained by studying a neural network's phase diagram, in which each phase is characterised by distinctive dynamics of the singular values of weight matrices. Taking inspiration from disordered systems, we start from the observation that the loss landscape of a multilayer neural network with mean squared error can be interpreted as a disordered system in feature space, where the learnt features are mapped to soft spin degrees of freedom, the initial variance of the weight matrices is interpreted as the strength of the disorder, and temperature is given by the ratio of the learning rate and the batch size. As the model is trained, three phases can be identified, in which the dynamics of weight matrices is qualitatively different. Employing a Langevin equation for stochastic gradient descent, previously derived using Dyson Brownian motion, we demonstrate that the three dynamical regimes can be classified effectively, providing practical guidance for the choice of hyperparameters of the optimiser.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Differentiation of Agent-Based Models</title>
<link>https://arxiv.org/abs/2509.03303</link>
<guid>https://arxiv.org/abs/2509.03303</guid>
<content:encoded><![CDATA[
arXiv:2509.03303v2 Announce Type: replace-cross 
Abstract: Agent-based models (ABMs) simulate complex systems by capturing the bottom-up interactions of individual agents comprising the system. Many complex systems of interest, such as epidemics or financial markets, involve thousands or even millions of agents. Consequently, ABMs often become computationally demanding and rely on the calibration of numerous free parameters, which has significantly hindered their widespread adoption. In this paper, we demonstrate that automatic differentiation (AD) techniques can effectively alleviate these computational burdens. By applying AD to ABMs, the gradients of the simulator become readily available, greatly facilitating essential tasks such as calibration and sensitivity analysis. Specifically, we show how AD enables variational inference (VI) techniques for efficient parameter calibration. Our experiments demonstrate substantial performance improvements and computational savings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape; and the SIR epidemiological model. Our approach thus significantly enhances the practicality and scalability of ABMs for studying complex systems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees</title>
<link>https://arxiv.org/abs/2509.07939</link>
<guid>https://arxiv.org/abs/2509.07939</guid>
<content:encoded><![CDATA[
arXiv:2509.07939v2 Announce Type: replace-cross 
Abstract: Recent advances in Large Language Models (LLMs) have driven interest in automating cybersecurity penetration testing workflows, offering the promise of faster and more consistent vulnerability assessment for enterprise systems. Existing LLM agents for penetration testing primarily rely on self-guided reasoning, which can produce inaccurate or hallucinated procedural steps. As a result, the LLM agent may undertake unproductive actions, such as exploiting unused software libraries or generating cyclical responses that repeat prior tactics. In this work, we propose a guided reasoning pipeline for penetration testing LLM agents that incorporates a deterministic task tree built from the MITRE ATT&amp;CK Matrix, a proven penetration testing kll chain, to constrain the LLM's reaoning process to explicitly defined tactics, techniques, and procedures. This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures. To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Our proposed reasoning pipeline guided the LLM agent through 71.8\%, 72.8\%, and 78.6\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively. Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and required 86.2\%, 118.7\%, and 205.9\% more model queries. This suggests that incorporating a deterministic task tree into LLM reasoning pipelines can enhance the accuracy and efficiency of automated cybersecurity assessments
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concentration inequalities for semidefinite least squares based on data</title>
<link>https://arxiv.org/abs/2509.13166</link>
<guid>https://arxiv.org/abs/2509.13166</guid>
<content:encoded><![CDATA[
arXiv:2509.13166v2 Announce Type: replace-cross 
Abstract: We study data-driven least squares (LS) problems with semidefinite (SD) constraints and derive finite-sample guarantees on the spectrum of their optimal solutions when these constraints are relaxed. In particular, we provide a high confidence bound allowing one to solve a simpler program in place of the full SDLS problem, while ensuring that the eigenvalues of the resulting solution are $\varepsilon$-close of those enforced by the SD constraints. The developed certificate, which consistently shrinks as the number of data increases, turns out to be easy-to-compute, distribution-free, and only requires independent and identically distributed samples. Moreover, when the SDLS is used to learn an unknown quadratic function, we establish bounds on the error between a gradient descent iterate minimizing the surrogate cost obtained with no SD constraints and the true minimizer.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patent Language Model Pretraining with ModernBERT</title>
<link>https://arxiv.org/abs/2509.14926</link>
<guid>https://arxiv.org/abs/2509.14926</guid>
<content:encoded><![CDATA[
arXiv:2509.14926v3 Announce Type: replace-cross 
Abstract: Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text. Prior approaches to patent NLP have primarily relied on fine-tuning general-purpose models or domain-adapted variants pretrained with limited data. In this work, we pretrain 3 domain-specific masked language models for patents, using the ModernBERT architecture and a curated corpus of over 60 million patent records. Our approach incorporates architectural optimizations, including FlashAttention, rotary embeddings, and GLU feed-forward layers. We evaluate our models on four downstream patent classification tasks. Our model, ModernBERT-base-PT, consistently outperforms the general-purpose ModernBERT baseline on three out of four datasets and achieves competitive performance with a baseline PatentBERT. Additional experiments with ModernBERT-base-VX and Mosaic-BERT-large demonstrate that scaling the model size and customizing the tokenizer further enhance performance on selected tasks. Notably, all ModernBERT variants retain substantially faster inference over - 3x that of PatentBERT - underscoring their suitability for time-sensitive applications. These results underscore the benefits of domain-specific pretraining and architectural improvements for patent-focused NLP tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Atlas Graphs for Dynamic Scene Decomposition and Editing</title>
<link>https://arxiv.org/abs/2509.16336</link>
<guid>https://arxiv.org/abs/2509.16336</guid>
<content:encoded><![CDATA[
arXiv:2509.16336v3 Announce Type: replace-cross 
Abstract: Learning editable high-resolution scene representations for dynamic scenes is an open problem with applications across the domains from autonomous driving to creative editing - the most successful approaches today make a trade-off between editability and supporting scene complexity: neural atlases represent dynamic scenes as two deforming image layers, foreground and background, which are editable in 2D, but break down when multiple objects occlude and interact. In contrast, scene graph models make use of annotated data such as masks and bounding boxes from autonomous-driving datasets to capture complex 3D spatial relationships, but their implicit volumetric node representations are challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a hybrid high-resolution scene representation, where every graph node is a view-dependent neural atlas, facilitating both 2D appearance editing and 3D ordering and positioning of scene elements. Fit at test-time, NAGs achieve state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR increase compared to existing methods - and make environmental editing possible in high resolution and visual quality - creating counterfactual driving scenarios with new backgrounds and edited vehicle appearance. We find that the method also generalizes beyond driving scenes and compares favorably - by more than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS video dataset with a diverse set of human and animal-centric scenes.
  Project Page: https://princeton-computational-imaging.github.io/nag/
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning</title>
<link>https://arxiv.org/abs/2510.13675</link>
<guid>https://arxiv.org/abs/2510.13675</guid>
<content:encoded><![CDATA[
arXiv:2510.13675v2 Announce Type: replace-cross 
Abstract: Open-domain visual entity recognition aims to identify and link entities depicted in images to a vast and evolving set of real-world concepts, such as those found in Wikidata. Unlike conventional classification tasks with fixed label sets, it operates under open-set conditions, where most target entities are unseen during training and exhibit long-tail distributions. This makes the task inherently challenging due to limited supervision, high visual ambiguity, and the need for semantic disambiguation. We propose a Knowledge-guided Contrastive Learning (KnowCoL) framework that combines both images and text descriptions into a shared semantic space grounded by structured information from Wikidata. By abstracting visual and textual inputs to a conceptual level, the model leverages entity descriptions, type hierarchies, and relational context to support zero-shot entity recognition. We evaluate our approach on the OVEN benchmark, a large-scale open-domain visual recognition dataset with Wikidata IDs as the label space. Our experiments show that using visual, textual, and structured knowledge greatly improves accuracy, especially for rare and unseen entities. Our smallest model improves the accuracy on unseen entities by 10.5% compared to the state-of-the-art, despite being 35 times smaller.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction</title>
<link>https://arxiv.org/abs/2511.08955</link>
<guid>https://arxiv.org/abs/2511.08955</guid>
<content:encoded><![CDATA[
arXiv:2511.08955v2 Announce Type: replace-cross 
Abstract: Simulating microstructure evolution (MicroEvo) is vital for materials design but demands high numerical accuracy, efficiency, and physical fidelity. Although recent studies on deep learning (DL) offer a promising alternative to traditional solvers, the field lacks standardized benchmarks. Existing studies are flawed due to a lack of comparing specialized MicroEvo DL models with state-of-the-art spatio-temporal architectures, an overemphasis on numerical accuracy over physical fidelity, and a failure to analyze error propagation over time. To address these gaps, we introduce MicroEvoEval, the first comprehensive benchmark for image-based microstructure evolution prediction. We evaluate 14 models, encompassing both domain-specific and general-purpose architectures, across four representative MicroEvo tasks with datasets specifically structured for both short- and long-term assessment. Our multi-faceted evaluation framework goes beyond numerical accuracy and computational cost, incorporating a curated set of structure-preserving metrics to assess physical fidelity. Our extensive evaluations yield several key insights. Notably, we find that modern architectures (e.g., VMamba), not only achieve superior long-term stability and physical fidelity but also operate with an order-of-magnitude greater computational efficiency. The results highlight the necessity of holistic evaluation and identify these modern architectures as a highly promising direction for developing efficient and reliable surrogate models in data-driven materials science.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls</title>
<link>https://arxiv.org/abs/2511.09148</link>
<guid>https://arxiv.org/abs/2511.09148</guid>
<content:encoded><![CDATA[
arXiv:2511.09148v2 Announce Type: replace-cross 
Abstract: Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuum Dropout for Neural Differential Equations</title>
<link>https://arxiv.org/abs/2511.10446</link>
<guid>https://arxiv.org/abs/2511.10446</guid>
<content:encoded><![CDATA[
arXiv:2511.10446v2 Announce Type: replace-cross 
Abstract: Neural Differential Equations (NDEs) excel at modeling continuous-time dynamics, effectively handling challenges such as irregular observations, missing values, and noise. Despite their advantages, NDEs face a fundamental challenge in adopting dropout, a cornerstone of deep learning regularization, making them susceptible to overfitting. To address this research gap, we introduce Continuum Dropout, a universally applicable regularization technique for NDEs built upon the theory of alternating renewal processes. Continuum Dropout formulates the on-off mechanism of dropout as a stochastic process that alternates between active (evolution) and inactive (paused) states in continuous time. This provides a principled approach to prevent overfitting and enhance the generalization capabilities of NDEs. Moreover, Continuum Dropout offers a structured framework to quantify predictive uncertainty via Monte Carlo sampling at test time. Through extensive experiments, we demonstrate that Continuum Dropout outperforms existing regularization methods for NDEs, achieving superior performance on various time series and image classification tasks. It also yields better-calibrated and more trustworthy probability estimates, highlighting its effectiveness for uncertainty-aware modeling.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery</title>
<link>https://arxiv.org/abs/2511.10387</link>
<guid>https://arxiv.org/abs/2511.10387</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer-VAE, PROSAIL, vegetation biophysical retrieval, Sentinel-2, self-supervised learning<br /><br />Summary:<br /><br />1. This paper introduces a physics-informed Transformer-VAE architecture designed to invert the PROSAIL radiative transfer model for simultaneous retrieval of vegetation canopy parameters from Sentinel-2 satellite imagery.<br /><br />2. Unlike previous hybrid approaches requiring real satellite images for training, this model is trained exclusively on simulated data, yet achieves comparable accuracy to state-of-the-art methods that rely on real-world imagery.<br /><br />3. A key innovation is integrating the PROSAIL model as a differentiable physical decoder within the Transformer-VAE framework, ensuring that the latent variables represent physically meaningful leaf and canopy traits.<br /><br />4. The method is validated on real-world datasets, including FRM4Veg and BelSAR, demonstrating accurate retrieval of leaf area index (LAI) and canopy chlorophyll content (CCC) without any in-situ labels or calibration.<br /><br />5. The proposed approach offers a cost-effective, self-supervised solution for global vegetation monitoring and highlights how combining physical models with advanced deep learning can enhance the inversion of radiative transfer models for large-scale remote sensing applications. <div>
arXiv:2511.10387v3 Announce Type: replace-cross 
Abstract: Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management. In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data. Unlike previous hybrid approaches that require real satellite images for self-supevised training. Our model is trained exclusively on simulated data, yet achieves performance on par with state-of-the-art methods that utilize real imagery. The Transformer-VAE incorporates the PROSAIL model as a differentiable physical decoder, ensuring that inferred latent variables correspond to physically plausible leaf and canopy properties. We demonstrate retrieval of leaf area index (LAI) and canopy chlorophyll content (CCC) on real-world field datasets (FRM4Veg and BelSAR) with accuracy comparable to models trained with real Sentinel-2 data. Our method requires no in-situ labels or calibration on real images, offering a cost-effective and self-supervised solution for global vegetation monitoring. The proposed approach illustrates how integrating physical models with advanced deep networks can improve the inversion of RTMs, opening new prospects for large-scale, physically-constrained remote sensing of vegetation traits.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding</title>
<link>https://arxiv.org/abs/2511.10492</link>
<guid>https://arxiv.org/abs/2511.10492</guid>
<content:encoded><![CDATA[
<div> Keywords: recommender systems, human priors, generative models, beyond-accuracy objectives, adapter heads  

<br /><br />Summary:  
1. The paper addresses the need to optimize recommender systems not only for accuracy but also for beyond-accuracy objectives such as diversity, novelty, and personalization, which are key to long-term user satisfaction.  
2. It highlights the existence of vast structured domain knowledge called human priors (e.g., item taxonomies, temporal patterns) accumulated by industry practitioners, which is generally applied through post-hoc adjustments but remains separated from core model training.  
3. Existing methods often discard these human priors by learning user intent in a fully unsupervised manner, and frequently require architecture-specific changes that limit flexibility.  
4. The authors propose a backbone-agnostic framework that integrates human priors directly into the end-to-end training of generative recommender models using lightweight, prior-conditioned adapter heads inspired by efficient large language model decoding techniques.  
5. Their framework enables the model to disentangle user intent along interpretable axes such as interaction types and long- versus short-term interests and introduces a hierarchical composition strategy to model complex interactions across different prior types.  
6. Experiments on three large-scale datasets demonstrate significant improvements in both accuracy and beyond-accuracy objectives, showing that human priors facilitate better utilization of longer context lengths and larger model sizes within the backbone model. <div>
arXiv:2511.10492v2 Announce Type: replace-cross 
Abstract: Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner. Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge Machine Learning for Cluster Counting in Next-Generation Drift Chambers</title>
<link>https://arxiv.org/abs/2511.10540</link>
<guid>https://arxiv.org/abs/2511.10540</guid>
<content:encoded><![CDATA[
<div> Drift chambers, cluster counting, machine learning, FPGA, real-time processing<br /><br />Summary:<br /><br />This article addresses the evolving challenges in particle tracking with drift chambers, particularly for future colliders like a Higgs factory which require higher granularity and enhanced particle identification methods. Traditional tracking methods are limited by their data processing capacity when handling the increased data volume from such high-granularity detectors. To overcome this, the authors propose implementing machine learning (ML) algorithms directly at the detector‚Äôs cell-level readout, also known as "edge" processing. These ML models perform cluster counting at the source, significantly reducing the data rate that must be handled off-detector. The developed ML algorithms outperform conventional derivative-based techniques in achieving better pion-kaon separation, a critical aspect of particle ID. Moreover, when these algorithms are synthesized to FPGA hardware, they demonstrate low latencies compatible with real-time operation demands of future collider experiments. This advancement not only supports the development of next-generation drift chambers but also contributes to the broader field of hardware-embedded ML applications in high energy physics, opening new opportunities for efficient, fast, and precise detector data processing at the edge. <div>
arXiv:2511.10540v2 Announce Type: replace-cross 
Abstract: Drift chambers have long been central to collider tracking, but future machines like a Higgs factory motivate higher granularity and cluster counting for particle ID, posing new data processing challenges. Machine learning (ML) at the "edge", or in cell-level readout, can dramatically reduce the off-detector data rate for high-granularity drift chambers by performing cluster counting at-source. We present machine learning algorithms for cluster counting in real-time readout of future drift chambers. These algorithms outperform traditional derivative-based techniques based on achievable pion-kaon separation. When synthesized to FPGA resources, they can achieve latencies consistent with real-time operation in a future Higgs factory scenario, thus advancing both R&amp;D for future collider detectors as well as hardware-based ML for edge applications in high energy physics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELECTRA: A Cartesian Network for 3D Charge Density Prediction with Floating Orbitals</title>
<link>https://arxiv.org/abs/2503.08305</link>
<guid>https://arxiv.org/abs/2503.08305</guid>
<content:encoded><![CDATA[
<div> Keywords: Electronic charge density, Floating orbitals, Cartesian tensor network, Rotation equivariance, Density functional theory (DFT)<br /><br />Summary:  
1. The paper introduces ELECTRA, an equivariant model designed to predict electronic charge densities using the concept of floating orbitals rather than atom-centered orbitals.  
2. Floating orbitals offer a more compact and accurate representation of electronic structures but require expert placement, which has hindered their broader application.  
3. ELECTRA addresses this challenge by employing a Cartesian tensor network trained in a data-driven fashion to predict both the spatial positions of these floating orbitals and their associated coefficients.  
4. A symmetry-breaking mechanism is incorporated to learn position displacements with lower symmetry than the input molecule, while maintaining rotation equivariance of the resulting charge density.  
5. The model leverages Gaussian orbitals, predicting their weights and covariance matrices inspired by Gaussian Splatting techniques used for spatial density representation.  
6. ELECTRA achieves a state-of-the-art trade-off between computational efficiency and accuracy on benchmark datasets.  
7. Importantly, the predicted charge densities can be used to initialize density functional theory (DFT) calculations, resulting in an average reduction of 50.72% in the number of self-consistent field iterations required for convergence on new molecules. <div>
arXiv:2503.08305v4 Announce Type: replace 
Abstract: We present the Electronic Tensor Reconstruction Algorithm (ELECTRA) - an equivariant model for predicting electronic charge densities using floating orbitals. Floating orbitals are a long-standing concept in the quantum chemistry community that promises more compact and accurate representations by placing orbitals freely in space, as opposed to centering all orbitals at the position of atoms. Finding the ideal placement of these orbitals requires extensive domain knowledge, though, which thus far has prevented widespread adoption. We solve this in a data-driven manner by training a Cartesian tensor network to predict the orbital positions along with orbital coefficients. This is made possible through a symmetry-breaking mechanism that is used to learn position displacements with lower symmetry than the input molecule while preserving the rotation equivariance of the charge density itself. Inspired by recent successes of Gaussian Splatting in representing densities in space, we are using Gaussian orbitals and predicting their weights and covariance matrices. Our method achieves a state-of-the-art balance between computational efficiency and predictive accuracy on established benchmarks. Furthermore, ELECTRA is able to lower the compute time required to arrive at converged DFT solutions - initializing calculations using our predicted densities yields an average 50.72 % reduction in self-consistent field (SCF) iterations on unseen molecules.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Generalized Spectral Framework to Expain Neural Scaling and Compression Dynamics</title>
<link>https://arxiv.org/abs/2511.07892</link>
<guid>https://arxiv.org/abs/2511.07892</guid>
<content:encoded><![CDATA[
<div> Keywords: empirical scaling laws, spectral framework, learning dynamics, model compression, spectral-temporal elasticity<br /><br />Summary: This paper addresses empirical scaling laws that characterize how test loss and performance metrics vary with model size, dataset size, and computational resources. It notes that while such laws hold within particular regimes, different scaling behaviors have been observed in related contexts like model compression. To unify these observations, the authors develop a generalized spectral framework grounded in recent advances in spectral analysis of neural representations. This framework extends the traditional linear spectral evolution function \( g(\lambda t) = \lambda t \) to a more general asymptotically polynomial form \( g(\lambda, t; \beta) \), introducing a novel parameter called spectral-temporal elasticity \(\rho(\beta)\). By doing so, it incorporates both lazy training regimes and feature-learning regimes as special cases, thus bridging previous theoretical models. The generalized formulation reveals an invariant relationship linking learning dynamics and compression effects, providing a unified perspective that captures diverse scaling phenomena. This approach helps explain observed discrepancies in scaling behaviors and offers a flexible theoretical tool for analyzing neural network performance across varying conditions and model modifications such as compression. <div>
arXiv:2511.07892v2 Announce Type: replace 
Abstract: Empirical scaling laws describe how test loss and other performance metrics depend on model size, dataset size, and compute. While such laws are consistent within specific regimes, apparently distinct scaling behaviors have been reported for related settings such as model compression. Motivated by recent progress in spectral analyses of neural representations, this paper develops a \emph{generalized spectral framework} that unifies learning dynamics and compression phenomena under a common functional ansatz. We generalize the spectral evolution function from the linear kernel form $g(\lambda t)=\lambda t$ to an asymptotically polynomial function $g(\lambda,t;\beta)$, characterized by an effective spectral--temporal elasticity $\rho(\beta)$. This framework recovers existing lazy and feature-learning theories as special cases and yields an invariant relation between learning and compression
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-driven Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.07904</link>
<guid>https://arxiv.org/abs/2511.07904</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Reward Function, Test-driven Reinforcement Learning, Trajectory Return Function, Multi-objective Optimization

<br /><br />Summary:  
Reinforcement learning (RL) is effective for robot control tasks but poses challenges in designing reward functions, which can lead to suboptimal task representations. To address this issue, the authors introduce a Test-driven Reinforcement Learning (TdRL) framework, utilizing multiple test functions instead of a single reward function. These test functions are categorized as pass-fail tests to define optimal goals and indicative tests to guide the learning process. The paper demonstrates that if a trajectory return function prioritizes trajectories closer to an optimal set, optimizing a maximum entropy policy based on this function results in policies more aligned with the optimal outcomes. Additionally, a lexicographic heuristic is presented for assessing the distance between trajectories and the optimal set for the learning of the trajectory return function. An algorithm for implementing TdRL is developed, and experimental results on the DeepMind Control Suite benchmark show that TdRL matches or surpasses traditional reward methods in training policies. The framework simplifies the design process while intrinsically supporting multi-objective optimization, proposing a fresh perspective for task objective representation to mitigate reward design challenges in RL. <div>
arXiv:2511.07904v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has been recognized as a powerful tool for robot control tasks. RL typically employs reward functions to define task objectives and guide agent learning. However, since the reward function serves the dual purpose of defining the optimal goal and guiding learning, it is challenging to design the reward function manually, which often results in a suboptimal task representation. To tackle the reward design challenge in RL, inspired by the satisficing theory, we propose a Test-driven Reinforcement Learning (TdRL) framework. In the TdRL framework, multiple test functions are used to represent the task objective rather than a single reward function. Test functions can be categorized as pass-fail tests and indicative tests, each dedicated to defining the optimal objective and guiding the learning process, respectively, thereby making defining tasks easier. Building upon such a task definition, we first prove that if a trajectory return function assigns higher returns to trajectories closer to the optimal trajectory set, maximum entropy policy optimization based on this return function will yield a policy that is closer to the optimal policy set. Then, we introduce a lexicographic heuristic approach to compare the relative distance relationship between trajectories and the optimal trajectory set for learning the trajectory return function. Furthermore, we develop an algorithm implementation of TdRL. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL matches or outperforms handcrafted reward methods in policy training, with greater design simplicity and inherent support for multi-objective optimization. We argue that TdRL offers a novel perspective for representing task objectives, which could be helpful in addressing the reward design challenges in RL applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Non-Stationary Time Series Forecasting with Temporal Stabilization and Frequency Differencing</title>
<link>https://arxiv.org/abs/2511.08229</link>
<guid>https://arxiv.org/abs/2511.08229</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, non-stationarity, dual-branch framework, temporal domain, frequency domain<br /><br />Summary:<br /><br />This paper addresses the challenges posed by non-stationary time series data, which are common in dynamic domains like energy, finance, transportation, and cloud computing. It introduces DTAF, a novel dual-branch framework designed to handle non-stationarity both in the temporal and frequency domains for improved long-term forecasting. In the temporal domain, the Temporal Stabilizing Fusion (TFS) module uses a non-stationary mixture of experts (MOE) filter to disentangle and suppress temporal non-stationary patterns while maintaining long-term dependencies. In the frequency domain, the Frequency Wave Modeling (FWM) module leverages frequency differencing to emphasize components exhibiting significant spectral shifts dynamically. By integrating the outputs of TFS and FWM, DTAF can adaptively generate robust forecasts that account for distribution shifts and spectral variability. The approach was tested extensively on real-world benchmarks, where it outperformed state-of-the-art methods, demonstrating superior accuracy under non-stationary conditions. The paper also provides open-source code for reproducibility and further research, which is accessible on GitHub at https://github.com/PandaJunk/DTAF. <div>
arXiv:2511.08229v5 Announce Type: replace 
Abstract: Time series forecasting is critical for decision-making across dynamic domains such as energy, finance, transportation, and cloud computing. However, real-world time series often exhibit non-stationarity, including temporal distribution shifts and spectral variability, which pose significant challenges for long-term time series forecasting. In this paper, we propose DTAF, a dual-branch framework that addresses non-stationarity in both the temporal and frequency domains. For the temporal domain, the Temporal Stabilizing Fusion (TFS) module employs a non-stationary mix of experts (MOE) filter to disentangle and suppress temporal non-stationary patterns while preserving long-term dependencies. For the frequency domain, the Frequency Wave Modeling (FWM) module applies frequency differencing to dynamically highlight components with significant spectral shifts. By fusing the complementary outputs of TFS and FWM, DTAF generates robust forecasts that adapt to both temporal and frequency domain non-stationarity. Extensive experiments on real-world benchmarks demonstrate that DTAF outperforms state-of-the-art baselines, yielding significant improvements in forecasting accuracy under non-stationary conditions. All codes are available at https://github.com/PandaJunk/DTAF.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Global and Local Bounds in Gaussian Process Regression via Chaining</title>
<link>https://arxiv.org/abs/2511.09144</link>
<guid>https://arxiv.org/abs/2511.09144</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian process regression, uncertainty bounds, chaining-based framework, kernel-specific refinements, local uncertainty quantification  

<br /><br />Summary: Gaussian process regression (GPR) is a widely-used nonparametric Bayesian method that offers predictive uncertainty estimates, essential for safety-critical applications. Previous research has presented various uncertainty bounds; however, most of these methods require specific input features and depend on posterior statistics or hyperparameter tuning, which limits robustness and fails to capture the global behavior of the model effectively. To overcome these challenges, this study introduces a chaining-based framework for estimating upper and lower bounds on expected extreme values for unseen data without needing specific input features. The authors provide kernel-specific refinements for commonly used kernels like RBF and Mat√©rn, ensuring tighter bounds compared to generic approaches. Furthermore, they enhance numerical tightness by avoiding analytical relaxations. In addition to global estimations, the study presents a novel technique for local uncertainty quantification at designated inputs, using chaining geometry through partition diameters to adapt to local structures without relying on posterior variance scaling. Experimental results confirm the theoretical findings and demonstrate that the proposed method significantly outperforms existing techniques on both synthetic and real-world datasets. <div>
arXiv:2511.09144v2 Announce Type: replace 
Abstract: Gaussian process regression (GPR) is a popular nonparametric Bayesian method that provides predictive uncertainty estimates and is widely used in safety-critical applications. While prior research has introduced various uncertainty bounds, most existing approaches require access to specific input features, and rely on posterior mean and variance estimates or the tuning of hyperparameters. These limitations hinder robustness and fail to capture the model's global behavior in expectation. To address these limitations, we propose a chaining-based framework for estimating upper and lower bounds on the expected extreme values over unseen data, without requiring access to specific input features. We provide kernel-specific refinements for commonly used kernels such as RBF and Mat\'ern, in which our bounds are tighter than generic constructions. We further improve numerical tightness by avoiding analytical relaxations. In addition to global estimation, we also develop a novel method for local uncertainty quantification at specified inputs. This approach leverages chaining geometry through partition diameters, adapting to local structures without relying on posterior variance scaling. Our experimental results validate the theoretical findings and demonstrate that our method outperforms existing approaches on both synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenePheno: Interpretable Gene Knockout-Induced Phenotype Abnormality Prediction from Gene Sequences</title>
<link>https://arxiv.org/abs/2511.09512</link>
<guid>https://arxiv.org/abs/2511.09512</guid>
<content:encoded><![CDATA[
<div> Genetics, Phenotype Prediction, Gene Knockout, Multi-label Learning, Interpretability<br /><br />Summary:  
This article addresses the challenge of understanding how genetic sequences influence phenotypes, particularly under gene knockout conditions. It highlights current limitations in predicting multiple phenotype abnormalities from gene sequences due to the complexity of gene-phenotype relationships and the reliance on curated genetic data. The authors introduce GenePheno, a novel interpretable multi-label prediction framework that directly predicts knockout-induced phenotypic abnormalities from gene sequences. GenePheno uses a contrastive multi-label learning objective to capture correlations between multiple phenotypes and applies exclusive regularization to maintain biological consistency in predictions. Additionally, it incorporates a gene function bottleneck layer that provides human interpretable concepts linking gene functions to phenotypic outcomes. To facilitate research in this domain, the study curates four datasets pairing canonical gene sequences with phenotypic abnormalities caused by gene knockouts. Experimental results show that GenePheno achieves state-of-the-art performance on both gene-centric metrics (F_max) and phenotype-centric metrics (AUC). Finally, case studies demonstrate GenePheno‚Äôs ability to reveal underlying gene functional mechanisms driving phenotype formation, offering interpretable insights that may guide future biological experimentation and scalable hypothesis testing. <div>
arXiv:2511.09512v2 Announce Type: replace 
Abstract: Exploring how genetic sequences shape phenotypes is a fundamental challenge in biology and a key step toward scalable, hypothesis-driven experimentation. The task is complicated by the large modality gap between sequences and phenotypes, as well as the pleiotropic nature of gene-phenotype relationships. Existing sequence-based efforts focus on the degree to which variants of specific genes alter a limited set of phenotypes, while general gene knockout induced phenotype abnormality prediction methods heavily rely on curated genetic information as inputs, which limits scalability and generalizability. As a result, the task of broadly predicting the presence of multiple phenotype abnormalities under gene knockout directly from gene sequences remains underexplored. We introduce GenePheno, the first interpretable multi-label prediction framework that predicts knockout induced phenotypic abnormalities from gene sequences. GenePheno employs a contrastive multi-label learning objective that captures inter-phenotype correlations, complemented by an exclusive regularization that enforces biological consistency. It further incorporates a gene function bottleneck layer, offering human interpretable concepts that reflect functional mechanisms behind phenotype formation. To support progress in this area, we curate four datasets with canonical gene sequences as input and multi-label phenotypic abnormalities induced by gene knockouts as targets. Across these datasets, GenePheno achieves state-of-the-art gene-centric $F_{\text{max}}$ and phenotype-centric AUC, and case studies demonstrate its ability to reveal gene functional mechanisms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Trilemma of Truth in Large Language Models</title>
<link>https://arxiv.org/abs/2506.23921</link>
<guid>https://arxiv.org/abs/2506.23921</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, truth probing, sAwMIL, multiple-instance learning, veracity classification  

<br /><br />Summary:  
This study challenges the common perception that large language models (LLMs) "know" facts in a human-like manner, clarifying that they store information as probabilistic internal knowledge formed during training. It critically evaluates current probing methods used to determine the truthfulness of statements derived from LLM activations, revealing flawed assumptions underlying these approaches. To overcome these limitations, the authors present sAwMIL (Sparse-Aware Multiple-Instance Learning), a novel multiclass framework that integrates multiple-instance learning with conformal prediction to classify statements into true, false, or neither categories by leveraging LLM internal states. The method was tested on 16 open-source LLMs, including both default and chat-based models, using three new carefully curated datasets. Key findings include (1) existing probing techniques generally fail to deliver reliable and generalizable directions for veracity and sometimes perform worse than zero-shot prompting; (2) the models encode truth and falsehood asymmetrically rather than in a balanced fashion; and (3) LLMs capture a third distinct signal separate from both true and false, suggesting a more complex internal representation of knowledge than previously recognized. <div>
arXiv:2506.23921v4 Announce Type: replace-cross 
Abstract: The public often attributes human-like qualities to large language models (LLMs) and assumes they "know" certain things. In reality, LLMs encode information retained during training as internal probabilistic knowledge. This study examines existing methods for probing the veracity of that knowledge and identifies several flawed underlying assumptions. To address these flaws, we introduce sAwMIL (Sparse-Aware Multiple-Instance Learning), a multiclass probing framework that combines multiple-instance learning with conformal prediction. sAwMIL leverages internal activations of LLMs to classify statements as true, false, or neither. We evaluate sAwMIL across 16 open-source LLMs, including default and chat-based variants, on three new curated datasets. Our results show that (1) common probing methods fail to provide a reliable and transferable veracity direction and, in some settings, perform worse than zero-shot prompting; (2) truth and falsehood are not encoded symmetrically; and (3) LLMs encode a third type of signal that is distinct from both true and false.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amorphous Solid Model of Vectorial Hopfield Neural Networks</title>
<link>https://arxiv.org/abs/2507.22787</link>
<guid>https://arxiv.org/abs/2507.22787</guid>
<content:encoded><![CDATA[
<div> Keywords: Hopfield model, vectorial extension, associative memory, spectral gap, connectivity  

<br /><br />Summary:  
This paper presents a three-dimensional vectorial extension of the Hopfield associative-memory model, where each neuron is represented by a unit vector on the sphere \( S^2 \). The synaptic connections are structured as \( 3 \times 3 \) blocks, formed through a vectorial Hebbian learning rule. The derived block-structured operator is analogous to the Hessian used in the study of amorphous solids, creating a rigid energy landscape with pronounced minima for the stored patterns. The authors demonstrate through simulations and spectral analysis that this vectorial model significantly outperforms the traditional binary Hopfield model. They find that the critical storage ratio \( \gamma_c \) increases roughly linearly with the coordination number \( Z \) in moderate connectivity scenarios. When \( Z \) exceeds approximately 40, a high-connectivity regime emerges where \( \gamma_c \) consistently surpasses the low \( Z \) linear extrapolation. Additionally, a persistent spectral gap is observed, which facilitates the separation of pattern modes from the bulk, leading to larger basins of attraction and enhanced resilience against initialization noise. The results indicate that integrating geometric constraints with amorphous-solid-like structures yields superior associative memory performance, particularly for high-connectivity ( \( Z \gtrsim 20-30 \) ) scenarios. <div>
arXiv:2507.22787v4 Announce Type: replace-cross 
Abstract: We introduce a three-dimensional vectorial extension of the Hopfield associative-memory model in which each neuron is a unit vector on $S^2$ and synaptic couplings are $3\times 3$ blocks generated through a vectorial Hebbian rule. The resulting block-structured operator is mathematically analogous to the Hessian of amorphous solids and induces a rigid energy landscape with deep minima for stored patterns. Simulations and spectral analysis show that the vectorial network substantially outperforms the classical binary Hopfield model. For moderate connectivity, the critical storage ratio $\gamma_c$ grows approximately linearly with the coordination number $Z$, while for $Z\gtrsim 40$ a high-connectivity regime emerges in which $\gamma_c$ systematically exceeds the extrapolated low-$Z$ linear fit. At the same time, a persistent spectral gap separates pattern modes from the bulk and basins of attraction enlarge, yielding enhanced robustness to initialization noise. Thus geometric constraints combined with amorphous-solid-inspired structure produce associative memories with superior storage and retrieval performance, especially in the high-connectivity ($Z \gtrsim 20$-$30$) regime.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably data-driven projection method for quadratic programming</title>
<link>https://arxiv.org/abs/2509.04524</link>
<guid>https://arxiv.org/abs/2509.04524</guid>
<content:encoded><![CDATA[
<div> Keywords: projection methods, convex quadratic programs, active set, learning guarantees, dimensionality reduction

<br /><br />Summary: This article discusses projection methods designed to enhance the scalability of high-dimensional optimization problems by reducing their dimensionality. It focuses on a recent data-driven approach for linear programs (LPs) proposed by Sakaue and Oki, which learns the projection matrix from observed problem instances. The authors analyze this approach's generalization guarantees when applied to convex quadratic programs (QPs). Unlike LPs, the optimal solutions of convex QPs are not limited to the vertices of their feasible regions, complicating the analysis of their optimal value function. To address this, the authors utilize Caratheodory's theorem to show that convex QP solutions can be localized within a feasible region defined by a specific active set. They introduce the unrolled active set method, which models the optimal value computation as a Goldberg-Jerrum algorithm with bounded complexities, providing learning guarantees. Furthermore, the analysis extends to various scenarios, including frameworks for learning to achieve optimal solutions and an input-aware approach that learns mappings from QP instances to projection matrices. This research establishes a foundation for improving QP optimization through data-driven methodologies. <div>
arXiv:2509.04524v3 Announce Type: replace-cross 
Abstract: Projection methods aim to reduce the dimensionality of the optimization instance, thereby improving the scalability of high-dimensional problems. Recently, Sakaue and Oki proposed a data-driven approach for linear programs (LPs), where the projection matrix is learned from observed problem instances drawn from an application-specific distribution of problems. We analyze the generalization guarantee for the data-driven projection matrix learning for convex quadratic programs (QPs). Unlike in LPs, the optimal solutions of convex QPs are not confined to the vertices of the feasible polyhedron, and this complicates the analysis of the optimal value function. To overcome this challenge, we demonstrate that the solutions of convex QPs can be localized within a feasible region corresponding to a special active set, utilizing Caratheodory's theorem. Building on such observation, we propose the unrolled active set method, which models the computation of the optimal value as a Goldberg-Jerrum (GJ) algorithm with bounded complexities, thereby establishing learning guarantees. We then further extend our analysis to other settings, including learning to match the optimal solution and input-aware setting, where we learn a mapping from QP problem instances to projection matrices.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperellipsoid Density Sampling: Exploitative Sequences to Accelerate High-Dimensional Optimization</title>
<link>https://arxiv.org/abs/2511.07836</link>
<guid>https://arxiv.org/abs/2511.07836</guid>
<content:encoded><![CDATA[
<div> Keywords: Curse of dimensionality, Adaptive sampling, Hyperellipsoid Density Sampling, Quasi-Monte Carlo, High-dimensional optimization<br /><br />Summary:<br /><br />The article addresses the challenge posed by the curse of dimensionality in optimization, where the search space expands exponentially, making traditional algorithms inefficient. It introduces an adaptive sampling technique called Hyperellipsoid Density Sampling (HDS), designed to accelerate optimization in high-dimensional spaces by providing an alternative to uniform quasi-Monte Carlo (QMC) sampling methods. HDS works by defining multiple hyperellipsoids throughout the parameter space and employs three types of unsupervised learning algorithms to avoid expensive high-dimensional geometric computations. This results in an intelligent, non-uniform sampling sequence that targets statistically promising regions, enhancing the quality of final optimization solutions. A notable feature of HDS is the option to incorporate Gaussian weights, allowing the sample distribution to focus on known areas of interest, thus broadening its applicability beyond optimization tasks. The method was benchmarked against the Sobol QMC method using differential evolution on 29 CEC2017 test functions. The experimental results demonstrated statistically significant improvements in solution accuracy, with p-values less than 0.05, and performance gains ranging from 3% in 30 dimensions to 37% in 10 dimensions. Overall, the paper establishes HDS as a robust and effective alternative to traditional QMC techniques for solving high-dimensional optimization problems. <div>
arXiv:2511.07836v3 Announce Type: replace-cross 
Abstract: The curse of dimensionality presents a pervasive challenge in optimization problems, with exponential expansion of the search space rapidly causing traditional algorithms to become inefficient or infeasible. An adaptive sampling strategy is presented to accelerate optimization in this domain as an alternative to uniform quasi-Monte Carlo (QMC) methods.
  This method, referred to as Hyperellipsoid Density Sampling (HDS), generates its sequences by defining multiple hyperellipsoids throughout the search space. HDS uses three types of unsupervised learning algorithms to circumvent high-dimensional geometric calculations, producing an intelligent, non-uniform sample sequence that exploits statistically promising regions of the parameter space and improves final solution quality in high-dimensional optimization problems.
  A key feature of the method is optional Gaussian weights, which may be provided to influence the sample distribution towards known locations of interest. This capability makes HDS versatile for applications beyond optimization, providing a focused, denser sample distribution where models need to concentrate their efforts on specific, non-uniform regions of the parameter space.
  The method was evaluated against Sobol, a standard QMC method, using differential evolution (DE) on the 29 CEC2017 benchmark test functions. The results show statistically significant improvements in solution geometric mean error (p < 0.05), with average performance gains ranging from 3% in 30D to 37% in 10D. This paper demonstrates the efficacy of HDS as a robust alternative to QMC sampling for high-dimensional optimization.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks</title>
<link>https://arxiv.org/abs/2511.07947</link>
<guid>https://arxiv.org/abs/2511.07947</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, model extraction attacks, watermarking, Class-Feature Watermarks, resilience  

<br /><br />Summary: Machine learning models are valuable assets but can be subjected to model extraction attacks (MEA) where adversaries replicate their functionality. To counter MEAs, model watermarking is employed, embedding markers for ownership verification. However, existing watermarks focus mainly on surviving MEAs by using representation entanglement but fail to address resilience against sequential MEAs and removal attacks. This study uncovers the underestimation of risks as current removal techniques are vulnerable due to entanglement. To address this, the authors introduce Watermark Removal attacK (WRK), which effectively diminishes watermark success rates by at least 88.79% across various benchmarks. For enhanced protection, they propose Class-Feature Watermarks (CFW) that utilize class-level artifacts and create synthetic classes using out-of-domain samples. This approach helps eliminate weak decision boundaries between original and artifact-modified samples. CFW optimizes the transferability of MEA and ensures stability after MEA outcomes. Experimental results across different domains indicate that CFW outperforms existing methods in resilience, maintaining a watermark success rate of at least 70.15% even under both MEA and WRK distortions, while also preserving the utility of the protected models. <div>
arXiv:2511.07947v2 Announce Type: replace-cross 
Abstract: Machine learning models constitute valuable intellectual property, yet remain vulnerable to model extraction attacks (MEA), where adversaries replicate their functionality through black-box queries. Model watermarking counters MEAs by embedding forensic markers for ownership verification. Current black-box watermarks prioritize MEA survival through representation entanglement, yet inadequately explore resilience against sequential MEAs and removal attacks. Our study reveals that this risk is underestimated because existing removal methods are weakened by entanglement. To address this gap, we propose Watermark Removal attacK (WRK), which circumvents entanglement constraints by exploiting decision boundaries shaped by prevailing sample-level watermark artifacts. WRK effectively reduces watermark success rates by at least 88.79% across existing watermarking benchmarks.
  For robust protection, we propose Class-Feature Watermarks (CFW), which improve resilience by leveraging class-level artifacts. CFW constructs a synthetic class using out-of-domain samples, eliminating vulnerable decision boundaries between original domain samples and their artifact-modified counterparts (watermark samples). CFW concurrently optimizes both MEA transferability and post-MEA stability. Experiments across multiple domains show that CFW consistently outperforms prior methods in resilience, maintaining a watermark success rate of at least 70.15% in extracted models even under the combined MEA and WRK distortion, while preserving the utility of protected models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Negative Flips via Margin Preserving Training</title>
<link>https://arxiv.org/abs/2511.08322</link>
<guid>https://arxiv.org/abs/2511.08322</guid>
<content:encoded><![CDATA[
<div> Keywords: AI system, negative flips, margin-calibration, image classification, focal distillation loss

<br /><br />Summary: The paper addresses the challenge of minimizing inconsistencies across versions of AI models, focusing on image classification. A significant issue identified is "negative flips," where updated models misclassify previously correctly identified samples, particularly as the number of training classes increases. The authors propose a novel method to mitigate negative flips by preserving the margins of the original model while allowing the learning of new classes. This approach involves introducing a margin-calibration term that maintains a larger relative margin between old and new classes. However, the proposed method also recognizes the potential negative impact on the accuracy of new classes if their logit margins are overly constrained, compared to an independently trained model. To balance this, the approach integrates a double-source focal distillation loss that draws from both the previous model and a new independently trained model. This enables the learning of an appropriate decision margin from both old and new data, even with logit margin calibration in place. Extensive experimental results on various image classification benchmarks indicate that this approach effectively reduces the rate of negative flips while maintaining high overall accuracy. <div>
arXiv:2511.08322v2 Announce Type: replace-cross 
Abstract: Minimizing inconsistencies across successive versions of an AI system is as crucial as reducing the overall error. In image classification, such inconsistencies manifest as negative flips, where an updated model misclassifies test samples that were previously classified correctly. This issue becomes increasingly pronounced as the number of training classes grows over time, since adding new categories reduces the margin of each class and may introduce conflicting patterns that undermine their learning process, thereby degrading performance on the original subset. To mitigate negative flips, we propose a novel approach that preserves the margins of the original model while learning an improved one. Our method encourages a larger relative margin between the previously learned and newly introduced classes by introducing an explicit margin-calibration term on the logits. However, overly constraining the logit margin for the new classes can significantly degrade their accuracy compared to a new independently trained model. To address this, we integrate a double-source focal distillation loss with the previous model and a new independently trained model, learning an appropriate decision margin from both old and new data, even under a logit margin calibration. Extensive experiments on image classification benchmarks demonstrate that our approach consistently reduces the negative flip rate with high overall accuracy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</title>
<link>https://arxiv.org/abs/2511.09057</link>
<guid>https://arxiv.org/abs/2511.09057</guid>
<content:encoded><![CDATA[
<div> Keywords: world model, video generation, action-conditioned, long-horizon, simulation  

<br /><br />Summary: The article presents PAN, a novel world model designed for intelligent agents that enables them to predict and reason about future states based on their actions. Unlike existing video generation models, which typically produce sequences in a linear fashion without causal control or long-term coherence, PAN allows for interactive and effective long-horizon simulations. It addresses the limitations of current world modeling efforts that focus on narrow domains by providing a generalizable approach that spans diverse environments. PAN integrates the Generative Latent Prediction (GLP) architecture, which utilizes an autoregressive latent dynamics backbone powered by a large language model. This approach grounds its simulations in a wealth of text-based knowledge and allows conditioning on language-specified actions. Additionally, PAN incorporates a video diffusion decoder to produce high-quality, temporally coherent visual outputs. Trained on extensive video-action pair datasets, PAN excels in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning, outperforming existing video generators and world models. The work represents a significant advancement towards creating general world models that support predictive simulation for intelligent reasoning and action. <div>
arXiv:2511.09057v3 Announce Type: replace-cross 
Abstract: A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Model Training to Model Raising</title>
<link>https://arxiv.org/abs/2511.09287</link>
<guid>https://arxiv.org/abs/2511.09287</guid>
<content:encoded><![CDATA[
<div> Keywords: AI alignment, model raising, training corpus, social interactions, human values

<br /><br />Summary: The article critiques current AI training methods, which typically align models with human values only after core capabilities are established, leading to potential misalignment. It introduces a new paradigm called "model raising," where alignment is integrated from the outset of a model's development. Key components include redesigning the training corpus by reframing training data from a first-person perspective, recontextualizing information as lived experiences, simulating social interactions, and carefully scaffolding the ordering of training data. This approach aims for an early commitment to values from the very first training token, making the intertwining of knowledge, skills, and values more intrinsic and harder to separate. The authors argue that in a rapidly evolving landscape where large language models may surpass human capabilities in various tasks, implementing this paradigm shift is essential for fostering AI systems that inherently align with human values throughout their learning process. <div>
arXiv:2511.09287v2 Announce Type: replace-cross 
Abstract: Current AI training methods align models with human values only after their core capabilities have been established, resulting in models that are easily misaligned and lack deep-rooted value systems. We propose a paradigm shift from "model training" to "model raising", in which alignment is woven into a model's development from the start. We identify several key components for this paradigm, all centered around redesigning the training corpus: reframing training data from a first-person perspective, recontextualizing information as lived experience, simulating social interactions, and scaffolding the ordering of training data. We expect that this redesign of the training corpus will lead to an early commitment to values from the first training token onward, such that knowledge, skills, and values are intrinsically much harder to separate. In an ecosystem in which large language model capabilities start overtaking human capabilities in many tasks, this seems to us like a critical need.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Softmax as a Lagrangian-Legendrian Seam</title>
<link>https://arxiv.org/abs/2511.11573</link>
<guid>https://arxiv.org/abs/2511.11573</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, differential geometry, softmax, entropy, information geometry

<br /><br />Summary: This article establishes a connection between machine learning and modern differential geometry, specifically through the logits-to-probabilities transition achieved by the softmax function. The authors model this transition as a geometric interface where two potential-generated, conservative descriptions converge along a Legendrian "seam" within a contact screen, represented by the probability simplex, situated in a folded symplectic collar. They identify bias-shift invariance as Reeb flow along the contact screen. Additionally, they introduce the Fenchel-Young equality and KL gap as a computable measure of distance to this seam. The paper offers concrete examples in the contexts of two- and three-class classification problems to illustrate their concepts. Moreover, the authors suggest future directions for machine learning research, including the development of compact logit models (both projective and spherical) and the exploration of global invariants. They also emphasize their potential connections to information geometry, where the dynamics occurring in the described geometric framework can be understood as replicator flows. This work serves as a significant initial step towards bridging machine learning with geometric principles. <div>
arXiv:2511.11573v1 Announce Type: new 
Abstract: This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian "seam" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora</title>
<link>https://arxiv.org/abs/2511.11574</link>
<guid>https://arxiv.org/abs/2511.11574</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Knowledge Distillation, Active Learning, M-RARU, Classification Accuracy

<br /><br />Summary:  
Large Language Models (LLMs) are effective in classification tasks but are hindered by high computational and financial costs, particularly in dynamic settings. Knowledge Distillation (KD) is a method where a larger "teacher" model trains a smaller "student" model to reduce costs. However, the distillation process can be expensive as it requires the teacher to label many samples, leading to excessive token consumption. To address this issue, the authors introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel Active Learning (AL) approach designed to create efficient student models at a lower cost while maintaining LLM performance. M-RARU strategically combines uncertainty with a randomized accept-reject mechanism, selecting only the most informative data points, thereby minimizing API calls and data processing times. The effectiveness of M-RARU is evaluated against random sampling using five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) across multiple benchmark datasets. The experimental results reveal that M-RARU can achieve up to an 80% reduction in sample requirements compared to random sampling, significantly enhancing classification accuracy while decreasing financial costs and overall training duration. <div>
arXiv:2511.11574v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM "teacher" trains a smaller and more efficient "student" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms</title>
<link>https://arxiv.org/abs/2511.11575</link>
<guid>https://arxiv.org/abs/2511.11575</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, algorithmic fairness, statistical significance, recidivism forecasting, bias

<br /><br />Summary: 
The paper addresses the growing concern over algorithmic fairness as machine learning algorithms are adopted in critical sectors like finance and healthcare. It critiques the current literature for lacking methods to determine whether disparities between privileged and protected groups are statistically significant or merely coincidental. To address this gap, the authors provide a robust framework that employs k-fold cross-validation to establish sampling distributions of fairness metrics. This framework is complemented by statistical tests that help identify meaningful violations of fairness, examining disparities in predicted vs. actual outcomes, model calibration, and employing causal inference techniques. The authors apply their methodology to recidivism forecasting algorithms sourced from the National Institute of Justice. Their findings indicate a statistically significant bias against Black individuals across multiple fairness definitions, while revealing varied results concerning White individuals. This underscores the necessity for thorough statistical evaluations when judging the fairness of algorithmic decision-making systems. The paper emphasizes that a rigorous approach is essential to ensure fairness in AI applications, ultimately advancing the discourse around ethical AI practices. <div>
arXiv:2511.11575v1 Announce Type: new 
Abstract: Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs</title>
<link>https://arxiv.org/abs/2511.11576</link>
<guid>https://arxiv.org/abs/2511.11576</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, optimization modeling, uncertainty, DAOpt framework, few-shot learning  

<br /><br />Summary:  
Recent advancements in large language models (LLMs) have significantly propelled research in automated optimization modeling. Traditionally, much of the focus has been on deterministic optimization with well-known parameters, neglecting the complexities introduced by uncertainty in real-world decision-making. This paper proposes the DAOpt framework, which addresses this gap by introducing a novel dataset called OptU tailored for uncertain settings. Additionally, the framework encompasses a multi-agent decision-making module and a simulation environment designed to evaluate LLMs on criteria such as out-of-sample feasibility and robustness. By integrating few-shot learning techniques, the authors enhance the modeling prowess of LLMs by incorporating domain knowledge derived from stochastic and robust optimization. This holistic approach not only expands the applicability of LLMs in uncertain decision-making scenarios but also sets the stage for future studies to further explore and leverage LLM capabilities in optimization. <div>
arXiv:2511.11576v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Positional and Symbolic Attention Behavior in Transformers</title>
<link>https://arxiv.org/abs/2511.11579</link>
<guid>https://arxiv.org/abs/2511.11579</guid>
<content:encoded><![CDATA[
<div> Keywords: Rotary Positional Encoding, Transformers, attention heads, positional information, model behavior

<br /><br />Summary: The paper investigates the encoding of positional and symbolic information in Transformers, focusing specifically on Rotary Positional Encoding (RoPE) and its effectiveness. It introduces general definitions for classifying attention head behavior as positional or symbolic, proving that these behaviors are mutually exclusive. A new metric is developed to quantify these behaviors, allowing for a deeper analysis of Transformer-based large language models (LLMs) that utilize RoPE. The study finds a strong correspondence between the behavior of attention heads and their frequency usage. To further explore these behaviors, the authors design canonical tasks that are purely positional or symbolic, revealing a causal relationship between Transformer performance and the appropriate leveraging of frequencies by attention heads. The results indicate that regulating access to specific frequencies can control the performance of the Transformer. Overall, the work deepens the understanding of RoPE and its implications for Transformer model behavior, demonstrating the importance of differentiating between positional and symbolic information during language processing. <div>
arXiv:2511.11579v1 Announce Type: new 
Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Anatomy of a Triton Attention Kernel</title>
<link>https://arxiv.org/abs/2511.11581</link>
<guid>https://arxiv.org/abs/2511.11581</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM inference, portability, Triton, GPU performance, attention kernel

<br /><br />Summary: This work presents advancements in developing a portable and efficient LLM inference platform that works across various hardware architectures without requiring low-level tuning. The authors focus on a critical component known as the paged attention kernel, developing a state-of-the-art implementation using the Triton language, which is designed for just-in-time compilation. They demonstrate that their kernel achieves impressive performance metrics on both NVIDIA and AMD GPUs. The study details the high-level approach taken, highlighting algorithmic and system-level enhancements that contribute to the performance boost. Additionally, they discuss the parameter auto-tuning process necessary to fully exploit the system's efficiency. The researchers also integrate their findings into a widely used inference server, resulting in a remarkable improvement of the Triton attention kernel's performance from 19.7% to 105.9% compared to the state-of-the-art. The outcomes emphasize the potential of open-source domain-specific languages like Triton in facilitating model portability and achieving superior performance across different GPU vendors, showcasing the significant impact this development can have in both academic and industrial applications of LLMs. <div>
arXiv:2511.11581v1 Announce Type: new 
Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations</title>
<link>https://arxiv.org/abs/2511.11583</link>
<guid>https://arxiv.org/abs/2511.11583</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Financial Recommendations, Knowledge Graphs, Retrieval-Augmented Generation, Behavioral Alignment

<br /><br />Summary:  
This paper addresses limitations of large language models (LLMs) in personalized financial recommendations, particularly their restricted context window, hallucination issues, and insufficient behavioral grounding. Building on the authors' prior work, FLARKO, which integrates structured knowledge graphs (KGs) into LLM prompts to better align advice with user behavior and market data, the authors introduce RAG-FLARKO, a retrieval-augmented extension designed to enhance scalability and relevance. RAG-FLARKO employs a multi-stage and parallel KG retrieval approach: it first extracts behaviorally relevant entities from a user‚Äôs transaction KG, then filters temporally consistent signals from a market KG using this retrieved context. This process constructs a compact, contextually grounded subgraph supplied to the LLM, reducing context overhead while focusing the model on pertinent information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly improves recommendation quality. Importantly, the framework allows smaller and more efficient language models to perform effectively in terms of profitability and behavioral alignment. This makes RAG-FLARKO a promising approach for deploying grounded financial AI systems in environments with limited computational resources. <div>
arXiv:2511.11583v1 Announce Type: new 
Abstract: Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Output Supervision Can Obfuscate the Chain of Thought</title>
<link>https://arxiv.org/abs/2511.11584</link>
<guid>https://arxiv.org/abs/2511.11584</guid>
<content:encoded><![CDATA[
<div> Obfuscated chain of thought, monitorability, safe-looking outputs, training generalization, mitigation strategies  

<br /><br />Summary:  
OpenAI (2025) identified that training language models against a chain of thought (CoT) monitor can result in obfuscated CoTs that hide undesirable behavior, which the monitor fails to detect. To counter this, OpenAI proposed restricting training to output monitors that do not access CoTs, aiming to keep CoTs monitorable. This paper demonstrates that such a training approach can still produce obfuscated CoTs through two mechanisms. First, models trained to generate safe-looking outputs may generalize this safety to their CoTs, causing the reasoning itself to appear safe despite underlying issues. Second, because later tokens depend on earlier ones, the presence of safe-looking CoTs can increase the likelihood of safe final outputs, effectively reinforcing the creation of deceptively safe CoTs. To address these challenges, the authors introduce two mitigation techniques targeting these specific mechanisms. These mitigations lead to an improved balance, achieving a Pareto improvement by enhancing both the monitorability of CoTs and the model‚Äôs task performance compared to standard training methods. This work highlights the nuanced risks involved in training with CoT monitors and proposes practical solutions to maintain transparency and safety in model reasoning processes. <div>
arXiv:2511.11584v1 Announce Type: new 
Abstract: OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge</title>
<link>https://arxiv.org/abs/2511.11585</link>
<guid>https://arxiv.org/abs/2511.11585</guid>
<content:encoded><![CDATA[
<div> Keywords: FedGen-Edge, federated learning, generative models, Low-Rank Adaptation, personalization

<br /><br />Summary: The paper introduces FedGen-Edge, a framework designed to enhance the training and adaptation of large generative models in federated settings, addressing challenges posed by computation, communication, and statistical/system heterogeneity. The approach utilizes a frozen, pre-trained global backbone along with lightweight client-side adapters, allowing only the adapters to be federated. By employing Low-Rank Adaptation (LoRA), client updates are constrained to a compact subspace, which significantly reduces uplink traffic by over 99% compared to traditional full-model FedAvg. Additionally, the method stabilizes aggregation in scenarios involving non-IID data and supports personalization, as each client retains a locally tuned adapter. The framework demonstrates lower perplexity and Fr√©chet Inception Distance (FID) scores along with faster convergence on language modeling (PTB) and image generation (CIFAR-10) tasks compared to strong existing baselines while maintaining the simplicity of a FedAvg-style server. An ablation study indicates that there are diminishing returns beyond moderate LoRA rank and highlights a trade-off between local training epochs and client drift. Overall, FedGen-Edge presents a viable solution for deploying privacy-preserving, resource-efficient, and personalized generative AI on diverse edge devices. <div>
arXiv:2511.11585v1 Announce Type: new 
Abstract: Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation</title>
<link>https://arxiv.org/abs/2511.11589</link>
<guid>https://arxiv.org/abs/2511.11589</guid>
<content:encoded><![CDATA[
<div> wildfire risk, machine learning, Random Forest, wildfire indicators, interpretability  

<br /><br />Summary:  
This paper presents WildfireGenome, a novel approach to wildfire risk assessment that improves interpretability and decision-scale relevance compared to existing coarse hazard maps and opaque machine learning models. First, it fuses seven federal wildfire risk indicators into a sign-aligned composite risk label using principal component analysis (PCA) at fine spatial resolution (H3 Level-8). Second, a Random Forest classifier is trained on these labels to predict local wildfire risk across ecologically diverse U.S. counties with accuracy ranging from 0.755 to 0.878 and Quadratic Weighted Kappa up to 0.951, while principal components explain 87-94% of indicator variance. Third, interpretability is enhanced through SHAP and ICE/PDP analyses, revealing county-specific nonlinear relationships between wildfire risk and environmental drivers. Across regions, needleleaf forest cover and elevation consistently emerge as dominant risk drivers, with wildfire risk increasing steeply when needleleaf coverage exceeds 30-40%. Model transferability tests indicate reliable performance between ecologically similar regions but significant degradation in unrelated contexts. Overall, WildfireGenome advances wildfire risk assessment by providing interpretable, locally detailed analytics that can inform vegetation management, zoning policies, and infrastructure planning to better mitigate wildfire hazards. <div>
arXiv:2511.11589v1 Announce Type: new 
Abstract: Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL</title>
<link>https://arxiv.org/abs/2511.11592</link>
<guid>https://arxiv.org/abs/2511.11592</guid>
<content:encoded><![CDATA[
<div> Keywords: maximum entropy, reinforcement learning, trajectory entropy, Q-value estimation, off-policy algorithm  

<br /><br />Summary:  
This paper addresses key limitations in maximum entropy reinforcement learning, specifically the instability caused by jointly updating Q-values and the temperature parameter, and the short-sighted nature of local entropy tuning. To overcome these, the authors propose the Trajectory Entropy-Constrained Reinforcement Learning (TECRL) framework, which introduces separate Q-functions for rewards and entropy. This separation stabilizes value target estimation by decoupling temperature updates from reward learning. Additionally, the entropy Q-function quantifies expected cumulative entropy, enabling the imposition of a trajectory-level entropy constraint that controls policy stochasticity over the long term, rather than just at individual steps. Based on TECRL, the authors develop an off-policy algorithm called DSAC-E by refining the distributional soft actor-critic method with three specific improvements, termed DSAC-T. Experimental evaluation on OpenAI Gym benchmarks demonstrates that DSAC-E outperforms existing methods by achieving higher returns and improved training stability. Overall, this work extends maximum entropy RL frameworks by addressing non-stationarity and local entropy tuning issues through trajectory-level entropy considerations and separate Q-function learning, resulting in a more stable and effective off-policy RL algorithm. <div>
arXiv:2511.11592v1 Announce Type: new 
Abstract: Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sound Logical Explanations for Mean Aggregation Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.11593</link>
<guid>https://arxiv.org/abs/2511.11593</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, knowledge graph completion, mean aggregation, non-negative weights, logical rules<br /><br />Summary:<br /><br />This paper studies graph neural networks (GNNs) that use mean aggregation functions with non-negative weights, referred to as MAGNNs, in the context of knowledge graph completion. The authors identify the exact class of monotonic logical rules that can soundly explain MAGNN predictions, addressing a gap in explainability for mean-aggregation GNNs. They also define a restricted fragment of first-order logic that can characterize any prediction made by MAGNNs, providing a theoretical basis for understanding these models. Experimentally, the paper shows that constraining mean-aggregation GNNs to non-negative weights results in performance that is comparable or better on standard inductive benchmarks, suggesting that this restriction does not harm predictive capability. Moreover, the study demonstrates that sound logical rules can indeed be extracted in practical scenarios, enabling insightful explanations for the model's behavior. Finally, the extracted sound rules can reveal weaknesses or issues in the trained models, offering a tool for diagnosing and improving GNNs. This work contributes both theoretical insights and practical techniques for enhancing the transparency and trustworthiness of GNNs in knowledge graph completion tasks. <div>
arXiv:2511.11593v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Loss Given Default Prediction Under Measurement-Induced Mixture Distributions: An Information-Theoretic Approach</title>
<link>https://arxiv.org/abs/2511.11596</link>
<guid>https://arxiv.org/abs/2511.11596</guid>
<content:encoded><![CDATA[
<div> Keywords: Loss Given Default, LGD modeling, recursive partitioning, mutual information, financial institutions

<br /><br />Summary: Loss Given Default (LGD) modeling faces a significant data quality challenge, as 90% of available training data consists of proxy estimates instead of actual recovery outcomes from bankruptcies. This mixture-contaminated training data leads to systematic failures in recursive partitioning methods, illustrated by Random Forest achieving a negative r-squared of -0.664 on held-out test data. In contrast, information-theoretic approaches that utilize Shannon entropy and mutual information demonstrate better generalization, with an r-squared of 0.191 and RMSE of 0.284 over 1,218 corporate bankruptcies spanning from 1980 to 2023. The analysis highlights that leverage-based features provide 1.510 bits of mutual information, while size effects contribute a mere 0.086 bits, challenging regulatory assumptions about scale-dependent recovery. The findings offer practical guidance for financial institutions deploying LGD models under Basel III requirements when sufficient representative outcome data is scarce. Additionally, the results have implications for other fields, such as medical outcomes research, climate forecasting, and technology reliability, where extended observation periods can lead to similar mixture structures in training data. <div>
arXiv:2511.11596v1 Announce Type: new 
Abstract: Loss Given Default (LGD) modeling faces a fundamental data quality constraint: 90% of available training data consists of proxy estimates based on pre-distress balance sheets rather than actual recovery outcomes from completed bankruptcy proceedings. We demonstrate that this mixture-contaminated training structure causes systematic failure of recursive partitioning methods, with Random Forest achieving negative r-squared (-0.664, worse than predicting the mean) on held-out test data. Information-theoretic approaches based on Shannon entropy and mutual information provide superior generalization, achieving r-squared of 0.191 and RMSE of 0.284 on 1,218 corporate bankruptcies (1980-2023). Analysis reveals that leverage-based features contain 1.510 bits of mutual information while size effects contribute only 0.086 bits, contradicting regulatory assumptions about scale-dependent recovery. These results establish practical guidance for financial institutions deploying LGD models under Basel III requirements when representative outcome data is unavailable at sufficient scale. The findings generalize to medical outcomes research, climate forecasting, and technology reliability-domains where extended observation periods create unavoidable mixture structure in training data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games</title>
<link>https://arxiv.org/abs/2511.11602</link>
<guid>https://arxiv.org/abs/2511.11602</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, Nash equilibria, Aspiration-based learning, Stochastic stability, Multi-player games

<br /><br />Summary: This paper addresses the limitations of reinforcement-based learning in distributed setups, particularly in multi-player weakly-acyclic games. While such learning methods can effectively filter out noise, they struggle to guarantee convergence to desirable pure Nash equilibria when players act independently. Previous research has mainly focused on potential and coordination games, leaving a gap in understanding broader game dynamics. To tackle this, the authors introduce aspiration-based perturbed learning automata (APLA), a novel payoff-based learning scheme designed for distributed optimization. In APLA, players adjust their action selection probabilities based on a combination of past selections and an aspiration factor that reflects their satisfaction. The study conducts a stochastic stability analysis of APLA in the context of multi-player positive-utility games while accounting for noisy observations. Importantly, the paper establishes a relationship between an infinite-dimensional Markov chain and a finite-dimensional counterpart, which enhances the understanding of stochastic stability in generic non-zero-sum games. Additionally, the findings are specialized to examine the dynamics within weakly acyclic games, expanding the theoretical framework for understanding player behavior in complex strategic environments. <div>
arXiv:2511.11602v1 Announce Type: new 
Abstract: Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques</title>
<link>https://arxiv.org/abs/2511.11604</link>
<guid>https://arxiv.org/abs/2511.11604</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet of Things, Industry 4.0, predictive maintenance, nuclear industry, data-driven methodologies  

<br /><br />Summary: The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly improved data-driven approaches within the nuclear industry, enhancing both safety and economic efficiency. This progress presents challenges in accurately predicting future maintenance requirements, which is essential for minimizing downtime and operational costs. The efficacy of data-driven methods in nuclear settings relies heavily on extensive domain knowledge due to the complexity of the systems. This paper proposes an innovative predictive maintenance methodology that integrates data-driven techniques with specialized knowledge from nuclear equipment. The originality of this research is twofold: it exposes the limitations of solely data-driven methods and emphasizes the critical role of domain knowledge in improving predictive model performance. The applied novelty is evident in its relevance to the highly regulated and sensitive nuclear industry, which faces significant security, economic, and environmental issues. A comprehensive case study highlights the methodology‚Äôs superiority over traditional approaches, showing that while data-driven methods have a limited prediction horizon of 3 hours and an F1 score of 56.36%, the hybrid approach extends the prediction horizon to 24 hours and achieves an F1 score of 93.12%. <div>
arXiv:2511.11604v1 Announce Type: new 
Abstract: The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.11607</link>
<guid>https://arxiv.org/abs/2511.11607</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Non-stationarity, Clustering Orthogonal Weight Modified layer, Sample Efficiency, DMControl Benchmark<br /><br />Summary:<br /><br />1. Reinforcement learning (RL) has achieved superhuman performances but often assumes that the environment is stationary, which is rarely the case in real-world settings. This mismatch causes significant challenges in learning efficiency and requires millions of iterations for training. 2. To overcome this, the authors propose the Clustering Orthogonal Weight Modified (COWM) layer, a novel component that can be seamlessly integrated into policy networks of any RL algorithm. 3. The COWM layer stabilizes the learning process by using clustering techniques combined with a projection matrix to mitigate the effects of environmental non-stationarity. 4. This approach accelerates learning speed, reduces gradient interference during training, and overall improves sample efficiency. 5. Empirical results demonstrate that COWM outperforms current state-of-the-art methods, achieving 9% improvement on vision-based tasks and 12.6% improvement on state-based DMControl benchmarks, while also exhibiting robustness and generality across multiple algorithms and task domains. <div>
arXiv:2511.11607v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models</title>
<link>https://arxiv.org/abs/2511.11622</link>
<guid>https://arxiv.org/abs/2511.11622</guid>
<content:encoded><![CDATA[
<div> Keywords: Tokenization, Transfer Learning, Time Series, Model Performance, Pretraining  

<br /><br />Summary:  
This work investigates how tokenizer design affects time series foundation models, focusing on scaling and quantization strategies. It highlights that the configuration of tokenizers is crucial for the model's representational capacity and stability. The study contrasts the impact of pretraining versus random initialization, revealing that pretrained models benefit significantly from well-designed tokenizers, especially when using smaller vocabulary sizes. Additionally, it emphasizes that ineffective tokenization may reduce or negate the advantages of pretraining. The findings underscore the necessity of meticulous tokenization for effective time series modeling. Furthermore, incorporating small, efficient vocabularies alongside pretrained weights is particularly beneficial in multi-modal forecasting scenarios where a shared vocabulary across various modalities is essential. The paper provides actionable recommendations for designing tokenizers and effectively utilizing transfer learning within discrete representation learning for continuous signals, ultimately guiding practitioners in optimizing their forecasting models. <div>
arXiv:2511.11622v1 Announce Type: new 
Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data</title>
<link>https://arxiv.org/abs/2511.11623</link>
<guid>https://arxiv.org/abs/2511.11623</guid>
<content:encoded><![CDATA[
<div> Graft-versus-host disease, liver transplantation, multi-modal deep learning, electronic health records, class imbalance<br /><br />Summary:  
This study addresses the challenge of early prediction of graft-versus-host disease (GVHD), a rare but deadly complication following liver transplantation. Using a cohort of 2,100 liver transplant patients from Mayo Clinic recorded between 1992 and 2025, which included 42 GVHD cases, the authors analyzed pre-transplant electronic health records (EHR). The dataset integrated four modalities: patient demographics, laboratory tests, diagnoses, and medications. To handle heterogeneous data, irregular records, missing values, and severe class imbalance, they developed a multi-modal deep learning framework that dynamically fuses information from all modalities and optimizes based on AUC. This approach outperformed single-modal and other multi-modal machine learning baselines, achieving an AUC of 0.836, AUPRC of 0.157, recall of 0.768, and specificity of 0.803. The framework effectively leverages complementary information across multiple data types, improving predictive performance in the face of real-world EHR challenges. The study demonstrates that multi-modal deep learning can significantly enhance early GVHD prediction accuracy, facilitating timely clinical intervention and potentially improving patient outcomes despite the extremely imbalanced nature of the data. <div>
arXiv:2511.11623v1 Announce Type: new 
Abstract: Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks</title>
<link>https://arxiv.org/abs/2511.11625</link>
<guid>https://arxiv.org/abs/2511.11625</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, medical imaging, federated learning, adversarial attacks, brain tumor detection  

<br /><br />Summary: Artificial intelligence (AI) is proving to be impactful in medical imaging, notably for brain tumor detection through MRI. Despite its promise, AI models face vulnerabilities during inference when trained via Federated Learning (FL), a method aimed at ensuring patient privacy. Adversarial attacks pose a significant risk as they can subtly manipulate medical images, leading to misdiagnoses while remaining undetectable by human observers. Current defenses struggle in decentralized and varied federated medical environments. This paper introduces MedFedPure, a personalized federated learning defense framework that safeguards diagnostic AI models during inference without sacrificing privacy or accuracy. MedFedPure incorporates three main components: a personalized FL model tailored to each institution's data distribution, a Masked Autoencoder (MAE) to identify suspicious inputs by revealing hidden perturbations, and an adaptive diffusion-based purification module that selectively cleans flagged scans. When evaluated using the Br35H brain MRI dataset, MedFedPure displayed significant improvements in adversarial robustness, with performance rising from 49.50% to 87.33% under strong attacks, while maintaining a clean accuracy of 97.67%. This framework offers a viable, real-time solution for implementing secure and privacy-respecting AI tools in clinical settings. <div>
arXiv:2511.11625v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.
  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion</title>
<link>https://arxiv.org/abs/2511.11627</link>
<guid>https://arxiv.org/abs/2511.11627</guid>
<content:encoded><![CDATA[
<div> Keywords: Full-waveform inversion, structure-aligned encoder, mixture-of-operators, velocity-field inversion, neural operators<br /><br />Summary:<br /><br />1. The paper addresses challenges in full-waveform inversion (FWI), which is a method to create detailed models of subsurface structures but suffers from ill-posedness, nonlinearity, and high computational costs. <br /><br />2. Existing deep learning approaches for FWI often rely on a single convolutional neural network (CNN) or neural operator, limiting their ability to generalize across unknown or complex geological environments and to distinguish between diverse geological types. <br /><br />3. The authors propose the Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture designed specifically for velocity-field inversion without prior knowledge of subsurface structures. This approach first uses a structure-aligned encoder to transform high-dimensional seismic wavefields into a physically consistent latent space, mitigating the mismatch between waveform and velocity domains and improving feature generalization and high-frequency detail recovery. <br /><br />4. An adaptive routing mechanism then dynamically selects and integrates multiple neural-operator experts‚Äîspectral, wavelet, multiscale, and local operators‚Äîto predict velocity models more effectively than single-operator methods. <br /><br />5. Evaluation on the OpenFWI benchmark and the Marmousi2 dataset demonstrates that SA-EMO substantially outperforms conventional CNN and single-operator models, achieving around 58.443% average reduction in mean absolute error (MAE) and about 10.308% better boundary resolution. Ablation studies confirm the importance of each major component: the structure-aligned encoder, expert fusion, and routing mechanism.<br /><br />6. This work establishes a new, scalable, efficient, and physically interpretable framework for full-waveform inversion, improving velocity-field prediction under complex geological conditions. <div>
arXiv:2511.11627v1 Announce Type: new 
Abstract: Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification</title>
<link>https://arxiv.org/abs/2511.11629</link>
<guid>https://arxiv.org/abs/2511.11629</guid>
<content:encoded><![CDATA[
<div> Keywords: Strain Gauge Status, time series classification, global features, hypergraph-based learning, recognition accuracy  

<br /><br />Summary:  
Strain Gauge Status (SGS) recognition is vital for intelligent manufacturing in the Internet of Things, enabling early detection of mechanical failures. Time series classification (TSC) algorithms are used to identify loading and unloading sequences generated by strain gauges. While deep learning models like convolutional neural networks (CNNs) excel in extracting local features from time series data, they struggle to capture essential global features, especially when local subsequences exhibit high similarity, such as in the case of aircraft wing SGS data during static strength tests. To address this limitation, the authors propose two key insights: constructing global features through feature engineering and learning high-order relationships between local features for capturing global features. To implement these insights, a hypergraph-based global feature learning and fusion framework is introduced, aimed at enhancing the representation of SGS time series and increasing recognition accuracy. The effectiveness of this method is validated on industrial SGS datasets and public UCR datasets, demonstrating improved generalization capabilities for unseen data in SGS recognition tasks. <div>
arXiv:2511.11629v1 Announce Type: new 
Abstract: Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models</title>
<link>https://arxiv.org/abs/2511.11630</link>
<guid>https://arxiv.org/abs/2511.11630</guid>
<content:encoded><![CDATA[
<div> Keywords: Grain Growth, Deep Learning, LSTM, Forecasting, Microstructural Engineering

<br /><br />Summary: This study investigates the role of grain growth in materials' mechanical behavior, focusing on predicting grain size distributions. Various deep learning techniques were assessed, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, for this purpose. Instead of using computationally intensive full-field simulations, the research utilized mean-field statistical descriptors derived from high-fidelity simulations. A dataset comprising 120 grain growth sequences was created, representing normalized grain size distributions over time. The models were trained to project future distributions based on short-term historical data through a recursive forecasting method. Results indicate that the LSTM network outperformed other models, achieving over 90% accuracy and showing stable performance across long forecasting horizons. Additionally, LSTM significantly reduced computation time from roughly 20 minutes per sequence to just a few seconds. Other architectures exhibited tendencies to diverge during longer-term predictions. The findings underscore the advantages of utilizing low-dimensional descriptors alongside LSTM-based forecasting for efficient and precise microstructure predictions, with meaningful implications for digital twin development and process optimization in materials engineering. <div>
arXiv:2511.11630v1 Announce Type: new 
Abstract: Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination</title>
<link>https://arxiv.org/abs/2511.11632</link>
<guid>https://arxiv.org/abs/2511.11632</guid>
<content:encoded><![CDATA[
<div> Keywords: few-shot learning, metric-based meta-learning, classifiers, meta-components, generalization  

<br /><br />Summary: In few-shot learning, the main challenge is to enable classifiers to generalize to unseen classes with only a few examples. A widely used approach to tackle this problem is through metric-based meta-learning. However, this method can lead to overfitting, as the deep metric learned from seen classes may not perform well on unseen classes. To address this issue, the authors propose a novel meta-learning algorithm that defines each classifier as a combination of meta-components. These meta-components are learned during meta-learning episodes focusing on seen classes. To ensure that these components do not become redundant and to promote diversity, an orthogonal regularizer is applied, which helps in capturing various shared substructures among different classifiers. The proposed method demonstrates improved generalization capabilities in extensive experiments conducted on several few-shot benchmark tasks, showing superior performance compared to existing methods. This suggests that by effectively structuring classifiers into diverse components and utilizing orthogonal regularization, the ability to learn from limited data can be significantly enhanced in the context of few-shot learning. <div>
arXiv:2511.11632v1 Announce Type: new 
Abstract: In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment</title>
<link>https://arxiv.org/abs/2511.11636</link>
<guid>https://arxiv.org/abs/2511.11636</guid>
<content:encoded><![CDATA[
<div> Keywords: PCOS, fairness, SHAP, Random Forest, calibration  

<br /><br />Summary: This paper introduces a new machine learning framework tailored to predict polycystic ovary syndrome (PCOS) with a focus on fairness and interpretability. The framework employs SHAP-based feature attributions alongside demographic audits, linking predictive insights to disparities among patient subgroups. Probabilistic calibration metrics, specifically Brier Score and Expected Calibration Error (ECE), are utilized to enhance the reliability of risk predictions. Various models, including Random Forest, SVM, and XGBoost, were calibrated for fairness comparison, with the calibrated Random Forest achieving a predictive accuracy of 90.8%. Key influential features identified through SHAP analysis included follicle count, weight gain, and menstrual irregularity, aligning with the Rotterdam diagnostic criteria. Notably, the model displayed age-related performance variations, excelling with women aged 25-35 (accuracy 90.9%) but underperforming for those under 25 (69.2%). It also demonstrated perfect precision in obese women and high recall in lean PCOS cases. Finally, a Streamlit-based web interface was developed for real-time PCOS risk assessment and interactive analysis, enhancing the practical applicability of AI in clinical settings. <div>
arXiv:2511.11636v1 Announce Type: new 
Abstract: This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches</title>
<link>https://arxiv.org/abs/2511.11638</link>
<guid>https://arxiv.org/abs/2511.11638</guid>
<content:encoded><![CDATA[
<div> Keywords: Physics-Informed Neural Networks, Regularized Long Wave equation, Adaptive loss weighting, Conservation laws, Nonlinear systems<br /><br />Summary: This research addresses the limitations of standard physics-informed neural networks (PINNs) in solving the regularized long wave (RLW) equation, which exhibit large error rates. Two enhanced PINN methods were developed: an adaptive approach utilizing self-adaptive loss weighting, and a conservative approach enforcing explicit conservation laws. These methods were tested on three benchmarks: single soliton propagation, interaction of two solitons, and the long-term evolution of an undular bore (up to time $t=250$). Results showed that PINN effectiveness is problem-dependent. The adaptive PINN outperformed others for problems involving complex nonlinear phenomena, such as soliton collisions. Conversely, the conservative PINN excelled in modeling long-term behavior of single solitons and undular bores. A key finding revealed that explicitly enforcing conservation laws can hinder optimization in highly nonlinear equation systems, indicating the need for special training techniques. Both adaptive and conservative PINNs achieved accuracy within $O(10^{-5})$ of established numerical solutions without requiring mesh discretization, highlighting PINNs‚Äô capability for mesh-free PDE solving. This study challenges the assumption that conservation enforcement invariably improves PINN performance and offers guidance for designing PINNs tailored to specific problem types. <div>
arXiv:2511.11638v1 Announce Type: new 
Abstract: Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EcoSpa: Efficient Transformer Training with Coupled Sparsity</title>
<link>https://arxiv.org/abs/2511.11641</link>
<guid>https://arxiv.org/abs/2511.11641</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer, Sparse Training, Structured Sparsity, Model Compression, Efficient Training<br /><br />Summary:  
Transformers are fundamental to modern AI but require substantial computational resources, creating system challenges. Existing sparse training techniques improve efficiency but overlook maintaining structural relationships between weight matrices, especially those interacting multiplicatively in attention and feed-forward layers, which causes performance loss at high sparsity. EcoSpa is introduced as a novel structured sparse training method that jointly evaluates and sparsifies pairs of coupled weight matrices, preserving their interaction by removing aligned rows and columns. This method defines a new granularity to measure the importance of structural components and applies coupled estimation and sparsification consistently across both pre-training and fine-tuning stages. EcoSpa‚Äôs evaluation on popular transformer architectures shows significant efficiency gains: it reduces memory usage by 50% and speeds up training by 21% on LLaMA-1B, achieves a 2.2√ó compression ratio with 2.4 lower perplexity on GPT-2-Medium, and provides a 1.6√ó speedup during inference. Importantly, the approach leverages standard PyTorch operations without needing custom hardware or specialized kernels, making efficient transformer training practical on common commodity hardware and accessible to a broader range of users. <div>
arXiv:2511.11641v1 Announce Type: new 
Abstract: Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\% memory reduction and 21\% faster training, achieves $2.2\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products</title>
<link>https://arxiv.org/abs/2511.11646</link>
<guid>https://arxiv.org/abs/2511.11646</guid>
<content:encoded><![CDATA[
<div> Keywords: product line extension, consumer attributes, deep learning model, Conditional Tabular Variational Auto-Encoder, marketing strategy

<br /><br />Summary: This paper focuses on product line extension as a marketing strategy aimed at enhancing a company's influence while maintaining brand image. To avoid excessive line extensions, it emphasizes the importance of aligning new product offerings with consumer needs. Marketers must first understand the key attributes of their primary customers before launching new line-extended products. The study introduces a novel deep learning model called Conditional Tabular Variational Auto-Encoder (CTVAE), which predicts changes in consumer attributes using synthetic data derived from large-scale tabular datasets of consumers and products. Experimental results indicate that the CTVAE outperforms existing models in prediction accuracy. Additionally, the paper provides insights into product line marketing strategies, particularly for new products that involve changes in containers or flavors. The proposed approach aims to mitigate the risk of cannibalization and helps in designing effective product images and marketing strategies for line extensions. Overall, the findings highlight the potential for CTVAE to improve decision-making in product line marketing by leveraging advanced data analytics tailored to consumer preferences. <div>
arXiv:2511.11646v1 Announce Type: new 
Abstract: Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection</title>
<link>https://arxiv.org/abs/2511.11647</link>
<guid>https://arxiv.org/abs/2511.11647</guid>
<content:encoded><![CDATA[
<div> Keywords: beam selection, transfer learning, Reinforcement Learning, energy efficiency, point cloud

<br /><br />Summary: This paper proposes a new approach for beam selection in 5G and future networks, leveraging transfer learning and Reinforcement Learning (RL) to enhance sustainability. Traditional RL-based models require significant training time and computational resources, particularly in varied propagation environments, which poses challenges for scalability and energy efficiency. To mitigate this, the authors model the environment as a point cloud, representing locations of gNodeBs and surrounding scatterers. By calculating the Chamfer distance between point clouds, the method identifies structurally similar environments, allowing for the reuse of pre-trained models. This innovation results in a 16-fold reduction in training time and resource consumption, improving energy efficiency. The approach minimizes retraining in new deployments, reducing power usage and contributing to the development of sustainable AI in wireless systems. Additionally, it accelerates deployment timelines while decreasing carbon emissions linked to training processes. The simulation results validate that this approach sustains high performance and significantly lowers energy costs, highlighting the potential of transfer learning in scalable, adaptive, and environmentally friendly RL-based beam selection in diverse and dynamic propagation landscapes. <div>
arXiv:2511.11647v1 Announce Type: new 
Abstract: This paper presents a novel and sustainable approach for improving beam selection in 5G and beyond networks using transfer learning and Reinforcement Learning (RL). Traditional RL-based beam selection models require extensive training time and computational resources, particularly when deployed in diverse environments with varying propagation characteristics posing a major challenge for scalability and energy efficiency. To address this, we propose modeling the environment as a point cloud, where each point represents the locations of gNodeBs (gNBs) and surrounding scatterers. By computing the Chamfer distance between point clouds, structurally similar environments can be efficiently identified, enabling the reuse of pre-trained models through transfer learning. This methodology leads to a 16x reduction in training time and computational overhead, directly contributing to energy efficiency. By minimizing the need for retraining in each new deployment, our approach significantly lowers power consumption and supports the development of green and sustainable Artificial Intelligence (AI) in wireless systems. Furthermore, it accelerates time-to-deployment, reduces carbon emissions associated with training, and enhances the viability of deploying AI-driven communication systems at the edge. Simulation results confirm that our approach maintains high performance while drastically cutting energy costs, demonstrating the potential of transfer learning to enable scalable, adaptive, and environmentally conscious RL-based beam selection strategies in dynamic and diverse propagation environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning</title>
<link>https://arxiv.org/abs/2511.11648</link>
<guid>https://arxiv.org/abs/2511.11648</guid>
<content:encoded><![CDATA[
<div> Keywords: Time Series Foundation Models, Data Valuation, In-Context Finetuning, Temporal Dependencies, Lightweight Approach

<br /><br />Summary:  
Time series foundation models (TSFMs) have gained capabilities due to extensive pretraining with diverse data, necessitating effective data valuation for their performance. Traditional methods like influence functions suffer from computational inefficiencies and fail to maintain temporal dependencies. This paper introduces LTSV, a Lightweight Time Series Valuation method utilizing in-context finetuning. LTSV derives its theoretical foundation from the approximation of influence functions, assessing a sample's contribution by measuring changes in context loss after finetuning. It leverages the strong generalization of TSFMs for effective data valuations. To address temporal dependencies, LTSV employs a technique called temporal block aggregation, which integrates influence scores across overlapping time windows. Experimental results from various time series datasets demonstrate that LTSV consistently delivers reliable valuation performance while ensuring manageable computational costs. The findings indicate that in-context finetuning on TSFMs offers a practical solution for bridging data attribution and model generalization in time series learning, highlighting the method's versatility and robustness in evaluating time series data effectively. <div>
arXiv:2511.11648v1 Announce Type: new 
Abstract: Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine</title>
<link>https://arxiv.org/abs/2511.11650</link>
<guid>https://arxiv.org/abs/2511.11650</guid>
<content:encoded><![CDATA[
<div> Keywords: leak detection, water distribution networks, data-driven, Support Vector Machines, anomaly detection  

<br /><br />Summary:  
Water loss due to leaks in Water Distribution Networks (WDNs) is a significant issue, necessitating effective management and detection systems. This paper introduces a novel leak detection method focused on analyzing water pressure measurements collected from various nodes within a WDN. The technique is entirely data-driven, relying only on the WDN's topology and pressure data collected in the absence of leaks. By employing a feature extractor and a one-class Support Vector Machine (SVM) trained on no-leak data, the approach identifies leaks as anomalies. The effectiveness of the proposed method is validated using a simulated dataset from the Modena WDN. Results indicate that this solution surpasses the performance of recent leak detection methods, highlighting its potential as a reliable option for managing water resources efficiently. The study emphasizes the importance of integrating data-driven techniques in advancing leak detection systems for WDNs, ultimately contributing to reducing water loss and improving resource management. <div>
arXiv:2511.11650v1 Announce Type: new 
Abstract: Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incomplete Depression Feature Selection with Missing EEG Channels</title>
<link>https://arxiv.org/abs/2511.11651</link>
<guid>https://arxiv.org/abs/2511.11651</guid>
<content:encoded><![CDATA[
<div> Keywords: depression, EEG, feature selection, IDFS-MEC, noise interference  

<br /><br />Summary: The article addresses the challenges of accurately detecting depression, a critical mental health disorder, using EEG data. It highlights that EEG features often contain redundant, irrelevant, and noisy information, which complicates analysis. Real-world data acquisition can also suffer from issues like electrode detachment and heavy noise, leading to data loss. To overcome these challenges, the authors propose a novel feature selection approach called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). This method incorporates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to minimize the impact of incomplete channels on model construction. Furthermore, it employs global redundancy minimization learning to eliminate redundant information among selected feature subsets. The effectiveness of IDFS-MEC is demonstrated through extensive experiments conducted on MODMA and PRED-d003 datasets. Results indicate that the EEG feature subsets selected using IDFS-MEC outperform ten popular feature selection methods under 3-, 64-, and 128-channel settings, thereby showcasing its potential for improving depression detection accuracy. <div>
arXiv:2511.11651v1 Announce Type: new 
Abstract: As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How many stations are sufficient? Exploring the effect of urban weather station density reduction on imputation accuracy of air temperature and humidity</title>
<link>https://arxiv.org/abs/2511.11652</link>
<guid>https://arxiv.org/abs/2511.11652</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban Weather Station Networks, WSN thinning, air temperature prediction, humidity monitoring, urban climate modeling<br /><br />Summary:<br /><br />1. The study addresses the challenge of costly and labor-intensive maintenance of Urban Weather Station Networks (WSNs) used for monitoring urban weather and climate.  
2. A step-wise station removal method is applied to an existing WSN in Freiburg, Germany, to reduce the number of stations while retaining predictive capability.  
3. The ability of reduced WSN subsets to reproduce air temperature and humidity patterns of the full original network is evaluated over one year following a simulated density reduction.  
4. Results show a significant reduction in station numbers is feasible with only a modest increase in prediction errors. Specifically, reducing stations from 42 to 4 raised root-mean-square errors (RMSE) from 0.69 K to 0.83 K for temperature and from 3.8% to 4.4% for relative humidity, representing 20% and 16% increases respectively.  
5. Predictive accuracy is lower for remote forest stations compared to urban or open areas but remains better than predictions from the advanced Surface Urban Energy and Water Balance Scheme (SUEWS) numerical model.  
6. Stations positioned at the interface between built-up and rural areas provide the greatest value for city-wide climate reconstruction.  
7. The findings support the potential for optimizing WSN deployment by thinning networks to allocate financial and personnel resources more efficiently in urban climate research. <div>
arXiv:2511.11652v1 Announce Type: new 
Abstract: Urban weather station networks (WSNs) are widely used to monitor urban weather and climate patterns and aid urban planning. However, maintaining WSNs is expensive and labor-intensive. Here, we present a step-wise station removal procedure to thin an existing WSN in Freiburg, Germany, and analyze the ability of WSN subsets to reproduce air temperature and humidity patterns of the entire original WSN for a year following a simulated reduction of WSN density. We found that substantial reductions in station numbers after one year of full deployment are possible while retaining high predictive accuracy. A reduction from 42 to 4 stations, for instance, increased mean prediction RMSEs from 0.69 K to 0.83 K for air temperature and from 3.8% to 4.4% for relative humidity, corresponding to RMSE increases of only 20% and 16%, respectively. Predictive accuracy is worse for remote stations in forests than for stations in built-up or open settings, but consistently better than a state-of-the-art numerical urban land-surface model (Surface Urban Energy and Water Balance Scheme). Stations located at the edges between built-up and rural areas are most valuable when reconstructing city-wide climate characteristics. Our study demonstrates the potential of thinning WSNs to maximize the efficient allocation of financial and personnel-related resources in urban climate research.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence of Multiagent Learning Systems for Traffic control</title>
<link>https://arxiv.org/abs/2511.11654</link>
<guid>https://arxiv.org/abs/2511.11654</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Agent Reinforcement Learning, Traffic Signal Control, Convergence, Stability, Stochastic Approximation  

<br /><br />Summary:  
This paper addresses the challenge of traffic congestion caused by rapid urbanization in cities such as Bangalore, emphasizing the importance of efficient Traffic Signal Control (TSC) systems. The authors focus on Multi-Agent Reinforcement Learning (MARL), where each traffic signal operates as an independent agent using Q-learning, a method that has shown empirical success in reducing commuter delays. Despite previous empirical work, there has been a lack of rigorous theoretical analysis regarding the stability and convergence of these independent learning agents when applied to cooperative traffic control tasks. The primary goal of the study is to fill this knowledge gap by providing a formal theoretical framework. Using stochastic approximation techniques, the paper analyzes the dynamics of the MARL learning process in the context of TSC. The key contribution is a convergence proof demonstrating that the specific multi-agent reinforcement learning algorithm applied to traffic signal control systems converges under prescribed conditions. This work extends previous single-agent asynchronous value iteration convergence proofs, validating the theoretical foundations of using independent learners in a cooperative multi-agent environment for traffic management. <div>
arXiv:2511.11654v1 Announce Type: new 
Abstract: Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Probabilistic Learnability of Compact Neural Network Preimage Bounds</title>
<link>https://arxiv.org/abs/2511.11656</link>
<guid>https://arxiv.org/abs/2511.11656</guid>
<content:encoded><![CDATA[
<div> preimage bounds, neural networks, randomized decision trees, bootstrap methods, statistical guarantees  

<br /><br />Summary:  
1. This paper addresses the challenge of computing preimage bounds for neural networks, a problem known to be #P-hard, which limits scalability of existing provable methods.  
2. It proposes a novel probabilistic approach aimed at achieving high-confidence guarantees with bounded error, instead of exact, often infeasible solutions.  
3. The core contribution is the introduction of RF-ProVe (Random Forest Property Verifier), a method that leverages an ensemble of randomized decision trees to identify candidate input regions satisfying target output properties.  
4. RF-ProVe refines these candidate regions through active resampling, allowing it to better capture complex data patterns in high-dimensional input spaces where the desired output property holds.  
5. The authors provide theoretical analysis establishing formal statistical guarantees regarding the purity of the found regions as well as their global coverage, ensuring reliability of approximations.  
6. RF-ProVe offers a practical and scalable solution to preimage approximation problems, particularly useful when exact solvers are unable to scale to larger or more complex networks. <div>
arXiv:2511.11656v1 Announce Type: new 
Abstract: Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\textbf{R}$andom $\textbf{F}$orest $\textbf{Pro}$perty $\textbf{Ve}$rifier ($\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization</title>
<link>https://arxiv.org/abs/2511.11663</link>
<guid>https://arxiv.org/abs/2511.11663</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, quantization, Fourier frequency domain, activation outliers, low-frequency truncation<br /><br />Summary: This paper presents SpecQuant, a novel two-stage framework designed for extreme ultra-low-bit quantization of both activations and weights in large language models (LLMs). The approach revisits LLM compression from the perspective of the Fourier frequency domain to enhance quantization efficiency and accuracy. In the first stage, activation outliers are smoothed and integrated into the weight matrix, simplifying the process for subsequent quantization steps. The second stage involves channel-wise low-frequency Fourier truncation, which suppresses high-frequency components while retaining the essential signal energy, thereby improving the robustness of quantization. The methodology relies on the understanding that weight energy predominantly concentrates in low-frequency components, allowing these to be preserved with minimal accuracy loss. Additionally, SpecQuant introduces a lightweight, adaptive truncation module that dynamically adjusts thresholds during inference based on the characteristics of each channel, enabling runtime adaptability. Experimental results on the LLaMA-3 8B model demonstrate that SpecQuant achieves 4-bit quantization for both weights and activations, reducing the zero-shot accuracy drop to only 1.5% compared to full-precision models. Furthermore, the method achieves a twofold speedup in inference and reduces memory usage by three times, highlighting its efficiency and practical benefits for deploying LLMs on end-user devices. <div>
arXiv:2511.11663v1 Announce Type: new 
Abstract: The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE</title>
<link>https://arxiv.org/abs/2511.11665</link>
<guid>https://arxiv.org/abs/2511.11665</guid>
<content:encoded><![CDATA[
<div> Rotary Positional Embeddings, Quaternion Rotary Embeddings, Clifford Algebra, Shift-equivariance, Multivector Encoding<br /><br />Summary:<br /><br />This paper addresses the limitations of existing Rotary Positional Embeddings (RoPE), particularly focusing on their extension to higher-dimensional inputs. Traditional RoPE methods are praised for their strong performance and shift-equivariance, a property often lost in non-commutative extensions such as Spherical RoPE. The authors introduce Quaternion Rotary Embeddings (QuatRo), leveraging quaternions to represent 3D rotations and parameterize rotation axes, which helps overcome the ambiguities tied to rotation order inherent in spherical rotations. They demonstrate that both Mixed RoPE and Spherical RoPE are particular cases within the QuatRo framework. Building on this, the authors propose a further generalization named Clifford Algebraic Rotary Embeddings (CARE), utilizing geometric algebra. CARE extends rotary embeddings beyond quaternions to Clifford rotors acting on multivectors, enabling two major advancements: rotary embeddings can now operate in arbitrary dimensions, and positional encoding is extended to multivectors with multiple grades rather than limited to vectors. Preliminary experiments compare the performance of spherical, quaternion, and Clifford-based rotary embeddings, laying the groundwork for future research. This work offers a unified and extensible mathematical foundation for sophisticated positional encoding in machine learning models. <div>
arXiv:2511.11665v1 Announce Type: new 
Abstract: Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2511.11666</link>
<guid>https://arxiv.org/abs/2511.11666</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian neural networks, sampling algorithms, SA-SGLD, stepsize, curvature

<br /><br />Summary: This paper addresses the challenges of scalable sampling algorithms for Bayesian neural networks (BNNs), particularly focusing on stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods like SGLD, which are sensitive to stepsize selection. The authors introduce a novel adaptive scheme named SA-SGLD, based on the 'SamAdams' framework, which utilizes time rescaling to adjust the stepsize dynamically depending on the local gradient norm. This adaptation allows the stepsize to shrink in regions of high curvature and expand in flatter areas, thereby enhancing both the stability and mixing of the sampling process. Importantly, SA-SGLD effectively avoids the introduction of bias that is often associated with other adaptive methods requiring divergence correction terms. The proposed method demonstrates its advantages by showing improved accuracy in posterior sampling compared to traditional SGLD. The results are supported by experiments conducted on high-curvature 2D toy problems and image classification tasks utilizing BNNs with sharp priors, underscoring the effectiveness and potential of SA-SGLD in practical applications. <div>
arXiv:2511.11666v1 Announce Type: new 
Abstract: Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion</title>
<link>https://arxiv.org/abs/2511.11667</link>
<guid>https://arxiv.org/abs/2511.11667</guid>
<content:encoded><![CDATA[
<div> Machine Unlearning, Knowledge Density, Layer Re-insertion, Large Language Models, Gradient Propagation<br /><br />Summary:<br /><br />This paper addresses the challenge of machine unlearning in Large Language Models (LLMs), which involves selectively removing harmful knowledge without retraining the model from scratch. Existing methods often fail to completely eliminate such knowledge, leaving residual harmful information that can be recovered. To overcome these issues, the authors propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel strategy that identifies and targets layers rich in harmful knowledge for removal. The method leverages knowledge density estimation to precisely locate these critical layers, enabling accurate and efficient unlearning. Furthermore, KUnBR introduces a layer re-insertion approach that extracts harmful knowledge-laden layers and re-inserts them into the original model architecture. This strategy bypasses gradient obstruction commonly caused by cover layers, facilitating effective gradient propagation necessary for thorough unlearning. Experimental results on various unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art performance in forgetting harmful knowledge while preserving the overall utility and capabilities of the LLM. The approach thus provides a promising solution for privacy, regulatory, and ethical concerns related to managing harmful knowledge in large pre-trained models. <div>
arXiv:2511.11667v1 Announce Type: new 
Abstract: Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do traveling waves make good positional encodings?</title>
<link>https://arxiv.org/abs/2511.11668</link>
<guid>https://arxiv.org/abs/2511.11668</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, positional encoding, RollPE, traveling waves, self-attention<br /><br />Summary:  
Transformers require positional encoding to handle the permutation invariance of their self-attention mechanism. Traditional positional encodings include absolute sinusoidal embeddings or learned positional vectors, but newer approaches focus on relative positional encodings to better capture translation equivariances. This paper introduces RollPE, a novel positional encoding method inspired by traveling waves, which applies a circular roll operation to the query and key tensors within self-attention. The roll operation creates a relative phase shift between positions, enabling the model to compute attention based on positional differences instead of absolute indices. Empirical results show that RollPE significantly outperforms classic absolute positional embeddings and achieves performance comparable to Rotary Positional Embeddings (RoPE). The authors also derive a continuous form of RollPE, which induces a topographic organization within the query and key space. Furthermore, a mathematical equivalence between RollPE and a specific configuration of RoPE is established, providing theoretical grounding. By interpreting RollPE through the concept of traveling waves, the work suggests potential simplifications of RoPE and offers insights into connections with biological processes of information flow in the brain. Overall, RollPE presents a simple yet effective alternative for positional encoding in Transformer models. <div>
arXiv:2511.11668v1 Announce Type: new 
Abstract: Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H-Model: Dynamic Neural Architectures for Adaptive Processing</title>
<link>https://arxiv.org/abs/2511.11669</link>
<guid>https://arxiv.org/abs/2511.11669</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network, dynamic routing, adaptive computation, architecture design, interpretability<br /><br />Summary:<br /> This article introduces a novel neural network architecture that can dynamically modify its internal structure based on input data through a routing mechanism. Each layer in the model actively influences how its outputs are transmitted across the network, enabling iterative and adaptive reasoning processes inspired by human thought patterns. The architecture aims to condition information flow not merely on the input data but also on the network's evolving internal state. Crucially, the work is conceptual and does not seek to outperform current state-of-the-art language models in benchmark tasks. Instead, it serves as a prototype framework to explore networks capable of learning both representations and the computational structure itself. The study emphasizes conceptual innovation over optimization and acknowledges practical limitations in computing resources and data, which constrain the scope of experimentation. Despite these constraints, initial experimental results indicate promise, suggesting that with greater computational capacity, further experiments could unlock the architecture's full potential. Overall, the paper opens new avenues for developing adaptable, possibly more interpretable neural architectures that integrate dynamic reasoning and flexible computation paths. <div>
arXiv:2511.11669v1 Announce Type: new 
Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of LLM-based Explanations for a Learning Analytics Dashboard</title>
<link>https://arxiv.org/abs/2511.11671</link>
<guid>https://arxiv.org/abs/2511.11671</guid>
<content:encoded><![CDATA[
<div> Learning Analytics, Large Language Model, Self-Regulated Learning, Meta-Cognitive Skills, Educational Technology<br /><br />Summary:<br /><br />1. Learning Analytics Dashboards are utilized to support self-regulated learning in digital learning environments by promoting meta-cognitive skills such as reflection.<br />2. The effectiveness of these dashboards can be limited by how interpretable the data they present is to learners.<br />3. To improve interpretability, the study employs a large language model (LLM) to generate verbal explanations of the data shown in the dashboard.<br />4. An expert study with 12 university-level educators compared LLM-generated explanations, standalone dashboards, and human teacher-provided explanations.<br />5. Results indicate that LLM-based explanations on learners' skill states and general learning recommendations are significantly preferred over the other conditions.<br />6. This suggests that integrating LLMs for interpretation in Learning Analytics Dashboards can enhance learners' understanding and experience.<br />7. Importantly, the LLM-generated explanations maintain the pedagogical standards respected and approved by educators.<br /><br />Overall, the study highlights the potential of large language models to improve the usability and educational impact of Learning Analytics Dashboards by providing clearer, teacher-aligned guidance to learners. <div>
arXiv:2511.11671v1 Announce Type: new 
Abstract: Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture</title>
<link>https://arxiv.org/abs/2511.11673</link>
<guid>https://arxiv.org/abs/2511.11673</guid>
<content:encoded><![CDATA[
<div> Keywords: Synergistic Fusion Layer, deep learning, lyrical content classification, Sentence-BERT, Expected Calibration Error  

<br /><br />Summary: This study presents a solution to the challenge of combining high-dimensional deep semantic features with simple, interpretable structural cues for classifying lyrical content. The authors introduce the Synergistic Fusion Layer (SFL), a deep learning model that employs a gated mechanism to adjust Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task is redefined as binary classification to differentiate a dominant, homogeneous cluster (Class 0) from all other content (Class 1), based on clustering UMAP-reduced lyrical embeddings. The SFL model achieved impressive performance with an accuracy of 0.9894 and a Macro F1 score of 0.9894, surpassing a Random Forest (RF) baseline that used feature concatenation, which achieved an accuracy of 0.9868. Additionally, the SFL model displayed significantly enhanced reliability, demonstrating a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5 times lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This validates the hypothesis that non-linear gating is superior to simple concatenation, establishing the SFL as a trustworthy and robust system for complex multimodal lyrical analysis. <div>
arXiv:2511.11673v1 Announce Type: new 
Abstract: This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff</title>
<link>https://arxiv.org/abs/2511.11675</link>
<guid>https://arxiv.org/abs/2511.11675</guid>
<content:encoded><![CDATA[
<div> Keywords: model pruning, model compression, sparsity, pruning-regrowth strategy, hardware constraints<br /><br />Summary:<br />Model pruning is a widely used technique for compressing neural networks by removing less important connections to reduce model size. However, when the level of pruning sparsity goes beyond a certain threshold, the model's performance dramatically declines, which happens with both iterative and one-shot pruning methods. This sharp drop in accuracy limits how much a model can be compressed, thereby making it difficult to meet the stringent size requirements imposed by some hardware platforms, sometimes rendering the pruned models unusable. To address this challenge, the authors propose a novel bidirectional pruning-regrowth strategy. Instead of pruning from a fully dense network, their method starts with an extremely compressed model that already satisfies hardware constraints. From this starting point, the strategy selectively regrows or regenerates critical connections that were previously removed to help recover the lost accuracy. This selective regrowth effectively mitigates the steep performance degradation that occurs under very high sparsity levels, enabling higher compression ratios without sacrificing model quality. As a result, the approach provides a practical and efficient way to create sparse models suitable for deployment on resource-constrained hardware platforms while maintaining robust accuracy. <div>
arXiv:2511.11675v1 Announce Type: new 
Abstract: As a widely adopted model compression technique, model pruning has demonstrated strong effectiveness across various architectures. However, we observe that when sparsity exceeds a certain threshold, both iterative and one-shot pruning methods lead to a steep decline in model performance. This rapid degradation limits the achievable compression ratio and prevents models from meeting the stringent size constraints required by certain hardware platforms, rendering them inoperable. To overcome this limitation, we propose a bidirectional pruning-regrowth strategy. Starting from an extremely compressed network that satisfies hardware constraints, the method selectively regenerates critical connections to recover lost performance, effectively mitigating the sharp accuracy drop commonly observed under high sparsity conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning with Preserving for Continual Multitask Learning</title>
<link>https://arxiv.org/abs/2511.11676</link>
<guid>https://arxiv.org/abs/2511.11676</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Continual Multitask Learning, Geometric Structure, Catastrophic Forgetting, Robustness

<br /><br />Summary: The paper introduces a new framework for Continual Multitask Learning (CMTL), where artificial intelligence models learn multiple tasks sequentially using a shared data stream, exemplified by applications in autonomous driving and medical imaging. Traditional continual learning methods often struggle due to the interference of fragmented, task-specific features, leading to "catastrophic forgetting" of previous tasks. The authors propose Learning with Preserving (LwP), which emphasizes maintaining the geometric structure of the representation space instead of just preserving task outputs. Central to LwP is the Dynamically Weighted Distance Preservation (DWDP) loss, designed to prevent representation drift by regularizing pairwise distances between latent data representations. This approach allows the model to retain implicit knowledge across various tasks and eliminates the need for a replay buffer, making it suitable for applications that prioritize privacy. Extensive evaluations demonstrate that LwP mitigates catastrophic forgetting effectively and outperforms state-of-the-art baselines in CMTL tasks. Furthermore, it shows exceptional robustness to distribution shifts and is the only method to exceed the performance of strong single-task learning baselines, highlighting its potential for dynamic real-world environments. <div>
arXiv:2511.11676v1 Announce Type: new 
Abstract: Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal Power Flow</title>
<link>https://arxiv.org/abs/2511.11677</link>
<guid>https://arxiv.org/abs/2511.11677</guid>
<content:encoded><![CDATA[
<div> Keywords: Learning to optimize, AC optimal power flow, homotopy method, self-supervised learning, power system operations<br /><br />Summary:<br /><br />1. The paper addresses the challenge of solving AC optimal power flow (AC-OPF) problems using learning to optimize (L2O) techniques, which aim to enable fast, reusable decision-making in real-time power system operations. <br />2. AC-OPF problems are inherently nonconvex, leading to complicated optimization landscapes where standard learning approaches often fail to find feasible and high-quality solutions.<br />3. The authors propose a homotopy-guided self-supervised L2O method that gradually deforms the objective and constraints during training, starting from a relaxed version of the problem with a wide basin of attraction and progressively shifting toward the original problem.<br />4. This homotopy-guided training approach enhances convergence stability and better enforces feasibility without relying on labeled optimal solutions or external solvers.<br />5. Evaluation on standard IEEE AC-OPF benchmarks demonstrates that the proposed method substantially improves feasibility rates compared to methods without homotopy guidance and achieves objective values close to those obtained by full AC-OPF solvers, highlighting the promise of homotopy-based heuristics for scalable, constraint-aware L2O in power system optimization. <div>
arXiv:2511.11677v1 Announce Type: new 
Abstract: Learning to optimize (L2O) parametric approximations of AC optimal power flow (AC-OPF) solutions offers the potential for fast, reusable decision-making in real-time power system operations. However, the inherent nonconvexity of AC-OPF results in challenging optimization landscapes, and standard learning approaches often fail to converge to feasible, high-quality solutions. This work introduces a \textit{homotopy-guided self-supervised L2O method} for parametric AC-OPF problems. The key idea is to construct a continuous deformation of the objective and constraints during training, beginning from a relaxed problem with a broad basin of attraction and gradually transforming it toward the original problem. The resulting learning process improves convergence stability and promotes feasibility without requiring labeled optimal solutions or external solvers. We evaluate the proposed method on standard IEEE AC-OPF benchmarks and show that homotopy-guided L2O significantly increases feasibility rates compared to non-homotopy baselines, while achieving objective values comparable to full OPF solvers. These findings demonstrate the promise of homotopy-based heuristics for scalable, constraint-aware L2O in power system optimization.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications</title>
<link>https://arxiv.org/abs/2511.11679</link>
<guid>https://arxiv.org/abs/2511.11679</guid>
<content:encoded><![CDATA[
<div> Keywords: free-boundary diffeomorphism, LSQC theory, Spectral Beltrami Network, SBN-Opt, surface mapping  

<br /><br />Summary: The paper addresses the complex problem of free-boundary diffeomorphism optimization, which is essential for accurate surface mapping but poses significant challenges due to the unconstrained boundary and the requirement for local bijectivity during large deformations. It introduces Numerical Least-Squares Quasiconformal (LSQC) theory as a solution, highlighting its benefits like provable existence, uniqueness, similarity-invariance, and resolution-independence. Traditionally, LSQC requires landmark conditioning, limiting its application in gradient-based optimization. To overcome this, the authors propose a novel neural surrogate called the Spectral Beltrami Network (SBN), which integrates LSQC energy into a multiscale mesh-spectral architecture. They further develop an optimization framework, SBN-Opt, that fine-tunes free-boundary diffeomorphism while allowing explicit control over local geometric distortion. Through extensive experiments focusing on density-equalizing maps and inconsistent surface registration, the study demonstrates that SBN-Opt outperforms traditional numerical algorithms, showcasing its efficacy and potential for broader applications in surface mapping problems. <div>
arXiv:2511.11679v1 Announce Type: new 
Abstract: Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP</title>
<link>https://arxiv.org/abs/2511.11680</link>
<guid>https://arxiv.org/abs/2511.11680</guid>
<content:encoded><![CDATA[
<div> Wildfire risk, Random Forest, SHAP, California, Ecosystem drivers<br /><br />Summary:<br /><br />This study addresses the global threat of wildfires, focusing on California, which faces recurring fires influenced by climate, topography, vegetation, and human activities. A comprehensive wildfire risk map was developed using the Random Forest (RF) algorithm combined with Explainable Artificial Intelligence (XAI) via Shapley Additive exPlanations (SHAP) to interpret model predictions. The model's performance was evaluated through spatial and temporal validation techniques. The RF model showed excellent predictive accuracy for grasslands (AUC 0.996) and forests (AUC 0.997). Spatial cross-validation indicated moderate model transferability with ROC-AUC scores of 0.6155 (forests) and 0.5416 (grasslands), while temporal validation demonstrated better generalization, especially for forests (ROC-AUC 0.6615, PR-AUC 0.8423). SHAP-based analysis identified ecosystem-specific key drivers: soil organic carbon, tree cover, and NDVI were most influential for forests; land surface temperature, elevation, and vegetation health indices predominated for grasslands. District-level risk classification pinpointed Central Valley and Northern Buttes with the highest grassland fire risk, and Northern Buttes and North Coast Redwoods as primary forest high-risk zones. Overall, the RF-SHAP framework presents a robust, interpretable, and adaptable tool for wildfire risk assessment, supporting informed decision-making and targeted mitigation strategies. <div>
arXiv:2511.11680v1 Announce Type: new 
Abstract: Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation</title>
<link>https://arxiv.org/abs/2511.11681</link>
<guid>https://arxiv.org/abs/2511.11681</guid>
<content:encoded><![CDATA[
<div> Keywords: cloud image segmentation, MPCM-Net, multi-scale context, feature extraction, CSRC dataset  

<br /><br />Summary:  
Ground-based cloud image segmentation is crucial for improving photovoltaic power forecasting. Current deep learning methods focus on encoder-decoder architectures but have limitations, such as reliance on dilated convolutions for multi-scale context extraction, which lacks partial feature effectiveness and inter-channel interoperability. Additionally, attention mechanisms do not adequately balance accuracy and throughput, while decoder modifications fail to capture global interdependencies among local features, hindering inference efficiency. To address these issues, the authors propose MPCM-Net, a multi-scale network that incorporates Partial attention Convolutions with Mamba architectures for better segmentation accuracy and computational efficiency. Key innovations include an encoder with MPAC, which features a MPC block for global spatial interaction and a MPA block for feature extraction with lower computational complexity. The decoder utilizes a M2B to reduce contextual loss via a SSHD, maintaining linear complexity while enhancing deep feature aggregation. Furthermore, the authors introduce the CSRC dataset, a fine-grained segmentation benchmark designed to address the shortcomings of existing public datasets. Extensive experiments show that MPCM-Net significantly outperforms state-of-the-art methods, achieving a favorable balance between segmentation accuracy and inference speed. The dataset and code are available on GitHub. <div>
arXiv:2511.11681v1 Announce Type: new 
Abstract: Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stratified Knowledge-Density Super-Network for Scalable Vision Transformers</title>
<link>https://arxiv.org/abs/2511.11683</link>
<guid>https://arxiv.org/abs/2511.11683</guid>
<content:encoded><![CDATA[
<div> Keywords: vision transformer, knowledge stratification, WPAC, PIAD, model compression  

<br /><br />Summary:  
This paper addresses the inefficiency of training and deploying multiple Vision Transformer (ViT) models for diverse resource constraints by proposing a single adaptable super-network. The authors present a method to transform a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights, enabling flexible extraction of sub-networks that maximize retained knowledge at different sizes. To concentrate knowledge effectively, they introduce Weighted PCA for Attention Contraction (WPAC), which applies token-wise weighted principal component analysis on intermediate features and injects the resulting transformations into adjacent layers, preserving original network functions while compacting critical weights. Complementing this, Progressive Importance-Aware Dropout (PIAD) progressively assesses and updates the importance of weight groups during training, enforcing a dropout regime that encourages knowledge stratification throughout the network. Experimental results demonstrate that WPAC surpasses existing pruning criteria in concentrating knowledge efficiently. Furthermore, the combined approach of WPAC and PIAD establishes a strong alternative to current state-of-the-art approaches in both model compression and model expansion, reducing computational overhead while maintaining performance. This work thus offers a practical and efficient framework for scalable ViT deployment across varying resource constraints. <div>
arXiv:2511.11683v1 Announce Type: new 
Abstract: Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \textbf{W}eighted \textbf{P}CA for \textbf{A}ttention \textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \textbf{P}rogressive \textbf{I}mportance-\textbf{A}ware \textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Bayesian Model for Multi-stage Censoring</title>
<link>https://arxiv.org/abs/2511.11684</link>
<guid>https://arxiv.org/abs/2511.11684</guid>
<content:encoded><![CDATA[
<div> Keywords: sequential decision, funnel structure, selective censoring, Bayesian model, healthcare disparities  

<br /><br />Summary:  
This work addresses sequential decision-making in healthcare settings characterized by funnel structures, where patients pass through progressively fewer stages with increasing decision costs, such as screening exams followed by diagnostic tests. A critical issue is that the true patient outcomes, like biopsy results, are only revealed at the funnel‚Äôs end, causing selective censoring that can bias risk estimation, particularly affecting underserved patient groups with more frequent censored outcomes. The authors develop a novel Bayesian model tailored for these funnel decision structures, building upon existing methods that handle selective labels and censoring. In synthetic experiments, the model successfully recovers true parameters and makes more accurate predictions for censored patients compared to baseline approaches. They then apply the model to emergency department data, focusing on in-hospital mortality which is only observed for admitted patients. Their analysis reveals gender-based differences in admission decisions; notably, the estimated mortality risk threshold for ICU admission is higher for women (5.1%) than for men (4.5%). This finding highlights potential biases or disparities in clinical decision-making. Overall, the study proposes a statistical framework that improves outcome prediction under censoring and uncovers important healthcare inequities. <div>
arXiv:2511.11684v1 Announce Type: new 
Abstract: Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R-Tuning: Wavelet-Decomposed Replay and Semantic Alignment for Continual Adaptation of Pretrained Time-Series Models</title>
<link>https://arxiv.org/abs/2511.11685</link>
<guid>https://arxiv.org/abs/2511.11685</guid>
<content:encoded><![CDATA[
arXiv:2511.11685v1 Announce Type: new 
Abstract: Pre-trained models have demonstrated exceptional generalization capabilities in time-series forecasting; however, adapting them to evolving data distributions remains a significant challenge. A key hurdle lies in accessing the original training data, as fine-tuning solely on new data often leads to catastrophic forgetting. To address this issue, we propose Replay Tuning (R-Tuning), a novel framework designed for the continual adaptation of pre-trained time-series models. R-Tuning constructs a unified latent space that captures both prior and current task knowledge through a frequency-aware replay strategy. Specifically, it augments model-generated samples via wavelet-based decomposition across multiple frequency bands, generating trend-preserving and fusion-enhanced variants to improve representation diversity and replay efficiency. To further reduce reliance on synthetic samples, R-Tuning introduces a latent consistency constraint that aligns new representations with the prior task space. This constraint guides joint optimization within a compact and semantically coherent latent space, ensuring robust knowledge retention and adaptation. Extensive experimental results demonstrate the superiority of R-Tuning, which reduces MAE and MSE by up to 46.9% and 46.8%, respectively, on new tasks, while preserving prior knowledge with gains of up to 5.7% and 6.0% on old tasks. Notably, under few-shot settings, R-Tuning outperforms all state-of-the-art baselines even when synthetic proxy samples account for only 5% of the new task dataset.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regularized Schr\"odinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems</title>
<link>https://arxiv.org/abs/2511.11686</link>
<guid>https://arxiv.org/abs/2511.11686</guid>
<content:encoded><![CDATA[
arXiv:2511.11686v1 Announce Type: new 
Abstract: Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schr\"odinger Bridge (RSB), an adaptation of Schr\"odinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling</title>
<link>https://arxiv.org/abs/2511.11688</link>
<guid>https://arxiv.org/abs/2511.11688</guid>
<content:encoded><![CDATA[
arXiv:2511.11688v1 Announce Type: new 
Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.11690</link>
<guid>https://arxiv.org/abs/2511.11690</guid>
<content:encoded><![CDATA[
arXiv:2511.11690v1 Announce Type: new 
Abstract: Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues</title>
<link>https://arxiv.org/abs/2511.11691</link>
<guid>https://arxiv.org/abs/2511.11691</guid>
<content:encoded><![CDATA[
arXiv:2511.11691v1 Announce Type: new 
Abstract: Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies "what" is highlighted and connects it to "why" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation</title>
<link>https://arxiv.org/abs/2511.11692</link>
<guid>https://arxiv.org/abs/2511.11692</guid>
<content:encoded><![CDATA[
arXiv:2511.11692v1 Announce Type: new 
Abstract: Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to "semantic over-smoothing" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL</title>
<link>https://arxiv.org/abs/2511.11696</link>
<guid>https://arxiv.org/abs/2511.11696</guid>
<content:encoded><![CDATA[
arXiv:2511.11696v1 Announce Type: new 
Abstract: This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking GNNs for OOD Materials Property Prediction with Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2511.11697</link>
<guid>https://arxiv.org/abs/2511.11697</guid>
<content:encoded><![CDATA[
arXiv:2511.11697v1 Announce Type: new 
Abstract: We present MatUQ, a benchmark framework for evaluating graph neural networks (GNNs) on out-of-distribution (OOD) materials property prediction with uncertainty quantification (UQ). MatUQ comprises 1,375 OOD prediction tasks constructed from six materials datasets using five OFM-based and a newly proposed structure-aware splitting strategy, SOAP-LOCO, which captures local atomic environments more effectively. We evaluate 12 representative GNN models under a unified uncertainty-aware training protocol that combines Monte Carlo Dropout and Deep Evidential Regression (DER), and introduce a novel uncertainty metric, D-EviU, which shows the strongest correlation with prediction errors in most tasks. Our experiments yield two key findings. First, the uncertainty-aware training approach significantly improves model prediction accuracy, reducing errors by an average of 70.6\% across challenging OOD scenarios. Second, the benchmark reveals that no single model dominates universally: earlier models such as SchNet and ALIGNN remain competitive, while newer models like CrystalFramer and SODNet demonstrate superior performance on specific material properties. These results provide practical insights for selecting reliable models under distribution shifts in materials discovery.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moirai 2.0: When Less Is More for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.11698</link>
<guid>https://arxiv.org/abs/2511.11698</guid>
<content:encoded><![CDATA[
arXiv:2511.11698v1 Announce Type: new 
Abstract: We introduce Moirai 2.0, a decoder-only time-series foundation model trained on a new corpus of 36M series. The model adopts quantile forecasting and multi-token prediction, improving both probabilistic accuracy and inference efficiency. On the Gift-Eval benchmark, it ranks among the top pretrained models while achieving a strong trade-off between accuracy, speed, and model size. Compared to Moirai 1.0, Moirai 2.0 replaces masked-encoder training, multi-patch inputs, and mixture-distribution outputs with a simpler decoder-only architecture, single patch, and quantile loss. Ablation studies isolate these changes -- showing that the decoder-only backbone along with recursive multi-quantile decoding contribute most to the gains. Additional experiments show that Moirai 2.0 outperforms larger models from the same family and exhibits robust domain-level results. In terms of efficiency and model size, Moirai 2.0 is twice as fast and thirty times smaller than its prior best version, Moirai 1.0-Large, while also performing better. Model performance plateaus with increasing parameter count and declines at longer horizons, motivating future work on data scaling and long-horizon modeling. We release code and evaluation details to support further research.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification</title>
<link>https://arxiv.org/abs/2511.11699</link>
<guid>https://arxiv.org/abs/2511.11699</guid>
<content:encoded><![CDATA[
arXiv:2511.11699v1 Announce Type: new 
Abstract: Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Neural Networks with Monte Carlo Dropout for Probabilistic Electricity Price Forecasting</title>
<link>https://arxiv.org/abs/2511.11701</link>
<guid>https://arxiv.org/abs/2511.11701</guid>
<content:encoded><![CDATA[
arXiv:2511.11701v1 Announce Type: new 
Abstract: Accurate electricity price forecasting is critical for strategic decision-making in deregulated electricity markets, where volatility stems from complex supply-demand dynamics and external factors. Traditional point forecasts often fail to capture inherent uncertainties, limiting their utility for risk management. This work presents a framework for probabilistic electricity price forecasting using Bayesian neural networks (BNNs) with Monte Carlo (MC) dropout, training separate models for each hour of the day to capture diurnal patterns. A critical assessment and comparison with the benchmark model, namely: generalized autoregressive conditional heteroskedasticity with exogenous variable (GARCHX) model and the LASSO estimated auto-regressive model (LEAR), highlights that the proposed model outperforms the benchmark models in terms of point prediction and intervals. This work serves as a reference for leveraging probabilistic neural models in energy market predictions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom</title>
<link>https://arxiv.org/abs/2511.11703</link>
<guid>https://arxiv.org/abs/2511.11703</guid>
<content:encoded><![CDATA[
arXiv:2511.11703v1 Announce Type: new 
Abstract: Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Vision-Language Math Reasoning via Rendered Text</title>
<link>https://arxiv.org/abs/2511.11704</link>
<guid>https://arxiv.org/abs/2511.11704</guid>
<content:encoded><![CDATA[
arXiv:2511.11704v1 Announce Type: new 
Abstract: We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs</title>
<link>https://arxiv.org/abs/2511.11705</link>
<guid>https://arxiv.org/abs/2511.11705</guid>
<content:encoded><![CDATA[
arXiv:2511.11705v1 Announce Type: new 
Abstract: This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling</title>
<link>https://arxiv.org/abs/2511.11706</link>
<guid>https://arxiv.org/abs/2511.11706</guid>
<content:encoded><![CDATA[
arXiv:2511.11706v1 Announce Type: new 
Abstract: Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FSC-Net: Fast-Slow Consolidation Networks for Continual Learning</title>
<link>https://arxiv.org/abs/2511.11707</link>
<guid>https://arxiv.org/abs/2511.11707</guid>
<content:encoded><![CDATA[
arXiv:2511.11707v1 Announce Type: new 
Abstract: Continual learning remains challenging due to catastrophic forgetting, where neural networks lose previously acquired knowledge when learning new tasks. Inspired by memory consolidation in neuroscience, we propose FSC-Net (Fast-Slow Consolidation Networks), a dual-network architecture that separates rapid task learning from gradual knowledge consolidation. Our method employs a fast network (NN1) for immediate adaptation to new tasks and a slow network (NN2) that consolidates knowledge through distillation and replay. Within the family of MLP-based NN1 variants we evaluated, consolidation effectiveness is driven more by methodology than architectural embellishments -- a simple MLP outperforms more complex similarity-gated variants by 1.2pp. Through systematic hyperparameter analysis, we observed empirically that pure replay without distillation during consolidation achieves superior performance, consistent with the hypothesis that distillation from the fast network introduces recency bias. On Split-MNIST (30 seeds), FSC-Net achieves 91.71% +/- 0.62% retention accuracy, a +4.27pp gain over the fast network alone (87.43% +/- 1.27%, paired t=23.585, p < 1e-10). On Split-CIFAR-10 (5 seeds), our method achieves 33.31% +/- 0.38% retention with an +8.20pp gain over the fast network alone (25.11% +/- 1.61%, paired t=9.75, p < 1e-3), demonstrating +8.20pp gain, though absolute performance (33.31%) remains modest and below random expectation, highlighting need for stronger backbones. Our results provide empirical evidence that the dual-timescale consolidation mechanism, rather than architectural complexity, is central to mitigating catastrophic forgetting in this setting.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control</title>
<link>https://arxiv.org/abs/2511.11711</link>
<guid>https://arxiv.org/abs/2511.11711</guid>
<content:encoded><![CDATA[
arXiv:2511.11711v1 Announce Type: new 
Abstract: Although sparse autoencoders (SAEs) are crucial for identifying interpretable features in neural networks, it is still challenging to distinguish between real computational patterns and erroneous correlations. We introduce Model-X knockoffs to SAE feature selection, using knock-off+ to control the false discovery rate (FDR) with finite-sample guarantees under the standard Model-X assumptions (in our case, via a Gaussian surrogate for the latent distribution). We select 129 features at a target FDR q=0.1 after analyzing 512 high-activity SAE latents for sentiment classification using Pythia-70M. About 25% of the latents under examination carry task-relevant signal, whereas 75% do not, according to the chosen set, which displays a 5.40x separation in knockoff statistics compared to non-selected features. Our method offers a re-producible and principled framework for reliable feature discovery by combining SAEs with multiple-testing-aware inference, advancing the foundations of mechanistic interpretability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning: From Reflection to Solution</title>
<link>https://arxiv.org/abs/2511.11712</link>
<guid>https://arxiv.org/abs/2511.11712</guid>
<content:encoded><![CDATA[
arXiv:2511.11712v1 Announce Type: new 
Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data</title>
<link>https://arxiv.org/abs/2511.11714</link>
<guid>https://arxiv.org/abs/2511.11714</guid>
<content:encoded><![CDATA[
arXiv:2511.11714v1 Announce Type: new 
Abstract: Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.
  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiscale Grassmann Manifolds for Single-Cell Data Analysis</title>
<link>https://arxiv.org/abs/2511.11717</link>
<guid>https://arxiv.org/abs/2511.11717</guid>
<content:encoded><![CDATA[
arXiv:2511.11717v1 Announce Type: new 
Abstract: Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast 3D Surrogate Modeling for Data Center Thermal Management</title>
<link>https://arxiv.org/abs/2511.11722</link>
<guid>https://arxiv.org/abs/2511.11722</guid>
<content:encoded><![CDATA[
arXiv:2511.11722v1 Announce Type: new 
Abstract: Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\%) and reduced carbon footprint.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm</title>
<link>https://arxiv.org/abs/2511.11727</link>
<guid>https://arxiv.org/abs/2511.11727</guid>
<content:encoded><![CDATA[
arXiv:2511.11727v1 Announce Type: new 
Abstract: Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural ODEs with Scale-Aware Residuals for Learning Stiff Biophysical Dynamics</title>
<link>https://arxiv.org/abs/2511.11734</link>
<guid>https://arxiv.org/abs/2511.11734</guid>
<content:encoded><![CDATA[
arXiv:2511.11734v1 Announce Type: new 
Abstract: Neural differential equations offer a powerful framework for modeling continuous-time dynamics, but forecasting stiff biophysical systems remains unreliable. Standard Neural ODEs and physics informed variants often require orders of magnitude more iterations, and even then may converge to suboptimal solutions that fail to preserve oscillatory frequency or amplitude. We introduce PhysicsInformed Neural ODEs with with Scale-Aware Residuals (PI-NODE-SR), a framework that combines a low-order explicit solver (Heun method) residual normalisation to balance contributions between state variables evolving on disparate timescales. This combination stabilises training under realistic iteration budgets and avoids reliance on computationally expensive implicit solvers. On the Hodgkin-Huxley equations, PI-NODE-SR learns from a single oscillation simulated with a stiff solver (Rodas5P) and extrapolates beyond 100 ms, capturing both oscillation frequency and near-correct amplitudes. Remarkably, end-to-end learning of the vector field enables PI-NODE-SR to recover morphological features such as sharp subthreshold curvature in gating variables that are typically reserved for higher-order solvers, suggesting that neural correction can offset numerical diffusion. While performance remains sensitive to initialisation, PI-NODE-SR consistently reduces long-horizon errors relative to baseline Neural-ODEs and PINNs, offering a principled route to stable and efficient learning of stiff biological dynamics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KAN/H: Kolmogorov-Arnold Network using Haar-like bases</title>
<link>https://arxiv.org/abs/2511.11736</link>
<guid>https://arxiv.org/abs/2511.11736</guid>
<content:encoded><![CDATA[
arXiv:2511.11736v1 Announce Type: new 
Abstract: This paper proposes KAN/H, a variant of Kolmogorov-Arnold Network (KAN) that uses a Haar-variant basis system having both global and local bases instead of B-spline. The resulting algorithm is applied to function approximation problems and MNIST. We show that it does not require most of the problem-specific hyper-parameter tunings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DK-Root: A Joint Data-and-Knowledge-Driven Framework for Root Cause Analysis of QoE Degradations in Mobile Networks</title>
<link>https://arxiv.org/abs/2511.11737</link>
<guid>https://arxiv.org/abs/2511.11737</guid>
<content:encoded><![CDATA[
arXiv:2511.11737v1 Announce Type: new 
Abstract: Diagnosing the root causes of Quality of Experience (QoE) degradations in operational mobile networks is challenging due to complex cross-layer interactions among kernel performance indicators (KPIs) and the scarcity of reliable expert annotations. Although rule-based heuristics can generate labels at scale, they are noisy and coarse-grained, limiting the accuracy of purely data-driven approaches. To address this, we propose DK-Root, a joint data-and-knowledge-driven framework that unifies scalable weak supervision with precise expert guidance for robust root-cause analysis. DK-Root first pretrains an encoder via contrastive representation learning using abundant rule-based labels while explicitly denoising their noise through a supervised contrastive objective. To supply task-faithful data augmentation, we introduce a class-conditional diffusion model that generates KPIs sequences preserving root-cause semantics, and by controlling reverse diffusion steps, it produces weak and strong augmentations that improve intra-class compactness and inter-class separability. Finally, the encoder and the lightweight classifier are jointly fine-tuned with scarce expert-verified labels to sharpen decision boundaries. Extensive experiments on a real-world, operator-grade dataset demonstrate state-of-the-art accuracy, with DK-Root surpassing traditional ML and recent semi-supervised time-series methods. Ablations confirm the necessity of the conditional diffusion augmentation and the pretrain-finetune design, validating both representation quality and classification gains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2511.11743</link>
<guid>https://arxiv.org/abs/2511.11743</guid>
<content:encoded><![CDATA[
arXiv:2511.11743v1 Announce Type: new 
Abstract: Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epistemic uncertainty-based routing across heterogeneous experts (BitNet ternary, 1-16 bit BitLinear, post-training quantization). Evaluated on audio classification benchmarks (ESC-50, Quinn, UrbanSound8K), our 4-bit quantization maintains 99.9 percent of 16-bit accuracy (0.858 vs 0.859 F1) with 4x compression and 41 percent energy savings versus 8-bit. Crucially, curiosity-driven routing reduces MoE latency variance by 82 percent (p = 0.008, Levene's test) from 230 ms to 29 ms standard deviation, enabling stable inference for battery-constrained devices. Statistical analysis confirms 4-bit/8-bit achieve practical equivalence with full precision (p > 0.05), while MoE architectures introduce 11 percent latency overhead (p < 0.001) without accuracy gains. At scale, deployment emissions dominate training by 10000x for models serving more than 1,000 inferences, making inference efficiency critical. Our information-theoretic routing demonstrates that adaptive quantization yields accurate (0.858 F1, 1.2M params), energy-efficient (3.87 F1/mJ), and predictable edge models, with simple 4-bit quantized architectures outperforming complex MoE for most deployments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Models: A Mathematical Introduction</title>
<link>https://arxiv.org/abs/2511.11746</link>
<guid>https://arxiv.org/abs/2511.11746</guid>
<content:encoded><![CDATA[
arXiv:2511.11746v1 Announce Type: new 
Abstract: We present a concise, self-contained derivation of diffusion-based generative models. Starting from basic properties of Gaussian distributions (densities, quadratic expectations, re-parameterisation, products, and KL divergences), we construct denoising diffusion probabilistic models from first principles. This includes the forward noising process, its closed-form marginals, the exact discrete reverse posterior, and the related variational bound. This bound simplifies to the standard noise-prediction goal used in practice. We then discuss likelihood estimation and accelerated sampling, covering DDIM, adversarially learned reverse dynamics (DDGAN), and multi-scale variants such as nested and latent diffusion, with Stable Diffusion as a canonical example. A continuous-time formulation follows, in which we derive the probability-flow ODE from the diffusion SDE via the continuity and Fokker-Planck equations, introduce flow matching, and show how rectified flows recover DDIM up to a time re-parameterisation. Finally, we treat guided diffusion, interpreting classifier guidance as a posterior score correction and classifier-free guidance as a principled interpolation between conditional and unconditional scores. Throughout, the focus is on transparent algebra, explicit intermediate steps, and consistent notation, so that readers can both follow the theory and implement the corresponding algorithms in practice.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation</title>
<link>https://arxiv.org/abs/2511.11750</link>
<guid>https://arxiv.org/abs/2511.11750</guid>
<content:encoded><![CDATA[
arXiv:2511.11750v1 Announce Type: new 
Abstract: Tropical Cyclone (TC) estimation aims to accurately estimate various TC attributes in real time. However, distribution shifts arising from the complex and dynamic nature of TC environmental fields, such as varying geographical conditions and seasonal changes, present significant challenges to reliable estimation. Most existing methods rely on multi-modal fusion for feature extraction but overlook the intrinsic distribution of feature representations, leading to poor generalization under out-of-distribution (OOD) scenarios. To address this, we propose an effective Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which imposes identity-oriented constraints to regulate the feature space under the guidance of prior physical knowledge, thereby dealing distribution variability with physical invariance. Specifically, the proposed IDOL employs the wind field model and dark correlation knowledge of TC to model task-shared and task-specific identity tokens. These tokens capture task dependencies and intrinsic physical invariances of TC, enabling robust estimation of TC wind speed, pressure, inner-core, and outer-core size under distribution shifts. Extensive experiments conducted on multiple datasets and tasks demonstrate the outperformance of the proposed IDOL, verifying that imposing identity-oriented constraints based on prior physical knowledge can effectively mitigates diverse distribution shifts in TC estimation.Code is available at https://github.com/Zjut-MultimediaPlus/IDOL.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain</title>
<link>https://arxiv.org/abs/2511.11753</link>
<guid>https://arxiv.org/abs/2511.11753</guid>
<content:encoded><![CDATA[
arXiv:2511.11753v1 Announce Type: new 
Abstract: Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sumudu Neural Operator for ODEs and PDEs</title>
<link>https://arxiv.org/abs/2511.11762</link>
<guid>https://arxiv.org/abs/2511.11762</guid>
<content:encoded><![CDATA[
arXiv:2511.11762v1 Announce Type: new 
Abstract: We introduce the Sumudu Neural Operator (SNO), a neural operator rooted in the properties of the Sumudu Transform. We leverage the relationship between the polynomial expansions of transform pairs to decompose the input space as coefficients, which are then transformed into the Sumudu Space, where the neural operator is parameterized. We evaluate the operator in ODEs (Duffing Oscillator, Lorenz System, and Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burger's Equation, Diffusion, Diffusion-Reaction, and Brusselator). SNO achieves superior performance to FNO on PDEs and demonstrates competitive accuracy with LNO on several PDE tasks, including the lowest error on the Euler-Bernoulli Beam and Diffusion Equation. Additionally, we apply zero-shot super-resolution to the PDE tasks to observe the model's capability of obtaining higher quality data from low-quality samples. These preliminary findings suggest promise for the Sumudu Transform as a neural operator design, particularly for certain classes of PDEs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Fair Representations with Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2511.11767</link>
<guid>https://arxiv.org/abs/2511.11767</guid>
<content:encoded><![CDATA[
arXiv:2511.11767v1 Announce Type: new 
Abstract: Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. In this paper, we try to circumvent these issues by integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach enables a balance between fairness and accuracy. To further facilitate this balance, we propose an adaptive penalty update mechanism that dynamically adjusts fairness constraints during the model training. We conduct numerical experiments on two real-world college admissions datasets, across three different optimization strategies. The results demonstrate the efficiency and robustness of KANs by consistently outperforming the baseline fair learning models, and maintaining high predictive accuracy while achieving competitive fairness across sensitive attributes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CATCHFed: Efficient Unlabeled Data Utilization for Semi-Supervised Federated Learning in Limited Labels Environments</title>
<link>https://arxiv.org/abs/2511.11778</link>
<guid>https://arxiv.org/abs/2511.11778</guid>
<content:encoded><![CDATA[
arXiv:2511.11778v1 Announce Type: new 
Abstract: Federated learning is a promising paradigm that utilizes distributed client resources while preserving data privacy. Most existing FL approaches assume clients possess labeled data, however, in real-world scenarios, client-side labels are often unavailable. Semi-supervised Federated learning, where only the server holds labeled data, addresses this issue. However, it experiences significant performance degradation as the number of labeled data decreases. To tackle this problem, we propose \textit{CATCHFed}, which introduces client-aware adaptive thresholds considering class difficulty, hybrid thresholds to enhance pseudo-label quality, and utilizes unpseudo-labeled data for consistency regularization. Extensive experiments across various datasets and configurations demonstrate that CATCHFed effectively leverages unlabeled client data, achieving superior performance even in extremely limited-label settings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coordinate Descent for Network Linearization</title>
<link>https://arxiv.org/abs/2511.11781</link>
<guid>https://arxiv.org/abs/2511.11781</guid>
<content:encoded><![CDATA[
arXiv:2511.11781v1 Announce Type: new 
Abstract: ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simplicial covering dimension of extremal concept classes</title>
<link>https://arxiv.org/abs/2511.11819</link>
<guid>https://arxiv.org/abs/2511.11819</guid>
<content:encoded><![CDATA[
arXiv:2511.11819v1 Announce Type: new 
Abstract: Dimension theory is a branch of topology concerned with defining and analyzing dimensions of geometric and topological spaces in purely topological terms. In this work, we adapt the classical notion of topological dimension (Lebesgue covering) to binary concept classes. The topological space naturally associated with a concept class is its space of realizable distributions. The loss function and the class itself induce a simplicial structure on this space, with respect to which we define a simplicial covering dimension.
  We prove that for finite concept classes, this simplicial covering dimension exactly characterizes the list replicability number (equivalently, global stability) in PAC learning. This connection allows us to apply tools from classical dimension theory to compute the exact list replicability number of the broad family of extremal concept classes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Constrained Policy Optimization for Cost-Effective LLM Agents</title>
<link>https://arxiv.org/abs/2511.11828</link>
<guid>https://arxiv.org/abs/2511.11828</guid>
<content:encoded><![CDATA[
arXiv:2511.11828v1 Announce Type: new 
Abstract: While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers</title>
<link>https://arxiv.org/abs/2511.11834</link>
<guid>https://arxiv.org/abs/2511.11834</guid>
<content:encoded><![CDATA[
arXiv:2511.11834v1 Announce Type: new 
Abstract: Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Trade-Off Between Transparency and Security in Adversarial Machine Learning</title>
<link>https://arxiv.org/abs/2511.11842</link>
<guid>https://arxiv.org/abs/2511.11842</guid>
<content:encoded><![CDATA[
arXiv:2511.11842v1 Announce Type: new 
Abstract: Transparency and security are both central to Responsible AI, but they may conflict in adversarial settings. We investigate the strategic effect of transparency for agents through the lens of transferable adversarial example attacks. In transferable adversarial example attacks, attackers maliciously perturb their inputs using surrogate models to fool a defender's target model. These models can be defended or undefended, with both players having to decide which to use. Using a large-scale empirical evaluation of nine attacks across 181 models, we find that attackers are more successful when they match the defender's decision; hence, obscurity could be beneficial to the defender. With game theory, we analyze this trade-off between transparency and security by modeling this problem as both a Nash game and a Stackelberg game, and comparing the expected outcomes. Our analysis confirms that only knowing whether a defender's model is defended or not can sometimes be enough to damage its security. This result serves as an indicator of the general trade-off between transparency and security, suggesting that transparency in AI systems can be at odds with security. Beyond adversarial machine learning, our work illustrates how game-theoretic reasoning can uncover conflicts between transparency and security.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Exogenous Signals for Hydrology Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.11849</link>
<guid>https://arxiv.org/abs/2511.11849</guid>
<content:encoded><![CDATA[
arXiv:2511.11849v1 Announce Type: new 
Abstract: Recent advances in time series research facilitate the development of foundation models. While many state-of-the-art time series foundation models have been introduced, few studies examine their effectiveness in specific downstream applications in physical science. This work investigates the role of integrating domain knowledge into time series models for hydrological rainfall-runoff modeling. Using the CAMELS-US dataset, which includes rainfall and runoff data from 671 locations with six time series streams and 30 static features, we compare baseline and foundation models. Results demonstrate that models incorporating comprehensive known exogenous inputs outperform more limited approaches, including foundation models. Notably, incorporating natural annual periodic time series contribute the most significant improvements.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production</title>
<link>https://arxiv.org/abs/2511.11880</link>
<guid>https://arxiv.org/abs/2511.11880</guid>
<content:encoded><![CDATA[
arXiv:2511.11880v1 Announce Type: new 
Abstract: Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better LLM Reasoning via Dual-Play</title>
<link>https://arxiv.org/abs/2511.11881</link>
<guid>https://arxiv.org/abs/2511.11881</guid>
<content:encoded><![CDATA[
arXiv:2511.11881v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLEX: Feature Importance from Layered Counterfactual Explanations</title>
<link>https://arxiv.org/abs/2511.11891</link>
<guid>https://arxiv.org/abs/2511.11891</guid>
<content:encoded><![CDATA[
arXiv:2511.11891v1 Announce Type: new 
Abstract: Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable "what-if" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design</title>
<link>https://arxiv.org/abs/2511.11894</link>
<guid>https://arxiv.org/abs/2511.11894</guid>
<content:encoded><![CDATA[
arXiv:2511.11894v1 Announce Type: new 
Abstract: Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm</title>
<link>https://arxiv.org/abs/2511.11902</link>
<guid>https://arxiv.org/abs/2511.11902</guid>
<content:encoded><![CDATA[
arXiv:2511.11902v1 Announce Type: new 
Abstract: Bidirectional Associative Memory (BAM) trained with Bidirectional Backpropagation (B-BP) often suffers from poor robustness and high sensitivity to noise and adversarial attacks. To address these issues, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which significantly improves the robustness and convergence behavior of BAM. Through comprehensive experiments, we identify two key principles -- orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) -- as central to enhancing the robustness of BAM. Motivated by these findings, we introduce new regularization strategies into B-BP, resulting in models with greatly improved resistance to corruption and adversarial perturbations. We further conduct an ablation study across different training strategies to determine the most robust configuration and evaluate BAM's performance under a variety of attack scenarios and memory capacities, including 50, 100, and 200 associative pairs. Among all methods, the SAME configuration, which integrates both OWM and GPA, achieves the strongest resilience. Overall, our results demonstrate that B-SRA and the proposed regularization strategies lead to substantially more robust associative memories and open new directions for building resilient neural architectures.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Study of Model Extraction Attacks on Graph Foundation Models</title>
<link>https://arxiv.org/abs/2511.11912</link>
<guid>https://arxiv.org/abs/2511.11912</guid>
<content:encoded><![CDATA[
arXiv:2511.11912v1 Announce Type: new 
Abstract: Graph machine learning has advanced rapidly in tasks such as link prediction, anomaly detection, and node classification. As models scale up, pretrained graph models have become valuable intellectual assets because they encode extensive computation and domain expertise. Building on these advances, Graph Foundation Models (GFMs) mark a major step forward by jointly pretraining graph and text encoders on massive and diverse data. This unifies structural and semantic understanding, enables zero-shot inference, and supports applications such as fraud detection and biomedical analysis. However, the high pretraining cost and broad cross-domain knowledge in GFMs also make them attractive targets for model extraction attacks (MEAs). Prior work has focused only on small graph neural networks trained on a single graph, leaving the security implications for large-scale and multimodal GFMs largely unexplored. This paper presents the first systematic study of MEAs against GFMs. We formalize a black-box threat model and define six practical attack scenarios covering domain-level and graph-specific extraction goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. To instantiate these attacks, we introduce a lightweight extraction method that trains an attacker encoder using supervised regression of graph embeddings. Even without contrastive pretraining data, this method learns an encoder that stays aligned with the victim text encoder and preserves its zero-shot inference ability on unseen graphs. Experiments on seven datasets show that the attacker can approximate the victim model using only a tiny fraction of its original training cost, with almost no loss in accuracy. These findings reveal that GFMs greatly expand the MEA surface and highlight the need for deployment-aware security defenses in large-scale graph learning systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Batch Matrix-form Equations and Implementation of Multilayer Perceptrons</title>
<link>https://arxiv.org/abs/2511.11918</link>
<guid>https://arxiv.org/abs/2511.11918</guid>
<content:encoded><![CDATA[
arXiv:2511.11918v1 Announce Type: new 
Abstract: Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.11928</link>
<guid>https://arxiv.org/abs/2511.11928</guid>
<content:encoded><![CDATA[
arXiv:2511.11928v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts</title>
<link>https://arxiv.org/abs/2511.11934</link>
<guid>https://arxiv.org/abs/2511.11934</guid>
<content:encoded><![CDATA[
arXiv:2511.11934v1 Announce Type: new 
Abstract: We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis</title>
<link>https://arxiv.org/abs/2511.11935</link>
<guid>https://arxiv.org/abs/2511.11935</guid>
<content:encoded><![CDATA[
arXiv:2511.11935v1 Announce Type: new 
Abstract: Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the "preprocessing gap" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning the relative composition of EEG signals using pairwise relative shift pretraining</title>
<link>https://arxiv.org/abs/2511.11940</link>
<guid>https://arxiv.org/abs/2511.11940</guid>
<content:encoded><![CDATA[
arXiv:2511.11940v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computation-aware Energy-harvesting Federated Learning: Cyclic Scheduling with Selective Participation</title>
<link>https://arxiv.org/abs/2511.11949</link>
<guid>https://arxiv.org/abs/2511.11949</guid>
<content:encoded><![CDATA[
arXiv:2511.11949v1 Announce Type: new 
Abstract: Federated Learning (FL) is a powerful paradigm for distributed learning, but its increasing complexity leads to significant energy consumption from client-side computations for training models. In particular, the challenge is critical in energy-harvesting FL (EHFL) systems where participation availability of each device oscillates due to limited energy. To address this, we propose FedBacys, a battery-aware EHFL framework using cyclic client participation based on users' battery levels. By clustering clients and scheduling them sequentially, FedBacys minimizes redundant computations, reduces system-wide energy usage, and improves learning stability. We also introduce FedBacys-Odd, a more energy-efficient variant that allows clients to participate selectively, further reducing energy costs without compromising performance. We provide a convergence analysis for our framework and demonstrate its superior energy efficiency and robustness compared to existing algorithms through numerical experiments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression</title>
<link>https://arxiv.org/abs/2511.11973</link>
<guid>https://arxiv.org/abs/2511.11973</guid>
<content:encoded><![CDATA[
arXiv:2511.11973v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $\beta$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCast: Reliability-aware Codebook Assisted Lightweight Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.11991</link>
<guid>https://arxiv.org/abs/2511.11991</guid>
<content:encoded><![CDATA[
arXiv:2511.11991v1 Announce Type: new 
Abstract: Time series forecasting is crucial for applications in various domains. Conventional methods often rely on global decomposition into trend, seasonal, and residual components, which become ineffective for real-world series dominated by local, complex, and highly dynamic patterns. Moreover, the high model complexity of such approaches limits their applicability in real-time or resource-constrained environments. In this work, we propose a novel \textbf{RE}liability-aware \textbf{C}odebook-\textbf{AS}sisted \textbf{T}ime series forecasting framework (\textbf{ReCast}) that enables lightweight and robust prediction by exploiting recurring local shapes. ReCast encodes local patterns into discrete embeddings through patch-wise quantization using a learnable codebook, thereby compactly capturing stable regular structures. To compensate for residual variations not preserved by quantization, ReCast employs a dual-path architecture comprising a quantization path for efficient modeling of regular structures and a residual path for reconstructing irregular fluctuations. A central contribution of ReCast is a reliability-aware codebook update strategy, which incrementally refines the codebook via weighted corrections. These correction weights are derived by fusing multiple reliability factors from complementary perspectives by a distributionally robust optimization (DRO) scheme, ensuring adaptability to non-stationarity and robustness to distribution shifts. Extensive experiments demonstrate that ReCast outperforms state-of-the-art (SOTA) models in accuracy, efficiency, and adaptability to distribution shifts.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selecting Fine-Tuning Examples by Quizzing VLMs</title>
<link>https://arxiv.org/abs/2511.12002</link>
<guid>https://arxiv.org/abs/2511.12002</guid>
<content:encoded><![CDATA[
arXiv:2511.12002v1 Announce Type: new 
Abstract: A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \textit{do} exemplify the target concept (e.g., a \textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation</title>
<link>https://arxiv.org/abs/2511.12033</link>
<guid>https://arxiv.org/abs/2511.12033</guid>
<content:encoded><![CDATA[
arXiv:2511.12033v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and na\"ively spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers</title>
<link>https://arxiv.org/abs/2511.12041</link>
<guid>https://arxiv.org/abs/2511.12041</guid>
<content:encoded><![CDATA[
arXiv:2511.12041v1 Announce Type: new 
Abstract: Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread</title>
<link>https://arxiv.org/abs/2511.12071</link>
<guid>https://arxiv.org/abs/2511.12071</guid>
<content:encoded><![CDATA[
arXiv:2511.12071v1 Announce Type: new 
Abstract: The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Treatment Stitching with Schr\"odinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies</title>
<link>https://arxiv.org/abs/2511.12075</link>
<guid>https://arxiv.org/abs/2511.12075</guid>
<content:encoded><![CDATA[
arXiv:2511.12075v1 Announce Type: new 
Abstract: Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the Schr\"odinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SenseRay-3D: Generalizable and Physics-Informed Framework for End-to-End Indoor Propagation Modeling</title>
<link>https://arxiv.org/abs/2511.12092</link>
<guid>https://arxiv.org/abs/2511.12092</guid>
<content:encoded><![CDATA[
arXiv:2511.12092v1 Announce Type: new 
Abstract: Modeling indoor radio propagation is crucial for wireless network planning and optimization. However, existing approaches often rely on labor-intensive manual modeling of geometry and material properties, resulting in limited scalability and efficiency. To overcome these challenges, this paper presents SenseRay-3D, a generalizable and physics-informed end-to-end framework that predicts three-dimensional (3D) path-loss heatmaps directly from RGB-D scans, thereby eliminating the need for explicit geometry reconstruction or material annotation. The proposed framework builds a sensing-driven voxelized scene representation that jointly encodes occupancy, electromagnetic material characteristics, and transmitter-receiver geometry, which is processed by a SwinUNETR-based neural network to infer environmental path-loss relative to free-space path-loss. A comprehensive synthetic indoor propagation dataset is further developed to validate the framework and to serve as a standardized benchmark for future research. Experimental results show that SenseRay-3D achieves a mean absolute error of 4.27 dB on unseen environments and supports real-time inference at 217 ms per sample, demonstrating its scalability, efficiency, and physical consistency. SenseRay-3D paves a new path for sense-driven, generalizable, and physics-consistent modeling of indoor propagation, marking a major leap beyond our pioneering EM DeepRay framework.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance</title>
<link>https://arxiv.org/abs/2511.12121</link>
<guid>https://arxiv.org/abs/2511.12121</guid>
<content:encoded><![CDATA[
arXiv:2511.12121v1 Announce Type: new 
Abstract: Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Anomaly Identification in Accounting Transactions via Multi-Head Self-Attention Networks</title>
<link>https://arxiv.org/abs/2511.12122</link>
<guid>https://arxiv.org/abs/2511.12122</guid>
<content:encoded><![CDATA[
arXiv:2511.12122v1 Announce Type: new 
Abstract: This study addresses the problem of dynamic anomaly detection in accounting transactions and proposes a real-time detection method based on a Transformer to tackle the challenges of hidden abnormal behaviors and high timeliness requirements in complex trading environments. The approach first models accounting transaction data by representing multi-dimensional records as time-series matrices and uses embedding layers and positional encoding to achieve low-dimensional mapping of inputs. A sequence modeling structure with multi-head self-attention is then constructed to capture global dependencies and aggregate features from multiple perspectives, thereby enhancing the ability to detect abnormal patterns. The network further integrates feed-forward layers and regularization strategies to achieve deep feature representation and accurate anomaly probability estimation. To validate the effectiveness of the method, extensive experiments were conducted on a public dataset, including comparative analysis, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms baseline models in AUC, F1-Score, Precision, and Recall, and maintains stable performance under different environmental conditions and data perturbations. These findings confirm the applicability and advantages of the Transformer-based framework for dynamic anomaly detection in accounting transactions and provide methodological support for intelligent financial risk control and auditing.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.12123</link>
<guid>https://arxiv.org/abs/2511.12123</guid>
<content:encoded><![CDATA[
arXiv:2511.12123v1 Announce Type: new 
Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairGSE: Fairness-Aware Graph Neural Network without High False Positive Rates</title>
<link>https://arxiv.org/abs/2511.12132</link>
<guid>https://arxiv.org/abs/2511.12132</guid>
<content:encoded><![CDATA[
arXiv:2511.12132v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) have emerged as the mainstream paradigm for graph representation learning due to their effective message aggregation. However, this advantage also amplifies biases inherent in graph topology, raising fairness concerns. Existing fairness-aware GNNs provide satisfactory performance on fairness metrics such as Statistical Parity and Equal Opportunity while maintaining acceptable accuracy trade-offs. Unfortunately, we observe that this pursuit of fairness metrics neglects the GNN's ability to predict negative labels, which renders their predictions with extremely high False Positive Rates (FPR), resulting in negative effects in high-risk scenarios. To this end, we advocate that classification performance should be carefully calibrated while improving fairness, rather than simply constraining accuracy loss. Furthermore, we propose Fair GNN via Structural Entropy (\textbf{FairGSE}), a novel framework that maximizes two-dimensional structural entropy (2D-SE) to improve fairness without neglecting false positives. Experiments on several real-world datasets show FairGSE reduces FPR by 39\% vs. state-of-the-art fairness-aware GNNs, with comparable fairness improvement.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusion-ResNet: A Lightweight multi-label NILM Model Using PCA-ICA Feature Fusion</title>
<link>https://arxiv.org/abs/2511.12139</link>
<guid>https://arxiv.org/abs/2511.12139</guid>
<content:encoded><![CDATA[
arXiv:2511.12139v1 Announce Type: new 
Abstract: Non-intrusive load monitoring (NILM) is an advanced load monitoring technique that uses data-driven algorithms to disaggregate the total power consumption of a household into the consumption of individual appliances. However, real-world NILM deployment still faces major challenges, including overfitting, low model generalization, and disaggregating a large number of appliances operating at the same time. To address these challenges, this work proposes an end-to-end framework for the NILM classification task, which consists of high-frequency labeled data, a feature extraction method, and a lightweight neural network. Within this framework, we introduce a novel feature extraction method that fuses Independent Component Analysis (ICA) and Principal Component Analysis (PCA) features. Moreover, we propose a lightweight architecture for multi-label NILM classification (Fusion-ResNet). The proposed feature-based model achieves a higher $F1$ score on average and across different appliances compared to state-of-the-art NILM classifiers while minimizing the training and inference time. Finally, we assessed the performance of our model against baselines with a varying number of simultaneously active devices. Results demonstrate that Fusion-ResNet is relatively robust to stress conditions with up to 15 concurrently active appliances.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variation-Bounded Loss for Noise-Tolerant Learning</title>
<link>https://arxiv.org/abs/2511.12143</link>
<guid>https://arxiv.org/abs/2511.12143</guid>
<content:encoded><![CDATA[
arXiv:2511.12143v1 Announce Type: new 
Abstract: Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finding Time Series Anomalies using Granular-ball Vector Data Description</title>
<link>https://arxiv.org/abs/2511.12147</link>
<guid>https://arxiv.org/abs/2511.12147</guid>
<content:encoded><![CDATA[
arXiv:2511.12147v1 Announce Type: new 
Abstract: Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions</title>
<link>https://arxiv.org/abs/2511.12154</link>
<guid>https://arxiv.org/abs/2511.12154</guid>
<content:encoded><![CDATA[
arXiv:2511.12154v1 Announce Type: new 
Abstract: We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Deep Alignment Through The Lens Of Incomplete Learning</title>
<link>https://arxiv.org/abs/2511.12155</link>
<guid>https://arxiv.org/abs/2511.12155</guid>
<content:encoded><![CDATA[
arXiv:2511.12155v1 Announce Type: new 
Abstract: Large language models exhibit systematic vulnerabilities to adversarial attacks despite extensive safety alignment. We provide a mechanistic analysis revealing that position-dependent gradient weakening during autoregressive training creates signal decay, leading to incomplete safety learning where safety training fails to transform model preferences in later response regions fully. We introduce base-favored tokens -- vocabulary elements where base models assign higher probability than aligned models -- as computational indicators of incomplete safety learning and develop a targeted completion method that addresses undertrained regions through adaptive penalties and hybrid teacher distillation. Experimental evaluation across Llama and Qwen model families demonstrates dramatic improvements in adversarial robustness, with 48--98% reductions in attack success rates while preserving general capabilities. These results establish both a mechanistic understanding and practical solutions for fundamental limitations in safety alignment methodologies.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Efficient Self-Supervised Algorithms for Fine-Grained Birdsong Analysis</title>
<link>https://arxiv.org/abs/2511.12158</link>
<guid>https://arxiv.org/abs/2511.12158</guid>
<content:encoded><![CDATA[
arXiv:2511.12158v1 Announce Type: new 
Abstract: Many bioacoustics, neuroscience, and linguistics research utilize birdsongs as proxy models to acquire knowledge in diverse areas. Developing models generally requires precisely annotated data at the level of syllables. Hence, automated and data-efficient methods that reduce annotation costs are in demand. This work presents a lightweight, yet performant neural network architecture for birdsong annotation called Residual-MLP-RNN. Then, it presents a robust three-stage training pipeline for developing reliable deep birdsong syllable detectors with minimal expert labor. The first stage is self-supervised learning from unlabeled data. Two of the most successful pretraining paradigms are explored, namely, masked prediction and online clustering. The second stage is supervised training with effective data augmentations to create a robust model for frame-level syllable detection. The third stage is semi-supervised post-training, which leverages the unlabeled data again. However, unlike the initial phase, this time it is aligned with the downstream task. The performance of this data-efficient approach is demonstrated for the complex song of the Canary in extreme label-scarcity scenarios. Canary has one of the most difficult songs to annotate, which implicitly validates the method for other birds. Finally, the potential of self-supervised embeddings is assessed for linear probing and unsupervised birdsong analysis.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FGM optimization in complex domains using Gaussian process regression based profile generation algorithm</title>
<link>https://arxiv.org/abs/2511.12171</link>
<guid>https://arxiv.org/abs/2511.12171</guid>
<content:encoded><![CDATA[
arXiv:2511.12171v1 Announce Type: new 
Abstract: This manuscript addresses the challenge of designing functionally graded materials (FGMs) for arbitrary-shaped domains. Towards this goal, the present work proposes a generic volume fraction profile generation algorithm based on Gaussian Process Regression (GPR). The proposed algorithm can handle complex-shaped domains and generate smooth FGM profiles while adhering to the specified volume fraction values at boundaries/part of boundaries. The resulting design space from GPR comprises diverse profiles, enhancing the potential for discovering optimal configurations. Further, the algorithm allows the user to control the smoothness of the underlying profiles and the size of the design space through a length scale parameter. Further, the proposed profile generation scheme is coupled with the genetic algorithm to find the optimum FGM profiles for a given application. To make the genetic algorithm consistent with the GPR profile generation scheme, the standard simulated binary crossover operator in the genetic algorithm has been modified with a projection operator. We present numerous thermoelastic optimization examples to demonstrate the efficacy of the proposed profile generation algorithm and optimization framework.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective</title>
<link>https://arxiv.org/abs/2511.12174</link>
<guid>https://arxiv.org/abs/2511.12174</guid>
<content:encoded><![CDATA[
arXiv:2511.12174v1 Announce Type: new 
Abstract: Diffusion models have shown great promise in data generation, yet generating time series data remains challenging due to the need to capture complex temporal dependencies and structural patterns. In this paper, we present \textit{TSGDiff}, a novel framework that rethinks time series generation from a graph-based perspective. Specifically, we represent time series as dynamic graphs, where edges are constructed based on Fourier spectrum characteristics and temporal dependencies. A graph neural network-based encoder-decoder architecture is employed to construct a latent space, enabling the diffusion process to model the structural representation distribution of time series effectively. Furthermore, we propose the Topological Structure Fidelity (Topo-FID) score, a graph-aware metric for assessing the structural similarity of time series graph representations. Topo-FID integrates two sub-metrics: Graph Edit Similarity, which quantifies differences in adjacency matrices, and Structural Entropy Similarity, which evaluates the entropy of node degree distributions. This comprehensive metric provides a more accurate assessment of structural fidelity in generated time series. Experiments on real-world datasets demonstrate that \textit{TSGDiff} generates high-quality synthetic time series data generation, faithfully preserving temporal dependencies and structural integrity, thereby advancing the field of synthetic time series generation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering</title>
<link>https://arxiv.org/abs/2511.12180</link>
<guid>https://arxiv.org/abs/2511.12180</guid>
<content:encoded><![CDATA[
arXiv:2511.12180v1 Announce Type: new 
Abstract: Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?</title>
<link>https://arxiv.org/abs/2511.12188</link>
<guid>https://arxiv.org/abs/2511.12188</guid>
<content:encoded><![CDATA[
arXiv:2511.12188v1 Announce Type: new 
Abstract: The recent success of large language models (LLMs) has sparked a growing interest in training large-scale models. As the model size continues to scale, concerns are growing about the depletion of high-quality, well-curated training data. This has led practitioners to explore training approaches like Federated Learning (FL), which can leverage the abundant data on edge devices while maintaining privacy. However, the decentralization of training datasets in FL introduces challenges to scaling large models, a topic that remains under-explored. This paper fills this gap and provides qualitative insights on generalizing the previous model scaling experience to federated learning scenarios. Specifically, we derive a PAC-Bayes (Probably Approximately Correct Bayesian) upper bound for the generalization error of models trained with stochastic algorithms in federated settings and quantify the impact of distributed training data on the optimal model size by finding the analytic solution of model size that minimizes this bound. Our theoretical results demonstrate that the optimal model size has a negative power law relationship with the number of clients if the total training compute is unchanged. Besides, we also find that switching to FL with the same training compute will inevitably reduce the upper bound of generalization performance that the model can achieve through training, and that estimating the optimal model size in federated scenarios should depend on the average training compute across clients. Furthermore, we also empirically validate the correctness of our results with extensive training runs on different models, network settings, and datasets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of Multi- and Single-objective Learning Algorithms for Imbalanced Data</title>
<link>https://arxiv.org/abs/2511.12191</link>
<guid>https://arxiv.org/abs/2511.12191</guid>
<content:encoded><![CDATA[
arXiv:2511.12191v1 Announce Type: new 
Abstract: Many machine learning tasks aim to find models that work well not for a single, but for a group of criteria, often opposing ones. One such example is imbalanced data classification, where, on the one hand, we want to achieve the best possible classification quality for data from the minority class without degrading the classification quality of the majority class. One solution is to propose an aggregate learning criterion and reduce the multi-objective learning task to a single-criteria optimization problem. Unfortunately, such an approach is characterized by ambiguity of interpretation since the value of the aggregated criterion does not indicate the value of the component criteria. Hence, there are more and more proposals for algorithms based on multi-objective optimization (MOO), which can simultaneously optimize multiple criteria. However, such an approach results in a set of multiple non-dominated solutions (Pareto front). The selection of a single solution from the Pareto front is a challenge itself, and much attention is paid to the issue of how to select it considering user preferences, as well as how to compare solutions returned by different MOO algorithms among themselves. Thus, a significant gap has been identified in the classifier evaluation methodology, i.e., how to reliably compare methods returning single solutions with algorithms returning solutions in the form of Pareto fronts.
  To fill the aforementioned gap, this article proposes a new, reliable way of evaluating algorithms based on multi-objective algorithms with methods that return single solutions while pointing out solutions from a Pareto front tailored to the user's preferences. This work focuses only on algorithm comparison, not their learning. The algorithms selected for this study are illustrative to help understand the proposed approach.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization</title>
<link>https://arxiv.org/abs/2511.12199</link>
<guid>https://arxiv.org/abs/2511.12199</guid>
<content:encoded><![CDATA[
arXiv:2511.12199v1 Announce Type: new 
Abstract: The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and its implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potential lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG function variants, and spike encoding schemes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignTree: Efficient Defense Against LLM Jailbreak Attacks</title>
<link>https://arxiv.org/abs/2511.12217</link>
<guid>https://arxiv.org/abs/2511.12217</guid>
<content:encoded><![CDATA[
arXiv:2511.12217v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chicken Swarm Kernel Particle Filter: A Structured Rejuvenation Approach with KLD-Efficient Sampling</title>
<link>https://arxiv.org/abs/2511.12222</link>
<guid>https://arxiv.org/abs/2511.12222</guid>
<content:encoded><![CDATA[
arXiv:2511.12222v1 Announce Type: new 
Abstract: Particle filters (PFs) are often combined with swarm intelligence (SI) algorithms, such as Chicken Swarm Optimization (CSO), for particle rejuvenation. Separately, Kullback--Leibler divergence (KLD) sampling is a common strategy for adaptively sizing the particle set. However, the theoretical interaction between SI-based rejuvenation kernels and KLD-based adaptive sampling is not yet fully understood.
  This paper investigates this specific interaction. We analyze, under a simplified modeling framework, the effect of the CSO rejuvenation step on the particle set distribution. We propose that the fitness-driven updates inherent in CSO can be approximated as a form of mean-square contraction. This contraction tends to produce a particle distribution that is more concentrated than that of a baseline PF, or in mathematical terms, a distribution that is plausibly more ``peaked'' in a majorization sense.
  By applying Karamata's inequality to the concave function that governs the expected bin occupancy in KLD-sampling, our analysis suggests a connection: under the stated assumptions, the CSO-enhanced PF (CPF) is expected to require a lower \emph{expected} particle count than the standard PF to satisfy the same statistical error bound. The goal of this study is not to provide a fully general proof, but rather to offer a tractable theoretical framework that helps to interpret the computational efficiency empirically observed when combining these techniques, and to provide a starting point for designing more efficient adaptive filters.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCI: An Equilibrium for Signal Intelligence</title>
<link>https://arxiv.org/abs/2511.12240</link>
<guid>https://arxiv.org/abs/2511.12240</guid>
<content:encoded><![CDATA[
arXiv:2511.12240v1 Announce Type: new 
Abstract: We present SCI, a closed-loop, control-theoretic framework that models interpretability as a regulated state. SCI formalizes the interpretive error Delta SP and actively drives SP(t) in [0, 1] ("Surgical Precision") toward a target via a projected update on the parameters Theta under a human-gain budget. The framework operates through three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; and (3) a Lyapunov-guided controller equipped with rollback, trust-region safeguards, and a descent condition. Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25-42% (mean 38%, 95% confidence interval 22-43%) relative to static explainers while maintaining AUC/F1 within approximately 1-2 percentage points of baseline. SCI also reduces SP variance from 0.030 to 0.011, indicating substantially more stable explanations. Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection</title>
<link>https://arxiv.org/abs/2511.12261</link>
<guid>https://arxiv.org/abs/2511.12261</guid>
<content:encoded><![CDATA[
arXiv:2511.12261v1 Announce Type: new 
Abstract: Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks</title>
<link>https://arxiv.org/abs/2511.12265</link>
<guid>https://arxiv.org/abs/2511.12265</guid>
<content:encoded><![CDATA[
arXiv:2511.12265v1 Announce Type: new 
Abstract: Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMSense: Adapting Vision-based Foundation Model for Multi-task Multi-modal Wireless Sensing</title>
<link>https://arxiv.org/abs/2511.12305</link>
<guid>https://arxiv.org/abs/2511.12305</guid>
<content:encoded><![CDATA[
arXiv:2511.12305v1 Announce Type: new 
Abstract: Large AI models have been widely adopted in wireless communications for channel modeling, beamforming, and resource optimization. However, most existing efforts remain limited to single-modality inputs and channel-specific objec- tives, overlooking the broader potential of large foundation models for unified wireless sensing. To bridge this gap, we propose MMSense, a multi-modal, multi-task foundation model that jointly addresses channel-centric, environment-aware, and human-centered sensing. Our framework integrates image, radar, LiDAR, and textual data by transforming them into vision- compatible representations, enabling effective cross-modal align- ment within a unified feature space. A modality gating mecha- nism adaptively fuses these representations, while a vision-based large language model backbone enables unified feature align- ment and instruction-driven task adaptation. Furthermore, task- specific sequential attention and uncertainty-based loss weighting mechanisms enhance cross-task generalization. Experiments on real wireless scenario datasets show that our approach outper- forms both task-specific and large-model baselines, confirming its strong generalization across heterogeneous sensing tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Self-Consistency for Efficient Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2511.12309</link>
<guid>https://arxiv.org/abs/2511.12309</guid>
<content:encoded><![CDATA[
arXiv:2511.12309v1 Announce Type: new 
Abstract: Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Learning of Symbolic Automata Over Rational Numbers</title>
<link>https://arxiv.org/abs/2511.12315</link>
<guid>https://arxiv.org/abs/2511.12315</guid>
<content:encoded><![CDATA[
arXiv:2511.12315v1 Announce Type: new 
Abstract: Automata learning has many applications in artificial intelligence and software engineering. Central to these applications is the $L^*$ algorithm, introduced by Angluin. The $L^*$ algorithm learns deterministic finite-state automata (DFAs) in polynomial time when provided with a minimally adequate teacher. Unfortunately, the $L^*$ algorithm can only learn DFAs over finite alphabets, which limits its applicability. In this paper, we extend $L^*$ to learn symbolic automata whose transitions use predicates over rational numbers, i.e., over infinite and dense alphabets. Our result makes the $L^*$ algorithm applicable to new settings like (real) RGX, and time series. Furthermore, our proposed algorithm is optimal in the sense that it asks a number of queries to the teacher that is at most linear with respect to the number of transitions, and to the representation size of the predicates.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data</title>
<link>https://arxiv.org/abs/2511.12316</link>
<guid>https://arxiv.org/abs/2511.12316</guid>
<content:encoded><![CDATA[
arXiv:2511.12316v1 Announce Type: new 
Abstract: We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LILogic Net: Compact Logic Gate Networks with Learnable Connectivity for Efficient Hardware Deployment</title>
<link>https://arxiv.org/abs/2511.12340</link>
<guid>https://arxiv.org/abs/2511.12340</guid>
<content:encoded><![CDATA[
arXiv:2511.12340v1 Announce Type: new 
Abstract: Efficient deployment of machine learning models ultimately requires taking hardware constraints into account. The binary logic gate is the fundamental building block of all digital chips. Designing models that operate directly on these units enables energy-efficient computation. Recent work has demonstrated the feasibility of training randomly connected networks of binary logic gates (such as OR and NAND) using gradient-based methods. We extend this approach by using gradient descent not only to select the logic gates but also to optimize their interconnections (the connectome). Optimizing the connections allows us to substantially reduce the number of logic gates required to fit a particular dataset. Our implementation is efficient both at training and inference: for instance, our LILogicNet model with only 8,000 gates can be trained on MNIST in under 5 minutes and achieves 98.45% test accuracy, matching the performance of state-of-the-art models that require at least two orders of magnitude more gates. Moreover, for our largest architecture with 256,000 gates, LILogicNet achieves 60.98% test accuracy on CIFAR-10 exceeding the performance of prior logic-gate-based models with a comparable gate budget. At inference time, the fully binarized model operates with minimal compute overhead, making it exceptionally efficient and well suited for deployment on low-power digital hardware.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2511.12351</link>
<guid>https://arxiv.org/abs/2511.12351</guid>
<content:encoded><![CDATA[
arXiv:2511.12351v1 Announce Type: new 
Abstract: Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BitSnap: Checkpoint Sparsification and Quantization in LLM Training</title>
<link>https://arxiv.org/abs/2511.12376</link>
<guid>https://arxiv.org/abs/2511.12376</guid>
<content:encoded><![CDATA[
arXiv:2511.12376v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to grow in size and complexity, efficient checkpoint saving\&amp;loading has become crucial for managing storage, memory usage, and fault tolerance in LLM training. The current works do not comprehensively take into account the optimization of these several aspects. This paper proposes a novel checkpoint sparsification and quantization method that adapts dynamically to different training stages and model architectures. We present a comprehensive analysis of existing lossy and lossless compression techniques, identify current limitations, and introduce our adaptive approach that balances compression ratio, speed, and precision impact throughout the training process. Experiments on different sizes of LLMs demonstrate that our bitmask-based sparsification method achieves 16x compression ratio without compromising model accuracy. Additionally, the cluster-based quantization method achieves 2x compression ratio with little precision loss.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CEDL: Centre-Enhanced Discriminative Learning for Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.12388</link>
<guid>https://arxiv.org/abs/2511.12388</guid>
<content:encoded><![CDATA[
arXiv:2511.12388v1 Announce Type: new 
Abstract: Supervised anomaly detection methods perform well in identifying known anomalies that are well represented in the training set. However, they often struggle to generalise beyond the training distribution due to decision boundaries that lack a clear definition of normality. Existing approaches typically address this by regularising the representation space during training, leading to separate optimisation in latent and label spaces. The learned normality is therefore not directly utilised at inference, and their anomaly scores often fall within arbitrary ranges that require explicit mapping or calibration for probabilistic interpretation. To achieve unified learning of geometric normality and label discrimination, we propose Centre-Enhanced Discriminative Learning (CEDL), a novel supervised anomaly detection framework that embeds geometric normality directly into the discriminative objective. CEDL reparameterises the conventional sigmoid-derived prediction logit through a centre-based radial distance function, unifying geometric and discriminative learning in a single end-to-end formulation. This design enables interpretable, geometry-aware anomaly scoring without post-hoc thresholding or reference calibration. Extensive experiments on tabular, time-series, and image data demonstrate that CEDL achieves competitive and balanced performance across diverse real-world anomaly detection tasks, validating its effectiveness and broad applicability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Dimension-Free Approximation of Deep Neural Networks for Symmetric Korobov Functions</title>
<link>https://arxiv.org/abs/2511.12398</link>
<guid>https://arxiv.org/abs/2511.12398</guid>
<content:encoded><![CDATA[
arXiv:2511.12398v1 Announce Type: new 
Abstract: Deep neural networks have been widely used as universal approximators for functions with inherent physical structures, including permutation symmetry. In this paper, we construct symmetric deep neural networks to approximate symmetric Korobov functions and prove that both the convergence rate and the constant prefactor scale at most polynomially with respect to the ambient dimension. This represents a substantial improvement over prior approximation guarantees that suffer from the curse of dimensionality. Building on these approximation bounds, we further derive a generalization-error rate for learning symmetric Korobov functions whose leading factors likewise avoid the curse of dimensionality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario</title>
<link>https://arxiv.org/abs/2511.12409</link>
<guid>https://arxiv.org/abs/2511.12409</guid>
<content:encoded><![CDATA[
arXiv:2511.12409v1 Announce Type: new 
Abstract: Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models</title>
<link>https://arxiv.org/abs/2511.12414</link>
<guid>https://arxiv.org/abs/2511.12414</guid>
<content:encoded><![CDATA[
arXiv:2511.12414v1 Announce Type: new 
Abstract: Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response "Sure" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the "Sure" rate approaches 100\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation</title>
<link>https://arxiv.org/abs/2511.12417</link>
<guid>https://arxiv.org/abs/2511.12417</guid>
<content:encoded><![CDATA[
arXiv:2511.12417v1 Announce Type: new 
Abstract: Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tailored Primitive Initialization is the Secret Key to Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.12429</link>
<guid>https://arxiv.org/abs/2511.12429</guid>
<content:encoded><![CDATA[
arXiv:2511.12429v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VISAGNN: Versatile Staleness-Aware Efficient Training on Large-Scale Graphs</title>
<link>https://arxiv.org/abs/2511.12434</link>
<guid>https://arxiv.org/abs/2511.12434</guid>
<content:encoded><![CDATA[
arXiv:2511.12434v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have shown exceptional success in graph representation learning and a wide range of real-world applications. However, scaling deeper GNNs poses challenges due to the neighbor explosion problem when training on large-scale graphs. To mitigate this, a promising class of GNN training algorithms utilizes historical embeddings to reduce computation and memory costs while preserving the expressiveness of the model. These methods leverage historical embeddings for out-of-batch nodes, effectively approximating full-batch training without losing any neighbor information-a limitation found in traditional sampling methods. However, the staleness of these historical embeddings often introduces significant bias, acting as a bottleneck that can adversely affect model performance. In this paper, we propose a novel VersatIle Staleness-Aware GNN, named VISAGNN, which dynamically and adaptively incorporates staleness criteria into the large-scale GNN training process. By embedding staleness into the message passing mechanism, loss function, and historical embeddings during training, our approach enables the model to adaptively mitigate the negative effects of stale embeddings, thereby reducing estimation errors and enhancing downstream accuracy. Comprehensive experiments demonstrate the effectiveness of our method in overcoming the staleness issue of existing historical embedding techniques, showcasing its superior performance and efficiency on large-scale benchmarks, along with significantly faster convergence.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global-Lens Transformers: Adaptive Token Mixing for Dynamic Link Prediction</title>
<link>https://arxiv.org/abs/2511.12442</link>
<guid>https://arxiv.org/abs/2511.12442</guid>
<content:encoded><![CDATA[
arXiv:2511.12442v1 Announce Type: new 
Abstract: Dynamic graph learning plays a pivotal role in modeling evolving relationships over time, especially for temporal link prediction tasks in domains such as traffic systems, social networks, and recommendation platforms. While Transformer-based models have demonstrated strong performance by capturing long-range temporal dependencies, their reliance on self-attention results in quadratic complexity with respect to sequence length, limiting scalability on high-frequency or large-scale graphs. In this work, we revisit the necessity of self-attention in dynamic graph modeling. Inspired by recent findings that attribute the success of Transformers more to their architectural design than attention itself, we propose GLFormer, a novel attention-free Transformer-style framework for dynamic graphs. GLFormer introduces an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals. To capture long-term dependencies, we further design a hierarchical aggregation module that expands the temporal receptive field by stacking local token mixers across layers. Experiments on six widely-used dynamic graph benchmarks show that GLFormer achieves SOTA performance, which reveals that attention-free architectures can match or surpass Transformer baselines in dynamic graph settings with significantly improved efficiency.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection</title>
<link>https://arxiv.org/abs/2511.12460</link>
<guid>https://arxiv.org/abs/2511.12460</guid>
<content:encoded><![CDATA[
arXiv:2511.12460v1 Announce Type: new 
Abstract: Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at https://github.com/hacilab/P3HF.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label Feature Selection</title>
<link>https://arxiv.org/abs/2511.12462</link>
<guid>https://arxiv.org/abs/2511.12462</guid>
<content:encoded><![CDATA[
arXiv:2511.12462v1 Announce Type: new 
Abstract: Multi-view multi-label data offers richer perspectives for artificial intelligence, but simultaneously presents significant challenges for feature selection due to the inherent complexity of interrelations among features, views and labels. Attention mechanisms provide an effective way for analyzing these intricate relationships. They can compute importance weights for information by aggregating correlations between Query and Key matrices to focus on pertinent values. However, existing attention-based feature selection methods predominantly focus on intra-view relationships, neglecting the complementarity of inter-view features and the critical feature-label correlations. Moreover, they often fail to account for feature redundancy, potentially leading to suboptimal feature subsets. To overcome these limitations, we propose a novel method based on Redundancy-optimized Multi-head Attention Networks for Multi-view Multi-label Feature Selection (RMAN-MMFS). Specifically, we employ each individual attention head to model intra-view feature relationships and use the cross-attention mechanisms between different heads to capture inter-view feature complementarity. Furthermore, we design static and dynamic feature redundancy terms: the static term mitigates redundancy within each view, while the dynamic term explicitly models redundancy between unselected and selected features across the entire selection process, thereby promoting feature compactness. Comprehensive evaluations on six real-world datasets, compared against six multi-view multi-label feature selection methods, demonstrate the superior performance of the proposed method.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction</title>
<link>https://arxiv.org/abs/2511.12467</link>
<guid>https://arxiv.org/abs/2511.12467</guid>
<content:encoded><![CDATA[
arXiv:2511.12467v1 Announce Type: new 
Abstract: This letter studies the problem of online multi-step-ahead prediction for unknown linear stochastic systems. Using conditional distribution theory, we derive an optimal parameterization of the prediction policy as a linear function of future inputs, past inputs, and past outputs. Based on this characterization, we propose an online least-squares algorithm to learn the policy and analyze its regret relative to the optimal model-based predictor. We show that the online algorithm achieves logarithmic regret with respect to the optimal Kalman filter in the multi-step setting. Furthermore, with new proof techniques, we establish an almost-sure regret bound that does not rely on fixed failure probabilities for sufficiently large horizons $N$. Finally, our analysis also reveals that, while the regret remains logarithmic in $N$, its constant factor grows polynomially with the prediction horizon $H$, with the polynomial order set by the largest Jordan block of eigenvalue 1 in the system matrix.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Model Based Signal Recovery Under 1-Bit Quantization</title>
<link>https://arxiv.org/abs/2511.12471</link>
<guid>https://arxiv.org/abs/2511.12471</guid>
<content:encoded><![CDATA[
arXiv:2511.12471v1 Announce Type: new 
Abstract: Diffusion models (DMs) have demonstrated to be powerful priors for signal recovery, but their application to 1-bit quantization tasks, such as 1-bit compressed sensing and logistic regression, remains a challenge. This difficulty stems from the inherent non-linear link function in these tasks, which is either non-differentiable or lacks an explicit characterization. To tackle this issue, we introduce Diff-OneBit, which is a fast and effective DM-based approach for signal recovery under 1-bit quantization. Diff-OneBit addresses the challenge posed by non-differentiable or implicit links functions via leveraging a differentiable surrogate likelihood function to model 1-bit quantization, thereby enabling gradient based iterations. This function is integrated into a flexible plug-and-play framework that decouples the data-fidelity term from the diffusion prior, allowing any pretrained DM to act as a denoiser within the iterative reconstruction process. Extensive experiments on the FFHQ, CelebA and ImageNet datasets demonstrate that Diff-OneBit gives high-fidelity reconstructed images, outperforming state-of-the-art methods in both reconstruction quality and computational efficiency across 1-bit compressed sensing and logistic regression tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design</title>
<link>https://arxiv.org/abs/2511.12489</link>
<guid>https://arxiv.org/abs/2511.12489</guid>
<content:encoded><![CDATA[
arXiv:2511.12489v1 Announce Type: new 
Abstract: Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncover and Unlearn Nuisances: Agnostic Fully Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2511.12491</link>
<guid>https://arxiv.org/abs/2511.12491</guid>
<content:encoded><![CDATA[
arXiv:2511.12491v1 Announce Type: new 
Abstract: Fully Test-Time Adaptation (FTTA) addresses domain shifts without access to source data and training protocols of the pre-trained models. Traditional strategies that align source and target feature distributions are infeasible in FTTA due to the absence of training data and unpredictable target domains. In this work, we exploit a dual perspective on FTTA, and propose Agnostic FTTA (AFTTA) as a novel formulation that enables the usage of off-the-shelf domain transformations during test-time to enable direct generalization to unforeseeable target data. To address this, we develop an uncover-and-unlearn approach. First, we uncover potential unwanted shifts between source and target domains by simulating them through predefined mappings and consider them as nuisances. Then, during test-time prediction, the model is enforced to unlearn these nuisances by regularizing the consequent shifts in latent representations and label predictions. Specifically, a mutual information-based criterion is devised and applied to guide nuisances unlearning in the feature space and encourage confident and consistent prediction in label space. Our proposed approach explicitly addresses agnostic domain shifts, enabling superior model generalization under FTTA constraints. Extensive experiments on various tasks, involving corruption and style shifts, demonstrate that our method consistently outperforms existing approaches.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Better IncomLDL: We Are Unaware of Hidden Labels in Advance</title>
<link>https://arxiv.org/abs/2511.12494</link>
<guid>https://arxiv.org/abs/2511.12494</guid>
<content:encoded><![CDATA[
arXiv:2511.12494v1 Announce Type: new 
Abstract: Label distribution learning (LDL) is a novel paradigm that describe the samples by label distribution of a sample. However, acquiring LDL dataset is costly and time-consuming, which leads to the birth of incomplete label distribution learning (IncomLDL). All the previous IncomLDL methods set the description degrees of "missing" labels in an instance to 0, but remains those of other labels unchanged. This setting is unrealistic because when certain labels are missing, the degrees of the remaining labels will increase accordingly. We fix this unrealistic setting in IncomLDL and raise a new problem: LDL with hidden labels (HidLDL), which aims to recover a complete label distribution from a real-world incomplete label distribution where certain labels in an instance are omitted during annotation. To solve this challenging problem, we discover the significance of proportional information of the observed labels and capture it by an innovative constraint to utilize it during the optimization process. We simultaneously use local feature similarity and the global low-rank structure to reveal the mysterious veil of hidden labels. Moreover, we theoretically give the recovery bound of our method, proving the feasibility of our method in learning from hidden labels. Extensive recovery and predictive experiments on various datasets prove the superiority of our method to state-of-the-art LDL and IncomLDL methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BSO: Binary Spiking Online Optimization Algorithm</title>
<link>https://arxiv.org/abs/2511.12502</link>
<guid>https://arxiv.org/abs/2511.12502</guid>
<content:encoded><![CDATA[
arXiv:2511.12502v1 Announce Type: new 
Abstract: Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning</title>
<link>https://arxiv.org/abs/2511.12507</link>
<guid>https://arxiv.org/abs/2511.12507</guid>
<content:encoded><![CDATA[
arXiv:2511.12507v1 Announce Type: new 
Abstract: Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Bias Mitigation via xLSTM-PINN: Memory-Gated Representation Refinement for Physics-Informed Learning</title>
<link>https://arxiv.org/abs/2511.12512</link>
<guid>https://arxiv.org/abs/2511.12512</guid>
<content:encoded><![CDATA[
arXiv:2511.12512v1 Announce Type: new 
Abstract: Physics-informed learning for PDEs is surging across scientific computing and industrial simulation, yet prevailing methods face spectral bias, residual-data imbalance, and weak extrapolation. We introduce a representation-level spectral remodeling xLSTM-PINN that combines gated-memory multiscale feature extraction with adaptive residual-data weighting to curb spectral bias and strengthen extrapolation. Across four benchmarks, we integrate gated cross-scale memory, a staged frequency curriculum, and adaptive residual reweighting, and verify with analytic references and extrapolation tests, achieving markedly lower spectral error and RMSE and a broader stable learning-rate window. Frequency-domain benchmarks show raised high-frequency kernel weights and a right-shifted resolvable bandwidth, shorter high-k error decay and time-to-threshold, and narrower error bands with lower MSE, RMSE, MAE, and MaxAE. Compared with the baseline PINN, we reduce MSE, RMSE, MAE, and MaxAE across all four benchmarks and deliver cleaner boundary transitions with attenuated high-frequency ripples in both frequency and field maps. This work suppresses spectral bias, widens the resolvable band and shortens the high-k time-to-threshold under the same budget, and without altering AD or physics losses improves accuracy, reproducibility, and transferability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regret Guarantees for Linear Contextual Stochastic Shortest Path</title>
<link>https://arxiv.org/abs/2511.12534</link>
<guid>https://arxiv.org/abs/2511.12534</guid>
<content:encoded><![CDATA[
arXiv:2511.12534v1 Announce Type: new 
Abstract: We define the problem of linear Contextual Stochastic Shortest Path (CSSP), where at the beginning of each episode, the learner observes an adversarially chosen context that determines the MDP through a fixed but unknown linear function. The learner's objective is to reach a designated goal state with minimal expected cumulative loss, despite having no prior knowledge of the transition dynamics, loss functions, or the mapping from context to MDP. In this work, we propose LR-CSSP, an algorithm that achieves a regret bound of $\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/ \delta))$, where $K$ is the number of episodes, $d$ is the context dimension, $S$ and $A$ are the sets of states and actions respectively, $B_\star$ bounds the optimal cumulative loss and $T_\star$, unknown to the learner, bounds the expected time for the optimal policy to reach the goal. In the case where all costs exceed $\ell_{\min}$, LR-CSSP attains a regret of $\widetilde O(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/\delta)/\ell_{\min}})$. Unlike in contextual finite-horizon MDPs, where limited knowledge primarily leads to higher losses and regret, in the CSSP setting, insufficient knowledge can also prolong episodes and may even lead to non-terminating episodes. Our analysis reveals that LR-CSSP effectively handles continuous context spaces, while ensuring all episodes terminate within a reasonable number of time steps.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Center-Outward q-Dominance: A Sample-Computable Proxy for Strong Stochastic Dominance in Multi-Objective Optimisation</title>
<link>https://arxiv.org/abs/2511.12545</link>
<guid>https://arxiv.org/abs/2511.12545</guid>
<content:encoded><![CDATA[
arXiv:2511.12545v1 Announce Type: new 
Abstract: Stochastic multi-objective optimization (SMOOP) requires ranking multivariate distributions; yet, most empirical studies perform scalarization, which loses information and is unreliable. Based on the optimal transport theory, we introduce the center-outward q-dominance relation and prove it implies strong first-order stochastic dominance (FSD). Also, we develop an empirical test procedure based on q-dominance, and derive an explicit sample size threshold, $n^*(\delta)$, to control the Type I error. We verify the usefulness of our approach in two scenarios: (1) as a ranking method in hyperparameter tuning; (2) as a selection method in multi-objective optimization algorithms. For the former, we analyze the final stochastic Pareto sets of seven multi-objective hyperparameter tuners on the YAHPO-MO benchmark tasks with q-dominance, which allows us to compare these tuners when the expected hypervolume indicator (HVI, the most common performance metric) of the Pareto sets becomes indistinguishable. For the latter, we replace the mean value-based selection in the NSGA-II algorithm with $q$-dominance, which shows a superior convergence rate on noise-augmented ZDT benchmark problems. These results establish center-outward q-dominance as a principled, tractable foundation for seeking truly stochastically dominant solutions for SMOOPs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching</title>
<link>https://arxiv.org/abs/2511.12548</link>
<guid>https://arxiv.org/abs/2511.12548</guid>
<content:encoded><![CDATA[
arXiv:2511.12548v1 Announce Type: new 
Abstract: First-order optimizers are reliable but slow in sharp, anisotropic regions. We study a curvature-adaptive method that periodically sketches a low-rank Hessian subspace via Hessian--vector products and preconditions gradients only in that subspace, leaving the orthogonal complement first-order. For L-smooth non-convex objectives, we recover the standard O(1/T) stationarity guarantee with a widened stable stepsize range; under a Polyak--Lojasiewicz (PL) condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. On CIFAR-10/100 with ResNet-18/34, the method enters the low-loss region substantially earlier: measured by epochs to a pre-declared train-loss threshold (0.75), it reaches the threshold 2.95x faster than Adam on CIFAR-100/ResNet-18, while matching final test accuracy. The approach is one-knob: performance is insensitive to the sketch rank k across {1,3,5}, and k=0 yields a principled curvature-free ablation. We release anonymized logs and scripts that regenerate all figures and tables.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Instabilities Induce Flatness Bias in Gradient Descent</title>
<link>https://arxiv.org/abs/2511.12558</link>
<guid>https://arxiv.org/abs/2511.12558</guid>
<content:encoded><![CDATA[
arXiv:2511.12558v1 Announce Type: new 
Abstract: Classical analyses of gradient descent (GD) define a stability threshold based on the largest eigenvalue of the loss Hessian, often termed sharpness. When the learning rate lies below this threshold, training is stable and the loss decreases monotonically. Yet, modern deep networks often achieve their best performance beyond this regime.
  We demonstrate that such instabilities induce an implicit bias in GD, driving parameters toward flatter regions of the loss landscape and thereby improving generalization. The key mechanism is the Rotational Polarity of Eigenvectors (RPE), a geometric phenomenon in which the leading eigenvectors of the Hessian rotate during training instabilities. These rotations, which increase with learning rates, promote exploration and provably lead to flatter minima.
  This theoretical framework extends to stochastic GD, where instability-driven flattening persists and its empirical effects outweigh minibatch noise. Finally, we show that restoring instabilities in Adam further improves generalization.
  Together, these results establish and understand the constructive role of training instabilities in deep learning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear time small coresets for k-mean clustering of segments with applications</title>
<link>https://arxiv.org/abs/2511.12564</link>
<guid>https://arxiv.org/abs/2511.12564</guid>
<content:encoded><![CDATA[
arXiv:2511.12564v1 Announce Type: new 
Abstract: We study the $k$-means problem for a set $\mathcal{S} \subseteq \mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \subseteq \mathbb{R}^d$ that minimize
  $D(\mathcal{S},X) := \sum_{S \in \mathcal{S}} \min_{x \in X} D(S,x)$, where $D(S,x) := \int_{p \in S} |p - x| dp$
  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\varepsilon > 0$, an $\varepsilon$-coreset is a weighted subset $C \subseteq \mathbb{R}^d$ that approximates $D(\mathcal{S},X)$ within a factor of $1 \pm \varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\varepsilon$, it produces a coreset of size $O(\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Machine Learning Model Efficiency through Quantization and Bit Depth Optimization: A Performance Analysis on Healthcare Data</title>
<link>https://arxiv.org/abs/2511.12568</link>
<guid>https://arxiv.org/abs/2511.12568</guid>
<content:encoded><![CDATA[
arXiv:2511.12568v1 Announce Type: new 
Abstract: This research aims to optimize intricate learning models by implementing quantization and bit-depth optimization techniques. The objective is to significantly cut time complexity while preserving model efficiency, thus addressing the challenge of extended execution times in intricate models. Two medical datasets were utilized as case studies to apply a Logistic Regression (LR) machine learning model. Using efficient quantization and bit depth optimization strategies the input data is downscaled from float64 to float32 and int32. The results demonstrated a significant reduction in time complexity, with only a minimal decrease in model accuracy post-optimization, showcasing the state-of-the-art optimization approach. This comprehensive study concludes that the impact of these optimization techniques varies depending on a set of parameters.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LMM-IR: Large-Scale Netlist-Aware Multimodal Framework for Static IR-Drop Prediction</title>
<link>https://arxiv.org/abs/2511.12581</link>
<guid>https://arxiv.org/abs/2511.12581</guid>
<content:encoded><![CDATA[
arXiv:2511.12581v1 Announce Type: new 
Abstract: Static IR drop analysis is a fundamental and critical task in the field of chip design. Nevertheless, this process can be quite time-consuming, potentially requiring several hours. Moreover, addressing IR drop violations frequently demands iterative analysis, thereby causing the computational burden. Therefore, fast and accurate IR drop prediction is vital for reducing the overall time invested in chip design. In this paper, we firstly propose a novel multimodal approach that efficiently processes SPICE files through large-scale netlist transformer (LNT). Our key innovation is representing and processing netlist topology as 3D point cloud representations, enabling efficient handling of netlist with up to hundreds of thousands to millions nodes. All types of data, including netlist files and image data, are encoded into latent space as features and fed into the model for static voltage drop prediction. This enables the integration of data from multiple modalities for complementary predictions. Experimental results demonstrate that our proposed algorithm can achieve the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization</title>
<link>https://arxiv.org/abs/2511.12601</link>
<guid>https://arxiv.org/abs/2511.12601</guid>
<content:encoded><![CDATA[
arXiv:2511.12601v1 Announce Type: new 
Abstract: Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PID-controlled Langevin Dynamics for Faster Sampling of Generative Models</title>
<link>https://arxiv.org/abs/2511.12603</link>
<guid>https://arxiv.org/abs/2511.12603</guid>
<content:encoded><![CDATA[
arXiv:2511.12603v1 Announce Type: new 
Abstract: Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \href{https://github.com/tsinghua-fib-lab/PIDLD}{https://github.com/tsinghua-fib-lab/PIDLD}.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions</title>
<link>https://arxiv.org/abs/2511.12628</link>
<guid>https://arxiv.org/abs/2511.12628</guid>
<content:encoded><![CDATA[
arXiv:2511.12628v1 Announce Type: new 
Abstract: Current federated-learning models deteriorate under heterogeneous (non-I.I.D.) client data, as their feature representations diverge and pixel- or patch-level objectives fail to capture the global topology which is essential for high-dimensional visual tasks. We propose FedTopo, a framework that integrates Topological-Guided Block Screening (TGBS) and Topological Embedding (TE) to leverage topological information, yielding coherently aligned cross-client representations by Topological Alignment Loss (TAL). First, Topology-Guided Block Screening (TGBS) automatically selects the most topology-informative block, i.e., the one with maximal topological separability, whose persistence-based signatures best distinguish within- versus between-class pairs, ensuring that subsequent analysis focuses on topology-rich features. Next, this block yields a compact Topological Embedding, which quantifies the topological information for each client. Finally, a Topological Alignment Loss (TAL) guides clients to maintain topological consistency with the global model during optimization, reducing representation drift across rounds. Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 under four non-I.I.D. partitions show that FedTopo accelerates convergence and improves accuracy over strong baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NFQ2.0: The CartPole Benchmark Revisited</title>
<link>https://arxiv.org/abs/2511.12644</link>
<guid>https://arxiv.org/abs/2511.12644</guid>
<content:encoded><![CDATA[
arXiv:2511.12644v1 Announce Type: new 
Abstract: This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back</title>
<link>https://arxiv.org/abs/2511.12659</link>
<guid>https://arxiv.org/abs/2511.12659</guid>
<content:encoded><![CDATA[
arXiv:2511.12659v1 Announce Type: new 
Abstract: The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\frac{DS^{1.5}}{\epsilon} + \frac{Nat}{\epsilon^2}$ where $\epsilon$ is the excess risk. This bound is tight up to a $\sqrt{DS}$ factor in the first term, nearly matching known $Nat/\epsilon^2$ and $DS/\epsilon$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $\epsilon$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLClear: Visually Verifiable Multi-Client Watermarking for Federated Learning</title>
<link>https://arxiv.org/abs/2511.12663</link>
<guid>https://arxiv.org/abs/2511.12663</guid>
<content:encoded><![CDATA[
arXiv:2511.12663v1 Announce Type: new 
Abstract: Federated learning (FL) enables multiple clients to collaboratively train a shared global model while preserving the privacy of their local data. Within this paradigm, the intellectual property rights (IPR) of client models are critical assets that must be protected. In practice, the central server responsible for maintaining the global model may maliciously manipulate the global model to erase client contributions or falsely claim sole ownership, thereby infringing on clients' IPR. Watermarking has emerged as a promising technique for asserting model ownership and protecting intellectual property. However, existing FL watermarking approaches remain limited, suffering from potential watermark collisions among clients, insufficient watermark security, and non-intuitive verification mechanisms. In this paper, we propose FLClear, a novel framework that simultaneously achieves collision-free watermark aggregation, enhanced watermark security, and visually interpretable ownership verification. Specifically, FLClear introduces a transposed model jointly optimized with contrastive learning to integrate the watermarking and main task objectives. During verification, the watermark is reconstructed from the transposed model and evaluated through both visual inspection and structural similarity metrics, enabling intuitive and quantitative ownership verification. Comprehensive experiments conducted over various datasets, aggregation schemes, and attack scenarios demonstrate the effectiveness of FLClear and confirm that it consistently outperforms state-of-the-art FL watermarking methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention-Enhanced Convolutional Autoencoder and Structured Delay Embeddings for Weather Prediction</title>
<link>https://arxiv.org/abs/2511.12682</link>
<guid>https://arxiv.org/abs/2511.12682</guid>
<content:encoded><![CDATA[
arXiv:2511.12682v1 Announce Type: new 
Abstract: Weather prediction is a quintessential problem involving the forecasting of a complex, nonlinear, and chaotic high-dimensional dynamical system. This work introduces an efficient reduced-order modeling (ROM) framework for short-range weather prediction and investigates fundamental questions in dimensionality reduction and reduced order modeling of such systems. Unlike recent AI-driven models, which require extensive computational resources, our framework prioritizes efficiency while achieving reasonable accuracy. Specifically, a ResNet-based convolutional autoencoder augmented by block attention modules is developed to reduce the dimensionality of high-dimensional weather data. Subsequently, a linear operator is learned in the time-delayed embedding of the latent space to efficiently capture the dynamics. Using the ERA5 reanalysis dataset, we demonstrate that this framework performs well in-distribution as evidenced by effectively predicting weather patterns within training data periods. We also identify important limitations in generalizing to future states, particularly in maintaining prediction accuracy beyond the training window. Our analysis reveals that weather systems exhibit strong temporal correlations that can be effectively captured through linear operations in an appropriately constructed embedding space, and that projection error rather than inference error is the main bottleneck. These findings shed light on some key challenges in reduced-order modeling of chaotic systems and point toward opportunities for hybrid approaches that combine efficient reduced-order models as baselines with more sophisticated AI architectures, particularly for applications in long-term climate modeling where computational efficiency is paramount.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning</title>
<link>https://arxiv.org/abs/2511.12695</link>
<guid>https://arxiv.org/abs/2511.12695</guid>
<content:encoded><![CDATA[
arXiv:2511.12695v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs</title>
<link>https://arxiv.org/abs/2511.12706</link>
<guid>https://arxiv.org/abs/2511.12706</guid>
<content:encoded><![CDATA[
arXiv:2511.12706v1 Announce Type: new 
Abstract: Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations</title>
<link>https://arxiv.org/abs/2511.12709</link>
<guid>https://arxiv.org/abs/2511.12709</guid>
<content:encoded><![CDATA[
arXiv:2511.12709v1 Announce Type: new 
Abstract: Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Oxytrees: Model Trees for Bipartite Learning</title>
<link>https://arxiv.org/abs/2511.12713</link>
<guid>https://arxiv.org/abs/2511.12713</guid>
<content:encoded><![CDATA[
arXiv:2511.12713v1 Announce Type: new 
Abstract: Bipartite learning is a machine learning task that aims to predict interactions between pairs of instances. It has been applied to various domains, including drug-target interactions, RNA-disease associations, and regulatory network inference. Despite being widely investigated, current methods still present drawbacks, as they are often designed for a specific application and thus do not generalize to other problems or present scalability issues. To address these challenges, we propose Oxytrees: proxy-based biclustering model trees. Oxytrees compress the interaction matrix into row- and column-wise proxy matrices, significantly reducing training time without compromising predictive performance. We also propose a new leaf-assignment algorithm that significantly reduces the time taken for prediction. Finally, Oxytrees employ linear models using the Kronecker product kernel in their leaves, resulting in shallower trees and thus even faster training. Using 15 datasets, we compared the predictive performance of ensembles of Oxytrees with that of the current state-of-the-art. We achieved up to 30-fold improvement in training times compared to state-of-the-art biclustering forests, while demonstrating competitive or superior performance in most evaluation settings, particularly in the inductive setting. Finally, we provide an intuitive Python API to access all datasets, methods and evaluation measures used in this work, thus enabling reproducible research in this field.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Robustness of Linear Classifiers to Targeted Data Poisoning</title>
<link>https://arxiv.org/abs/2511.12722</link>
<guid>https://arxiv.org/abs/2511.12722</guid>
<content:encoded><![CDATA[
arXiv:2511.12722v1 Announce Type: new 
Abstract: Data poisoning is a training-time attack that undermines the trustworthiness of learned models. In a targeted data poisoning attack, an adversary manipulates the training dataset to alter the classification of a targeted test point. Given the typically large size of training dataset, manual detection of poisoning is difficult. An alternative is to automatically measure a dataset's robustness against such an attack, which is the focus of this paper. We consider a threat model wherein an adversary can only perturb the labels of the training dataset, with knowledge limited to the hypothesis space of the victim's model. In this setting, we prove that finding the robustness is an NP-Complete problem, even when hypotheses are linear classifiers. To overcome this, we present a technique that finds lower and upper bounds of robustness. Our implementation of the technique computes these bounds efficiently in practice for many publicly available datasets. We experimentally demonstrate the effectiveness of our approach. Specifically, a poisoning exceeding the identified robustness bounds significantly impacts test point classification. We are also able to compute these bounds in many more cases where state-of-the-art techniques fail.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAYA: Layer-wise Attention Aggregation for Interpretable Depth-Aware Neural Networks</title>
<link>https://arxiv.org/abs/2511.12723</link>
<guid>https://arxiv.org/abs/2511.12723</guid>
<content:encoded><![CDATA[
arXiv:2511.12723v1 Announce Type: new 
Abstract: Deep neural networks typically rely on the representation produced by their final hidden layer to make predictions, implicitly assuming that this single vector fully captures the semantics encoded across all preceding transformations. However, intermediate layers contain rich and complementary information -- ranging from low-level patterns to high-level abstractions -- that is often discarded when the decision head depends solely on the last representation. This paper revisits the role of the output layer and introduces LAYA (Layer-wise Attention Aggregator), a novel output head that dynamically aggregates internal representations through attention. Instead of projecting only the deepest embedding, LAYA learns input-conditioned attention weights over layer-wise features, yielding an interpretable and architecture-agnostic mechanism for synthesizing predictions. Experiments on vision and language benchmarks show that LAYA consistently matches or improves the performance of standard output heads, with relative gains of up to about one percentage point in accuracy, while providing explicit layer-attribution scores that reveal how different abstraction levels contribute to each decision. Crucially, these interpretability signals emerge directly from the model's computation, without any external post hoc explanations. The code to reproduce LAYA is publicly available at: https://github.com/gvessio/LAYA.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convolutional Model Trees</title>
<link>https://arxiv.org/abs/2511.12725</link>
<guid>https://arxiv.org/abs/2511.12725</guid>
<content:encoded><![CDATA[
arXiv:2511.12725v1 Announce Type: new 
Abstract: A method for creating a forest of model trees to fit samples of a function defined on images is described in several steps: down-sampling the images, determining a tree's hyperplanes, applying convolutions to the hyperplanes to handle small distortions of training images, and creating forests of model trees to increase accuracy and achieve a smooth fit. A 1-to-1 correspondence among pixels of images, coefficients of hyperplanes and coefficients of leaf functions offers the possibility of dealing with larger distortions such as arbitrary rotations or changes of perspective. A theoretical method for smoothing forest outputs to produce a continuously differentiable approximation is described. Within that framework, a training procedure is proved to converge.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering</title>
<link>https://arxiv.org/abs/2511.12742</link>
<guid>https://arxiv.org/abs/2511.12742</guid>
<content:encoded><![CDATA[
arXiv:2511.12742v1 Announce Type: new 
Abstract: As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop" that can lead to training instability or \textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes</title>
<link>https://arxiv.org/abs/2511.12745</link>
<guid>https://arxiv.org/abs/2511.12745</guid>
<content:encoded><![CDATA[
arXiv:2511.12745v1 Announce Type: new 
Abstract: Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.12751</link>
<guid>https://arxiv.org/abs/2511.12751</guid>
<content:encoded><![CDATA[
arXiv:2511.12751v1 Announce Type: new 
Abstract: Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Online Learning of Deep Koopman Linear Embeddings</title>
<link>https://arxiv.org/abs/2511.12760</link>
<guid>https://arxiv.org/abs/2511.12760</guid>
<content:encoded><![CDATA[
arXiv:2511.12760v1 Announce Type: new 
Abstract: We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers</title>
<link>https://arxiv.org/abs/2511.12764</link>
<guid>https://arxiv.org/abs/2511.12764</guid>
<content:encoded><![CDATA[
arXiv:2511.12764v1 Announce Type: new 
Abstract: When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector (\(\mathrm{INC}\)), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that \(\mathrm{INC}\) reduces the error amplification on the order of \(\Delta t^{-1} + L\), where \(\Delta t\) is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test \(\mathrm{INC}\) in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. INC improves the long-term trajectory performance (\(R^2\)) by up to 158.7\%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. INC thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at https://github.com/tum-pbs/INC
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MolEdit: Knowledge Editing for Multimodal Molecule Language Models</title>
<link>https://arxiv.org/abs/2511.12770</link>
<guid>https://arxiv.org/abs/2511.12770</guid>
<content:encoded><![CDATA[
arXiv:2511.12770v1 Announce Type: new 
Abstract: Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: https://github.com/LzyFischer/MolEdit.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation</title>
<link>https://arxiv.org/abs/2511.12779</link>
<guid>https://arxiv.org/abs/2511.12779</guid>
<content:encoded><![CDATA[
arXiv:2511.12779v1 Announce Type: new 
Abstract: We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a $2\%$ error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by $16\%$ on average, while delivering up to $26\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of $19\%$. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data</title>
<link>https://arxiv.org/abs/2511.12788</link>
<guid>https://arxiv.org/abs/2511.12788</guid>
<content:encoded><![CDATA[
arXiv:2511.12788v1 Announce Type: new 
Abstract: The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\boldsymbol{\theta} = \{\theta_d, \theta_a, \theta_b, \theta_p, \theta_c\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Look-back Horizon for Time Series Forecasting in Federated Learning</title>
<link>https://arxiv.org/abs/2511.12791</link>
<guid>https://arxiv.org/abs/2511.12791</guid>
<content:encoded><![CDATA[
arXiv:2511.12791v1 Announce Type: new 
Abstract: Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Genomic Next-Token Predictors are In-Context Learners</title>
<link>https://arxiv.org/abs/2511.12797</link>
<guid>https://arxiv.org/abs/2511.12797</guid>
<content:encoded><![CDATA[
arXiv:2511.12797v1 Announce Type: new 
Abstract: In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?
  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Alignment Game: A Theory of Long-Horizon Alignment Through Recursive Curation</title>
<link>https://arxiv.org/abs/2511.12804</link>
<guid>https://arxiv.org/abs/2511.12804</guid>
<content:encoded><![CDATA[
arXiv:2511.12804v1 Announce Type: new 
Abstract: In self-consuming generative models that train on their own outputs, alignment with user preferences becomes a recursive rather than one-time process. We provide the first formal foundation for analyzing the long-term effects of such recursive retraining on alignment. Under a two-stage curation mechanism based on the Bradley-Terry (BT) model, we model alignment as an interaction between two factions: the Model Owner, who filters which outputs should be learned by the model, and the Public User, who determines which outputs are ultimately shared and retained through interactions with the model. Our analysis reveals three structural convergence regimes depending on the degree of preference alignment: consensus collapse, compromise on shared optima, and asymmetric refinement. We prove a fundamental impossibility theorem: no recursive BT-based curation mechanism can simultaneously preserve diversity, ensure symmetric influence, and eliminate dependence on initialization. Framing the process as dynamic social choice, we show that alignment is not a static goal but an evolving equilibrium, shaped both by power asymmetries and path dependence.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expressive Temporal Specifications for Reward Monitoring</title>
<link>https://arxiv.org/abs/2511.12808</link>
<guid>https://arxiv.org/abs/2511.12808</guid>
<content:encoded><![CDATA[
arXiv:2511.12808v1 Announce Type: new 
Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs</title>
<link>https://arxiv.org/abs/2511.12817</link>
<guid>https://arxiv.org/abs/2511.12817</guid>
<content:encoded><![CDATA[
arXiv:2511.12817v1 Announce Type: new 
Abstract: The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Catastrophic Forgetting in Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2511.12828</link>
<guid>https://arxiv.org/abs/2511.12828</guid>
<content:encoded><![CDATA[
arXiv:2511.12828v1 Announce Type: new 
Abstract: Catastrophic forgetting is a longstanding challenge in continual learning, where models lose knowledge from earlier tasks when learning new ones. While various mitigation strategies have been proposed for Multi-Layer Perceptrons (MLPs), recent architectural advances like Kolmogorov-Arnold Networks (KANs) have been suggested to offer intrinsic resistance to forgetting by leveraging localized spline-based activations. However, the practical behavior of KANs under continual learning remains unclear, and their limitations are not well understood. To address this, we present a comprehensive study of catastrophic forgetting in KANs and develop a theoretical framework that links forgetting to activation support overlap and intrinsic data dimension. We validate these analyses through systematic experiments on synthetic and vision tasks, measuring forgetting dynamics under varying model configurations and data complexity. Further, we introduce KAN-LoRA, a novel adapter design for parameter-efficient continual fine-tuning of language models, and evaluate its effectiveness in knowledge editing tasks. Our findings reveal that while KANs exhibit promising retention in low-dimensional algorithmic settings, they remain vulnerable to forgetting in high-dimensional domains such as image classification and language modeling. These results advance the understanding of KANs' strengths and limitations, offering practical insights for continual learning system design.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Evaluation of Representation Learning Methods in Particle Physics Foundation Models</title>
<link>https://arxiv.org/abs/2511.12829</link>
<guid>https://arxiv.org/abs/2511.12829</guid>
<content:encoded><![CDATA[
arXiv:2511.12829v1 Announce Type: new 
Abstract: We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity with Improved Efficiency</title>
<link>https://arxiv.org/abs/2511.12838</link>
<guid>https://arxiv.org/abs/2511.12838</guid>
<content:encoded><![CDATA[
arXiv:2511.12838v1 Announce Type: new 
Abstract: Higher-order Graph Neural Networks (HOGNNs) based on the 2-FWL test achieve superior expressivity by modeling 2- and 3-node interactions, but at $\mathcal{O}(n^3)$ computational cost. However, this computational burden is typically mitigated by existing efficiency methods at the cost of reduced expressivity. We propose \textbf{Co-Sparsify}, a connectivity-aware sparsification framework that eliminates \emph{provably redundant} computations while preserving full 2-FWL expressive power. Our key insight is that 3-node interactions are expressively necessary only within \emph{biconnected components} -- maximal subgraphs where every pair of nodes lies on a cycle. Outside these components, structural relationships can be fully captured via 2-node message passing or global readout, rendering higher-order modeling unnecessary. Co-Sparsify restricts 2-node message passing to connected components and 3-node interactions to biconnected ones, removing computation without approximation or sampling. We prove that Co-Sparsified GNNs are as expressive as the 2-FWL test. Empirically, on PPGN, Co-Sparsify matches or exceeds accuracy on synthetic substructure counting tasks and achieves state-of-the-art performance on real-world benchmarks (ZINC, QM9). This study demonstrates that high expressivity and scalability are not mutually exclusive: principled, topology-guided sparsification enables powerful, efficient GNNs with theoretical guarantees.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees</title>
<link>https://arxiv.org/abs/2511.12846</link>
<guid>https://arxiv.org/abs/2511.12846</guid>
<content:encoded><![CDATA[
arXiv:2511.12846v1 Announce Type: new 
Abstract: Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability</title>
<link>https://arxiv.org/abs/2511.12852</link>
<guid>https://arxiv.org/abs/2511.12852</guid>
<content:encoded><![CDATA[
arXiv:2511.12852v1 Announce Type: new 
Abstract: Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An approach of deep reinforcement learning for maximizing the net present value of stochastic projects</title>
<link>https://arxiv.org/abs/2511.12865</link>
<guid>https://arxiv.org/abs/2511.12865</guid>
<content:encoded><![CDATA[
arXiv:2511.12865v1 Announce Type: new 
Abstract: This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Fundamental Limits of LLMs at Scale</title>
<link>https://arxiv.org/abs/2511.12869</link>
<guid>https://arxiv.org/abs/2511.12869</guid>
<content:encoded><![CDATA[
arXiv:2511.12869v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples</title>
<link>https://arxiv.org/abs/2511.12881</link>
<guid>https://arxiv.org/abs/2511.12881</guid>
<content:encoded><![CDATA[
arXiv:2511.12881v1 Announce Type: new 
Abstract: Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Method of Manufactured Learning for Solver-free Training of Neural Operators</title>
<link>https://arxiv.org/abs/2511.12890</link>
<guid>https://arxiv.org/abs/2511.12890</guid>
<content:encoded><![CDATA[
arXiv:2511.12890v1 Announce Type: new 
Abstract: Training neural operators to approximate mappings between infinite-dimensional function spaces often requires extensive datasets generated by either demanding experimental setups or computationally expensive numerical solvers. This dependence on solver-based data limits scalability and constrains exploration across physical systems. Here we introduce the Method of Manufactured Learning (MML), a solver-independent framework for training neural operators using analytically constructed, physics-consistent datasets. Inspired by the classical method of manufactured solutions, MML replaces numerical data generation with functional synthesis, i.e., smooth candidate solutions are sampled from controlled analytical spaces, and the corresponding forcing fields are derived by direct application of the governing differential operators. During inference, setting these forcing terms to zero restores the original governing equations, allowing the trained neural operator to emulate the true solution operator of the system. The framework is agnostic to network architecture and can be integrated with any operator learning paradigm. In this paper, we employ Fourier neural operator as a representative example. Across canonical benchmarks including heat, advection, Burgers, and diffusion-reaction equations. MML achieves high spectral accuracy, low residual errors, and strong generalization to unseen conditions. By reframing data generation as a process of analytical synthesis, MML offers a scalable, solver-agnostic pathway toward constructing physically grounded neural operators that retain fidelity to governing laws without reliance on expensive numerical simulations or costly experimental data for training.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Functional Mean Flow in Hilbert Space</title>
<link>https://arxiv.org/abs/2511.12898</link>
<guid>https://arxiv.org/abs/2511.12898</guid>
<content:encoded><![CDATA[
arXiv:2511.12898v1 Announce Type: new 
Abstract: We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive Entropy Bounds for Density and Conditional Density Decomposition</title>
<link>https://arxiv.org/abs/2511.12903</link>
<guid>https://arxiv.org/abs/2511.12903</guid>
<content:encoded><![CDATA[
arXiv:2511.12903v1 Announce Type: new 
Abstract: This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.
  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.
  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinkedIn Profile Characteristics and Professional Success Indicators</title>
<link>https://arxiv.org/abs/2511.12905</link>
<guid>https://arxiv.org/abs/2511.12905</guid>
<content:encoded><![CDATA[
arXiv:2511.12905v1 Announce Type: new 
Abstract: This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking</title>
<link>https://arxiv.org/abs/2511.12934</link>
<guid>https://arxiv.org/abs/2511.12934</guid>
<content:encoded><![CDATA[
arXiv:2511.12934v1 Announce Type: new 
Abstract: In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>APT: Affine Prototype-Timestamp For Time Series Forecasting Under Distribution Shift</title>
<link>https://arxiv.org/abs/2511.12945</link>
<guid>https://arxiv.org/abs/2511.12945</guid>
<content:encoded><![CDATA[
arXiv:2511.12945v1 Announce Type: new 
Abstract: Time series forecasting under distribution shift remains challenging, as existing deep learning models often rely on local statistical normalization (e.g., mean and variance) that fails to capture global distribution shift. Methods like RevIN and its variants attempt to decouple distribution and pattern but still struggle with missing values, noisy observations, and invalid channel-wise affine transformation. To address these limitations, we propose Affine Prototype Timestamp (APT), a lightweight and flexible plug-in module that injects global distribution features into the normalization-forecasting pipeline. By leveraging timestamp conditioned prototype learning, APT dynamically generates affine parameters that modulate both input and output series, enabling the backbone to learn from self-supervised, distribution-aware clustered instances. APT is compatible with arbitrary forecasting backbones and normalization strategies while introducing minimal computational overhead. Extensive experiments across six benchmark datasets and multiple backbone-normalization combinations demonstrate that APT significantly improves forecasting performance under distribution shift.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series</title>
<link>https://arxiv.org/abs/2511.12951</link>
<guid>https://arxiv.org/abs/2511.12951</guid>
<content:encoded><![CDATA[
arXiv:2511.12951v1 Announce Type: new 
Abstract: Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&amp;P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series</title>
<link>https://arxiv.org/abs/2511.12955</link>
<guid>https://arxiv.org/abs/2511.12955</guid>
<content:encoded><![CDATA[
arXiv:2511.12955v1 Announce Type: new 
Abstract: Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems</title>
<link>https://arxiv.org/abs/2511.12979</link>
<guid>https://arxiv.org/abs/2511.12979</guid>
<content:encoded><![CDATA[
arXiv:2511.12979v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&amp;A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks</title>
<link>https://arxiv.org/abs/2511.12985</link>
<guid>https://arxiv.org/abs/2511.12985</guid>
<content:encoded><![CDATA[
arXiv:2511.12985v1 Announce Type: new 
Abstract: Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Branching Policies for MILPs with Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2511.12986</link>
<guid>https://arxiv.org/abs/2511.12986</guid>
<content:encoded><![CDATA[
arXiv:2511.12986v1 Announce Type: new 
Abstract: Branch-and-Bound (B\&amp;B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\&amp;B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs</title>
<link>https://arxiv.org/abs/2511.13010</link>
<guid>https://arxiv.org/abs/2511.13010</guid>
<content:encoded><![CDATA[
arXiv:2511.13010v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training</title>
<link>https://arxiv.org/abs/2511.13016</link>
<guid>https://arxiv.org/abs/2511.13016</guid>
<content:encoded><![CDATA[
arXiv:2511.13016v1 Announce Type: new 
Abstract: Reward design is central to reinforcement learning from human feedback (RLHF) and alignment research. In this work, we propose a unified framework to study hard, continuous, and hybrid reward structures for fine-tuning large language models (LLMs) on mathematical reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on the GSM8K dataset, we formalize and empirically evaluate reward formulations that incorporate correctness, perplexity, reasoning quality, and consistency. We introduce an adaptive hybrid reward scheduler that transitions between discrete and continuous signals, balancing exploration and stability. Our results show that hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches, offering insights for alignment via adaptive reward modeling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Final-Stage Bottleneck: A Systematic Dissection of the R-Learner for Network Causal Inference</title>
<link>https://arxiv.org/abs/2511.13018</link>
<guid>https://arxiv.org/abs/2511.13018</guid>
<content:encoded><![CDATA[
arXiv:2511.13018v1 Announce Type: new 
Abstract: The R-Learner is a powerful, theoretically-grounded framework for estimating heterogeneous treatment effects, prized for its robustness to nuisance model errors. However, its application to network data, where causal heterogeneity is often graph-dependent, presents a critical challenge to its core assumption of a well-specified final-stage model. In this paper, we conduct a large-scale empirical study to systematically dissect the R-Learner framework on graphs. We provide the first rigorous evidence that the primary driver of performance is the inductive bias of the final-stage CATE estimator, an effect that dominates the choice of nuisance models. Our central finding is the quantification of a catastrophic "representation bottleneck": we prove with overwhelming statistical significance (p < 0.001) that R-Learners with a graph-blind final stage fail completely (MSE > 4.0), even when paired with powerful GNN nuisance models. Conversely, our proposed end-to-end Graph R-Learner succeeds and significantly outperforms a strong, non-DML GNN T-Learner baseline. Furthermore, we identify and provide a mechanistic explanation for a subtle, topology-dependent "nuisance bottleneck," linking it to GNN over-squashing via a targeted "Hub-Periphery Trade-off" analysis. Our findings are validated across diverse synthetic and semi-synthetic benchmarks. We release our code as a reproducible benchmark to facilitate future research on this critical "final-stage bottleneck."
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Time-Scale Invariant Population-Level Neural Representations</title>
<link>https://arxiv.org/abs/2511.13022</link>
<guid>https://arxiv.org/abs/2511.13022</guid>
<content:encoded><![CDATA[
arXiv:2511.13022v1 Announce Type: new 
Abstract: General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale mismatches affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment</title>
<link>https://arxiv.org/abs/2511.13023</link>
<guid>https://arxiv.org/abs/2511.13023</guid>
<content:encoded><![CDATA[
arXiv:2511.13023v1 Announce Type: new 
Abstract: Despite the growing interest in Small Language Models (SLMs) as resource-efficient alternatives to Large Language Models (LLMs), their deployment on edge devices remains challenging due to unresolved efficiency gaps in model compression. While quantization has proven effective for LLMs, its applicability to SLMs is significantly underexplored, with critical questions about differing quantization bottlenecks and efficiency profiles. This paper introduces SLMQuant, the first systematic benchmark for evaluating LLM compression techniques when applied to SLMs. Through comprehensive multi-track evaluations across diverse architectures and tasks, we analyze how state-of-the-art quantization methods perform on SLMs. Our findings reveal fundamental disparities between SLMs and LLMs in quantization sensitivity, demonstrating that direct transfer of LLM-optimized techniques leads to suboptimal results due to SLMs' unique architectural characteristics and training dynamics. We identify key factors governing effective SLM quantization and propose actionable design principles for SLM-tailored compression. SLMQuant establishes a foundational framework for advancing efficient SLM deployment on low-end devices in edge applications, and provides critical insights for deploying lightweight language models in resource-constrained scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow</title>
<link>https://arxiv.org/abs/2511.13035</link>
<guid>https://arxiv.org/abs/2511.13035</guid>
<content:encoded><![CDATA[
arXiv:2511.13035v1 Announce Type: new 
Abstract: We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bi-View Embedding Fusion: A Hybrid Learning Approach for Knowledge Graph's Nodes Classification Addressing Problems with Limited Data</title>
<link>https://arxiv.org/abs/2511.13044</link>
<guid>https://arxiv.org/abs/2511.13044</guid>
<content:encoded><![CDATA[
arXiv:2511.13044v1 Announce Type: new 
Abstract: Traditional Machine Learning (ML) methods require large amounts of data to perform well, limiting their applicability in sparse or incomplete scenarios and forcing the usage of additional synthetic data to improve the model training. To overcome this challenge, the research community is looking more and more at Graph Machine Learning (GML) as it offers a powerful alternative by using relationships within data. However, this method also faces limitations, particularly when dealing with Knowledge Graphs (KGs), which can hide huge information due to their semantic nature. This study introduces Bi-View, a novel hybrid approach that increases the informative content of node features in KGs to generate enhanced Graph Embeddings (GEs) that are used to improve GML models without relying on additional synthetic data. The proposed work combines two complementary GE techniques: Node2Vec, which captures structural patterns through unsupervised random walks, and GraphSAGE, which aggregates neighbourhood information in a supervised way. Node2Vec embeddings are first computed to represent the graph topology, and node features are then enriched with centrality-based metrics, which are used as input for the GraphSAGE model. Moreover, a fusion layer combines the original Node2Vec embeddings with the GraphSAGE-influenced representations, resulting in a dual-perspective embedding space. Such a fusion captures both topological and semantic properties of the graph, enabling the model to exploit informative features that may exist in the dataset but that are not explicitly represented. Our approach improves downstream task performance, especially in scenarios with poor initial features, giving the basis for more accurate and precise KG-enanched GML models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information</title>
<link>https://arxiv.org/abs/2511.13049</link>
<guid>https://arxiv.org/abs/2511.13049</guid>
<content:encoded><![CDATA[
arXiv:2511.13049v1 Announce Type: new 
Abstract: We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \textit{share a common subspace}. We assume that a large amount $M$ of \textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\widetilde{O}\left(\sqrt{\frac{nd}{M}}\right)$ and $\widetilde{O}\left(\sqrt{\frac{dr}{N}}\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting</title>
<link>https://arxiv.org/abs/2511.13052</link>
<guid>https://arxiv.org/abs/2511.13052</guid>
<content:encoded><![CDATA[
arXiv:2511.13052v1 Announce Type: new 
Abstract: Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to "undesirable" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks</title>
<link>https://arxiv.org/abs/2511.13053</link>
<guid>https://arxiv.org/abs/2511.13053</guid>
<content:encoded><![CDATA[
arXiv:2511.13053v1 Announce Type: new 
Abstract: Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by conducting a geometric analysis of the network's energy landscape. We introduce a novel metric, ``Pinnacle Sharpness,'' to quantify the local stability of attractors. By systematically varying the kernel width and storage load, we uncover a rich phase diagram of attractor shapes. Our central finding is the emergence of a ``ridge of optimization,'' where the network maximizes attractor stability under challenging high-load and global-kernel conditions. Through a theoretical decomposition of the landscape gradient into a direct ``driving'' force and an indirect ``feedback'' force, we reveal the origin of this phenomenon. The optimization ridge corresponds to a regime of strong anti-correlation between the two forces, where the direct force, amplified by the high storage load, dominates the opposing collective feedback force. This demonstrates a sophisticated self-organization mechanism: the network adaptively harnesses inter-pattern interactions as a cooperative feedback control system to sculpt a robust energy landscape. Our findings provide a new physical picture for the stability of high-capacity associative memories and offer principles for their design.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latency and Ordering Effects in Online Decisions</title>
<link>https://arxiv.org/abs/2511.13060</link>
<guid>https://arxiv.org/abs/2511.13060</guid>
<content:encoded><![CDATA[
arXiv:2511.13060v1 Announce Type: new 
Abstract: Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_\Phi$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \ge L_{\mathrm{ideal}} + g_1(\lambda) + g_2(\varepsilon_\star) + g_{12}(\lambda,\varepsilon_\star) - D_{\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\mathrm{ncx}}\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity</title>
<link>https://arxiv.org/abs/2511.13061</link>
<guid>https://arxiv.org/abs/2511.13061</guid>
<content:encoded><![CDATA[
arXiv:2511.13061v1 Announce Type: new 
Abstract: Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Adaptive Graph Mixture of Models</title>
<link>https://arxiv.org/abs/2511.13062</link>
<guid>https://arxiv.org/abs/2511.13062</guid>
<content:encoded><![CDATA[
arXiv:2511.13062v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning</title>
<link>https://arxiv.org/abs/2511.13078</link>
<guid>https://arxiv.org/abs/2511.13078</guid>
<content:encoded><![CDATA[
arXiv:2511.13078v1 Announce Type: new 
Abstract: Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time prediction of breast cancer sites using deformation-aware graph neural network</title>
<link>https://arxiv.org/abs/2511.13082</link>
<guid>https://arxiv.org/abs/2511.13082</guid>
<content:encoded><![CDATA[
arXiv:2511.13082v1 Announce Type: new 
Abstract: Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions</title>
<link>https://arxiv.org/abs/2511.13103</link>
<guid>https://arxiv.org/abs/2511.13103</guid>
<content:encoded><![CDATA[
arXiv:2511.13103v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning</title>
<link>https://arxiv.org/abs/2511.13116</link>
<guid>https://arxiv.org/abs/2511.13116</guid>
<content:encoded><![CDATA[
arXiv:2511.13116v1 Announce Type: new 
Abstract: Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schr\"odinger Bridges</title>
<link>https://arxiv.org/abs/2511.13124</link>
<guid>https://arxiv.org/abs/2511.13124</guid>
<content:encoded><![CDATA[
arXiv:2511.13124v1 Announce Type: new 
Abstract: Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schr\"odinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.13133</link>
<guid>https://arxiv.org/abs/2511.13133</guid>
<content:encoded><![CDATA[
arXiv:2511.13133v1 Announce Type: new 
Abstract: Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.
  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching</title>
<link>https://arxiv.org/abs/2511.13144</link>
<guid>https://arxiv.org/abs/2511.13144</guid>
<content:encoded><![CDATA[
arXiv:2511.13144v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs</title>
<link>https://arxiv.org/abs/2511.13147</link>
<guid>https://arxiv.org/abs/2511.13147</guid>
<content:encoded><![CDATA[
arXiv:2511.13147v1 Announce Type: new 
Abstract: Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Warm-starting active-set solvers using graph neural networks</title>
<link>https://arxiv.org/abs/2511.13174</link>
<guid>https://arxiv.org/abs/2511.13174</guid>
<content:encoded><![CDATA[
arXiv:2511.13174v1 Announce Type: new 
Abstract: Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach</title>
<link>https://arxiv.org/abs/2511.13178</link>
<guid>https://arxiv.org/abs/2511.13178</guid>
<content:encoded><![CDATA[
arXiv:2511.13178v1 Announce Type: new 
Abstract: With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction</title>
<link>https://arxiv.org/abs/2511.13185</link>
<guid>https://arxiv.org/abs/2511.13185</guid>
<content:encoded><![CDATA[
arXiv:2511.13185v1 Announce Type: new 
Abstract: Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play</title>
<link>https://arxiv.org/abs/2511.13186</link>
<guid>https://arxiv.org/abs/2511.13186</guid>
<content:encoded><![CDATA[
arXiv:2511.13186v1 Announce Type: new 
Abstract: Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $\epsilon$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\times$ faster convergence and 30$\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer</title>
<link>https://arxiv.org/abs/2511.13198</link>
<guid>https://arxiv.org/abs/2511.13198</guid>
<content:encoded><![CDATA[
arXiv:2511.13198v1 Announce Type: new 
Abstract: Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs</title>
<link>https://arxiv.org/abs/2511.13223</link>
<guid>https://arxiv.org/abs/2511.13223</guid>
<content:encoded><![CDATA[
arXiv:2511.13223v1 Announce Type: new 
Abstract: Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Laplace Learning in Wasserstein Space</title>
<link>https://arxiv.org/abs/2511.13229</link>
<guid>https://arxiv.org/abs/2511.13229</guid>
<content:encoded><![CDATA[
arXiv:2511.13229v1 Announce Type: new 
Abstract: The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning
  methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical
  notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to
  an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator
  on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through
  numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing</title>
<link>https://arxiv.org/abs/2511.13234</link>
<guid>https://arxiv.org/abs/2511.13234</guid>
<content:encoded><![CDATA[
arXiv:2511.13234v1 Announce Type: new 
Abstract: Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance ({\sigma}=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification</title>
<link>https://arxiv.org/abs/2511.13237</link>
<guid>https://arxiv.org/abs/2511.13237</guid>
<content:encoded><![CDATA[
arXiv:2511.13237v1 Announce Type: new 
Abstract: Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\geq10\%$ higher confidence while improving sparsity in $\geq40\%$.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms</title>
<link>https://arxiv.org/abs/2511.13238</link>
<guid>https://arxiv.org/abs/2511.13238</guid>
<content:encoded><![CDATA[
arXiv:2511.13238v1 Announce Type: new 
Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incoherent Beliefs &amp; Inconsistent Actions in Large Language Models</title>
<link>https://arxiv.org/abs/2511.13240</link>
<guid>https://arxiv.org/abs/2511.13240</guid>
<content:encoded><![CDATA[
arXiv:2511.13240v1 Announce Type: new 
Abstract: Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering and Mitigating Transient Blindness in Multimodal Model Editing</title>
<link>https://arxiv.org/abs/2511.13243</link>
<guid>https://arxiv.org/abs/2511.13243</guid>
<content:encoded><![CDATA[
arXiv:2511.13243v1 Announce Type: new 
Abstract: Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seek and You Shall Fold</title>
<link>https://arxiv.org/abs/2511.13244</link>
<guid>https://arxiv.org/abs/2511.13244</guid>
<content:encoded><![CDATA[
arXiv:2511.13244v1 Announce Type: new 
Abstract: Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs</title>
<link>https://arxiv.org/abs/2511.13250</link>
<guid>https://arxiv.org/abs/2511.13250</guid>
<content:encoded><![CDATA[
arXiv:2511.13250v1 Announce Type: new 
Abstract: We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KForge: Program Synthesis for Diverse AI Hardware Accelerators</title>
<link>https://arxiv.org/abs/2511.13274</link>
<guid>https://arxiv.org/abs/2511.13274</guid>
<content:encoded><![CDATA[
arXiv:2511.13274v1 Announce Type: new 
Abstract: GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.
  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning</title>
<link>https://arxiv.org/abs/2511.13322</link>
<guid>https://arxiv.org/abs/2511.13322</guid>
<content:encoded><![CDATA[
arXiv:2511.13322v1 Announce Type: new 
Abstract: Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tab-PET: Graph-Based Positional Encodings for Tabular Transformers</title>
<link>https://arxiv.org/abs/2511.13338</link>
<guid>https://arxiv.org/abs/2511.13338</guid>
<content:encoded><![CDATA[
arXiv:2511.13338v1 Announce Type: new 
Abstract: Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model</title>
<link>https://arxiv.org/abs/2511.13339</link>
<guid>https://arxiv.org/abs/2511.13339</guid>
<content:encoded><![CDATA[
arXiv:2511.13339v1 Announce Type: new 
Abstract: Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning</title>
<link>https://arxiv.org/abs/2511.13351</link>
<guid>https://arxiv.org/abs/2511.13351</guid>
<content:encoded><![CDATA[
arXiv:2511.13351v1 Announce Type: new 
Abstract: Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs</title>
<link>https://arxiv.org/abs/2511.13373</link>
<guid>https://arxiv.org/abs/2511.13373</guid>
<content:encoded><![CDATA[
arXiv:2511.13373v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finding Kissing Numbers with Game-theoretic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.13391</link>
<guid>https://arxiv.org/abs/2511.13391</guid>
<content:encoded><![CDATA[
arXiv:2511.13391v1 Announce Type: new 
Abstract: Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast and Robust Simulation-Based Inference With Optimization Monte Carlo</title>
<link>https://arxiv.org/abs/2511.13394</link>
<guid>https://arxiv.org/abs/2511.13394</guid>
<content:encoded><![CDATA[
arXiv:2511.13394v1 Announce Type: new 
Abstract: Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation</title>
<link>https://arxiv.org/abs/2511.13414</link>
<guid>https://arxiv.org/abs/2511.13414</guid>
<content:encoded><![CDATA[
arXiv:2511.13414v1 Announce Type: new 
Abstract: Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction</title>
<link>https://arxiv.org/abs/2511.13419</link>
<guid>https://arxiv.org/abs/2511.13419</guid>
<content:encoded><![CDATA[
arXiv:2511.13419v1 Announce Type: new 
Abstract: Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression</title>
<link>https://arxiv.org/abs/2511.13421</link>
<guid>https://arxiv.org/abs/2511.13421</guid>
<content:encoded><![CDATA[
arXiv:2511.13421v1 Announce Type: new 
Abstract: While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($\Theta(\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \textit{i.e.}, $E(K, N) \approx K$ for $K \le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes</title>
<link>https://arxiv.org/abs/2511.13444</link>
<guid>https://arxiv.org/abs/2511.13444</guid>
<content:encoded><![CDATA[
arXiv:2511.13444v1 Announce Type: new 
Abstract: Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hardware optimization on Android for inference of AI models</title>
<link>https://arxiv.org/abs/2511.13453</link>
<guid>https://arxiv.org/abs/2511.13453</guid>
<content:encoded><![CDATA[
arXiv:2511.13453v1 Announce Type: new 
Abstract: The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure</title>
<link>https://arxiv.org/abs/2511.13457</link>
<guid>https://arxiv.org/abs/2511.13457</guid>
<content:encoded><![CDATA[
arXiv:2511.13457v1 Announce Type: new 
Abstract: Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-task GINN-LP for Multi-target Symbolic Regression</title>
<link>https://arxiv.org/abs/2511.13463</link>
<guid>https://arxiv.org/abs/2511.13463</guid>
<content:encoded><![CDATA[
arXiv:2511.13463v1 Announce Type: new 
Abstract: In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate</title>
<link>https://arxiv.org/abs/2511.13465</link>
<guid>https://arxiv.org/abs/2511.13465</guid>
<content:encoded><![CDATA[
arXiv:2511.13465v1 Announce Type: new 
Abstract: Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction</title>
<link>https://arxiv.org/abs/2511.13469</link>
<guid>https://arxiv.org/abs/2511.13469</guid>
<content:encoded><![CDATA[
arXiv:2511.13469v1 Announce Type: new 
Abstract: Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Machine Learning via Contrastive Training</title>
<link>https://arxiv.org/abs/2511.13497</link>
<guid>https://arxiv.org/abs/2511.13497</guid>
<content:encoded><![CDATA[
arXiv:2511.13497v1 Announce Type: new 
Abstract: Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Naga: Vedic Encoding for Deep State Space Models</title>
<link>https://arxiv.org/abs/2511.13510</link>
<guid>https://arxiv.org/abs/2511.13510</guid>
<content:encoded><![CDATA[
arXiv:2511.13510v1 Announce Type: new 
Abstract: This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data</title>
<link>https://arxiv.org/abs/2511.13514</link>
<guid>https://arxiv.org/abs/2511.13514</guid>
<content:encoded><![CDATA[
arXiv:2511.13514v1 Announce Type: new 
Abstract: Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{\"o}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images</title>
<link>https://arxiv.org/abs/2511.13527</link>
<guid>https://arxiv.org/abs/2511.13527</guid>
<content:encoded><![CDATA[
arXiv:2511.13527v1 Announce Type: new 
Abstract: Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-Aware Graph Representation Learning with Limited Demographic Information</title>
<link>https://arxiv.org/abs/2511.13540</link>
<guid>https://arxiv.org/abs/2511.13540</guid>
<content:encoded><![CDATA[
arXiv:2511.13540v1 Announce Type: new 
Abstract: Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries</title>
<link>https://arxiv.org/abs/2511.13541</link>
<guid>https://arxiv.org/abs/2511.13541</guid>
<content:encoded><![CDATA[
arXiv:2511.13541v1 Announce Type: new 
Abstract: A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise</title>
<link>https://arxiv.org/abs/2511.13561</link>
<guid>https://arxiv.org/abs/2511.13561</guid>
<content:encoded><![CDATA[
arXiv:2511.13561v1 Announce Type: new 
Abstract: Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>P1: Mastering Physics Olympiads with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.13612</link>
<guid>https://arxiv.org/abs/2511.13612</guid>
<content:encoded><![CDATA[
arXiv:2511.13612v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization</title>
<link>https://arxiv.org/abs/2511.13625</link>
<guid>https://arxiv.org/abs/2511.13625</guid>
<content:encoded><![CDATA[
arXiv:2511.13625v1 Announce Type: new 
Abstract: Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Multimodal Representation Learning in Paediatric Kidney Disease</title>
<link>https://arxiv.org/abs/2511.13637</link>
<guid>https://arxiv.org/abs/2511.13637</guid>
<content:encoded><![CDATA[
arXiv:2511.13637v1 Announce Type: new 
Abstract: Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures</title>
<link>https://arxiv.org/abs/2511.13640</link>
<guid>https://arxiv.org/abs/2511.13640</guid>
<content:encoded><![CDATA[
arXiv:2511.13640v1 Announce Type: new 
Abstract: The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FuseSampleAgg: Fused Neighbor Sampling and Aggregation for Mini-batch GNNs</title>
<link>https://arxiv.org/abs/2511.13645</link>
<guid>https://arxiv.org/abs/2511.13645</guid>
<content:encoded><![CDATA[
arXiv:2511.13645v1 Announce Type: new 
Abstract: We present FuseSampleAgg, a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for one and two hop GraphSAGE. By eliminating block materialization and extra kernel launches, FuseSampleAgg reduces memory traffic and overhead while preserving GraphSAGE mean semantics via saved index replay. Across the Reddit, ogbn-arxiv, and ogbn-products benchmarks (batch size 1024, automatic mixed precision enabled), we observe step time speedups up to 51x on ogbn-products, about 4x on Reddit with fanouts 10-10 and 15-10, and about 3.3x on ogbn-arxiv at larger fanouts, with peak GPU memory reductions up to 100x, 36x, and about 3.5x, respectively. The operator is deterministic, integrates with standard PyTorch optimizers, and ships with scripts that reproduce all tables and figures from CSV logs. Code and scripts are available at https://github.com/SV25-22/FuseSampleAgg.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weight-sparse transformers have interpretable circuits</title>
<link>https://arxiv.org/abs/2511.13653</link>
<guid>https://arxiv.org/abs/2511.13653</guid>
<content:encoded><![CDATA[
arXiv:2511.13653v1 Announce Type: new 
Abstract: Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning</title>
<link>https://arxiv.org/abs/2511.13654</link>
<guid>https://arxiv.org/abs/2511.13654</guid>
<content:encoded><![CDATA[
arXiv:2511.13654v1 Announce Type: new 
Abstract: In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scientific Data Compression and Super-Resolution Sampling</title>
<link>https://arxiv.org/abs/2511.13675</link>
<guid>https://arxiv.org/abs/2511.13675</guid>
<content:encoded><![CDATA[
arXiv:2511.13675v1 Announce Type: new 
Abstract: Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Learning from Scarce Data via Multi-Task Constrained Optimization</title>
<link>https://arxiv.org/abs/2511.13680</link>
<guid>https://arxiv.org/abs/2511.13680</guid>
<content:encoded><![CDATA[
arXiv:2511.13680v1 Announce Type: new 
Abstract: A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \emph{cross-learning} framework to overcome data scarcity by jointly estimating \emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers</title>
<link>https://arxiv.org/abs/2511.13685</link>
<guid>https://arxiv.org/abs/2511.13685</guid>
<content:encoded><![CDATA[
arXiv:2511.13685v1 Announce Type: new 
Abstract: In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Calibration for Decision Making</title>
<link>https://arxiv.org/abs/2511.13699</link>
<guid>https://arxiv.org/abs/2511.13699</guid>
<content:encoded><![CDATA[
arXiv:2511.13699v1 Announce Type: new 
Abstract: A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.
  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning stochasticity: a nonparametric framework for intrinsic noise estimation</title>
<link>https://arxiv.org/abs/2511.13701</link>
<guid>https://arxiv.org/abs/2511.13701</guid>
<content:encoded><![CDATA[
arXiv:2511.13701v1 Announce Type: new 
Abstract: Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification</title>
<link>https://arxiv.org/abs/2511.13702</link>
<guid>https://arxiv.org/abs/2511.13702</guid>
<content:encoded><![CDATA[
arXiv:2511.13702v1 Announce Type: new 
Abstract: Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering</title>
<link>https://arxiv.org/abs/2511.13705</link>
<guid>https://arxiv.org/abs/2511.13705</guid>
<content:encoded><![CDATA[
arXiv:2511.13705v1 Announce Type: new 
Abstract: Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI "Gene Expression Cancer RNA-Seq" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Black Box to Insight: Explainable AI for Extreme Event Preparedness</title>
<link>https://arxiv.org/abs/2511.13712</link>
<guid>https://arxiv.org/abs/2511.13712</guid>
<content:encoded><![CDATA[
arXiv:2511.13712v1 Announce Type: new 
Abstract: As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization</title>
<link>https://arxiv.org/abs/2506.14157</link>
<guid>https://arxiv.org/abs/2506.14157</guid>
<content:encoded><![CDATA[
arXiv:2506.14157v1 Announce Type: cross 
Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Limitations of Quantum Advantage in Unsupervised Machine Learning</title>
<link>https://arxiv.org/abs/2511.10709</link>
<guid>https://arxiv.org/abs/2511.10709</guid>
<content:encoded><![CDATA[
arXiv:2511.10709v1 Announce Type: cross 
Abstract: Machine learning models are used for pattern recognition analysis of big data, without direct human intervention. The task of unsupervised learning is to find the probability distribution that would best describe the available data, and then use it to make predictions for observables of interest. Classical models generally fit the data to Boltzmann distribution of Hamiltonians with a large number of tunable parameters. Quantum extensions of these models replace classical probability distributions with quantum density matrices. An advantage can be obtained only when features of density matrices that are absent in classical probability distributions are exploited. Such situations depend on the input data as well as the targeted observables. Explicit examples are discussed that bring out the constraints limiting possible quantum advantage. The problem-dependent extent of quantum advantage has implications for both data analysis and sensing applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Architecture, Scaling Laws, and Economics: A Quick Summary</title>
<link>https://arxiv.org/abs/2511.11572</link>
<guid>https://arxiv.org/abs/2511.11572</guid>
<content:encoded><![CDATA[
arXiv:2511.11572v1 Announce Type: cross 
Abstract: The current standard architecture of Large Language Models (LLMs) with QKV self-attention is briefly summarized, including the architecture of a typical Transformer. Scaling laws for compute (flops) and memory (parameters plus data) are given, along with their present (2025) rough cost estimates for the parameters of present LLMs of various scales, including discussion of whether DeepSeek should be viewed as a special case. Nothing here is new, but this material seems not otherwise readily available in summary form.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social and Physical Attributes-Defined Trust Evaluation for Effective Collaborator Selection in Human-Device Coexistence Systems</title>
<link>https://arxiv.org/abs/2511.11578</link>
<guid>https://arxiv.org/abs/2511.11578</guid>
<content:encoded><![CDATA[
arXiv:2511.11578v1 Announce Type: cross 
Abstract: In human-device coexistence systems, collaborations among devices are determined by not only physical attributes such as network topology but also social attributes among human users. Consequently, trust evaluation of potential collaborators based on these multifaceted attributes becomes critical for ensuring the eventual outcome. However, due to the high heterogeneity and complexity of physical and social attributes, efficiently integrating them for accurate trust evaluation remains challenging. To overcome this difficulty, a canonical correlation analysis-enhanced hypergraph self-supervised learning (HSLCCA) method is proposed in this research. First, by treating all attributes as relationships among connected devices, a relationship hypergraph is constructed to comprehensively capture inter-device relationships across three dimensions: spatial attribute-related, device attribute-related, and social attribute-related. Next, a self-supervised learning framework is developed to integrate these multi-dimensional relationships and generate device embeddings enriched with relational semantics. In this learning framework, the relationship hypergraph is augmented into two distinct views to enhance semantic information. A parameter-sharing hypergraph neural network is then utilized to learn device embeddings from both views. To further enhance embedding quality, a CCA approach is applied, allowing the comparison of data between the two views. Finally, the trustworthiness of devices is calculated based on the learned device embeddings. Extensive experiments demonstrate that the proposed HSLCCA method significantly outperforms the baseline algorithm in effectively identifying trusted devices.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators</title>
<link>https://arxiv.org/abs/2511.11601</link>
<guid>https://arxiv.org/abs/2511.11601</guid>
<content:encoded><![CDATA[
arXiv:2511.11601v1 Announce Type: cross 
Abstract: While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Skill and Chance: A Unified Framework for the Geometry of Games</title>
<link>https://arxiv.org/abs/2511.11611</link>
<guid>https://arxiv.org/abs/2511.11611</guid>
<content:encoded><![CDATA[
arXiv:2511.11611v1 Announce Type: cross 
Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Network-based Reliability Analysis of Buried Pipelines</title>
<link>https://arxiv.org/abs/2511.11613</link>
<guid>https://arxiv.org/abs/2511.11613</guid>
<content:encoded><![CDATA[
arXiv:2511.11613v1 Announce Type: cross 
Abstract: Buried pipelines transporting oil and gas across geohazard-prone regions are exposed to potential ground movement, leading to the risk of significant strain demand and structural failure. Reliability analysis, which determines the probability of failure after accounting for pertinent uncertainties, is essential for ensuring the safety of pipeline systems. However, traditional reliability analysis methods involving computationally intensive numerical models, such as finite element simulations of pipeline subjected to ground movement, have limited applications; this is partly because stochastic sampling approaches require repeated simulations over a large number of samples for the uncertain variables when estimating low probabilities. This study introduces Physics-Informed Neural Network for Reliability Analysis (PINN-RA) for buried pipelines subjected to ground movement, which integrates PINN-based surrogate model with Monte Carlo Simulation (MCS) to achieve efficient reliability assessment. To enable its application under uncertain variables associated with soil properties and ground movement, the PINN-based surrogate model is extended to solve a parametric differential equation system, namely the governing equation of pipelines embedded in soil with different properties. The findings demonstrate that PINN-RA significantly reduces the computational effort required and thus accelerates reliability analysis. By eliminating the need for repetitive numerical evaluations of pipeline subjected to permanent ground movement, the proposed approach provides an efficient and scalable tool for pipeline reliability assessment, enabling rapid decision-making in geohazard-prone regions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight Hopfield Neural Networks for Bioacoustic Detection and Call Monitoring of Captive Primates</title>
<link>https://arxiv.org/abs/2511.11615</link>
<guid>https://arxiv.org/abs/2511.11615</guid>
<content:encoded><![CDATA[
arXiv:2511.11615v1 Announce Type: cross 
Abstract: Passive acoustic monitoring is a sustainable method of monitoring wildlife and environments that leads to the generation of large datasets and, currently, a processing backlog. Academic research into automating this process is focused on the application of resource intensive convolutional neural networks which require large pre-labelled datasets for training and lack flexibility in application. We present a viable alternative relevant in both wild and captive settings; a transparent, lightweight and fast-to-train associative memory AI model with Hopfield neural network (HNN) architecture. Adapted from a model developed to detect bat echolocation calls, this model monitors captive endangered black-and-white ruffed lemur Varecia variegata vocalisations. Lemur social calls of interest when monitoring welfare are stored in the HNN in order to detect other call instances across the larger acoustic dataset. We make significant model improvements by storing an additional signal caused by movement and achieve an overall accuracy of 0.94. The model can perform $340$ classifications per second, processing over 5.5 hours of audio data per minute, on a standard laptop running other applications. It has broad applicability and trains in milliseconds. Our lightweight solution reduces data-to-insight turnaround times and can accelerate decision making in both captive and wild settings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance</title>
<link>https://arxiv.org/abs/2511.11616</link>
<guid>https://arxiv.org/abs/2511.11616</guid>
<content:encoded><![CDATA[
arXiv:2511.11616v1 Announce Type: cross 
Abstract: The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(\epsilon \in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\%$ and the Byzantine fault tolerance of $f < n/3$.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges</title>
<link>https://arxiv.org/abs/2511.11624</link>
<guid>https://arxiv.org/abs/2511.11624</guid>
<content:encoded><![CDATA[
arXiv:2511.11624v1 Announce Type: cross 
Abstract: Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omics-scale polymer computational database transferable to real-world artificial intelligence applications</title>
<link>https://arxiv.org/abs/2511.11626</link>
<guid>https://arxiv.org/abs/2511.11626</guid>
<content:encoded><![CDATA[
arXiv:2511.11626v1 Announce Type: cross 
Abstract: Developing large-scale foundational datasets is a critical milestone in advancing artificial intelligence (AI)-driven scientific innovation. However, unlike AI-mature fields such as natural language processing, materials science, particularly polymer research, has significantly lagged in developing extensive open datasets. This lag is primarily due to the high costs of polymer synthesis and property measurements, along with the vastness and complexity of the chemical space. This study presents PolyOmics, an omics-scale computational database generated through fully automated molecular dynamics simulation pipelines that provide diverse physical properties for over $10^5$ polymeric materials. The PolyOmics database is collaboratively developed by approximately 260 researchers from 48 institutions to bridge the gap between academia and industry. Machine learning models pretrained on PolyOmics can be efficiently fine-tuned for a wide range of real-world downstream tasks, even when only limited experimental data are available. Notably, the generalisation capability of these simulation-to-real transfer models improve significantly as the size of the PolyOmics database increases, exhibiting power-law scaling. The emergence of scaling laws supports the "more is better" principle, highlighting the significance of ultralarge-scale computational materials data for improving real-world prediction performance. This unprecedented omics-scale database reveals vast unexplored regions of polymer materials, providing a foundation for AI-driven polymer science.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding</title>
<link>https://arxiv.org/abs/2511.11634</link>
<guid>https://arxiv.org/abs/2511.11634</guid>
<content:encoded><![CDATA[
arXiv:2511.11634v1 Announce Type: cross 
Abstract: The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications</title>
<link>https://arxiv.org/abs/2511.11640</link>
<guid>https://arxiv.org/abs/2511.11640</guid>
<content:encoded><![CDATA[
arXiv:2511.11640v1 Announce Type: cross 
Abstract: Speculative backpropagation has emerged as a promising technique to accelerate the training of neural networks by overlapping the forward and backward passes. Leveraging speculative weight updates when error gradients fall within a specific threshold reduces training time without substantially compromising accuracy. In this work, we implement speculative backpropagation on the MNIST dataset using OpenMP as the parallel programming platform. OpenMP's multi-threading capabilities enable simultaneous execution of forward and speculative backpropagation steps, significantly improving training speed. The application is planned for synthesis on a state-of-the-art FPGA to demonstrate its potential for hardware acceleration. Our CPU-based experimental results demonstrate that speculative backpropagation achieves a maximum speedup of 24% in execution time when using a threshold of 0.25, and accuracy remaining within 3-4% of the baseline across various epochs. Additionally, when comparing individual step execution time, speculative backpropagation yields a maximum speedup of 35% over the baseline, demonstrating the effectiveness of overlapping forward and backward passes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Environmental Impact of Ensemble Techniques in Recommender Systems</title>
<link>https://arxiv.org/abs/2511.11649</link>
<guid>https://arxiv.org/abs/2511.11649</guid>
<content:encoded><![CDATA[
arXiv:2511.11649v1 Announce Type: cross 
Abstract: Ensemble techniques in recommender systems have demonstrated accuracy improvements of 10-30%, yet their environmental impact remains unmeasured. While deep learning recommendation algorithms can generate up to 3,297 kg CO2 per paper, ensemble methods have not been sufficiently evaluated for energy consumption. This thesis investigates how ensemble techniques influence environmental impact compared to single optimized models.
  We conducted 93 experiments across two frameworks (Surprise for rating prediction, LensKit for ranking) on four datasets spanning 100,000 to 7.8 million interactions. We evaluated four ensemble strategies (Average, Weighted, Stacking/Rank Fusion, Top Performers) against simple baselines and optimized single models, measuring energy consumption with a smart plug.
  Results revealed a non-linear accuracy-energy relationship. Ensemble methods achieved 0.3-5.7% accuracy improvements while consuming 19-2,549% more energy depending on dataset size and strategy. The Top Performers ensemble showed best efficiency: 0.96% RMSE improvement with 18.8% energy overhead on MovieLens-1M, and 5.7% NDCG improvement with 103% overhead on MovieLens-100K. Exhaustive averaging strategies consumed 88-270% more energy for comparable gains. On the largest dataset (Anime, 7.8M interactions), the Surprise ensemble consumed 2,005% more energy (0.21 Wh vs. 0.01 Wh) for 1.2% accuracy improvement, producing 53.8 mg CO2 versus 2.6 mg CO2 for the single model.
  This research provides one of the first systematic measurements of energy and carbon footprint for ensemble recommender systems, demonstrates that selective strategies offer superior efficiency over exhaustive averaging, and identifies scalability limitations at industrial scale. These findings enable informed decisions about sustainable algorithm selection in recommender systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.11653</link>
<guid>https://arxiv.org/abs/2511.11653</guid>
<content:encoded><![CDATA[
arXiv:2511.11653v1 Announce Type: cross 
Abstract: Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems</title>
<link>https://arxiv.org/abs/2511.11678</link>
<guid>https://arxiv.org/abs/2511.11678</guid>
<content:encoded><![CDATA[
arXiv:2511.11678v1 Announce Type: cross 
Abstract: The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Inequality-based Approach for Probabilistic WCET Estimation</title>
<link>https://arxiv.org/abs/2511.11682</link>
<guid>https://arxiv.org/abs/2511.11682</guid>
<content:encoded><![CDATA[
arXiv:2511.11682v1 Announce Type: cross 
Abstract: Estimating the probabilistic Worst-Case Execution Time (pWCET) is essential for ensuring the timing correctness of real-time applications, such as in robot IoT systems and autonomous driving systems. While methods based on Extreme Value Theory (EVT) can provide tight bounds, they suffer from model uncertainty due to the need to decide where the upper tail of the distribution begins. Conversely, inequality-based approaches avoid this issue but can yield pessimistic results for heavy-tailed distributions. This paper proposes a method to reduce such pessimism by incorporating saturating functions (arctangent and hyperbolic tangent) into Chebyshev's inequality, which mitigates the influence of large outliers while preserving mathematical soundness. Evaluations on synthetic and real-world data from the Autoware autonomous driving stack demonstrate that the proposed method achieves safe and tighter bounds for such distributions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation</title>
<link>https://arxiv.org/abs/2511.11693</link>
<guid>https://arxiv.org/abs/2511.11693</guid>
<content:encoded><![CDATA[
arXiv:2511.11693v1 Announce Type: cross 
Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks</title>
<link>https://arxiv.org/abs/2511.11729</link>
<guid>https://arxiv.org/abs/2511.11729</guid>
<content:encoded><![CDATA[
arXiv:2511.11729v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized.
  We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows</title>
<link>https://arxiv.org/abs/2511.11739</link>
<guid>https://arxiv.org/abs/2511.11739</guid>
<content:encoded><![CDATA[
arXiv:2511.11739v1 Announce Type: cross 
Abstract: Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D printing, where noise may cause structural or economic failures. This contribution presents a noise-aware decision-making algorithm that quantifies and models device-specific noise profiles to manage variability adaptively. It uses distributional analysis and pairwise divergence metrics with clustering to choose between single-device and robust multi-device Bayesian optimization strategies. Unlike conventional methods that assume homogeneous devices or generic robustness, this framework explicitly leverages inter-device differences to enhance performance, reproducibility, and efficiency. An experimental case study involving three nominally identical 3D printers (same brand, model, and close serial numbers) demonstrates reduced redundancy, lower resource usage, and improved reliability. Overall, this framework establishes a paradigm for precision- and resource-aware optimization in scalable, automated experimental platforms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Socrates-Mol: Self-Oriented Cognitive Reasoning through Autonomous Trial-and-Error with Empirical-Bayesian Screening for Molecules</title>
<link>https://arxiv.org/abs/2511.11769</link>
<guid>https://arxiv.org/abs/2511.11769</guid>
<content:encoded><![CDATA[
arXiv:2511.11769v1 Announce Type: cross 
Abstract: Molecular property prediction is fundamental to chemical engineering applications such as solvent screening. We present Socrates-Mol, a framework that transforms language models into empirical Bayesian reasoners through context engineering, addressing cold start problems without model fine-tuning. The system implements a reflective-prediction cycle where initial outputs serve as priors, retrieved molecular cases provide evidence, and refined predictions form posteriors, extracting reusable chemical rules from sparse data. We introduce ranking tasks aligned with industrial screening priorities and employ cross-model self-consistency across five language models to reduce variance. Experiments on amine solvent LogP prediction reveal task-dependent patterns: regression achieves 72% MAE reduction and 112% R-squared improvement through self-consistency, while ranking tasks show limited gains due to systematic multi-model biases. The framework reduces deployment costs by over 70% compared to full fine-tuning, providing a scalable solution for molecular property prediction while elucidating the task-adaptive nature of self-consistency mechanisms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction</title>
<link>https://arxiv.org/abs/2511.11770</link>
<guid>https://arxiv.org/abs/2511.11770</guid>
<content:encoded><![CDATA[
arXiv:2511.11770v1 Announce Type: cross 
Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Measure of a Model: From Intelligence to Generality</title>
<link>https://arxiv.org/abs/2511.11773</link>
<guid>https://arxiv.org/abs/2511.11773</guid>
<content:encoded><![CDATA[
arXiv:2511.11773v1 Announce Type: cross 
Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Mitigating Systematics in Large-Scale Surveys via Few-Shot Optimal Transport-Based Feature Alignment</title>
<link>https://arxiv.org/abs/2511.11787</link>
<guid>https://arxiv.org/abs/2511.11787</guid>
<content:encoded><![CDATA[
arXiv:2511.11787v1 Announce Type: cross 
Abstract: Systematics contaminate observables, leading to distribution shifts relative to theoretically simulated signals-posing a major challenge for using pre-trained models to label such observables. Since systematics are often poorly understood and difficult to model, removing them directly and entirely may not be feasible. To address this challenge, we propose a novel method that aligns learned features between in-distribution (ID) and out-of-distribution (OOD) samples by optimizing a feature-alignment loss on the representations extracted from a pre-trained ID model. We first experimentally validate the method on the MNIST dataset using possible alignment losses, including mean squared error and optimal transport, and subsequently apply it to large-scale maps of neutral hydrogen. Our results show that optimal transport is particularly effective at aligning OOD features when parity between ID and OOD samples is unknown, even with limited data-mimicking real-world conditions in extracting information from large-scale surveys. Our code is available at https://github.com/sultan-hassan/feature-alignment-for-OOD-generalization.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreDN: Spectral Disentanglement for Time Series Forecasting via Learnable Frequency Decomposition</title>
<link>https://arxiv.org/abs/2511.11817</link>
<guid>https://arxiv.org/abs/2511.11817</guid>
<content:encoded><![CDATA[
arXiv:2511.11817v1 Announce Type: cross 
Abstract: Time series forecasting is essential in a wide range of real world applications. Recently, frequency-domain methods have attracted increasing interest for their ability to capture global dependencies. However, when applied to non-stationary time series, these methods encounter the $\textit{spectral entanglement}$ and the computational burden of complex-valued learning. The $\textit{spectral entanglement}$ refers to the overlap of trends, periodicities, and noise across the spectrum due to $\textit{spectral leakage}$ and the presence of non-stationarity. However, existing decompositions are not suited to resolving spectral entanglement. To address this, we propose the Frequency Decomposition Network (FreDN), which introduces a learnable Frequency Disentangler module to separate trend and periodic components directly in the frequency domain. Furthermore, we propose a theoretically supported ReIm Block to reduce the complexity of complex-valued operations while maintaining performance. We also re-examine the frequency-domain loss function and provide new theoretical insights into its effectiveness. Extensive experiments on seven long-term forecasting benchmarks demonstrate that FreDN outperforms state-of-the-art methods by up to 10\%. Furthermore, compared with standard complex-valued architectures, our real-imaginary shared-parameter design reduces the parameter count and computational cost by at least 50\%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Computational Method for Solving the Stochastic Joint Replenishment Problem in High Dimensions</title>
<link>https://arxiv.org/abs/2511.11830</link>
<guid>https://arxiv.org/abs/2511.11830</guid>
<content:encoded><![CDATA[
arXiv:2511.11830v1 Announce Type: cross 
Abstract: We consider a discrete-time formulation for a class of high-dimensional stochastic joint replenishment problems. First, we approximate the problem by a continuous-time impulse control problem. Exploiting connections among the impulse control problem, backward stochastic differential equations (BSDEs) with jumps, and the stochastic target problem, we develop a novel, simulation-based computational method that relies on deep neural networks to solve the impulse control problem. Based on that solution, we propose an implementable inventory control policy for the original (discrete-time) stochastic joint replenishment problem, and test it against the best available benchmarks in a series of test problems. For the problems studied thus far, our method matches or beats the best benchmark we could find, and it is computationally feasible up to at least 50 dimensions -- that is, 50 stock-keeping units (SKUs).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.11831</link>
<guid>https://arxiv.org/abs/2511.11831</guid>
<content:encoded><![CDATA[
arXiv:2511.11831v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning</title>
<link>https://arxiv.org/abs/2511.11837</link>
<guid>https://arxiv.org/abs/2511.11837</guid>
<content:encoded><![CDATA[
arXiv:2511.11837v1 Announce Type: cross 
Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling X-ray photon pile-up with a normalizing flow</title>
<link>https://arxiv.org/abs/2511.11863</link>
<guid>https://arxiv.org/abs/2511.11863</guid>
<content:encoded><![CDATA[
arXiv:2511.11863v1 Announce Type: cross 
Abstract: The dynamic range of imaging detectors flown on-board X-ray observatories often only covers a limited flux range of extrasolar X-ray sources. The analysis of bright X-ray sources is complicated by so-called pile-up, which results from high incident photon flux. This nonlinear effect distorts the measured spectrum, resulting in biases in the inferred physical parameters, and can even lead to a complete signal loss in extreme cases. Piled-up data are commonly discarded due to resulting intractability of the likelihood. As a result, a large number of archival observations remain underexplored. We present a machine learning solution to this problem, using a simulation-based inference framework that allows us to estimate posterior distributions of physical source parameters from piled-up eROSITA data. We show that a normalizing flow produces better-constrained posterior densities than traditional mitigation techniques, as more data can be leveraged. We consider model- and calibration-dependent uncertainties and the applicability of such an algorithm to real data in the eROSITA archive.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts</title>
<link>https://arxiv.org/abs/2511.11883</link>
<guid>https://arxiv.org/abs/2511.11883</guid>
<content:encoded><![CDATA[
arXiv:2511.11883v1 Announce Type: cross 
Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetting-MarI: LLM Unlearning via Marginal Information Regularization</title>
<link>https://arxiv.org/abs/2511.11914</link>
<guid>https://arxiv.org/abs/2511.11914</guid>
<content:encoded><![CDATA[
arXiv:2511.11914v1 Announce Type: cross 
Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Additive Large Language Models for Semi-Structured Text</title>
<link>https://arxiv.org/abs/2511.11922</link>
<guid>https://arxiv.org/abs/2511.11922</guid>
<content:encoded><![CDATA[
arXiv:2511.11922v1 Announce Type: cross 
Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCA recovery thresholds in low-rank matrix inference with sparse noise</title>
<link>https://arxiv.org/abs/2511.11927</link>
<guid>https://arxiv.org/abs/2511.11927</guid>
<content:encoded><![CDATA[
arXiv:2511.11927v1 Announce Type: cross 
Abstract: We study the high-dimensional inference of a rank-one signal corrupted by sparse noise. The noise is modelled as the adjacency matrix of a weighted undirected graph with finite average connectivity in the large size limit. Using the replica method from statistical physics, we analytically compute the typical value of the top eigenvalue, the top eigenvector component density, and the overlap between the signal vector and the top eigenvector. The solution is given in terms of recursive distributional equations for auxiliary probability density functions which can be efficiently solved using a population dynamics algorithm. Specialising the noise matrix to Poissonian and Random Regular degree distributions, the critical signal strength is analytically identified at which a transition happens for the recovery of the signal via the top eigenvector, thus generalising the celebrated BBP transition to the sparse noise case. In the large-connectivity limit, known results for dense noise are recovered. Analytical results are in agreement with numerical diagonalisation of large matrices.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing XR Auditory Realism via Multimodal Scene-Aware Acoustic Rendering</title>
<link>https://arxiv.org/abs/2511.11930</link>
<guid>https://arxiv.org/abs/2511.11930</guid>
<content:encoded><![CDATA[
arXiv:2511.11930v1 Announce Type: cross 
Abstract: In Extended Reality (XR), rendering sound that accurately simulates real-world acoustics is pivotal in creating lifelike and believable virtual experiences. However, existing XR spatial audio rendering methods often struggle with real-time adaptation to diverse physical scenes, causing a sensory mismatch between visual and auditory cues that disrupts user immersion. To address this, we introduce SAMOSA, a novel on-device system that renders spatially accurate sound by dynamically adapting to its physical environment. SAMOSA leverages a synergistic multimodal scene representation by fusing real-time estimations of room geometry, surface materials, and semantic-driven acoustic context. This rich representation then enables efficient acoustic calibration via scene priors, allowing the system to synthesize a highly realistic Room Impulse Response (RIR). We validate our system through technical evaluation using acoustic metrics for RIR synthesis across various room configurations and sound types, alongside an expert evaluation (N=12). Evaluation results demonstrate SAMOSA's feasibility and efficacy in enhancing XR auditory realism.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InData: Towards Secure Multi-Step, Tool-Based Data Analysis</title>
<link>https://arxiv.org/abs/2511.11933</link>
<guid>https://arxiv.org/abs/2511.11933</guid>
<content:encoded><![CDATA[
arXiv:2511.11933v1 Announce Type: cross 
Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Deep Learning Framework for Thyroid Nodule Segmentation and Malignancy Classification from Ultrasound Images</title>
<link>https://arxiv.org/abs/2511.11937</link>
<guid>https://arxiv.org/abs/2511.11937</guid>
<content:encoded><![CDATA[
arXiv:2511.11937v1 Announce Type: cross 
Abstract: Ultrasound-based risk stratification of thyroid nodules is a critical clinical task, but it suffers from high inter-observer variability. While many deep learning (DL) models function as "black boxes," we propose a fully automated, two-stage framework for interpretable malignancy prediction. Our method achieves interpretability by forcing the model to focus only on clinically relevant regions. First, a TransUNet model automatically segments the thyroid nodule. The resulting mask is then used to create a region of interest around the nodule, and this localised image is fed directly into a ResNet-18 classifier. We evaluated our framework using 5-fold cross-validation on a clinical dataset of 349 images, where it achieved a high F1-score of 0.852 for predicting malignancy. To validate its performance, we compared it against a strong baseline using a Random Forest classifier with hand-crafted morphological features, which achieved an F1-score of 0.829. The superior performance of our DL framework suggests that the implicit visual features learned from the localised nodule are more predictive than explicit shape features alone. This is the first fully automated end-to-end pipeline for both detecting thyroid nodules on ultrasound images and predicting their malignancy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Neutrino Oscillation Measurements through Event Classification</title>
<link>https://arxiv.org/abs/2511.11938</link>
<guid>https://arxiv.org/abs/2511.11938</guid>
<content:encoded><![CDATA[
arXiv:2511.11938v1 Announce Type: cross 
Abstract: Precise neutrino energy reconstruction is essential for next-generation long-baseline oscillation experiments, yet current methods remain limited by large uncertainties in neutrino-nucleus interaction modeling. Even so, it is well established that different interaction channels produce systematically varying amounts of missing energy and therefore yield different reconstruction performance--information that standard calorimetric approaches do not exploit. We introduce a strategy that incorporates this structure by classifying events according to their underlying interaction type prior to energy reconstruction. Using supervised machine-learning techniques trained on labeled generator events, we leverage intrinsic kinematic differences among quasi-elastic scattering, meson-exchange current, resonance production, and deep-inelastic scattering processes. A cross-generator testing framework demonstrates that this classification approach is robust to microphysics mismodeling and, when applied to a simulated DUNE $\nu_\mu$ disappearance analysis, yields improved accuracy and sensitivity. These results highlight a practical path toward reducing reconstruction-driven systematics in future oscillation measurements.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes</title>
<link>https://arxiv.org/abs/2511.11945</link>
<guid>https://arxiv.org/abs/2511.11945</guid>
<content:encoded><![CDATA[
arXiv:2511.11945v1 Announce Type: cross 
Abstract: In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization</title>
<link>https://arxiv.org/abs/2511.11946</link>
<guid>https://arxiv.org/abs/2511.11946</guid>
<content:encoded><![CDATA[
arXiv:2511.11946v1 Announce Type: cross 
Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Micro-Doppler Spectrogram-based ViT Multiclass Target Classification</title>
<link>https://arxiv.org/abs/2511.11951</link>
<guid>https://arxiv.org/abs/2511.11951</guid>
<content:encoded><![CDATA[
arXiv:2511.11951v1 Announce Type: cross 
Abstract: In this paper, we propose a new Temporal MDS-Vision Transformer (T-MDS-ViT) for multiclass target classification using millimeter-wave FMCW radar micro-Doppler spectrograms. Specifically, we design a transformer-based architecture that processes stacked range-velocity-angle (RVA) spatiotemporal tensors via patch embeddings and cross-axis attention mechanisms to explicitly model the sequential nature of MDS data across multiple frames. The T-MDS-ViT exploits mobility-aware constraints in its attention layer correspondences to maintain separability under target overlaps and partial occlusions. Next, we apply an explainable mechanism to examine how the attention layers focus on characteristic high-energy regions of the MDS representations and their effect on class-specific kinematic features. We also demonstrate that our proposed framework is superior to existing CNN-based methods in terms of classification accuracy while achieving better data efficiency and real-time deployability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Entropy Calibration of Language Models</title>
<link>https://arxiv.org/abs/2511.11966</link>
<guid>https://arxiv.org/abs/2511.11966</guid>
<content:encoded><![CDATA[
arXiv:2511.11966v1 Announce Type: cross 
Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian--AI Fusion for Epidemiological Decision Making: Calibrated Risk, Honest Uncertainty, and Hyperparameter Intelligence</title>
<link>https://arxiv.org/abs/2511.11983</link>
<guid>https://arxiv.org/abs/2511.11983</guid>
<content:encoded><![CDATA[
arXiv:2511.11983v1 Announce Type: cross 
Abstract: Modern epidemiological analytics increasingly use machine learning models that offer strong prediction but often lack calibrated uncertainty. Bayesian methods provide principled uncertainty quantification, yet are viewed as difficult to integrate with contemporary AI workflows. This paper proposes a unified Bayesian and AI framework that combines Bayesian prediction with Bayesian hyperparameter optimization.
  We use Bayesian logistic regression to obtain calibrated individual-level disease risk and credible intervals on the Pima Indians Diabetes dataset. In parallel, we use Gaussian-process Bayesian optimization to tune penalized Cox survival models on the GBSG2 breast cancer cohort. This yields a two-layer system: a Bayesian predictive layer that represents risk as a posterior distribution, and a Bayesian optimization layer that treats model selection as inference over a black-box objective.
  Simulation studies in low- and high-dimensional regimes show that the Bayesian layer provides reliable coverage and improved calibration, while Bayesian shrinkage improves AUC, Brier score, and log-loss. Bayesian optimization consistently pushes survival models toward near-oracle concordance. Overall, Bayesian reasoning enhances both what we infer and how we search, enabling calibrated risk and principled hyperparameter intelligence for epidemiological decision making.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams</title>
<link>https://arxiv.org/abs/2511.11992</link>
<guid>https://arxiv.org/abs/2511.11992</guid>
<content:encoded><![CDATA[
arXiv:2511.11992v1 Announce Type: cross 
Abstract: Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks</title>
<link>https://arxiv.org/abs/2511.11993</link>
<guid>https://arxiv.org/abs/2511.11993</guid>
<content:encoded><![CDATA[
arXiv:2511.11993v1 Announce Type: cross 
Abstract: Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy</title>
<link>https://arxiv.org/abs/2511.12006</link>
<guid>https://arxiv.org/abs/2511.12006</guid>
<content:encoded><![CDATA[
arXiv:2511.12006v1 Announce Type: cross 
Abstract: Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>